2020.acl-demos.2,P19-1595,0,0.104085,"f transferring knowledge from a teacher model to a student model, which is usually smaller than the teacher. The student model is trained to mimic the outputs of the teacher model. Before the birth of BERT, KD had been applied to several specific tasks like machine translation (Kim and Rush, 2016; Tan et al., 2019) in NLP. While the recent studies of distilling large pre-trained models focus on finding general distillation methods that work on various tasks and are receiving more and more attention (Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019a; Tang et al., 2019; Liu et al., 2019a; Clark et al., 2019; Zhao et al., 2019). Though various distillation methods have been proposed, they usually share a common workflow: firstly, train a teacher model, then optimize the student model by minimizing some losses that are calculated between the outputs of the teacher and the student. Therefore it is desirable to have a reusable distillation workflow framework and treat different distillation strategies and tricks as plugins so that they could be easily and arbitrarily added to the framework. In this way, we could also achieve great flexibility in experimenting with different combinations of distillat"
2020.acl-demos.2,C18-1166,0,0.0258997,"Missing"
2020.acl-demos.2,D18-1269,0,0.0161826,"nd CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003) for named entity recognition (NER) task. For Chinese datasets, we use the Chinese part of XNLI 4 More results are presented in the online documentation: https://textbrewer.readthedocs.io 12 Dataset Task Metrics #Train #Dev MNLI SQuAD CoNLL-2003 Classification MRC NER Acc EM/F1 F1 393K 88K 23K 20K 11K 6K XNLI LCQMC CMRC 2018 DRCD Classification Classification MRC MRC Acc Acc EM/F1 EM/F1 393K 293K 10K 27K 2.5K 8.8K 3.4K 3.5K Model Table 1: A summary of the datasets used in experiments. The size of CoNLL-2003 is measured in number of entities. (Conneau et al., 2018), LCQMC (Liu et al., 2018), CMRC 2018 (Cui et al., 2019b) and DRCD (Shao et al., 2018). XNLI is the multilingual version of MNLI. LCQMC is a large-scale Chinese question matching corpus. CMRC 2018 and DRCD are two span-extraction machine reading comprehension datasets similar to SQuAD. The statistics of the datasets are listed in Table 1. Models. All the teachers are BERTBASE -based models. For English tasks, teachers are initialized with the weights released by Google5 and converted into PyTorch format via Transformers6 . For Chinese tasks, teacher is initialized with the pre-trained RoBERTa-"
2020.acl-demos.2,2021.ccl-1.108,0,0.0842688,"Missing"
2020.acl-demos.2,D16-1264,0,0.0298942,"rocedures for all NLP tasks, we encourage users to implement their own evaluation functions as the callbacks for the best practice. 4 Experiments In this section, we conduct several experiments to show TextBrewer’s ability to distill large pretrained models on different NLP tasks and achieve results are comparable with or even higher than the public distilled BERT models with similar numbers of parameters. 4 4.1 Settings Datasets and tasks. We conduct experiments on both English and Chinese datasets. For English datasets, We use MNLI (Wang et al., 2019) for text classification task, SQuAD1.1 (Rajpurkar et al., 2016) for span-extraction machine reading comprehension (MRC) task and CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003) for named entity recognition (NER) task. For Chinese datasets, we use the Chinese part of XNLI 4 More results are presented in the online documentation: https://textbrewer.readthedocs.io 12 Dataset Task Metrics #Train #Dev MNLI SQuAD CoNLL-2003 Classification MRC NER Acc EM/F1 F1 393K 88K 23K 20K 11K 6K XNLI LCQMC CMRC 2018 DRCD Classification Classification MRC MRC Acc Acc EM/F1 EM/F1 393K 293K 10K 27K 2.5K 8.8K 3.4K 3.5K Model Table 1: A summary of the datasets used in experimen"
2020.acl-demos.2,D19-1600,1,0.830779,"d entity recognition (NER) task. For Chinese datasets, we use the Chinese part of XNLI 4 More results are presented in the online documentation: https://textbrewer.readthedocs.io 12 Dataset Task Metrics #Train #Dev MNLI SQuAD CoNLL-2003 Classification MRC NER Acc EM/F1 F1 393K 88K 23K 20K 11K 6K XNLI LCQMC CMRC 2018 DRCD Classification Classification MRC MRC Acc Acc EM/F1 EM/F1 393K 293K 10K 27K 2.5K 8.8K 3.4K 3.5K Model Table 1: A summary of the datasets used in experiments. The size of CoNLL-2003 is measured in number of entities. (Conneau et al., 2018), LCQMC (Liu et al., 2018), CMRC 2018 (Cui et al., 2019b) and DRCD (Shao et al., 2018). XNLI is the multilingual version of MNLI. LCQMC is a large-scale Chinese question matching corpus. CMRC 2018 and DRCD are two span-extraction machine reading comprehension datasets similar to SQuAD. The statistics of the datasets are listed in Table 1. Models. All the teachers are BERTBASE -based models. For English tasks, teachers are initialized with the weights released by Google5 and converted into PyTorch format via Transformers6 . For Chinese tasks, teacher is initialized with the pre-trained RoBERTa-wwm-ext 7 (Cui et al., 2019a). We test the performance"
2020.acl-demos.2,N19-1423,0,0.0382516,"ain a teacher model, then optimize the student model by minimizing some losses that are calculated between the outputs of the teacher and the student. Therefore it is desirable to have a reusable distillation workflow framework and treat different distillation strategies and tricks as plugins so that they could be easily and arbitrarily added to the framework. In this way, we could also achieve great flexibility in experimenting with different combinations of distillation strategies and comparing their effects. Introduction Large pre-trained language models, such as GPT (Radford, 2018), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b) and XLNet (Yang et al., 2019) have achieved great success in many NLP tasks and greatly contributed to the progress of NLP research. However, one big issue of these models is the high demand for computing resources — they usually have hundreds of millions of parameters, and take several gigabytes of memory to train and inference — which makes it impractical to deploy them on mobile devices or online systems. From a research point of view, we are tempted to ask: is it necessary to have such a big model that contains hundreds of millions of parameters to achieve a h"
2020.acl-demos.2,D19-1441,0,0.0537409,"Missing"
2020.acl-demos.2,D16-1139,0,0.0315073,"tillation methods and can be extended with custom code. As a case study, we use TextBrewer to distill BERT on several typical NLP tasks. With simple configurations, we achieve results that are comparable with or even higher than the public distilled BERT models with similar numbers of parameters. 1 1 KD is a technique of transferring knowledge from a teacher model to a student model, which is usually smaller than the teacher. The student model is trained to mimic the outputs of the teacher model. Before the birth of BERT, KD had been applied to several specific tasks like machine translation (Kim and Rush, 2016; Tan et al., 2019) in NLP. While the recent studies of distilling large pre-trained models focus on finding general distillation methods that work on various tasks and are receiving more and more attention (Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019a; Tang et al., 2019; Liu et al., 2019a; Clark et al., 2019; Zhao et al., 2019). Though various distillation methods have been proposed, they usually share a common workflow: firstly, train a teacher model, then optimize the student model by minimizing some losses that are calculated between the outputs of the teacher and the student. T"
2020.acl-demos.2,D19-6122,0,0.0225655,"of parameters. 1 1 KD is a technique of transferring knowledge from a teacher model to a student model, which is usually smaller than the teacher. The student model is trained to mimic the outputs of the teacher model. Before the birth of BERT, KD had been applied to several specific tasks like machine translation (Kim and Rush, 2016; Tan et al., 2019) in NLP. While the recent studies of distilling large pre-trained models focus on finding general distillation methods that work on various tasks and are receiving more and more attention (Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019a; Tang et al., 2019; Liu et al., 2019a; Clark et al., 2019; Zhao et al., 2019). Though various distillation methods have been proposed, they usually share a common workflow: firstly, train a teacher model, then optimize the student model by minimizing some losses that are calculated between the outputs of the teacher and the student. Therefore it is desirable to have a reusable distillation workflow framework and treat different distillation strategies and tricks as plugins so that they could be easily and arbitrarily added to the framework. In this way, we could also achieve great flexibility in experimenting w"
2020.acl-demos.2,W17-2623,0,0.0218015,"he teacher and various students on Chinese tasks. Table 4: Results of multi-teacher distillation. All the models are BERTBASE . Different teachers are trained with different random seeds. For each task, the ensemble is the average of three teachers’ results. CMRC 2018 has a relatively small training set, DA has a much more significant effect. cases. It may be a hint that narrow-and-deep models are better than wide-and-shallow models. Finally, data augmentation (DA) is critical. For the experiments in the last line in Table 2, we use additional datasets during distillation: a subset of NewsQA (Trischler et al., 2017) training set is used in SQuAD; passages from the HotpotQA (Yang et al., 2018) training set is used in CoNLL-2003. The augmentation datasets significantly improve the performance, especially when the size of the training set is small, like CoNLL-2003. We next show the effectiveness of MultiTeacherDistiller, which distills an ensemble of teachers to a single student model. For each task, we train three BERTBASE teacher models with different seeds. The student is also a BERTBASE model. The temperature is set to 8, and intermediate losses are not used. As Table 4 shows, for each task, the student"
2020.acl-demos.2,D18-1259,0,0.0256078,"distillation. All the models are BERTBASE . Different teachers are trained with different random seeds. For each task, the ensemble is the average of three teachers’ results. CMRC 2018 has a relatively small training set, DA has a much more significant effect. cases. It may be a hint that narrow-and-deep models are better than wide-and-shallow models. Finally, data augmentation (DA) is critical. For the experiments in the last line in Table 2, we use additional datasets during distillation: a subset of NewsQA (Trischler et al., 2017) training set is used in SQuAD; passages from the HotpotQA (Yang et al., 2018) training set is used in CoNLL-2003. The augmentation datasets significantly improve the performance, especially when the size of the training set is small, like CoNLL-2003. We next show the effectiveness of MultiTeacherDistiller, which distills an ensemble of teachers to a single student model. For each task, we train three BERTBASE teacher models with different seeds. The student is also a BERTBASE model. The temperature is set to 8, and intermediate losses are not used. As Table 4 shows, for each task, the student achieves the best performance, even higher than the ensemble result. 5 XNLI A"
2020.acl-demos.2,W03-0419,0,\N,Missing
2020.acl-main.10,N18-1014,0,0.0126173,"act type control the style of sentence. To improve generalization capability of DA, delexicalization technique (Wen et al., 2015a,b; Duˇsek and Jurˇc´ıcˇ ek, 2016; Tran and Nguyen, 2017a) is widely used to replace all values in reference sentence by their corresponding slot in DA, creating pairs of delexicalized input DAs and output templates. Hence the most important step in NLG is to generate templates correctly given an input DA. However, this step can introduce missing and misplaced slots, because of modeling errors or unaligned training data (Balakrishnan et al., 2019; Nie et al., 2019; Juraska et al., 2018). Lexicalization is followed after a template is generated, replacing slots in template with corresponding values in DA. 2.2 (1) ρ(x, yi ) = #({s |s = t; t ∈ yi ; s ∈ x}), (3) where function # computes the size of a set. During evaluation, system f KNN first ranks the templates in set Y by distant function ρ and then selects the top k (beam size) templates. 3 Architeture Figure 1 shows the architecture of Iterative Rectification Network. It consists of two components: a pointer rewriter to produce templates with improved performance metrics and an experience replay buffer to gather and sample"
2020.acl-main.10,J14-4003,0,0.0247447,"ency. The third iteration achieves slot consistency, after which a natural language, though slightly different from the reference text, is generated via lexicalization. 7 Related Work Conventional approaches for solving NLG task are mostly pipeline-based, dividing it into sentence planning and surface realisation (Dethlefs et al., 2013; Stent et al., 2004; Walker et al., 2002). Oh and Rudnicky (2000) introduce a class-based ngram language model and a rule-based reranker. Ratnaparkhi (2002) address the limitations of ngram language models by using more sophisticated syntactic dependency trees. Mairesse and Young (2014) employ a phrase-based generator that learn from a semantically aligned corpus. Despite their robustness, these models are costly to create and maintain as they heavily rely on handcrafted rules. Recent works (Wen et al., 2015b; Duˇsek and Jurˇc´ıcˇ ek, 2016; Tran and Nguyen, 2017a) build data-driven models based on end-to-end learning. Wen et al. (2015a) combine two recurrent neural network (RNN) based models with a CNN reranker to generate required utterances. Wen et al. (2015b) introduce a novel SC-LSTM with an additional reading cell to jointly learn gating mechanism and language model. Du"
2020.acl-main.10,P19-1256,0,0.030871,"Missing"
2020.acl-main.10,W00-0306,0,0.299465,"slot $AUDIO$ but inserts slot $PRICE$. The output template from the first iteration of IRN has a removal of the inserted $PRICE$ slot. The second iteration has improved language fluency but no progress in slot-inconsistency. The third iteration achieves slot consistency, after which a natural language, though slightly different from the reference text, is generated via lexicalization. 7 Related Work Conventional approaches for solving NLG task are mostly pipeline-based, dividing it into sentence planning and surface realisation (Dethlefs et al., 2013; Stent et al., 2004; Walker et al., 2002). Oh and Rudnicky (2000) introduce a class-based ngram language model and a rule-based reranker. Ratnaparkhi (2002) address the limitations of ngram language models by using more sophisticated syntactic dependency trees. Mairesse and Young (2014) employ a phrase-based generator that learn from a semantically aligned corpus. Despite their robustness, these models are costly to create and maintain as they heavily rely on handcrafted rules. Recent works (Wen et al., 2015b; Duˇsek and Jurˇc´ıcˇ ek, 2016; Tran and Nguyen, 2017a) build data-driven models based on end-to-end learning. Wen et al. (2015a) combine two recurren"
2020.acl-main.10,P19-1080,0,0.0469294,"eness. 1 Reference Missing Misplace Table 1: An exmaple (including mistaken generations) extracted from SF Hotel (Wen et al., 2015b) dataset. Errors are marked in colors (missing, misplaced). Tran and Nguyen, 2017a) have attracted much attention. They implicitly learn sentence planning and surface realisation end-to-end with cross entropy objectives. For example, Duˇsek and Jurˇc´ıcˇ ek (2016) employ an attentive encoder-decoder model, which applies attention mechanism over input slot value pairs. Although neural generators can be trained end-to-end, they suffer from hallucination phenomenon (Balakrishnan et al., 2019). Examples in Table 1 show a misplacement error of an unseen slot AREA and a missing error of slot NAME by an end-to-end trained model, when compared against its input DA. Motivated by this observation, in this paper, we define slot consistency of NLG systems as all slot values of input DAs shall appear in output sentences without misplacement. We also observe that, for task-oriented dialogue systems, input DAs are mostly with simple logic forms, therefore enabling retrieval-based methods e.g. K-Nearest Neighbour (KNN) to handle the majority of test cases. Furthermore, there exists a discrepan"
2020.acl-main.10,P13-1123,0,0.0577967,"Missing"
2020.acl-main.10,P16-2008,0,0.0325552,"Missing"
2020.acl-main.10,P04-1011,0,0.435422,"r (KNN) to handle the majority of test cases. Furthermore, there exists a discrepancy between the training criterion of cross entropy loss and evaluation metric of slot error rate (ERR), similarly to that observed in neural machine translation (Ranzato et al., 2015). Therefore, it is beneficial to use training methods that integrate the evaluation metrics in their objectives. Introduction Natural Language Generation (NLG), as a critical component of task-oriented dialogue systems, converts a meaning representation, i.e., dialogue act (DA), into natural language sentences. Traditional methods (Stent et al., 2004; Konstas and Lapata, 2013; Wong and Mooney, 2007) are mostly pipeline-based, dividing the generation process into sentence planing and surface realization. Despite their robustness, they heavily rely on handcrafted rules and domain-specific knowledge. In addition, the generated sentences of rule-based approaches are rather rigid, without the variance of human language. More recently, neural network based models (Wen et al., 2015a,b; Duˇsek and Jurˇc´ıcˇ ek, 2016; ∗ † inform(NAME = pickwick hotel, PRICERANGE = moderate) the hotel named pickwick hotel is in a moderate price range this is a mode"
2020.acl-main.10,K17-1044,0,0.35106,"pplies a bootstrapping algorithm to sample training candidates and uses reinforcement learning to incorporate discrete reward related to slot inconsistency into training. Comprehensive studies have been conducted on multiple benchmark datasets, showing that the proposed methods have significantly reduced the slot error rate (ERR) for all strong baselines. Human evaluations also have confirmed its effectiveness. 1 Reference Missing Misplace Table 1: An exmaple (including mistaken generations) extracted from SF Hotel (Wen et al., 2015b) dataset. Errors are marked in colors (missing, misplaced). Tran and Nguyen, 2017a) have attracted much attention. They implicitly learn sentence planning and surface realisation end-to-end with cross entropy objectives. For example, Duˇsek and Jurˇc´ıcˇ ek (2016) employ an attentive encoder-decoder model, which applies attention mechanism over input slot value pairs. Although neural generators can be trained end-to-end, they suffer from hallucination phenomenon (Balakrishnan et al., 2019). Examples in Table 1 show a misplacement error of an unseen slot AREA and a missing error of slot NAME by an end-to-end trained model, when compared against its input DA. Motivated by th"
2020.acl-main.10,W17-5528,0,0.186184,"pplies a bootstrapping algorithm to sample training candidates and uses reinforcement learning to incorporate discrete reward related to slot inconsistency into training. Comprehensive studies have been conducted on multiple benchmark datasets, showing that the proposed methods have significantly reduced the slot error rate (ERR) for all strong baselines. Human evaluations also have confirmed its effectiveness. 1 Reference Missing Misplace Table 1: An exmaple (including mistaken generations) extracted from SF Hotel (Wen et al., 2015b) dataset. Errors are marked in colors (missing, misplaced). Tran and Nguyen, 2017a) have attracted much attention. They implicitly learn sentence planning and surface realisation end-to-end with cross entropy objectives. For example, Duˇsek and Jurˇc´ıcˇ ek (2016) employ an attentive encoder-decoder model, which applies attention mechanism over input slot value pairs. Although neural generators can be trained end-to-end, they suffer from hallucination phenomenon (Balakrishnan et al., 2019). Examples in Table 1 show a misplacement error of an unseen slot AREA and a missing error of slot NAME by an end-to-end trained model, when compared against its input DA. Motivated by th"
2020.acl-main.10,W15-4639,0,0.039746,"Missing"
2020.acl-main.10,N16-1015,0,0.0929326,"ved after the last token of the utterance is generated. 5.2 N X ∇ log π(aj |hj ), j=1 (14) where θ denotes model parameters. r(a) − b is the advantage function per REINFORCE. b is a baseline. Through experiments, we find that b = BLEU(y, z) performs better (Weaver and Tao, 2001) than tricks such as simple averaging of the 1 PN likelihood N j=1 log π(aj |hj ). 6 6.1 Experiments Experiment Setup We assess the model performances on four NLG datasets of different domains. The SF Hotel and SF Restaurant benchmarks are collected in (Wen et al., 2015a) while Laptop and TV benchmarks are released by (Wen et al., 2016). Each dataset is evaluated with five strong baseline methods, including HLSTM (Wen et al., 2015a), SC-LSTM (Wen et al., 2015b), TGen (Duˇsek and Jurˇc´ıcˇ ek, 2016), ARoA (Tran and Nguyen, 2017b) and RALSTM (Tran and Nguyen, 2017a). Following these prior works, the evaluation metrics consist of BLEU and slot error rate (ERR), which is computed as (12) j=1 r(a) = γ SC rSC + γ LM rLM + γ DS rDS ∇J RL (θ) = (r(a) − b) ∗ Policy Gradient We utilize supervised learning in Eq. (9) to initialize our model with the labels extracted from distant supervision. After its convergence, we continuously tune"
2020.acl-main.10,D15-1199,0,0.0996522,"proving general NLG systems to produce both correct and fluent responses. It applies a bootstrapping algorithm to sample training candidates and uses reinforcement learning to incorporate discrete reward related to slot inconsistency into training. Comprehensive studies have been conducted on multiple benchmark datasets, showing that the proposed methods have significantly reduced the slot error rate (ERR) for all strong baselines. Human evaluations also have confirmed its effectiveness. 1 Reference Missing Misplace Table 1: An exmaple (including mistaken generations) extracted from SF Hotel (Wen et al., 2015b) dataset. Errors are marked in colors (missing, misplaced). Tran and Nguyen, 2017a) have attracted much attention. They implicitly learn sentence planning and surface realisation end-to-end with cross entropy objectives. For example, Duˇsek and Jurˇc´ıcˇ ek (2016) employ an attentive encoder-decoder model, which applies attention mechanism over input slot value pairs. Although neural generators can be trained end-to-end, they suffer from hallucination phenomenon (Balakrishnan et al., 2019). Examples in Table 1 show a misplacement error of an unseen slot AREA and a missing error of slot NAME"
2020.acl-main.10,N07-1022,0,0.0373466,"Furthermore, there exists a discrepancy between the training criterion of cross entropy loss and evaluation metric of slot error rate (ERR), similarly to that observed in neural machine translation (Ranzato et al., 2015). Therefore, it is beneficial to use training methods that integrate the evaluation metrics in their objectives. Introduction Natural Language Generation (NLG), as a critical component of task-oriented dialogue systems, converts a meaning representation, i.e., dialogue act (DA), into natural language sentences. Traditional methods (Stent et al., 2004; Konstas and Lapata, 2013; Wong and Mooney, 2007) are mostly pipeline-based, dividing the generation process into sentence planing and surface realization. Despite their robustness, they heavily rely on handcrafted rules and domain-specific knowledge. In addition, the generated sentences of rule-based approaches are rather rigid, without the variance of human language. More recently, neural network based models (Wen et al., 2015a,b; Duˇsek and Jurˇc´ıcˇ ek, 2016; ∗ † inform(NAME = pickwick hotel, PRICERANGE = moderate) the hotel named pickwick hotel is in a moderate price range this is a moderate hotel [NAME] the pickwick hotel in fort mason"
2020.acl-main.166,P19-1535,1,0.762994,"2016b; Zhang et al., 2018b), previous works decouple policy learning from response generation, and then use utterance-level latent variables (Zhao et al., 2019) or keywords (Yao et al., 2018) as RL actions to guide response generation. In this work, we investigate how to use prior dialog-transition information to facilitate dialog policy learning. Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Dinan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Liu et al., 2019; Bao et al., 2019; Xu et al., 2020). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn policy learning, instead of dialog informativeness improvement. Specifically, we are motivated by (Xu et al., 2020). The method in (Xu et al., 2020) has the issue of cross-domain transfer since it relies on labor-intensive knowledge graph grounded multiturn dialog datasets for model training. Compared with them, our conversational graph is automatically built from dialog datasets, which introduces very low cost for train"
2020.acl-main.166,D18-1256,0,0.0169801,", 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et al., 2018; Yao et al., 2018; Zhao et al., 2019). However, these utterance-level methods tend to produce less coherent multi-turn dialogs since it is quite challenging to learn semantic transitions in a dialog flow merely from dialog data without the help of prior information. In this paper, we propose to represent prior information about dialog transition (between a message and its response) as a graph, and optimize dialog policy based on the graph, to foster a more coherent dialog. To this end, we propose a novel conversational graph (CG) grounded policy learning frame1835 Proceedings of the 58th Annu"
2020.acl-main.166,D17-1259,0,0.0192369,"onse “It’s so ...” with “sleepy” and r¯ as input. Notice all the howvertices are from the same set rather than completely independent of each other. How to effectively learn dialog strategies is an enduring challenge for open-domain multi-turn conversation generation. To address this challenge, previous works investigate word-level policy models that simultaneously learn dialog policy and language generation from dialog corpora (Li et al., 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et al., 2018; Yao et al., 2018; Zhao et al., 2019). However, these utterance-level methods tend to produce less coherent multi-turn dialogs since it is quite challenging to l"
2020.acl-main.166,N16-1014,0,0.66715,"t response with two sub-steps: firstly, obtains a response representation r¯ using both M3 and a message representation (from a message-encoder); Next, produces a response “It’s so ...” with “sleepy” and r¯ as input. Notice all the howvertices are from the same set rather than completely independent of each other. How to effectively learn dialog strategies is an enduring challenge for open-domain multi-turn conversation generation. To address this challenge, previous works investigate word-level policy models that simultaneously learn dialog policy and language generation from dialog corpora (Li et al., 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et"
2020.acl-main.166,D16-1127,0,0.489962,"t response with two sub-steps: firstly, obtains a response representation r¯ using both M3 and a message representation (from a message-encoder); Next, produces a response “It’s so ...” with “sleepy” and r¯ as input. Notice all the howvertices are from the same set rather than completely independent of each other. How to effectively learn dialog strategies is an enduring challenge for open-domain multi-turn conversation generation. To address this challenge, previous works investigate word-level policy models that simultaneously learn dialog policy and language generation from dialog corpora (Li et al., 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et"
2020.acl-main.166,D16-1230,0,0.114845,"Missing"
2020.acl-main.166,D19-1187,1,0.845143,"models (Li et al., 2016b; Zhang et al., 2018b), previous works decouple policy learning from response generation, and then use utterance-level latent variables (Zhao et al., 2019) or keywords (Yao et al., 2018) as RL actions to guide response generation. In this work, we investigate how to use prior dialog-transition information to facilitate dialog policy learning. Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Dinan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Liu et al., 2019; Bao et al., 2019; Xu et al., 2020). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn policy learning, instead of dialog informativeness improvement. Specifically, we are motivated by (Xu et al., 2020). The method in (Xu et al., 2020) has the issue of cross-domain transfer since it relies on labor-intensive knowledge graph grounded multiturn dialog datasets for model training. Compared with them, our conversational graph is automatically built from dialog datasets, which introduces very"
2020.acl-main.166,W15-4640,0,0.0973246,"Missing"
2020.acl-main.166,N19-1123,0,0.0334427,"Missing"
2020.acl-main.166,D18-1255,0,0.0342656,"egeneration issue of word-level policy models (Li et al., 2016b; Zhang et al., 2018b), previous works decouple policy learning from response generation, and then use utterance-level latent variables (Zhao et al., 2019) or keywords (Yao et al., 2018) as RL actions to guide response generation. In this work, we investigate how to use prior dialog-transition information to facilitate dialog policy learning. Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Dinan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Liu et al., 2019; Bao et al., 2019; Xu et al., 2020). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn policy learning, instead of dialog informativeness improvement. Specifically, we are motivated by (Xu et al., 2020). The method in (Xu et al., 2020) has the issue of cross-domain transfer since it relies on labor-intensive knowledge graph grounded multiturn dialog datasets for model training. Compared with them, our conversational graph is automatically built from di"
2020.acl-main.166,C16-1316,0,0.253008,"rocedure, they randomly select a mechanism for response generation. As shown in Figure 3, the generator consists of a RNN based message encoder, a set of responding mechanisms, and a decoder. First, given a dialog message, the message-encoder represents it as a vector x. Second, the generator uses a responding mechanism (selected by policy) to convert x into a response representation r¯. Finally, r¯ and a keyword (selected by policy) are fed into the decoder for response generation. To ensure that the given keyword will appear in generated responses, we introduce another Seq2BF based decoder (Mou et al., 2016) to replace the original RNN decoder. Moreover, this generator is trained on a dataset with pairs of [the message, a keyword extracted from a response]-the response.3 3.2 CG Construction Given a dialog corpus D, we construct the CG with three steps: what-vertex construction, how-vertex construction, and edge construction. 3 If multiple keywords are extracted from the response, we randomly choose one; and if no keyword exists in the response, we randomly sample a word from the response to serve as “keyword”. 1837 What-vertex construction To extract content words from D as what-vertices, we use"
2020.acl-main.166,D14-1162,0,0.0831011,"Missing"
2020.acl-main.166,P15-1152,0,0.0691534,"t by grid search. The weights of the third/sixth factors are set as 0 by default because they are proposed for target-guided conversation. 1839 rameters, and the parameters of other modules stay intact during RL training. 3.8 NLG As described in Section 3.1, we use the mechanism selected by how-policy to convert x into a response representation r¯. Then we feed the keyword in the selected what-vertex and r¯ into a Seq2BF decoder (Mou et al., 2016) for response generation. Experiments and Results9 4 4.1 Datasets We conduct experiments on two widely used opendomain dialog corpora. Weibo corpus (Shang et al., 2015). This is a large micro-blogging corpora. After data cleaning, we obtain 2.6 million pairs for training, 10k pairs for validation and 10k pairs for testing. We use publicly-available lexical analysis tools10 to obtain POS tag features for this dataset and then we further use this feature to extract keywords from utterances. We use Tencent AI Lab Embedding11 for embedding initialization in models. Persona dialog corpus (Zhang et al., 2018a). This ia a crowd-sourced dialog corpora where each participant plays the part of an assigned persona. To evaluate policy controllability brought by CGPolicy"
2020.acl-main.166,P18-1205,0,0.27686,"o sub-steps: firstly, obtains a response representation r¯ using both M3 and a message representation (from a message-encoder); Next, produces a response “It’s so ...” with “sleepy” and r¯ as input. Notice all the howvertices are from the same set rather than completely independent of each other. How to effectively learn dialog strategies is an enduring challenge for open-domain multi-turn conversation generation. To address this challenge, previous works investigate word-level policy models that simultaneously learn dialog policy and language generation from dialog corpora (Li et al., 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et al., 2018; Yao et al."
2020.acl-main.565,D18-1547,0,0.188613,"Missing"
2020.acl-main.565,W17-5506,0,0.364581,"ringer street no traffic 200 Alester Ave 2 miles gas station Valero road block nearby 899 Ames Ct 5 miles hospital Stanford Childrens Health moderate traffic 481 Amaranta Ave 1 miles parking garage Palo Alto Garage R moderate traffic Driver Address to the gas station. Car Valero is located at 200 Alester Ave. Driver OK , please give me directions via a route that avoids all heavy traffic. Car Since there is a road block nearby, I found another route for you and I sent it on your screen. Figure 1: Example of a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver"
2020.acl-main.565,E17-2075,0,0.249266,"ere is a road block nearby, I found another route for you and I sent it on your screen. Figure 1: Example of a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver’s query about the “gas station”, the end-to-end dialogue system directly generates system response given the query and a corresponding knowledge base (KB). Email corresponding. Distance POI type 5672 barringer street Dialogue Introduction ∗ Address Though achieving promising performance, endto-end models rely on a considerable amount of labeled data, which limits their usefulness for new and extended dom"
2020.acl-main.565,N19-1375,0,0.421861,"igure 1: Example of a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver’s query about the “gas station”, the end-to-end dialogue system directly generates system response given the query and a corresponding knowledge base (KB). Email corresponding. Distance POI type 5672 barringer street Dialogue Introduction ∗ Address Though achieving promising performance, endto-end models rely on a considerable amount of labeled data, which limits their usefulness for new and extended domains. In practice, we cannot collect rich datasets for each new domain. Hence, it is"
2020.acl-main.565,D18-1498,0,0.0614214,"Missing"
2020.acl-main.565,P82-1020,0,0.80062,"Missing"
2020.acl-main.565,P18-1133,0,0.108761,"et al. (2019b), We hired human experts and asked them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5. Results are illustrated in Table 4. We can see that our framework outperforms GLMP on all metrics, which is consistent with the automatic evaluation. 4 Related Work Existing end-to-end task-oriented systems can be classified into two main classes. A series of work trains a single model on the mixed multi-domain dataset. Eric et al. (2017) augments the vocabulary distribution by concatenating KB attention to generatge entities. Lei et al. (2018) first integrates track dialogue believes in end-to-end task-oriented dialog. Madotto et al. (2018) combines end-toend memory network (Sukhbaatar et al., 2015) into sequence generation. Gangi Reddy et al. (2019) proposes a multi-level memory architecture which first addresses queries, followed by results and finally each key-value pair within a result. Wu et al. (2019a) proposes a global-to-locally pointer mechanism to query the knowledge base. Compared with their models, our framework can not only explicitly utilize domain-specific knowledge but also consider different relevance between each"
2020.acl-main.565,P17-1001,0,0.0269491,"red and final domain-specific feature: d f shprivate : (hsdec,t , hdec,t ) → hfdec,t . (17) Finally, we denote the dynamic fusion function |D| i as dynamic(hsdec,t , {hddec,t }i=1 ). Similar to Sec0 tion 2.2, we replace [hdec,t , hdec,t ] in Eq. 8 with 0 [hfdec,t , hfdec,t ]. The other components are kept the same as the shared-private encoder-decoder framework. Train 2,425 1,839 Dev 302 117 Test 304 141 Table 1: Statistics of datasets. Adversarial Training To encourage the model to learn domain-shared features, we apply adversarial learning on the shared encoder and decoder module. Following Liu et al. (2017), a gradient reversal layer (Ganin and Lempitsky, 2014) is introduced after the domain classifier layer. The adversarial training loss is denoted as Ladv . We follow Qin et al. (2019a) and the final loss function of our Dynamic fusion network is defined as: L = γb Lbasic + γm Lmoe + γa Ladv , (18) where Lbasic keep the same as GLMP (Wu et al., 2019a), γb , γm and γa are hyper-parameters. More details about Lbasic and Ladv can be found in appendix. 3 where θs represents the parameters of encoderm represents the parameters decoder model, θdec of the MoE module (Eq. 15) in the decoder and ei ∈ {0"
2020.acl-main.565,P18-1136,0,0.411253,"route for you and I sent it on your screen. Figure 1: Example of a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver’s query about the “gas station”, the end-to-end dialogue system directly generates system response given the query and a corresponding knowledge base (KB). Email corresponding. Distance POI type 5672 barringer street Dialogue Introduction ∗ Address Though achieving promising performance, endto-end models rely on a considerable amount of labeled data, which limits their usefulness for new and extended domains. In practice, we cannot collect rich"
2020.acl-main.565,D19-1214,1,0.822917,"a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver’s query about the “gas station”, the end-to-end dialogue system directly generates system response given the query and a corresponding knowledge base (KB). Email corresponding. Distance POI type 5672 barringer street Dialogue Introduction ∗ Address Though achieving promising performance, endto-end models rely on a considerable amount of labeled data, which limits their usefulness for new and extended domains. In practice, we cannot collect rich datasets for each new domain. Hence, it is important to consi"
2020.acl-main.565,D19-1013,1,0.882709,"Missing"
2020.acl-main.565,C18-1320,1,0.527232,"ent it on your screen. Figure 1: Example of a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver’s query about the “gas station”, the end-to-end dialogue system directly generates system response given the query and a corresponding knowledge base (KB). Email corresponding. Distance POI type 5672 barringer street Dialogue Introduction ∗ Address Though achieving promising performance, endto-end models rely on a considerable amount of labeled data, which limits their usefulness for new and extended domains. In practice, we cannot collect rich datasets for each"
2020.acl-main.565,N18-1138,0,0.0548659,"Missing"
2020.acl-main.565,P18-1135,0,0.402986,"ynamic Domain-Specific Fusion Shared Module A Module B Module C Module Shared Module Mixed Data Domain A Domain B Domain C Domain A Mixed Data Domain B A Module B Module C Module Domain C Mixed Data (a) (c) (b) (d) Figure 2: Methods for multi-domain dialogue. Previous work either trains a general model on mixed multi-domain mixed datasets (a), or on each domain separately (b). The basic shared-private framework is shown (c). Our proposed extension with dynamic fusion mechanism is shown (d). to incorporate domain-shared and domain-private features is shared-private framework (Liu et al., 2017; Zhong et al., 2018; Wu et al., 2019b). Shown in Figure 2(c), it includes a shared module to capture domain-shared feature and a private module for each domain. The method explicitly differentiates shared and private knowledge. However, this framework still has two issues: (1) given a new domain with extremely little data, the private module can fail to effectively extract the corresponding domain knowledge. (2) the framework neglects the fine-grained relevance across certain subsets of domains. (e.g. schedule domain is more relevant to the navigation than to the weather domain.) To address the above issues, we"
2020.acl-main.599,P19-1620,0,0.156639,"nswer” should be given if there is no suitable short answer. 2.2 • We achieve state-of-the-art performance on both long and short answer leaderboard of NQ at the time of submission (Jun. 25th, 2019), and our model surpasses single human performance on the development dataset at both long and short answer criteria. Preliminary Data Preprocessing Since the average length of the documents in NQ is too long to be considered as one training instance, we first split each document into a list of document fragments with overlapping windows of tokens, like in the original BERT model for the MRC tasks (Alberti et al., 2019b; Devlin et al., 2019). Then we generate an instance from a document fragment by concatenating a “[CLS]” token, tokenized question, a “[SEP]” token, tokens from the content of the doc6709 Output Layer Document Fragment Add & Norm Paragraph Feed-Forward Sentence Add & Norm Concatenate Token Graph Integration Token-Level Self-Attention Sentence-Level Self-Attention N× Paragraph-Level Self-Attention Figure 4: The graph on the left is an illustration of the graph integration layer. The graph on the right shows the incoming information when updating a paragraph node. The solid lines represent the"
2020.acl-main.599,P17-1171,0,0.270549,"al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extract the short answer from the selected long answer. Despite the effectiveness of these approaches, they treat the long and short answer extraction as two individual sub-tasks during training and fail to model this multi-grained characteristic of this benchmark, while we argue that the two sub-tasks of NQ should be considered simultaneously to obtain accurate results. According to Kwiatkowski et al. (2019), a valid long answer must contain all of the information required to answer the question. Besides, an accurate short answer should be helpful to confirm the long answer. For instance,"
2020.acl-main.599,P16-1046,0,0.238693,"agraphs 6708 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6708–6718 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (long answer) describing the entity bowling hall of fame, then try to confirm if the location (short answer) of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer. In this way, the two-grained answers can provide evidence for each other. To address the two sub-tasks together, instead of using conventional documents modeling methods like hierarchical RNNs (Cheng and Lapata, 2016; Yang et al., 2016; Nallapati et al., 2017; Narayan et al., 2018), we propose to use graph attention networks (Velickovic et al., 2018) and BERT (Devlin et al., 2019), directly model representations at tokens, sentences, paragraphs, and documents, the four different levels of granularity to capture hierarchical nature of documents. In this way, we directly derive scores of long answers from its paragraph-level representations and obtain scores of short answers from the start and end positions on the token-level representations. Thus the long and short answer selection tasks can be trained joi"
2020.acl-main.599,P17-1147,0,0.0527221,"dataset. and questions that are from real user queries. Besides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al"
2020.acl-main.599,N18-2075,0,0.0309702,"normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani e"
2020.acl-main.599,Q19-1026,0,0.165659,"ar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extract the short answer from the selected long answer. Despite the effectiveness of these approaches,"
2020.acl-main.599,N18-2078,0,0.0307711,"k et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes are represented by different"
2020.acl-main.599,P18-1078,0,0.295567,"ides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extract the short answer from the selected long answe"
2020.acl-main.599,P17-1055,1,0.862058,"ns that are from real user queries. Besides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extrac"
2020.acl-main.599,N18-1158,0,0.0177026,"ion for Computational Linguistics, pages 6708–6718 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (long answer) describing the entity bowling hall of fame, then try to confirm if the location (short answer) of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer. In this way, the two-grained answers can provide evidence for each other. To address the two sub-tasks together, instead of using conventional documents modeling methods like hierarchical RNNs (Cheng and Lapata, 2016; Yang et al., 2016; Nallapati et al., 2017; Narayan et al., 2018), we propose to use graph attention networks (Velickovic et al., 2018) and BERT (Devlin et al., 2019), directly model representations at tokens, sentences, paragraphs, and documents, the four different levels of granularity to capture hierarchical nature of documents. In this way, we directly derive scores of long answers from its paragraph-level representations and obtain scores of short answers from the start and end positions on the token-level representations. Thus the long and short answer selection tasks can be trained jointly to promote each other. At inference time, we use a pipeline s"
2020.acl-main.599,D16-1244,0,0.153285,"Missing"
2020.acl-main.599,N16-1174,0,0.432339,"of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6708–6718 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (long answer) describing the entity bowling hall of fame, then try to confirm if the location (short answer) of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer. In this way, the two-grained answers can provide evidence for each other. To address the two sub-tasks together, instead of using conventional documents modeling methods like hierarchical RNNs (Cheng and Lapata, 2016; Yang et al., 2016; Nallapati et al., 2017; Narayan et al., 2018), we propose to use graph attention networks (Velickovic et al., 2018) and BERT (Devlin et al., 2019), directly model representations at tokens, sentences, paragraphs, and documents, the four different levels of granularity to capture hierarchical nature of documents. In this way, we directly derive scores of long answers from its paragraph-level representations and obtain scores of short answers from the start and end positions on the token-level representations. Thus the long and short answer selection tasks can be trained jointly to promote eac"
2020.acl-main.599,D16-1264,0,0.0512332,"ong and short answer criteria. 1 Figure 1: An example from NQ dataset. and questions that are from real user queries. Besides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to selec"
2020.acl-main.599,K19-1074,0,0.0605159,"Missing"
2020.acl-main.599,D16-1103,0,0.0241031,"Xiong et al., 2017; Cui et al., 2017; Devlin et al., 2019; Lv et al., 2020). They normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have s"
2020.acl-main.599,N18-2074,0,0.0339776,"graph integration layer and pass it to the feedforward layer. 6711 3.3.4 Feed-Forward Layer Following the inner structure of the transformer (Vaswani et al., 2017), we also utilize an additional fully connected feed-forward network at the end of our graph encoder. It consists of two linear transformations with a GELU activation in between. GELU is Gaussian Error Linear Unit activation (Hendrycks and Gimpel, 2016), and we use GELU as the non-linear activation, which is consistent with BERT. 3.3.5 Inspired by positional encoding in Vaswani et al. (2017) and relative position representations in Shaw et al. (2018), we introduce a novel relational embedding on our constructed graph, which aims at modeling the relative position information between nodes on the multi-granularity document structure. We make the edges in our document modeling graph to embed relative positional information. We modify equation 1 and 2 for eij and z i to introduce our relational embedding as follows: eij = zi = X αij  h0i = j∈Ni ,oj +1=oi  h0j + aij + boi , Output Layer The objective function is defined as the negative sum of the log probabilities of the predicted distributions, averaged over all the training instances. The"
2020.acl-main.599,D18-1246,0,0.0199332,"evel encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes are represented by different 6715 types of edges in the graph. 6 Jaco"
2020.acl-main.599,P18-1030,0,0.0200399,"Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes"
2020.acl-main.599,D18-1244,0,0.031553,"Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes"
2020.acl-main.599,D15-1167,1,0.734635,"Cui et al., 2017; Devlin et al., 2019; Lv et al., 2020). They normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibil"
2020.acl-main.599,P18-1158,0,0.0352041,"l., 2017; Lai et al., 2017; Trischler et al., 2017; Yang et al., 2018). Lots of work has begun to build end-to-end deep learning models and has achieved good results (Seo et al., 2017; Xiong et al., 2017; Cui et al., 2017; Devlin et al., 2019; Lv et al., 2020). They normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network m"
2020.acl-main.599,W17-2623,0,\N,Missing
2020.acl-main.599,D17-1082,0,\N,Missing
2020.acl-main.599,D18-1259,0,\N,Missing
2020.acl-main.599,N19-1423,0,\N,Missing
2020.acl-main.98,N16-1014,0,0.232101,"nate the goal predicted by our model, all the related knowledge and the dialog context as its input. Turn-level results Dialog-level results Methods↓ Metrics→ Fluency Appro. Infor. Proactivity Goal success rate Coherence S2S +gl. +kg. MGCG R +gl. +kg. MGCG G +gl. +kg. 1.08 1.98 1.94 0.23 0.60 0.75 0.37 1.28 1.68 0.94 1.22 1.34 0.37 0.68 0.82 0.49 0.83 0.91 Table 5: Human evaluation results at the level of turns and dialogs. 5.3 Automatic Evaluations Metrics For automatic evaluation, we use several common metrics such as BLEU (Papineni et al., 2002), F1, perplexity (PPL), and DISTINCT (DIST2) (Li et al., 2016) to measure the relevance, fluency, and diversity of generated responses. Following the setting in previous work (Wu et al., 2019; Zhang et al., 2018a), we also measure the performance of all models using Hits@1 and Hits@3.6 Here we let each model to select the best response from 10 candidates. Those 10 candidate responses consist of the ground-truth response generated by humans and nine randomly sampled ones from the training set. Moreover, we also evaluate the knowledge-selection capability of each model by calculating knowledge precision/recall/F1 scores as done in Wu et al. (2019).7 In add"
2020.acl-main.98,D18-1255,0,0.356097,"The goal-planning module can conduct dialog management to control the dialog 1037 • We identify the task of conversational recommendation over multi-type dialogs. • To facilitate the study of this task, we create a novel dialog dataset DuRecDial, with rich variability of dialog types and domains as shown in Table 1. • We propose a conversation generation framework with a novel mixed-goal driven dialog policy mechanism. Datasets↓ Metrics→ Facebook Rec (Dodge et al., 2016) REDIAL (Li et al., 2018) GoRecDial (Kang et al., 2019) OpenDialKG (Moon et al., 2019) CMU DoG (Zhou et al., 2018a) IIT DoG (Moghe et al., 2018) Wizard-of-wiki (Dinan et al., 2019) OpenDialKG (Moon et al., 2019) DuConv (Wu et al., 2019) KdConv (Zhou et al., 2020) DuRecDial #Dialogs #Utterances Dialog types Domains 1M 10k 9k 12k 4k 9k 22k 3k 29k 4.5k 10.2k 6M 163k 170k 143k 130k 90k 202k 38k 270k 86k 156k Rec. Rec., chitchat Rec. Rec. Chitchat Chitchat Chitchat Chitchat Chitchat Chitchat Rec., chitchat, QA, task Movie Movie Movie Movie, book Movie Movie 1365 Wikipedia topics Sports, music Movie Movie, music, travel Movie, music, movie star, food, restaurant, news, weather User profile No No Yes No No No No No No No Yes Table 1: Compari"
2020.acl-main.98,P19-1081,0,0.263116,"oal planning module and a goal-guided responding module. The goal-planning module can conduct dialog management to control the dialog 1037 • We identify the task of conversational recommendation over multi-type dialogs. • To facilitate the study of this task, we create a novel dialog dataset DuRecDial, with rich variability of dialog types and domains as shown in Table 1. • We propose a conversation generation framework with a novel mixed-goal driven dialog policy mechanism. Datasets↓ Metrics→ Facebook Rec (Dodge et al., 2016) REDIAL (Li et al., 2018) GoRecDial (Kang et al., 2019) OpenDialKG (Moon et al., 2019) CMU DoG (Zhou et al., 2018a) IIT DoG (Moghe et al., 2018) Wizard-of-wiki (Dinan et al., 2019) OpenDialKG (Moon et al., 2019) DuConv (Wu et al., 2019) KdConv (Zhou et al., 2020) DuRecDial #Dialogs #Utterances Dialog types Domains 1M 10k 9k 12k 4k 9k 22k 3k 29k 4.5k 10.2k 6M 163k 170k 143k 130k 90k 202k 38k 270k 86k 156k Rec. Rec., chitchat Rec. Rec. Chitchat Chitchat Chitchat Chitchat Chitchat Chitchat Rec., chitchat, QA, task Movie Movie Movie Movie, book Movie Movie 1365 Wikipedia topics Sports, music Movie Movie, music, travel Movie, music, movie star, food, restaurant, news, weather User p"
2020.acl-main.98,P02-1040,0,0.106605,"nts “with(without) knowledge”. For “S2S +gl.+kg.”, we simply concatenate the goal predicted by our model, all the related knowledge and the dialog context as its input. Turn-level results Dialog-level results Methods↓ Metrics→ Fluency Appro. Infor. Proactivity Goal success rate Coherence S2S +gl. +kg. MGCG R +gl. +kg. MGCG G +gl. +kg. 1.08 1.98 1.94 0.23 0.60 0.75 0.37 1.28 1.68 0.94 1.22 1.34 0.37 0.68 0.82 0.49 0.83 0.91 Table 5: Human evaluation results at the level of turns and dialogs. 5.3 Automatic Evaluations Metrics For automatic evaluation, we use several common metrics such as BLEU (Papineni et al., 2002), F1, perplexity (PPL), and DISTINCT (DIST2) (Li et al., 2016) to measure the relevance, fluency, and diversity of generated responses. Following the setting in previous work (Wu et al., 2019; Zhang et al., 2018a), we also measure the performance of all models using Hits@1 and Hits@3.6 Here we let each model to select the best response from 10 candidates. Those 10 candidate responses consist of the ground-truth response generated by humans and nine randomly sampled ones from the training set. Moreover, we also evaluate the knowledge-selection capability of each model by calculating knowledge p"
2020.acl-main.98,P13-2089,0,0.526547,"ender proactively leads a multi-type dialog to approach recommendation targets and then makes multiple recommendations with rich interaction behavior. This dataset allows us to systematically investigate different parts of the overall problem, e.g., how to naturally lead a dialog, how to interact with users for recommendation. Finally we establish baseline results on DuRecDial for future studies.1 1 Introduction In recent years, there has been a significant increase in the work of conversational recommendation due to the rise of voice-based bots (Christakopoulou et al., 2016; Li et al., 2018; Reschke et al., 2013; Warnestal, 2005). They focus on how to provide high-quality recommendations through dialog-based interactions with users. These work fall into two categories: (1) task-oriented dialogmodeling approaches (Christakopoulou et al., 2016; Sun and Zhang, 2018; Warnestal, 2005); (2) nontask dialog-modeling approaches with more freeform interactions (Kang et al., 2019; Li et al., 2018). ∗ This work was done at Baidu. Corresponding author: Wanxiang Che. 1 Dataset and codes are publicly available at https://github.com/PaddlePaddle/models/ tree/develop/PaddleNLP/Research/ACL2020-DuRecDial. † Almost all"
2020.acl-main.98,P19-1565,0,0.111519,"Missing"
2020.acl-main.98,D19-1203,0,0.344356,"on DuRecDial for future studies.1 1 Introduction In recent years, there has been a significant increase in the work of conversational recommendation due to the rise of voice-based bots (Christakopoulou et al., 2016; Li et al., 2018; Reschke et al., 2013; Warnestal, 2005). They focus on how to provide high-quality recommendations through dialog-based interactions with users. These work fall into two categories: (1) task-oriented dialogmodeling approaches (Christakopoulou et al., 2016; Sun and Zhang, 2018; Warnestal, 2005); (2) nontask dialog-modeling approaches with more freeform interactions (Kang et al., 2019; Li et al., 2018). ∗ This work was done at Baidu. Corresponding author: Wanxiang Che. 1 Dataset and codes are publicly available at https://github.com/PaddlePaddle/models/ tree/develop/PaddleNLP/Research/ACL2020-DuRecDial. † Almost all these work focus on a single type of dialogs, either task oriented dialogs for recommendation, or recommendation oriented open-domain conversation. Moreover, they assume that both sides in the dialog (especially the user) are aware of the conversational goal from the beginning. In many real-world applications, there are multiple dialog types in human-bot conver"
2020.acl-main.98,D14-1181,0,0.0103428,"Current goal prediction (a) Goal-planning module Figure 3: The architecture of our multi-goal driven conversation generation framework (denoted as MGCG). K. These goals will be used as answers for training of the goal-planning module, while the tuples of [context, a ground-truth goal, K, response] will be used for training of the responding module. 4.2 Goal-planning Model As shown in Figure 3(a), we divide the task of goal planning into two sub-tasks, goal completion estimation, and current goal prediction. Goal completion estimation For this subtask, we use Convolutional neural network (CNN)(Kim, 2014) to estimate the probability of goal completion by: PGC (l = 1|X, gt−1 ). (1) Current goal prediction If gt−1 is not completed (PGC &lt; 0.5), then gc = gt−1 , where gc is the goal for Y . Otherwise, we use CNN based multi-task classification to predict the current goal by maximizing the following probability: gt = arg max PGP (g ty , g tp |X, G 0 , Pi k , K), (2) gc = gt , (3) s g ty ,g tp where g ty is a candidate dialog type and g tp is a candidate dialog topic. 4.3 Retrieval-based Response Model we modify the original retrieval model to suit our task by emphasizing the use of goals. As shown"
2020.acl-main.98,D14-1007,1,0.863561,"m/PaddlePaddle/models/ tree/develop/PaddleNLP/Research/ACL2020-DuRecDial. † Almost all these work focus on a single type of dialogs, either task oriented dialogs for recommendation, or recommendation oriented open-domain conversation. Moreover, they assume that both sides in the dialog (especially the user) are aware of the conversational goal from the beginning. In many real-world applications, there are multiple dialog types in human-bot conversations (called multi-type dialogs), such as chit-chat, task oriented dialogs, recommendation dialogs, and even question answering (Ram et al., 2018; Wang et al., 2014; Zhou et al., 2018b). Therefore it is crucial to study how to proactively and naturally make conversational recommendation by the bots in the context of multi-type human-bot communication. For example, the bots could proactively make recommendations after question answering or a task dialog to improve user experience, or it could lead a dialog from chitchat to approach a given product as commercial advertisement. However, to our knowledge, there is less previous work on this problem. To address this challenge, we present a novel task, conversational recommendation over multitype dialogs, wher"
2020.acl-main.98,P19-1369,1,0.765759,"the task of conversational recommendation over multi-type dialogs. • To facilitate the study of this task, we create a novel dialog dataset DuRecDial, with rich variability of dialog types and domains as shown in Table 1. • We propose a conversation generation framework with a novel mixed-goal driven dialog policy mechanism. Datasets↓ Metrics→ Facebook Rec (Dodge et al., 2016) REDIAL (Li et al., 2018) GoRecDial (Kang et al., 2019) OpenDialKG (Moon et al., 2019) CMU DoG (Zhou et al., 2018a) IIT DoG (Moghe et al., 2018) Wizard-of-wiki (Dinan et al., 2019) OpenDialKG (Moon et al., 2019) DuConv (Wu et al., 2019) KdConv (Zhou et al., 2020) DuRecDial #Dialogs #Utterances Dialog types Domains 1M 10k 9k 12k 4k 9k 22k 3k 29k 4.5k 10.2k 6M 163k 170k 143k 130k 90k 202k 38k 270k 86k 156k Rec. Rec., chitchat Rec. Rec. Chitchat Chitchat Chitchat Chitchat Chitchat Chitchat Rec., chitchat, QA, task Movie Movie Movie Movie, book Movie Movie 1365 Wikipedia topics Sports, music Movie Movie, music, travel Movie, music, movie star, food, restaurant, news, weather User profile No No Yes No No No No No No No Yes Table 1: Comparison of our dataset DuRecDial to recommendation dialog datasets and knowledge grounded dialog"
2020.acl-main.98,D18-1076,0,\N,Missing
2020.acl-main.98,2020.acl-main.635,0,\N,Missing
2020.coling-main.589,C10-3004,1,0.625896,"rt Chinese pre-trained language models show that there is still much room for them to surpass human performance, indicating the proposed data is challenging. 2 The Proposed Dataset 2.1 Task Definition Generally, the reading comprehension task can be described as a triple 〈P, Q, A〉, where P represents Passage, Q represents Question, and the A represents Answer. Specifically, for sentence cloze-style reading comprehension task, we select several sentences in the passages and replace with special marks (for example, [BLANK]), forming an incomplete passage. The sentences are identified using LTP (Che et al., 2010), and we further split the sentence with comma and period mark, as some of the sentences are too long. The selected sentences form a candidate list, and the machine should fill in the blanks with these candidate sentences to form a complete passage. Note that, to add more difficulties, we could also add the fake candidates, which do not belong to any blanks in the passage. 2.2 Passage Selection The raw material of the proposed dataset is from children’s books, containing fairy tales and narratives, which is the proper genre for testing the sentence-level inference ability, requiring the correc"
2020.coling-main.589,C16-1167,1,0.920479,"ms based on the pre-trained models, and the results show that the stateof-the-art model still underperforms human performance by a large margin. We release the dataset and baseline system to further facilitate our community. Resources available through https://github.com/ymcui/cmrc2019 1 Introduction Machine Reading Comprehension (MRC) is a task to comprehend given articles and answer the questions based on them, which is an important ability for artificial intelligence. The recent MRC research was originated from the cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), which requires to fill in the blank with a word or named entity, and following works on these datasets have laid the foundations of this research (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017). Later on, SQuAD (Rajpurkar et al., 2016) was proposed, and the answer transformed from a single word to a span, which has become a representative span-extraction dataset and massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Bes"
2020.coling-main.589,P17-1055,1,0.852937,"to further facilitate our community. Resources available through https://github.com/ymcui/cmrc2019 1 Introduction Machine Reading Comprehension (MRC) is a task to comprehend given articles and answer the questions based on them, which is an important ability for artificial intelligence. The recent MRC research was originated from the cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), which requires to fill in the blank with a word or named entity, and following works on these datasets have laid the foundations of this research (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017). Later on, SQuAD (Rajpurkar et al., 2016) was proposed, and the answer transformed from a single word to a span, which has become a representative span-extraction dataset and massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Besides the MRC in English text, we have also seen rapid progress on Chinese MRC research. Cui et al. (2016) proposed the first Chinese cloze-style reading comprehension dataset: People Dai"
2020.coling-main.589,L18-1431,1,0.890885,"jpurkar et al., 2016) was proposed, and the answer transformed from a single word to a span, which has become a representative span-extraction dataset and massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Besides the MRC in English text, we have also seen rapid progress on Chinese MRC research. Cui et al. (2016) proposed the first Chinese cloze-style reading comprehension dataset: People Daily & Children’s Fairy Tale (PD&CFT). Later, Cui et al. (2018) proposed another dataset for CMRC 2017, which is gathered from children’s reading books, consisting of both cloze and natural questions. He et al. (2018) proposed a large-scale open-domain Chinese reading comprehension dataset (DuReader), which consists of 200k queries annotated from the user query logs on the search engine. In span-extraction MRC, Cui et al. (2019b) proposed CMRC 2018 dataset for Simplified Chinese, and Shao et al. (2018) proposed DRCD dataset for Traditional Chinese, similar to the popular dataset SQuAD (Rajpurkar et al., 2016). Zheng et al. (2019) proposed a large-scale Ch"
2020.coling-main.589,D19-1600,1,0.310512,"Missing"
2020.coling-main.589,2020.findings-emnlp.58,1,0.795757,"Missing"
2020.coling-main.589,N19-1423,0,0.0241785,"of 200k queries annotated from the user query logs on the search engine. In span-extraction MRC, Cui et al. (2019b) proposed CMRC 2018 dataset for Simplified Chinese, and Shao et al. (2018) proposed DRCD dataset for Traditional Chinese, similar to the popular dataset SQuAD (Rajpurkar et al., 2016). Zheng et al. (2019) proposed a large-scale Chinese idiom cloze dataset. Though various efforts have been made, most of these datasets stop at token-level or span-level inference, which neglect the importance of long-range reasoning of the context. Moreover, powerful pretrained models such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6717 Proceedings of the 28th International Conference on Computational Linguistics, pages 6717–6723 Barcelona, Spain (Online), December 8-13, 2020 [Passage] ”森林里有一棵大树，树上有一个鸟窝。[BLANK1]，还从来没有看到过 鸟宝宝长什么样。小松鼠说：“我爬到树上去看过，鸟宝宝光溜溜的， 身上一根羽毛也没有。” “我不相信，”小白兔说，“所有的鸟都是有羽 毛的。” “鸟宝宝没有羽毛。”小松鼠说，“你不信自己去看。” 小白兔 不会爬树，它没有办法去看。小白兔说：“我请蓝狐狸去看一看，我相 信蓝狐狸的话。” 小松鼠说：“蓝狐狸跟你一样，也不会爬树。” 蓝狐 狸说：“我有魔法树叶，我能变成一只狐狸鸟。” [BLANK2]，一下子飞 到了树顶"
2020.coling-main.589,P17-1168,0,0.0210363,"tate our community. Resources available through https://github.com/ymcui/cmrc2019 1 Introduction Machine Reading Comprehension (MRC) is a task to comprehend given articles and answer the questions based on them, which is an important ability for artificial intelligence. The recent MRC research was originated from the cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), which requires to fill in the blank with a word or named entity, and following works on these datasets have laid the foundations of this research (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017). Later on, SQuAD (Rajpurkar et al., 2016) was proposed, and the answer transformed from a single word to a span, which has become a representative span-extraction dataset and massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Besides the MRC in English text, we have also seen rapid progress on Chinese MRC research. Cui et al. (2016) proposed the first Chinese cloze-style reading comprehension dataset: People Daily & Children’s Fairy T"
2020.coling-main.589,W18-2605,0,0.0261255,"massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Besides the MRC in English text, we have also seen rapid progress on Chinese MRC research. Cui et al. (2016) proposed the first Chinese cloze-style reading comprehension dataset: People Daily & Children’s Fairy Tale (PD&CFT). Later, Cui et al. (2018) proposed another dataset for CMRC 2017, which is gathered from children’s reading books, consisting of both cloze and natural questions. He et al. (2018) proposed a large-scale open-domain Chinese reading comprehension dataset (DuReader), which consists of 200k queries annotated from the user query logs on the search engine. In span-extraction MRC, Cui et al. (2019b) proposed CMRC 2018 dataset for Simplified Chinese, and Shao et al. (2018) proposed DRCD dataset for Traditional Chinese, similar to the popular dataset SQuAD (Rajpurkar et al., 2016). Zheng et al. (2019) proposed a large-scale Chinese idiom cloze dataset. Though various efforts have been made, most of these datasets stop at token-level or span-level inference, which neglect the im"
2020.coling-main.589,P16-1086,0,0.0314597,"t and baseline system to further facilitate our community. Resources available through https://github.com/ymcui/cmrc2019 1 Introduction Machine Reading Comprehension (MRC) is a task to comprehend given articles and answer the questions based on them, which is an important ability for artificial intelligence. The recent MRC research was originated from the cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), which requires to fill in the blank with a word or named entity, and following works on these datasets have laid the foundations of this research (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017). Later on, SQuAD (Rajpurkar et al., 2016) was proposed, and the answer transformed from a single word to a span, which has become a representative span-extraction dataset and massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Besides the MRC in English text, we have also seen rapid progress on Chinese MRC research. Cui et al. (2016) proposed the first Chinese cloze-style reading comprehension d"
2020.coling-main.589,D17-1082,0,0.0203873,": The king married another queen 1: She had a pretty daughter named Snow White 2: The king was also passed away 3: who is most beautiful of all? 4: she was very happy 5: she was very angry [Answers] 4, 3, 2, 1, 0 [Answers] 1, 0, 3, 5 Figure 1: Examples of the proposed CMRC 2019 dataset. The candidate with underline means it is a fake candidate (does not belong to any blank). For clarity, we also provide an English example. 2019) have surpassed human performance on various MRC datasets, such as SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), RACE (Lai et al., 2017), etc. To further test the machine comprehension ability, In this paper, we propose a new task called Sentence Cloze-style Machine Reading Comprehension (SC-MRC). The proposed task preserves the simplicity of cloze-style reading comprehension but requires sentence-level inference when filling the blanks. Figure 1 shows an example of the proposed dataset. We conclude our contributions in three aspects. • We propose a new machine reading comprehension task called Sentence Cloze-style Machine Reading Comprehension (SC-MRC), which aims to test the ability of sentence-level inference. • We release"
2020.coling-main.589,D16-1264,0,0.0906982,"through https://github.com/ymcui/cmrc2019 1 Introduction Machine Reading Comprehension (MRC) is a task to comprehend given articles and answer the questions based on them, which is an important ability for artificial intelligence. The recent MRC research was originated from the cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), which requires to fill in the blank with a word or named entity, and following works on these datasets have laid the foundations of this research (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017). Later on, SQuAD (Rajpurkar et al., 2016) was proposed, and the answer transformed from a single word to a span, which has become a representative span-extraction dataset and massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Besides the MRC in English text, we have also seen rapid progress on Chinese MRC research. Cui et al. (2016) proposed the first Chinese cloze-style reading comprehension dataset: People Daily & Children’s Fairy Tale (PD&CFT). Later, Cui et al. (2018) pro"
2020.coling-main.589,P18-2124,0,0.0246221,"4: 小动物们只看到过鸟妈妈和鸟爸爸在鸟窝里飞进飞出 5: 小松鼠变成了一只蓝色的大鸟 [Candidates] 0: The king married another queen 1: She had a pretty daughter named Snow White 2: The king was also passed away 3: who is most beautiful of all? 4: she was very happy 5: she was very angry [Answers] 4, 3, 2, 1, 0 [Answers] 1, 0, 3, 5 Figure 1: Examples of the proposed CMRC 2019 dataset. The candidate with underline means it is a fake candidate (does not belong to any blank). For clarity, we also provide an English example. 2019) have surpassed human performance on various MRC datasets, such as SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), RACE (Lai et al., 2017), etc. To further test the machine comprehension ability, In this paper, we propose a new task called Sentence Cloze-style Machine Reading Comprehension (SC-MRC). The proposed task preserves the simplicity of cloze-style reading comprehension but requires sentence-level inference when filling the blanks. Figure 1 shows an example of the proposed dataset. We conclude our contributions in three aspects. • We propose a new machine reading comprehension task called Sentence Cloze-style Machine Reading Comprehension (SC-MRC), which aims to test th"
2020.coling-main.589,Q19-1016,0,0.0235357,"松鼠变成了一只蓝色的大鸟 [Candidates] 0: The king married another queen 1: She had a pretty daughter named Snow White 2: The king was also passed away 3: who is most beautiful of all? 4: she was very happy 5: she was very angry [Answers] 4, 3, 2, 1, 0 [Answers] 1, 0, 3, 5 Figure 1: Examples of the proposed CMRC 2019 dataset. The candidate with underline means it is a fake candidate (does not belong to any blank). For clarity, we also provide an English example. 2019) have surpassed human performance on various MRC datasets, such as SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), RACE (Lai et al., 2017), etc. To further test the machine comprehension ability, In this paper, we propose a new task called Sentence Cloze-style Machine Reading Comprehension (SC-MRC). The proposed task preserves the simplicity of cloze-style reading comprehension but requires sentence-level inference when filling the blanks. Figure 1 shows an example of the proposed dataset. We conclude our contributions in three aspects. • We propose a new machine reading comprehension task called Sentence Cloze-style Machine Reading Comprehension (SC-MRC), which aims to test the ability of sentence-level"
2020.coling-main.589,P17-1018,0,0.0194371,"recent MRC research was originated from the cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), which requires to fill in the blank with a word or named entity, and following works on these datasets have laid the foundations of this research (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017). Later on, SQuAD (Rajpurkar et al., 2016) was proposed, and the answer transformed from a single word to a span, which has become a representative span-extraction dataset and massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Besides the MRC in English text, we have also seen rapid progress on Chinese MRC research. Cui et al. (2016) proposed the first Chinese cloze-style reading comprehension dataset: People Daily & Children’s Fairy Tale (PD&CFT). Later, Cui et al. (2018) proposed another dataset for CMRC 2017, which is gathered from children’s reading books, consisting of both cloze and natural questions. He et al. (2018) proposed a large-scale open-domain Chinese reading comprehension dataset (DuR"
2020.coling-main.589,P18-1158,0,0.0154653,"from the cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), which requires to fill in the blank with a word or named entity, and following works on these datasets have laid the foundations of this research (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017). Later on, SQuAD (Rajpurkar et al., 2016) was proposed, and the answer transformed from a single word to a span, which has become a representative span-extraction dataset and massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Besides the MRC in English text, we have also seen rapid progress on Chinese MRC research. Cui et al. (2016) proposed the first Chinese cloze-style reading comprehension dataset: People Daily & Children’s Fairy Tale (PD&CFT). Later, Cui et al. (2018) proposed another dataset for CMRC 2017, which is gathered from children’s reading books, consisting of both cloze and natural questions. He et al. (2018) proposed a large-scale open-domain Chinese reading comprehension dataset (DuReader), which consists of 200k queri"
2020.coling-main.589,P19-1075,0,0.0222042,"Fairy Tale (PD&CFT). Later, Cui et al. (2018) proposed another dataset for CMRC 2017, which is gathered from children’s reading books, consisting of both cloze and natural questions. He et al. (2018) proposed a large-scale open-domain Chinese reading comprehension dataset (DuReader), which consists of 200k queries annotated from the user query logs on the search engine. In span-extraction MRC, Cui et al. (2019b) proposed CMRC 2018 dataset for Simplified Chinese, and Shao et al. (2018) proposed DRCD dataset for Traditional Chinese, similar to the popular dataset SQuAD (Rajpurkar et al., 2016). Zheng et al. (2019) proposed a large-scale Chinese idiom cloze dataset. Though various efforts have been made, most of these datasets stop at token-level or span-level inference, which neglect the importance of long-range reasoning of the context. Moreover, powerful pretrained models such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6717 Proceedings of the 28th International Conference on Computational Linguistics, pages 6717–6723 B"
2020.conll-shared.6,E17-2039,0,0.252146,"Missing"
2020.conll-shared.6,2020.conll-shared.2,0,0.367904,"We didn’t find any existing parser for PTG. Thus we design a list-based arc-eager transitionbased parser for PTG. Abstract Meaning Representation (AMR), proposed by Banarescu et al. (2013), is a broadcoverage sentence-level semantic formalism (Flavor (2)) used to encode the meaning of natural language sentences. AMR can be regarded as a rooted labeled directed acyclic graph. Nodes in AMR graphs represent concepts and labeled directed edges are relations between the concepts. We directly employ state-of-the-art parser of Cai and Lam (2020). Discourse Representation Graphs (DRG) are proposed by Abzianidze et al. (2020) (Flavor (2)), which are derived from the DRS annotations in the Parallel Meaning Bank (Bos et al., 2017; Abzianidze et al., 2017). Its concepts are represented by WordNet 3.0 (Fellbaum, 1998) senses and semantic roles by the adapted version of VerbNet (Schuler, 2006) roles. Similar to PTG, we don’t find any existing parser for DRG, thus we modify the AMR parser to process the DRG. 3 3.1 buffer holding unprocessed words. E is a set of labeled dependency arcs. V is a set of graph nodes including concept nodes and surface tokens. The initial state is ([0], [ ], [1, · · · , n], [ ], V ) , where V"
2020.conll-shared.6,W13-2322,0,0.0987643,"ment (or anchoring) information effectively. Prague Tectogrammatical Graphs (PTG) are graph-structured multi-layered semantic representation formalism (Flavor (1)) proposed by Zeman and Hajiˇc (2020). PTG graphs essentially recast core predicate–argument structure in the form of mostly anchored dependency graphs, albeit introducing ‘empty’ (or generated, in FGD terminology) nodes, for which there is no corresponding surface token. We didn’t find any existing parser for PTG. Thus we design a list-based arc-eager transitionbased parser for PTG. Abstract Meaning Representation (AMR), proposed by Banarescu et al. (2013), is a broadcoverage sentence-level semantic formalism (Flavor (2)) used to encode the meaning of natural language sentences. AMR can be regarded as a rooted labeled directed acyclic graph. Nodes in AMR graphs represent concepts and labeled directed edges are relations between the concepts. We directly employ state-of-the-art parser of Cai and Lam (2020). Discourse Representation Graphs (DRG) are proposed by Abzianidze et al. (2020) (Flavor (2)), which are derived from the DRS annotations in the Parallel Meaning Bank (Bos et al., 2017; Abzianidze et al., 2017). Its concepts are represented by"
2020.conll-shared.6,P13-1023,0,0.176134,"multiple nodes anchored to overlapping sub-strings, and (b) Flavor (2), including AMR and DRG, not considering the correspondence between nodes and the surface tokens. • Pretrained Language Model Our systems benefit a lot from the pretrained language models, i.e., BERT (Devlin et al., 2019), ELECTRA (Clark et al., 2020) and XLM-RoBERTa (Conneau et al., 2020). 2 Background In the following, we will give a brief introduction to these frameworks and our corresponding solutions. Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework (Flavor (1)) firstly proposed by Abend and Rappoport (2013), which treats input words as terminal nodes. The non-terminal node might govern one or more nodes, which may be discontinuous. Moreover, one node can have multiple governing (parent) nodes through multiple edges, consisting of a single primary edge and other remote edges. Relationships 1 See http://mrp.nlpl.eu/ for further technical details, information on how to obtain the data, and official results. 65 Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing, pages 65–72 c Online, Nov. 19-20, 2020. 2020 Association for Computational Linguistics between nodes"
2020.conll-shared.6,2020.acl-main.119,0,0.186025,"frameworks (DRG, AMR). In the final evaluation, our system is ranked 3rd among the seven team both in Cross-Framework Track and Cross-Lingual Track, with the macro-averaged MRP F1 score of 0.81/0.69. 1 • Transition-based Parser for Flavor (1) Following Che et al. (2019), the top system in CoNLL 2019 shared task (Oepen et al., 2019), we employ the transition-based parser for Flavor (1) frameworks since it’s very flexible in predicting the anchor information. We directly use their parser for UCCA and EDS. And we design a new parser for PTG. • Iterative Inference Parser for Flavor (2) Recently, Cai and Lam (2020) proposed Graph⇔Sequence Iterative Inference system for AMR parsing, which treats parsing as a series of dual decisions on the input sequence and the incrementally constructed graph, achieving state-of-the-art results. We adopt their model for Flavor (2) frameworks (AMR, DRG). Introduction The goal of the CoNLL 2020 shared task (Oepen et al., 2020) is to develop a unified parsing system to process all five semantic graph banks across different languages. This task combines five frameworks for graph-based meaning representation, each with its specific formal and linguistic assumptions, includin"
2020.conll-shared.6,2020.conll-shared.0,0,0.0819124,"Missing"
2020.conll-shared.6,K19-2007,1,0.791076,"Missing"
2020.conll-shared.6,oepen-lonning-2006-discriminant,0,0.205929,", and official results. 65 Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing, pages 65–72 c Online, Nov. 19-20, 2020. 2020 Association for Computational Linguistics between nodes are represented by edge labels. The primary edges form a tree structure, whereas the remote edges introduce reentrancy, forming directed acyclic graphs (DAGs). We directly employ the system of Che et al. (2019), which achieves the 1st at CoNLL 2019 shared task. Elementary Dependency Structure (EDS) is a graph-structured semantic representation formalism (Flavor (1)) proposed by Oepen and Lønning (2006). Che et al. (2019) introduce a neural encoder-decoder transition-based parser for the EDS graph, which extracts the node alignment (or anchoring) information effectively. Prague Tectogrammatical Graphs (PTG) are graph-structured multi-layered semantic representation formalism (Flavor (1)) proposed by Zeman and Hajiˇc (2020). PTG graphs essentially recast core predicate–argument structure in the form of mostly anchored dependency graphs, albeit introducing ‘empty’ (or generated, in FGD terminology) nodes, for which there is no corresponding surface token. We didn’t find any existing parser for"
2020.conll-shared.6,W04-2708,0,0.219119,"Missing"
2020.conll-shared.6,2020.conll-shared.3,0,0.395447,"Missing"
2020.conll-shared.6,2020.acl-main.747,0,0.0444384,"Missing"
2020.conll-shared.6,N19-1423,0,0.0948688,"ature of the relationship they assume between the linguistic surface string and the nodes of the graph. They call this relation anchoring. Therefore, the involved five frameworks could be divided into two classes: (a) Flavor (1), including UCCA, EDS, and PTG, allowing arbitrary parts of the sentence as node anchors, as well as multiple nodes anchored to overlapping sub-strings, and (b) Flavor (2), including AMR and DRG, not considering the correspondence between nodes and the surface tokens. • Pretrained Language Model Our systems benefit a lot from the pretrained language models, i.e., BERT (Devlin et al., 2019), ELECTRA (Clark et al., 2020) and XLM-RoBERTa (Conneau et al., 2020). 2 Background In the following, we will give a brief introduction to these frameworks and our corresponding solutions. Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework (Flavor (1)) firstly proposed by Abend and Rappoport (2013), which treats input words as terminal nodes. The non-terminal node might govern one or more nodes, which may be discontinuous. Moreover, one node can have multiple governing (parent) nodes through multiple edges, consisting of a single primary edge and other remot"
2020.conll-shared.6,P15-1033,0,0.0236567,"ll be determined by rule, which will be introduced in Section 5. • T ERMINALX creates new non-terminal nodes with label X. • N ODEX creates a new node on the buffer as a parent of the first element on the stack, with X as its label. Transition-based Parser for Flavor (1) Background • N ODE -ROOTX creates a new node on the buffer as a child of the root with X as its label. A tuple (S, L, B, E, V ) is used to represent the parsing state, where S is a stack holding processed words, L is a list holding words popped out of S that will be pushed back in the future, and B is a 2 We recommend reading Dyer et al. (2015) for more details. 66 Before Transition Transition Stack Buffer Nodes Edges S S|x S|x S |y, x S |x, y S |y, x S |x, y S |x, y [root] x|B B B B B B B B ∅ V V V V V V V V V E E E E E E E E E After Transition Buffer Nodes Edges |x B B y|B B B B B x|B ∅ V V V ∪ {y} V V V V V V E E E ∪ {(y, x)X } E ∪ {(x, y)X } E ∪ {(x, y)X } E ∪ {(x, y)∗X } E ∪ {(x, y)∗X } E E S S S S S S S S ∅ S HIFT R EDUCE N ODEX L EFT-E DGEX R IGHT-E DGEX L EFT-R EMOTEX R IGHT-R EMOTEX S WAP F INISH Condition Stack |x |y, x |x, y |y, x |x, y |y x 6= root   x 6∈ w1:n , y 6= root,  y 6 ;G x i(x) &lt; i(y) Table 1: The transition"
2020.conll-shared.6,P17-1104,0,0.0615865,"S S S S ∅ S HIFT R EDUCE N ODEX L EFT-E DGEX R IGHT-E DGEX L EFT-R EMOTEX R IGHT-R EMOTEX S WAP F INISH Condition Stack |x |y, x |x, y |y, x |x, y |y x 6= root   x 6∈ w1:n , y 6= root,  y 6 ;G x i(x) &lt; i(y) Table 1: The transition set of UCCA parser. We write the Stack with its top to the right and the Buffer with its head to the left. (·, ·)X denotes a primary X-labeled edge, and (·, ·)∗X a remote X-labeled edge. i(x) is a running index for the created nodes. In addition to the specified conditions, the prospective child in an E DGE transition must not already have a primary parent. From (Hershcovich et al., 2017). Before Transition Transition Stack List Buffer Nodes Edges S S|x S|x S|y S|x S S S S|y [root] L L L L L L L L L ∅ x|B B y|B x|B B x|B x|B x|B x|B ∅ V V V V V V V V V V E E E E E E E E E E After Transition Stack S HIFT R EDUCE R IGHT-E DGEX L EFT-E DGEX PASS D ROP T OP N ODE -S TARTX N ODE -E ND F INISH S S S S S S S S S ∅ |L|x |x |y |L |y |y Condition List Buffer Nodes Edges ∅ L L L x|L ∅ L L L ∅ B B y|B x|B B B x|B x|B x|B ∅ V V V V V V V ∪ Top(x) V ∪ {ystart=x,label=X } V ∪ {yend=x } V E E E ∪ {(x, y)X } E ∪ {(x, y)X } E E E E E E concept(x) concept(x) ∧ concept(y) concept(x) ∧ concept(y)"
2020.conll-shared.6,2020.conll-shared.1,0,0.205933,"Missing"
2020.conll-shared.6,K19-2001,0,0.0803779,"Missing"
2020.emnlp-main.142,N15-1029,0,0.0367516,"Missing"
2020.emnlp-main.142,N09-2028,0,0.107911,"Missing"
2020.emnlp-main.142,Q14-1011,0,0.0193313,"adapt to the idiosyncrasies of the target disfluency detection data. 3 Experiment 3.1 Settings Dataset. English Switchboard (SWBD) (Godfrey et al., 1992) is the standard and largest (1.73 × 105 sentences for training ) corpus used for disfluency detection. We use English Switchboard as main data. Following the experiment settings in Charniak and Johnson (2001), we split the Switchboard corpus into train, dev and test set as follows: train data consists of all sw[23]∗.dff files, dev data consists of all sw4[5, 6, 7, 8, 9]∗.dff files and test data consists of all sw4[0, 1]∗.dff files. Following Honnibal and Johnson (2014), we lower-case the text and remove all punctuations and partial words.2 We also discard the ‘um’ and ‘uh’ tokens and merge ‘you know’ and ‘i mean’ into single tokens. In addition to Switchboard, we test our models on three out-of-domain publicly available datasets annotated with disfluencies (Zayats et al., 2014; Zayats and Ostendorf, 2018): • CallHome: phone conversations between family members and close friends; • SCOTUS: transcribed Supreme Court oral arguments between justices and advocates; • FCIC: two transcribed hearings from Financial Crisis Inquiry Commission. Corpora SWBD SCOTUS Cal"
2020.emnlp-main.142,N01-1016,0,0.0769255,"d of fine-tuning the ELECTRA model. Although the difference of distribution between our pseudo data and the golden disfluency detection data limits the performance of teacher model, this stage converges faster than fine-tuning the ELECTRA model as it only needs to adapt to the idiosyncrasies of the target disfluency detection data. 3 Experiment 3.1 Settings Dataset. English Switchboard (SWBD) (Godfrey et al., 1992) is the standard and largest (1.73 × 105 sentences for training ) corpus used for disfluency detection. We use English Switchboard as main data. Following the experiment settings in Charniak and Johnson (2001), we split the Switchboard corpus into train, dev and test set as follows: train data consists of all sw[23]∗.dff files, dev data consists of all sw4[5, 6, 7, 8, 9]∗.dff files and test data consists of all sw4[0, 1]∗.dff files. Following Honnibal and Johnson (2014), we lower-case the text and remove all punctuations and partial words.2 We also discard the ‘um’ and ‘uh’ tokens and merge ‘you know’ and ‘i mean’ into single tokens. In addition to Switchboard, we test our models on three out-of-domain publicly available datasets annotated with disfluencies (Zayats et al., 2014; Zayats and Ostendor"
2020.emnlp-main.142,D18-1490,0,0.0142884,"nding. IM Figure 1: A sentence from the English Switchboard corpus with disfluencies annotated. RM=Reparandum, IM=Interregnum, RP=Repair. The preceding RM is corrected by the following RP. corrections. Table 1 gives a few examples. Interregnums are relatively easier to detect as they are often fixed phrases, e.g. “uh”, “you know”. On the other hand, reparandums are more difficult to detect in that they are in free form. As a result, most previous disfluency detection work focuses on detecting reparandums. Most work (Zayats and Ostendorf, 2018; Lou and Johnson, 2017; Wang et al., 2017; Jamshid Lou et al., 2018; Zayats and Ostendorf, 2019) on disfluency detection heavily relies on human-annotated corpora, which is scarce and expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance, self-supervised learning (Wang et al., 2019) and semi-supervised learning techniques (Wang et al., 2018), but they still require human-annotated corpora. In this work, we completely remove the need of humanannotated corpora and propose a novel method to train a disfluency detection system in a completely unsupervised manner, relying on nothing but unlabeled text corpora"
2020.emnlp-main.142,N19-1282,0,0.0901314,"Missing"
2020.emnlp-main.142,N19-1423,0,0.0429222,"ing domain, selfsupervised research mainly focus on word embedding (Mikolov et al., 2013a,b) and language model learning (Bengio et al., 2003; Peters et al., 2018; Radford et al., 2018). For word embedding learning, the idea is to train a model that maps each word to a feature vector, such that it is easy to predict the words in the context given the vector. This converts an apparently unsupervised problem into a “self-supervised” one: learning a function from a given word to the words surrounding it. Language model pre-training (Bengio et al., 2003; Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019) is another line of self-supervised learning task. A trained language model learns a function to predict the likelihood of occurrence of a word based on the surrounding sequence of words used in the text. There are mainly two existing strategies for applying pre-trained language representations to down-stream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo (Peters et al., 2018), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT)"
2020.emnlp-main.142,P17-2087,0,0.128012,"e types: restarts, repetitions and *Email corresponding. IM Figure 1: A sentence from the English Switchboard corpus with disfluencies annotated. RM=Reparandum, IM=Interregnum, RP=Repair. The preceding RM is corrected by the following RP. corrections. Table 1 gives a few examples. Interregnums are relatively easier to detect as they are often fixed phrases, e.g. “uh”, “you know”. On the other hand, reparandums are more difficult to detect in that they are in free form. As a result, most previous disfluency detection work focuses on detecting reparandums. Most work (Zayats and Ostendorf, 2018; Lou and Johnson, 2017; Wang et al., 2017; Jamshid Lou et al., 2018; Zayats and Ostendorf, 2019) on disfluency detection heavily relies on human-annotated corpora, which is scarce and expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance, self-supervised learning (Wang et al., 2019) and semi-supervised learning techniques (Wang et al., 2018), but they still require human-annotated corpora. In this work, we completely remove the need of humanannotated corpora and propose a novel method to train a disfluency detection system in a completely unsupervised manner,"
2020.emnlp-main.142,2020.acl-main.346,0,0.0699336,"-based 1819 model requires large annotated tree-banks that contain both disfluencies and syntactic structures. All of the above works heavily rely on humanannotated data. There exist a limited effort to tackle the training data bottleneck. Wang et al. (2018) and Dong et al. (2019) use an autoencoder method to help for disfluency detection by jointly training the autoencoder model and disfluency detection model. Wang et al. (2019) use self-supervised learning to tackle the training data bottleneck. Their selfsupervised method can substantially reduce the need for human-annotated training data. Lou and Johnson (2020) shows that self-training and ensembling are effective methods for improving disfluency detection. These semi-supervised methods achieve higher performance by introducing pseudo training sentences. However, the performance still relies on human-annotated data. We explore unsupervised disfluency detection, taking inspiration from the success of self-supervised learning and self-training on disfluency detection. Self-Supervised Representation Learning Self-supervised learning aims to train a network on an auxiliary task where ground-truth is obtained automatically. Over the last few years, many"
2020.emnlp-main.142,N06-1020,0,0.121635,"ations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018) and BERT (Devlin et al., 2019), introduces minimal task-specific parameters and is trained on the downstream tasks by simply fine-tuning the pre-trained parameters. Motivated by the success of self-supervised learning, we use self-supervised learning method to train a weak disfluency detection model as teacher model. We also train a sentence grammaticality judgment model to help select sentences with highquality pseudo labels. Self-Training Self-training (McClosky et al., 2006) first uses labeled data to train a good teacher model, then use the teacher model to label unlabeled data and finally use the labeled data and unlabeled data to jointly train a student model. Self-training has also been shown to work well for a variety of tasks including leveraging noisy data (Veit et al., 2017), semantic segmentation (Babakhin et al., 2019), text classification (Li et al., 2019). Xie et al. (2019) present Noisy Student Training, which extends the idea of self-training with the use of equal-or-larger student models and noise added to the student during learning. Our model bui"
2020.emnlp-main.142,N18-1202,0,0.194956,"tuning ELECTRA-Small fine-tuning ELECTRA-Base fine-tuning Teacher fine-tuning Unsupervised teacher Our unsupervised P R F1 91.9 92.2 91.6 92.9 92.5 86.8 90.2 85.1 89.8 89.5 91.2 92.1 62.0 89.1 88.4 90.9 90.5 92.0 92.3 72.3 89.6 Method Unsupervised teacher ELECTRA-Base Teacher fine-tuning Pattern-match Our unsupervised Table 3: Experiment results on the Switchboard dev set. “ ∗ fine-tuning” means “ fine-tuning ∗ model” on the Switchboard train set. The first part (from row 1 to row 5) is the supervised method using complicated hand-crafted features or contextualized word embeddings (e.g. ELMo (Peters et al., 2018) and ELECTRA), the second part (row 6 to 7) is the unsupervised methods. Method UBT (Wu et al., 2015) Bi-LSTM (Zayats et al., 2016) NCM (Lou and Johnson, 2017) Transition-based (Wang et al., 2017) Self-supervised(Wang et al., 2019) Self-training(Lou and Johnson, 2020) EGBC(Bach and Huang, 2019) Our Method P R F1 90.3 91.8 91.1 93.4 87.5 95.7 88.2 80.5 80.6 84.1 87.3 93.8 88.3 87.8 85.1 85.9 86.8 87.5 90.2 90.6 91.8 88.0 Table 4: Comparison with previous state-of-the-art methods on the Switchboard test set. The first part (from row 1 to row 4) is the methods without using contextualized word em"
2020.emnlp-main.142,N13-1102,0,0.0591073,"Missing"
2020.emnlp-main.142,D13-1013,0,0.0517005,"Missing"
2020.emnlp-main.142,C18-1299,0,0.0605723,"”. On the other hand, reparandums are more difficult to detect in that they are in free form. As a result, most previous disfluency detection work focuses on detecting reparandums. Most work (Zayats and Ostendorf, 2018; Lou and Johnson, 2017; Wang et al., 2017; Jamshid Lou et al., 2018; Zayats and Ostendorf, 2019) on disfluency detection heavily relies on human-annotated corpora, which is scarce and expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance, self-supervised learning (Wang et al., 2019) and semi-supervised learning techniques (Wang et al., 2018), but they still require human-annotated corpora. In this work, we completely remove the need of humanannotated corpora and propose a novel method to train a disfluency detection system in a completely unsupervised manner, relying on nothing but unlabeled text corpora. Our model builds upon the recent work on Noisy Student Training (Xie et al., 2019), a semisupervised learning approach based on the idea of 1813 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1813–1822, c November 16–20, 2020. 2020 Association for Computational Linguistics Algorithm"
2020.emnlp-main.142,C16-1027,1,0.891168,"sing contextualized word embeddings (e.g. BERT and ELECTRA). 1 Type repair repair repetition restart RP Annotation [ the + they ’re ] voice activated [ we want + {well} in our area we want ] to [we got + {uh} we got ] to talking [ we would like + ] let’s go to the Table 1: Different types of disfluencies. Introduction Automatic speech recognition (ASR) outputs often contain various disfluencies, which is a characteristic of spontaneous speech and create barriers to subsequent text processing tasks like parsing, machine translation, and summarization. Disfluency detection (Zayats et al., 2016; Wang et al., 2016; Wu et al., 2015) focuses on recognizing the disfluencies from ASR outputs. As shown in Figure 1, a standard annotation of the disfluency structure indicates the reparandum (words that the speaker intends to discard), the interruption point (denoted as ‘+’, marking the end of the reparandum), an optional interregnum (filled pauses, discourse cue words, etc.) and the associated repair (Shriberg, 1994). Ignoring the interregnum, disfluencies are categorized into three types: restarts, repetitions and *Email corresponding. IM Figure 1: A sentence from the English Switchboard corpus with disfluen"
2020.emnlp-main.142,D17-1296,1,0.904698,"titions and *Email corresponding. IM Figure 1: A sentence from the English Switchboard corpus with disfluencies annotated. RM=Reparandum, IM=Interregnum, RP=Repair. The preceding RM is corrected by the following RP. corrections. Table 1 gives a few examples. Interregnums are relatively easier to detect as they are often fixed phrases, e.g. “uh”, “you know”. On the other hand, reparandums are more difficult to detect in that they are in free form. As a result, most previous disfluency detection work focuses on detecting reparandums. Most work (Zayats and Ostendorf, 2018; Lou and Johnson, 2017; Wang et al., 2017; Jamshid Lou et al., 2018; Zayats and Ostendorf, 2019) on disfluency detection heavily relies on human-annotated corpora, which is scarce and expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance, self-supervised learning (Wang et al., 2019) and semi-supervised learning techniques (Wang et al., 2018), but they still require human-annotated corpora. In this work, we completely remove the need of humanannotated corpora and propose a novel method to train a disfluency detection system in a completely unsupervised manner, relying on nothing"
2020.emnlp-main.142,P15-1048,0,0.248866,"word embeddings (e.g. BERT and ELECTRA). 1 Type repair repair repetition restart RP Annotation [ the + they ’re ] voice activated [ we want + {well} in our area we want ] to [we got + {uh} we got ] to talking [ we would like + ] let’s go to the Table 1: Different types of disfluencies. Introduction Automatic speech recognition (ASR) outputs often contain various disfluencies, which is a characteristic of spontaneous speech and create barriers to subsequent text processing tasks like parsing, machine translation, and summarization. Disfluency detection (Zayats et al., 2016; Wang et al., 2016; Wu et al., 2015) focuses on recognizing the disfluencies from ASR outputs. As shown in Figure 1, a standard annotation of the disfluency structure indicates the reparandum (words that the speaker intends to discard), the interruption point (denoted as ‘+’, marking the end of the reparandum), an optional interregnum (filled pauses, discourse cue words, etc.) and the associated repair (Shriberg, 1994). Ignoring the interregnum, disfluencies are categorized into three types: restarts, repetitions and *Email corresponding. IM Figure 1: A sentence from the English Switchboard corpus with disfluencies annotated. RM"
2020.emnlp-main.142,D16-1109,0,0.0287976,"Missing"
2020.emnlp-main.142,N19-1008,0,0.0420599,": A sentence from the English Switchboard corpus with disfluencies annotated. RM=Reparandum, IM=Interregnum, RP=Repair. The preceding RM is corrected by the following RP. corrections. Table 1 gives a few examples. Interregnums are relatively easier to detect as they are often fixed phrases, e.g. “uh”, “you know”. On the other hand, reparandums are more difficult to detect in that they are in free form. As a result, most previous disfluency detection work focuses on detecting reparandums. Most work (Zayats and Ostendorf, 2018; Lou and Johnson, 2017; Wang et al., 2017; Jamshid Lou et al., 2018; Zayats and Ostendorf, 2019) on disfluency detection heavily relies on human-annotated corpora, which is scarce and expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance, self-supervised learning (Wang et al., 2019) and semi-supervised learning techniques (Wang et al., 2018), but they still require human-annotated corpora. In this work, we completely remove the need of humanannotated corpora and propose a novel method to train a disfluency detection system in a completely unsupervised manner, relying on nothing but unlabeled text corpora. Our model builds upon the r"
2020.emnlp-main.142,C10-1154,0,0.0533282,"Missing"
2020.emnlp-main.634,I05-5002,0,\N,Missing
2020.emnlp-main.634,W07-1401,0,\N,Missing
2020.emnlp-main.634,D13-1170,0,\N,Missing
2020.emnlp-main.634,S17-2001,0,\N,Missing
2020.emnlp-main.634,D17-1169,0,\N,Missing
2020.emnlp-main.634,N18-1202,0,\N,Missing
2020.emnlp-main.634,N19-1213,0,\N,Missing
2020.emnlp-main.634,K16-1002,0,\N,Missing
2020.emnlp-main.634,N19-1423,0,\N,Missing
2020.emnlp-main.634,N18-1101,0,\N,Missing
2020.emnlp-main.634,Q19-1040,0,\N,Missing
2020.emnlp-main.634,U19-1011,0,\N,Missing
2020.findings-emnlp.163,P19-1544,0,0.302918,"erent intents and ensure that the ratio of sentences has 1-3 intents is [0.3, 0.5, 0.2]. a slot-gated joint model to explicitly consider the Finally, we get the 45,000 utterances for training, correlation between slot filling and intent detection. 3) Bi-Model. Wang et al. (2018) proposes the Bi2,500 utterances for validation and 2500 utterances model to consider the cross-impact between the for testing on the MixSNIPS dataset. Similarly, we construct another multi-intent SLU dataset, Mix- intent detection and slot filling. ATIS, from the ATIS dataset (Hemphill et al., 1990). 4) SF-ID Network. Haihong et al. (2019) proposes There are 18,000 utterances for training, 1,000 ut- an SF-ID network to establish direct connections terances for validation and 1,000 utterances for test- for the slot filling and intent detection to help them promote each other mutually. ing. The constructed datasets have been released 5) Stack-Propagation. Qin et al. (2019) adopts a for future research. joint model with Stack-Propagation to capture the 2 The official DSTC4 pilot tasks’ Handbook intent semantic knowledge and perform the tokenhttp://www.colips.org/workshop/dstc4/ DSTC4_pilot_tasks.pdf level intent detection to furth"
2020.findings-emnlp.163,N19-1055,0,0.347591,"of input vectors X ∈ RT ×d (d represents the mapped dimension) to queries Q, keys K and values V matrices by using different linear projections parameters W q , W k , W v . Attention weight is computed by dot product between Q, K and the self-attention output A ∈ RT ×d is a weighted sum of values:   QK > A = softmax √ V , (1) dk where dk denotes the dimension of keys. We concatenate these two representations as the final encoding representation: E = [H ||A] , (2) where E = {e1 , . . . , eT } ∈ RT ×2d and ||is concatenation operation. 2.2 Intent Detection Decoder We follow Gangadharaiah and Narayanaswamy (2019) to perform multiple intent detection as the multi-label classification problem. We compute the utterance context vector over E = {e1 , . . . , eT } ∈ RT ×2d . In our case, we use a self-attention module (Zhong et al., 2018; Goo et al., 2018) to capture relevant context: pt = softmax(we et + b) , X c= pt et , (3) (4) t where we ∈ R1×2d is the trainable parameters, pt is corresponding normalized self-attention score. c is the weighted sum of each element et and utilized for intent detection: y I = σ(W i (LeakyReLU(W c c+bc ))+bi ) , (5) where W i , W c are trainable parameters of the inI } is t"
2020.findings-emnlp.163,N18-2118,0,0.384193,"he model should directly detect its all intents (PlayMusic and GetWeather). Hence, it is important to consider multi-intent SLU. Unlike the prior single intent SLU model which can simply leverage the utterance’s single intent to guide slot prediction (Goo et al., 2018; Qin et al., 2019), multi-intent SLU faces to multiple intents and presents a unique challenge that is worth studying: how to effectively incorporate multiple intents information to lead the slot prediction. To this end, Gangadharaiah and Narayanaswamy (2019) first explored the multi-task framework with the slot-gated mechanism (Goo et al., 2018) for joint multiple intent detection and slot filling. Their model incorporated intent information by simply treating an intent context vector as multiple intents information. While this is a direct method for incorporating multiple intents information, it does not offer fine-grained intent information integration for token-level slot filling in the sense that each token is guided with the same complex intents information, which is shown in Figure 1(a). In addition, providing the same intent information for all tokens may introduce ambiguity, where it’s hard for each token to capture the relat"
2020.findings-emnlp.163,H90-1021,0,0.305227,"trast to prior work simply incorporate multiple intents information statically where the same intents information is used for guiding all tokens, our intent-slot interaction graph is constructed adaptively with graph attention network over each token. This encourages our model to automatically filter the irrelevant information and capture important intent at the token-level. We first conduct experiments on the multi-intent benchmark dataset DSTC4 (Schuster et al., 2019). Then, to verify the generalization of our framework, we empirically construct two large-scale multiintent datasets MixATIS (Hemphill et al., 1990) and MixSNIPS (Coucke et al., 2018). The results of these experiments show the effectiveness of our framework by outperforming the current state-ofthe-art method. To the best of our knowledge, there are no public large-scale multiple intents datasets and we hope the release of it would push forward the research of multi-intent SLU. In addition, our framework achieves state-of-the-art performance on two public single-intent datasets including ATIS (Tur and De Mori, 2011) and SNIPS (Coucke et al., 2018), which further verifies the generalization of the proposed model. To facilitate future resear"
2020.findings-emnlp.163,D18-1417,0,0.0846973,"work, we jointly perform multi-label intent detection and slot prediction, while they only consider the subtask intent detection. Slot Filling Slot filling can be treated as a sequence labeling task. The popular approaches are conditional random fields (CRF) (Raymond and Riccardi, 2007) and recurrent neural networks (RNN) (Xu and Sarikaya, 2013a; Yao et al., 2014). Recently, Shen et al. (2018) and Tan et al. (2018) introduce the self-attention mechanism for CRF-free sequential labeling. Joint Model To consider the high correlation between intent and slots, many joint models (Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019; Qin et al., 2019) are proposed to solve two tasks. Goo et al. (2018); Li et al. (2018); Zhang et al. (2019) propose to utilize the intent information to guide the slot filling. Qin et al. (2019) further utilize a stack-propagation framework for better leveraging intent semantic information to guide the slot filling, which achieves the state-of-the-art performance. Wang et al. (2018) and E et al. (2019) consider the cross-impact between the slot and intents. Our framework follows those state-of-the-art joint model paradigm, and further focus"
2020.findings-emnlp.163,D19-1097,0,0.17481,"on and slot prediction, while they only consider the subtask intent detection. Slot Filling Slot filling can be treated as a sequence labeling task. The popular approaches are conditional random fields (CRF) (Raymond and Riccardi, 2007) and recurrent neural networks (RNN) (Xu and Sarikaya, 2013a; Yao et al., 2014). Recently, Shen et al. (2018) and Tan et al. (2018) introduce the self-attention mechanism for CRF-free sequential labeling. Joint Model To consider the high correlation between intent and slots, many joint models (Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019; Qin et al., 2019) are proposed to solve two tasks. Goo et al. (2018); Li et al. (2018); Zhang et al. (2019) propose to utilize the intent information to guide the slot filling. Qin et al. (2019) further utilize a stack-propagation framework for better leveraging intent semantic information to guide the slot filling, which achieves the state-of-the-art performance. Wang et al. (2018) and E et al. (2019) consider the cross-impact between the slot and intents. Our framework follows those state-of-the-art joint model paradigm, and further focus on the multiple intents scenario while the above jo"
2020.findings-emnlp.163,D19-1214,1,0.85237,"nces model to consider the cross-impact between the for testing on the MixSNIPS dataset. Similarly, we construct another multi-intent SLU dataset, Mix- intent detection and slot filling. ATIS, from the ATIS dataset (Hemphill et al., 1990). 4) SF-ID Network. Haihong et al. (2019) proposes There are 18,000 utterances for training, 1,000 ut- an SF-ID network to establish direct connections terances for validation and 1,000 utterances for test- for the slot filling and intent detection to help them promote each other mutually. ing. The constructed datasets have been released 5) Stack-Propagation. Qin et al. (2019) adopts a for future research. joint model with Stack-Propagation to capture the 2 The official DSTC4 pilot tasks’ Handbook intent semantic knowledge and perform the tokenhttp://www.colips.org/workshop/dstc4/ DSTC4_pilot_tasks.pdf level intent detection to further alleviate the error 1811 Model Attention BiRNN Slot-Gated Slot-gated Intent Bi-Model SF-ID Stack-Propagation (concatenation) Stack-Propagation (sigmoid-decoder) Joint Multiple ID-SF AGIF Slot (F1) 86.6 88.1 86.7 85.5 87.7 86.6 87.4 87.5 88.1 MixATIS Intent (F1) Intent (Acc) 71.6 65.7 66.2 72.3 63.7 76.0 79.0 71.9 80.6 73.1 81.2* 75.8"
2020.findings-emnlp.163,N18-2050,0,0.296264,"ose an alignment-based RNN with the attention MixSNIPS. MixSNIPS dataset is collected from the Snips personal voice assistant (Coucke et al., mechanism, which implicitly learns the relationship between slot and intent. 2018) by using conjunctions, e.g., “and”, to connect 2) Slot-Gated Atten. Goo et al. (2018) proposes sentences with different intents and ensure that the ratio of sentences has 1-3 intents is [0.3, 0.5, 0.2]. a slot-gated joint model to explicitly consider the Finally, we get the 45,000 utterances for training, correlation between slot filling and intent detection. 3) Bi-Model. Wang et al. (2018) proposes the Bi2,500 utterances for validation and 2500 utterances model to consider the cross-impact between the for testing on the MixSNIPS dataset. Similarly, we construct another multi-intent SLU dataset, Mix- intent detection and slot filling. ATIS, from the ATIS dataset (Hemphill et al., 1990). 4) SF-ID Network. Haihong et al. (2019) proposes There are 18,000 utterances for training, 1,000 ut- an SF-ID network to establish direct connections terances for validation and 1,000 utterances for test- for the slot filling and intent detection to help them promote each other mutually. ing. The"
2020.findings-emnlp.163,D18-1348,0,0.0540665,"ble 4 shows the experiment results of the proposed models on the ATIS and SNIPS datasets. From the table, we can see that our model outperforms all the compared baselines and achieves state-of-the-art performance. This demonstrates the generalizability and effectiveness of our framework whether handling multiintent or single-intent SLU. 4 Related Work Intent Detection Intent detection is formulated as an utterance classification problem. Different classification methods, such as support vector machine (SVM) and RNN (Haffner et al., 2003; Sarikaya et al., 2011), have been proposed to solve it. Xia et al. (2018) adopts a capsule-based neural network with self-attention for intent detection. 1814 However, the above models mainly focus on the single intent scenario, which can not handle the complex multiple intent scenario. Xu and Sarikaya (2013b) and Kim et al. (2017a) explore the complex scenario, where multiple intents are assigned to a user’s utterance. Xu and Sarikaya (2013b) use log-linear models to achieve this, while we use neural network models. Compared with their work, we jointly perform multi-label intent detection and slot prediction, while they only consider the subtask intent detection."
2020.findings-emnlp.163,P19-1519,0,0.0568845,"an be treated as a sequence labeling task. The popular approaches are conditional random fields (CRF) (Raymond and Riccardi, 2007) and recurrent neural networks (RNN) (Xu and Sarikaya, 2013a; Yao et al., 2014). Recently, Shen et al. (2018) and Tan et al. (2018) introduce the self-attention mechanism for CRF-free sequential labeling. Joint Model To consider the high correlation between intent and slots, many joint models (Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019; Qin et al., 2019) are proposed to solve two tasks. Goo et al. (2018); Li et al. (2018); Zhang et al. (2019) propose to utilize the intent information to guide the slot filling. Qin et al. (2019) further utilize a stack-propagation framework for better leveraging intent semantic information to guide the slot filling, which achieves the state-of-the-art performance. Wang et al. (2018) and E et al. (2019) consider the cross-impact between the slot and intents. Our framework follows those state-of-the-art joint model paradigm, and further focus on the multiple intents scenario while the above joint models do not consider. Recently, Gangadharaiah and Narayanaswamy (2019) propose a joint model to conside"
2020.findings-emnlp.163,P18-1135,0,0.057693,"Missing"
2020.findings-emnlp.163,N19-1380,0,0.0200537,"ach token has the ability to capture different relevant intent information so that fine-grained multiple intents integration can be achieved. In contrast to prior work simply incorporate multiple intents information statically where the same intents information is used for guiding all tokens, our intent-slot interaction graph is constructed adaptively with graph attention network over each token. This encourages our model to automatically filter the irrelevant information and capture important intent at the token-level. We first conduct experiments on the multi-intent benchmark dataset DSTC4 (Schuster et al., 2019). Then, to verify the generalization of our framework, we empirically construct two large-scale multiintent datasets MixATIS (Hemphill et al., 1990) and MixSNIPS (Coucke et al., 2018). The results of these experiments show the effectiveness of our framework by outperforming the current state-ofthe-art method. To the best of our knowledge, there are no public large-scale multiple intents datasets and we hope the release of it would push forward the research of multi-intent SLU. In addition, our framework achieves state-of-the-art performance on two public single-intent datasets including ATIS ("
2020.findings-emnlp.58,C10-3004,1,0.668986,"f the whole word, which is easier for the model to predict. In Chinese condition, WordPiece tokenizer no longer split the word into small fragments, as Chinese characters are not formed by alphabet-like symbols. We use the traditional Chinese Word Segmentation (CWS) tool to split the text into several words. In this way, we could adopt whole word masking in Chinese to mask the word instead of individual Chinese characters. For implementation, we strictly followed the original whole word masking codes and did not change other components, such as the percentage of word masking, etc. We use LTP (Che et al., 2010) for Chinese word segmentation to identify the word boundaries. 659 Chinese English Original Sentence + CWS + BERT Tokenizer 使用语言模型来预测下一个词的概率。 使用 语言 模型 来 预测 下 一个 词 的 概率 。 使用语言模型来预测下一个词的概率。 we use a language model to predict the probability of the next word. we use a language model to pre ##di ##ct the pro ##ba ##bility of the next word . Original Masking + WWM ++ N-gram Masking +++ Mac Masking 使 用 语 言 [M] 型 来 [M] 测 下 一 个 词 的 概 率 。 使 用 语 言 [M] [M] 来 [M] [M] 下 一 个 词 的 概 率 。 使 用 [M] [M] [M] [M] 来 [M] [M] 下 一 个 词 的 概 率 。 使用语法建模来预见下一个词的几率。 we use a language [M] to [M] ##di ##ct the pro [M] ##bility"
2020.findings-emnlp.58,D18-1536,0,0.0401552,"on various natural language processing tasks, covering a wide spectrum of text length, i.e., from sentencelevel to document-level. Task details are shown in Table 2. Specifically, we choose the following eight popular Chinese datasets. 5 https://cloud.google.com/tpu/ • Machine Reading Comprehension (MRC): CMRC 2018 (Cui et al., 2019b), DRCD (Shao et al., 2018), CJRC (Duan et al., 2019). • Single Sentence Classification (SSC): ChnSentiCorp (Tan and Zhang, 2008), THUCNews (Li and Sun, 2007). • Sentence Pair Classification (SPC): XNLI (Conneau et al., 2018), LCQMC (Liu et al., 2018), BQ Corpus (Chen et al., 2018). In order to make a fair comparison, for each dataset, we keep the same hyper-parameters (such as maximum length, warm-up steps, etc.) and only tune the initial learning rate from 1e-5 to 5e-5 for each task. Note that the initial learning rates are tuned on original Chinese BERT, and it would be possible to achieve another gains by tuning the learning rate individually. We run the same experiment ten times to ensure the reliability of results. The best initial learning rate is determined by selecting the best average development set performance. We report the maximum and average scores to bot"
2020.findings-emnlp.58,D18-1241,0,0.0205389,"could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrating that the pre-trained language models have 1 https://github.com/ymcui/MacBERT become new fundamental components in natural language processing field. Starting from BERT, the community have made great and rapid progress on optimizing the pretrained language models, such as ERNIE (Sun et al., 2019a), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), SpanBERT (Joshi e"
2020.findings-emnlp.58,D18-1269,0,0.0475474,"rained language models, we carried out extensive experiments on various natural language processing tasks, covering a wide spectrum of text length, i.e., from sentencelevel to document-level. Task details are shown in Table 2. Specifically, we choose the following eight popular Chinese datasets. 5 https://cloud.google.com/tpu/ • Machine Reading Comprehension (MRC): CMRC 2018 (Cui et al., 2019b), DRCD (Shao et al., 2018), CJRC (Duan et al., 2019). • Single Sentence Classification (SSC): ChnSentiCorp (Tan and Zhang, 2008), THUCNews (Li and Sun, 2007). • Sentence Pair Classification (SPC): XNLI (Conneau et al., 2018), LCQMC (Liu et al., 2018), BQ Corpus (Chen et al., 2018). In order to make a fair comparison, for each dataset, we keep the same hyper-parameters (such as maximum length, warm-up steps, etc.) and only tune the initial learning rate from 1e-5 to 5e-5 for each task. Note that the initial learning rates are tuned on original Chinese BERT, and it would be possible to achieve another gains by tuning the learning rate individually. We run the same experiment ten times to ensure the reliability of results. The best initial learning rate is determined by selecting the best average development set per"
2020.findings-emnlp.58,D19-1600,1,0.93983,"on both left and right context in all Transformer layers. Primarily, BERT consists of two pre-training tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP). Later, they further proposed a technique called whole word masking (wwm) for optimizing the original masking in the MLM task. In this setting, instead of randomly selecting WordPiece (Wu et al., 2016) tokens to mask, we always mask all of the tokens corresponding to a whole word at once. This will explicitly force the model to recover the whole word in the MLM pre-training task instead of just recovering WordPiece tokens (Cui et al., 2019a), which is much more challenging. As the whole word masking only affects the masking strategy of the pre-training process, it would not bring additional burdens on down-stream tasks. Moreover, as training pre-trained language models are computationally expensive, they also release all the pretrained models as well as the source codes, which stimulates the community to have great interests in the research of pre-trained language models. 2.2 ERNIE ERNIE (Enhanced Representation through kNowledge IntEgration) (Sun et al., 2019a) is designed to optimize the masking process of BERT, which include"
2020.findings-emnlp.58,P19-1285,0,0.0713452,"Missing"
2020.findings-emnlp.58,N19-1423,0,0.480575,"he community. We also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways, especially the masking strategy that adopts MLM as correction (Mac). We carried out extensive experiments on eight Chinese NLP tasks to revisit the existing pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrat"
2020.findings-emnlp.58,Q19-1026,0,0.0190409,"ances on many NLP tasks, and we also ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrating that the pre-trained language models have 1 https://github.com/ymcui/MacBERT become new fundamental components in natural language processing field. Starting from BERT, the community have made great and rapid progress on optimizing the pretrained language models, such as ERNIE (Sun et al., 2019a), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), SpanBERT (Joshi et al., 2019), ALBERT (Lan et al., 2019), ELEC"
2020.findings-emnlp.58,D17-1082,0,0.0475345,"lso ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrating that the pre-trained language models have 1 https://github.com/ymcui/MacBERT become new fundamental components in natural language processing field. Starting from BERT, the community have made great and rapid progress on optimizing the pretrained language models, such as ERNIE (Sun et al., 2019a), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), SpanBERT (Joshi et al., 2019), ALBERT (Lan et al., 2019), ELECTRA (Clark et al., 2020),"
2020.findings-emnlp.58,D07-1081,0,0.024255,"4.2 Setups for Fine-tuning Tasks To thoroughly test these pre-trained language models, we carried out extensive experiments on various natural language processing tasks, covering a wide spectrum of text length, i.e., from sentencelevel to document-level. Task details are shown in Table 2. Specifically, we choose the following eight popular Chinese datasets. 5 https://cloud.google.com/tpu/ • Machine Reading Comprehension (MRC): CMRC 2018 (Cui et al., 2019b), DRCD (Shao et al., 2018), CJRC (Duan et al., 2019). • Single Sentence Classification (SSC): ChnSentiCorp (Tan and Zhang, 2008), THUCNews (Li and Sun, 2007). • Sentence Pair Classification (SPC): XNLI (Conneau et al., 2018), LCQMC (Liu et al., 2018), BQ Corpus (Chen et al., 2018). In order to make a fair comparison, for each dataset, we keep the same hyper-parameters (such as maximum length, warm-up steps, etc.) and only tune the initial learning rate from 1e-5 to 5e-5 for each task. Note that the initial learning rates are tuned on original Chinese BERT, and it would be possible to achieve another gains by tuning the learning rate individually. We run the same experiment ten times to ensure the reliability of results. The best initial learning r"
2020.findings-emnlp.58,C18-1166,0,0.0996317,"Missing"
2020.findings-emnlp.58,2021.ccl-1.108,0,0.114968,"Missing"
2020.findings-emnlp.58,P18-2124,0,0.0291812,"proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrating that the pre-trained language models have 1 https://github.com/ymcui/MacBERT become new fundamental components in natural language processing field. Starting from BERT, the community have made great and rapid progress on optimizing the pretrained language models, such as ERNIE (Sun et al., 2019a), XLNet (Yang et al.,"
2020.findings-emnlp.58,D16-1264,0,0.0704602,"019), we only compare BERT (Devlin et al., 2019), BERT-wwm, BERT-wwm-ext, RoBERTawwm-ext, RoBERTa-wwm-ext-large, ELECTRA, along with our MacBERT to ensure relatively fair comparisons among different models, where all models are trained by ourselves except for the original Chinese BERT by Devlin et al. (2019). We carried out experiments under TensorFlow framework (Abadi et al., 2016) with slight modifications to the fine-tuning scripts6 provided by Devlin et al. (2019) to better adapt to Chinese. 5 • CMRC 2018: A span-extraction machine reading comprehension dataset, which is similar to SQuAD (Rajpurkar et al., 2016) that extract a passage span for the given question. • DRCD: This is also a span-extraction MRC dataset but in Traditional Chinese. • CJRC: Similar to CoQA (Reddy et al., 2019), which has yes/no questions, no-answer questions, and span-extraction questions. The data is collected from Chinese law judgment documents. Note that we only use small-train-data.json for training. F1 EM F1 BERT BERT-wwm BERT-wwm-ext RoBERTa-wwm-ext ELECTRA-base MacBERT-base 83.1 (82.7) 84.3 (83.4) 85.0 (84.5) 86.6 (85.9) 87.5 (87.0) 89.4 (89.2) 89.9 (89.6) 90.5 (90.2) 91.2 (90.9) 92.5 (92.2) 92.5 (92.3) 94.3 (94.1) 82."
2020.findings-emnlp.58,Q19-1016,0,0.0917529,"results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrating that the pre-trained language models have 1 https://github.com/ymcui/MacBERT become new fundamental components in natural language processing field. Starting from BERT, the community have made great and rapid progress on optimizing the pretrained language models, such as ERNIE (Sun et al., 2019a), XLNet (Yang et al., 2019), RoBERTa (Liu et al."
2020.findings-emnlp.58,L18-1431,1,0.872933,"or clarity, we do not list ‘ext’ models, where the other parameters are the same with the one that is not trained on extended data. 4.2 Setups for Fine-tuning Tasks To thoroughly test these pre-trained language models, we carried out extensive experiments on various natural language processing tasks, covering a wide spectrum of text length, i.e., from sentencelevel to document-level. Task details are shown in Table 2. Specifically, we choose the following eight popular Chinese datasets. 5 https://cloud.google.com/tpu/ • Machine Reading Comprehension (MRC): CMRC 2018 (Cui et al., 2019b), DRCD (Shao et al., 2018), CJRC (Duan et al., 2019). • Single Sentence Classification (SSC): ChnSentiCorp (Tan and Zhang, 2008), THUCNews (Li and Sun, 2007). • Sentence Pair Classification (SPC): XNLI (Conneau et al., 2018), LCQMC (Liu et al., 2018), BQ Corpus (Chen et al., 2018). In order to make a fair comparison, for each dataset, we keep the same hyper-parameters (such as maximum length, warm-up steps, etc.) and only tune the initial learning rate from 1e-5 to 5e-5 for each task. Note that the initial learning rates are tuned on original Chinese BERT, and it would be possible to achieve another gains by tuning the"
2020.nlptea-1.5,W13-4414,0,0.0315376,"de gap for our system to solve the Chinese grammatical error diagnosis. Table 5 shows the performances on error correction. We achieve the second-highest correction top1 score. Since we only provide zero or one candidate word, our correction top1 score is the same as our correction top3 score. 5 Related Work The researchers used many different methods to study the English Grammatical Error Correction task and achieved good results (Ng et al., 2014). Compared with English, the research time of Chinese grammatical error diagnosis system is short, the data sets and effective methods are lacking. Chen et al. (2013) still used n-gram as the main method, and added Web resources to improve detection performance. Lin and Chu (2015) established a scoring system using n-gram, and get better correction options. In recent years, Chinese grammatical error diagnosis has been cited as a shared task of NLPTEA CGED. Many methods are proposed to solve this task (Yu et al., 2014; Lee et al., 2015, 2016). Zheng et al. (2016) proposed a BiLSTMCRF model based on character embedding on bigram embedding. Shiue et al. (2017) combined machine learning with traditional n-gram methods, using Bi-LSTM to detect the location of e"
2020.nlptea-1.5,W18-3707,1,0.822656,"ility of RoBERTa and n-gram, visual similarity, and phonological similarity (Hong et al., 2019). Afterward, we select the character with the highest score as the correction result. For the multi-character correction, we also select the top 20 characters generated by RoBERTa at each position. We put these characters together to form words and reserved those in the vocabulary as candidates. In addition to the four kinds of features at the single-character correction, we also consider Levenshtein distance between the error words and candidate words. 4 4.1 Experiment Dataset Following the work of Fu et al. (2018), We trained our single models using training units that contain both the erroneous and the corrected sentences from 2016 (HSK Track), 2017 and 2018 training data sets. CGED 2016 HSK track training set consists of 10,071 training units with a total of 24,797 grammatical errors, categorized as redundant (5,538 instances), missing (6,623), word selection (10,949) and word ordering (1,687). CGED 2017 training set consists of 10,449 training units 39 model FPR Detection level Identification Position Precision Recall F1 Precision Recall F1 Precision Recall F1 BERT 0.6333 0.6974 0.8626 0.7713 0.5406"
2020.nlptea-1.5,W04-1213,0,0.14189,"edundant words (R), word selection errors (S) and word ordering errors (W). The input sentence may contain one or more such errors. Given a sentence, the system needs to indicate: (1) If the sentence is correct or not; (2) What kind of errors the sentence contains; (3) The exact error position; (4) Possible corrections for S-type and M-type errors. Some typical examples are shown in Table 1. 3 3.1 Methodology Error Detection We treat the error detection problem as a sequence tagging problem. Specifically, given a sentence x, we generate a corresponding label sequence y using the BIO encoding (Kim et al., 2004). We then combine ResNet and transformer encoder to solve the tagging problem. h0i = We wi + Wp (1) hli = transformer block(hl−1 i ) (2) yiBERT = softmax(Wo hL i + bo ) (3) where wi is a current token, and N denotes the sequence length. Equation 1 thus creates an input embedding. Here, transformer block includes selfattention and fully connected layers, and outputs 37 hli . l is the number of the current layer, l ≥ 1. L is the total number of layers of BERT. Equation 3 denotes the output layer. Wo is an output weight matrix, bo is a bias for the output layer, and yiBERT is a grammatical error"
2020.nlptea-1.5,P17-1080,0,0.0310529,"r of the current layer, l ≥ 1. L is the total number of layers of BERT. Equation 3 denotes the output layer. Wo is an output weight matrix, bo is a bias for the output layer, and yiBERT is a grammatical error detection prediction. and dropout values. The basic ensemble selection procedure is very simple: 1. Start with the empty ensemble. 2. Add to the ensemble the model in the library that maximizes the ensemble’s performance to the Chinese grammatical error detection metric on validation set. Integrating ResNet Deep neural networks learn different representations for each layer. For example, Belinkov et al. (2017) demonstrated that in a machine translation task, the low layers of the network learn to represent the word structure, while higher layers are more focused on word meaning. For tasks that emphasize the grammatical nature such as Chinese grammatical error detection, information from the lower layers is considered to be important. In this work, we use the residual learning framework (He et al., 2016) to combine the information from word embedding with the information from deep layer. Given a sequence S = w0 , ......, wN as input, ResBERT is formulated as follows: h0i = We wi + Wp (4) hli = trans"
2020.nlptea-1.5,W16-4906,0,0.052228,"Missing"
2020.nlptea-1.5,I17-4012,0,0.0280461,"o improve detection performance. Lin and Chu (2015) established a scoring system using n-gram, and get better correction options. In recent years, Chinese grammatical error diagnosis has been cited as a shared task of NLPTEA CGED. Many methods are proposed to solve this task (Yu et al., 2014; Lee et al., 2015, 2016). Zheng et al. (2016) proposed a BiLSTMCRF model based on character embedding on bigram embedding. Shiue et al. (2017) combined machine learning with traditional n-gram methods, using Bi-LSTM to detect the location of errors and adding additional linguistic information, POS, ngram. Li et al. (2017) used Bi-LSTM to generate Testing Results Table 4 shows the performances on error detection. Our system achieves the best F1 scores at the identification level and position level. Although we achieve the highest position-level F1 score of 41 the probability of each characters, and used two strategies to decide whether a character is correct or not. Liao et al. (2017) used the LSTM-CRF model to detect dependencies between outputs to better detect error messages. Yang et al. (2017) added more linguistic information on LSTM-CRF model such as POS, n-gram, PMI score and dependency features. Their s"
2020.nlptea-1.5,W16-4907,1,0.770855,"on task and achieved good results (Ng et al., 2014). Compared with English, the research time of Chinese grammatical error diagnosis system is short, the data sets and effective methods are lacking. Chen et al. (2013) still used n-gram as the main method, and added Web resources to improve detection performance. Lin and Chu (2015) established a scoring system using n-gram, and get better correction options. In recent years, Chinese grammatical error diagnosis has been cited as a shared task of NLPTEA CGED. Many methods are proposed to solve this task (Yu et al., 2014; Lee et al., 2015, 2016). Zheng et al. (2016) proposed a BiLSTMCRF model based on character embedding on bigram embedding. Shiue et al. (2017) combined machine learning with traditional n-gram methods, using Bi-LSTM to detect the location of errors and adding additional linguistic information, POS, ngram. Li et al. (2017) used Bi-LSTM to generate Testing Results Table 4 shows the performances on error detection. Our system achieves the best F1 scores at the identification level and position level. Although we achieve the highest position-level F1 score of 41 the probability of each characters, and used two strategies to decide whether a"
2020.nlptea-1.5,I17-4011,0,0.0273139,"ased on character embedding on bigram embedding. Shiue et al. (2017) combined machine learning with traditional n-gram methods, using Bi-LSTM to detect the location of errors and adding additional linguistic information, POS, ngram. Li et al. (2017) used Bi-LSTM to generate Testing Results Table 4 shows the performances on error detection. Our system achieves the best F1 scores at the identification level and position level. Although we achieve the highest position-level F1 score of 41 the probability of each characters, and used two strategies to decide whether a character is correct or not. Liao et al. (2017) used the LSTM-CRF model to detect dependencies between outputs to better detect error messages. Yang et al. (2017) added more linguistic information on LSTM-CRF model such as POS, n-gram, PMI score and dependency features. Their system achieved the best F1-scores in identification level and position level on CGED2017 task. Fu et al. (2018) added richer features on BiLSTM-CRF model such as word segmentation, Gaussian ePMI, combination of POS and PMI. They also adopted a probabilistic ensemble approach to improve system performance. Their system achieved the best F1-score in identification leve"
2020.nlptea-1.5,O15-2003,0,0.0196646,"rection. We achieve the second-highest correction top1 score. Since we only provide zero or one candidate word, our correction top1 score is the same as our correction top3 score. 5 Related Work The researchers used many different methods to study the English Grammatical Error Correction task and achieved good results (Ng et al., 2014). Compared with English, the research time of Chinese grammatical error diagnosis system is short, the data sets and effective methods are lacking. Chen et al. (2013) still used n-gram as the main method, and added Web resources to improve detection performance. Lin and Chu (2015) established a scoring system using n-gram, and get better correction options. In recent years, Chinese grammatical error diagnosis has been cited as a shared task of NLPTEA CGED. Many methods are proposed to solve this task (Yu et al., 2014; Lee et al., 2015, 2016). Zheng et al. (2016) proposed a BiLSTMCRF model based on character embedding on bigram embedding. Shiue et al. (2017) combined machine learning with traditional n-gram methods, using Bi-LSTM to detect the location of errors and adding additional linguistic information, POS, ngram. Li et al. (2017) used Bi-LSTM to generate Testing R"
2020.nlptea-1.5,2021.ccl-1.108,0,0.0988146,"Missing"
2020.nlptea-1.5,W14-1701,0,0.0181731,"ompared with WA ensemble model, the stepwise selection ensemble model also achieves more than 4 point improvements. 4.5 0.4041 among all teams, there still has a wide gap for our system to solve the Chinese grammatical error diagnosis. Table 5 shows the performances on error correction. We achieve the second-highest correction top1 score. Since we only provide zero or one candidate word, our correction top1 score is the same as our correction top3 score. 5 Related Work The researchers used many different methods to study the English Grammatical Error Correction task and achieved good results (Ng et al., 2014). Compared with English, the research time of Chinese grammatical error diagnosis system is short, the data sets and effective methods are lacking. Chen et al. (2013) still used n-gram as the main method, and added Web resources to improve detection performance. Lin and Chu (2015) established a scoring system using n-gram, and get better correction options. In recent years, Chinese grammatical error diagnosis has been cited as a shared task of NLPTEA CGED. Many methods are proposed to solve this task (Yu et al., 2014; Lee et al., 2015, 2016). Zheng et al. (2016) proposed a BiLSTMCRF model base"
2020.nlptea-1.5,P17-2064,0,0.0173616,"Chinese grammatical error diagnosis system is short, the data sets and effective methods are lacking. Chen et al. (2013) still used n-gram as the main method, and added Web resources to improve detection performance. Lin and Chu (2015) established a scoring system using n-gram, and get better correction options. In recent years, Chinese grammatical error diagnosis has been cited as a shared task of NLPTEA CGED. Many methods are proposed to solve this task (Yu et al., 2014; Lee et al., 2015, 2016). Zheng et al. (2016) proposed a BiLSTMCRF model based on character embedding on bigram embedding. Shiue et al. (2017) combined machine learning with traditional n-gram methods, using Bi-LSTM to detect the location of errors and adding additional linguistic information, POS, ngram. Li et al. (2017) used Bi-LSTM to generate Testing Results Table 4 shows the performances on error detection. Our system achieves the best F1 scores at the identification level and position level. Although we achieve the highest position-level F1 score of 41 the probability of each characters, and used two strategies to decide whether a character is correct or not. Liao et al. (2017) used the LSTM-CRF model to detect dependencies be"
2020.nlptea-1.5,I17-4006,0,0.0445756,"Missing"
2021.acl-long.136,N16-1014,0,0.0183544,"ces and information inconsistency. “0” means that there are more than two incoherence errors in a session. “1” means that there are only one error. “2” means that there are no errors. Finally, we compute the average score of all the sessions. (2) Dialog engagement (Enga.) This metric measures how interesting a dialogs is. It is “1” if a dialog is interesting and the human is willing to continue the conversation, otherwise “0”. (3) Length of high-quality dialog (Length) A high-quality dialog ends if the model tends to produce dull responses or two consecutive utterances are highly overlapping (Li et al., 2016b). Single-turn Metrics. We use the following metrics: (1) Single-turn Coherence (Single.T.-Coh.) “0” if a response is inappropriate as an reply, otherwise “1”; (2) Informativeness (Info.) “0” if a response is a “safe” response, e.g. “I don’t know”, or it is highly overlapped with context, otherwise “1”; (3) Distinct (Dist.-i) It is an automatic metric for response diversity (Li et al., 2016a). 5.3 Experiment Results As shown in Table 2, GCS significantly outperforms all the baselines in terms of all the metrics except “Length-of-dialog” (sign test, p-value &lt; 0.01). It indicates that GCS can g"
2021.acl-long.136,D16-1127,0,0.0292141,"ces and information inconsistency. “0” means that there are more than two incoherence errors in a session. “1” means that there are only one error. “2” means that there are no errors. Finally, we compute the average score of all the sessions. (2) Dialog engagement (Enga.) This metric measures how interesting a dialogs is. It is “1” if a dialog is interesting and the human is willing to continue the conversation, otherwise “0”. (3) Length of high-quality dialog (Length) A high-quality dialog ends if the model tends to produce dull responses or two consecutive utterances are highly overlapping (Li et al., 2016b). Single-turn Metrics. We use the following metrics: (1) Single-turn Coherence (Single.T.-Coh.) “0” if a response is inappropriate as an reply, otherwise “1”; (2) Informativeness (Info.) “0” if a response is a “safe” response, e.g. “I don’t know”, or it is highly overlapped with context, otherwise “1”; (3) Distinct (Dist.-i) It is an automatic metric for response diversity (Li et al., 2016a). 5.3 Experiment Results As shown in Table 2, GCS significantly outperforms all the baselines in terms of all the metrics except “Length-of-dialog” (sign test, p-value &lt; 0.01). It indicates that GCS can g"
2021.acl-long.136,D19-1187,1,0.845921,"models (Chotimongkol, 2008; Ritter et al., 2010; Zhai and Williams, 2014) or variational auto-encoder (Shi et al., 2019). However, the number of their dialog states is limited to only dozens or hundreds, which cannot cover fine-grained semantics in chitchat. Moreover, our method can discover a hierarchical dialog structure, which is different from the non-hierarchical dialog structures in most previous work. 2.2 Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Moghe et al., 2018; Dinan et al., 2019; Liu et al., 2019; Xu et al., 2020c,a). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn dialog modeling. 2.3 Latent variable models for chitchat Recently, latent variables are utilized to improve diversity (Serban et al., 2017; Zhao et al., 2017; Gu et al., 2019; Gao et al., 2019; Ghandeharioun et al., 2019), control responding styles (Zhao et al., 2018; Li et al., 2020) and incorporate knowledge (Kim et al., 2020) in dialogs. Our work differs from 1727 4 ai.baidu.com/tech/nlp basic/dependency parsing Em"
2021.acl-long.136,D18-1255,0,0.0143329,"task-oriented dialogs via hidden Markov models (Chotimongkol, 2008; Ritter et al., 2010; Zhai and Williams, 2014) or variational auto-encoder (Shi et al., 2019). However, the number of their dialog states is limited to only dozens or hundreds, which cannot cover fine-grained semantics in chitchat. Moreover, our method can discover a hierarchical dialog structure, which is different from the non-hierarchical dialog structures in most previous work. 2.2 Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Moghe et al., 2018; Dinan et al., 2019; Liu et al., 2019; Xu et al., 2020c,a). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn dialog modeling. 2.3 Latent variable models for chitchat Recently, latent variables are utilized to improve diversity (Serban et al., 2017; Zhao et al., 2017; Gu et al., 2019; Gao et al., 2019; Ghandeharioun et al., 2019), control responding styles (Zhao et al., 2018; Li et al., 2020) and incorporate knowledge (Kim et al., 2020) in dialogs. Our work differs from 1727 4 ai.baidu.co"
2021.acl-long.136,P02-1040,0,0.110099,"evaluate the quality of the initialized graph (denoted as Phrase Graph) that consists of only phrases (as vertices) and initial edges (between phrases) in Section 3.2. For more details, please refer to Appendix A.1. 4.2 Evaluation Metrics We evaluate discovered dialog structure graph with both automatic evaluation and human evaluation. For automatic evaluation, we use two metrics to evaluate the performance of reconstruction: (1) NLL is the negative log likelihood of dialog utterances; (2) BLEU-1/2 measures how much that reconstructed sentences contains 1/2-gram overlaps with input sentences (Papineni et al., 2002). The two metrics indicate how well the learned dialog structure graph can capture important semantic information in dialog dataset. Further, we manually evaluate the quality of edges and vertices in the graph. For edges, (1) S-U Appr. for multi-turn dialog coherence. It measures the appropriateness of Sess-Utter edges, where these edges provide crucial prior information to ensure multi-turn dialog coherence (see results in Section 5.4). “1” if an utterance-level vertex is relevant to its session-level vertex (topic), otherwise “0”. (2) U-U Appr. for single-turn dialog coherence: It measures t"
2021.acl-long.136,N10-1020,0,0.0678486,". (2) we propose a novel model, DVAE-GNN, for hierarchical dialog struc3 Co-occurrence means that two utterance-level vertices are mapped by two adjacent utterances in a session. 7 预定了酒店 Have booked a hotel room 找到房子了么 Have you found a place to live? Speak 1: 嗯，我 提前订好了酒店。 [Yes, I have booked a hotel in advance.] Response Utterance-level semantic vertex Related Work 2.1 Utterance-level semantic vertex 6 Dialog structure learning for task-oriented dialogs There are previous work on discovering humanreadable dialog structure for task-oriented dialogs via hidden Markov models (Chotimongkol, 2008; Ritter et al., 2010; Zhai and Williams, 2014) or variational auto-encoder (Shi et al., 2019). However, the number of their dialog states is limited to only dozens or hundreds, which cannot cover fine-grained semantics in chitchat. Moreover, our method can discover a hierarchical dialog structure, which is different from the non-hierarchical dialog structures in most previous work. 2.2 Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Moghe et al., 2018; Dinan et al., 2019; Liu et al., 2019; Xu et al., 2020c,a). In this"
2021.acl-long.136,2020.emnlp-main.378,0,0.027344,"e growing interests in leveraging knowledge bases for generation of more informative responses (Moghe et al., 2018; Dinan et al., 2019; Liu et al., 2019; Xu et al., 2020c,a). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn dialog modeling. 2.3 Latent variable models for chitchat Recently, latent variables are utilized to improve diversity (Serban et al., 2017; Zhao et al., 2017; Gu et al., 2019; Gao et al., 2019; Ghandeharioun et al., 2019), control responding styles (Zhao et al., 2018; Li et al., 2020) and incorporate knowledge (Kim et al., 2020) in dialogs. Our work differs from 1727 4 ai.baidu.com/tech/nlp basic/dependency parsing Embedding space of session-level semantic vertices … Embedding space of utterance-level semantic vertices … Vectors of session-level semantic vertices Vectors of utterance-level semantic vertices Vectors of utterance phrases 放假过来找我玩啊 [Let‘s gather on holiday] 好啊，好久没见面了 [Yep, long time no see] 我明天准备去长沙上班 [I’ll go to Changsha tomorrow] 1 放假过来找我玩啊 [Let‘s gather on holiday] 1 3 2 RNN Encoder FFN 1 GNN 2 5 1 你租房子了么 [Oh, have you rent a room yet?] 2 3 4 Emb RNN Decode"
2021.acl-long.136,P15-1152,0,0.0107824,"hierarchical latent dialog states (at the level of both session and utterance) and their transitions from corpus as a dialog structure graph. Then we leverage it as background knowledge to facilitate dialog management in a RL based dialog system. Experimental results on two benchmark corpora confirm that DVAE-GNN can discover meaningful dialog structure graph, and the use of dialog structure as background knowledge can significantly improve multi-turn coherence. 1 Introduction With the aim of building a machine to converse with humans naturally, some work investigate neural generative models (Shang et al., 2015; Serban et al., 2017). While these models can generate locally relevant dialogs, they struggle to organize individual utterances into globally coherent flow (Yu et al., 2016; Xu et al., 2020b). The possible reason is that it is difficult to control the overall dialog flow without background knowledge about dialog structure.1 However, due to the complexity of opendomain conversation, it is laborious and costly to annotate dialog structure manually. Therefore, it is ∗ Equal contribution. Corresponding author: Wanxiang Che. 1 Dialog structure means dialog states and their transitions. of great i"
2021.acl-long.136,N19-1178,0,0.326662,"The possible reason is that it is difficult to control the overall dialog flow without background knowledge about dialog structure.1 However, due to the complexity of opendomain conversation, it is laborious and costly to annotate dialog structure manually. Therefore, it is ∗ Equal contribution. Corresponding author: Wanxiang Che. 1 Dialog structure means dialog states and their transitions. of great importance to discover open-domain dialog structure from corpus in an unsupervised way for coherent dialog generation. Some studies tried to discover dialog structure from task-oriented dialogs (Shi et al., 2019). However, the number of their dialog states is limited to only dozens or hundreds, which cannot cover finegrained semantics in open-domain dialogs. Furthermore, the dialog structures they discovered generally only contain utterance-level semantics (non-hierarchical), without session-level semantics (chatting topics) that are essential in open-domain dialogs (Wu et al., 2019; Kang et al., 2019; Xu et al., 2020c).2 Thus, in order to provide a full picture of open-domain dialog structure, it is desirable to discover a two-layer directed graph that contains session-level semantics in the upper-la"
2021.acl-long.136,P19-1371,0,0.0113493,"in Appendix B. 5 Experiments for Graph Grounded Dialog Generation To confirm the benefits of discovered dialog structure graph for coherent conversation generation, we conduct experiments on the graph discovered from Weibo corpus. All the systems (including baselines) are trained on Weibo corpus. 5.1 Models We carefully select the following six baselines. MMPMS It is the multi-mapping based neural open-domain conversational model with posterior mapping selection mechanism (Chen et al., 2019), which is a SOTA model on the Weibo Corpus. MemGM It is the memory-augmented opendomain dialog model (Tian et al., 2019), which learns to cluster U-R pairs for response generation. HRED It is the hierarchical recurrent encoderdecoder model (Serban et al., 2016). CVAE It is the Conditional Variational AutoEncoder based neural open-domain conversational model (Zhao et al., 2017). VHCR-EI This variational hierarchical RNN model can learn hierarchical latent variables from open-domain dialogs (Ghandeharioun et al., 2019). It is a SOTA dialog model with hierarchical VAE. DVRNN-RL It discovers dialog structure graph for task-oriented dialog modeling (Shi et al., 2019). GCS It is our proposed dialog structure graph gr"
2021.acl-long.136,P19-1369,1,0.807053,"eir transitions. of great importance to discover open-domain dialog structure from corpus in an unsupervised way for coherent dialog generation. Some studies tried to discover dialog structure from task-oriented dialogs (Shi et al., 2019). However, the number of their dialog states is limited to only dozens or hundreds, which cannot cover finegrained semantics in open-domain dialogs. Furthermore, the dialog structures they discovered generally only contain utterance-level semantics (non-hierarchical), without session-level semantics (chatting topics) that are essential in open-domain dialogs (Wu et al., 2019; Kang et al., 2019; Xu et al., 2020c).2 Thus, in order to provide a full picture of open-domain dialog structure, it is desirable to discover a two-layer directed graph that contains session-level semantics in the upper-layer vertices, utterance-level semantics in the lower-layer vertices, and edges among these vertices. In this paper, we propose a novel discrete variational auto-encoder with graph neural network (DVAE-GNN) to discover a two-layer dialog structure from chitchat corpus. Intuitively, since discrete dialog states are easier to capture transitions for dialog coherence, we use dis"
2021.acl-long.15,W17-7607,0,0.0673819,"Missing"
2021.acl-long.15,P19-1544,0,0.0346958,"Missing"
2021.acl-long.15,N18-2118,0,0.402918,"essive model generates outputs word by word from left-to-right direction. The gray color denotes the unseen information when model decodes for the word Denver. (b) Non-autoregressive model can produce outputs in parallel. AN denotes airport name. Introduction Spoken Language Understanding (SLU) (Young et al., 2013) is a critical component in spoken dialog systems, which aims to understand user’s queries. It typically includes two sub-tasks: intent detection and slot filling (Tur and De Mori, 2011). Since intents and slots are closely tied, dominant single-intent SLU systems in the literature (Goo et al., 2018; Li et al., 2018; Liu et al., 2019b; E et al., 2019; Qin et al., 2019; Teng et al., 2021; Qin et al., 2021b,c) adopt joint models to consider the correlation between the two tasks, which have obtained remarkable success. Multi-intent SLU means that the system can handle an utterance containing multiple intents, which is shown to be more practical in the real-world scenario, attracting increasing attention. To this end, ∗ O Xu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multi-intent SLU. However, their models only consider the multiple intent detection while ignoring slot fi"
2021.acl-long.15,H90-1021,0,0.262393,"Missing"
2021.acl-long.15,P82-1020,0,0.788566,"Missing"
2021.acl-long.15,D19-1549,0,0.0957343,"ly, multiple intent detection can handle utterances with multiple intents, which has attracted increasing attention. To the end, Xu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multiple intent detection. Gangadharaiah and Narayanaswamy (2019) first apply a multi-task framework with a slot-gate mechanism to jointly model the multiple intent detection and slot fill185 Graph Neural Network for NLP Graph neural networks that operate directly on graph structures to model the structural information, which has been applied successfully in various NLP tasks. Linmei et al. (2019) and Huang and Carley (2019) explore graph attention network (GAT) (Veliˇckovi´c et al., 2018) for classification task to incorporate the dependency parser information. Cetoli et al. (2017) and Liu et al. (2019a) apply graph neural network to model the non-local contextual information for sequence labeling tasks. Yasunaga et al. (2017) and Feng et al. (2020a) successfully apply a graph network to model the discourse information for the summarization generation task, which achieved promising performance. Graph structure are successfully applied for dialogue direction (Feng et al., 2020b; Fu et al., 2020; Qin et al., 2020a"
2021.acl-long.15,D18-1417,0,0.105296,"ates outputs word by word from left-to-right direction. The gray color denotes the unseen information when model decodes for the word Denver. (b) Non-autoregressive model can produce outputs in parallel. AN denotes airport name. Introduction Spoken Language Understanding (SLU) (Young et al., 2013) is a critical component in spoken dialog systems, which aims to understand user’s queries. It typically includes two sub-tasks: intent detection and slot filling (Tur and De Mori, 2011). Since intents and slots are closely tied, dominant single-intent SLU systems in the literature (Goo et al., 2018; Li et al., 2018; Liu et al., 2019b; E et al., 2019; Qin et al., 2019; Teng et al., 2021; Qin et al., 2021b,c) adopt joint models to consider the correlation between the two tasks, which have obtained remarkable success. Multi-intent SLU means that the system can handle an utterance containing multiple intents, which is shown to be more practical in the real-world scenario, attracting increasing attention. To this end, ∗ O Xu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multi-intent SLU. However, their models only consider the multiple intent detection while ignoring slot filling task. Recen"
2021.acl-long.15,2020.acl-main.574,0,0.0142879,"ent detection decoder (§3.2) and a global-local graphinteraction graph decoder for slot filling (§3.3). Both intent detection and slot filling are optimized simultaneously via a joint learning scheme. 3.1 Self-attentive Encoder Following Qin et al. (2019), we utilize a selfattentive encoder with BiLSTM and self-attention mechanism to obtain the shared utterance representation, which can incorporate temporal features within word orders and contextual information. BiLSTM The bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) have been successfully applied to sequence labeling tasks (Li et al., 2020, 2021). We adopt BiLSTM to read the input sequence {x1 , x2 , . . . , xn } forwardly and backwardly to produce contextsensitive hidden states H = {h1 , h2 , . . . , hn }, by repeatedly applying the hi = BiLSTM (φemb (xi ), hi−1 , hi+1 ), where φemb is embedding function. Self-Attention Following Vaswani et al. (2017), we map the matrix of input vectors X ∈ Rn×d (d represents the mapped dimension) to queries Q, keys K and values V matrices by using different linear projections. Then, the self-attention output C ∈ Rn×d is a weighted sum of values: 179  C = softmax QK &gt; √ dk  V. (1) Intent Det"
2021.acl-long.15,D19-1488,0,0.028307,"ent scenario. More recently, multiple intent detection can handle utterances with multiple intents, which has attracted increasing attention. To the end, Xu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multiple intent detection. Gangadharaiah and Narayanaswamy (2019) first apply a multi-task framework with a slot-gate mechanism to jointly model the multiple intent detection and slot fill185 Graph Neural Network for NLP Graph neural networks that operate directly on graph structures to model the structural information, which has been applied successfully in various NLP tasks. Linmei et al. (2019) and Huang and Carley (2019) explore graph attention network (GAT) (Veliˇckovi´c et al., 2018) for classification task to incorporate the dependency parser information. Cetoli et al. (2017) and Liu et al. (2019a) apply graph neural network to model the non-local contextual information for sequence labeling tasks. Yasunaga et al. (2017) and Feng et al. (2020a) successfully apply a graph network to model the discourse information for the summarization generation task, which achieved promising performance. Graph structure are successfully applied for dialogue direction (Feng et al., 2020b; Fu et"
2021.acl-long.15,2020.findings-emnlp.163,1,0.728742,"which have obtained remarkable success. Multi-intent SLU means that the system can handle an utterance containing multiple intents, which is shown to be more practical in the real-world scenario, attracting increasing attention. To this end, ∗ O Xu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multi-intent SLU. However, their models only consider the multiple intent detection while ignoring slot filling task. Recently, Gangadharaiah and Narayanaswamy (2019) make the first attempt to propose a multi-task framework to joint model the multiple intent detection and slot filling. Qin et al. (2020b) further propose an adaptive interaction framework (AGIF) to achieve fine-grained multi-intent information integration for slot filling, obtaining state-of-the-art performance. Though achieving the promising performance, the existing multi-intent SLU joint models heavily rely on an autoregressive fashion, as shown in Figure 1(a), leading to two issues: Corresponding author. • Slow inference speed. The autoregressive models make the generation of slot outputs must be done through the left-to-right pass, which cannot achieve parallelizable, leading to slow inference speed. 178 Proceedings of t"
2021.acl-long.15,D19-1097,0,0.273219,"by word from left-to-right direction. The gray color denotes the unseen information when model decodes for the word Denver. (b) Non-autoregressive model can produce outputs in parallel. AN denotes airport name. Introduction Spoken Language Understanding (SLU) (Young et al., 2013) is a critical component in spoken dialog systems, which aims to understand user’s queries. It typically includes two sub-tasks: intent detection and slot filling (Tur and De Mori, 2011). Since intents and slots are closely tied, dominant single-intent SLU systems in the literature (Goo et al., 2018; Li et al., 2018; Liu et al., 2019b; E et al., 2019; Qin et al., 2019; Teng et al., 2021; Qin et al., 2021b,c) adopt joint models to consider the correlation between the two tasks, which have obtained remarkable success. Multi-intent SLU means that the system can handle an utterance containing multiple intents, which is shown to be more practical in the real-world scenario, attracting increasing attention. To this end, ∗ O Xu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multi-intent SLU. However, their models only consider the multiple intent detection while ignoring slot filling task. Recently, Gangadharaiah"
2021.acl-long.15,2021.ccl-1.108,0,0.0934371,"Missing"
2021.acl-long.15,D19-1214,1,0.533383,"on. The gray color denotes the unseen information when model decodes for the word Denver. (b) Non-autoregressive model can produce outputs in parallel. AN denotes airport name. Introduction Spoken Language Understanding (SLU) (Young et al., 2013) is a critical component in spoken dialog systems, which aims to understand user’s queries. It typically includes two sub-tasks: intent detection and slot filling (Tur and De Mori, 2011). Since intents and slots are closely tied, dominant single-intent SLU systems in the literature (Goo et al., 2018; Li et al., 2018; Liu et al., 2019b; E et al., 2019; Qin et al., 2019; Teng et al., 2021; Qin et al., 2021b,c) adopt joint models to consider the correlation between the two tasks, which have obtained remarkable success. Multi-intent SLU means that the system can handle an utterance containing multiple intents, which is shown to be more practical in the real-world scenario, attracting increasing attention. To this end, ∗ O Xu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multi-intent SLU. However, their models only consider the multiple intent detection while ignoring slot filling task. Recently, Gangadharaiah and Narayanaswamy (2019) make the"
2021.acl-long.15,N18-2050,0,0.0272234,"milarly, the slot filling task objective is: L2 , − NS n X X (j,S) yˆi   (j,S) log yi , (15) i=1 j=1 where NI is the number of single intent labels and NS is the number of slot labels. The final joint objective is formulated as: L = αL1 + βL2 , Baselines We compare our model with the following best baselines: (1) Attention BiRNN. Liu and Lane (2016) propose an alignment-based RNN for joint slot filling and intent detection; (2) Slot-Gated Atten. Goo et al. (2018) propose a slot-gated joint model, explicitly considering the correlation between slot filling and intent detection; (3) Bi-Model. Wang et al. (2018) propose the Bi-model to model the bi-directional between the intent detection and slot filling; (4) SF-ID Network. E et al. (2019) proposes the SF-ID network to establish a direct connection between the two tasks; (5) Stack-Propagation. Qin et al. (2019) adopt a stack-propagation framework to explicitly incorporate intent detection for guiding slot filling; (6) Joint Multiple ID-SF. Gangadharaiah and Narayanaswamy (2019) propose a multi-task framework with slot-gated mechanism for multiple intent detection and slot filling; (7) AGIF Qin et al. (2020b) proposes an adaptive interaction network"
2021.acl-long.15,2020.emnlp-main.152,0,0.06274,"Missing"
2021.acl-long.15,D18-1348,0,0.0221418,"ed multiple intent information integration for token-level slot filling, achieving the state-of-the-art performance. Their models adopt the autoregressive architecture for joint multiple intent detection and slot filling. In contrast, we propose a non-autoregressive approach, achieving parallel decoding. To the best of our knowledge, we are the first to explore a non-autoregressive architecture for multiple intent detection and slot filling. Related Work Slot Filling and Intent Detection Recently, joint models (Zhang and Wang, 2016; HakkaniT¨ur et al., 2016; Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019b; Qin et al., 2019; Zhang et al., 2019; Wu et al., 2020; Qin et al., 2021b; Ni et al., 2021) are proposed to consider the strong correlation between intent detection and slot filling have obtained remarkable success. Compared with their work, we focus on jointly modeling multiple intent detection and slot filling while they only consider the single-intent scenario. More recently, multiple intent detection can handle utterances with multiple intents, which has attracted increasing attention. To the end, Xu and Sarikaya (2013) and Kim et al. (2017) begin to exp"
2021.acl-long.15,K17-1045,0,0.0281906,"te mechanism to jointly model the multiple intent detection and slot fill185 Graph Neural Network for NLP Graph neural networks that operate directly on graph structures to model the structural information, which has been applied successfully in various NLP tasks. Linmei et al. (2019) and Huang and Carley (2019) explore graph attention network (GAT) (Veliˇckovi´c et al., 2018) for classification task to incorporate the dependency parser information. Cetoli et al. (2017) and Liu et al. (2019a) apply graph neural network to model the non-local contextual information for sequence labeling tasks. Yasunaga et al. (2017) and Feng et al. (2020a) successfully apply a graph network to model the discourse information for the summarization generation task, which achieved promising performance. Graph structure are successfully applied for dialogue direction (Feng et al., 2020b; Fu et al., 2020; Qin et al., 2020a, 2021a). In our work, we apply a global-locally graph interaction network to model the slot dependency and interaction between the multiple intents and slots. 6 Conclusion In this paper, we investigated a non-autoregressive model for joint multiple intent detection and slot filling. To this end, we proposed"
2021.acl-long.15,P19-1519,0,0.0181884,", achieving the state-of-the-art performance. Their models adopt the autoregressive architecture for joint multiple intent detection and slot filling. In contrast, we propose a non-autoregressive approach, achieving parallel decoding. To the best of our knowledge, we are the first to explore a non-autoregressive architecture for multiple intent detection and slot filling. Related Work Slot Filling and Intent Detection Recently, joint models (Zhang and Wang, 2016; HakkaniT¨ur et al., 2016; Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019b; Qin et al., 2019; Zhang et al., 2019; Wu et al., 2020; Qin et al., 2021b; Ni et al., 2021) are proposed to consider the strong correlation between intent detection and slot filling have obtained remarkable success. Compared with their work, we focus on jointly modeling multiple intent detection and slot filling while they only consider the single-intent scenario. More recently, multiple intent detection can handle utterances with multiple intents, which has attracted increasing attention. To the end, Xu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multiple intent detection. Gangadharaiah and Narayanaswamy (2019"
2021.acl-long.201,2020.acl-main.721,0,0.0204073,"dge of one document type cannot be easily transferred into another, so that these models often need to be re-trained once the document type is changed. Thereby the local invariance in general document layout (key-value pairs in a left-right layout, tables in a grid layout, etc.) cannot be fully exploited. To this end, the second direction relies on the deep fusion among textual, visual, and layout information from a great number of unlabeled documents in different domains, where pre-training techniques play an important role in learning the cross-modality interaction in an end-to-end fashion (Lockard et al., 2020; Xu et al., 2020). In this way, the pre-trained 2579 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2579–2591 August 1–6, 2021. ©2021 Association for Computational Linguistics models absorb cross-modal knowledge from different document types, where the local invariance among these layouts and styles is preserved. Furthermore, when the model needs to be transferred into another domain with different document formats, only a few labeled samples would be sufficient to fine-t"
2021.acl-long.201,N18-2074,0,0.0284803,"his paper follows the second direction, and we explore how to further improve the pre-training strategies for the VrDU tasks. In this paper, we present an improved version of LayoutLM (Xu et al., 2020), aka LayoutLMv2. Different from the vanilla LayoutLM model where visual embeddings are combined in the fine-tuning stage, we integrate the visual information in the pre-training stage in LayoutLMv2 by taking advantage of the Transformer architecture to learn the cross-modality interaction between visual and textual information. In addition, inspired by the 1-D relative position representations (Shaw et al., 2018; Raffel et al., 2020; Bao et al., 2020), we propose the spatial-aware self-attention mechanism for LayoutLMv2, which involves a 2-D relative position representation for token pairs. Different from the absolute 2-D position embeddings that LayoutLM uses to model the page layout, the relative position embeddings explicitly provide a broader view for the contextual spatial modeling. For the pre-training strategies, we use two new training objectives for LayoutLMv2 in addition to the masked visual-language modeling. The first is the proposed text-image alignment strategy, which aligns the text li"
2021.acl-long.201,2020.acl-main.580,0,0.256687,"y the style and format of each type as well as the document content. Therefore, to accurately recognize the text fields of interest, it is inevitable to take advantage of the cross-modality nature of visually-rich documents, where the textual, visual, and layout information should be jointly modeled and learned end-to-end in a single framework. The recent progress of VrDU lies primarily in two directions. The first direction is usually built on the shallow fusion between textual and visual/layout/style information (Yang et al., 2017; Liu et al., 2019; Sarkhel and Nandi, 2019; Yu et al., 2020; Majumder et al., 2020; Wei et al., 2020; Zhang et al., 2020). These approaches leverage the pre-trained NLP and CV models individually and combine the information from multiple modalities for supervised learning. Although good performance has been achieved, the domain knowledge of one document type cannot be easily transferred into another, so that these models often need to be re-trained once the document type is changed. Thereby the local invariance in general document layout (key-value pairs in a left-right layout, tables in a grid layout, etc.) cannot be fully exploited. To this end, the second direction relie"
2021.acl-long.201,D16-1264,0,0.127266,"Missing"
2021.acl-long.201,D19-1514,0,0.0179133,"ition representation for token pairs. Different from the absolute 2-D position embeddings that LayoutLM uses to model the page layout, the relative position embeddings explicitly provide a broader view for the contextual spatial modeling. For the pre-training strategies, we use two new training objectives for LayoutLMv2 in addition to the masked visual-language modeling. The first is the proposed text-image alignment strategy, which aligns the text lines and the corresponding image regions. The second is the text-image matching strategy popular in previous vision-language pre-training models (Tan and Bansal, 2019; Lu et al., 2019; Su et al., 2020; Chen et al., 2020; Sun et al., 2019), where the model learns whether the document image and textual content are correlated. We select six publicly available benchmark datasets as the downstream tasks to evaluate the performance of the pre-trained LayoutLMv2 model, which are the FUNSD dataset (Jaume et al., 2019) for form understanding, the CORD dataset (Park et al., 2019) and the SROIE dataset (Huang et al., 2019) for receipt understanding, the Kleister-NDA dataset (Grali´nski et al., 2020) for long document understanding with a complex layout, the RVL-CDIP"
2021.acl-long.264,2020.acl-main.421,0,0.0139564,"ge of example consistency R1 . Although word alignment is a possible solution to map the predicted label distributions between translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on different target languages; (2) translate-train-all: translationbased augmentation is available, a"
2021.acl-long.264,2020.tacl-1.30,0,0.0152671,"ible solution to map the predicted label distributions between translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on different target languages; (2) translate-train-all: translationbased augmentation is available, and the models are fine-tuned on the concatenation of English t"
2021.acl-long.264,2020.acl-main.747,0,0.0682084,"Missing"
2021.acl-long.264,D18-1269,0,0.0258158,"do not change nword . Thus the three data augmentation strategies will not affect the usage of example consistency R1 . Although word alignment is a possible solution to map the predicted label distributions between translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on differen"
2021.acl-long.264,2020.acl-main.536,0,0.184978,"arization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation. In addition, we employ model consistency to regularize the models trained with two augmented versions of the same training set. Experimental results on the XTREME benchmark show that our method1 significantly improves crosslingual fine-tuning across various tasks, including text classification, question answering, and sequence labeling. 1 Introduction Pre-trained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Chi et al., 2020) have shown great transferability across languages. By fine-tuning on labeled data in a source language, the models can generalize to other target languages, even without any additional training. Such generalization ability reduces the required annotation efforts, which is prohibitively expensive for low-resource languages. Recent work has demonstrated that data augmentation is helpful for cross-lingual transfer, e.g., translating source language training data into target languages (Singh et al., 2019), and generating codeswitch data by randomly replacing input words in the"
2021.acl-long.264,N19-1423,0,0.032175,"Missing"
2021.acl-long.264,E14-1049,0,0.0225658,"ary to the first strand. We focus on the cross-lingual setting, where consistency regularization has not been fully explored. • We study four types of data augmentations that can be easily plugged into cross-lingual fine-tuning. • We give instructions on how to apply X T UNE to various downstream tasks, such as classification, span extraction, and sequence labeling. • We conduct extensive experiments to show that X T UNE consistently improves the performance of cross-lingual fine-tuning. 2 Related Work Cross-Lingual Transfer Besides learning crosslingual word embeddings (Mikolov et al., 2013; Faruqui and Dyer, 2014; Guo et al., 2015; Xu et al., 2018; Wang et al., 2019), most recent work of cross-lingual transfer is based on pre-trained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Chi et al., 2020). These models generate multilingual contextualized word representations for different languages with a shared encoder and show promising cross-lingual transferability. Cross-Lingual Data Augmentation Machine translation has been successfully applied to the cross-lingual scenario as data augmentation. A common way to use machine translation is to finetune models on both source"
2021.acl-long.264,2020.acl-main.627,0,0.19623,"s on seven datasets. Experimental results show that our method outperforms conventional fine-tuning with data augmentation. We also demonstrate that X T UNE is flexible to be plugged in various 3403 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3403–3417 August 1–6, 2021. ©2021 Association for Computational Linguistics tasks, such as classification, span extraction, and sequence labeling. We summarize our contributions as follows: the target language. Besides, Qin et al. (2020) finetuned models on multilingual code-switch data, which achieves considerable improvements. • We propose X T UNE, a cross-lingual finetuning method to better utilize data augmentations based on consistency regularization. Consistency Regularization One strand of work in consistency regularization focused on regularizing model predictions to be invariant to small perturbations on image data. The small perturbations can be random noise (Zheng et al., 2016), adversarial noise (Miyato et al., 2019; Carmon et al., 2019) and various data augmentation approaches (Hu et al., 2017; Ye et al., 2019; X"
2021.acl-long.264,P15-1119,1,0.817926,"We focus on the cross-lingual setting, where consistency regularization has not been fully explored. • We study four types of data augmentations that can be easily plugged into cross-lingual fine-tuning. • We give instructions on how to apply X T UNE to various downstream tasks, such as classification, span extraction, and sequence labeling. • We conduct extensive experiments to show that X T UNE consistently improves the performance of cross-lingual fine-tuning. 2 Related Work Cross-Lingual Transfer Besides learning crosslingual word embeddings (Mikolov et al., 2013; Faruqui and Dyer, 2014; Guo et al., 2015; Xu et al., 2018; Wang et al., 2019), most recent work of cross-lingual transfer is based on pre-trained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Chi et al., 2020). These models generate multilingual contextualized word representations for different languages with a shared encoder and show promising cross-lingual transferability. Cross-Lingual Data Augmentation Machine translation has been successfully applied to the cross-lingual scenario as data augmentation. A common way to use machine translation is to finetune models on both source language training"
2021.acl-long.264,2020.acl-main.197,0,0.0945311,"Missing"
2021.acl-long.264,P17-1178,0,0.028767,"translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on different target languages; (2) translate-train-all: translationbased augmentation is available, and the models are fine-tuned on the concatenation of English training data and its translated data on all target languages"
2021.acl-long.264,2020.acl-main.653,0,0.0247876,". Although word alignment is a possible solution to map the predicted label distributions between translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on different target languages; (2) translate-train-all: translationbased augmentation is available, and the models are fine-tune"
2021.acl-long.264,2020.acl-main.170,0,0.0340809,"(DA , θ, θ∗ ) where λ1 and λ2 are the corresponding weights of two regularization methods. Notice that the data augmentation strategies A, A0 , and A∗ can be either different or the same, which are tuned as hyper-parameters. 3.2 Gaussian Noise Data Augmentation We consider four types of data augmentation strategies in this work, which are shown in Figure 2. We aim to study the impact of different data augmentation strategies on cross-lingual transferability. 3.2.1 Subword Sampling Representing a sentence in different subword sequences can be viewed as a data augmentation strategy (Kudo, 2018; Provilkov et al., 2020). We utilize XLM-R (Conneau et al., 2020a) as our pre-trained Anchor points have been shown useful to improve cross-lingual transferability. Conneau et al. (2020b) analyzed the impact of anchor points in pre-training cross-lingual language models. Following Qin et al. (2020), we generate code-switch data in multiple languages as data augmentation. We randomly select words in the original text in the source language and replace them with target language words in the bilingual dictionaries to obtain code-switch data. Intuitively, this type of data augmentation explicitly helps pre-trained cross-"
2021.acl-long.264,D19-1575,1,0.837666,"ing, where consistency regularization has not been fully explored. • We study four types of data augmentations that can be easily plugged into cross-lingual fine-tuning. • We give instructions on how to apply X T UNE to various downstream tasks, such as classification, span extraction, and sequence labeling. • We conduct extensive experiments to show that X T UNE consistently improves the performance of cross-lingual fine-tuning. 2 Related Work Cross-Lingual Transfer Besides learning crosslingual word embeddings (Mikolov et al., 2013; Faruqui and Dyer, 2014; Guo et al., 2015; Xu et al., 2018; Wang et al., 2019), most recent work of cross-lingual transfer is based on pre-trained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Chi et al., 2020). These models generate multilingual contextualized word representations for different languages with a shared encoder and show promising cross-lingual transferability. Cross-Lingual Data Augmentation Machine translation has been successfully applied to the cross-lingual scenario as data augmentation. A common way to use machine translation is to finetune models on both source language training data and translated data in all targ"
2021.emnlp-demo.6,N19-4010,0,0.0636961,"Missing"
2021.emnlp-demo.6,2020.findings-emnlp.58,1,0.843454,"the need for any knowledge. The following code snippet in Figure 4 shows 45 Chinese Word Segmentation F Part-of-Speech Tagging FLAS Named Entity Recognition F Dependency Parsing F Stanza (Qi et al., 2020) 92.40 98.10 89.50 84.98 - - N-LTP trained separately N-LTP trained jointly with distillation 98.55 99.18 98.35 98.69 95.41 95.97 90.12 90.19 74.47 76.62 79.23 79.49 Model Semantic Dependency Parsing F Semantic Role Labeling F Table 2: Main Results. “-"" represents the absence of tasks in the Stanza toolkit and we cannot report the results. The N-LTP model is based on the Chinese ELECTRA base (Cui et al., 2020). The learning ratio (lr) for teacher models, student model and CRF layer is {1e − 4}, {1e − 4}, {1e − 3}, respectively. The gradient clip value adopted in our experiment is 1.0 and the warmup proportion is 0.02. We use BertAdam (Devlin et al., 2019) to optimize the parameters and adopted the suggested hyper-parameters for optimization. 4.2 Results We compare N-LTP with the state-of-the-art toolkit Stanza. For a fair comparison, we conduct experiments on the same datasets that Stanza adopted. The results are shown in Table 2, we have the following observations: • N-LTP outperforms Stanza on fo"
2021.emnlp-demo.6,N19-1423,0,0.127433,"lated tasks, which has obtained remarkable success on various NLP tasks (Qin et al., 2019; Wang et al., 2020; Zhou et al., 2021). Inspired by this, we adopt the SOTA pre-trained model (ELECTRA) (Clark et al., 2020) as the shared encoder to capture shared knowledge across six Chinese tasks. Given an input utterance s = (s1 , s2 , . . . , sn ), we first construct the input sequence by adding specific tokens s = ([CLS], s1 , s2 , . . . , sn , [SEP]), where [CLS] is the special symbol for representing the whole sequence, and [SEP] is the special symbol to separate non-consecutive token sequences (Devlin et al., 2019). ELECTRA takes the constructed input and output the corresponding hidden representations of sequence H = (h[CLS] , h1 , h2 , . . . , hn , h[SEP] ). 2.2 hˆi = AdaptedTransformer(hi ), (3) ˆ [CLS] , h ˆ 1, h ˆ 2, . . . , h ˆ n, h ˆ [SEP] ) are ˆ = (h where H the updated representations. Finally, similar to CWS and POS, we use a linear decoder to classify label for each word: yi = Softmax(W NER hˆi + bNER ), (4) where yi denotes the NER label probability distribution of each character. Chinese word segmentation (CWS) is a preliminary and important task for Chinese natural language processing (NL"
2021.emnlp-demo.6,C18-1233,0,0.0601213,"Missing"
2021.emnlp-demo.6,2020.findings-emnlp.425,0,0.0420593,"Missing"
2021.emnlp-demo.6,C10-3004,1,0.702421,"Missing"
2021.emnlp-demo.6,C96-1058,0,0.462606,"bution of each character. Chinese word segmentation (CWS) is a preliminary and important task for Chinese natural language processing (NLP). In N-LTP, following Xue (2003), CWS is regarded as a character based sequence labeling problem. Specifically, given the hidden representations H = (h[CLS] , h1 , h2 , . . . , hn , h[SEP] ), we adopt a linear decoder to classify each character: 2.5 Dependency Parsing Dependency parsing is the task to analyze the semantic structure of a sentence. In N-LTP, we implement a deep biaffine neural dependency parser (Dozat and Manning, 2017) and einser algorithm (Eisner, 1996) to obtain the parsing result, which is formulated as: (1) where yi denotes the label probability distribution of each character; W CWS and bCWS are trainable parameters. 2.3 Named Entity Recognition The named entity recognition (NER) is the task of finding the start and end of an entity (people, locations, organizations, etc.) in a sentence and assigning a class for this entity. Traditional, NER is regarded as a sequence labeling task. After obtaining the hidden representations H, we follow Yan et al. (2019a) to adopt the Adapted-Transformer to consider directionand distance-aware characteris"
2021.emnlp-demo.6,S12-1050,1,0.760309,"ithm. More specifically, instead of simply shuffling the datasets for our multi-task models, we follow the task sampling procedure from Bowman et al. (2018), where the probability of training on an example for a particular task τ is proportional to |Dτ |0.75 . This ensures that tasks with large datasets don’t overly dominate the training. The above process is also used for scoring a l labeled dependency i↶ j, by extending the 1-dim vector s into L dims, where L is the total number of dependency labels. 2.6 Semantic Dependency Parsing Similar to dependency parsing, semantic dependency parsing (Che et al., 2012, SDP) is a task to capture the semantic structure of a sentence. Specifically, given an input sentence, SDP aims at determining all the word pairs related to each other semantically and assigning specific predefined semantic relations. Following Dozat and Manning (2017), we adopt a biaffine module to perform the task, using pi↶ j = sigmoid(yi↶ j ). Knowledge Distillation (7) If pi↶ j &gt; 0.5, wordi to wordj exists an edge. 2.7 3 Semantic Role Labeling N-LTP is a PyTorch-based Chinese NLP toolkit based on the above model. All the configurations can be initialized from JSON files, and thus it is"
2021.emnlp-demo.6,I17-2014,0,0.0229729,"rocess NLP tasks in Chinese quickly. Recently, Qi et al. (2020) introduce the Python NLP toolkit Stanza for multi-lingual languages, including Chinese language. Though Stanza can be directly applied for processing the Chinese texts, it suffers from several limitations. First, it only supports part of Chinese NLP tasks. For example, it fails to handle semantic parsing analysis, resulting in incomplete analysis in Chinese NLP. Second, it trained each task separately, ignoring the shared knowledge across the related tasks, which has been proven effective for Chinese NLP tasks (Qian et al., 2015; Hsieh et al., 2017; Chang et al., 2018). Third, independent modeling method will occupy more Introduction There is a wide of range of existing natural language processing (NLP) toolkits such as CoreNLP (Manning et al., 2014), UDPipe (Straka and Straková, 2017), FLAIR (Akbik et al., 2019), spaCy,1 and Stanza (Qi et al., 2020) in English, which makes it easier for users to build tools with sophisticated linguistic processing. Recently, the need for Chinese NLP has a dramatic increase in many downstream applications. A Chinese NLP platform usually includes lexical analysis (Chinese word segmentation (CWS), part-of"
2021.emnlp-demo.6,P19-1595,0,0.340222,"ology platform supporting six fundamental Chinese NLP tasks: lexical analysis (Chinese word segmentation, part-of-speech tagging, and named entity recognition), syntactic parsing (dependency parsing), and semantic parsing (semantic dependency parsing and semantic role labeling). Unlike the existing state-of-the-art toolkits, such as Stanza, that adopt an independent model for each task, N-LTP adopts the multi-task framework by using a shared pre-trained model, which has the advantage of capturing the shared knowledge across relevant Chinese tasks. In addition, a knowledge distillation method (Clark et al., 2019) where the single-task model teaches the multi-task model is further introduced to encourage the multi-task model to surpass its single-task teacher. Finally, we provide a collection of easy-to-use APIs and a visualization tool to make users to use and view the processing results more easily and directly. To the best of our knowledge, this is the first toolkit to support six Chinese NLP fundamental tasks. Source code, documentation, and pre-trained models are available at https: //github.com/HIT-SCIR/ltp. 1 preprocess Output Sentence Split lexical analysis Word Segmentation Visualization Tool"
2021.emnlp-demo.6,D19-1399,0,0.04531,"Missing"
2021.emnlp-demo.6,O03-4002,0,0.138111,"tructed input and output the corresponding hidden representations of sequence H = (h[CLS] , h1 , h2 , . . . , hn , h[SEP] ). 2.2 hˆi = AdaptedTransformer(hi ), (3) ˆ [CLS] , h ˆ 1, h ˆ 2, . . . , h ˆ n, h ˆ [SEP] ) are ˆ = (h where H the updated representations. Finally, similar to CWS and POS, we use a linear decoder to classify label for each word: yi = Softmax(W NER hˆi + bNER ), (4) where yi denotes the NER label probability distribution of each character. Chinese word segmentation (CWS) is a preliminary and important task for Chinese natural language processing (NLP). In N-LTP, following Xue (2003), CWS is regarded as a character based sequence labeling problem. Specifically, given the hidden representations H = (h[CLS] , h1 , h2 , . . . , hn , h[SEP] ), we adopt a linear decoder to classify each character: 2.5 Dependency Parsing Dependency parsing is the task to analyze the semantic structure of a sentence. In N-LTP, we implement a deep biaffine neural dependency parser (Dozat and Manning, 2017) and einser algorithm (Eisner, 1996) to obtain the parsing result, which is formulated as: (1) where yi denotes the label probability distribution of each character; W CWS and bCWS are trainable"
2021.emnlp-demo.6,D18-1191,0,0.0422662,"Missing"
2021.emnlp-demo.6,2020.tacl-1.6,0,0.0494743,"Missing"
2021.emnlp-demo.6,2020.acl-demos.14,0,0.276993,"ing, and semantic parsing. In addition, we provide the visualization tool and easy-to-use API to help users easily use N-LTP. tagging, and named entity recognition (NER)), syntactic parsing (dependency parsing (DEP)), and semantic parsing (semantic dependency parsing (SDP) and semantic role labeling (SRL)). Unfortunately, there are relatively fewer high-performance and high-efficiency toolkits for Chinese NLP tasks. To fill this gap, it’s important to build a Chinese NLP toolkit to support rich Chinese fundamental NLP tasks, and make researchers process NLP tasks in Chinese quickly. Recently, Qi et al. (2020) introduce the Python NLP toolkit Stanza for multi-lingual languages, including Chinese language. Though Stanza can be directly applied for processing the Chinese texts, it suffers from several limitations. First, it only supports part of Chinese NLP tasks. For example, it fails to handle semantic parsing analysis, resulting in incomplete analysis in Chinese NLP. Second, it trained each task separately, ignoring the shared knowledge across the related tasks, which has been proven effective for Chinese NLP tasks (Qian et al., 2015; Hsieh et al., 2017; Chang et al., 2018). Third, independent mod"
2021.emnlp-demo.6,2021.acl-long.485,0,0.0153385,"utput the corresponding POS sequence labels, which is formulated as: SDP SRL CWS ℎ[&'(] ℎ! ℎ"" … ℎ$ ℎ[(*+] s# [SEP] Shared Encoder ELECTRA yi = Softmax(W POS hi + bPOS) , Input Sentences [CLS] s! s"" … where yi denotes the POS label probability distribution of the i-th character; hi is the first sub-token representation of word si . Figure 2: The architecture of the proposed model. 2.1 Shared Encoder 2.4 Multi-task framework uses a shared encoder to extract the shared knowledge across related tasks, which has obtained remarkable success on various NLP tasks (Qin et al., 2019; Wang et al., 2020; Zhou et al., 2021). Inspired by this, we adopt the SOTA pre-trained model (ELECTRA) (Clark et al., 2020) as the shared encoder to capture shared knowledge across six Chinese tasks. Given an input utterance s = (s1 , s2 , . . . , sn ), we first construct the input sequence by adding specific tokens s = ([CLS], s1 , s2 , . . . , sn , [SEP]), where [CLS] is the special symbol for representing the whole sequence, and [SEP] is the special symbol to separate non-consecutive token sequences (Devlin et al., 2019). ELECTRA takes the constructed input and output the corresponding hidden representations of sequence H = (h"
2021.emnlp-demo.6,D15-1211,0,0.01296,"make researchers process NLP tasks in Chinese quickly. Recently, Qi et al. (2020) introduce the Python NLP toolkit Stanza for multi-lingual languages, including Chinese language. Though Stanza can be directly applied for processing the Chinese texts, it suffers from several limitations. First, it only supports part of Chinese NLP tasks. For example, it fails to handle semantic parsing analysis, resulting in incomplete analysis in Chinese NLP. Second, it trained each task separately, ignoring the shared knowledge across the related tasks, which has been proven effective for Chinese NLP tasks (Qian et al., 2015; Hsieh et al., 2017; Chang et al., 2018). Third, independent modeling method will occupy more Introduction There is a wide of range of existing natural language processing (NLP) toolkits such as CoreNLP (Manning et al., 2014), UDPipe (Straka and Straková, 2017), FLAIR (Akbik et al., 2019), spaCy,1 and Stanza (Qi et al., 2020) in English, which makes it easier for users to build tools with sophisticated linguistic processing. Recently, the need for Chinese NLP has a dramatic increase in many downstream applications. A Chinese NLP platform usually includes lexical analysis (Chinese word segment"
2021.emnlp-demo.6,D19-1214,1,0.82673,"dden representations H as input and output the corresponding POS sequence labels, which is formulated as: SDP SRL CWS ℎ[&'(] ℎ! ℎ"" … ℎ$ ℎ[(*+] s# [SEP] Shared Encoder ELECTRA yi = Softmax(W POS hi + bPOS) , Input Sentences [CLS] s! s"" … where yi denotes the POS label probability distribution of the i-th character; hi is the first sub-token representation of word si . Figure 2: The architecture of the proposed model. 2.1 Shared Encoder 2.4 Multi-task framework uses a shared encoder to extract the shared knowledge across related tasks, which has obtained remarkable success on various NLP tasks (Qin et al., 2019; Wang et al., 2020; Zhou et al., 2021). Inspired by this, we adopt the SOTA pre-trained model (ELECTRA) (Clark et al., 2020) as the shared encoder to capture shared knowledge across six Chinese tasks. Given an input utterance s = (s1 , s2 , . . . , sn ), we first construct the input sequence by adding specific tokens s = ([CLS], s1 , s2 , . . . , sn , [SEP]), where [CLS] is the special symbol for representing the whole sequence, and [SEP] is the special symbol to separate non-consecutive token sequences (Devlin et al., 2019). ELECTRA takes the constructed input and output the corresponding hi"
2021.emnlp-demo.6,W96-0213,0,0.881383,"Missing"
2021.emnlp-demo.6,K17-3009,0,0.0303773,"t suffers from several limitations. First, it only supports part of Chinese NLP tasks. For example, it fails to handle semantic parsing analysis, resulting in incomplete analysis in Chinese NLP. Second, it trained each task separately, ignoring the shared knowledge across the related tasks, which has been proven effective for Chinese NLP tasks (Qian et al., 2015; Hsieh et al., 2017; Chang et al., 2018). Third, independent modeling method will occupy more Introduction There is a wide of range of existing natural language processing (NLP) toolkits such as CoreNLP (Manning et al., 2014), UDPipe (Straka and Straková, 2017), FLAIR (Akbik et al., 2019), spaCy,1 and Stanza (Qi et al., 2020) in English, which makes it easier for users to build tools with sophisticated linguistic processing. Recently, the need for Chinese NLP has a dramatic increase in many downstream applications. A Chinese NLP platform usually includes lexical analysis (Chinese word segmentation (CWS), part-of-speech (POS) 1 Processor Toolkit https://spacy.io 42 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 42–49 August 1–6, 2021. ©2021 Association for Computational Linguistics"
2021.emnlp-demo.6,2020.emnlp-main.75,0,0.0856429,"Missing"
2021.emnlp-main.257,2020.emnlp-main.367,0,0.0571655,"Missing"
2021.emnlp-main.257,2020.tacl-1.30,0,0.0249007,"es on PAWS-X, POS and NER. three types of cross-lingual understanding tasks Then increasing the vocabulary from VO C AP250K from XTREME benchmark (Hu et al., 2020), into VO C AP500K mitigates the gap and bring imcluding two classification datasets: XNLI (Conprovements on six datasets except for PAWS-X, neau et al., 2018), PAWS-X (Yang et al., 2019), which only includes seven high-resource languages. three span extraction datasets: XQuAD (Artetxe However, increasing the size of vocabulary diet al., 2020), MLQA (Lewis et al., 2020), TyDiQArectly learned with Sentencepiece from J OINT250K GoldP (Clark et al., 2020), and two sequence labelto J OINT500K does not improve the performance ing datasets: NER (Pan et al., 2017), POS (Zeman as our VO C AP method does, showing the imporet al., 2019). The statistics of the datasets are shown tance of selecting language-specific subword units in the appendix. and leveraging how much vocabulary capacity each Implementation Details We adapt the Trans- language requires. former architecture from the base model setting Since increasing vocabulary size brings the isin Conneau et al. (2020), i.e., 12 layers and 768 sues of model size and pre-training speed, we study hidd"
2021.emnlp-main.257,2020.acl-main.747,0,0.24302,". In order to address the issues, we propose k-NN-based target sampling to accelerate the expensive softmax. Our experiments show that the multilingual vocabulary learned with VO C AP benefits cross-lingual language model pre-training. Moreover, k-NN-based target sampling mitigates the side-effects of increasing the vocabulary size while achieving comparable performance and faster pre-training speed. The code and the pretrained multilingual vocabularies are available at https://github. com/bozheng-hit/VoCapXLM. 1 Introduction Pretrained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2021b; Xue et al., 2020) have recently shown great success in improving cross-lingual transferability. These models encode texts from different languages into universal representations with a shared multilingual vocabulary and a shared Transformer encoder (Vaswani et al., 2017). By pretraining cross-lingual language models on the largescale multilingual corpus, the models achieve stateof-the-art performance on various downstream tasks, e.g., cross-lingual question answering and cross-lingual sentence classification. Although the Transformer architecture used in most pretrained mo"
2021.emnlp-main.257,P18-1007,0,0.374283,"020). Meanwhile, state-of-the-art pretrained cross-lingual language models use the shared multilingual vocabulary of 250K subword units to represent more than 100 languages (Conneau et al., 2020; Chi et al., 2021b; Xue et al., 2020). Although some subword units are shared across languages, no more than 2.5K language-specific subword units on average are allocated for each language, which is still relatively small. Besides, the multilingual vocabulary is trained on the combined multilingual corpus with subword segmentation algorithms like BPE (Sennrich et al., 2015) and unigram language model (Kudo, 2018). During vocabulary construction, these algorithms tend to select more subword units shared across languages with common scripts like Latin and Cyrillic (Chung et al., 2020b), but have a lower chance to select language-specific subword units. It is hard to determine how much vocabulary capacity a particular language requires and whether the shared multilingual vocabulary has allocated enough vocabulary capacity to represent the language. In this paper, we propose VO C AP, an algorithm to allocate large vocabulary for cross-lingual language model by separately evaluating the required vocabulary"
2021.emnlp-main.257,2020.acl-main.653,0,0.0420862,"uestion answering ness of our methods, we conduct experiments on datasets but degrades on PAWS-X, POS and NER. three types of cross-lingual understanding tasks Then increasing the vocabulary from VO C AP250K from XTREME benchmark (Hu et al., 2020), into VO C AP500K mitigates the gap and bring imcluding two classification datasets: XNLI (Conprovements on six datasets except for PAWS-X, neau et al., 2018), PAWS-X (Yang et al., 2019), which only includes seven high-resource languages. three span extraction datasets: XQuAD (Artetxe However, increasing the size of vocabulary diet al., 2020), MLQA (Lewis et al., 2020), TyDiQArectly learned with Sentencepiece from J OINT250K GoldP (Clark et al., 2020), and two sequence labelto J OINT500K does not improve the performance ing datasets: NER (Pan et al., 2017), POS (Zeman as our VO C AP method does, showing the imporet al., 2019). The statistics of the datasets are shown tance of selecting language-specific subword units in the appendix. and leveraging how much vocabulary capacity each Implementation Details We adapt the Trans- language requires. former architecture from the base model setting Since increasing vocabulary size brings the isin Conneau et al. (202"
2021.emnlp-main.257,2021.ccl-1.108,0,0.0879979,"Missing"
2021.emnlp-main.257,P17-1178,0,0.0880255,"us. 94 92 −280 −160 −280 −260 −240 −220 ALP −200 −180 −160 −150 Low res. Mid res. High res. Figure 3: F1 score on NER task with different vocabularies versus their ALP on the monolingual corpus. Figure 4: Comparison of vocabulary capacity of different-resourced languages. Shorter bars indicate larger vocabulary capacity. vocabularies for each language on the corresponding monolingual corpus, with vocabulary size ranging from 1K to 30K. Then we pretrain monolingual language models with the corresponding monolingual vocabularies. We evaluate these pretrained models on two downstream tasks: NER (Pan et al., 2017) and POS (Zeman et al., 2019) from the XTREME benchmark since there is annotated task data for a large number of languages. The vocabularies are learned on the reconstructed CommonCrawl corpus (Chi et al., 2021b; Conneau et al., 2020) using SentencePiece (Kudo and Richardson, 2018) with the unigram language model (Kudo, 2018). The unigram distributions are also counted on the CommonCrawl corpus. The Wikipedia corpus is used for all pre-training experiments in this paper since it is easier to run experiments due to its smaller size. More details about the pre-training data can be found in the a"
2021.emnlp-main.257,D19-1382,0,0.0210914,"ulary directly learned on multilingual corpus with 4.1 Setup SentencePiece, i.e., XLM-R250K and J OINT250K , Fine-Tuning Datasets To validate the effectiveour VO C AP250K improves on question answering ness of our methods, we conduct experiments on datasets but degrades on PAWS-X, POS and NER. three types of cross-lingual understanding tasks Then increasing the vocabulary from VO C AP250K from XTREME benchmark (Hu et al., 2020), into VO C AP500K mitigates the gap and bring imcluding two classification datasets: XNLI (Conprovements on six datasets except for PAWS-X, neau et al., 2018), PAWS-X (Yang et al., 2019), which only includes seven high-resource languages. three span extraction datasets: XQuAD (Artetxe However, increasing the size of vocabulary diet al., 2020), MLQA (Lewis et al., 2020), TyDiQArectly learned with Sentencepiece from J OINT250K GoldP (Clark et al., 2020), and two sequence labelto J OINT500K does not improve the performance ing datasets: NER (Pan et al., 2017), POS (Zeman as our VO C AP method does, showing the imporet al., 2019). The statistics of the datasets are shown tance of selecting language-specific subword units in the appendix. and leveraging how much vocabulary capacit"
2021.emnlp-main.356,2020.tacl-1.30,0,0.0230087,"lingual corpora can gual, and cross-lingual conversational recombring performance improvement in comparison mendation baselines on DuRecDial 2.0. Experwith monolingual task setting, such as for the tasks iment results show that the use of additional English data can bring performance improveof task-oriented dialog (Schuster et al., 2019b), ment for Chinese conversational recommensemantic parsing (Li et al., 2021), QA and readdation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role labelling In recent years, there has been a significant in- (Akbik et al., 2015) and NLI (Conneau et al., 2018). crease in the research topic of conversational rec- Therefore it is necessary to create multilingual conversational recommendation dataset that might enommendation due"
2021.emnlp-main.356,2020.acl-main.747,0,0.0765699,"Missing"
2021.emnlp-main.356,D18-1269,0,0.0242468,"ddation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role labelling In recent years, there has been a significant in- (Akbik et al., 2015) and NLI (Conneau et al., 2018). crease in the research topic of conversational rec- Therefore it is necessary to create multilingual conversational recommendation dataset that might enommendation due to the rise of voice-based bots (Kang et al., 2019; Li et al., 2018; Sun and Zhang, hance model performance when compared with 2018; Christakopoulou et al., 2016; Warnestal, monolingual training setting, and it could provide 2005). These works focus on how to provide recom- a new benchmark dataset for the study of multilingual modeling techniques. mendation service in a more user-friendly manner To facilitate the study of this"
2021.emnlp-main.356,2020.emnlp-main.438,0,0.0859544,"Missing"
2021.emnlp-main.356,2020.emnlp-main.654,0,0.0324503,"ntents and slots (Li et al., dataset (DuRecDial 2.0) to enable researchers 2018; Kang et al., 2019). Recently more and more to explore a challenging task of multilingual and cross-lingual conversational recommendaefforts are devoted to the research line of the section. The difference between DuRecDial 2.0 ond category and many datasets have been created, and existing conversational recommendation including English dialog datasets (Dodge et al., datasets is that the data item (Profile, Goal, 2016; Li et al., 2018; Kang et al., 2019; Moon Knowledge, Context, Response) in DuRecDial et al., 2019; Hayati et al., 2020) and Chinese dialog 2.0 is annotated in two languages, both Endatasets (Liu et al., 2020b; Zhou et al., 2020). glish and Chinese, while other datasets are built with the setting of a single language. We However, to the best of our knowledge, almost collect 8.2k dialogs aligned across English and all these datasets are constructed in the setting Chinese languages (16.5k dialogs and 255k utof a single language, and there is no publicly terances in total) that are annotated by crowdavailable multilingual dataset for conversational sourced workers with strict quality control prorecommendation. Pre"
2021.emnlp-main.356,D19-1249,0,0.0164608,"ndation. Previous work on other NLP cedure. We then build monolingual, multilintasks have proved that multilingual corpora can gual, and cross-lingual conversational recombring performance improvement in comparison mendation baselines on DuRecDial 2.0. Experwith monolingual task setting, such as for the tasks iment results show that the use of additional English data can bring performance improveof task-oriented dialog (Schuster et al., 2019b), ment for Chinese conversational recommensemantic parsing (Li et al., 2021), QA and readdation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role labelling In recent years, there has been a significant in- (Akbik et al., 2015) and NLI (Conneau et al., 2018). crease in the research topic of conversational rec- Therefo"
2021.emnlp-main.356,D19-1203,0,0.210984,"ng2 , Zheng-Yu Niu2 , Hua Wu2 , Wanxiang Che1† 1 Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, Harbin, China 2 Baidu Inc., Beijing, China {zmliu, car}@ir.hit.edu.cn {wanghaifeng, niuzhengyu, wu_hua}@baidu.com Abstract et al., 2016; Sun and Zhang, 2018); (2) non-task dialog-modeling approaches that can conduct more In this paper, we provide a bilingual paralfree-form interactions for recommendation, withlel human-to-human recommendation dialog out pre-defined user intents and slots (Li et al., dataset (DuRecDial 2.0) to enable researchers 2018; Kang et al., 2019). Recently more and more to explore a challenging task of multilingual and cross-lingual conversational recommendaefforts are devoted to the research line of the section. The difference between DuRecDial 2.0 ond category and many datasets have been created, and existing conversational recommendation including English dialog datasets (Dodge et al., datasets is that the data item (Profile, Goal, 2016; Li et al., 2018; Kang et al., 2019; Moon Knowledge, Context, Response) in DuRecDial et al., 2019; Hayati et al., 2020) and Chinese dialog 2.0 is annotated in two languages, both Endatasets (Liu et"
2021.emnlp-main.356,C12-1089,0,0.0384622,"a can bring performance improveof task-oriented dialog (Schuster et al., 2019b), ment for Chinese conversational recommensemantic parsing (Li et al., 2021), QA and readdation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role labelling In recent years, there has been a significant in- (Akbik et al., 2015) and NLI (Conneau et al., 2018). crease in the research topic of conversational rec- Therefore it is necessary to create multilingual conversational recommendation dataset that might enommendation due to the rise of voice-based bots (Kang et al., 2019; Li et al., 2018; Sun and Zhang, hance model performance when compared with 2018; Christakopoulou et al., 2016; Warnestal, monolingual training setting, and it could provide 2005). These works focus on how to provi"
2021.emnlp-main.356,2020.acl-main.653,0,0.0690059,"Missing"
2021.emnlp-main.356,2021.eacl-main.257,0,0.0464825,"Missing"
2021.emnlp-main.356,N16-1014,0,0.0884497,"Missing"
2021.emnlp-main.356,2020.tacl-1.47,0,0.0591359,", 2019). Recently more and more to explore a challenging task of multilingual and cross-lingual conversational recommendaefforts are devoted to the research line of the section. The difference between DuRecDial 2.0 ond category and many datasets have been created, and existing conversational recommendation including English dialog datasets (Dodge et al., datasets is that the data item (Profile, Goal, 2016; Li et al., 2018; Kang et al., 2019; Moon Knowledge, Context, Response) in DuRecDial et al., 2019; Hayati et al., 2020) and Chinese dialog 2.0 is annotated in two languages, both Endatasets (Liu et al., 2020b; Zhou et al., 2020). glish and Chinese, while other datasets are built with the setting of a single language. We However, to the best of our knowledge, almost collect 8.2k dialogs aligned across English and all these datasets are constructed in the setting Chinese languages (16.5k dialogs and 255k utof a single language, and there is no publicly terances in total) that are annotated by crowdavailable multilingual dataset for conversational sourced workers with strict quality control prorecommendation. Previous work on other NLP cedure. We then build monolingual, multilintasks have proved tha"
2021.emnlp-main.356,L18-1560,0,0.012717,"riented dialog (Schuster et al., 2019b), ment for Chinese conversational recommensemantic parsing (Li et al., 2021), QA and readdation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role labelling In recent years, there has been a significant in- (Akbik et al., 2015) and NLI (Conneau et al., 2018). crease in the research topic of conversational rec- Therefore it is necessary to create multilingual conversational recommendation dataset that might enommendation due to the rise of voice-based bots (Kang et al., 2019; Li et al., 2018; Sun and Zhang, hance model performance when compared with 2018; Christakopoulou et al., 2016; Warnestal, monolingual training setting, and it could provide 2005). These works focus on how to provide recom- a new benchmark dataset for"
2021.emnlp-main.356,2020.acl-main.98,1,0.911879,", 2019). Recently more and more to explore a challenging task of multilingual and cross-lingual conversational recommendaefforts are devoted to the research line of the section. The difference between DuRecDial 2.0 ond category and many datasets have been created, and existing conversational recommendation including English dialog datasets (Dodge et al., datasets is that the data item (Profile, Goal, 2016; Li et al., 2018; Kang et al., 2019; Moon Knowledge, Context, Response) in DuRecDial et al., 2019; Hayati et al., 2020) and Chinese dialog 2.0 is annotated in two languages, both Endatasets (Liu et al., 2020b; Zhou et al., 2020). glish and Chinese, while other datasets are built with the setting of a single language. We However, to the best of our knowledge, almost collect 8.2k dialogs aligned across English and all these datasets are constructed in the setting Chinese languages (16.5k dialogs and 255k utof a single language, and there is no publicly terances in total) that are annotated by crowdavailable multilingual dataset for conversational sourced workers with strict quality control prorecommendation. Previous work on other NLP cedure. We then build monolingual, multilintasks have proved tha"
2021.emnlp-main.356,2020.lrec-1.494,0,0.0246919,"Missing"
2021.emnlp-main.356,P19-1081,0,0.0642011,"Missing"
2021.emnlp-main.356,P19-1369,1,0.858284,"Missing"
2021.emnlp-main.356,P17-1163,0,0.0377515,"Missing"
2021.emnlp-main.356,2020.coling-main.365,0,0.0251136,"ore and more to explore a challenging task of multilingual and cross-lingual conversational recommendaefforts are devoted to the research line of the section. The difference between DuRecDial 2.0 ond category and many datasets have been created, and existing conversational recommendation including English dialog datasets (Dodge et al., datasets is that the data item (Profile, Goal, 2016; Li et al., 2018; Kang et al., 2019; Moon Knowledge, Context, Response) in DuRecDial et al., 2019; Hayati et al., 2020) and Chinese dialog 2.0 is annotated in two languages, both Endatasets (Liu et al., 2020b; Zhou et al., 2020). glish and Chinese, while other datasets are built with the setting of a single language. We However, to the best of our knowledge, almost collect 8.2k dialogs aligned across English and all these datasets are constructed in the setting Chinese languages (16.5k dialogs and 255k utof a single language, and there is no publicly terances in total) that are annotated by crowdavailable multilingual dataset for conversational sourced workers with strict quality control prorecommendation. Previous work on other NLP cedure. We then build monolingual, multilintasks have proved that multilingual corpor"
2021.emnlp-main.356,Q17-1022,0,0.0479985,"Missing"
2021.emnlp-main.356,P02-1040,0,0.113776,"Missing"
2021.emnlp-main.356,N19-1380,0,0.108235,"and there is no publicly terances in total) that are annotated by crowdavailable multilingual dataset for conversational sourced workers with strict quality control prorecommendation. Previous work on other NLP cedure. We then build monolingual, multilintasks have proved that multilingual corpora can gual, and cross-lingual conversational recombring performance improvement in comparison mendation baselines on DuRecDial 2.0. Experwith monolingual task setting, such as for the tasks iment results show that the use of additional English data can bring performance improveof task-oriented dialog (Schuster et al., 2019b), ment for Chinese conversational recommensemantic parsing (Li et al., 2021), QA and readdation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role"
2021.findings-acl.207,D18-1316,0,0.0174526,"n return x(t) else return None end if This helps to determine the word substituting order in the proposed method. In this work, we use a combination of the changes found in the unlabelled attachment score (UAS) and in the labelled attachment score (LAS) to measure word importance. Specifically, the importance of a word xi in sentence x is computed as 2.3.1 Generating Process We collect substitutes from the following methods: BERT-Based Method: We use BERT to generate candidates for each target word from its context. This method generates only single subwords. Embedding-Based Method: Following Alzantot et al. (2018), we use word embeddings of Mrkˇsi´c et al. (2016)2 to compute the N nearest neighbours of each target word according to their cosine similarity and use them as candidates. Sememe-Based Method: The sememes of a word represent its core meaning (Dong and Dong, 2006). Following Zang et al. (2020), we collect the substitutes of the target word x based on the rule that one of the substitutes the senses of x∗ must have the same sememe annotations as one of senses of x. Synonym-Based Method: We use WordNet3 to extract synonyms of each target word as candidates. 2.3.2 Filtering Process Generation of S"
2021.findings-acl.207,N19-1423,0,0.0124486,"Preserving the syntactic structure enables us to use the gold syntactic structure of the original sentence in the evaluation process. While preserving the fluency ensures that ungrammatical adversarial examples, which not only fool the target model but also confuse humans, will not be considered valid. Therefore in this paper, we evaluate the quality of an adversarial example in two aspects, namely the fluency and syntactic structure preservation. Recently, Zheng et al. (2020) proposed the first dependency parser attacking algorithm based on word-substitution which depended entirely on BERT (Devlin et al., 2019) to generate candidate substitutes. The rational was that the use of the pre-trained language model will ensure fluency of the adversarial examples. However, we find that using BERT alone is far from enough to preserve fluency. Therefore, in this paper, we propose a method to generate better adversarial examples for dependency parsing with four types of candidate generators and filters. Specifically, our method consists of three steps: (i) determining the substitution order, (ii) generating and filtering candidate substitutes for each word, (iii) searching for the best possible combination of"
2021.findings-acl.207,2020.emnlp-main.182,0,0.356771,"operties of adversarial attacks. We find that (i) the introduction of out-of-vocabulary (OOV, words not in the embedding’s vocabulary) and out-of-training (OOT, words not in the training set of the parser) words in adversarial examples are two main factors that harm models’ performance; (ii) adversarial examples generated against a parser strongly depend on the type of the parser, the token embeddings and even the random seed. Adversarial training (Goodfellow et al., 2015), where adversarial examples are added in the training stage, has been commonly used in previous work (Zheng et al., 2020; Han et al., 2020) to improve a parser’s robustness. Only a limited number of adversarial examples have been used in such cases, and Zheng et al. (2020) argued that overuse of them may lead to a performance drop on the clean data. However, we show that with improvement in the quality of adversarial examples produced in our method, more adversarial examples can be used in the training stage to further improve the parsing models’ robustness without producing any apparent harm in their performance on the clean data. Inspired by our second finding, we propose to improve the parsers’ robustness by combining models t"
2021.findings-acl.207,2020.iwpt-1.7,1,0.754643,"or, and we propose to generate better examples by using more generators and stricter filters. Han et al. (2020) proposed an approach to attack structured prediction models with a seq2seq model (Wang et al., 2016) and evaluated this model on dependency parsing. They used two reference parsers in addition to the victim parser to supervise the training of the adversarial example generator, and found that the three parsers produce better results when they have different inductive biases embedded to make the attack successful. This finding is quite close in spirit to our conclusion in Section 4.5. Hu et al. (2020) also put forth efforts to modify the text in syntactic tasks while preserving the original syntactic structure. However, their goal is to preserve privacy via the modification of words that could disclose sensitive information. 6 Conclusion In this paper, we propose a method for generating high-quality adversarial examples for dependency parsing and show its effectiveness based on automatic and human evaluation. We investigate This work was supported by the National Key R&D Program of China via grant 2020AAA0106501 and the National Natural Science Foundation of China (NSFC) via grant 61976072"
2021.findings-acl.207,N16-1082,0,0.0336606,"ii) the true dependency tree of x∗ should be the same as that of x. In this paper, these two constraints are ensured through the use of various filters (see Section 2.3) and are used to evaluate the quality of adversarial examples (see details on fluency and syntactic structure preservation in Section 3.3). 2.2 Word Importance Ranking Word importance ranking in our model is based on the observation that some words have a stronger influence on model prediction than others. Such word importance is typically computed by setting each word to unknown and examining the changes in their predictions (Li et al., 2016; Ren et al., 2019). 2345 Algorithm 1 Dependency Parsing Attack generation methods, then apply filters to discard inappropriate substitutes, ensuring both diversity and quality of the generated candidates. Input: Sentence example x(0) = {x1 , x2 , . . . , xN }, maximum percentage of words allowed to be modified γ Output: Adversarial example x(i) 1: for i = 1 to N do 2: Compute word importance I(x(0) , xi ) via Eq. 1 3: end for 4: Create a set W of all words xi ∈ x(0) sorted by the descending order of their importance I(x(0) , xi ). 5: t = 0 6: for each word xj in W do 7: Build candidate set Cj"
2021.findings-acl.207,2021.ccl-1.108,0,0.0377807,"Missing"
2021.findings-acl.207,P18-1130,0,0.0126623,"rds in the sentence exceeds a threshold γ, we stop the process. Otherwise, we search for a substitute for the next target word. 3 3.1 Experimental Setup Target Parsers and Token Embeddings We choose the following two strong and commonly used English parsers, one graph-based, the other transition-based, as target models, both of which achieve performance close to the state-of-the-art. Deep Biaffine Parser (Dozat and Manning, 2017) is a graph-based parser that scores each candidate arc independently and relies on a decoding algorithm to search for the highest-scoring tree. Stack-Pointer Parser (Ma et al., 2018) is a transition-based parser that incrementally builds the dependency tree with pre-defined operations. We used the following four types of token embeddings to study their influence on each parsers’ robustness. To focus on the influence of the embeddings, we use only the embeddings as input to the parsers: GloVe (Pennington et al., 2014) is a frequently used static word embedding. RoBERTa (Liu et al., 2019) is a pre-trained language model based on a masked language modelling object, which learns to predict a randomly masked token based on its context. It produces contextualised word piece emb"
2021.findings-acl.207,de-marneffe-etal-2006-generating,0,0.0517672,"Missing"
2021.findings-acl.207,2020.findings-emnlp.341,0,0.0200122,"model (i.e., a random seed). We use these insights to improve the robustness of English parsing models, relying on adversarial training and model ensembling.1 1 Introduction Neural network-based models have achieved great successes in a wide range of NLP tasks. However, recent work has shown that their performance can be easily undermined with adversarial examples that would pose no confusion for humans (Zhang et al., 2020). As an increasing number of successful adversarial attackers have been developed for NLP tasks, the quality of the adversarial examples they generate has been questioned (Morris et al., 2020). The definition of a valid successful adversarial example differs across target tasks. In semantic tasks such as sentiment analysis (Zhang et al., 2019) and textual entailment (Jin et al., 2020), a valid successful adversarial example needs to be able to alter the prediction of the target model while ∗ Work partially done while at the University of Edinburgh. 1 Our code is available at: https://github.com/ WangYuxuan93/DepAttacker.git preserving the semantic content and fluency of the original text. In contrast, in the less explored field of attacking syntactic tasks, the syntactic structure,"
2021.findings-acl.207,N16-1018,0,0.0609948,"Missing"
2021.findings-acl.207,D14-1162,0,0.0855441,"ich achieve performance close to the state-of-the-art. Deep Biaffine Parser (Dozat and Manning, 2017) is a graph-based parser that scores each candidate arc independently and relies on a decoding algorithm to search for the highest-scoring tree. Stack-Pointer Parser (Ma et al., 2018) is a transition-based parser that incrementally builds the dependency tree with pre-defined operations. We used the following four types of token embeddings to study their influence on each parsers’ robustness. To focus on the influence of the embeddings, we use only the embeddings as input to the parsers: GloVe (Pennington et al., 2014) is a frequently used static word embedding. RoBERTa (Liu et al., 2019) is a pre-trained language model based on a masked language modelling object, which learns to predict a randomly masked token based on its context. It produces contextualised word piece embeddings. ELECTRA (Clark et al., 2020) is a pre-trained language model based on a replaced token detection object, which learns to predict whether each token in the corrupted input has been replaced. It produces contextualised word piece embeddings. ELMo (Peters et al., 2018) is a pre-trained language representation model based on characte"
2021.findings-acl.207,N18-1202,0,0.0174184,", we use only the embeddings as input to the parsers: GloVe (Pennington et al., 2014) is a frequently used static word embedding. RoBERTa (Liu et al., 2019) is a pre-trained language model based on a masked language modelling object, which learns to predict a randomly masked token based on its context. It produces contextualised word piece embeddings. ELECTRA (Clark et al., 2020) is a pre-trained language model based on a replaced token detection object, which learns to predict whether each token in the corrupted input has been replaced. It produces contextualised word piece embeddings. ELMo (Peters et al., 2018) is a pre-trained language representation model based on character embeddings and bidirectional language modelling. 3.2 Datasets and Experimental Settings We train the target parsers and evaluate the proposed method on the English Penn Treebank (PTB) dataset,7 converted into Stanford dependencies using version 3.3.0 of the Stanford dependency converter (de Marneffe et al., 2006) (PTB-SD-3.3.0). We follow the standard PTB split, using section 2-21 for training, section 22 as a development set and 23 as a test set. It is important to note that when converting PTB into Stanford dependencies, Zhen"
2021.findings-acl.207,P19-1103,1,0.805871,"ndency tree of x∗ should be the same as that of x. In this paper, these two constraints are ensured through the use of various filters (see Section 2.3) and are used to evaluate the quality of adversarial examples (see details on fluency and syntactic structure preservation in Section 3.3). 2.2 Word Importance Ranking Word importance ranking in our model is based on the observation that some words have a stronger influence on model prediction than others. Such word importance is typically computed by setting each word to unknown and examining the changes in their predictions (Li et al., 2016; Ren et al., 2019). 2345 Algorithm 1 Dependency Parsing Attack generation methods, then apply filters to discard inappropriate substitutes, ensuring both diversity and quality of the generated candidates. Input: Sentence example x(0) = {x1 , x2 , . . . , xN }, maximum percentage of words allowed to be modified γ Output: Adversarial example x(i) 1: for i = 1 to N do 2: Compute word importance I(x(0) , xi ) via Eq. 1 3: end for 4: Create a set W of all words xi ∈ x(0) sorted by the descending order of their importance I(x(0) , xi ). 5: t = 0 6: for each word xj in W do 7: Build candidate set Cj for xj following t"
2021.findings-acl.207,D16-1058,0,0.0368721,"Morris et al. (2020) reported that quite a number of these techniques introduce grammatical errors. In syntactic tasks, Zheng et al. (2020) recently proposed the first dependency parser attacking method which depends entirely on BERT to generate candidates. However, we show that the quality of adversarial examples generated by their method is relatively low due to the limitation of the BERTbased generator, and we propose to generate better examples by using more generators and stricter filters. Han et al. (2020) proposed an approach to attack structured prediction models with a seq2seq model (Wang et al., 2016) and evaluated this model on dependency parsing. They used two reference parsers in addition to the victim parser to supervise the training of the adversarial example generator, and found that the three parsers produce better results when they have different inductive biases embedded to make the attack successful. This finding is quite close in spirit to our conclusion in Section 4.5. Hu et al. (2020) also put forth efforts to modify the text in syntactic tasks while preserving the original syntactic structure. However, their goal is to preserve privacy via the modification of words that could"
2021.findings-acl.207,2020.acl-main.540,0,0.0323383,"e importance of a word xi in sentence x is computed as 2.3.1 Generating Process We collect substitutes from the following methods: BERT-Based Method: We use BERT to generate candidates for each target word from its context. This method generates only single subwords. Embedding-Based Method: Following Alzantot et al. (2018), we use word embeddings of Mrkˇsi´c et al. (2016)2 to compute the N nearest neighbours of each target word according to their cosine similarity and use them as candidates. Sememe-Based Method: The sememes of a word represent its core meaning (Dong and Dong, 2006). Following Zang et al. (2020), we collect the substitutes of the target word x based on the rule that one of the substitutes the senses of x∗ must have the same sememe annotations as one of senses of x. Synonym-Based Method: We use WordNet3 to extract synonyms of each target word as candidates. 2.3.2 Filtering Process Generation of Substitute Candidates We apply the following four types of filters to discard candidates which are likely inappropriate, either in terms of syntactic preservation or fluency. POS Filter: We first filter out substitutes with different part-of-speech (POS) tags from the original word.4 This filte"
2021.findings-acl.207,P19-1559,0,0.0216978,"ing.1 1 Introduction Neural network-based models have achieved great successes in a wide range of NLP tasks. However, recent work has shown that their performance can be easily undermined with adversarial examples that would pose no confusion for humans (Zhang et al., 2020). As an increasing number of successful adversarial attackers have been developed for NLP tasks, the quality of the adversarial examples they generate has been questioned (Morris et al., 2020). The definition of a valid successful adversarial example differs across target tasks. In semantic tasks such as sentiment analysis (Zhang et al., 2019) and textual entailment (Jin et al., 2020), a valid successful adversarial example needs to be able to alter the prediction of the target model while ∗ Work partially done while at the University of Edinburgh. 1 Our code is available at: https://github.com/ WangYuxuan93/DepAttacker.git preserving the semantic content and fluency of the original text. In contrast, in the less explored field of attacking syntactic tasks, the syntactic structure, rather than the semantic content, must be preserved while also maintaining the fluency. Preserving the syntactic structure enables us to use the gold sy"
2021.findings-acl.207,2020.acl-main.590,0,0.151193,"tacking syntactic tasks, the syntactic structure, rather than the semantic content, must be preserved while also maintaining the fluency. Preserving the syntactic structure enables us to use the gold syntactic structure of the original sentence in the evaluation process. While preserving the fluency ensures that ungrammatical adversarial examples, which not only fool the target model but also confuse humans, will not be considered valid. Therefore in this paper, we evaluate the quality of an adversarial example in two aspects, namely the fluency and syntactic structure preservation. Recently, Zheng et al. (2020) proposed the first dependency parser attacking algorithm based on word-substitution which depended entirely on BERT (Devlin et al., 2019) to generate candidate substitutes. The rational was that the use of the pre-trained language model will ensure fluency of the adversarial examples. However, we find that using BERT alone is far from enough to preserve fluency. Therefore, in this paper, we propose a method to generate better adversarial examples for dependency parsing with four types of candidate generators and filters. Specifically, our method consists of three steps: (i) determining the su"
2021.findings-acl.216,2020.findings-emnlp.184,0,0.0894579,"s more intuitive to use non-autoregressive language models such as BERT to directly correct the Chinese spelling errors. Hong et al. (2019) propose the FASPell model to predict candidate characters based on the BERT model and exploit the phonological and visual similarity information to select candidate characters. Zhang et al. (2020) propose a model named Soft-Masked BERT, which consists of a detection network and a correction network based on BERT. Cheng et al. (2020) propose to incorporate phonological and visual similarity knowledge into BERT via a specialized graph convolutional network. Bao et al. (2020) design a chunk-based framework and extend the traditional confusion sets with semantical candidates to cover different types of errors. Although these non-autoregressive methods mentioned above have achieved state of the art in the CSC task so far, these methods still suffer from the incoherent problems that exist in nonautoregressive models (Gu et al., 2018; Gu and Kong, 2020). In this paper, we propose a novel model DCN which learns the dependencies between the adjacent Chinese characters and alleviates the incoherent problem. 3 3.1 Chinese spelling check (CSC) is a challenging task that re"
2021.findings-acl.216,2020.acl-main.81,0,0.122708,"nese spelling errors. Here, “户秃” should be corrected to “糊涂” (confused). Introduction Chinese spelling check (CSC) is an important task which can be utilized in many natural language applications such as optical character recognition (OCR) (Wang et al., 2018; Hong et al., 2019) and essay scoring. Meanwhile, CSC is a challenging task which requires human-level natural language understanding ability (Liu et al., 2010, 2013; Xin et al., 2014). Recently, BERT-based non-autoregressive language models have achieved state-of-the-art performance in the CSC task (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020). These works fine-tune BERT-based models using CSC training data. During the training phase, all the target Chinese characters will be involved as labels. In the inference stage, the models predict the most likely Chinese character from a candidate set at each position. When the most likely character is different from the input character, the input character will be considered as a spelling error and corrected to the most likely character. Based on the powerful generalization ability of BERT (Devlin et al., 2019), these works have achieved better performance than other models. However, these"
2021.findings-acl.216,N12-1067,0,0.0326906,"egressive language models in the CSC task. • We propose a simple and effective Pinyin Enhanced Candidate Generator to incorporate phonological information and generate better candidate characters. • Experimental results show that our proposed method achieves state-of-the-art performance on three human-annotated datasets. For reproducibility, our code for this paper is available at https://github.com/destwang/DCN. 2 Related Work ability. With the development of deep learning techniques, the CSC task has recently made more progress. CSC is similar to the grammatical error correction (GEC) task (Dahlmeier and Ng, 2012). The difference between them is that CSC only focuses on Chinese spelling errors, while GEC also includes errors that need insertion and deletion. Most models in the GEC task use an autoregressive Seq2Seq model to correct a sentence. Similarly, Seq2Seq models can also be used in the CSC task. Wang et al. (2019) propose an autoregressive pointer network which generates a Chinese character from the confusion set rather than the entire vocabulary. Although the autoregressive Seq2Seq model has the ability to correct the spelling errors, it is usually slow. The input and output are so similar that"
2021.findings-acl.216,N19-1423,0,0.00705992,"te-of-the-art performance in the CSC task (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020). These works fine-tune BERT-based models using CSC training data. During the training phase, all the target Chinese characters will be involved as labels. In the inference stage, the models predict the most likely Chinese character from a candidate set at each position. When the most likely character is different from the input character, the input character will be considered as a spelling error and corrected to the most likely character. Based on the powerful generalization ability of BERT (Devlin et al., 2019), these works have achieved better performance than other models. However, these works on the CSC task rely on the incorrect independence assumption, which may lead to an incoherent problem. Concretely, they assume that the predicted tokens are independent of each other, which generally does not hold in natural language (Yang et al., 2019; Gu and Kong, 2020). For the CSC task, one spelling error may have multiple corrections. Ignoring the corrected context may result in a correction conflict. As shown in Table 1, “户秃” may be corrected as “糊涂” (confused) or “尴尬” (embarrassed). Because of the in"
2021.findings-acl.216,D19-5522,0,0.114934,"idate Chinese characters via a Pinyin Enhanced Candidate Generator and then utilizes an attention-based network to model the dependencies between two adjacent Chinese characters. The experimental results show that our proposed method achieves a new state-ofthe-art performance on three human-annotated datasets. 1 Table 1: An example of Chinese spelling errors. Here, “户秃” should be corrected to “糊涂” (confused). Introduction Chinese spelling check (CSC) is an important task which can be utilized in many natural language applications such as optical character recognition (OCR) (Wang et al., 2018; Hong et al., 2019) and essay scoring. Meanwhile, CSC is a challenging task which requires human-level natural language understanding ability (Liu et al., 2010, 2013; Xin et al., 2014). Recently, BERT-based non-autoregressive language models have achieved state-of-the-art performance in the CSC task (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020). These works fine-tune BERT-based models using CSC training data. During the training phase, all the target Chinese characters will be involved as labels. In the inference stage, the models predict the most likely Chinese character from a candidate set at ea"
2021.findings-acl.216,C10-2085,0,0.0377928,"een two adjacent Chinese characters. The experimental results show that our proposed method achieves a new state-ofthe-art performance on three human-annotated datasets. 1 Table 1: An example of Chinese spelling errors. Here, “户秃” should be corrected to “糊涂” (confused). Introduction Chinese spelling check (CSC) is an important task which can be utilized in many natural language applications such as optical character recognition (OCR) (Wang et al., 2018; Hong et al., 2019) and essay scoring. Meanwhile, CSC is a challenging task which requires human-level natural language understanding ability (Liu et al., 2010, 2013; Xin et al., 2014). Recently, BERT-based non-autoregressive language models have achieved state-of-the-art performance in the CSC task (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020). These works fine-tune BERT-based models using CSC training data. During the training phase, all the target Chinese characters will be involved as labels. In the inference stage, the models predict the most likely Chinese character from a candidate set at each position. When the most likely character is different from the input character, the input character will be considered as a spelling erro"
2021.findings-acl.216,W13-4409,0,0.0302237,"Missing"
2021.findings-acl.216,2021.ccl-1.108,0,0.041413,"Missing"
2021.findings-acl.216,D19-1510,0,0.0126642,"nese spelling errors, while GEC also includes errors that need insertion and deletion. Most models in the GEC task use an autoregressive Seq2Seq model to correct a sentence. Similarly, Seq2Seq models can also be used in the CSC task. Wang et al. (2019) propose an autoregressive pointer network which generates a Chinese character from the confusion set rather than the entire vocabulary. Although the autoregressive Seq2Seq model has the ability to correct the spelling errors, it is usually slow. The input and output are so similar that it would be “wasteful” to completely regenerate a sequence (Malmi et al., 2019). Since the input and output have the same number of Chinese characters, and the correct and incorrect Chinese characters correspond to each other, it is more intuitive to use non-autoregressive language models such as BERT to directly correct the Chinese spelling errors. Hong et al. (2019) propose the FASPell model to predict candidate characters based on the BERT model and exploit the phonological and visual similarity information to select candidate characters. Zhang et al. (2020) propose a model named Soft-Masked BERT, which consists of a detection network and a correction network based on"
2021.findings-acl.216,D18-1273,0,0.0447473,"generates the candidate Chinese characters via a Pinyin Enhanced Candidate Generator and then utilizes an attention-based network to model the dependencies between two adjacent Chinese characters. The experimental results show that our proposed method achieves a new state-ofthe-art performance on three human-annotated datasets. 1 Table 1: An example of Chinese spelling errors. Here, “户秃” should be corrected to “糊涂” (confused). Introduction Chinese spelling check (CSC) is an important task which can be utilized in many natural language applications such as optical character recognition (OCR) (Wang et al., 2018; Hong et al., 2019) and essay scoring. Meanwhile, CSC is a challenging task which requires human-level natural language understanding ability (Liu et al., 2010, 2013; Xin et al., 2014). Recently, BERT-based non-autoregressive language models have achieved state-of-the-art performance in the CSC task (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020). These works fine-tune BERT-based models using CSC training data. During the training phase, all the target Chinese characters will be involved as labels. In the inference stage, the models predict the most likely Chinese character from a"
2021.findings-acl.216,P19-1578,0,0.0124904,"ets. For reproducibility, our code for this paper is available at https://github.com/destwang/DCN. 2 Related Work ability. With the development of deep learning techniques, the CSC task has recently made more progress. CSC is similar to the grammatical error correction (GEC) task (Dahlmeier and Ng, 2012). The difference between them is that CSC only focuses on Chinese spelling errors, while GEC also includes errors that need insertion and deletion. Most models in the GEC task use an autoregressive Seq2Seq model to correct a sentence. Similarly, Seq2Seq models can also be used in the CSC task. Wang et al. (2019) propose an autoregressive pointer network which generates a Chinese character from the confusion set rather than the entire vocabulary. Although the autoregressive Seq2Seq model has the ability to correct the spelling errors, it is usually slow. The input and output are so similar that it would be “wasteful” to completely regenerate a sequence (Malmi et al., 2019). Since the input and output have the same number of Chinese characters, and the correct and incorrect Chinese characters correspond to each other, it is more intuitive to use non-autoregressive language models such as BERT to direct"
2021.findings-acl.216,W13-4406,0,0.0507596,"Missing"
2021.findings-acl.216,W14-6825,0,0.0130065,"characters. The experimental results show that our proposed method achieves a new state-ofthe-art performance on three human-annotated datasets. 1 Table 1: An example of Chinese spelling errors. Here, “户秃” should be corrected to “糊涂” (confused). Introduction Chinese spelling check (CSC) is an important task which can be utilized in many natural language applications such as optical character recognition (OCR) (Wang et al., 2018; Hong et al., 2019) and essay scoring. Meanwhile, CSC is a challenging task which requires human-level natural language understanding ability (Liu et al., 2010, 2013; Xin et al., 2014). Recently, BERT-based non-autoregressive language models have achieved state-of-the-art performance in the CSC task (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020). These works fine-tune BERT-based models using CSC training data. During the training phase, all the target Chinese characters will be involved as labels. In the inference stage, the models predict the most likely Chinese character from a candidate set at each position. When the most likely character is different from the input character, the input character will be considered as a spelling error and corrected to the mo"
2021.findings-acl.216,2020.acl-main.82,0,0.105649,"1: An example of Chinese spelling errors. Here, “户秃” should be corrected to “糊涂” (confused). Introduction Chinese spelling check (CSC) is an important task which can be utilized in many natural language applications such as optical character recognition (OCR) (Wang et al., 2018; Hong et al., 2019) and essay scoring. Meanwhile, CSC is a challenging task which requires human-level natural language understanding ability (Liu et al., 2010, 2013; Xin et al., 2014). Recently, BERT-based non-autoregressive language models have achieved state-of-the-art performance in the CSC task (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020). These works fine-tune BERT-based models using CSC training data. During the training phase, all the target Chinese characters will be involved as labels. In the inference stage, the models predict the most likely Chinese character from a candidate set at each position. When the most likely character is different from the input character, the input character will be considered as a spelling error and corrected to the most likely character. Based on the powerful generalization ability of BERT (Devlin et al., 2019), these works have achieved better performance than other mo"
2021.findings-acl.282,D19-1649,0,0.0227033,"tly learns the intent and slot tasks by sharing the BERT encoder. LD-Proto is also a prototypical model similar to JointProto. The only difference is that it is enhanced by the logits-dependency tricks (Goo et al., 2018), where joint learning is achieved by depending on the intent and slot prediction on the logits of the accompanying task. Implements For both ours and baseline models, we determine the hyperparameters on the development set. We use ADAM (Kingma and Ba, 2015) for training and set batch size as 4 and learning rate 3195 as 10−5 . We adopt embedding tricks of Pairs-Wise Embedding (Gao et al., 2019; Hou et al., 2020a) and Gradual Unfreezing (Howard and Ruder, 2018). The λ and α in Section 3.2 are both set as 0.5. We implement both our and baseline models with the few-shot platform MetaDialog.5 Besides, to use the information in target domains and make a fair comparison with fine-tuning baselines, we explore the performance of the similarity-based model under fine-tuning setting (+FT) and enhance the model with a fine-tune process similar to Meta-JOSFIN. In addition, following the suggestions of Hou et al. (2020a), we investigate adding Transition Rules (+TR) between slot tags, which ban"
2021.findings-acl.282,N19-1423,0,0.0298506,"her crossattention scores. For example, “PlayVideo” and “film” are more related, so the corresponding score is larger. x being associated with intent label li as: p(li |x, S) exp (S IM(Eintent (x), Cintenti )) , =P j exp (S IM (Eintent (x), Cintentj )) and estimates the probability of the kth token in x belonging to the ith slot class as: p(ti |k, x, S) exp (S IM(Eslot (xk ), Csloti )) , =P j exp (S IM (Eslot (xk ), Cslotj )) where Cintenti and Csloti are prototypes derived with support examples. Eintent (·) and Eslot (·) are embedder functions for intent and slot respectively. We adopt BERT (Devlin et al., 2019) as the embedder, and the sentence embedding Eintent (x) is calculated as the averaged embedding of its tokens. We use the dot-product similarity for function S IM(·, ·). 3.2 Proposed Method In this section, we introduce the proposed Contrastive Prototype Merging network (ConProm). Firstly, we describe the few-shot intent detection and slot filling with Prototypical network (§3.1). Based on that, we present two key components of ConProm: the Prototype Merging mechanism that adaptively connects two metric spaces of intent and slot (§3.2) and the Contrastive Alignment Learning that jointly refin"
2021.findings-acl.282,2020.coling-main.438,0,0.0330009,"d by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on joint dialogue understanding (Goo et al., 2018; Li et al., 2018; Zhang et al., 2019; Qin et al., 2019; Wang et al., 2018; E et al., 2019; Wu et al., 2020; Gangadharaiah and Narayanaswamy, 2019; Liu et al., 2019; Qin et al.,"
2021.findings-acl.282,P19-1544,0,0.0471014,"Missing"
2021.findings-acl.282,D19-1403,0,0.0136268,"ntence Level Slot Accuracy, which considers a sentence to be correct when all slots are correct. As shown in Table 4, there is a huge gap in the slot accuracy score between LD-Proto and ConProm, which explains the gap in Joint score. 5 Related Work Few-shot learning is one of the most important direction for machine learning area (Fei-Fei, 2006; Fink, 2004) and often achieved by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explo"
2021.findings-acl.282,N18-2118,0,0.370359,"ery Example: Play The Lord of the Rings on my mobile. Few-Shot Learning (FSL) that committed to learning new problems with only a few examples (Miller et al., 2000; Vinyals et al., 2016) is promising to break the data-shackles of current deep learning. Commonly, existing FSL methods learn a single few-shot task each time. But, real-world applications, such as dialogue language understanding, usually contain multiple closely related tasks (e.g., intent detection and slot filling) and often benefit from jointly learning these tasks (Worsham and Kalita, 2020; Chen et al., 2019; Qin et al., 2019; Goo et al., 2018). In few-shot scenarios, such requirements of joint learning present new challenges for FSL techniques to capture task relations from only a few examples and jointly learn multiple tasks. † Where can I buy face masks product nearby? |intent: FindShop Find the nearest barbecue food. |intent: FindRestaurant Introduction * Equal Support Examples: Figure 1: Examples of the few-shot joint dialogue language understanding. On each domain, given a few labeled support examples, the model predicts the intent and slot labels for unseen query examples. Joint learning benefits from capturing the relation b"
2021.findings-acl.282,2020.acl-main.128,1,0.873147,"Cintenti , i F F where Cintent and Cslot are the fused prototypes i j of ith intent and the jth slot respectively. At last, we obtain the representation of merged prototypes C 0 by combining the origin prototype 1 C with the fused prototype C F : 0 F Cintent = α × Cintent + (1 − α) × Cintent , 0 F Cslot = α × Cslot + (1 − α) × Cslot , where the α is a hyper-parameter that controls the importance of intent-slot relation. 3.3 Contrastive Alignment Learning Similarity-based few-shot learning relies heavily on a good metric space, where different classes should be well separated from each other (Hou et al., 2020a; Yoon et al., 2019). In joint-learning scenarios, there are further requests to connect metric spaces of joint learned tasks and jointly optimize these metric spaces. In response to the above requests, we argue that the distribution of prototypes of dialogue language understanding should fit these intuitions: (1) different intent prototypes should be far away and the same as slot prototypes (Intra-Contrastive); (2) the slot prototypes should close to the related intent prototypes and should be far away from the unrelated intent prototypes (Inter-Contrastive).2 To achieve these, we introduce"
2021.findings-acl.282,C18-1105,1,0.816207,"to and ConProm, which explains the gap in Joint score. 5 Related Work Few-shot learning is one of the most important direction for machine learning area (Fei-Fei, 2006; Fink, 2004) and often achieved by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on joint dialogue under"
2021.findings-acl.282,P18-1031,0,0.0206039,"der. LD-Proto is also a prototypical model similar to JointProto. The only difference is that it is enhanced by the logits-dependency tricks (Goo et al., 2018), where joint learning is achieved by depending on the intent and slot prediction on the logits of the accompanying task. Implements For both ours and baseline models, we determine the hyperparameters on the development set. We use ADAM (Kingma and Ba, 2015) for training and set batch size as 4 and learning rate 3195 as 10−5 . We adopt embedding tricks of Pairs-Wise Embedding (Gao et al., 2019; Hou et al., 2020a) and Gradual Unfreezing (Howard and Ruder, 2018). The λ and α in Section 3.2 are both set as 0.5. We implement both our and baseline models with the few-shot platform MetaDialog.5 Besides, to use the information in target domains and make a fair comparison with fine-tuning baselines, we explore the performance of the similarity-based model under fine-tuning setting (+FT) and enhance the model with a fine-tune process similar to Meta-JOSFIN. In addition, following the suggestions of Hou et al. (2020a), we investigate adding Transition Rules (+TR) between slot tags, which bans illegal slot prediction, such as “I” tag after “O” tag. 4.3 Main R"
2021.findings-acl.282,2020.findings-emnlp.163,1,0.79015,"Missing"
2021.findings-acl.282,2020.nlp4convai-1.12,0,0.119962,"he loss for intent detection and slot filling. Combining with the loss of Contrastive Alignment Learning, we train the entire model with the following objective function: Lall = CEintent + CEslot + LContrastive 4 Experiments We evaluate our method on the dialogue language understanding task of 1-shot/5-shot setting, which transfers knowledge from source domains (training) to an unseen target domain (testing) containing only 1-shot/5-shot support set. 4.1 Few-shot Dataset Construction To simulate the few-shot learning situation, we follow previous few-shot learning works (Vinyals et al., 2016; Krone et al., 2020a; Finn et al., 2017) and construct the dataset into a few-shot episode style, where the model is trained and evaluated with a series of few-shot episodes. Each episode contains a support set and query set. However, different from the single-task problem, joint-learning examples are associated with multiple labels. Therefore, we cannot guarantee that each label appears K times while sampling examples for the K-shot support set. To remedy this, we build support sets with the Mini-Including Algorithm (Hou et al., 2020a), which is intended for such situations. It constructs support set generally"
2021.findings-acl.282,D17-1035,0,0.0217313,"etrics for evaluation: Intent Accuracy, Slot F1-score, Joint Accuracy.4 For joint dialogue language understanding, Joint Accuracy is the most important metric among all three metrics (Hou et al., 2020c). It evaluates the sentence level accuracy, which considers one sentence is correct only when all its slots and intents are correct. To conduct a robust evaluation under few-shot setting, we validate the models on multiple fewshot episodes (i.e., support-query set pairs) from different domains and take the average score as final results. To control the non-deterministic neural network training (Reimers and Gurevych, 2017), we report the average score of 5 random seeds for all results. 4.2 Baselines We compare our model with two kinds of strong baseline: fine-tune based transfer learning methods 4 We calculate the Slot F1-score with the conlleval script https://www.clips.uantwerpen.be/ conll2000/chunking/conlleval.txt 3194 Snips Models FewJoint Intent Acc. Slot F1 Joint Acc. Intent Acc. Slot F1 Joint Acc. SepProto JointProto LD-Proto LD-Proto+TR ConProm (Ours) ConProm+TR (Ours) 98.23±0.66 92.57±0.57 97.25±0.71 97.53±0.30 96.67±1.45 96.17±0.76 43.90±1.98 42.63±2.03 47.81±2.53 51.03±2.40 53.05±0.81 55.84±0.85 9.4"
2021.findings-acl.282,P19-1547,0,0.0563973,"Missing"
2021.findings-acl.282,D18-1417,0,0.0286999,"al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on joint dialogue understanding (Goo et al., 2018; Li et al., 2018; Zhang et al., 2019; Qin et al., 2019; Wang et al., 2018; E et al., 2019; Wu et al., 2020; Gangadharaiah and Narayanaswamy, 2019; Liu et al., 2019; Qin et al., 2020), few-shot joint dialogue understanding is less investigated. Krone et al. (2020b) and Bhathiya and Thayasivam (2020) make the earliest attempts by directly adopt general and classic few-shot learning methods such as MAML and prototypical network. These methods achieve joint learning by sharing the embedding between intent detection and slot filling task, which model the relation between intent and slot task implicitly. By contras"
2021.findings-acl.282,D19-1097,0,0.0138918,"enario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on joint dialogue understanding (Goo et al., 2018; Li et al., 2018; Zhang et al., 2019; Qin et al., 2019; Wang et al., 2018; E et al., 2019; Wu et al., 2020; Gangadharaiah and Narayanaswamy, 2019; Liu et al., 2019; Qin et al., 2020), few-shot joint dialogue understanding is less investigated. Krone et al. (2020b) and Bhathiya and Thayasivam (2020) make the earliest attempts by directly adopt general and classic few-shot learning methods such as MAML and prototypical network. These methods achieve joint learning by sharing the embedding between intent detection and slot filling task, which model the relation between intent and slot task implicitly. By contrast, we explicitly model the interaction between intent and slot with attentive information fusion and constrastive loss. Experiment results also dem"
2021.findings-acl.282,2020.acl-main.3,0,0.0131908,"Related Work Few-shot learning is one of the most important direction for machine learning area (Fei-Fei, 2006; Fink, 2004) and often achieved by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on joint dialogue understanding (Goo et al., 2018; Li et al., 2018; Zhang et al."
2021.findings-acl.282,P18-1194,0,0.021577,"ore between LD-Proto and ConProm, which explains the gap in Joint score. 5 Related Work Few-shot learning is one of the most important direction for machine learning area (Fei-Fei, 2006; Fink, 2004) and often achieved by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on jo"
2021.findings-acl.282,D19-1334,0,0.0271581,"correct. As shown in Table 4, there is a huge gap in the slot accuracy score between LD-Proto and ConProm, which explains the gap in Joint score. 5 Related Work Few-shot learning is one of the most important direction for machine learning area (Fei-Fei, 2006; Fink, 2004) and often achieved by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold."
2021.findings-acl.282,D19-1214,1,0.914392,"tent: PlayVoice Query Example: Play The Lord of the Rings on my mobile. Few-Shot Learning (FSL) that committed to learning new problems with only a few examples (Miller et al., 2000; Vinyals et al., 2016) is promising to break the data-shackles of current deep learning. Commonly, existing FSL methods learn a single few-shot task each time. But, real-world applications, such as dialogue language understanding, usually contain multiple closely related tasks (e.g., intent detection and slot filling) and often benefit from jointly learning these tasks (Worsham and Kalita, 2020; Chen et al., 2019; Qin et al., 2019; Goo et al., 2018). In few-shot scenarios, such requirements of joint learning present new challenges for FSL techniques to capture task relations from only a few examples and jointly learn multiple tasks. † Where can I buy face masks product nearby? |intent: FindShop Find the nearest barbecue food. |intent: FindRestaurant Introduction * Equal Support Examples: Figure 1: Examples of the few-shot joint dialogue language understanding. On each domain, given a few labeled support examples, the model predicts the intent and slot labels for unseen query examples. Joint learning benefits from captu"
2021.findings-acl.282,D19-1045,0,0.0235225,"evaluating the Sentence Level Slot Accuracy, which considers a sentence to be correct when all slots are correct. As shown in Table 4, there is a huge gap in the slot accuracy score between LD-Proto and ConProm, which explains the gap in Joint score. 5 Related Work Few-shot learning is one of the most important direction for machine learning area (Fei-Fei, 2006; Fink, 2004) and often achieved by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou e"
2021.findings-acl.282,N18-2050,0,0.0137719,"a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on joint dialogue understanding (Goo et al., 2018; Li et al., 2018; Zhang et al., 2019; Qin et al., 2019; Wang et al., 2018; E et al., 2019; Wu et al., 2020; Gangadharaiah and Narayanaswamy, 2019; Liu et al., 2019; Qin et al., 2020), few-shot joint dialogue understanding is less investigated. Krone et al. (2020b) and Bhathiya and Thayasivam (2020) make the earliest attempts by directly adopt general and classic few-shot learning methods such as MAML and prototypical network. These methods achieve joint learning by sharing the embedding between intent detection and slot filling task, which model the relation between intent and slot task implicitly. By contrast, we explicitly model the interaction between intent and"
2021.findings-acl.282,2020.emnlp-main.152,0,0.0310237,"Missing"
2021.findings-acl.282,D18-1348,0,0.0283958,"2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on joint dialogue understanding (Goo et al., 2018; Li et al., 2018; Zhang et al., 2019; Qin et al., 2019; Wang et al., 2018; E et al., 2019; Wu et al., 2020; Gangadharaiah and Narayanaswamy, 2019; Liu et al., 2019; Qin et al., 2020), few-shot joint dialogue understand"
2021.findings-acl.282,P19-1277,0,0.0267868,"e is a huge gap in the slot accuracy score between LD-Proto and ConProm, which explains the gap in Joint score. 5 Related Work Few-shot learning is one of the most important direction for machine learning area (Fei-Fei, 2006; Fink, 2004) and often achieved by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a sing"
2021.findings-acl.282,N18-1109,0,0.0174798,"siders a sentence to be correct when all slots are correct. As shown in Table 4, there is a huge gap in the slot accuracy score between LD-Proto and ConProm, which explains the gap in Joint score. 5 Related Work Few-shot learning is one of the most important direction for machine learning area (Fei-Fei, 2006; Fink, 2004) and often achieved by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent dete"
2021.findings-acl.282,P19-1519,0,0.0195517,"t al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on joint dialogue understanding (Goo et al., 2018; Li et al., 2018; Zhang et al., 2019; Qin et al., 2019; Wang et al., 2018; E et al., 2019; Wu et al., 2020; Gangadharaiah and Narayanaswamy, 2019; Liu et al., 2019; Qin et al., 2020), few-shot joint dialogue understanding is less investigated. Krone et al. (2020b) and Bhathiya and Thayasivam (2020) make the earliest attempts by directly adopt general and classic few-shot learning methods such as MAML and prototypical network. These methods achieve joint learning by sharing the embedding between intent detection and slot filling task, which model the relation between intent and slot task implicitly. By contrast, we explicitly mod"
2021.mrqa-1.10,2021.naacl-main.280,0,0.0335491,"ked in the sentence, training like Translation Language Model (TLM) WEAM takes the representation of the masked ap(Conneau and Lample, 2019). In the MMLM, the ple and Apfel for multilingual prediction and crossmodel predicts the masked tokens with the mono- lingual prediction respectively to recover the origilingual context; in the TLM, the model can attend nal word apple. to both the contexts in the source language and Through the two ways of prediction, both the target language. Variations of TLM model can be contextual representations from the last transformer found in Huang et al. (2019); Chi et al. (2021); layer and the static embeddings from the embedOuyang et al. (2020). ding layer can be aligned. We evaluated our method While it is possible for the model to learn the on the word-level machine reading comprehension alignment knowledge by itself, some works have task MLQA (Lewis et al., 2019) and the sentence∗ Equal contribution. level classification task XNLI (Conneau et al., 100 Proceedings of the 3rd Workshop on Machine Reading for Question Answering , pages 100–105 Nov 10th, 2021 ©2021 Association for Computational Linguistics Bilingual Parallel Sentences [CLS] Is it raining today Word Al"
2021.mrqa-1.10,2020.acl-main.747,0,0.0255275,"ion. neau et al., 2020) have shown significant effective- The multilingual prediction task predicts the origness in transfer learning on various cross-lingual inal masked word in the standard way. while the tasks. The pre-training methods of the multilin- cross-lingual task predicts the corresponding word gual language models can be divided into two from the representations in the other language. For groups: unsupervised pre-training like Multilingual example, if the words apple and Apfel (German Masked Language Model (MMLM) (Devlin et al., for apple) appear in the the English–German par2019; Conneau et al., 2020), and supervised pre- allel sentence and apple is masked in the sentence, training like Translation Language Model (TLM) WEAM takes the representation of the masked ap(Conneau and Lample, 2019). In the MMLM, the ple and Apfel for multilingual prediction and crossmodel predicts the masked tokens with the mono- lingual prediction respectively to recover the origilingual context; in the TLM, the model can attend nal word apple. to both the contexts in the source language and Through the two ways of prediction, both the target language. Variations of TLM model can be contextual representations fro"
2021.mrqa-1.10,D18-1269,0,0.0638705,"Missing"
2021.mrqa-1.10,N19-1423,0,0.665077,"nd the contextual lingual machine reading comprehension task MLQA and natural language interface task representations of different languages in the multiXNLI. The results show that WEAM can siglingual pre-trained models. nificantly improve the zero-shot performance. Specifically, in the pre-training stage, we first use FastAlign to identify bilingual word pairs in parallel bilingual sentence pairs as our prior knowl1 Introduction edge. Then we randomly mask some tokens in the Large-scale multilingual pre-trained language mod- bilingual sentence pairs. For each masked token, els such as mBERT (Devlin et al., 2019), XLM WEAM performs two kinds of predictions: a mul(Conneau and Lample, 2019) and XLM-R (Con- tilingual prediction and a cross-lingual prediction. neau et al., 2020) have shown significant effective- The multilingual prediction task predicts the origness in transfer learning on various cross-lingual inal masked word in the standard way. while the tasks. The pre-training methods of the multilin- cross-lingual task predicts the corresponding word gual language models can be divided into two from the representations in the other language. For groups: unsupervised pre-training like Multilingual ex"
2021.mrqa-1.10,D19-1252,0,0.0450842,"Missing"
2021.mrqa-1.10,2021.emnlp-main.3,0,0.0538111,"Missing"
2021.starsem-1.30,D17-1215,0,0.0672231,"Missing"
2021.starsem-1.30,2020.acl-main.197,0,0.0289178,"Missing"
2021.starsem-1.30,D17-1082,0,0.160345,"2020; Jiang et al., 2019; Pereira et al., 2020; Liu et al., 2020). They mostly adopt a general AT strategy, but focus less on the adaptation of AT to NLP tasks. To explore this adaptation, in this work, we aim to apply adversarial training on machine reading comprehension (MRC) tasks, which exhibit complex NLP characteristics. The objective of MRC is to let a machine read the given passages and ask it to answer the related questions. There are several types of MRC tasks. In this work we focus on span-based extractive RC (Rajpurkar et al., 2016, 2018; Yang et al., 2018) and multiple-choice RC (Lai et al., 2017). To apply adversarial training on MRC tasks, we notice that there are several salient characteristics of MRC compared to other tasks such as image classification: (1) The inputs are discrete. Unlike pixels, which can take continuous values, words are discrete tokens. (2) The tokens in the input sequences are not independent. A word may occur in an input sequence several times. After the embedding layer, these occurrences are represented by the word vectors with the same value and hold the same semantic meaning (although the word may be polysemous). (3) The roles of passages and questions are"
2021.starsem-1.30,2021.ccl-1.108,0,0.0274086,"Missing"
2021.starsem-1.30,D18-1307,0,0.0170897,". Introduction Neural networks have achieved superior performance on many tasks, but they are vulnerable to adversarial examples (Szegedy et al., 2014) – examples that have been mixed with certain perturbations. Adversarial training (AT) (Goodfellow et al., 2015) uses both clean and adversarial examples to improve the robustness of the model for image classification. In the field of NLP, Miyato et al. (2017) have applied adversarial training on text classification tasks and improved the model performance. From then on, many AT methods has been proposed (Wu et al., 2017; Yasunaga et al., 2018; Bekoulis et al., 2018; Zhu et al., 2020; Jiang et al., 2019; Pereira et al., 2020; Liu et al., 2020). They mostly adopt a general AT strategy, but focus less on the adaptation of AT to NLP tasks. To explore this adaptation, in this work, we aim to apply adversarial training on machine reading comprehension (MRC) tasks, which exhibit complex NLP characteristics. The objective of MRC is to let a machine read the given passages and ask it to answer the related questions. There are several types of MRC tasks. In this work we focus on span-based extractive RC (Rajpurkar et al., 2016, 2018; Yang et al., 2018) and multip"
2021.starsem-1.30,2020.repl4nlp-1.8,0,0.0185849,"ance on many tasks, but they are vulnerable to adversarial examples (Szegedy et al., 2014) – examples that have been mixed with certain perturbations. Adversarial training (AT) (Goodfellow et al., 2015) uses both clean and adversarial examples to improve the robustness of the model for image classification. In the field of NLP, Miyato et al. (2017) have applied adversarial training on text classification tasks and improved the model performance. From then on, many AT methods has been proposed (Wu et al., 2017; Yasunaga et al., 2018; Bekoulis et al., 2018; Zhu et al., 2020; Jiang et al., 2019; Pereira et al., 2020; Liu et al., 2020). They mostly adopt a general AT strategy, but focus less on the adaptation of AT to NLP tasks. To explore this adaptation, in this work, we aim to apply adversarial training on machine reading comprehension (MRC) tasks, which exhibit complex NLP characteristics. The objective of MRC is to let a machine read the given passages and ask it to answer the related questions. There are several types of MRC tasks. In this work we focus on span-based extractive RC (Rajpurkar et al., 2016, 2018; Yang et al., 2018) and multiple-choice RC (Lai et al., 2017). To apply adversarial traini"
2021.starsem-1.30,P18-2124,0,0.0345611,"Missing"
2021.starsem-1.30,D16-1264,0,0.333566,"et al., 2017; Yasunaga et al., 2018; Bekoulis et al., 2018; Zhu et al., 2020; Jiang et al., 2019; Pereira et al., 2020; Liu et al., 2020). They mostly adopt a general AT strategy, but focus less on the adaptation of AT to NLP tasks. To explore this adaptation, in this work, we aim to apply adversarial training on machine reading comprehension (MRC) tasks, which exhibit complex NLP characteristics. The objective of MRC is to let a machine read the given passages and ask it to answer the related questions. There are several types of MRC tasks. In this work we focus on span-based extractive RC (Rajpurkar et al., 2016, 2018; Yang et al., 2018) and multiple-choice RC (Lai et al., 2017). To apply adversarial training on MRC tasks, we notice that there are several salient characteristics of MRC compared to other tasks such as image classification: (1) The inputs are discrete. Unlike pixels, which can take continuous values, words are discrete tokens. (2) The tokens in the input sequences are not independent. A word may occur in an input sequence several times. After the embedding layer, these occurrences are represented by the word vectors with the same value and hold the same semantic meaning (although the w"
2021.starsem-1.30,2020.emnlp-main.583,1,0.696811,"amUpdate({θ, E}, gK ) end Experiments Setup Datasets. We perform experiments on several English MRC tasks, including span-based extractive MRC tasks – SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), HotpotQA (Yang et al., 2018), and multiple-choice MRC task RACE (Lai et al., 2017). We also test model robustness on the adversarial datasets AddSent andAddOneSent (Jia and Liang, 2017). Model Settings. We build the MRC model with RoBERTa (Liu et al., 2019), following the standard model structure for SQuAD and RACE (Devlin et al., 2018). For HotpotQA, we follow the model in Shao et al. (2020). It uses RoBERTa as the encoder followed by a multi-task prediction layer. We denote the passage as P and the question as Q. To construct the inputs, for span-based extractive RC, we concatenate each P and Q with modeldependent special tokens; for multiple-choice RC with m options for each example, we append each option to the concatenation of P and Q, and construct m input sequences from each example. When applying AT or PQAT, we only perturb 310 Model SQuAD 1.1 EM F1 SQuAD 2.0 EM F1 HotpotQA joint EM joint F1 RACE Acc BASE setting RoBERTa PQAT 84.72 85.87 91.54 92.33 79.77 81.66 83.18 84.79"
2021.starsem-1.30,D17-1187,0,0.0306079,"bed differently depending on their roles. Introduction Neural networks have achieved superior performance on many tasks, but they are vulnerable to adversarial examples (Szegedy et al., 2014) – examples that have been mixed with certain perturbations. Adversarial training (AT) (Goodfellow et al., 2015) uses both clean and adversarial examples to improve the robustness of the model for image classification. In the field of NLP, Miyato et al. (2017) have applied adversarial training on text classification tasks and improved the model performance. From then on, many AT methods has been proposed (Wu et al., 2017; Yasunaga et al., 2018; Bekoulis et al., 2018; Zhu et al., 2020; Jiang et al., 2019; Pereira et al., 2020; Liu et al., 2020). They mostly adopt a general AT strategy, but focus less on the adaptation of AT to NLP tasks. To explore this adaptation, in this work, we aim to apply adversarial training on machine reading comprehension (MRC) tasks, which exhibit complex NLP characteristics. The objective of MRC is to let a machine read the given passages and ask it to answer the related questions. There are several types of MRC tasks. In this work we focus on span-based extractive RC (Rajpurkar et"
2021.starsem-1.30,D18-1259,0,0.0870263,", 2018; Bekoulis et al., 2018; Zhu et al., 2020; Jiang et al., 2019; Pereira et al., 2020; Liu et al., 2020). They mostly adopt a general AT strategy, but focus less on the adaptation of AT to NLP tasks. To explore this adaptation, in this work, we aim to apply adversarial training on machine reading comprehension (MRC) tasks, which exhibit complex NLP characteristics. The objective of MRC is to let a machine read the given passages and ask it to answer the related questions. There are several types of MRC tasks. In this work we focus on span-based extractive RC (Rajpurkar et al., 2016, 2018; Yang et al., 2018) and multiple-choice RC (Lai et al., 2017). To apply adversarial training on MRC tasks, we notice that there are several salient characteristics of MRC compared to other tasks such as image classification: (1) The inputs are discrete. Unlike pixels, which can take continuous values, words are discrete tokens. (2) The tokens in the input sequences are not independent. A word may occur in an input sequence several times. After the embedding layer, these occurrences are represented by the word vectors with the same value and hold the same semantic meaning (although the word may be polysemous). (3"
2021.starsem-1.30,N18-1089,0,0.0193435,"epending on their roles. Introduction Neural networks have achieved superior performance on many tasks, but they are vulnerable to adversarial examples (Szegedy et al., 2014) – examples that have been mixed with certain perturbations. Adversarial training (AT) (Goodfellow et al., 2015) uses both clean and adversarial examples to improve the robustness of the model for image classification. In the field of NLP, Miyato et al. (2017) have applied adversarial training on text classification tasks and improved the model performance. From then on, many AT methods has been proposed (Wu et al., 2017; Yasunaga et al., 2018; Bekoulis et al., 2018; Zhu et al., 2020; Jiang et al., 2019; Pereira et al., 2020; Liu et al., 2020). They mostly adopt a general AT strategy, but focus less on the adaptation of AT to NLP tasks. To explore this adaptation, in this work, we aim to apply adversarial training on machine reading comprehension (MRC) tasks, which exhibit complex NLP characteristics. The objective of MRC is to let a machine read the given passages and ask it to answer the related questions. There are several types of MRC tasks. In this work we focus on span-based extractive RC (Rajpurkar et al., 2016, 2018; Yang e"
C10-1019,W09-1207,1,0.940573,"at the word at position w has the sense s. For semantic role labeling, the predicate role(p, a, r) is defined as mentioned in above. Different from Meza-Ruiz and Riedel (2009), which only used sense number as word sense representation, we use a triple (lemma, part-ofspeech, sense num) to represent the word sense s. For example, (hit, v, 01) denotes that the verb “hit” has sense number 01. Obviously, our representation can distinguish different word senses which have the identical sense number. In addition, we use one argument classification stage with predicate role to label semantic roles as Che et al. (2009). Similarly, no argument identification stage is used in our model. The approach can improve the recall of the system. In addition to the hidden predicates, we define observable predicates to represent the information available in the corpus. Table 1 presents these predicates. 4.1 Local Formula A local formula means that its groundings relate any number of observed ground atoms to exactly one hidden ground atom. For example lemma(p, +l1 )∧lemma(a, +l2 ) ⇒ role(p, a, +r) 163 Predicates word(i, w) pos(i, t) lemma(i, l) chdpos(i, t) chddep(i, d) f irstLemma(i, l) lastLemma(i, l) posF rame(i, f r)"
C10-1019,N10-1030,1,0.878373,"9) have joined semantic role labeling and predicate senses disambiguation with Markov logic. The above idea, that the predicate senses and the semantic role labeling can help each other, may be inspired by Hajiˇc et al. (2009), Surdeanu et al. (2008), and Dang and Palmer (2005). They have shown that semantic role features are helpful to disambiguate verb senses and vice versa. Besides predicate senses, Dahlmeier et al. (2009) proposed a joint model to maximize probability of the preposition senses and the semantic role of prepositional phrases. 162 Except for predicate and preposition senses, Che et al. (2010) explored all word senses for semantic role labeling. They showed that all word senses can improve the semantic role labeling performance significantly. However, the golden word senses were used in their experiments. The results are still unknown when an automatic word sense disambiguation system is used. In this paper, we not only use all word senses disambiguated by an automatic system, but also make the semantic role labeling results to help word sense disambiguation synchronously with a joint model. 3 Markov Logic Markov logic can be understood as a knowledge representation with a weight a"
C10-1019,P09-1058,0,0.0396312,"he verb sense disambiguation (Dang and Palmer, 2005). More people used predicate senses in semantic role labeling (Hajiˇc et al., 2009; Surdeanu et al., 2008). However, both of the pipeline methods ignore possible dependencies between the word senses and semantic roles, and can result in the error propagation problem. The same problem also appears in other natural language processing tasks. In order to make different natural language processing tasks be able to help each other, jointly modeling methods become popular recently, such as joint Chinese word segmentation and part-ofspeech tagging (Kruengkrai et al., 2009; Zhang and Clark, 2008; Jiang et al., 2008), joint lemmatization and part-of-speech prediction (Toutanova and Cherry, 2009), joint morphological segmentation and syntactic parsing (Goldberg and Tsarfaty, 2008), joint text and aspect ratings for sentiment summarization (Titov and McDonald, 2008), and joint parsing and named entity recognition (Finkel and Manning, 2009). For semantic role labeling, Dahlmeier et al. (2009) proposed a method to maximize the joint probability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169,"
C10-1019,N09-1018,0,0.294815,"and McDonald, 2008), and joint parsing and named entity recognition (Finkel and Manning, 2009). For semantic role labeling, Dahlmeier et al. (2009) proposed a method to maximize the joint probability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169, Beijing, August 2010 tic role of preposition phrases and the preposition sense. In order to do better joint learning, a novel statistical relational learning framework, Markov logic (Domingos and Lowd, 2009) was introduced to join semantic role labeling and predicate senses (Meza-Ruiz and Riedel, 2009). Markov logic combines the first order logic and Markov networks, to develop a joint probability model over all related rules. Global constraints (introduced by Punyakanok et al. (2008)) among semantic roles can be easily added into Markov logic. And the more important, the jointly modeling can be realized using Markov logic naturally. Besides predicates and prepositions, other word senses are also important information for recognizing semantic roles. For example, if we know “cat” is an “agent” of the predicate “hit” in a sentence, we can guess that “dog” can also be an “agent” of “hit”, thou"
C10-1019,D08-1068,0,0.0380997,"task (semantic role labeling), their methods have been used to jointly learn for two tasks (semantic role labeling and syntactic parsing). However, it is easy for the re-ranking model to loss the optimal result, if it is not included in the top n results. In addition, the integer linear programming model can only use hard constraints. A lot of engineering work is also required in both models. Recently, Markov logic (Domingos and Lowd, 2009) became a hot framework for joint model. It has been successfully used in temporal relations recognition (Yoshikawa et al., 2009), co-reference resolution (Poon and Domingos, 2008), etc. It is very easy to do joint modeling using Markov logic. The only work is to define relevant formulas. Meza-Ruiz and Riedel (2009) have joined semantic role labeling and predicate senses disambiguation with Markov logic. The above idea, that the predicate senses and the semantic role labeling can help each other, may be inspired by Hajiˇc et al. (2009), Surdeanu et al. (2008), and Dang and Palmer (2005). They have shown that semantic role features are helpful to disambiguate verb senses and vice versa. Besides predicate senses, Dahlmeier et al. (2009) proposed a joint model to maximize"
C10-1019,D09-1047,0,0.0844701,"ural language processing tasks be able to help each other, jointly modeling methods become popular recently, such as joint Chinese word segmentation and part-ofspeech tagging (Kruengkrai et al., 2009; Zhang and Clark, 2008; Jiang et al., 2008), joint lemmatization and part-of-speech prediction (Toutanova and Cherry, 2009), joint morphological segmentation and syntactic parsing (Goldberg and Tsarfaty, 2008), joint text and aspect ratings for sentiment summarization (Titov and McDonald, 2008), and joint parsing and named entity recognition (Finkel and Manning, 2009). For semantic role labeling, Dahlmeier et al. (2009) proposed a method to maximize the joint probability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169, Beijing, August 2010 tic role of preposition phrases and the preposition sense. In order to do better joint learning, a novel statistical relational learning framework, Markov logic (Domingos and Lowd, 2009) was introduced to join semantic role labeling and predicate senses (Meza-Ruiz and Riedel, 2009). Markov logic combines the first order logic and Markov networks, to develop a joint probability model over all related"
C10-1019,J08-2005,0,0.438317,"robability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169, Beijing, August 2010 tic role of preposition phrases and the preposition sense. In order to do better joint learning, a novel statistical relational learning framework, Markov logic (Domingos and Lowd, 2009) was introduced to join semantic role labeling and predicate senses (Meza-Ruiz and Riedel, 2009). Markov logic combines the first order logic and Markov networks, to develop a joint probability model over all related rules. Global constraints (introduced by Punyakanok et al. (2008)) among semantic roles can be easily added into Markov logic. And the more important, the jointly modeling can be realized using Markov logic naturally. Besides predicates and prepositions, other word senses are also important information for recognizing semantic roles. For example, if we know “cat” is an “agent” of the predicate “hit” in a sentence, we can guess that “dog” can also be an “agent” of “hit”, though it does not appear in the training data. Similarly, the semantic role information can also help to disambiguate word senses. In addition, the predicate sense and the argument sense ca"
C10-1019,P05-1006,0,0.376097,"nd the sense label is “hit.01”. The argument headed by the token “cat” at position 1 with sense “feline mammal” (cat.01) is referring to the player (A0), and the argument headed by the token “ball” at position 5 with sense “round object that is hit in games” (ball.01) is referring to the game object (A1) being hit. Normally, semantic role labeling and word sense disambiguation are regarded as two independent tasks, i.e., the word sense information is rarely used in a semantic role labeling system and vice versa. A few researchers have used semantic roles to help the verb sense disambiguation (Dang and Palmer, 2005). More people used predicate senses in semantic role labeling (Hajiˇc et al., 2009; Surdeanu et al., 2008). However, both of the pipeline methods ignore possible dependencies between the word senses and semantic roles, and can result in the error propagation problem. The same problem also appears in other natural language processing tasks. In order to make different natural language processing tasks be able to help each other, jointly modeling methods become popular recently, such as joint Chinese word segmentation and part-ofspeech tagging (Kruengkrai et al., 2009; Zhang and Clark, 2008; Jian"
C10-1019,W08-2121,0,0.0926125,"Missing"
C10-1019,N09-1037,0,0.291477,"nguage processing tasks. In order to make different natural language processing tasks be able to help each other, jointly modeling methods become popular recently, such as joint Chinese word segmentation and part-ofspeech tagging (Kruengkrai et al., 2009; Zhang and Clark, 2008; Jiang et al., 2008), joint lemmatization and part-of-speech prediction (Toutanova and Cherry, 2009), joint morphological segmentation and syntactic parsing (Goldberg and Tsarfaty, 2008), joint text and aspect ratings for sentiment summarization (Titov and McDonald, 2008), and joint parsing and named entity recognition (Finkel and Manning, 2009). For semantic role labeling, Dahlmeier et al. (2009) proposed a method to maximize the joint probability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169, Beijing, August 2010 tic role of preposition phrases and the preposition sense. In order to do better joint learning, a novel statistical relational learning framework, Markov logic (Domingos and Lowd, 2009) was introduced to join semantic role labeling and predicate senses (Meza-Ruiz and Riedel, 2009). Markov logic combines the first order logic and Markov networks,"
C10-1019,P08-1036,0,0.0282814,"rror propagation problem. The same problem also appears in other natural language processing tasks. In order to make different natural language processing tasks be able to help each other, jointly modeling methods become popular recently, such as joint Chinese word segmentation and part-ofspeech tagging (Kruengkrai et al., 2009; Zhang and Clark, 2008; Jiang et al., 2008), joint lemmatization and part-of-speech prediction (Toutanova and Cherry, 2009), joint morphological segmentation and syntactic parsing (Goldberg and Tsarfaty, 2008), joint text and aspect ratings for sentiment summarization (Titov and McDonald, 2008), and joint parsing and named entity recognition (Finkel and Manning, 2009). For semantic role labeling, Dahlmeier et al. (2009) proposed a method to maximize the joint probability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169, Beijing, August 2010 tic role of preposition phrases and the preposition sense. In order to do better joint learning, a novel statistical relational learning framework, Markov logic (Domingos and Lowd, 2009) was introduced to join semantic role labeling and predicate senses (Meza-Ruiz and Riede"
C10-1019,P08-1043,0,0.0380625,"possible dependencies between the word senses and semantic roles, and can result in the error propagation problem. The same problem also appears in other natural language processing tasks. In order to make different natural language processing tasks be able to help each other, jointly modeling methods become popular recently, such as joint Chinese word segmentation and part-ofspeech tagging (Kruengkrai et al., 2009; Zhang and Clark, 2008; Jiang et al., 2008), joint lemmatization and part-of-speech prediction (Toutanova and Cherry, 2009), joint morphological segmentation and syntactic parsing (Goldberg and Tsarfaty, 2008), joint text and aspect ratings for sentiment summarization (Titov and McDonald, 2008), and joint parsing and named entity recognition (Finkel and Manning, 2009). For semantic role labeling, Dahlmeier et al. (2009) proposed a method to maximize the joint probability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169, Beijing, August 2010 tic role of preposition phrases and the preposition sense. In order to do better joint learning, a novel statistical relational learning framework, Markov logic (Domingos and Lowd, 2009) w"
C10-1019,P09-1055,0,0.0609524,"c et al., 2009; Surdeanu et al., 2008). However, both of the pipeline methods ignore possible dependencies between the word senses and semantic roles, and can result in the error propagation problem. The same problem also appears in other natural language processing tasks. In order to make different natural language processing tasks be able to help each other, jointly modeling methods become popular recently, such as joint Chinese word segmentation and part-ofspeech tagging (Kruengkrai et al., 2009; Zhang and Clark, 2008; Jiang et al., 2008), joint lemmatization and part-of-speech prediction (Toutanova and Cherry, 2009), joint morphological segmentation and syntactic parsing (Goldberg and Tsarfaty, 2008), joint text and aspect ratings for sentiment summarization (Titov and McDonald, 2008), and joint parsing and named entity recognition (Finkel and Manning, 2009). For semantic role labeling, Dahlmeier et al. (2009) proposed a method to maximize the joint probability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169, Beijing, August 2010 tic role of preposition phrases and the preposition sense. In order to do better joint learning, a nov"
C10-1019,J08-2002,0,0.0296886,"sent a Markov logic model which can easily express useful global constraints and jointly disambiguate all word senses and label semantic roles. Experiments on the OntoNotes 3.0 corpus show that (1) the automatic all word sense disambiguation and semantic role labeling tasks can help each other when using pipeline approaches, and more important, (2) the joint approach using Markov logic leads to higher accuracy for word sense disambiguation and performance (F1 ) for semantic role labeling than pipeline approaches. 2 Related Work Joint models were often used in semantic role labeling community. Toutanova et al. (2008) and Punyakanok et al. (2008) presented a re-ranking model and an integer linear programming model respectively to jointly learn a global optimal semantic roles assignment. Besides jointly learning semantic role assignment of different constituents for one task (semantic role labeling), their methods have been used to jointly learn for two tasks (semantic role labeling and syntactic parsing). However, it is easy for the re-ranking model to loss the optimal result, if it is not included in the top n results. In addition, the integer linear programming model can only use hard constraints. A lot"
C10-1019,P09-1046,0,0.0336476,"role assignment of different constituents for one task (semantic role labeling), their methods have been used to jointly learn for two tasks (semantic role labeling and syntactic parsing). However, it is easy for the re-ranking model to loss the optimal result, if it is not included in the top n results. In addition, the integer linear programming model can only use hard constraints. A lot of engineering work is also required in both models. Recently, Markov logic (Domingos and Lowd, 2009) became a hot framework for joint model. It has been successfully used in temporal relations recognition (Yoshikawa et al., 2009), co-reference resolution (Poon and Domingos, 2008), etc. It is very easy to do joint modeling using Markov logic. The only work is to define relevant formulas. Meza-Ruiz and Riedel (2009) have joined semantic role labeling and predicate senses disambiguation with Markov logic. The above idea, that the predicate senses and the semantic role labeling can help each other, may be inspired by Hajiˇc et al. (2009), Surdeanu et al. (2008), and Dang and Palmer (2005). They have shown that semantic role features are helpful to disambiguate verb senses and vice versa. Besides predicate senses, Dahlmeie"
C10-1019,N06-2015,0,0.0544942,"Missing"
C10-1019,P08-1101,0,0.0511629,"ion (Dang and Palmer, 2005). More people used predicate senses in semantic role labeling (Hajiˇc et al., 2009; Surdeanu et al., 2008). However, both of the pipeline methods ignore possible dependencies between the word senses and semantic roles, and can result in the error propagation problem. The same problem also appears in other natural language processing tasks. In order to make different natural language processing tasks be able to help each other, jointly modeling methods become popular recently, such as joint Chinese word segmentation and part-ofspeech tagging (Kruengkrai et al., 2009; Zhang and Clark, 2008; Jiang et al., 2008), joint lemmatization and part-of-speech prediction (Toutanova and Cherry, 2009), joint morphological segmentation and syntactic parsing (Goldberg and Tsarfaty, 2008), joint text and aspect ratings for sentiment summarization (Titov and McDonald, 2008), and joint parsing and named entity recognition (Finkel and Manning, 2009). For semantic role labeling, Dahlmeier et al. (2009) proposed a method to maximize the joint probability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169, Beijing, August 2010 t"
C10-1019,P08-1102,0,0.0863211,"Missing"
C10-1019,D08-1105,0,0.0586961,"Missing"
C10-3004,W09-1207,1,0.237403,"nnotated with lexical tags, including word segmentation, part-of-speech tagging, and named entity recognition tags2 . 2.2 NER WSD Parser SRL 3 Speed 185KB/s 56.3KB/s 14.4KB/s 7.2KB/s 0.2KB/s 1.3KB/s Table 1: The performance and speed for each module. the dependency syntactic parsing subtask in the CoNLL-2009 Syntactic and Semantic Dependencies in Multiple Languages Shared Task (Hajiˇc et al., 2009). 6. Semantic Role Labeling (SRL): SRL is to identify the relations between predicates in a sentence and their associated arguments. The module is based on syntactic parser. A maximum entropy model (Che et al., 2009) is adopted here which achieved the ﬁrst place in the joint task of syntactic and semantic dependencies of the CoNLL2009 Shared Task. Table 1 shows the performance and speed of each module in detail. The performances are obtained with n-fold cross-validation method. The speed is gotten on a machine with Xeon 2.0GHz CPU and 4G Memory. At present, LTP processes these modules with a cascaded mechanism, i.e., some higher-level processing modules depend on other lower-level modules. For example, WSD needs to take the output of POSTag as input; while before POSTag, the document must be processed wit"
C10-3004,S07-1034,1,0.697039,"Missing"
C10-3004,J96-1002,0,\N,Missing
C12-1103,P11-1048,0,0.0128095,"cient Greek, and Hungarian. For these languages, morphological analysis requires the disambiguation of POS tags, gender, case, etc. They show that the joint model can well capture the interaction between morphology and syntax and achieve gains on both subtasks. (Rush et al., 2010) propose dual decomposition (DD) for integrating different NLP subtasks at the test phase. They experiment with two cases, one integrating a phrase-structure parser and a dependency parser, and the other integrating a phrase-structure parser and a POS tagger. Both cases show that DD can help the individual subtasks. (Auli and Lopez, 2011) conduct an extensive comparison of LBP and DD for joint CCG supertagging and parsing. They show that LBP and DD achieves similar parsing accuracy improvement but has largely different convergence characteristics. Moreover, their work focuses on integrating two separately-trained sub-models, and they find that training the integrated model on LBP leads to large improvement drops compared with separately-trained models. 3 Pipeline POS tagging and dependency parsing The pipeline method treats POS tagging and dependency parsing as two cascaded problems. First, an optimal POS tag sequence ˆt is de"
C12-1103,C10-1011,0,0.178642,"Dependency features fdep (x, t, h, m, l) Sibling features fsib (x, t, h, m, l, s) Grandchild features fgrd (x, t, h, m, l, g) Atomic features incorporated l, wh, w m , t h , t m , t h±1 , t m±1 , t b , d i r(h, m), d ist(h, m) l, wh, ws , w m , t h , t m , t s , t h±1 , t m±1 , t s±1 , d i r(h, m), d ist(h, m) l, wh, w m , w g , t h , t m , t g , t h±1 , t m±1 , t g±1 , d i r(h, m), d i r(m, g) Table 1: Brief illustration of the syntactic features. b is an index between h and m. d i r(i, j) and d ist(i, j) denote the direction and distance of the dependency (i, j). Please refer to Table 4 of (Bohnet, 2010) for the complete feature list. CRF-based POS tagging. We adopt the first-order CRF to build our baseline POS tagger. As a conditional log-linear probabilistic model, CRF defines the probability of a tag sequence as X exp(Scorepos (x, t′ )) P(t|x) = exp(Scorepos (x, t))/ Scorepos (x, t) = wpos · fpos (x, t) = X 1≤i≤n t′ wpu · fpu (x, t i ) + wpb · fpb (x, t i−1 , t i ) (3) where fpos/pu/pb (.) refers to the feature vectors and wpos/pu/pb is the corresponding weight vectors. We call fpu (x, t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt"
C12-1103,D12-1133,0,0.229202,"peline models in the parsing accuracy but lead to substantial tagging accuracy drop. Compared with their work, we propose a better training algorithm for the joint models that can improve both tagging and parsing accuracies. In addition, our joint model adopts richer features and handles labeled dependency parsing. (Hatori et al., 2011) propose the first transition-based joint model for Chinese POS tagging and unlabeled dependency parsing and gain large improvement in the parsing accuracy. However, their joint models only slightly improve the tagging accuracy over a sequential tagging model. (Bohnet and Nivre, 2012) propose a transition-based joint model which can handle labeled non-projective dependency parsing. They conduct experiments on a variety of languages including Chinese, English, Czech, and German. Similarly, their joint model largely improves the parsing accuracy but only slightly increases the tagging accuracy. Differently, we are the first work on joint POS tagging and dependency parsing that achieves large improvement in the tagging accuracy. (Smith and Eisner, 2008) apply loopy belief propagation (LBP) to dependency parsing and points out that LBP can naturally represent POS tags as laten"
C12-1103,D07-1101,0,0.509725,"t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing accuracy on a variety of languages (Koo and Collins, 2010; Bohnet, 2010). The score of a dependency tree is factored into scores of the three kinds of subtrees in Figure 2. Scoresyn (x, t, d) = wsyn · fsyn (x, t, d) = X {(h,m,l)}⊆d + X wdep · fdep (x, t, h, m, l) {(h,m,l),(h,s)}⊆d + X wsib · fsib (x, t, h, m, l, s) {(h,m,l),(m,g)}⊆d (4) wgrd · fgrd (x, t, h, m, l, g) For syntactic features, we adopt those of (Bohnet, 2010) which include three categories corres"
C12-1103,W08-2102,0,0.0186802,"Missing"
C12-1103,P05-1022,0,0.0567146,"Missing"
C12-1103,P12-2003,1,0.88498,"Missing"
C12-1103,W02-1001,0,0.0361869,"ted by x = w1 ...w n , part-of-speech (POS) tagging aims to find an optimal tag sequence t = t 1 ...t n , where t i ∈ T (1 ≤ i ≤ n) and T is a predefined tag set. POS tags are designed to represent word classes so that words of the same POS tag play a similar role in syntactic structures. The size of T is usually much less than the vocabulary size. Typically, POS tagging is treated as a sequence labeling problem, and has been previously addressed by machine learning algorithms, such as maximum-entropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). Figure 1 gives an example sentence from Penn Chinese Treebank 5.1 (CTB5). The lowest three rows present the n-best POS tags for each word, produced by a state-ofthe-art CRF model. Looking at the 1-best POS tags, we can see that the CRF model makes four errors, i.e. de/DEC→DEG, ouwen/NR→NN, xiaoli/VV→NN, and liwupudui/NR→NN. In fact, (DEC,DEG) and (NN,VV) ambiguities, which usually require long-distance syntactic knowledge to resolve, are very difficult for the sequential labeling models. NMOD ROOT DEP VMOD VMOD  1 SUB AMOD á 2 193 gang man just turned 19 AD VV CD JJ 19 P VMOD  4 VMOD  &apos;"
C12-1103,W09-1201,0,0.0527682,"Missing"
C12-1103,I11-1136,0,0.318758,"is most closely related to (Li et al., 2011) who present the first work on joint models for Chinese POS tagging and unlabeled dependency parsing. Similar to us, their joint models are based on graph-based dependency parsing. They find that the joint models largely outperform the pipeline models in the parsing accuracy but lead to substantial tagging accuracy drop. Compared with their work, we propose a better training algorithm for the joint models that can improve both tagging and parsing accuracies. In addition, our joint model adopts richer features and handles labeled dependency parsing. (Hatori et al., 2011) propose the first transition-based joint model for Chinese POS tagging and unlabeled dependency parsing and gain large improvement in the parsing accuracy. However, their joint models only slightly improve the tagging accuracy over a sequential tagging model. (Bohnet and Nivre, 2012) propose a transition-based joint model which can handle labeled non-projective dependency parsing. They conduct experiments on a variety of languages including Chinese, English, Czech, and German. Similarly, their joint model largely improves the parsing accuracy but only slightly increases the tagging accuracy."
C12-1103,P10-1110,0,0.190632,"the discriminative power of the POS features in resolving such syntaxinsensitive POS ambiguities are suppressed in the joint models when trained with AP or PA. Compared with AP and PA, SPA raises the weight of the POS features and can better utilize the disambiguation power of both the POS and syntactic features, leading to large tagging accuracy boost. On the other hand, better tagging results can further help parsing. 6 Experiments Data. We conduct experiments on CTB5 (Xue et al., 2005). Following the standard practice, we adopt the data split of (Duan et al., 2007; Zhang and Clark, 2008b; Huang and Sagae, 2010) and adopt Penn2Malt2 for constituent-to-dependency conversion with the head-finding rules of (Zhang and Clark, 2008b). We also evaluate our models on another version of CTB5 used in (Bohnet and Nivre, 2012) to compare with their joint model. We thank Bernd Bohnet for sharing their dataset. We refer to their dataset as CTB5-Bohnet. We carefully compare CTB5 with CTB5-Bohnet and find that except for the mismatch of about 30 sentence, the datasets differ in both dependency structures and dependency labels. After discussions with Bernd Bohnet, we find out that they adopt Yue Zhang’s constituent-t"
C12-1103,D08-1008,0,0.0157962,") , ˆt, d) AP wjoint (7) PA computes the update step τjoint by considering the loss of the best result, the score distance, and the feature vector distance.  ˆ − Scorejoint (x( j) , t( j) , d( j) ) + ρpos (t( j) , ˆt) + ρsyn (d( j) , d) ˆ Scorejoint (x( j) , ˆt, d)  τjoint = ( j) ( j) ( j) ( j) ˆ ˆ 2 kfjoint (x , t , d ) − fjoint (x , t, d)k (8) PA  (k+1) (k) ˆ w =w + τjoint (fjoint (x( j) , t( j) , d( j) ) − fjoint (x( j) , ˆt, d)) joint joint ( j) ˆ is the where ρpos (t , ˆt) is the incorrect POS tag number in ˆt according to t( j) , and ρsyn (d( j) , d) ˆ according to d( j) . Following (Johansson and Nugues, 2008), dependency error number in d ˆ increases by 1 for an incorrect dependency and by 0.5 for a correct dependency ρsyn (d( j) , d) with a wrong label. Theoretically, Eq. 8 computes the smallest update that makes the correct hypothesis outscores the returned highest-scoring hypothesis by the overall error. We can see that AP and PA use the same update step for the POS features fpos (.) and syntactic features fsyn (.). Therefore, the weights of the POS features and the syntactic features are of the same scale after training is completed. We argue that this is problematic since the number of the sy"
C12-1103,P08-1068,0,0.333682,"Missing"
C12-1103,P10-1001,0,0.392309,"igram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing accuracy on a variety of languages (Koo and Collins, 2010; Bohnet, 2010). The score of a dependency tree is factored into scores of the three kinds of subtrees in Figure 2. Scoresyn (x, t, d) = wsyn · fsyn (x, t, d) = X {(h,m,l)}⊆d + X wdep · fdep (x, t, h, m, l) {(h,m,l),(h,s)}⊆d + X wsib · fsib (x, t, h, m, l, s) {(h,m,l),(m,g)}⊆d (4) wgrd · fgrd (x, t, h, m, l, g) For syntactic features, we adopt those of (Bohnet, 2010) which include three categories corresponding to the three typ"
C12-1103,P11-1089,0,0.0153191,"ctive dependency parsing. They conduct experiments on a variety of languages including Chinese, English, Czech, and German. Similarly, their joint model largely improves the parsing accuracy but only slightly increases the tagging accuracy. Differently, we are the first work on joint POS tagging and dependency parsing that achieves large improvement in the tagging accuracy. (Smith and Eisner, 2008) apply loopy belief propagation (LBP) to dependency parsing and points out that LBP can naturally represent POS tags as latent variables so that the POS tags can be inferred jointly with the parse. (Lee et al., 2011) extend the LBP based approach of (Smith and Eisner, 2008) and study joint morphological disambiguation and dependency parsing for morphologically-rich languages including Latin, Czech, Ancient Greek, and Hungarian. For these languages, morphological analysis requires the disambiguation of POS tags, gender, case, etc. They show that the joint model can well capture the interaction between morphology and syntax and achieve gains on both subtasks. (Rush et al., 2010) propose dual decomposition (DD) for integrating different NLP subtasks at the test phase. They experiment with two cases, one inte"
C12-1103,D11-1109,1,0.93535,"score that are previously defined in the pipeline models. Scorejoint (x, t, d) = Scorepos (x, t) + Scoresyn (x, t, d) = wpos · fpos (x, t) + wsyn · fsyn (x, t, d) (6) = wpos⊕syn · fpos⊕syn (x, t, d) = wjoint · fjoint (x, t, d) where ⊕ denotes vector concatenation. Note that our joint model incorporates the same POS and syntactic features with the pipeline models. Under the joint model, the weights of POS and syntactic features, denoted by wpos⊕syn or wjoint , are simultaneously learned. Therefore, they can interact with each other to determine an optimal joint result. 4.1 Decoding Similar to (Li et al., 2011), we extend the parsing algorithm of (Carreras, 2007) using the idea of (Eisner, 2000) and propose a dynamic programming (DP) based decoding algorithm for our joint model. Figure 3 illustrates the basic DP structures and operations. The key idea is to augment the basic DP structures in the parsing algorithm (namely spans) with a few POS tags. A span means a partially built structure spanning a sub-sentence. For example, the leftside span in Figure 3(a), which is called an incomplete span and is denoted by I(h,m,l)(t h ,t m ) , represents a partial tree spanning wh...w m with wh being tagged as"
C12-1103,D10-1004,0,0.0469191,"Missing"
C12-1103,P05-1012,0,0.236675,"ctors. We call fpu (x, t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing accuracy on a variety of languages (Koo and Collins, 2010; Bohnet, 2010). The score of a dependency tree is factored into scores of the three kinds of subtrees in Figure 2. Scoresyn (x, t, d) = wsyn · fsyn (x, t, d) = X {(h,m,l)}⊆d + X wdep · fdep (x, t, h, m, l) {(h,m,l),(h,s)}⊆d + X wsib · fsib (x, t, h, m, l, s) {(h,m,l),(m,g)}⊆d (4) wgrd · fgrd (x, t, h, m, l, g) For syntactic features, we adopt those of (Bohnet, 2010) which include three c"
C12-1103,N07-1051,0,0.0863404,"Missing"
C12-1103,W96-0213,0,0.889178,"NG 2012, Mumbai, December 2012. 1681 1 Introduction Given an input sentence of n words, denoted by x = w1 ...w n , part-of-speech (POS) tagging aims to find an optimal tag sequence t = t 1 ...t n , where t i ∈ T (1 ≤ i ≤ n) and T is a predefined tag set. POS tags are designed to represent word classes so that words of the same POS tag play a similar role in syntactic structures. The size of T is usually much less than the vocabulary size. Typically, POS tagging is treated as a sequence labeling problem, and has been previously addressed by machine learning algorithms, such as maximum-entropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). Figure 1 gives an example sentence from Penn Chinese Treebank 5.1 (CTB5). The lowest three rows present the n-best POS tags for each word, produced by a state-ofthe-art CRF model. Looking at the 1-best POS tags, we can see that the CRF model makes four errors, i.e. de/DEC→DEG, ouwen/NR→NN, xiaoli/VV→NN, and liwupudui/NR→NN. In fact, (DEC,DEG) and (NN,VV) ambiguities, which usually require long-distance syntactic knowledge to resolve, are very difficult for the sequential labeling models. NMOD ROOT DEP VMO"
C12-1103,D10-1001,0,0.012255,"nts out that LBP can naturally represent POS tags as latent variables so that the POS tags can be inferred jointly with the parse. (Lee et al., 2011) extend the LBP based approach of (Smith and Eisner, 2008) and study joint morphological disambiguation and dependency parsing for morphologically-rich languages including Latin, Czech, Ancient Greek, and Hungarian. For these languages, morphological analysis requires the disambiguation of POS tags, gender, case, etc. They show that the joint model can well capture the interaction between morphology and syntax and achieve gains on both subtasks. (Rush et al., 2010) propose dual decomposition (DD) for integrating different NLP subtasks at the test phase. They experiment with two cases, one integrating a phrase-structure parser and a dependency parser, and the other integrating a phrase-structure parser and a POS tagger. Both cases show that DD can help the individual subtasks. (Auli and Lopez, 2011) conduct an extensive comparison of LBP and DD for joint CCG supertagging and parsing. They show that LBP and DD achieves similar parsing accuracy improvement but has largely different convergence characteristics. Moreover, their work focuses on integrating tw"
C12-1103,D08-1016,0,0.0105209,"e parsing accuracy. However, their joint models only slightly improve the tagging accuracy over a sequential tagging model. (Bohnet and Nivre, 2012) propose a transition-based joint model which can handle labeled non-projective dependency parsing. They conduct experiments on a variety of languages including Chinese, English, Czech, and German. Similarly, their joint model largely improves the parsing accuracy but only slightly increases the tagging accuracy. Differently, we are the first work on joint POS tagging and dependency parsing that achieves large improvement in the tagging accuracy. (Smith and Eisner, 2008) apply loopy belief propagation (LBP) to dependency parsing and points out that LBP can naturally represent POS tags as latent variables so that the POS tags can be inferred jointly with the parse. (Lee et al., 2011) extend the LBP based approach of (Smith and Eisner, 2008) and study joint morphological disambiguation and dependency parsing for morphologically-rich languages including Latin, Czech, Ancient Greek, and Hungarian. For these languages, morphological analysis requires the disambiguation of POS tags, gender, case, etc. They show that the joint model can well capture the interaction"
C12-1103,D09-1058,0,0.154484,"Missing"
C12-1103,P08-1101,0,0.469583,"list. CRF-based POS tagging. We adopt the first-order CRF to build our baseline POS tagger. As a conditional log-linear probabilistic model, CRF defines the probability of a tag sequence as X exp(Scorepos (x, t′ )) P(t|x) = exp(Scorepos (x, t))/ Scorepos (x, t) = wpos · fpos (x, t) = X 1≤i≤n t′ wpu · fpu (x, t i ) + wpb · fpb (x, t i−1 , t i ) (3) where fpos/pu/pb (.) refers to the feature vectors and wpos/pu/pb is the corresponding weight vectors. We call fpu (x, t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing ac"
C12-1103,D08-1059,0,0.230453,"list. CRF-based POS tagging. We adopt the first-order CRF to build our baseline POS tagger. As a conditional log-linear probabilistic model, CRF defines the probability of a tag sequence as X exp(Scorepos (x, t′ )) P(t|x) = exp(Scorepos (x, t))/ Scorepos (x, t) = wpos · fpos (x, t) = X 1≤i≤n t′ wpu · fpu (x, t i ) + wpb · fpb (x, t i−1 , t i ) (3) where fpos/pu/pb (.) refers to the feature vectors and wpos/pu/pb is the corresponding weight vectors. We call fpu (x, t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing ac"
C12-1103,J11-1005,0,0.00551549,"Missing"
C12-1103,P11-2033,0,0.150722,"Missing"
C12-1188,J04-4004,0,0.0330972,"rithm that is first proposed by (Wolpert, 1992) and (Breiman, 1996). It has been exploited in a number of NLP tasks for integration. We mainly concern the works of stacked learning applied on POS tagging and dependency parsing. The work of (Li et al., 2011a) presented a mostly recent work for stacking POS taggers. They exploit the output of a CRF POS tagger to help a perceptronbased POS tagger with syntactic features. (McDonald, 2006) proposed the first stacking work of dependency parsing. The author incorporated parse decisions of two constituent-based parsers, Collins parser (Collins, 1999; Bikel, 2004) and Charniak parser (Charniak, 2000), into the second-order MST parser. Then (Nivre and McDonald, 2008) suggested integrating graph- and transition-based models by stacking, and more detailed analysis was given in (McDonald and Nivre, 2011). (Martins et al., 2008) also demonstrated that stacking transition- and graphbased parsers can improve parsing performance significantly and meanwhile offer theoretical interpretations for stacking. In our paper, stacked leaning is applied on the joint tasks of Chinese POS tagging and dependency parsing. 3 Two Models for Joint Chinese POS Tagging and Depen"
C12-1188,D12-1133,0,0.0193385,"out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different with that of (Li et al., 2011b). The differences may be induced by the different manners of modeling the joint task. Thirdly, the constituent-based joint model processes joint Chinese POS tagging and dependency parsing in an indirectly way. It performs the POS taggin"
C12-1188,D07-1101,0,0.172376,"nd the guided transition-based joint model. Section 5 describes our constituent-based model and the further stacking model. Section 6 reports the experimental results. Section 7 gives the systematic analysis of the joint models. Finally, in Section 8 we conclude this paper and point out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error pattern"
C12-1188,A00-2018,0,0.0563471,"olpert, 1992) and (Breiman, 1996). It has been exploited in a number of NLP tasks for integration. We mainly concern the works of stacked learning applied on POS tagging and dependency parsing. The work of (Li et al., 2011a) presented a mostly recent work for stacking POS taggers. They exploit the output of a CRF POS tagger to help a perceptronbased POS tagger with syntactic features. (McDonald, 2006) proposed the first stacking work of dependency parsing. The author incorporated parse decisions of two constituent-based parsers, Collins parser (Collins, 1999; Bikel, 2004) and Charniak parser (Charniak, 2000), into the second-order MST parser. Then (Nivre and McDonald, 2008) suggested integrating graph- and transition-based models by stacking, and more detailed analysis was given in (McDonald and Nivre, 2011). (Martins et al., 2008) also demonstrated that stacking transition- and graphbased parsers can improve parsing performance significantly and meanwhile offer theoretical interpretations for stacking. In our paper, stacked leaning is applied on the joint tasks of Chinese POS tagging and dependency parsing. 3 Two Models for Joint Chinese POS Tagging and Dependency parsing A dependency tree for a"
C12-1188,P05-1022,0,0.0310453,"specially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat"
C12-1188,P12-2003,1,0.910792,"stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent parsing. Thus we can also adopt a PCFG model for joint Chinese POS tagging and dependency parsing. We denote it by constituent-based joint model. (Sun, 2012) proposed to improve the Chinese parsing acc"
C12-1188,W02-1001,0,0.0331342,"odifier word (or child) is w m . The task of dependency parsing is to find an optimum dependency tree d for the input sentence x. Generally, the POS tag sequence of the sentence t = t 1 · · · t n (where t i ∈ T, 1 ≤ i ≤ n, T is the POS tag set) is taken as an input for dependency parsing, which is determined by the task of POS tagging, thus forming a pipeline model of the two tasks. POS tagging is a typical sequence labeling problems which can be resolved by algorithms such as maximum-entropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and averaged perceptron (Collins, 2002). The goal of joint models of the two tasks is to find an optimum dependency tree and an optimum POS ˆ for x concurrently. tag sequence (ˆt, d) 3.1 Graph-based Joint Model The graph-based joint model is first proposed by (Li et al., 2011b). Such a model is extended from a graph-based model for dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). In the model, the score of a dependency tree along with POS tags on each node is factored into scores of small parts. (Li et al., 2011b) have introduced several different graph-based joint model"
C12-1188,W03-0433,0,0.0480722,"Missing"
C12-1188,I11-1136,0,0.107354,"tem: (1) Dependency parsing suffers the problem of error propagation; (2) POS tagging cannot exploit useful, important syntactic information for disambiguation. For Chinese POS tagging and dependency parsing, a pipeline system seriously suffers these two problems. The study presented in (Li et al., 2011b) demonstrates the error propagation factor. The authors develop a graph-based joint model for Chinese POS tagging and dependency parsing. The most interesting thing they found is that even with lower tagging accuracy, a joint model could achieve higher parsing accuracy. The work presented in (Hatori et al., 2011) also demonstrates a joint model can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependen"
C12-1188,P10-1110,0,0.0722255,"cy Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different with that of (Li et al., 2011b). The differences may be induced by the different manners of modeling the joint task. Thirdly, the constituent-based joint model processes joint Chinese POS tagging and dependency parsing in an indirectly way. It performs the POS tagging and dependency parsing by a conversion from the initial output of a PCFG parser. In Chinese POS tagg"
C12-1188,P10-1001,0,0.0624319,"ansition-based joint model. Section 5 describes our constituent-based model and the further stacking model. Section 6 reports the experimental results. Section 7 gives the systematic analysis of the joint models. Finally, in Section 8 we conclude this paper and point out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different"
C12-1188,I11-1171,1,0.250948,"Mumbai, December 2012. 3071 1 Introduction Part-of-speech (POS) tagging and dependency parsing are two fundamental natural language processing (NLP) tasks. Typically, POS tagging is a preprocessing step for dependency parsing, especially in a pipeline architecture. There are two main problems in a pipeline system: (1) Dependency parsing suffers the problem of error propagation; (2) POS tagging cannot exploit useful, important syntactic information for disambiguation. For Chinese POS tagging and dependency parsing, a pipeline system seriously suffers these two problems. The study presented in (Li et al., 2011b) demonstrates the error propagation factor. The authors develop a graph-based joint model for Chinese POS tagging and dependency parsing. The most interesting thing they found is that even with lower tagging accuracy, a joint model could achieve higher parsing accuracy. The work presented in (Hatori et al., 2011) also demonstrates a joint model can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked lear"
C12-1188,D11-1109,1,0.320009,"Mumbai, December 2012. 3071 1 Introduction Part-of-speech (POS) tagging and dependency parsing are two fundamental natural language processing (NLP) tasks. Typically, POS tagging is a preprocessing step for dependency parsing, especially in a pipeline architecture. There are two main problems in a pipeline system: (1) Dependency parsing suffers the problem of error propagation; (2) POS tagging cannot exploit useful, important syntactic information for disambiguation. For Chinese POS tagging and dependency parsing, a pipeline system seriously suffers these two problems. The study presented in (Li et al., 2011b) demonstrates the error propagation factor. The authors develop a graph-based joint model for Chinese POS tagging and dependency parsing. The most interesting thing they found is that even with lower tagging accuracy, a joint model could achieve higher parsing accuracy. The work presented in (Hatori et al., 2011) also demonstrates a joint model can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked lear"
C12-1188,D08-1017,0,0.11201,"he performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic con"
C12-1188,P05-1012,0,0.376473,"dels including the guided graph-based joint model and the guided transition-based joint model. Section 5 describes our constituent-based model and the further stacking model. Section 6 reports the experimental results. Section 7 gives the systematic analysis of the joint models. Finally, in Section 8 we conclude this paper and point out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al.,"
C12-1188,J11-1007,0,0.0608408,"ey propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parse"
C12-1188,E06-1011,0,0.241065,"ed graph-based joint model and the guided transition-based joint model. Section 5 describes our constituent-based model and the further stacking model. Section 6 reports the experimental results. Section 7 gives the systematic analysis of the joint models. Finally, in Section 8 we conclude this paper and point out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certa"
C12-1188,J08-4003,0,0.161113,"and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different with that of (Li et al., 2011b). The differences may be induced by the different manners of modeling the joint task. Thirdly, the constituent-based joint model processes joint Chinese POS tagging and dependency parsing in an indirectly way. It performs the POS tagging and dependency parsing by a conversion from the initial output of a PCFG pars"
C12-1188,P08-1108,0,0.37237,"odel can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that,"
C12-1188,P06-1055,0,0.370553,"that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent"
C12-1188,N07-1051,0,0.546016,"of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent parsing. Thus we can als"
C12-1188,C10-1120,0,0.0226267,"ndency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) p"
C12-1188,P11-1139,0,0.0211513,"ing accuracy, a joint model could achieve higher parsing accuracy. The work presented in (Hatori et al., 2011) also demonstrates a joint model can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from grap"
C12-1188,P12-1026,0,0.110473,"uent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent parsing. Thus we can also adopt a PCFG model for joint Chinese POS tagging and dependency parsing. We denote it by constituent-based joint model. (Sun, 2012) proposed to improve the Chinese parsing accuracy by a PCFG parser. Similarly, (Sun and Uszkoreit, 2012) exploited a PCFG parser to enhance Chinese POS tagging. Thus it is reasonable to investigate the performance of constituent-based joint models and to improve the performance of joint Chinese POS tagging and dependency parsing by a constituent-based joint model. In this paper, first we study the integration of a graph-based joint model (JGraph) and a transition-based joint model (JTrans) by stacked learning. The stacked learning is implemented using a two-level architecture, where the level-0 consists of one or more predictors of which the results are exploited as input to enhance the level-1"
C12-1188,W03-3023,0,0.373203,"er. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent parsing. Thus we can also adopt a PCFG model for joint Chinese POS tagging and dependency parsing. We denote it by constituent-based joint model. (Sun, 201"
C12-1188,D08-1059,0,0.0212821,"odels, the probability should be much lower with a negative impact. We name the features related with the consistency of the two level-0 models as guided consistent features. Table 2 lists the guided consistent features used in this work. Both the guided graph-based joint model and the guided transition-based joint model are considered. 6 6.1 Experiments Experimental Settings We use CTB5.1 to conduct our experiments. Following the works of (Li et al., 2011b) and (Hatori et al., 2011), we use the standard split of CTB5.1 described in (Duan et al., 2007) and the conversion rules of CS-to-DS in (Zhang and Clark, 2008). We use the standard tagging accuracy to evaluate POS tagging. For dependency parsing, we use word accuracy (also known as dependency accuracy or UAS), root accuracy and complete 3078 The Guided Graph-based Joint Model: JGraph(JTrans, JConst) pos dep JTrans JConst JTrans JTrans {Whether ˆt m is identical to ˆt m ?} ⊗{ˆt m ◦ t m , ˆt m ◦ wm ◦ t m } ˆ JTrans and d ˆ JConst ?} ⊗ {Whether hx m is in {Whether the heads of m are identical in d ˆ JTrans ?} ⊗{t h , t m , t h ◦ t m } d The Guided Transition-based Joint Model: JTrans(JGraph, JConst) pos syn JGraph JConst JGraph JGraph {Whether ˆt m is"
C12-1188,P11-2033,0,0.0805884,"2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different with that of (Li et al., 2011b). The differences may be induced by the different manners of modeling the joint task. Thirdly, the constituent-based joint model processes joint Chinese POS tagging and dependency parsing in an indirectly way. It performs the POS tagging and dependency parsing by a conversion from the initial output of a PCFG parser. In Chinese POS tagging, (Sun and Uszkoreit,"
C12-1188,J03-4003,0,\N,Missing
C14-1048,apidianaki-2008-translation,0,0.144502,"(we |wc ) exceed some threshold 0 < δ < 1. The second column of Table 1 presents the extraction results on a sample of source language words with the corresponding translation words. 3.2 Clustering of Translation Words For each source language word, its translation words are then clustered so as to separate different senses. At the clustering time, we first represent each translation word with a feature vector (point), so that we can measure the similarities between points. Then we perform clustering on these feature vectors, representing different senses in different clusters. Different from Apidianaki (2008) who represents all occurrences of the translation words with their contexts in the foreign language for clustering, we adopt the embeddings of the translation words as the representations and directly perform clustering on the translation words,3 rather than the contexts of occurrences. The embedding representation is chosen for two reasons: (1) Word embeddings encode rich lexical semantics. They can be directly used to measure word similarities. (2) Embedding representation of the translation words leads to extremely high-efficiency clustering, because the number of translation words is orde"
C14-1048,J93-2003,0,0.0375018,"different senses in different clusters (). Once word senses are effectively induced for each word, we are able to form the sense-labeled training data of RNNLMs by tagging each word occurrence in the source language text with its associated sense cluster (®). Finally, the sense-tagged corpus is used to train the sense-specific word embeddings in a standard manner (¯). 3.1 Translation Words Extraction Given bilingual data after word alignment, we present a way of extracting translation words for source language words by exploiting the translation probability produced by word alignment models (Brown et al., 1993; Och and Ney, 2003; Liang et al., 2006). More formally, we notate the Chinese sentence as c = (c1 , ..., cI ) and English sentence as e = (e1 , ..., eJ ). The alignment models can be generally factored as: P p(c|e) = a p(a, c|e) Q p(a, c|e) = Jj=1 pd (aj |aj− , j)pt (cj |eaj ) (3) (4) where a is the alignment specifying the position of an English word aligned to each Chinese word, pd (aj |aj− , j) is the distortion probability, and pt (cj |eaj ) is the translation probability which we use. 499 SL Word 制服 花 法 领导 Translation Words Translation Word Clusters Nearest Neighbours investment, overpow"
C14-1048,N13-1006,1,0.799372,"Missing"
C14-1048,J81-4005,0,0.804567,"Missing"
C14-1048,P14-1113,1,0.133577,"word embeddings 9 10 www.icl.pku.edu.cn/icl groups/corpus/dwldform1.asp Person, Location, Organization, Date, Time, Number and Miscellany 504 0.90 +SingleEmb Polysemous(2) Polysemous(3) +SenseEmb 0.80 0.75 0.70 0.60 0.65 per−token accuracy 0.85 Baseline Monosemous Figure 4: Per-token accuracy on the polysemous and monosemous words in the NER test data. Polysemous(k) represents the set of words that have more than or equal to k senses defined in HowNet. are shown to capture many relational similarities, which can be recovered by vector arithmetic in the embedding space (Mikolov et al., 2013b; Fu et al., 2014). Klementiev et al. (2012) and Zou et al. (2013) learned cross-lingual word embeddings by utilizing MT word alignments in bilingual parallel data to constrain translational equivalence. Most previous NNLMs induce single embedding for each word, ignoring the polysemous property of languages. In an attempt to capture the different senses or usage of a word, Reisinger and Mooney (2010) and Huang et al. (2012) proposed multi-prototype models for inducing multiple embeddings for each word. They did this by clustering the contexts of words. These multi-prototype models simply induced a fixed number"
C14-1048,1992.tmi-1.9,0,0.332596,"ng stochastic gradient descent (SGD), in which back propagation through time (BPTT) is used to efficiently compute the gradients. In the RNNLM, U is the embedding matrix, where each column vector represents a word. As discussed in Section 1, the RNNLM and even most NNLMs ignore the polysemy phenomenon in natural languages and induce a single embedding for each word. We address this issue and introduce an effective approach for capturing polysemy in the next section. 3 Sense-specific Word Embedding Learning In our approach, WSI is performed prior to the training of word embeddings. Inspired by Gale et al. (1992) and Chan and Ng (2005), who used bilingual data for automatically generating training examples of WSD, we present a bilingual approach for unsupervised WSI, as shown in Figure 1. First, we extract the translations of the source language words from bilingual data (¬). Since there may be multiple translations for the same sense of a source language word, it is straightforward to cluster the translation words, exhibiting different senses in different clusters (). Once word senses are effectively induced for each word, we are able to form the sense-labeled training data of RNNLMs by tagging each"
C14-1048,P12-1092,0,0.872734,"). They are also shown to be effective as input to NLP systems (Collobert et al., 2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013). In recent years, neural network language models (NNLMs) have become popular architectures for learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an attempt to better capture the multiple senses or usages of a word, several multi-prototype models have been proposed (Reisinger and Mooney, 2010; Huang et al., 2012). These multi-prototype models simply induce K prototypes (embeddings) for every word in the vocabulary, where K is predefined as a fixed value. These models still may not capture the real senses of words, because different words may have different number of senses. We present a novel and simple method of learning sense-specific word embeddings by using bilingual parallel data. In this method, word sense induction (WSI) is performed prior to the training of NNLMs. We exploit bilingual parallel data for WSI, which is motivated by the intuition that the same word in the source language with diff"
C14-1048,S12-1049,0,0.0285058,"ich benefits many practical applications. Therefore, we first evaluate our embeddings using a similarity measurement. Word similarities are calculated using the MaxSim and AvgSim metric (Reisinger and Mooney, 2010): M axSim(u, v) = max1≤i≤ku ,1≤j≤kv s(ui , v j ) Pku Pkv 1 i j AvgSim(u, v) = ku ×k i=1 j=1 s(u , v ) v (5) (6) where ku and kv are the number of the induced senses for words u and v, respectively. s(·, ·) can be any standard similarity measure. In this study, we use the cosine similarity. Previous works used the WordSim-353 dataset (Finkelstein et al., 2002) or the Chinese version (Jin and Wu, 2012) for the evaluation of general word similarity. These datasets rarely contain polysemous words, and thus is unsuitable for our evaluation. To the best of our knowledge, no datasets for polysemous word similarity evaluation have been published yet, either in English or Chinese. In order to fill this gap in the research community, we manually construct a Chinese polysemous word similarity dataset. 5.2.1 Chinese Polysemous Word Similarity Dataset Construction We adopt the HowNet database (Dong and Dong, 2006) in constructing the dataset. HowNet is a Chinese knowledge database that maintains compr"
C14-1048,W13-3513,0,0.0127418,"language words is varied, the commonlyused k-means algorithm becomes inappropriate for this situation. Instead, we employ affinity propagation (AP) algorithm (Frey and Dueck, 2007) for clustering. In AP, each cluster is represented by one of the samples of it, which we call an exemplar. AP finds the exemplars iteratively based on the concept of “message passing”. AP has the major advantage that the number of the resulting clusters is dynamic, which mainly depends on the distribution of the data. Compared with other possible clustering approaches, such as hierarchical agglomerative clustering (Kartsaklis et al., 2013), AP determines the number of resulting clusters automatically without using any partition criterions. The third column of Table 1 lists the resulting clusters of the translation words for the sampled polysemous words. We can see that the resulting clusters are meaningful: senses are well represented by clusters of translation words. 3.3 Cross-lingual Word Sense Projection The produced clusters are then projected back into the source language to identify word senses. 3 The publicly available word embeddings proposed by Collobert et al. (2011) are used. 500 For each occurrence wo of the word w"
C14-1048,C12-1089,0,0.0290816,"10 www.icl.pku.edu.cn/icl groups/corpus/dwldform1.asp Person, Location, Organization, Date, Time, Number and Miscellany 504 0.90 +SingleEmb Polysemous(2) Polysemous(3) +SenseEmb 0.80 0.75 0.70 0.60 0.65 per−token accuracy 0.85 Baseline Monosemous Figure 4: Per-token accuracy on the polysemous and monosemous words in the NER test data. Polysemous(k) represents the set of words that have more than or equal to k senses defined in HowNet. are shown to capture many relational similarities, which can be recovered by vector arithmetic in the embedding space (Mikolov et al., 2013b; Fu et al., 2014). Klementiev et al. (2012) and Zou et al. (2013) learned cross-lingual word embeddings by utilizing MT word alignments in bilingual parallel data to constrain translational equivalence. Most previous NNLMs induce single embedding for each word, ignoring the polysemous property of languages. In an attempt to capture the different senses or usage of a word, Reisinger and Mooney (2010) and Huang et al. (2012) proposed multi-prototype models for inducing multiple embeddings for each word. They did this by clustering the contexts of words. These multi-prototype models simply induced a fixed number of embeddings for every wo"
C14-1048,N06-1014,0,0.488068,"(). Once word senses are effectively induced for each word, we are able to form the sense-labeled training data of RNNLMs by tagging each word occurrence in the source language text with its associated sense cluster (®). Finally, the sense-tagged corpus is used to train the sense-specific word embeddings in a standard manner (¯). 3.1 Translation Words Extraction Given bilingual data after word alignment, we present a way of extracting translation words for source language words by exploiting the translation probability produced by word alignment models (Brown et al., 1993; Och and Ney, 2003; Liang et al., 2006). More formally, we notate the Chinese sentence as c = (c1 , ..., cI ) and English sentence as e = (e1 , ..., eJ ). The alignment models can be generally factored as: P p(c|e) = a p(a, c|e) Q p(a, c|e) = Jj=1 pd (aj |aj− , j)pt (cj |eaj ) (3) (4) where a is the alignment specifying the position of an English word aligned to each Chinese word, pd (aj |aj− , j) is the distortion probability, and pt (cj |eaj ) is the translation probability which we use. 499 SL Word 制服 花 法 领导 Translation Words Translation Word Clusters Nearest Neighbours investment, overpower, investment, uniform 穿着dress , 警服poli"
C14-1048,N13-1090,0,0.281323,"g the sense-level word similarities. We further feed our embeddings as features in Chinese named entity recognition and obtain noticeable improvements against single embeddings. 1 Introduction Word embeddings are conventionally defined as compact, real-valued, and low-dimensional vector representations for words. Each dimension of word embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic characteristics. Word embeddings can be used straightforwardly for computing word similarities, which benefits many practical applications (Socher et al., 2011; Mikolov et al., 2013a). They are also shown to be effective as input to NLP systems (Collobert et al., 2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013). In recent years, neural network language models (NNLMs) have become popular architectures for learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an attempt to better capture the multiple senses or usages of a word, several multi-prototype models have been proposed (Reisinger and Mooney, 2010;"
C14-1048,J03-1002,0,0.00534514,"different clusters (). Once word senses are effectively induced for each word, we are able to form the sense-labeled training data of RNNLMs by tagging each word occurrence in the source language text with its associated sense cluster (®). Finally, the sense-tagged corpus is used to train the sense-specific word embeddings in a standard manner (¯). 3.1 Translation Words Extraction Given bilingual data after word alignment, we present a way of extracting translation words for source language words by exploiting the translation probability produced by word alignment models (Brown et al., 1993; Och and Ney, 2003; Liang et al., 2006). More formally, we notate the Chinese sentence as c = (c1 , ..., cI ) and English sentence as e = (e1 , ..., eJ ). The alignment models can be generally factored as: P p(c|e) = a p(a, c|e) Q p(a, c|e) = Jj=1 pd (aj |aj− , j)pt (cj |eaj ) (3) (4) where a is the alignment specifying the position of an English word aligned to each Chinese word, pd (aj |aj− , j) is the distortion probability, and pt (cj |eaj ) is the translation probability which we use. 499 SL Word 制服 花 法 领导 Translation Words Translation Word Clusters Nearest Neighbours investment, overpower, investment, uni"
C14-1048,N10-1013,0,0.776045,"2011; Mikolov et al., 2013a). They are also shown to be effective as input to NLP systems (Collobert et al., 2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013). In recent years, neural network language models (NNLMs) have become popular architectures for learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an attempt to better capture the multiple senses or usages of a word, several multi-prototype models have been proposed (Reisinger and Mooney, 2010; Huang et al., 2012). These multi-prototype models simply induce K prototypes (embeddings) for every word in the vocabulary, where K is predefined as a fixed value. These models still may not capture the real senses of words, because different words may have different number of senses. We present a novel and simple method of learning sense-specific word embeddings by using bilingual parallel data. In this method, word sense induction (WSI) is performed prior to the training of NNLMs. We exploit bilingual parallel data for WSI, which is motivated by the intuition that the same word in the sour"
C14-1048,J98-1004,0,0.56021,"Missing"
C14-1048,P10-1040,0,0.632179,"ents against single embeddings. 1 Introduction Word embeddings are conventionally defined as compact, real-valued, and low-dimensional vector representations for words. Each dimension of word embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic characteristics. Word embeddings can be used straightforwardly for computing word similarities, which benefits many practical applications (Socher et al., 2011; Mikolov et al., 2013a). They are also shown to be effective as input to NLP systems (Collobert et al., 2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013). In recent years, neural network language models (NNLMs) have become popular architectures for learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an attempt to better capture the multiple senses or usages of a word, several multi-prototype models have been proposed (Reisinger and Mooney, 2010; Huang et al., 2012). These multi-prototype models simply induce K prototypes (embeddings) for every word in the vocabulary, where K is predefine"
C14-1048,N13-1063,0,0.015,"mbeddings. 1 Introduction Word embeddings are conventionally defined as compact, real-valued, and low-dimensional vector representations for words. Each dimension of word embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic characteristics. Word embeddings can be used straightforwardly for computing word similarities, which benefits many practical applications (Socher et al., 2011; Mikolov et al., 2013a). They are also shown to be effective as input to NLP systems (Collobert et al., 2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013). In recent years, neural network language models (NNLMs) have become popular architectures for learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an attempt to better capture the multiple senses or usages of a word, several multi-prototype models have been proposed (Reisinger and Mooney, 2010; Huang et al., 2012). These multi-prototype models simply induce K prototypes (embeddings) for every word in the vocabulary, where K is predefined as a fixed value"
C14-1048,D13-1141,0,0.169902,"d similarities from the dataset. The whole evaluation dataset will be publicly available for the research community.8 Word 制服 出 花 面 Paired word Category Mean.Sim Std.Dev 征服conquer synonym 8.60 0.29 重点key point unrelated 0.12 0.19 进enter autonym 7.90 0.97 发表publish near-synonym 7.86 0.76 茎plant stem sibling 7.80 0.12 费用cost topic-related 5.86 0.90 食物f ood hypernym 6.50 0.71 Table 2: Sample word pairs of our dataset. The unrelated words are randomly sampled. M ean.Sim represents the mean similarity of the annotations, Std.Dev represents the standard deviation. 5.2.2 Evaluation Results Following Zou et al. (2013), we use Spearman’s ρ correlation and Kendall’s τ correlation for evaluation. The results are shown in Table 3. By utilizing sense-specific embeddings, our approach significantly outperforms the single-version using either MaxSim or AvgSim measurement. For comparison with multi-prototype methods, we borrow the context-clustering idea from Huang et al. (2012), which was first presented by Sch¨utze (1998). The occurrences of a word are represented by the average embeddings of its context words. Following Huang et al.’s settings, we use a context window of size 10 and all occurrences of a word ar"
C14-1051,D12-1133,0,0.168726,"r the Zhang scheme, and increases of 0.30 on UAS and 0.36 on LAS for the Stanford scheme. The results also demonstrate similar conclusions with the experiments on English dataset. 5 Related Work Our work is mainly inspired by the work of joint models. There are a number of successful studies on joint modeling pipelined tasks where one task is a prerequisite step of another task, for example, the joint model of word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), the joint model of POS-tagging and parsing (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012), the joint model of word segmentation, POS-tagging and parsing (Hatori et Model Baseline Our joint model UAS 79.07 80.20‡ Zhang LAS 76.08 77.07‡ CM 27.96 30.10 UAS 80.33 80.63 Stanford LAS CM 75.29 31.14 75.65 31.20 Table 6: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. 537 al., 2012; Zhang et al., 2013b; Zhang et al., 2014), and the joint model of morphological and syntactic analysis tasks (Bohnet et al., 2013). In our work, we propose a joint model on parallel tasks, to parse two heterogeneous dependency trees"
C14-1051,Q13-1034,0,0.0186811,"nd Clark, 2010), the joint model of POS-tagging and parsing (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012), the joint model of word segmentation, POS-tagging and parsing (Hatori et Model Baseline Our joint model UAS 79.07 80.20‡ Zhang LAS 76.08 77.07‡ CM 27.96 30.10 UAS 80.33 80.63 Stanford LAS CM 75.29 31.14 75.65 31.20 Table 6: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. 537 al., 2012; Zhang et al., 2013b; Zhang et al., 2014), and the joint model of morphological and syntactic analysis tasks (Bohnet et al., 2013). In our work, we propose a joint model on parallel tasks, to parse two heterogeneous dependency trees simultaneously. There has been a line of work on exploiting multiple treebanks with heterogeneous dependencies to enhance dependency parsing. Li et al. (2012) proposed a feature-based stacking model to enhance a specific target dependency parser with the help of another treebank. Zhou and Zhao (2013) presented a joint inference framework to combine the parsing results based on two different treebanks. All these work are case studies of annotation adaptation from different sources, which have"
C14-1051,P13-1104,0,0.0498278,"r the following question: parsing heterogeneous dependencies jointly or separately, which is better? We conduct experiments with two different schemes on the Penn Treebank and the Chinese Penn Treebank respectively, arriving at the same conclusion that jointly parsing heterogeneous dependencies can give improved performances for both schemes over the individual models. 1 Introduction Dependency parsing has been intensively studied in recent years (McDonald et al., 2005; Nivre, 2008; Zhang and Clark, 2008; Huang et al., 2009; Koo and Collins, 2010; Zhang and Nivre, 2011; Sartorio et al., 2013; Choi and McCallum, 2013; Martins et al., 2013). Widely-used corpus for training a dependency parser is usually constructed according to a specific constituent-to-dependency conversion scheme. Several conversion schemes for certain languages have been available. For example, the English language has at least four schemes based on the Penn Treebank (PTB), including the Yamada scheme (Yamada and Matsumoto, 2003), the CoNLL 2007 scheme (Nilsson et al., 2007), the Stanford scheme (de Marneffe and Manning, 2008) and the LTH scheme (Johansson and Nugues, 2007). There are different conversion schemes for the Chinese Penn Tr"
C14-1051,P04-1015,0,0.0101157,"top element S0 and the second top element S1 on the stack, with the dependency label being specified by l; and the pop-root action defines the root node of a dependency tree when there is only one element on the stack and no element in the queue. During decoding, each state may have several actions. We employ a fixed beam to reduce the search space. The low-score states are pruned from the beam when it is full. The feature templates in our baseline are shown by Table 1, referring to baseline feature templates. We learn the feature weights by the averaged percepron algorithm with early-update (Collins and Roark, 2004; Zhang and Clark, 2011). 3 The Proposed Joint Model The aforementioned baseline model can only handle a single dependency tree. In order to parse multiple dependency trees for a sentence, we usually use individual dependency parsers. This method is not able to exploit the correlations across different dependency schemes. The joint model to parse multiple dependency trees with a single model is an elegant way to exploit these correlations fully. Inspired by this, we make a novel extension to the baseline arc-standard transition system, arriving at a joint model to parse two heterogeneous depen"
C14-1051,W08-1301,0,0.151442,"Missing"
C14-1051,N13-1070,0,0.051856,"Missing"
C14-1051,I11-1136,0,0.0705947,"it more reliable than one alone. In this paper, we investigate the influences of the correlations between different dependency schemes on parsing performances. We propose a joint model to parse heterogeneous dependencies from two schemes simultaneously, so that the correlations can be fully used by their interactions in a single model. Joint models have been widely studied to enhance multiple tasks in NLP community, including joint word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a state-of-the-art transition-based dependency parsing model. We conduct experiments on PTB with the Yamad"
C14-1051,P12-1110,0,0.240838,"lations between different dependency schemes on parsing performances. We propose a joint model to parse heterogeneous dependencies from two schemes simultaneously, so that the correlations can be fully used by their interactions in a single model. Joint models have been widely studied to enhance multiple tasks in NLP community, including joint word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a state-of-the-art transition-based dependency parsing model. We conduct experiments on PTB with the Yamada and the Stanford schemes, and also on CTB 5.1 with the Zhang and the Stanford schemes. Th"
C14-1051,D09-1127,0,0.595101,"full use of the correlations between heterogeneous dependencies, and finally we can answer the following question: parsing heterogeneous dependencies jointly or separately, which is better? We conduct experiments with two different schemes on the Penn Treebank and the Chinese Penn Treebank respectively, arriving at the same conclusion that jointly parsing heterogeneous dependencies can give improved performances for both schemes over the individual models. 1 Introduction Dependency parsing has been intensively studied in recent years (McDonald et al., 2005; Nivre, 2008; Zhang and Clark, 2008; Huang et al., 2009; Koo and Collins, 2010; Zhang and Nivre, 2011; Sartorio et al., 2013; Choi and McCallum, 2013; Martins et al., 2013). Widely-used corpus for training a dependency parser is usually constructed according to a specific constituent-to-dependency conversion scheme. Several conversion schemes for certain languages have been available. For example, the English language has at least four schemes based on the Penn Treebank (PTB), including the Yamada scheme (Yamada and Matsumoto, 2003), the CoNLL 2007 scheme (Nilsson et al., 2007), the Stanford scheme (de Marneffe and Manning, 2008) and the LTH schem"
C14-1051,P08-1102,0,0.0759747,"Missing"
C14-1051,P09-1059,0,0.241947,"se two heterogeneous dependency trees simultaneously. There has been a line of work on exploiting multiple treebanks with heterogeneous dependencies to enhance dependency parsing. Li et al. (2012) proposed a feature-based stacking model to enhance a specific target dependency parser with the help of another treebank. Zhou and Zhao (2013) presented a joint inference framework to combine the parsing results based on two different treebanks. All these work are case studies of annotation adaptation from different sources, which have been done for Chinese word segmentation and POS-tagging as well (Jiang et al., 2009; Sun and Wan, 2012). In contrast to their work, we study the heterogeneous annotations derived from the same source. We use a unified model to parsing heterogeneous dependencies together. Our joint parsing model exploits a transition-based framework with global learning and beam-search decoding (Zhang and Clark, 2011), extended from a arc-standard transition-based parsing model (Huang et al., 2009). The transition-based framework is easily adapted to a number of joint models, including joint word segmentation and POS-tagging (Zhang and Clark, 2010), the joint POS-tagging and parsing (Hatori e"
C14-1051,W07-2416,0,0.0902035,"Missing"
C14-1051,P10-1001,0,0.141219,"elations between heterogeneous dependencies, and finally we can answer the following question: parsing heterogeneous dependencies jointly or separately, which is better? We conduct experiments with two different schemes on the Penn Treebank and the Chinese Penn Treebank respectively, arriving at the same conclusion that jointly parsing heterogeneous dependencies can give improved performances for both schemes over the individual models. 1 Introduction Dependency parsing has been intensively studied in recent years (McDonald et al., 2005; Nivre, 2008; Zhang and Clark, 2008; Huang et al., 2009; Koo and Collins, 2010; Zhang and Nivre, 2011; Sartorio et al., 2013; Choi and McCallum, 2013; Martins et al., 2013). Widely-used corpus for training a dependency parser is usually constructed according to a specific constituent-to-dependency conversion scheme. Several conversion schemes for certain languages have been available. For example, the English language has at least four schemes based on the Penn Treebank (PTB), including the Yamada scheme (Yamada and Matsumoto, 2003), the CoNLL 2007 scheme (Nilsson et al., 2007), the Stanford scheme (de Marneffe and Manning, 2008) and the LTH scheme (Johansson and Nugues"
C14-1051,P09-1058,0,0.0793126,"rom the Yamada scheme and the label “pobj” from the Stanford scheme on a same dependency “atypoint” can make it more reliable than one alone. In this paper, we investigate the influences of the correlations between different dependency schemes on parsing performances. We propose a joint model to parse heterogeneous dependencies from two schemes simultaneously, so that the correlations can be fully used by their interactions in a single model. Joint models have been widely studied to enhance multiple tasks in NLP community, including joint word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a"
C14-1051,D11-1109,1,0.915722,"typoint” can make it more reliable than one alone. In this paper, we investigate the influences of the correlations between different dependency schemes on parsing performances. We propose a joint model to parse heterogeneous dependencies from two schemes simultaneously, so that the correlations can be fully used by their interactions in a single model. Joint models have been widely studied to enhance multiple tasks in NLP community, including joint word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a state-of-the-art transition-based dependency parsing model. We conduct experiments"
C14-1051,P12-1071,1,0.867181,"ectively exploited according to the above analysis. We assume that the first and second processing schemes are s1 and s2 respectively, to facilitate the below descriptions. We can see that the joint model behaves similarly to a pipeline reranking model, in optimizing scheme s1 ’s parsing performances. First we get K-best (K equals the beam size of the joint model) candidates for scheme s1 , and then employ additional evidences from scheme s2 ’s result, to rerank the K-best candidates, obtaining a better result. The joint model also behaves similarly to a pipeline feature-based stacking model (Li et al., 2012), in optimizing scheme s2 ’s parsing performances. After acquiring the best result of scheme s1 , we can use it to generate guided features to parse dependencies of scheme s2 . Thus additional information from scheme s1 can be imported into the parsing model of scheme s2 . Different with the pipeline reranking and the feature-based stacking models, we employ a single model to achieve the two goals, making the interactions between the two schemes be better performed. 4 Experiments 4.1 Experimental Settings In order to evaluate the baseline and joint models, we conduct experiments on English and"
C14-1051,P13-2109,0,0.0479035,"LAS are shown without the new features. While for Stanford scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the Stanford dependencies from Stanford parser version 2.0.5 and the results wi"
C14-1051,P05-1012,0,0.146151,"Missing"
C14-1051,J08-4003,0,0.0685433,"Missing"
C14-1051,N12-1054,0,0.272811,"heme, losses of 0.15 on UAS and 0.21 on LAS are shown without the new features. While for Stanford scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the Stanford dependencies from Stanford p"
C14-1051,P13-1014,0,0.0321302,"Missing"
C14-1051,P12-1025,0,0.32961,"dependency trees simultaneously. There has been a line of work on exploiting multiple treebanks with heterogeneous dependencies to enhance dependency parsing. Li et al. (2012) proposed a feature-based stacking model to enhance a specific target dependency parser with the help of another treebank. Zhou and Zhao (2013) presented a joint inference framework to combine the parsing results based on two different treebanks. All these work are case studies of annotation adaptation from different sources, which have been done for Chinese word segmentation and POS-tagging as well (Jiang et al., 2009; Sun and Wan, 2012). In contrast to their work, we study the heterogeneous annotations derived from the same source. We use a unified model to parsing heterogeneous dependencies together. Our joint parsing model exploits a transition-based framework with global learning and beam-search decoding (Zhang and Clark, 2011), extended from a arc-standard transition-based parsing model (Huang et al., 2009). The transition-based framework is easily adapted to a number of joint models, including joint word segmentation and POS-tagging (Zhang and Clark, 2010), the joint POS-tagging and parsing (Hatori et al., 2012; Bohnet"
C14-1051,W03-3023,0,0.121476,"Missing"
C14-1051,D08-1059,0,0.558312,"t model which can make full use of the correlations between heterogeneous dependencies, and finally we can answer the following question: parsing heterogeneous dependencies jointly or separately, which is better? We conduct experiments with two different schemes on the Penn Treebank and the Chinese Penn Treebank respectively, arriving at the same conclusion that jointly parsing heterogeneous dependencies can give improved performances for both schemes over the individual models. 1 Introduction Dependency parsing has been intensively studied in recent years (McDonald et al., 2005; Nivre, 2008; Zhang and Clark, 2008; Huang et al., 2009; Koo and Collins, 2010; Zhang and Nivre, 2011; Sartorio et al., 2013; Choi and McCallum, 2013; Martins et al., 2013). Widely-used corpus for training a dependency parser is usually constructed according to a specific constituent-to-dependency conversion scheme. Several conversion schemes for certain languages have been available. For example, the English language has at least four schemes based on the Penn Treebank (PTB), including the Yamada scheme (Yamada and Matsumoto, 2003), the CoNLL 2007 scheme (Nilsson et al., 2007), the Stanford scheme (de Marneffe and Manning, 200"
C14-1051,D10-1082,0,0.259738,"the label “pobj” from the Stanford scheme on a same dependency “atypoint” can make it more reliable than one alone. In this paper, we investigate the influences of the correlations between different dependency schemes on parsing performances. We propose a joint model to parse heterogeneous dependencies from two schemes simultaneously, so that the correlations can be fully used by their interactions in a single model. Joint models have been widely studied to enhance multiple tasks in NLP community, including joint word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a state-of-the-art transi"
C14-1051,J11-1005,0,0.149178,"d POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a state-of-the-art transition-based dependency parsing model. We conduct experiments on PTB with the Yamada and the Stanford schemes, and also on CTB 5.1 with the Zhang and the Stanford schemes. The results show that our joint model gives improved performances over the individual baseline models for both schemes on both English and Chinese languages, demonstrating positive effects of the correlations between the two schemes. We make the source code freely available at http://sourceforge.net/ projects/zpar/,version0.7. 2 Baseline Traditionally, the dependency"
C14-1051,D12-1030,0,0.0156056,"h the Zhang and the Stanford schemes. The results show that our joint model gives improved performances over the individual baseline models for both schemes on both English and Chinese languages, demonstrating positive effects of the correlations between the two schemes. We make the source code freely available at http://sourceforge.net/ projects/zpar/,version0.7. 2 Baseline Traditionally, the dependency parsers of different schemes are trained with their corpus separately, using a state-of-the-art dependency parsing algorithm (Zhang and Clark, 2008; Huang et al., 2009; Koo and Collins, 2010; Zhang and McDonald, 2012; Choi and McCallum, 2013). In this work, we exploit a transition-based arc-standard dependency parsing model combined with global learning and beam-search decoding as the baseline. which is initially proposed by Huang et al. (2009). In the following, we give a detailed description of the model. In a typical transition-based system for dependency parsing, we define a transition state, which consists of a stack to save partial-parsed trees and a queue to save unprocessed words. The parsing is performed incrementally via a set of transition actions. The transition actions are used to change cont"
C14-1051,P14-2107,0,0.0472412,"rd scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the Stanford dependencies from Stanford parser version 2.0.5 and the results with mark ∗∗ show the numbers for the Stanford dependencies fro"
C14-1051,P11-2033,0,0.160805,"ures proposed by us. For the Yamada scheme, losses of 0.15 on UAS and 0.21 on LAS are shown without the new features. While for Stanford scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the"
C14-1051,D13-1093,0,0.10027,"the new features. While for Stanford scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the Stanford dependencies from Stanford parser version 2.0.5 and the results with mark ∗∗ show the"
C14-1051,P13-1013,1,0.684759,"the new features. While for Stanford scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the Stanford dependencies from Stanford parser version 2.0.5 and the results with mark ∗∗ show the"
C14-1051,P14-1125,1,0.90386,"word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), the joint model of POS-tagging and parsing (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012), the joint model of word segmentation, POS-tagging and parsing (Hatori et Model Baseline Our joint model UAS 79.07 80.20‡ Zhang LAS 76.08 77.07‡ CM 27.96 30.10 UAS 80.33 80.63 Stanford LAS CM 75.29 31.14 75.65 31.20 Table 6: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. 537 al., 2012; Zhang et al., 2013b; Zhang et al., 2014), and the joint model of morphological and syntactic analysis tasks (Bohnet et al., 2013). In our work, we propose a joint model on parallel tasks, to parse two heterogeneous dependency trees simultaneously. There has been a line of work on exploiting multiple treebanks with heterogeneous dependencies to enhance dependency parsing. Li et al. (2012) proposed a feature-based stacking model to enhance a specific target dependency parser with the help of another treebank. Zhou and Zhao (2013) presented a joint inference framework to combine the parsing results based on two different treebanks. All"
C14-1051,P13-2019,0,0.0201629,"s with mark ‡ demonstrates that the p-value is below 10−3 using t-test. 537 al., 2012; Zhang et al., 2013b; Zhang et al., 2014), and the joint model of morphological and syntactic analysis tasks (Bohnet et al., 2013). In our work, we propose a joint model on parallel tasks, to parse two heterogeneous dependency trees simultaneously. There has been a line of work on exploiting multiple treebanks with heterogeneous dependencies to enhance dependency parsing. Li et al. (2012) proposed a feature-based stacking model to enhance a specific target dependency parser with the help of another treebank. Zhou and Zhao (2013) presented a joint inference framework to combine the parsing results based on two different treebanks. All these work are case studies of annotation adaptation from different sources, which have been done for Chinese word segmentation and POS-tagging as well (Jiang et al., 2009; Sun and Wan, 2012). In contrast to their work, we study the heterogeneous annotations derived from the same source. We use a unified model to parsing heterogeneous dependencies together. Our joint parsing model exploits a transition-based framework with global learning and beam-search decoding (Zhang and Clark, 2011),"
C14-1051,D07-1096,0,\N,Missing
C14-1129,N07-1039,0,0.193334,"外形” (appearance). According, h外形, 新颖i (happearance, noveli) is the T-P collocation. Generally, T-P collocation is a basic and complete sentiment unit, thus is very useful for many sentiment analysis applications. Features derived from syntactic parse trees are particularly useful for T-P collocation extraction (AbATT basi et al., 2008; Duric and Song, 2012). For example, the syntactic relation “Adj x Noun”, where the ATT denotes an attributive syntactic relation, can be used as an important evidence to extract the T-P collocation h外形, 新颖i (happearance, noveli) in the above sentiment sentence (Bloom et al., 2007; Qiu et al., 2011; Xu et al., 2013). However, one major problem of these approaches is the “naturalness” of sentiment sentences, that is, such sentences are more natural or spontaneous compared with normal sentences, thus posing a challenge to syntactic parsers. Accordingly, many wrong syntactic features have been produced and these can further result in the poor performance of T-P collocation extraction. Taking the sentence in Figure 1(a) as an example, because the word “多亏” (fortunately) is so chatty,1 the parsing result is wrong. Thus, are unable to extract the T-P collocation h键盘, 好i (hke"
C14-1129,J92-4003,0,0.0373749,"ures (cluster(·)) as the other latent semantic feature to further improve the generalization over common words. Word clustering features contain some semantic information and have been successfully used in several natural language processing tasks, including NER (Miller et al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words 外观 and 样子 (appearance) belong to the same word cluster, although they have a different suffix or prefix. Both words are important for T-P collocation extraction and should be retained. We used the Brown word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were obtained from the fifth edition of Chinese Gigaword (LDC2011T13). Finally, similar to McDonald (2006), we also added the dependency relation between a word and its parent as the syntactic features. Intuitively, the dependency relations are helpful in carrying out sentence compression. For example, the ROOT relation typically indicates that the word should not be removed because it is the main verb of a sentence. 4 Experiments 4.1 Experimental Setup 4.1.1 Corpus We conducted the experiments on a Chinese corpus of four product domains, w"
C14-1129,C10-3004,1,0.802646,"3.1 Problem Analysis First, we conducted an error analysis for the results of current T-P collocation extraction, from which we observed that the “naturalness” of sentiment sentences is one of the main problems. For examples: • Chatty form: some sentiment sentences are so chatty, that they bring many difficulties to the parser. For example, in the sentence “多亏键盘好” (fortunately the keyboard is good) shown in Figure 1, the usage of the chatty word “多亏” (fortunately) affects the accuracy of the syntactic parser. 2 A Chinese natural language processing toolkit, Language Technology Platform (LTP) (Che et al., 2010), was used as our dependency parser. More information about the syntactic relations can be found in their paper. The state-of-the-art graph-based dependency parsing model, in the toolkit, was trained on Chinese Dependency Treebank 1.0 (LDC2012T05). 1362 ROOT ROOT ROOT ROOT SBV ADV comp POB 除了 照片 好 besides photo good ATT RAD POB SBV 照片 好 photo good SBV comp 屏幕 给 人 的 感觉 不错 screen for people feel good (a) parse tree 1 before and after compression SBV 屏幕 不错 screen good (b) parse tree 2 before and after compression Figure 3: “Naturalness” problem of sentiment sentences. • Conjunction word usage: co"
C14-1129,N13-1006,1,0.818055,"ehicle), which is also the suffix of the three words. Given that all of them may become targets, they tend to be retained in compressed sentences. The verbs, 感 觉 and 感 到, can be denoted by their prefix feel (感), and can be removed from original sentences because they are feeling words. We used word clustering features (cluster(·)) as the other latent semantic feature to further improve the generalization over common words. Word clustering features contain some semantic information and have been successfully used in several natural language processing tasks, including NER (Miller et al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words 外观 and 样子 (appearance) belong to the same word cluster, although they have a different suffix or prefix. Both words are important for T-P collocation extraction and should be retained. We used the Brown word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were obtained from the fifth edition of Chinese Gigaword (LDC2011T13). Finally, similar to McDonald (2006), we also added the dependency relation between a word and its parent as the syntactic features. Intuitively, the dependenc"
C14-1129,1993.eamt-1.1,0,0.165369,"Missing"
C14-1129,P08-1109,0,0.0239906,"ing the Chinese data. 1360 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1360–1369, Dublin, Ireland, August 23-29 2014. ROOT ROOT SBV SBV VOB 多亏 键盘 好 fortunately keyboard good 键盘 好 keyboard good (a) before compression (b) after compression Figure 1: Parse trees before and after compression. This idea is motivated by the observation that, current syntactic parsers usually perform accurately for short, simple and formal sentences, whereas error rates increase for longer, more complex or more natural and spontaneous sentences (Finkel et al., 2008). Hence, the improvement in syntactic parsing performance would have a ripple effect over T-P collocation extraction. For example, we can compress the sentence in Figure 1(a) into a shortened sentence in Figure 1(b) by removing the chatty part “多亏” (fortunately). We can see that the shortened sentence is now well-formed (in Chinese) and its parse tree is correct, making it easier to accurately extract T-P collocation. Traditional sentence compression aims to obtain a shorter grammatical sentence by retaining important information (usually important grammar structure) (Jing, 2000). For example,"
C14-1129,N10-1131,0,0.0556384,"model to improve this task. 5.2 Sentence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem by using discriminative models. Aside from above extractive sentence compression approaches, there is another research line, namely, abstractive approach, which compresses an original sentence by reorderi"
C14-1129,N07-1023,0,0.019638,"is no previous work using a sentence compression model to improve this task. 5.2 Sentence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem by using discriminative models. Aside from above extractive sentence compression approaches, there is another research line, namely, abstractive a"
C14-1129,A00-1043,0,0.0568815,"ces (Finkel et al., 2008). Hence, the improvement in syntactic parsing performance would have a ripple effect over T-P collocation extraction. For example, we can compress the sentence in Figure 1(a) into a shortened sentence in Figure 1(b) by removing the chatty part “多亏” (fortunately). We can see that the shortened sentence is now well-formed (in Chinese) and its parse tree is correct, making it easier to accurately extract T-P collocation. Traditional sentence compression aims to obtain a shorter grammatical sentence by retaining important information (usually important grammar structure) (Jing, 2000). For example, the sentence “Overall, this is a great camera.” can be compressed into “This is a camera.” by removing the adverbial “overall” and the modifier “great”. However, the modifier “great” is a polarity word and very important for sentiment analysis. Therefore, Sent Comp model for sentiment sentences is different from the traditional compression models, because it needs to retain the important sentiment information, such as the polarity word. Hence, using Sent Comp, the above sentence can be compressed into “This is a great camera.” We regard Sent Comp as a sequence labeling task, whi"
C14-1129,P08-1068,0,0.0190562,"hree words. Given that all of them may become targets, they tend to be retained in compressed sentences. The verbs, 感 觉 and 感 到, can be denoted by their prefix feel (感), and can be removed from original sentences because they are feeling words. We used word clustering features (cluster(·)) as the other latent semantic feature to further improve the generalization over common words. Word clustering features contain some semantic information and have been successfully used in several natural language processing tasks, including NER (Miller et al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words 外观 and 样子 (appearance) belong to the same word cluster, although they have a different suffix or prefix. Both words are important for T-P collocation extraction and should be retained. We used the Brown word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were obtained from the fifth edition of Chinese Gigaword (LDC2011T13). Finally, similar to McDonald (2006), we also added the dependency relation between a word and its parent as the syntactic features. Intuitively, the dependency relations are helpful in carrying out se"
C14-1129,D13-1047,0,0.0165015,"ly affect the performance of syntactic parser. Once our sentiment sentence compression method can improve the quality of parsing, the performance of T-P collocation extraction task can be improved as well. Note that, to date, there is no previous work using a sentence compression model to improve this task. 5.2 Sentence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sente"
C14-1129,E06-1038,0,0.441306,"mation and have been successfully used in several natural language processing tasks, including NER (Miller et al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words 外观 and 样子 (appearance) belong to the same word cluster, although they have a different suffix or prefix. Both words are important for T-P collocation extraction and should be retained. We used the Brown word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were obtained from the fifth edition of Chinese Gigaword (LDC2011T13). Finally, similar to McDonald (2006), we also added the dependency relation between a word and its parent as the syntactic features. Intuitively, the dependency relations are helpful in carrying out sentence compression. For example, the ROOT relation typically indicates that the word should not be removed because it is the main verb of a sentence. 4 Experiments 4.1 Experimental Setup 4.1.1 Corpus We conducted the experiments on a Chinese corpus of four product domains, which came from the Task3 of the Chinese Opinion Analysis Evaluation (COAE) (Zhao et al., 2008).6 Table 2 describes the corpus, 5 6 www.keenage.com www.ir-china."
C14-1129,N04-1043,0,0.0112461,"various kinds of 车 (vehicle), which is also the suffix of the three words. Given that all of them may become targets, they tend to be retained in compressed sentences. The verbs, 感 觉 and 感 到, can be denoted by their prefix feel (感), and can be removed from original sentences because they are feeling words. We used word clustering features (cluster(·)) as the other latent semantic feature to further improve the generalization over common words. Word clustering features contain some semantic information and have been successfully used in several natural language processing tasks, including NER (Miller et al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words 外观 and 样子 (appearance) belong to the same word cluster, although they have a different suffix or prefix. Both words are important for T-P collocation extraction and should be retained. We used the Brown word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were obtained from the fifth edition of Chinese Gigaword (LDC2011T13). Finally, similar to McDonald (2006), we also added the dependency relation between a word and its parent as the syntactic features. Intuiti"
C14-1129,C10-1089,0,0.0122581,"hod can improve the quality of parsing, the performance of T-P collocation extraction task can be improved as well. Note that, to date, there is no previous work using a sentence compression model to improve this task. 5.2 Sentence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem"
C14-1129,J11-1002,0,0.535699,"cording, h外形, 新颖i (happearance, noveli) is the T-P collocation. Generally, T-P collocation is a basic and complete sentiment unit, thus is very useful for many sentiment analysis applications. Features derived from syntactic parse trees are particularly useful for T-P collocation extraction (AbATT basi et al., 2008; Duric and Song, 2012). For example, the syntactic relation “Adj x Noun”, where the ATT denotes an attributive syntactic relation, can be used as an important evidence to extract the T-P collocation h外形, 新颖i (happearance, noveli) in the above sentiment sentence (Bloom et al., 2007; Qiu et al., 2011; Xu et al., 2013). However, one major problem of these approaches is the “naturalness” of sentiment sentences, that is, such sentences are more natural or spontaneous compared with normal sentences, thus posing a challenge to syntactic parsers. Accordingly, many wrong syntactic features have been produced and these can further result in the poor performance of T-P collocation extraction. Taking the sentence in Figure 1(a) as an example, because the word “多亏” (fortunately) is so chatty,1 the parsing result is wrong. Thus, are unable to extract the T-P collocation h键盘, 好i (hkeyboard, goodi). To"
C14-1129,W13-3508,0,0.0116971,"compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem by using discriminative models. Aside from above extractive sentence compression approaches, there is another research line, namely, abstractive approach, which compresses an original sentence by reordering, substituting, and inserting, as well as removing (C"
C14-1129,P05-1036,0,0.0267549,". Note that, to date, there is no previous work using a sentence compression model to improve this task. 5.2 Sentence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem by using discriminative models. Aside from above extractive sentence compression approaches, there is another research l"
C14-1129,P08-1040,0,0.0661594,"modifier “great”. However, the modifier “great” is a polarity word and very important for sentiment analysis. Therefore, Sent Comp model for sentiment sentences is different from the traditional compression models, because it needs to retain the important sentiment information, such as the polarity word. Hence, using Sent Comp, the above sentence can be compressed into “This is a great camera.” We regard Sent Comp as a sequence labeling task, which can be solved by a conditional random fields (CRF) model. Instead of seeking the manual rules on parse trees for compression, as in other studies (Vickrey and Koller, 2008), this method is an automatic procedure. In this work, we introduce some sentiment-related features to retain the sentiment information for Sent Comp. We apply Sent Comp as the first step in the T-P collocation extraction task. First, we compress the sentiment sentences into easier to parse ones using Sent Comp, after which we employ the state-of-theart T-P collocation extraction approach on the compressed sentences. Experimental results on a Chinese corpus of four product domains show the effectiveness of our approach. The main contributions of this paper are as follows: • We present a framew"
C14-1129,D11-1038,0,0.0140719,"ntence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem by using discriminative models. Aside from above extractive sentence compression approaches, there is another research line, namely, abstractive approach, which compresses an original sentence by reordering, substituting, and inser"
C14-1129,P13-1173,0,0.156232,"(happearance, noveli) is the T-P collocation. Generally, T-P collocation is a basic and complete sentiment unit, thus is very useful for many sentiment analysis applications. Features derived from syntactic parse trees are particularly useful for T-P collocation extraction (AbATT basi et al., 2008; Duric and Song, 2012). For example, the syntactic relation “Adj x Noun”, where the ATT denotes an attributive syntactic relation, can be used as an important evidence to extract the T-P collocation h外形, 新颖i (happearance, noveli) in the above sentiment sentence (Bloom et al., 2007; Qiu et al., 2011; Xu et al., 2013). However, one major problem of these approaches is the “naturalness” of sentiment sentences, that is, such sentences are more natural or spontaneous compared with normal sentences, thus posing a challenge to syntactic parsers. Accordingly, many wrong syntactic features have been produced and these can further result in the poor performance of T-P collocation extraction. Taking the sentence in Figure 1(a) as an example, because the word “多亏” (fortunately) is so chatty,1 the parsing result is wrong. Thus, are unable to extract the T-P collocation h键盘, 好i (hkeyboard, goodi). To solve the “natura"
C16-1002,Q16-1031,0,0.148922,"008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition actions. Besides, Duong et al. (2015b) focus on low resource parsing where the target language has a small treebank of ∼3K tokens. Their models may sacrifice accuracy on target"
C16-1002,P16-1231,0,0.0254544,"stack, a buffer and a set of dependency arcs. Traditional parsing models deal with features extracted from manually defined feature templates in a discrete feature space, which suffers from the problems of Sparsity, Incompleteness and Expensive feature computation. The neural network model proposed by Chen and Manning (2014) instead represents features as continuous, low-dimensional vectors and use a cube activation function for implicit feature composition. More recently, this architecture has been improved in several different ways (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016). Here, we employ the LSTM-based architecture enhanced with character bidirectional LSTMs (Ballesteros et al., 2015) for the following major reasons: 3 Duong et al. (2015b) used L2 regularizers to tie the lexical embeddings with a bilingual dictionary. 14 Char-BiLSTM … ? ?? &lt;w&gt; ?? ?? Stack LSTM RecNN ?? ?? v e &lt;/w&gt; … ?? ??+1 … ?1 o Buffer LSTM Action LSTM … l ?? ?? ? ? ?1 ? ? (a) (b) Figure 2: The LSTM-based neural parser (a) and the Char-BiLSTM for modeling words (b). • Compared with Chen & Manning’s architecture, it makes full use of the non-local features by modeling the full history inform"
C16-1002,D15-1041,0,0.052963,". (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. To the best of our knowledge, we present the first work that successfully integrate both monolingual and multilingual treebanks for parsing, with or without consistent annotation schemes. 3 Approach This section describes the deep multi-task learning architecture, using a formalism that extends on the transition-based dependency parsing model with LSTM networks (Dyer et al., 2015) which is further enhanced by modeling characters (Ballesteros et al., 2015). We first revisit the parsing approach of Ballesteros et al. (2015), then present our framework for learning with multi-typed source treebanks. 3.1 Transition-based Neural Parsing Neural models for parsing have gained a lot of interests in recent years, particularly boosted by Chen and Manning (2014). The heart of transition-based parsing is the challenge of representing the state (configuration) of a transition system, based on which the most likely transition action is determined. Typically, a state includes three primary components, a stack, a buffer and a set of dependency arcs. Tradition"
C16-1002,D12-1133,0,0.0622979,"r approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010). More recently, the idea of neural multi-task learning was applied to sequence-to-"
C16-1002,W06-2920,0,0.0880729,"us referred to as deep multi-task learning. We find that different parameter sharing strategies should be applied for different typed source treebanks adaptively, due to the different types of consistencies and inconsistencies (Figure 1). We investigate the effect of multilingual transfer parsing using the Universal Dependency Treebanks (UDT) (McDonald et al., 2013). We show that our approach improves significantly over strong supervised baseline systems in six languages. We further study the effect of monolingual heterogeneous transfer parsing using UDT and the C O NLL-X shared task dataset (Buchholz and Marsi, 2006). We consider using UDT and CoNLL-X as source treebanks respectively, to investigate their mutual benefits. Experiment results show significant improvements under both settings. Moreover, indirect comparisons on the Chinese Penn Treebank 5.1 (CTB5) using the Chinese Dependency Treebank (CDT)1 as source treebank show the merits of our approach over previous work.2 2 Related Work The present work is related to several strands of previous studies. Monolingual resources for parsing Exploiting heterogeneous treebanks for parsing has been explored in various ways. Niu et al. (2009) automatically con"
C16-1002,D08-1092,0,0.0308853,"ank of ∼3K tokens. Their models may sacrifice accuracy on target languages with a large treebank. Ammar et al. (2016) and Vilares et al. (2016) instead train a single parser on a multilingual set of rich-resource treebanks, which is a more similar setting to ours. We refer to their approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharin"
C16-1002,D14-1082,0,0.0307899,"g, with or without consistent annotation schemes. 3 Approach This section describes the deep multi-task learning architecture, using a formalism that extends on the transition-based dependency parsing model with LSTM networks (Dyer et al., 2015) which is further enhanced by modeling characters (Ballesteros et al., 2015). We first revisit the parsing approach of Ballesteros et al. (2015), then present our framework for learning with multi-typed source treebanks. 3.1 Transition-based Neural Parsing Neural models for parsing have gained a lot of interests in recent years, particularly boosted by Chen and Manning (2014). The heart of transition-based parsing is the challenge of representing the state (configuration) of a transition system, based on which the most likely transition action is determined. Typically, a state includes three primary components, a stack, a buffer and a set of dependency arcs. Traditional parsing models deal with features extracted from manually defined feature templates in a discrete feature space, which suffers from the problems of Sparsity, Incompleteness and Expensive feature computation. The neural network model proposed by Chen and Manning (2014) instead represents features as"
C16-1002,P10-1003,0,0.0239738,"e the target language has a small treebank of ∼3K tokens. Their models may sacrifice accuracy on target languages with a large treebank. Ammar et al. (2016) and Vilares et al. (2016) instead train a single parser on a multilingual set of rich-resource treebanks, which is a more similar setting to ours. We refer to their approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving mul"
C16-1002,P15-1166,1,0.812783,"works can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010). More recently, the idea of neural multi-task learning was applied to sequence-to-sequence problems with recurrent neural networks. Dong et al. (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. To the best of our knowledge, we present the first work that successfully integrate both monolingual and multilingual treebanks for parsing, with or without consistent annotation schemes. 3 Approach This section describes the deep multi-task learning architecture, using a formalism that exte"
C16-1002,P15-2139,0,0.461231,"(Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition actions. Besides, Duong et al. (2015b) focus on low resource parsing where the target language has a small treebank of ∼3K tokens. Their models may sa"
C16-1002,D15-1040,0,0.405925,"(Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition actions. Besides, Duong et al. (2015b) focus on low resource parsing where the target language has a small treebank of ∼3K tokens. Their models may sa"
C16-1002,P15-1033,0,0.12877,"translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. To the best of our knowledge, we present the first work that successfully integrate both monolingual and multilingual treebanks for parsing, with or without consistent annotation schemes. 3 Approach This section describes the deep multi-task learning architecture, using a formalism that extends on the transition-based dependency parsing model with LSTM networks (Dyer et al., 2015) which is further enhanced by modeling characters (Ballesteros et al., 2015). We first revisit the parsing approach of Ballesteros et al. (2015), then present our framework for learning with multi-typed source treebanks. 3.1 Transition-based Neural Parsing Neural models for parsing have gained a lot of interests in recent years, particularly boosted by Chen and Manning (2014). The heart of transition-based parsing is the challenge of representing the state (configuration) of a transition system, based on which the most likely transition action is determined. Typically, a state includes three p"
C16-1002,P15-1119,1,0.866599,"en made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition actions. Besides, Duong et al. (2015b) focus on low resource parsing where the target"
C16-1002,P12-1110,0,0.0139446,"ilar setting to ours. We refer to their approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010). More recently, the idea of neural multi"
C16-1002,J13-4006,0,0.0462287,"ial of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010). More recently, the idea of neural multi-task learning was applied to sequence-to-sequence problems with recurrent neural networks. Dong et al. (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence m"
C16-1002,D09-1127,0,0.0242498,"ge has a small treebank of ∼3K tokens. Their models may sacrifice accuracy on target languages with a large treebank. Ammar et al. (2016) and Vilares et al. (2016) instead train a single parser on a multilingual set of rich-resource treebanks, which is a more similar setting to ours. We refer to their approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks with"
C16-1002,N13-1013,0,0.666564,".e., treebanks). Numerous efforts have been made towards the construction of treebanks which established the benchmark research on dependency parsing, such as the widely-used Penn Treebank (Marcus et al., 1993). However, the heavy cost of treebanking typically limits the existing treebanks in both scale and coverage of languages. To address the problem, a variety of authors have proposed to exploit existing heterogeneous treebanks with different annotation schemes via grammar conversion (Niu et al., 2009), quasi-synchronous grammar features (Li et al., 2012) or shared feature representations (Johansson, 2013) for the enhancement of parsing models. Despite their effectiveness in specific datasets, these methods typically lack the scalability of exploiting richer source treebanks, such as cross-lingual treebanks. In this paper, we aim at developing a universal framework for transfer parsing that can exploit multityped source treebanks to improve parsing of a target treebank. Specifically, we will consider two kinds of source treebanks, that are multilingual universal treebanks and monolingual heterogeneous treebanks. Cross-lingual supervision has proven highly beneficial for parsing low-resource lan"
C16-1002,D11-1109,1,0.821766,"We refer to their approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010). More recently, the idea of neural multi-task learning wa"
C16-1002,P12-1071,1,0.935143,"availability and scale of annotated training data (i.e., treebanks). Numerous efforts have been made towards the construction of treebanks which established the benchmark research on dependency parsing, such as the widely-used Penn Treebank (Marcus et al., 1993). However, the heavy cost of treebanking typically limits the existing treebanks in both scale and coverage of languages. To address the problem, a variety of authors have proposed to exploit existing heterogeneous treebanks with different annotation schemes via grammar conversion (Niu et al., 2009), quasi-synchronous grammar features (Li et al., 2012) or shared feature representations (Johansson, 2013) for the enhancement of parsing models. Despite their effectiveness in specific datasets, these methods typically lack the scalability of exploiting richer source treebanks, such as cross-lingual treebanks. In this paper, we aim at developing a universal framework for transfer parsing that can exploit multityped source treebanks to improve parsing of a target treebank. Specifically, we will consider two kinds of source treebanks, that are multilingual universal treebanks and monolingual heterogeneous treebanks. Cross-lingual supervision has p"
C16-1002,J93-2004,0,0.0535212,"datasets in various languages demonstrate that our approach can make effective use of arbitrary source treebanks to improve target parsing models. 1 Introduction As a long-standing central problem in natural language processing (NLP), dependency parsing has been dominated by data-driven approaches for decades. The foundation of data-driven parsing is the availability and scale of annotated training data (i.e., treebanks). Numerous efforts have been made towards the construction of treebanks which established the benchmark research on dependency parsing, such as the widely-used Penn Treebank (Marcus et al., 1993). However, the heavy cost of treebanking typically limits the existing treebanks in both scale and coverage of languages. To address the problem, a variety of authors have proposed to exploit existing heterogeneous treebanks with different annotation schemes via grammar conversion (Niu et al., 2009), quasi-synchronous grammar features (Li et al., 2012) or shared feature representations (Johansson, 2013) for the enhancement of parsing models. Despite their effectiveness in specific datasets, these methods typically lack the scalability of exploiting richer source treebanks, such as cross-lingua"
C16-1002,D11-1006,0,0.0824227,"rsing models. Despite their effectiveness in specific datasets, these methods typically lack the scalability of exploiting richer source treebanks, such as cross-lingual treebanks. In this paper, we aim at developing a universal framework for transfer parsing that can exploit multityped source treebanks to improve parsing of a target treebank. Specifically, we will consider two kinds of source treebanks, that are multilingual universal treebanks and monolingual heterogeneous treebanks. Cross-lingual supervision has proven highly beneficial for parsing low-resource languages (Hwa et al., 2005; McDonald et al., 2011), implying that different languages have a great deal of common ground in grammars. But unfortunately, linguistic inconsistencies also exist in both typologies and lexical representations across languages. Figure 1(a) illustrates two sentences in German and English with universal dependency annotations. The typological differences (subject-verb-object order) results in the opposite directions of the dobj arcs, while the rest arcs remain consistent. Similar problems also come with monolingual heterogeneous treebanks. Figure 1(b) shows an English sentence annotated with respectively the universa"
C16-1002,P09-1006,1,0.95269,"decades. The foundation of data-driven parsing is the availability and scale of annotated training data (i.e., treebanks). Numerous efforts have been made towards the construction of treebanks which established the benchmark research on dependency parsing, such as the widely-used Penn Treebank (Marcus et al., 1993). However, the heavy cost of treebanking typically limits the existing treebanks in both scale and coverage of languages. To address the problem, a variety of authors have proposed to exploit existing heterogeneous treebanks with different annotation schemes via grammar conversion (Niu et al., 2009), quasi-synchronous grammar features (Li et al., 2012) or shared feature representations (Johansson, 2013) for the enhancement of parsing models. Despite their effectiveness in specific datasets, these methods typically lack the scalability of exploiting richer source treebanks, such as cross-lingual treebanks. In this paper, we aim at developing a universal framework for transfer parsing that can exploit multityped source treebanks to improve parsing of a target treebank. Specifically, we will consider two kinds of source treebanks, that are multilingual universal treebanks and monolingual he"
C16-1002,P08-1108,0,0.0257172,"t feature-level with discrete representations, which limits its scalability to multilingual treebanks where feature surfaces might be totally different. On the contrary, our approach is capable of utilizing representation-level parameter sharing, making full use of the multi-level abstractive representations generated by deep neural network. This is the key that makes our framework scalable to multi-typed treebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt"
C16-1002,P09-1040,0,0.0260673,"hin the stack and buffer are modeled with a recursive neural network (RecNN) as described in Dyer et al. (2015). Next, a linear mapping (W) is applied to the concatenation of st , bt and at , and passed through a component-wise ReLU: pt = ReLU(W[st ; bt ; at ] + d) (2) Finally, the probability of next action z ∈ A(S, B) is estimated using a softmax function: p(z∣pt ) = exp(gz⊺ pt + qz ) Σz ′ ∈A(S,B) exp(gz⊺′ pt + qz ′ ) (3) where A(S, B) represents the set of valid actions given the current content in the stack and buffer. We apply the non-projective transition system originally introduced by Nivre (2009) since most of the treebanks we consider in this study has a noticeable proportion of non-projective trees. In the S WAPbased system, both the stack and buffer may contain tree fragments, so RecNN is applied both in S and B to obtain representations of each position. 3.2 Deep Multi-task Learning Multi-task learning (MTL) is the procedure of inductive transfer that improves learning for one task by using the information contained in the training signals of other related tasks. It does this by learning tasks in parallel while using a shared representation. A good overview, especially focusing on"
C16-1002,petrov-etal-2012-universal,0,0.248618,"cific task with hierarchical abstractions, which gives us the flexibility to control parameter sharing in different levels accordingly. In this study, different parameter sharing strategies are applied according to the source and target treebanks being used. We consider two different scenarios: MTL with multilingual universal treebanks as source (M ULTI -U NIV) and MTL with monolingual heterogeneous treebanks as source (M ONO H ETERO). Table 1 presents our parameter sharing strategies for each setting. M ULTI -U NIV Multilingual universal treebanks are annotated with the same set of POS tags (Petrov et al., 2012), dependency relations, and share the same set of transition actions. However, the vocabularies (word, characters) are language-specific. Therefore, it makes sense to share the lookup tables (embeddings) of POS tags (Epos ), relations (Erel ) and actions (Eact ), but separate the character embeddings (Echar ) as well as the Char-BiLSTMs (BiLSTM(chars)). Additionally, linguistic typologies such as the order of subject-verb-object and adjective-noun (Figure 1(a)) varies across languages, which result in the divergence of inherent grammars of transition action sequences. So we set the action LSTM"
C16-1002,D15-1039,0,0.0201532,"ebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition"
C16-1002,D09-1086,0,0.0258726,"sent work is related to several strands of previous studies. Monolingual resources for parsing Exploiting heterogeneous treebanks for parsing has been explored in various ways. Niu et al. (2009) automatically convert the dependency-structure CDT into the phrasestructure style of CTB5 using a trained constituency parser on CTB5, and then combine the converted treebanks for constituency parsing. Li et al. (2012) capture the annotation inconsistencies among different treebanks by designing several types of transformation patterns, based on which they introduce quasi-synchronous grammar features (Smith and Eisner, 2009) to augment the baseline parsing models. Johansson (2013) also adopts the idea of parameter sharing to incorporate multiple treebanks. They focuse on parameter sharing at feature-level with discrete representations, which limits its scalability to multilingual treebanks where feature surfaces might be totally different. On the contrary, our approach is capable of utilizing representation-level parameter sharing, making full use of the multi-level abstractive representations generated by deep neural network. This is the key that makes our framework scalable to multi-typed treebanks and thus mor"
C16-1002,N12-1052,0,0.0751652,"Missing"
C16-1002,C14-1175,0,0.0149779,"o multi-typed treebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks tha"
C16-1002,D07-1099,0,0.0297611,"… ?1 o Buffer LSTM Action LSTM … l ?? ?? ? ? ?1 ? ? (a) (b) Figure 2: The LSTM-based neural parser (a) and the Char-BiLSTM for modeling words (b). • Compared with Chen & Manning’s architecture, it makes full use of the non-local features by modeling the full history information of a state with stack LSTMs. • By modeling words, stack, buffer and action sequence separately which indicate hierarchical abstractions of representations, we can control the information flow across tasks via parameter sharing with more flexibility (Section 3.2). Besides, we did not use the earlier ISBN parsing model (Titov and Henderson, 2007) due to its lack of scalability to large vocabulary. Figure 2(a) illustrates the transition-based parsing architecture using LSTMs. Bidirectional LSTMs are used for modeling the word representations (Figure 2(b)), which we refer to as Char-BiLSTMs henceforth. Char-BiLSTMs learn features for each word, and then the representation of each token can be calculated as: → ← Ð t] + b) x = ReLU(V[Ð w; w; (1) where t is the POS tag embedding. The token embeddings are then fed into subsequent LSTM layers to obtain representations of the stack, buffer and action sequence respectively referred to as st ,"
C16-1002,D08-1017,0,0.0311419,"on parameter sharing at feature-level with discrete representations, which limits its scalability to multilingual treebanks where feature surfaces might be totally different. On the contrary, our approach is capable of utilizing representation-level parameter sharing, making full use of the multi-level abstractive representations generated by deep neural network. This is the key that makes our framework scalable to multi-typed treebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Amm"
C16-1002,P16-2069,0,0.0406272,"Missing"
C16-1002,P15-1032,0,0.011917,"te includes three primary components, a stack, a buffer and a set of dependency arcs. Traditional parsing models deal with features extracted from manually defined feature templates in a discrete feature space, which suffers from the problems of Sparsity, Incompleteness and Expensive feature computation. The neural network model proposed by Chen and Manning (2014) instead represents features as continuous, low-dimensional vectors and use a cube activation function for implicit feature composition. More recently, this architecture has been improved in several different ways (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016). Here, we employ the LSTM-based architecture enhanced with character bidirectional LSTMs (Ballesteros et al., 2015) for the following major reasons: 3 Duong et al. (2015b) used L2 regularizers to tie the lexical embeddings with a bilingual dictionary. 14 Char-BiLSTM … ? ?? &lt;w&gt; ?? ?? Stack LSTM RecNN ?? ?? v e &lt;/w&gt; … ?? ??+1 … ?1 o Buffer LSTM Action LSTM … l ?? ?? ? ? ?1 ? ? (a) (b) Figure 2: The LSTM-based neural parser (a) and the Char-BiLSTM for modeling words (b). • Compared with Chen & Manning’s architecture, it makes full use of the non-local feat"
C16-1002,D15-1213,0,0.0299657,"te different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition actions. Besides, Duong et al. (2015b) focus on low resource parsing where the target language has a small tree"
C16-1002,D08-1059,0,0.0374319,"which limits its scalability to multilingual treebanks where feature surfaces might be totally different. On the contrary, our approach is capable of utilizing representation-level parameter sharing, making full use of the multi-level abstractive representations generated by deep neural network. This is the key that makes our framework scalable to multi-typed treebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual"
C16-1002,C14-1051,1,0.858613,"bility to multilingual treebanks where feature surfaces might be totally different. On the contrary, our approach is capable of utilizing representation-level parameter sharing, making full use of the multi-level abstractive representations generated by deep neural network. This is the key that makes our framework scalable to multi-typed treebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing"
C16-1002,P15-1117,0,0.055438,"imary components, a stack, a buffer and a set of dependency arcs. Traditional parsing models deal with features extracted from manually defined feature templates in a discrete feature space, which suffers from the problems of Sparsity, Incompleteness and Expensive feature computation. The neural network model proposed by Chen and Manning (2014) instead represents features as continuous, low-dimensional vectors and use a cube activation function for implicit feature composition. More recently, this architecture has been improved in several different ways (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016). Here, we employ the LSTM-based architecture enhanced with character bidirectional LSTMs (Ballesteros et al., 2015) for the following major reasons: 3 Duong et al. (2015b) used L2 regularizers to tie the lexical embeddings with a bilingual dictionary. 14 Char-BiLSTM … ? ?? &lt;w&gt; ?? ?? Stack LSTM RecNN ?? ?? v e &lt;/w&gt; … ?? ??+1 … ?1 o Buffer LSTM Action LSTM … l ?? ?? ? ? ?1 ? ? (a) (b) Figure 2: The LSTM-based neural parser (a) and the Char-BiLSTM for modeling words (b). • Compared with Chen & Manning’s architecture, it makes full use of the non-local features by modeling th"
C16-1027,N01-1016,0,0.880966,"Missing"
C16-1027,C10-3004,1,0.742669,"itchboard data. and ‘i mean’ into single token. Automatic POS-tags generated from pocket crf (Qian and Liu, 2013) are used as POS-tag in our experiments. No public Chinese corpus is available now. For our Chinese experiments, we collect about 200k spoken sentences from minutes of meetings and annotate them with only disfluency annotations according to the guideline proposed by (Meteer et al., 1995). We respectively select about 20k sentences for development and testing. The rest are used for training. We use the word segmentation and POS-tag tools provided by the Language Technology Platform (Che et al., 2010) for preprocessing the original data in our experiments. Metric. Following previous works (Ferguson et al., 2015; Wu et al., 2015), token-based precision (P), recall (R), and f-score (F1) are used as the evaluation metrics. 5.2 Performance of disfluency detection on English Swtichboard corpus We build a baseline system using the Conditional Random Field (CRF) model. The hand-crafted discrete features of our CRF refer to those in (Ferguson et al., 2015). Table 3 shows the result of our model on both the development and test set. We compare our neural attention-based model to four previous top p"
C16-1027,2013.iwslt-papers.12,0,0.632629,"Missing"
C16-1027,D14-1179,0,0.0408974,"Missing"
C16-1027,P15-1033,0,0.0303834,"nce xk is selected, we will use it to update the state of the decoder RNN and select another word from the words ranging from 282 (xk+1 , ..., xl ) (as shown in Figure 2). To learn the parameters of our neural attention-based model, we minimize the negative log-probability of the output sequence over the input data {(xi , yi )}N n=1 during training: − N X log(p(yi |xi )) = − i=1 N X T Y log( sof tmax(ut )) i=1 (7) t=1 The detailed learning algorithm is shown in Algorithm 1. 4 Network training 4.1 Parameters Pretrained word embedding. There are lots of methods for creating word embeddings. As (Dyer et al., 2015) does, we use a variant of the skip n-gram model introduced by (Ling et al., 2015), named “structured skip n-gram”, where a different set of parameters are used to predict each context word depending on its position relative to the target word. The hyperparameters of the model are the same as in the skip n-gram model defined in word2vec (Mikolov et al., 2013). We set the window size to 5, and use a negative sampling rate to 10. The AFP portion of English Gigaword corpus (version 5) is used as the training corpus. Hyper-Parameters. The LSTMs of both encoder and decoder has two hidden layers and"
C16-1027,N15-1029,0,0.627938,"ous disfluency detection works focus on detecting the repair type disfluencies. A flight to um mean Denver ! Boston !&quot;# I!&quot;# !&quot; $ $ # Tuesday FP RM IM RP Figure 1: A sentence with disfluencies annotated in the style of (Shriberg, 1994) and the Switchboard corpus. FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair. We follow previous works in evaluating the system on the accuracy with which it identifies speech-repairs, marked reparandum above. Modeling long-range dependencies between repair phrases is one of the core problems for disfluency detection. Previous sequence tagging methods (Ferguson et al., 2015; Georgila, 2009; Qian and Liu, 2013) require carefully designed features to capture the information of long distance, but usually suffer from the sparsity problem. Another line of syntax-based disfluency detection works (Honnibal and Johnson, 2014; Wu et al., 2015) try to model the repair phrases on a syntax tree by compressing the unrelated * Corresponding author: Wanxiang Che This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 278 Proceedings of COLING 2016, the 26th International Conference on C"
C16-1027,N09-2028,0,0.214941,"n works focus on detecting the repair type disfluencies. A flight to um mean Denver ! Boston !&quot;# I!&quot;# !&quot; $ $ # Tuesday FP RM IM RP Figure 1: A sentence with disfluencies annotated in the style of (Shriberg, 1994) and the Switchboard corpus. FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair. We follow previous works in evaluating the system on the accuracy with which it identifies speech-repairs, marked reparandum above. Modeling long-range dependencies between repair phrases is one of the core problems for disfluency detection. Previous sequence tagging methods (Ferguson et al., 2015; Georgila, 2009; Qian and Liu, 2013) require carefully designed features to capture the information of long distance, but usually suffer from the sparsity problem. Another line of syntax-based disfluency detection works (Honnibal and Johnson, 2014; Wu et al., 2015) try to model the repair phrases on a syntax tree by compressing the unrelated * Corresponding author: Wanxiang Che This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 278 Proceedings of COLING 2016, the 26th International Conference on Computational Lin"
C16-1027,Q14-1011,0,0.620852,"Switchboard corpus. FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair. We follow previous works in evaluating the system on the accuracy with which it identifies speech-repairs, marked reparandum above. Modeling long-range dependencies between repair phrases is one of the core problems for disfluency detection. Previous sequence tagging methods (Ferguson et al., 2015; Georgila, 2009; Qian and Liu, 2013) require carefully designed features to capture the information of long distance, but usually suffer from the sparsity problem. Another line of syntax-based disfluency detection works (Honnibal and Johnson, 2014; Wu et al., 2015) try to model the repair phrases on a syntax tree by compressing the unrelated * Corresponding author: Wanxiang Che This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 278 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 278–287, Osaka, Japan, December 11-17 2016. they/E had/E they/E used/E to/E have/E well they do have television monitors stationed throughout our buildings. and the/E other/E one/E is/E her husband"
C16-1027,P04-1005,0,0.474051,"hinese annotated corpus In addition to English experiments, we also apply our method on Chinese annotated data. As there is no standard Chinese corpus, no Chinese experimental results are reported in (Honnibal and Johnson, 2014) and (Qian and Liu, 2013). We only use the CRF-based labeling model as our baselines. Table 6 shows the results of Chinese disfluency detection. Our models outperform the CRF model by more than 7 points on f-score which shows that our method is more effective. 6 Related Work Most related works on disfluency detection are aimed at detecting repair type of disfluencies. (Johnson and Charniak, 2004) proposed a TAG-based noisy channel model for disfluency detection. The TAG model was used to find rough copies. Following the work of (Johnson and Charniak, 2004), (Zwarts and Johnson, 2011) extended the TAG model using minimal expected f-loss oriented n-best reranking with additional corpus for language model training. (Qian and Liu, 2013) proposed a muiti-step learning method using weighted max-margin markov network (M3 N). They showed that M3 N model outperformed many other labeling models such as CRF model. (Ferguson et al., 2015) used the Semi-Markov CRF model for disfluency detection an"
C16-1027,N06-2019,0,0.687698,"nd Johnson, 2011) extended the TAG model using minimal expected f-loss oriented n-best reranking with additional corpus for language model training. (Qian and Liu, 2013) proposed a muiti-step learning method using weighted max-margin markov network (M3 N). They showed that M3 N model outperformed many other labeling models such as CRF model. (Ferguson et al., 2015) used the Semi-Markov CRF model for disfluency detection and achieved high f-score by integrating prosodic features. Many syntax-based approaches have been proposed which jointly perform dependency parsing and disfluency detection. (Lease and Johnson, 2006) involved disfluency detection in a PCFG parser to parse the input along with detecting disfluencies. (Rasooli and Tetreault, 2013) designed a joint model for both disfluency detection and dependency parsing. (Honnibal and Johnson, 2014) presented a new joint model by extending the original transition actions with a new “Edit” transition. This model achieved good performance on both disfluency detection and parsing. (Wu et al., 2015) proposed a right-toleft transition-based joint method and achieved the state-of-the-art performance compared with previous syntax-based approaches. RNN had been u"
C16-1027,N15-1142,0,0.0156349,"ct another word from the words ranging from 282 (xk+1 , ..., xl ) (as shown in Figure 2). To learn the parameters of our neural attention-based model, we minimize the negative log-probability of the output sequence over the input data {(xi , yi )}N n=1 during training: − N X log(p(yi |xi )) = − i=1 N X T Y log( sof tmax(ut )) i=1 (7) t=1 The detailed learning algorithm is shown in Algorithm 1. 4 Network training 4.1 Parameters Pretrained word embedding. There are lots of methods for creating word embeddings. As (Dyer et al., 2015) does, we use a variant of the skip n-gram model introduced by (Ling et al., 2015), named “structured skip n-gram”, where a different set of parameters are used to predict each context word depending on its position relative to the target word. The hyperparameters of the model are the same as in the skip n-gram model defined in word2vec (Mikolov et al., 2013). We set the window size to 5, and use a negative sampling rate to 10. The AFP portion of English Gigaword corpus (version 5) is used as the training corpus. Hyper-Parameters. The LSTMs of both encoder and decoder has two hidden layers and their dimensions are set to 100. Pretrained word embeddings have 100 dimensions a"
C16-1027,N13-1102,0,0.357147,"detecting the repair type disfluencies. A flight to um mean Denver ! Boston !&quot;# I!&quot;# !&quot; $ $ # Tuesday FP RM IM RP Figure 1: A sentence with disfluencies annotated in the style of (Shriberg, 1994) and the Switchboard corpus. FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair. We follow previous works in evaluating the system on the accuracy with which it identifies speech-repairs, marked reparandum above. Modeling long-range dependencies between repair phrases is one of the core problems for disfluency detection. Previous sequence tagging methods (Ferguson et al., 2015; Georgila, 2009; Qian and Liu, 2013) require carefully designed features to capture the information of long distance, but usually suffer from the sparsity problem. Another line of syntax-based disfluency detection works (Honnibal and Johnson, 2014; Wu et al., 2015) try to model the repair phrases on a syntax tree by compressing the unrelated * Corresponding author: Wanxiang Che This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 278 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical P"
C16-1027,D13-1013,0,0.761413,"guage model training. (Qian and Liu, 2013) proposed a muiti-step learning method using weighted max-margin markov network (M3 N). They showed that M3 N model outperformed many other labeling models such as CRF model. (Ferguson et al., 2015) used the Semi-Markov CRF model for disfluency detection and achieved high f-score by integrating prosodic features. Many syntax-based approaches have been proposed which jointly perform dependency parsing and disfluency detection. (Lease and Johnson, 2006) involved disfluency detection in a PCFG parser to parse the input along with detecting disfluencies. (Rasooli and Tetreault, 2013) designed a joint model for both disfluency detection and dependency parsing. (Honnibal and Johnson, 2014) presented a new joint model by extending the original transition actions with a new “Edit” transition. This model achieved good performance on both disfluency detection and parsing. (Wu et al., 2015) proposed a right-toleft transition-based joint method and achieved the state-of-the-art performance compared with previous syntax-based approaches. RNN had been used to disfluency detection. (Hough and Schlangen, 2015) explored incremental detection, with an objective that combines detection"
C16-1027,D15-1044,0,0.043799,"Missing"
C16-1027,P15-1048,0,0.771224,"ed Pause, RM=Reparandum, IM=Interregnum, RP=Repair. We follow previous works in evaluating the system on the accuracy with which it identifies speech-repairs, marked reparandum above. Modeling long-range dependencies between repair phrases is one of the core problems for disfluency detection. Previous sequence tagging methods (Ferguson et al., 2015; Georgila, 2009; Qian and Liu, 2013) require carefully designed features to capture the information of long distance, but usually suffer from the sparsity problem. Another line of syntax-based disfluency detection works (Honnibal and Johnson, 2014; Wu et al., 2015) try to model the repair phrases on a syntax tree by compressing the unrelated * Corresponding author: Wanxiang Che This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 278 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 278–287, Osaka, Japan, December 11-17 2016. they/E had/E they/E used/E to/E have/E well they do have television monitors stationed throughout our buildings. and the/E other/E one/E is/E her husband is in the navy. th"
C16-1027,P11-1071,0,0.0538044,"ted in (Honnibal and Johnson, 2014) and (Qian and Liu, 2013). We only use the CRF-based labeling model as our baselines. Table 6 shows the results of Chinese disfluency detection. Our models outperform the CRF model by more than 7 points on f-score which shows that our method is more effective. 6 Related Work Most related works on disfluency detection are aimed at detecting repair type of disfluencies. (Johnson and Charniak, 2004) proposed a TAG-based noisy channel model for disfluency detection. The TAG model was used to find rough copies. Following the work of (Johnson and Charniak, 2004), (Zwarts and Johnson, 2011) extended the TAG model using minimal expected f-loss oriented n-best reranking with additional corpus for language model training. (Qian and Liu, 2013) proposed a muiti-step learning method using weighted max-margin markov network (M3 N). They showed that M3 N model outperformed many other labeling models such as CRF model. (Ferguson et al., 2015) used the Semi-Markov CRF model for disfluency detection and achieved high f-score by integrating prosodic features. Many syntax-based approaches have been proposed which jointly perform dependency parsing and disfluency detection. (Lease and Johnson"
C16-1120,C10-3009,0,0.119928,"ctic path. Besides, they still need a relatively small amount of feature engineering to make use of the local contexts. Another line of research focuses on neural models (Collobert et al., 2011; Zhou and Xu, 2015; FitzGerald et al., 2015), which have shown great effectiveness in automatic feature learning on a variety of NLP tasks. Most recently, Roth and Lapata (2016) employ LSTM-based recurrent neural networks to obtain the representations of syntactic path features, which is similar to our work. Aside from the distributed path features, they also use a set of binary input feature sets from Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also been relying heavily on human-engineered features (Rink and Harabagiu, 2010). Recent years have seen a great deal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) us"
C16-1120,D12-1133,0,0.0489228,"(2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and semantic role labeling (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012; Henderson et al., 2013; Llu´ıs et al., 2013). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural modeling for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. This work also inspires us in this study to develop a unified architecture for SRL and RC in prior to joint training. Recently, the idea of neural multi-task learning was applied to sequence-to-sequence pro"
C16-1120,H05-1091,0,0.100275,"generated by our model to get the global optimization. We use the three constraints defined in Che et al. (2008): • C1: Each word should be labeled with one and only one label (including NULL). • C2: Roles with a small probability (smaller than 0.3) should never be labeled (except for NULL). • C3: Some roles (except for NULL) usually appear once for a predicate in a sentence. Hence a non-duplicate-roles list is utilized for each language. 5 Multi-task Learning The commonalities between SRL and RC inspire us to explore their potential mutual benefits. According to the Shortest Path Hypothesis (Bunescu and Mooney, 2005), if e1 and e2 are two entities mentioned in the same sentence such that they are observed to be in a certain relationship R, they often indicate two arguments of the same predicate or a sequence of predicates. To gain more insights, let’s look at the following example in RC: 1268 Instrument-Agency(e2 , e1 ) “The author[e1 ] of a keygen uses a disassembler[e2 ] to look at the raw assembly code.” Here, the “Instrument-Agency” relation provides significant evidences that author and disassembler are two arguments of a certain predicate, most likely with semantic roles A0 (agent) and A1 (patient)."
C16-1120,W08-2134,1,0.792996,"¶ ´                           ¸                            ¶ (4) p(c∣p) = softmax(gc⊺ p + qc ) (5) Global Context Syntactic Path Our model is trained by minimizing the cross-entropy loss: L(θ) = − ∑N i=0 log p(ci ∣pi ), where N is number of training instances. 4.4 Post-Inference with Integer Linear Programming for SRL SRL is a structure prediction problem and the predicted results should satisfy some structural constraints. For instance, some roles only appear once for a predicate in a sentence. Following Punyakanok et al. (2004) and Che et al. (2008), we apply ILP on the probability distributions at each token generated by our model to get the global optimization. We use the three constraints defined in Che et al. (2008): • C1: Each word should be labeled with one and only one label (including NULL). • C2: Roles with a small probability (smaller than 0.3) should never be labeled (except for NULL). • C3: Some roles (except for NULL) usually appear once for a predicate in a sentence. Hence a non-duplicate-roles list is utilized for each language. 5 Multi-task Learning The commonalities between SRL and RC inspire us to explore their potentia"
C16-1120,W09-1207,1,0.825439,"C, which effectively captures global contextual features, syntactical features and lexical semantic features. • We show that SRL can be significantly improved by jointly training with RC, reaching new stateof-the-art performance. 2 Related Work The present work ties together several strands of previous studies. Semantic Role Labeling A great deal of previous SRL research has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introdu"
C16-1120,C10-3004,1,0.762732,"rix-Vector Recursive Neural Network (MV-RNN) model of Socher et al. (2012), the CNN model of Zeng et al. (2014), the tensor-based model of Yu et al. (2014), the CNN model using ranking loss (dos Santos et al., 2015), and the dependency-based neural network models (Liu et al., 2015; Xu et al., 2015b). Word embeddings are pretrained using word2vec on large-scale unlabeled data. For English, Catalan, German and Spanish, we use the latest Wikipedia data. For Chinese, we obtain the raw text from Xinhua news section (2000–2010) of the fifth edition of Chinese Gigaword (LDC2011T13). The LTP toolkit (Che et al., 2010) is applied to segment Chinese text into words. We adopt predicate-wise training for SRL and sentence-wise training for RC, and use stochastic gradient descent for optimization. Initial learning rate is set to η0 = 0.1 and updated as ηt = η0 /(1 + 0.1t) on each epoch t. Our hyperparameters for the unified model are listed in Table 1. When training RC-only models, the LSTM input/hidden dimension is set to 200, and the dimension of hidden layer is 400. Dimension of embeddings word POS NE WordNet 200 25 25 25 Dimension of layers LSTM input LSTM hidden hidden 100 100 200 Table 1: Hyperparameters s"
C16-1120,J81-4005,0,0.764154,"Missing"
C16-1120,P15-1166,1,0.725108,"., 2013). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural modeling for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. This work also inspires us in this study to develop a unified architecture for SRL and RC in prior to joint training. Recently, the idea of neural multi-task learning was applied to sequence-to-sequence problems with recurrent neural networks. Dong et al. (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. Liu et al. (2016) incorporate different kinds of corpus for implicit discourse relation classification using multi-task neural networks. More recently, multi-task learning has also been applied to sentence compression (Klerke et al., 2016) and machine translation quality estimation (Shah and"
C16-1120,P15-1061,0,0.10748,"midhuber (1997). The LSTMs take as input the token representation xi in each position. The hidden state vectors of the two directions’ LSTM units corresponding to each target word are then concatenated as its global context representation: Ð → ← Ð Ð → ← Ð Rgc Rgc (2) e1 = [ h e1 ; h e1 ]; e2 = [ h e2 ; h e2 ] Note that an important difference between our model and previous neural models is that we utilize the hidden state vectors of e1 and e2 instead of the representation of the whole sentence, which frees us from using position-related features (Zeng et al., 2014; Collobert et al., 2011; dos Santos et al., 2015). 4.3 Syntactic Path Representation We define the nearest common ancestor token of e1 and e2 as nca(e1 , e2 ). Then the path from e1 , e2 to nca(e1 , e2 ), i.e., e1 → . . . → nca(e1 , e2 ) and nca(e1 , e2 ) ← . . . ← e2 , are also modeled with bidirectional LSTMs, as shown in Figure 2 (right panel). We use two kinds of syntactic paths, including a generic path that takes the token representation xi as input, and a relation path that takes the dependency relations along the path as input (Figure 2). These two paths are modeled with BiLSTMgen and BiLSTMrel respectively. The hidden state vectors"
C16-1120,D15-1112,0,0.036994,"Missing"
C16-1120,J02-3001,0,0.823726,"owards the understanding of natural language sentences. Multi-typed semantic relations have been defined between two terms in a sentence in natural language processing (NLP) to promote various applications. For instance, the task of Semantic Role Labeling (SRL) defines shallow semantic dependencies between arguments and predicates, identifying the semantic roles, e.g., who did what to whom, where, when, and how. SRL has been a long-standing and challenging problem in NLP, primarily because it is strongly dependent on rich contextual and syntactical features used by the underlying classifiers (Gildea and Jurafsky, 2002). Another instance is Relation Classification (RC) which assigns sentences with two marked entities (or nominals) to a predefined set of relations (Hendrickx et al., 2010). Compared with SRL, relations defined in RC express much deeper semantics. Figure 1 shows example annotations of SRL and RC respectively. These two problems are typically studied separately in different communities. Hence the connections between them are neglected, both in data resources and approaches. In this paper, we show that SRL and RC have a lot of common ground and can be modeled with a unified model. We start by loo"
C16-1120,P12-1110,0,0.0555057,"16). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and semantic role labeling (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012; Henderson et al., 2013; Llu´ıs et al., 2013). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural modeling for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. This work also inspires us in this study to develop a unified architecture for SRL and RC in prior to joint training. Recently, the idea of neural multi-task learnin"
C16-1120,J13-4006,0,0.0450169,"or-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and semantic role labeling (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012; Henderson et al., 2013; Llu´ıs et al., 2013). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural modeling for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. This work also inspires us in this study to develop a unified architecture for SRL and RC in prior to joint training. Recently, the idea of neural multi-task learning was applied to sequence-to-sequence problems with recurrent neu"
C16-1120,S10-1006,0,0.134728,"Missing"
C16-1120,N16-1179,0,0.0329688,"e-to-sequence problems with recurrent neural networks. Dong et al. (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. Liu et al. (2016) incorporate different kinds of corpus for implicit discourse relation classification using multi-task neural networks. More recently, multi-task learning has also been applied to sentence compression (Klerke et al., 2016) and machine translation quality estimation (Shah and Specia, 2016). 3 Problem Definition This section gives formal definitions of the two tasks to be investigated: SRL and RC. 3.1 Semantic Role Labeling We follow the setup of the CoNLL-2009 shared task. Given a sentence s, each token is annotated with a predicated POS tag and predicted word lemma. Some tokens are also marked as predicates. Besides, 1266 UNESCO ?1 Lexical Semantic Features ?2 is ?1 ?2 holding ?1 ?2 its ?1 meetings ?1 ?2 ?2 in ?1 Paris ?1 ?2 ROOT ?2 SBJ VC UNESCO is is holding nearest common ancestor ???? ???????? ????????? ???"
C16-1120,N15-1121,0,0.0244141,"Missing"
C16-1120,D11-1109,1,0.853687,"et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and semantic role labeling (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012; Henderson et al., 2013; Llu´ıs et al., 2013). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural modeling for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. This work also inspires us in this study to develop a unified architecture for SRL and RC in prior to joint training. Recently, the idea of neural multi-task learning was applied to"
C16-1120,P15-2047,0,0.130443,"ave seen a great deal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS taggin"
C16-1120,Q13-1018,0,0.135887,"Missing"
C16-1120,P14-5010,0,0.00386489,"rve that SRL converges about 4 times slower than RC by running them separately, hence we sample from SRL 4 times often than RC during training. Despite the lack of theoretical guarantee, we found it working well in practice. Second, the key for multi-task learning to work is parameter sharing. Given the unified architecture, we can share most of the network parameters for knowledge transfer. Note that different dependency parses might be used for SRL and RC in practice. In this work, we use the officially provided predicted parses from CoNLL-2009 shared task in SRL, but adopt Stanford parser (Manning et al., 2014) to obtain parses for sentences in RC. These kinds of parses are quite different in terms of both the head-finding rules and the dependency relations. Therefore, we set the parameters involving dependency path modeling as task-specific, i.e., BiLSTMgen , BiLSTMrel and Wsp (Figure 2). The output weights (g) are task-specific as standard of multi-task learning, in order to handle different set of relations to be classified in SRL and RC. 6 Experiment In this section, we first describe data and our experimental settings, then the results and analysis. 6.1 Data and Settings For SRL, we evaluate on"
C16-1120,S14-2082,0,0.0236389,"reat deal of previous SRL research has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approaches typically suffer from high computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds of basic feature sets. Howev"
C16-1120,P16-1105,0,0.0255862,"ng neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and semantic role labeling (Hat"
C16-1120,J08-2003,0,0.0372843,"re templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approaches typically suffer from high computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds of basic feature sets. However, tensor-based approaches cannot well generalize the high-sparsity structural features like syntactic path. Besides, they still need a relatively small amount of feature engineering to make use of the local contexts. Another line of research f"
C16-1120,C04-1197,0,0.144135,"Missing"
C16-1120,S10-1057,0,0.218415,"ffectiveness in automatic feature learning on a variety of NLP tasks. Most recently, Roth and Lapata (2016) employ LSTM-based recurrent neural networks to obtain the representations of syntactic path features, which is similar to our work. Aside from the distributed path features, they also use a set of binary input feature sets from Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also been relying heavily on human-engineered features (Rink and Harabagiu, 2010). Recent years have seen a great deal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs"
C16-1120,P16-1113,0,0.0848515,"gh computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds of basic feature sets. However, tensor-based approaches cannot well generalize the high-sparsity structural features like syntactic path. Besides, they still need a relatively small amount of feature engineering to make use of the local contexts. Another line of research focuses on neural models (Collobert et al., 2011; Zhou and Xu, 2015; FitzGerald et al., 2015), which have shown great effectiveness in automatic feature learning on a variety of NLP tasks. Most recently, Roth and Lapata (2016) employ LSTM-based recurrent neural networks to obtain the representations of syntactic path features, which is similar to our work. Aside from the distributed path features, they also use a set of binary input feature sets from Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also been relying heavily on human-engineered features (Rink and Harabagiu, 2010). Recent years have seen a great deal of work on using neural networks to alleviat"
C16-1120,D14-1045,0,0.0554788,"Missing"
C16-1120,N16-1069,0,0.0260159,". (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. Liu et al. (2016) incorporate different kinds of corpus for implicit discourse relation classification using multi-task neural networks. More recently, multi-task learning has also been applied to sentence compression (Klerke et al., 2016) and machine translation quality estimation (Shah and Specia, 2016). 3 Problem Definition This section gives formal definitions of the two tasks to be investigated: SRL and RC. 3.1 Semantic Role Labeling We follow the setup of the CoNLL-2009 shared task. Given a sentence s, each token is annotated with a predicated POS tag and predicted word lemma. Some tokens are also marked as predicates. Besides, 1266 UNESCO ?1 Lexical Semantic Features ?2 is ?1 ?2 holding ?1 ?2 its ?1 meetings ?1 ?2 ?2 in ?1 Paris ?1 ?2 ROOT ?2 SBJ VC UNESCO is is holding nearest common ancestor ???? ???????? ????????? ????????? SBJ Global Context Representation ??? ??? ROOT VC Syntactic"
C16-1120,D12-1110,0,0.304758,"yntactic path features, which is similar to our work. Aside from the distributed path features, they also use a set of binary input feature sets from Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also been relying heavily on human-engineered features (Rink and Harabagiu, 2010). Recent years have seen a great deal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination"
C16-1120,P03-1002,0,0.0441816,"ificantly improved by jointly training with RC, reaching new stateof-the-art performance. 2 Related Work The present work ties together several strands of previous studies. Semantic Role Labeling A great deal of previous SRL research has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, t"
C16-1120,J08-2002,0,0.0306309,"mantic Role Labeling A great deal of previous SRL research has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approaches typically suffer from high computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds o"
C16-1120,N16-1065,0,0.0356887,"Missing"
C16-1120,D15-1062,0,0.151139,"eal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and se"
C16-1120,D15-1206,0,0.0976347,"eal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and se"
C16-1120,W04-3212,0,0.0766239,"ointly training with RC, reaching new stateof-the-art performance. 2 Related Work The present work ties together several strands of previous studies. Semantic Role Labeling A great deal of previous SRL research has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approa"
C16-1120,D14-1041,0,0.0363967,"esearch has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approaches typically suffer from high computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds of basic feature sets. However, tensor-based appro"
C16-1120,N15-1155,0,0.0531329,"Missing"
C16-1120,C14-1220,0,0.564155,"Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also been relying heavily on human-engineered features (Rink and Harabagiu, 2010). Recent years have seen a great deal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL"
C16-1120,P15-1109,0,0.0258044,"pturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approaches typically suffer from high computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds of basic feature sets. However, tensor-based approaches cannot well generalize the high-sparsity structural features like syntactic path. Besides, they still need a relatively small amount of feature engineering to make use of the local contexts. Another line of research focuses on neural models (Collobert et al., 2011; Zhou and Xu, 2015; FitzGerald et al., 2015), which have shown great effectiveness in automatic feature learning on a variety of NLP tasks. Most recently, Roth and Lapata (2016) employ LSTM-based recurrent neural networks to obtain the representations of syntactic path features, which is similar to our work. Aside from the distributed path features, they also use a set of binary input feature sets from Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also"
C18-1105,P05-1074,0,0.093071,"ese classic approaches, adding noise to the image, randomly interpolating a pair of images (Zhang et al., 2018) are also proposed in previous works. However, these signal transformation approaches are not directly applicable to language because order of words in language may form rigorous syntactic and semantic meaning (Zhang et al., 2015). Therefore, the best way of data augmentation in language usually involves generating the alternative expressions. Paraphrasing is the most studied techniques in natural language processing for generating alternative expressions (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Callison-Burch, 2008). However, generic paraphrasing technique has been reported not helpful for specific problem (Narayan et al., 2016). Most of the successful work that applying paraphrasing for data augmentation requires special tailored paraphrasing techniques. For example, Wang and Yang (2015) performed word-level paraphrasing to extend their corpus on twitter that contains annoying behaviors. Fader et al. (2013) derived question templates from seed paraphrases and bootstrap the templates to achieve the enlarged open-domain QA dataset. Narayan et al. (2016) constructed latent variable P"
C18-1105,P01-1008,0,0.0305918,"sky et al., 2012). Beyond these classic approaches, adding noise to the image, randomly interpolating a pair of images (Zhang et al., 2018) are also proposed in previous works. However, these signal transformation approaches are not directly applicable to language because order of words in language may form rigorous syntactic and semantic meaning (Zhang et al., 2015). Therefore, the best way of data augmentation in language usually involves generating the alternative expressions. Paraphrasing is the most studied techniques in natural language processing for generating alternative expressions (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Callison-Burch, 2008). However, generic paraphrasing technique has been reported not helpful for specific problem (Narayan et al., 2016). Most of the successful work that applying paraphrasing for data augmentation requires special tailored paraphrasing techniques. For example, Wang and Yang (2015) performed word-level paraphrasing to extend their corpus on twitter that contains annoying behaviors. Fader et al. (2013) derived question templates from seed paraphrases and bootstrap the templates to achieve the enlarged open-domain QA dataset. Narayan et al. (2"
C18-1105,D08-1021,0,0.0143971,"se to the image, randomly interpolating a pair of images (Zhang et al., 2018) are also proposed in previous works. However, these signal transformation approaches are not directly applicable to language because order of words in language may form rigorous syntactic and semantic meaning (Zhang et al., 2015). Therefore, the best way of data augmentation in language usually involves generating the alternative expressions. Paraphrasing is the most studied techniques in natural language processing for generating alternative expressions (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Callison-Burch, 2008). However, generic paraphrasing technique has been reported not helpful for specific problem (Narayan et al., 2016). Most of the successful work that applying paraphrasing for data augmentation requires special tailored paraphrasing techniques. For example, Wang and Yang (2015) performed word-level paraphrasing to extend their corpus on twitter that contains annoying behaviors. Fader et al. (2013) derived question templates from seed paraphrases and bootstrap the templates to achieve the enlarged open-domain QA dataset. Narayan et al. (2016) constructed latent variable PCFG for questions and a"
C18-1105,W17-5506,0,0.0578439,"nd syntactical alternatives. To further encourage diverse generation, we incorporate a novel diversity rank into the utterance representation. When training the seq2seq model, the diversity rank is also used to filter the over-alike pairs of alternatives. These approaches lead to diversely augmented data that significantly improves the LU performance in the domains that labeled data is scarce. We conduct experiments on the Airline Travel Information System dataset (ATIS, Price 1990) along with a newly annotated layer of slot filling over the Stanford Multi-turn, Multi-domain Dialogue Dataset (Eric and Manning, 2017).1 On the small proportion of ATIS which contains 129 utterances, our method outperforms the baseline by a 6.38 F-score on slot filling. On the medium proportion, this improvement is 2.87. Similar trends are witnessed on our LU annotation over Stanford dialogue dataset which the average improvement on three new domains is 10.04 on 100 utterances and 0.47 on 500 utterances. The major contributions of this paper include: • We propose a data augmentation framework for LU (§2) using the seq2seq model. A novel diversity rank (§3) is used to encourage our seq2seq model to generate diverse utterances"
C18-1105,P13-1158,0,0.640951,"training data makes LU vulnerable to unseen utterances which are syntactically different but semantically related to the existing training data, and further harms the whole task-oriented dialogue system pipeline. Data augmentation, which enlarges the size of training data in machine learning systems, is an effective solution to the data insufficiency problem. Success has been achieved with data augmentation on a wide range of problems including computer vision (Krizhevsky et al., 2012), speech recognition (Hannun et al., 2014), text classification (Zhang et al., 2015), and question answering (Fader et al., 2013). However, its application in the task-oriented dialogue system is less studied. Kurata et al. (2016a) presented the only work we know that tried to augment data for LU. In their paper, an encoder-decoder is This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ * Email corresponding. Licence details: http:// 1234 Proceedings of the 27th International Conference on Computational Linguistics, pages 1234–1245 Santa Fe, New Mexico, USA, August 20-26, 2018. learned to reconstruct the utterances in the training data. During the aug"
C18-1105,D13-1111,0,0.024998,"diverse rank k, we use the standard seq2seq model to generate the alternative delexicalised utterance d0 . In our seq2seq model, we append #k to the end of the input utterances and the model is formalized as Y p(d0 |d, k) = p(d0t |d1 , ..., dn , #k, d01 , ..., d0t−1 ) t where n is the number of words for the input utterance d. In this paper, we follow the seq2seq model for neural machine translation and use the input-feeding network in (Luong et al., 2015) with attention as our seq2seq model. During testing, we use beam search with beam size of 10 to yield more than one translation following Gimpel et al. (2013) and Vijayakumar et al. (2016). To train the seq2seq model, our basic assumption is that if d and d0 contain the same semantic frames, they can be generated from each other. Generally, we assume each pair of delexicalised utterances in the cluster Cs makes a pair of generation. However, it’s nontrivial to assign diverse ranks to training data. What’s more, to prevent the model from just producing produce lexical paraphrases (like “show me” to “give me”), we propose to also consider the diversities when generating training translations for the seq2seq model. We will talk about the details in Se"
C18-1105,P16-1002,0,0.0493762,"encoder’s output hidden states are randomly perturbed to yield different utterances. The work of Kurata et al. (2016a) augments one single utterance by adding noise without considering its relation with other utterances. Besides theirs, there are also works which explicitly consider the paraphrasing relations between instances that share the same output. These works achieve improvements on tasks like text classification and question answering. Paraphrasing techniques including word-level substitution (Zhang et al., 2015; Wang and Yang, 2015), hand-crafted rules generation (Fader et al., 2013; Jia and Liang, 2016), and grammar-tree generation (Narayan et al., 2016) have been explored. Compared with these work, Kurata et al. (2016a) has the advantage of fully data-driven method and can easily switch to new domain without too much domain-specific knowledge, but doesn’t make use of the relations between instances within the training data. In this paper, we study the problem of data augmentation for LU and propose a novel data-driven framework that models relations between utterances of the same semantic frame in the training data. A sequence-to-sequence (seq2seq, Sutskever et al. 2014) model lies in the c"
C18-1105,P17-4012,0,0.0152664,"’t associate the semantic class of the slot with corresponding segment in the utterance. Our annotation focus on assigning the slot to its corresponding segment. During the annotation, each dialogue was processed by two annotators. Data statistics, Kappa value (Snow et al., 2008), and inner annotator agreement measured by F-score on the three domains are shown in Table 1. Evaluation. We evaluate our data augmentation’s effect on LU with F-score. conlleval is used in the same way with previous works (Mesnil et al., 2013; Mesnil et al., 2015; Chen et al., 2016a). Implementation. We use OpenNMT (Klein et al., 2017) as the implementation of our seq2seq model. We set the number of layers in LSTM as 2 and the size of hidden states as 500. Utterances that are longer than 50 are truncated. We adopt the same training setting as Luong et al. (2015) and use Adam (Kingma and Ba, 2014) to train the seq2seq model. Learning rate is halved when perplexity on the development set doesn’t decrease. During generation, we replace the model-yielded unknown token (unk) with the source word that has the highest attention score. For the slot tagging model, we set both the dimension for word embedding and the size of hidden s"
C18-1105,W18-5007,0,0.0132646,"features (Chen et al., 2016a) and representation in broader scope on sentence-level (Kurata et al., 2016c) and dialogue history-level (Chen et al., 2016b) have also been studied. Our augmentation method is orthogonal to these works and it’s hopeful to achieve more improvements with their works. Dialogue management is also a key component of task-oriented dialogue system, which mainly focuses on dialogue policy. However, optimal dialogue policy is hard to obtain from a static corpus due to the vast space of conversation process. A solution is to transform the static corpus into user simulator (Kreyssig et al., 2018), and most user simulators work on user semantics level. (Eckert et al., ; Schatzmann et al., 2007a; Asri et al., 2016; Scheffler and Young, 2000; Scheffler and Young, 2001; Pietquin and Dutoit, 2006; Georgila et al., 2005; Cuay´ahuitl et al., 2005). Recent work starts to generate user utterance directly to reduce data annotation(Kreyssig et al., 2018). In recent years, Generative Adversarial Network (GAN, Goodfellow et al. 2014) draws a lot of research attention. Its ability of generating adversarial examples is attractive for data augmentation. However, it hasn’t been tried in data augmentat"
C18-1105,D16-1223,0,0.144508,"cally related to the existing training data, and further harms the whole task-oriented dialogue system pipeline. Data augmentation, which enlarges the size of training data in machine learning systems, is an effective solution to the data insufficiency problem. Success has been achieved with data augmentation on a wide range of problems including computer vision (Krizhevsky et al., 2012), speech recognition (Hannun et al., 2014), text classification (Zhang et al., 2015), and question answering (Fader et al., 2013). However, its application in the task-oriented dialogue system is less studied. Kurata et al. (2016a) presented the only work we know that tried to augment data for LU. In their paper, an encoder-decoder is This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ * Email corresponding. Licence details: http:// 1234 Proceedings of the 27th International Conference on Computational Linguistics, pages 1234–1245 Santa Fe, New Mexico, USA, August 20-26, 2018. learned to reconstruct the utterances in the training data. During the augmenting process, the encoder’s output hidden states are randomly perturbed to yield different uttera"
C18-1105,D15-1166,0,0.0513772,"ective numbers of ranks during testing in Section 3. Data Augmentation as Seq2Seq Generation. When given the delexicalised input utterance d and the specified diverse rank k, we use the standard seq2seq model to generate the alternative delexicalised utterance d0 . In our seq2seq model, we append #k to the end of the input utterances and the model is formalized as Y p(d0 |d, k) = p(d0t |d1 , ..., dn , #k, d01 , ..., d0t−1 ) t where n is the number of words for the input utterance d. In this paper, we follow the seq2seq model for neural machine translation and use the input-feeding network in (Luong et al., 2015) with attention as our seq2seq model. During testing, we use beam search with beam size of 10 to yield more than one translation following Gimpel et al. (2013) and Vijayakumar et al. (2016). To train the seq2seq model, our basic assumption is that if d and d0 contain the same semantic frames, they can be generated from each other. Generally, we assume each pair of delexicalised utterances in the cluster Cs makes a pair of generation. However, it’s nontrivial to assign diverse ranks to training data. What’s more, to prevent the model from just producing produce lexical paraphrases (like “show m"
C18-1105,W16-6625,0,0.148768,"ed to yield different utterances. The work of Kurata et al. (2016a) augments one single utterance by adding noise without considering its relation with other utterances. Besides theirs, there are also works which explicitly consider the paraphrasing relations between instances that share the same output. These works achieve improvements on tasks like text classification and question answering. Paraphrasing techniques including word-level substitution (Zhang et al., 2015; Wang and Yang, 2015), hand-crafted rules generation (Fader et al., 2013; Jia and Liang, 2016), and grammar-tree generation (Narayan et al., 2016) have been explored. Compared with these work, Kurata et al. (2016a) has the advantage of fully data-driven method and can easily switch to new domain without too much domain-specific knowledge, but doesn’t make use of the relations between instances within the training data. In this paper, we study the problem of data augmentation for LU and propose a novel data-driven framework that models relations between utterances of the same semantic frame in the training data. A sequence-to-sequence (seq2seq, Sutskever et al. 2014) model lies in the core of our framework which takes a delexicalised utt"
C18-1105,D14-1162,0,0.0824305,"and Ba, 2014) to train the seq2seq model. Learning rate is halved when perplexity on the development set doesn’t decrease. During generation, we replace the model-yielded unknown token (unk) with the source word that has the highest attention score. For the slot tagging model, we set both the dimension for word embedding and the size of hidden state to 100. We also vary dropout rate in {0, 0.1, 0.2} considering its regularization power on small size of data. The batch size is set to 16 in all the experiments. Best hyperparameter settings are determined on the development set. GloVe embedding (Pennington et al., 2014) is used to initialize the word embedding in the model. Adam with the suggested settings in Kingma and Ba (2014) is used to train the parameters. Reimers and Gurevych (2017) pointed out that neural network training is nondeterministic and depends on the seed for the random number generator. We witness dramatic changes of the slot tagging performance using different random seeds. To control for this effect, we take their suggestions and report the average of 5 differently-seeded runs. 5.2 Results on ATIS Table 2 shows the slot tagging results on the ATIS dataset. Our baseline model is the vanil"
C18-1105,H90-1020,0,0.0685793,"kever et al. 2014) model lies in the core of our framework which takes a delexicalised utterance and generates its lexical and syntactical alternatives. To further encourage diverse generation, we incorporate a novel diversity rank into the utterance representation. When training the seq2seq model, the diversity rank is also used to filter the over-alike pairs of alternatives. These approaches lead to diversely augmented data that significantly improves the LU performance in the domains that labeled data is scarce. We conduct experiments on the Airline Travel Information System dataset (ATIS, Price 1990) along with a newly annotated layer of slot filling over the Stanford Multi-turn, Multi-domain Dialogue Dataset (Eric and Manning, 2017).1 On the small proportion of ATIS which contains 129 utterances, our method outperforms the baseline by a 6.38 F-score on slot filling. On the medium proportion, this improvement is 2.87. Similar trends are witnessed on our LU annotation over Stanford dialogue dataset which the average improvement on three new domains is 10.04 on 100 utterances and 0.47 on 500 utterances. The major contributions of this paper include: • We propose a data augmentation framewor"
C18-1105,D17-1035,0,0.0171839,"unknown token (unk) with the source word that has the highest attention score. For the slot tagging model, we set both the dimension for word embedding and the size of hidden state to 100. We also vary dropout rate in {0, 0.1, 0.2} considering its regularization power on small size of data. The batch size is set to 16 in all the experiments. Best hyperparameter settings are determined on the development set. GloVe embedding (Pennington et al., 2014) is used to initialize the word embedding in the model. Adam with the suggested settings in Kingma and Ba (2014) is used to train the parameters. Reimers and Gurevych (2017) pointed out that neural network training is nondeterministic and depends on the seed for the random number generator. We witness dramatic changes of the slot tagging performance using different random seeds. To control for this effect, we take their suggestions and report the average of 5 differently-seeded runs. 5.2 Results on ATIS Table 2 shows the slot tagging results on the ATIS dataset. Our baseline model is the vanilla BiLSTM slot tagger and our augmented slot tagger use the same architecture but is trained with the augmented data generated by our method. Compared with the vanilla tagge"
C18-1105,N07-2038,0,0.0354073,"ve the enlarged open-domain QA dataset. Narayan et al. (2016) constructed latent variable PCFG for questions and augment the training data by sampling from the grammar. All these works assume the same output (i.e. 1242 class in text classification, answer in question answering) for input paraphrases. Our method resembles theirs in the assumption for input paraphrases, but differs on using the seq2seq generation which is purely data-driven and doesn’t rely on special tailored domain knowledge. Besides these methods, works that introduce errors to language understanding have also been proposed (Schatzmann et al., 2007b; Sagae et al., 2012). Language understanding, as an important component in the task-oriented dialogue system pipeline, has drawn a lot of research attention in recent year, especially when enhanced by the rich representation power of the neural network, like recurrent neural network, LSTM (Yao et al., 2013; Yao et al., 2014; Mesnil et al., 2013; Mesnil et al., 2015) and memory network (Chen et al., 2016b). Rich linguistic features (Chen et al., 2016a) and representation in broader scope on sentence-level (Kurata et al., 2016c) and dialogue history-level (Chen et al., 2016b) have also been st"
C18-1105,D08-1027,0,0.0119547,"Missing"
C18-1105,D15-1306,0,0.316183,"the utterances in the training data. During the augmenting process, the encoder’s output hidden states are randomly perturbed to yield different utterances. The work of Kurata et al. (2016a) augments one single utterance by adding noise without considering its relation with other utterances. Besides theirs, there are also works which explicitly consider the paraphrasing relations between instances that share the same output. These works achieve improvements on tasks like text classification and question answering. Paraphrasing techniques including word-level substitution (Zhang et al., 2015; Wang and Yang, 2015), hand-crafted rules generation (Fader et al., 2013; Jia and Liang, 2016), and grammar-tree generation (Narayan et al., 2016) have been explored. Compared with these work, Kurata et al. (2016a) has the advantage of fully data-driven method and can easily switch to new domain without too much domain-specific knowledge, but doesn’t make use of the relations between instances within the training data. In this paper, we study the problem of data augmentation for LU and propose a novel data-driven framework that models relations between utterances of the same semantic frame in the training data. A"
C18-1320,D17-1040,0,0.114797,"uses key representations to retrieve the corresponding values. However, not all KBs are presented in key-value forms. Besides, an important component of classic pipeline, dialogue state tracker, is not properly modeled, making it difficult to precisely retrieve from KB. In this paper, we propose a novel framework that takes the advantages from both classic pipeline models and Seq2Seq models. We introduce dialogue states into Seq2Seq learning, but in a implicit way. Distributions in classic state tracking are modeled as a group of representation vectors computed by an attention-based network (Britz et al., 2017), which can be considered as a dialogue state representation that aggregates information for each slot. And training this representation doesn’t require annotation of dialogue state tracking. Our model queries the KB entries in an attention-based method as well, so that the querying is differentiable, without domain-specific pre-defined action spaces. Meanwhile we compute the representation for KB using entry-level attention and aggregate the representation with dialogue state representation to form a memory matrix of dialogue history and KB information. While decoding, we perform an attention"
C18-1320,P17-1045,0,0.0265912,"ch tried to directly retrieve entities from knowledge base. Yin et al. (2016b) has built a system to encode all table cells and assign a score vector to each row. Our framework resembles the second line of research, but can generate multiple entities to form natural language responses. He et al. (2017) has built two symmetric dialogue agents with private knowledge, and has applied knowledge graph reasoning into Seq2Seq learning, which is distantly related with our framework. In the sense of the KB forms, Yin et al. (2016a) retrieved entities based on (subject, relation, object) triples. While Dhingra et al. (2017) applied a soft-KB lookup on an entity-centric knowledge base to compute the probability of that the user knows the values of slots, and has tried to model the posterior distributions over all slots. However, our framework doesn’t require entity-centric knowledge base. 5 Conclusion In this paper, we proposed a framework that leverages dialogue state representation, which is tracked by an attention-based methods. Our framework performed an entry-level soft lookup over the knowledge base, and applied copying mechanism to retrieve entities from knowledge base while decoding. This framework was tr"
C18-1320,E17-2075,0,0.182949,"achieved success on machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015). This success spurs the interests to apply Seq2Seq models into dialogue systems. Seq2Seq models map dialogue history directly into the response in current turn, while requires a minimum amount of hand-crafting. However, conventional Seq2Seq doesn’t model the exterior data retrieval explicitly, which makes it hard for Seq2Seq to generate information stored in KB like meeting time and address, but this kind of retrieval is easy to achieve for classic pipeline. To tackle with the problem, Eric and Manning (2017) use an additional copy mechanism to retrieve entities that occurs in both KB and dialogue history. Eric et al. (2017) further introduced retrieval from key-value KB where the model uses key representations to retrieve the corresponding values. However, not all KBs are presented in key-value forms. Besides, an important component of classic pipeline, dialogue state tracker, is not properly modeled, making it difficult to precisely retrieve from KB. In this paper, we propose a novel framework that takes the advantages from both classic pipeline models and Seq2Seq models. We introduce dialogue s"
C18-1320,W17-5506,0,0.353214,"spurs the interests to apply Seq2Seq models into dialogue systems. Seq2Seq models map dialogue history directly into the response in current turn, while requires a minimum amount of hand-crafting. However, conventional Seq2Seq doesn’t model the exterior data retrieval explicitly, which makes it hard for Seq2Seq to generate information stored in KB like meeting time and address, but this kind of retrieval is easy to achieve for classic pipeline. To tackle with the problem, Eric and Manning (2017) use an additional copy mechanism to retrieve entities that occurs in both KB and dialogue history. Eric et al. (2017) further introduced retrieval from key-value KB where the model uses key representations to retrieve the corresponding values. However, not all KBs are presented in key-value forms. Besides, an important component of classic pipeline, dialogue state tracker, is not properly modeled, making it difficult to precisely retrieve from KB. In this paper, we propose a novel framework that takes the advantages from both classic pipeline models and Seq2Seq models. We introduce dialogue states into Seq2Seq learning, but in a implicit way. Distributions in classic state tracking are modeled as a group of"
C18-1320,P16-1154,0,0.290294,"s information for each slot. And training this representation doesn’t require annotation of dialogue state tracking. Our model queries the KB entries in an attention-based method as well, so that the querying is differentiable, without domain-specific pre-defined action spaces. Meanwhile we compute the representation for KB using entry-level attention and aggregate the representation with dialogue state representation to form a memory matrix of dialogue history and KB information. While decoding, we perform an attention over memory and an attention over input, incorporating copying mechanism (Gu et al., 2016) that allows model to copy words from KBs to enhance the capability of retrieving accurate entities. We evaluate the proposed framework on Stanford Multi-turn, Multi-domain Dialogue Dataset (Eric et al., 2017), to test the effectiveness of our framework and flexibility to apply to different domains. We compare our model with other Seq2Seq models and discovered that our model has outperformed other 3782 address distance poi_type poi traffic_info moderate traffic 899 Ames Ct 5 miles hospital Stanford Childrens Health … … … … … 409 Bollard St 5 miles grocery store willows market no traffic Entry"
C18-1320,P17-1162,0,0.0157569,"n and explicit semantic frames. Our soft KB attention can be considered as a process to retrieve entries, which has been explored in many QA and dialogue work. One line of this research includes creating well-defined API calls to query the KB (Williams et al., 2017; Wen et al., 2017a). And another line of research tried to directly retrieve entities from knowledge base. Yin et al. (2016b) has built a system to encode all table cells and assign a score vector to each row. Our framework resembles the second line of research, but can generate multiple entities to form natural language responses. He et al. (2017) has built two symmetric dialogue agents with private knowledge, and has applied knowledge graph reasoning into Seq2Seq learning, which is distantly related with our framework. In the sense of the KB forms, Yin et al. (2016a) retrieved entities based on (subject, relation, object) triples. While Dhingra et al. (2017) applied a soft-KB lookup on an entity-centric knowledge base to compute the probability of that the user knows the values of slots, and has tried to model the posterior distributions over all slots. However, our framework doesn’t require entity-centric knowledge base. 5 Conclusion"
C18-1320,I17-1074,0,0.0308298,"-to-end training. In contrast to their work, our framework trained the state tracker jointly with the end-to-end dialogue training. Liu and 3789 Lane (2017) built a turn-level LSTM to model the dialogue state and generate probability distribution for each slot. Bordes and Weston (2017) built a system by applying memory network to store the previous dialogue history. But the responses are retrieved from templates, which is significantly different from our neural generative responses. Another type of work tried to build an end-to-end system as a task completion dialogue system (Li et al., 2016; Li et al., 2017; Peng et al., 2017). These modeled are trained through an end-to-end fashion via reinforcement learning algorithm using the reward from user simulators. However, their dialogue responses are not generated from the dialogue history directly but from a set of pre-defined action and explicit semantic frames. Our soft KB attention can be considered as a process to retrieve entries, which has been explored in many QA and dialogue work. One line of this research includes creating well-defined API calls to query the KB (Williams et al., 2017; Wen et al., 2017a). And another line of research tried to"
C18-1320,D15-1166,0,0.111171,"y involves querying the knowledge base. The action is then converted to its natural language expression using natural language generation. Both natural language understanding and dialogue state tracking require a large amount of domain specific annotation for training, which is expensive to obtain. Besides, the design of actions and the explicit forms of semantic frames require a lot of knowledge from human experts, which are domain-specific as well. Neural generative models, typically Seq2Seq models, have achieved success on machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015). This success spurs the interests to apply Seq2Seq models into dialogue systems. Seq2Seq models map dialogue history directly into the response in current turn, while requires a minimum amount of hand-crafting. However, conventional Seq2Seq doesn’t model the exterior data retrieval explicitly, which makes it hard for Seq2Seq to generate information stored in KB like meeting time and address, but this kind of retrieval is easy to achieve for classic pipeline. To tackle with the problem, Eric and Manning (2017) use an additional copy mechanism to retrieve entities that occurs in both KB and dia"
C18-1320,D17-1237,0,0.0154634,"In contrast to their work, our framework trained the state tracker jointly with the end-to-end dialogue training. Liu and 3789 Lane (2017) built a turn-level LSTM to model the dialogue state and generate probability distribution for each slot. Bordes and Weston (2017) built a system by applying memory network to store the previous dialogue history. But the responses are retrieved from templates, which is significantly different from our neural generative responses. Another type of work tried to build an end-to-end system as a task completion dialogue system (Li et al., 2016; Li et al., 2017; Peng et al., 2017). These modeled are trained through an end-to-end fashion via reinforcement learning algorithm using the reward from user simulators. However, their dialogue responses are not generated from the dialogue history directly but from a set of pre-defined action and explicit semantic frames. Our soft KB attention can be considered as a process to retrieve entries, which has been explored in many QA and dialogue work. One line of this research includes creating well-defined API calls to query the KB (Williams et al., 2017; Wen et al., 2017a). And another line of research tried to directly retrieve e"
C18-1320,W15-4639,0,0.0683434,"Missing"
C18-1320,E17-1042,0,0.0949969,"Missing"
C18-1320,W13-4065,0,0.196477,"a task-oriented dialogue, where an agent provides with location information for a user. The requirements for the agents to accomplish users’ demands usually involve querying the knowledge base (KB), like acquiring address from location information KB in Figure 1. Typical machine learning approaches model the problem as a partially observable Markov Decision Process (POMDP) (Williams and Young, 2007; Young et al., 2013), where a pipeline system is introduced. The pipeline system consists of four components: natural language understanding (NLU, Tur and De Mori, 2011) , dialogue state tracking (Williams et al., 2013; Williams, 2012), dialogue policy learning (Young et al., 2010) and natural language generation (Wen et al., 2015). Taking the utterance in Figure 1 for example, NLU maps the utterance “Address to the gas station” into semantic slot “POI type”. Dialogue state tracker keeps the probability of “gas station” close to 1 against other values of slot “POI type”. Given a semantic frame as a dialogue state, which is the combination of distributions of these slots, dialogue policy learning generates the next pre-defined system action, This work is licenced under a Creative Commons Attribution 4.0 Inte"
C18-1320,P17-1062,0,0.0361382,"nd system as a task completion dialogue system (Li et al., 2016; Li et al., 2017; Peng et al., 2017). These modeled are trained through an end-to-end fashion via reinforcement learning algorithm using the reward from user simulators. However, their dialogue responses are not generated from the dialogue history directly but from a set of pre-defined action and explicit semantic frames. Our soft KB attention can be considered as a process to retrieve entries, which has been explored in many QA and dialogue work. One line of this research includes creating well-defined API calls to query the KB (Williams et al., 2017; Wen et al., 2017a). And another line of research tried to directly retrieve entities from knowledge base. Yin et al. (2016b) has built a system to encode all table cells and assign a score vector to each row. Our framework resembles the second line of research, but can generate multiple entities to form natural language responses. He et al. (2017) has built two symmetric dialogue agents with private knowledge, and has applied knowledge graph reasoning into Seq2Seq learning, which is distantly related with our framework. In the sense of the KB forms, Yin et al. (2016a) retrieved entities base"
C18-1320,W12-1812,0,0.027281,"ue, where an agent provides with location information for a user. The requirements for the agents to accomplish users’ demands usually involve querying the knowledge base (KB), like acquiring address from location information KB in Figure 1. Typical machine learning approaches model the problem as a partially observable Markov Decision Process (POMDP) (Williams and Young, 2007; Young et al., 2013), where a pipeline system is introduced. The pipeline system consists of four components: natural language understanding (NLU, Tur and De Mori, 2011) , dialogue state tracking (Williams et al., 2013; Williams, 2012), dialogue policy learning (Young et al., 2010) and natural language generation (Wen et al., 2015). Taking the utterance in Figure 1 for example, NLU maps the utterance “Address to the gas station” into semantic slot “POI type”. Dialogue state tracker keeps the probability of “gas station” close to 1 against other values of slot “POI type”. Given a semantic frame as a dialogue state, which is the combination of distributions of these slots, dialogue policy learning generates the next pre-defined system action, This work is licenced under a Creative Commons Attribution 4.0 International Licence"
C18-1320,W16-0106,0,0.0308129,"hrough an end-to-end fashion via reinforcement learning algorithm using the reward from user simulators. However, their dialogue responses are not generated from the dialogue history directly but from a set of pre-defined action and explicit semantic frames. Our soft KB attention can be considered as a process to retrieve entries, which has been explored in many QA and dialogue work. One line of this research includes creating well-defined API calls to query the KB (Williams et al., 2017; Wen et al., 2017a). And another line of research tried to directly retrieve entities from knowledge base. Yin et al. (2016b) has built a system to encode all table cells and assign a score vector to each row. Our framework resembles the second line of research, but can generate multiple entities to form natural language responses. He et al. (2017) has built two symmetric dialogue agents with private knowledge, and has applied knowledge graph reasoning into Seq2Seq learning, which is distantly related with our framework. In the sense of the KB forms, Yin et al. (2016a) retrieved entities based on (subject, relation, object) triples. While Dhingra et al. (2017) applied a soft-KB lookup on an entity-centric knowledg"
C18-1320,W16-0105,0,0.0316997,"hrough an end-to-end fashion via reinforcement learning algorithm using the reward from user simulators. However, their dialogue responses are not generated from the dialogue history directly but from a set of pre-defined action and explicit semantic frames. Our soft KB attention can be considered as a process to retrieve entries, which has been explored in many QA and dialogue work. One line of this research includes creating well-defined API calls to query the KB (Williams et al., 2017; Wen et al., 2017a). And another line of research tried to directly retrieve entities from knowledge base. Yin et al. (2016b) has built a system to encode all table cells and assign a score vector to each row. Our framework resembles the second line of research, but can generate multiple entities to form natural language responses. He et al. (2017) has built two symmetric dialogue agents with private knowledge, and has applied knowledge graph reasoning into Seq2Seq learning, which is distantly related with our framework. In the sense of the KB forms, Yin et al. (2016a) retrieved entities based on (subject, relation, object) triples. While Dhingra et al. (2017) applied a soft-KB lookup on an entity-centric knowledg"
D11-1109,D07-1101,0,0.442565,"d) = {(h,m)}⊆d m h dependency h s m sibling g + m h grandparent {(h,s)(h,m)}⊆d + g s h grand-sibling m h s t tri-sibling ∑ ∑ {(g,h),(h,m)}⊆d ∑ m + Figure 1: Different types of scoring parts used in current graph-based models (Koo and Collins, 2010). Eisner (1996) proposes an O(n3 ) decoding algorithm for dependency parsing. Based on the algorithm, McDonald et al. (2005) propose the firstorder model, in which the scoring parts only contains dependencies. The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3 ) parsing time. The secondorder model of Carreras (2007) incorporates both sibling and grandparent parts, and needs O(n4 ) parsing time. However, the grandparent parts are restricted to those composed of outermost grandchildren. Koo and Collins (2010) propose efficient decoding algorithms of O(n4 ) for third-order models. In their paper, they implement two versions of third-order models, Model 1 and Model 2 according to their naming. Model 1 incorporates only grand-sibling parts, while Model 2 incorporates both grand-sibling and tri-sibling parts. Their experiments on English and Czech show that Model 1 and Model 2 obtain nearly the same parsing ac"
D11-1109,P05-1022,0,0.355797,"Missing"
D11-1109,C10-1019,1,0.878284,"Missing"
D11-1109,D07-1022,0,0.0164053,"Missing"
D11-1109,W02-1001,0,0.285248,"et al. (2010) describes an efficient and simple inference algorithm based on dual decomposition and linear programming relaxation to combine a lexicalized constituent parser and a trigram POS tagger. 1181 ˆt = arg max Scorepos (x, t) t ˆ is determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). We use perceptron to build our POS tagging baseline for two reasons. Firstly, as a linear model, perceptron is simple, fast, and effective. It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al., 2007). Secondly, perceptron has been successfully applied to dependency parsing as well (Koo and Collins, 2010). In this paper, perceptron is used in all models including the POS tagging model, the dependency parsing models and the joint models. In a perceptron, the score of a tag sequence is Scorepos (x, t) = wpos · fpos (x, t) 2.2 Dependency Parsing Recently"
D11-1109,N09-1046,0,0.0525872,"Missing"
D11-1109,C96-1058,0,0.797918,"me with Model 1 in Koo and Collins (2010), but without using grand-sibling features.2 • The third-order model (O3): the same with Model 1 in Koo and Collins (2010). We adopt linear models to define the score of a dependency tree. For the third-order model, the score of a dependency tree is represented as: ∑ wdep · fdep (x, t, h, m) Scoresyn (x, t, d) = {(h,m)}⊆d m h dependency h s m sibling g + m h grandparent {(h,s)(h,m)}⊆d + g s h grand-sibling m h s t tri-sibling ∑ ∑ {(g,h),(h,m)}⊆d ∑ m + Figure 1: Different types of scoring parts used in current graph-based models (Koo and Collins, 2010). Eisner (1996) proposes an O(n3 ) decoding algorithm for dependency parsing. Based on the algorithm, McDonald et al. (2005) propose the firstorder model, in which the scoring parts only contains dependencies. The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3 ) parsing time. The secondorder model of Carreras (2007) incorporates both sibling and grandparent parts, and needs O(n4 ) parsing time. However, the grandparent parts are restricted to those composed of outermost grandchildren. Koo and Collins (2010) propose efficient decoding algorithms of O(n4 ) for"
D11-1109,N09-1037,0,0.0607158,"Missing"
D11-1109,P08-1043,0,0.0977694,"Missing"
D11-1109,P10-1110,0,0.400604,"tructures. On the contrary, joint models of version 2 can incorporate both aforementioned feature sets, but have higher complexity. These two versions of models will be thoroughly compared in the experiments. 1185 We then define the allowable candidate POS tags of the word wi to be Ti (x) = {t : t ∈ T , P (ti = t|x) ≥ λt × pmaxi (x)} where λt is the pruning threshold. Ti (x) is used to constrain the POS search space by replacing T in Algorithm 1. 5 Experiments We use the Penn Chinese Treebank 5.1 (CTB5) (Xue et al., 2005). Following the setup of Duan et al. (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 11371147) sets. We use the head-finding rules of Zhang and Clark (2008b) to turn the bracketed sentences into dependency structures. We use the standard tagging accuracy to evaluate POS tagging. For dependency parsing, we use word accuracy (also known as dependency accuracy), root accuracy and complete match rate (all excluding punctuation) . For the averaged training, we train each model for 15 iterations and select the parameters that perform best on the development"
D11-1109,P08-1102,0,0.111307,"Missing"
D11-1109,P10-1001,0,0.270992,"2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). We use perceptron to build our POS tagging baseline for two reasons. Firstly, as a linear model, perceptron is simple, fast, and effective. It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al., 2007). Secondly, perceptron has been successfully applied to dependency parsing as well (Koo and Collins, 2010). In this paper, perceptron is used in all models including the POS tagging model, the dependency parsing models and the joint models. In a perceptron, the score of a tag sequence is Scorepos (x, t) = wpos · fpos (x, t) 2.2 Dependency Parsing Recently, graph-based dependency parsing has gained more and more interest due to its state-ofthe-art accuracy. Graph-based dependency parsing views the problem as finding the highest scoring tree from a directed graph. Based on dynamic programming decoding, it can efficiently find an optimal tree in a huge search space. In a graph-based model, the score"
D11-1109,P09-1058,0,0.227854,"Missing"
D11-1109,P03-1056,0,0.0427835,"where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. A dependency tree is denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 &lt; m ≤ n}, where (h, m) represents a dependency wh → wm whose head word (or father) is wh and modifier (or child) is wm . w0 is an artificial root token which is used to simplify the formalization of the problem. The pipelined method treats POS tagging and dependency parsing as two cascaded problems. First, 1 It should be noted that it is straightforward to simultaneously do POS tagging and constituent parsing, as POS tags can be regarded as non-terminals in the constituent structure (Levy and Manning, 2003). In addition, Rush et al. (2010) describes an efficient and simple inference algorithm based on dual decomposition and linear programming relaxation to combine a lexicalized constituent parser and a trigram POS tagger. 1181 ˆt = arg max Scorepos (x, t) t ˆ is determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 20"
D11-1109,P10-1113,0,0.0383995,"Missing"
D11-1109,C10-1080,0,0.0394632,"Missing"
D11-1109,E06-1011,0,0.689881,"the third-order model, the score of a dependency tree is represented as: ∑ wdep · fdep (x, t, h, m) Scoresyn (x, t, d) = {(h,m)}⊆d m h dependency h s m sibling g + m h grandparent {(h,s)(h,m)}⊆d + g s h grand-sibling m h s t tri-sibling ∑ ∑ {(g,h),(h,m)}⊆d ∑ m + Figure 1: Different types of scoring parts used in current graph-based models (Koo and Collins, 2010). Eisner (1996) proposes an O(n3 ) decoding algorithm for dependency parsing. Based on the algorithm, McDonald et al. (2005) propose the firstorder model, in which the scoring parts only contains dependencies. The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3 ) parsing time. The secondorder model of Carreras (2007) incorporates both sibling and grandparent parts, and needs O(n4 ) parsing time. However, the grandparent parts are restricted to those composed of outermost grandchildren. Koo and Collins (2010) propose efficient decoding algorithms of O(n4 ) for third-order models. In their paper, they implement two versions of third-order models, Model 1 and Model 2 according to their naming. Model 1 incorporates only grand-sibling parts, while Model 2 incorporates both grand-sibling and tri-sibling parts"
D11-1109,P05-1012,0,0.930686,"der model (O3): the same with Model 1 in Koo and Collins (2010). We adopt linear models to define the score of a dependency tree. For the third-order model, the score of a dependency tree is represented as: ∑ wdep · fdep (x, t, h, m) Scoresyn (x, t, d) = {(h,m)}⊆d m h dependency h s m sibling g + m h grandparent {(h,s)(h,m)}⊆d + g s h grand-sibling m h s t tri-sibling ∑ ∑ {(g,h),(h,m)}⊆d ∑ m + Figure 1: Different types of scoring parts used in current graph-based models (Koo and Collins, 2010). Eisner (1996) proposes an O(n3 ) decoding algorithm for dependency parsing. Based on the algorithm, McDonald et al. (2005) propose the firstorder model, in which the scoring parts only contains dependencies. The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3 ) parsing time. The secondorder model of Carreras (2007) incorporates both sibling and grandparent parts, and needs O(n4 ) parsing time. However, the grandparent parts are restricted to those composed of outermost grandchildren. Koo and Collins (2010) propose efficient decoding algorithms of O(n4 ) for third-order models. In their paper, they implement two versions of third-order models, Model 1 and Model 2 ac"
D11-1109,J08-4003,0,0.155196,"Missing"
D11-1109,N07-1051,0,0.116748,"Missing"
D11-1109,W96-0213,0,0.524192,"d as non-terminals in the constituent structure (Levy and Manning, 2003). In addition, Rush et al. (2010) describes an efficient and simple inference algorithm based on dual decomposition and linear programming relaxation to combine a lexicalized constituent parser and a trigram POS tagger. 1181 ˆt = arg max Scorepos (x, t) t ˆ is determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). We use perceptron to build our POS tagging baseline for two reasons. Firstly, as a linear model, perceptron is simple, fast, and effective. It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al., 2007). Secondly, perceptron has been successfully applied to dependency parsing as well (Koo and Collins, 2010). In this paper, perceptron is used in all models including the POS tagging model, the dependency parsing models and the joint models. In a perceptron, the score o"
D11-1109,D10-1001,0,0.0739428,"POS tag set. A dependency tree is denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 &lt; m ≤ n}, where (h, m) represents a dependency wh → wm whose head word (or father) is wh and modifier (or child) is wm . w0 is an artificial root token which is used to simplify the formalization of the problem. The pipelined method treats POS tagging and dependency parsing as two cascaded problems. First, 1 It should be noted that it is straightforward to simultaneously do POS tagging and constituent parsing, as POS tags can be regarded as non-terminals in the constituent structure (Levy and Manning, 2003). In addition, Rush et al. (2010) describes an efficient and simple inference algorithm based on dual decomposition and linear programming relaxation to combine a lexicalized constituent parser and a trigram POS tagger. 1181 ˆt = arg max Scorepos (x, t) t ˆ is determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002"
D11-1109,P07-1096,0,0.0207597,"s determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). We use perceptron to build our POS tagging baseline for two reasons. Firstly, as a linear model, perceptron is simple, fast, and effective. It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al., 2007). Secondly, perceptron has been successfully applied to dependency parsing as well (Koo and Collins, 2010). In this paper, perceptron is used in all models including the POS tagging model, the dependency parsing models and the joint models. In a perceptron, the score of a tag sequence is Scorepos (x, t) = wpos · fpos (x, t) 2.2 Dependency Parsing Recently, graph-based dependency parsing has gained more and more interest due to its state-ofthe-art accuracy. Graph-based dependency parsing views the problem as finding the highest scoring tree from a directed graph. Based on dynamic programming de"
D11-1109,W08-2121,0,0.0983419,"Missing"
D11-1109,P09-1055,0,0.0150803,"Missing"
D11-1109,C10-1135,0,0.0414692,"Missing"
D11-1109,P08-1101,0,0.518379,"of this paper is organized as follows. Section 2 describes the pipelined method, including the POS tagging and parsing models. Section 3 discusses the joint models and the decoding algorithms, while Section 4 presents the pruning techniques. Section 5 reports the experimental results and error analysis. We review previous work closely related to our method in Section 6, and conclude this paper in Section 7. an optimal POS tag sequence ˆt is determined. 2 where fpos (x, t) refers to the feature vector and wpos is the corresponding weight vector. For POS tagging features, we follow the work of Zhang and Clark (2008a). Three feature sets are considered: POS unigram, bigram and trigram features. For brevity, we will refer to the three sets as wi ti , ti−1 ti and ti−2 ti−1 ti . Given wpos , we adopt the Viterbi algorithm to get the optimal tagging sequence. The Baseline Pipelined Method Given an input sentence x = w1 ...wn , we denote its POS tag sequence by t = t1 ...tn , where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. A dependency tree is denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 &lt; m ≤ n}, where (h, m) represents a dependency wh → wm whose head word (or father) is wh and modifier (or child) is wm . w0 is"
D11-1109,D08-1059,0,0.828377,"of this paper is organized as follows. Section 2 describes the pipelined method, including the POS tagging and parsing models. Section 3 discusses the joint models and the decoding algorithms, while Section 4 presents the pruning techniques. Section 5 reports the experimental results and error analysis. We review previous work closely related to our method in Section 6, and conclude this paper in Section 7. an optimal POS tag sequence ˆt is determined. 2 where fpos (x, t) refers to the feature vector and wpos is the corresponding weight vector. For POS tagging features, we follow the work of Zhang and Clark (2008a). Three feature sets are considered: POS unigram, bigram and trigram features. For brevity, we will refer to the three sets as wi ti , ti−1 ti and ti−2 ti−1 ti . Given wpos , we adopt the Viterbi algorithm to get the optimal tagging sequence. The Baseline Pipelined Method Given an input sentence x = w1 ...wn , we denote its POS tag sequence by t = t1 ...tn , where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. A dependency tree is denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 &lt; m ≤ n}, where (h, m) represents a dependency wh → wm whose head word (or father) is wh and modifier (or child) is wm . w0 is"
D11-1109,W09-1201,0,\N,Missing
D14-1012,P05-1001,0,0.0142042,"Missing"
D14-1012,J92-4003,0,0.478376,"tomatically extracted prototypes for each target label. More foundationally, the reason for the above factors lies in the high-dimensional and sparse lexical feature representation, which completely ignores the similarity between features, especially word features. To overcome this weakness, an effective way is to learn more generalized representations of words by exploiting the numerous unlabeled data, in a semi-supervised manner. After which, the generalized word representations can be used as extra features to facilitate the supervised systems. Liang (2005) learned Brown clusters of words (Brown et al., 1992) from unlabeled data and use them as features to promote the supervised NER and Chinese word segmentation. Brown clusters of words can be seen as a generalized word representation distributed in a discrete and low-dimensional vocabulary space. Contextually similar words are grouped in the same cluster. The Brown clustering of words was also adopted in dependency parsing (Koo et al., 2008) and POS tagging for online conversational text (Owoputi et al., 2013), demonstrating significant improvements. We carefully compare and analyze these approaches in the task of NER. Experimental results are pr"
D14-1012,P05-1045,0,0.0106467,"Missing"
D14-1012,C14-1048,1,0.150649,"Missing"
D14-1012,N04-1043,0,0.0453105,"rd is an NE. To verify this, we extract new prototypes considering only two labels, namely, NE and non-NE, using the same metric in Section 3.4. As shown in the last row of Table 5, higher performance is achieved. Table 5: Performance of the NE/non-NE classification on the CoNLL-2003 development dataset using different embedding features. 6 Related Studies Semi-supervised learning with generalized word representations is a simple and general way of improving supervised NLP systems. One common approach for inducing generalized word representations is to use clustering (e.g., Brown clustering) (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Huang and Yates, 2009). Aside from word clustering, word embeddings have been widely studied. Bengio et al. (2003) propose a feed-forward neural network based language model (NNLM), which uses an embedding layer to map each word to a dense continuousvalued and low-dimensional vector (parameters), and then use these vectors as the input to predict the probability distribution of the next word. The NNLM can be seen as a joint learning framework for language modeling and word representations. Alternative models for learning word embeddings are mostly inspired by t"
D14-1012,N06-1041,0,0.0307987,"Similarities between words and clusters are measured by Euclidean distance. Moreover, different number of clusters n contain information of different granularities. Therefore, we combine the cluster features of different ns to better utilize the embeddings. 3.4 NE Type B-PER I-PER B-ORG I-ORG B-LOC I-LOC B-MISC I-MISC O Table 1: Prototypes extracted from the CoNLL2003 NER training data using NPMI. Distributional Prototype Features We propose a novel kind of embedding features, named distributional prototype features for supervised models. This is mainly inspired by prototype-driven learning (Haghighi and Klein, 2006) which was originally introduced as a primarily unsupervised approach for sequence modeling. In prototype-driven learning, a few prototypical examples are specified for each target label, which can be treated as an injection of prior knowledge. This sparse prototype information is then propagated across an unlabeled corpus through distributional similarities. The basic motivation of the distributional prototype features is that similar words are supposed to be tagged with the same label. This hypothesis makes great sense in tasks such as NER and POS tagging. For example, suppose Michael is a p"
D14-1012,P09-1056,0,0.00803374,"es considering only two labels, namely, NE and non-NE, using the same metric in Section 3.4. As shown in the last row of Table 5, higher performance is achieved. Table 5: Performance of the NE/non-NE classification on the CoNLL-2003 development dataset using different embedding features. 6 Related Studies Semi-supervised learning with generalized word representations is a simple and general way of improving supervised NLP systems. One common approach for inducing generalized word representations is to use clustering (e.g., Brown clustering) (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Huang and Yates, 2009). Aside from word clustering, word embeddings have been widely studied. Bengio et al. (2003) propose a feed-forward neural network based language model (NNLM), which uses an embedding layer to map each word to a dense continuousvalued and low-dimensional vector (parameters), and then use these vectors as the input to predict the probability distribution of the next word. The NNLM can be seen as a joint learning framework for language modeling and word representations. Alternative models for learning word embeddings are mostly inspired by the feed-forward NNLM, including the Hierarchical Log-Bi"
D14-1012,N13-1039,0,0.0161671,"Missing"
D14-1012,W09-1119,0,0.0510686,"gorithm is a binary tree, where each word is uniquely identified by its path from the root. Thus each word can be represented as a bit-string with a specific length. Following the setting of Owoputi et al. (2013), we will use the prefix features of hierarchical clusters to take advantage of the word similarity in different granularities. Concretely, the Brown cluster feature template is: 5.2 Table 3 shows the performances of NER on the test dataset. Our baseline is slightly lower than that of Turian et al. (2010), because they use the BILOU encoding of NE types which outperforms BIO encoding (Ratinov and Roth, 2009).8 Nonetheless, our conclusions hold. As we can see, all of the three approaches we investigate in this study achieve better performance than the direct use of the dense continuous embedding features. To our surprise, even the binarized embedding features (BinarizedEmb) outperform the continuous version (DenseEmb). This provides clear evidence that directly using the dense continuous embeddings as features in CRF indeed cannot fully • bci+k , −2 ≤ k ≤ 2. • prefix (bci+k , p), p ∈ {2,4,6,...,16}, −2 ≤ k ≤ 2. prefix takes the p-length prefix of the Brown cluster coding bci+k . 5 Experiments 5.1"
D14-1012,P12-1092,0,0.0416081,"points higher than the performance of the dense embedding features 9 Statistical significant with p-value &lt; 0.001 by two-tailed t-test. 116 Setting Baseline +DenseEmb +BinarizedEmb +ClusterEmb +DistPrototype Time (ms) / sent 1.04 4.75 1.25 1.16 2.31 very frequent words, while lower sparsity for midfrequent words. It indicates that for words that are very rare or very frequent, BinarizedEmb just omit most of the features. This is reasonable also for the very frequent words, since they usually have rich and diverse context distributions and their embeddings cannot be well learned by our models (Huang et al., 2012). 0.70 Table 4: Running time of different features on a Intel(R) Xeon(R) E5620 2.40GHz machine. to accelerate the DistPrototype, by increasing the threshold of DistSim(z, w). However, this is indeed an issue of trade-off between efficiency and accuracy. ● ● 0.60 Sparsity 0.65 ● ● ● ● Analysis 0.55 5.3 In this section, we conduct analyses to show the reasons for the improvements. ● ● ● 0.50 256 5.3.1 ● ● Rare words 1k 4k 16k 64k Frequency of word in unlabeled data As discussed by Turian et al. (2010), much of the NER F1 is derived from decisions regarding rare words. Therefore, in order to show"
D14-1012,W03-0419,0,0.0367445,"Missing"
D14-1012,P08-1068,0,0.70957,"tributional prototype approach performs the best. Moreover, the combination of the approaches provides additive improvements, outperforming the dense and continuous embedding features by nearly 2 points of F1 score. 1 Introduction Learning generalized representation of words is an effective way of handling data sparsity caused by high-dimensional lexical features in NLP systems, such as named entity recognition (NER) and dependency parsing. As a typical lowdimensional and generalized word representation, Brown clustering of words has been studied for a long time. For example, Liang (2005) and Koo et al. (2008) used the Brown cluster features for semi-supervised learning of various NLP tasks and achieved significant improvements. ∗ • Are the continuous embedding features fit for the generalized linear models that are most widely adopted in NLP? • How can the generalized linear models better utilize the embedding features? According to the results provided by Turian et 1 Generalized linear models refer to the models that describe the data as a combination of linear basis functions, either directly in the input variables space or through some transformation of the probability distributions (e.g., logl"
D14-1012,P10-1040,0,0.526341,"on is on, word embedding preserves rich linguistic regularities of words with each dimension hopefully representing a latent feature. Similar words are expected to be distributed close to one another in the embedding space. Consequently, word embeddings can be beneficial for a variety of NLP applications in different ways, among which the most simple and general way is to be fed as features to enhance existing supervised NLP systems. Previous work has demonstrated effectiveness of the continuous word embedding features in several tasks such as chunking and NER using generalized linear models (Turian et al., 2010).1 However, there still remain two fundamental problems that should be addressed: Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems, which is regarded as a simple semi-supervised learning mechanism. However, fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain. In this study, we investigate and analyze three different approaches, including a new proposed distributional prototype approach, for utilizing the embedding features. The presen"
D14-1012,P06-1141,0,0.0341421,"Missing"
D14-1012,I13-1183,0,0.226518,"Missing"
D14-1012,W13-5708,1,0.852635,"Missing"
D14-1012,N13-1063,0,0.380291,"ith the one-hot vector of w). The probability of its context word c is then computed using a log-linear function: exp(vc&gt; vw ) &gt; c0 ∈V exp(vc0 vw ) P (c|w; θ) = P Binarization of Embeddings where mean(v) is the mean value of vector v, U+ is a string feature which turns on when the value (Cij ) falls into the upper part of the positive list. Similarly, B− refers to the bottom part of the negative list. The insight behind φ is that we only consider the features with strong opinions (i.e., positive or negative) on each dimension and omit the values close to zero. 3.3 (1) Clustering of Embeddings Yu et al. (2013) introduced clustering embeddings to overcome the disadvantage that word embeddings are not suitable for linear models. They suggested that the high-dimensional cluster features make samples from different classes better separated by linear models. where V is the vocabulary. The parameters θ are vwi , vci for w, c ∈ V and i = 1, ..., d. Then, the 2 The term similar should be viewed depending on the specific task. 112 In this study, we again investigate this approach. Concretely, each word is treated as a single sample. The batch k-means clustering algorithm (Sculley, 2010) is used,3 and each c"
D14-1093,J09-4006,0,0.05641,"rces by joining the partially annotated sentences derived using each resource, training our CRF model with these partially annotated sentences and the fully annotated PD sentences. The tests are performed on medicine and computer domains. Table 7 shows the results, where further improvements are made on both domains when the two types of resources are combined. 6 Related Work There has been a line of research on making use of unlabeled data for word segmentation. Zhao and Kit (2008) improve segmentation performance by mutual information between characters, collected from large unlabeled data; Li and Sun (2009) use punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words; Sun and Xu (2011) explore several statistical features derived from unlabeled data to help improve character-based word segmentation. These investigations mainly focus on in-domain accuracies. Liu and Zhang (2012) 871 0.78 0.9525 0.9522 0.8775 0.9225 0.8750 0.9200 0.76 0.9175 0.75 0.8725 0.77 0.9519 0.74 0.9150 0.8700 0.73 0.9516 0 50 100 150 0 (a) Finance 50 100 150 (b) Medicine 0.87 0.750 0.938 0.9222 0.745 0.936 0.9219 0.740 0.934 0.9225 0.86 0.9216 0.85 0.84 0.735"
D14-1093,C12-2073,1,0.579408,"om different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled domain-specific lexicons to improve target-domain segmentation accuracies. Both 864 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864–874, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computation"
D14-1093,P08-1099,0,0.00794877,"e structure perceptron (Collins, 2002), although they study two different sources of information. In contrast to their work, we unify both types of information under the CRF framework. CRF has been used for Chinese word segmentation (Tseng, 2005; Shi and Wang, 2007; Zhao and Kit, 2008; Wang et al., 2011). However, most previous work train a CRF by using full annotation only. In contrast, we study CRF based segmentation by using both full and partial annotation. Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introduce knowledge by incorporating a manually annotated feature distribution into the regularizer, and the JESS-CM (Suzuki and Isozaki, 2008), which use a EM-like method to iteratively optimize the parameter on both the annotated data and unlabeled data. In contrast, we directly incorporate the likelihood of partial annotation into the objective function. The work that is the most similar to ours is Tsuboi et al. (2008), who modify the CRF learning objective for partial data. They focus on Japanese lexical analysis using manually collected partial data, while we investigate the effec"
D14-1093,W02-1001,0,0.0713606,"main difference into two types: different vocabulary and different POS distributions. While the first type of difference can be effectively resolved by using lexicon for each domain, the second type of difference needs to be resolved by using annotated sentences. They found that given the same manual annotation time, a combination of the lexicon and sentence is the most effective. Jiang et al. (2013) use 160K Wikipedia sentences to improves segmentation accuracies on several domains. Both Zhang et al. (2014) and Jiang et al. (2013) work on discriminative models using the structure perceptron (Collins, 2002), although they study two different sources of information. In contrast to their work, we unify both types of information under the CRF framework. CRF has been used for Chinese word segmentation (Tseng, 2005; Shi and Wang, 2007; Zhao and Kit, 2008; Wang et al., 2011). However, most previous work train a CRF by using full annotation only. In contrast, we study CRF based segmentation by using both full and partial annotation. Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introd"
D14-1093,J05-4005,0,0.291234,"Missing"
D14-1093,C12-2116,0,0.0277869,"pedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled domain-specific lexicons to improve target-domain segmentation accuracies. Both 864 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864–874, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics to improve segmentation. However, no empirical results have been reported on a unified approach to deal with different types of free data. We use a conditional rand"
D14-1093,D12-1075,0,0.028153,"he best reported in the literature. 1 浦 东 开 发 与 法 制 建 设 b m b m b m b m b m b m b m b m b m e e e e e e e e e s s s s s s s s s Figure 1: The segmentation problem, illustrated using the sentence “浦东 (Pudong) 开发 (development) 与 (and) 法制 (legal) 建设 (construction)”. Possible segmentation labels are drawn under each character, where b, m, e, s stand for the beginning, middle, end of a multi-character word, and a single character word, respectively. The path shows the correct segmentation by choosing one label for each character. methods are competitive given the same amount of annotation effects (Garrette and Baldridge, 2012; Zhang et al., 2014). However, obtaining manually annotated data can be expensive. On the other hand, there are free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and semiannotated web pages such as Wikipedia. In the last case, word-boundary information is contained in hyperlinks and other markup annotations. Such free data offer a useful alternative for improving the segmentation performance, especially on domains that are not identical to newswire, and for which little annotation is available."
D14-1093,D11-1090,0,0.225529,"is contained in hyperlinks and other markup annotations. Such free data offer a useful alternative for improving the segmentation performance, especially on domains that are not identical to newswire, and for which little annotation is available. In this paper, we investigate techniques for adopting freely available data to help improve the performance on Chinese word segmentation. We propose a simple but robust method for constructing partial segmentation from different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token-"
D14-1093,P08-1076,0,0.0121424,"formation under the CRF framework. CRF has been used for Chinese word segmentation (Tseng, 2005; Shi and Wang, 2007; Zhao and Kit, 2008; Wang et al., 2011). However, most previous work train a CRF by using full annotation only. In contrast, we study CRF based segmentation by using both full and partial annotation. Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introduce knowledge by incorporating a manually annotated feature distribution into the regularizer, and the JESS-CM (Suzuki and Isozaki, 2008), which use a EM-like method to iteratively optimize the parameter on both the annotated data and unlabeled data. In contrast, we directly incorporate the likelihood of partial annotation into the objective function. The work that is the most similar to ours is Tsuboi et al. (2008), who modify the CRF learning objective for partial data. They focus on Japanese lexical analysis using manually collected partial data, while we investigate the effect of partial annotation from freely available sources for Chinese segmentation. 7 Conclusion In this paper, we investigated the problem of domain adapt"
D14-1093,P09-1059,0,0.511663,"and for which little annotation is available. In this paper, we investigate techniques for adopting freely available data to help improve the performance on Chinese word segmentation. We propose a simple but robust method for constructing partial segmentation from different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled"
D14-1093,P13-1075,0,0.533434,"Missing"
D14-1093,C08-1113,0,0.183793,"-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled domain-specific lexicons to improve target-domain segmentation accuracies. Both 864 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864–874, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics to improve segmentation. However, no empirical results have been reported on a unified approach to deal with different types of free data. We use a conditional random fields (Lafferty et al., 2001; Tsuboi et al., 2008) variant that can leverage the partial annotations obtained from different sources of free annotation. Training is achieved by a modification to the learning objective, incorporating partial annotation likelihood, so that a single model can be trained consistently with a mixture of full and partial annotation. Experimental results show that our method of using partially annotated data can consistently improves cross-domain segmentation performance. We obtain results which are competitive to the best reported in the literature. Our segmentor is freely released at https://github.com/ ExpResults/"
D14-1093,I11-1035,0,0.0409989,"perlinks and other markup annotations. Such free data offer a useful alternative for improving the segmentation performance, especially on domains that are not identical to newswire, and for which little annotation is available. In this paper, we investigate techniques for adopting freely available data to help improve the performance on Chinese word segmentation. We propose a simple but robust method for constructing partial segmentation from different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised"
D14-1093,C96-1035,0,0.0297556,"form of partial annotation with same unresolved ambiguities, as shown in Figure 2, and use them together with available full annotation (Figure 1) as the training data for the segmentor. In this section, we describe in detail how to obtain partially annotated sentences from each resource, respectively. 2.1 Lexicons In this scenario, we assume that there are unlabeled sentences along with a lexicon for the target domain. We obtain partially segmented sentences by extracting word boundaries from the unlabeled sentences with the help of the lexicon. Previous matching methods (Wu and Tseng, 1993; Wong and Chan, 1996) for Chinese word segmentation largely rely on the lexicons, and are generally considered being weak in ambiguity resolution (Gao 865 People’s Daily Wikipedia 看到 (saw) 海南 (Hainan) 旅游业 (tourist industry) 充满 (full) 希望 (hope) saw tourist industry in Hainan is full of hope 主要(mainly) 是(is) 旅游 (tourist) 业 (industry) 和(and) 软件 (software) 产业(industry) mainly is tourist industry and software industry (a) Case of incompatible annotation on “旅游业(tourist industry)” between People’s Daily and Wikipedia. Literature Computer 《说文解字 (Shuo Wen Jie Zi, a book) 段(segmented) 注(annotated) 》 the segmented and annot"
D14-1093,W03-1728,0,0.317988,"omains that are not identical to newswire, and for which little annotation is available. In this paper, we investigate techniques for adopting freely available data to help improve the performance on Chinese word segmentation. We propose a simple but robust method for constructing partial segmentation from different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-sup"
D14-1093,P07-1106,1,0.561664,"identical to newswire, and for which little annotation is available. In this paper, we investigate techniques for adopting freely available data to help improve the performance on Chinese word segmentation. We propose a simple but robust method for constructing partial segmentation from different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverag"
D14-1093,E14-1062,1,0.310899,"ture. 1 浦 东 开 发 与 法 制 建 设 b m b m b m b m b m b m b m b m b m e e e e e e e e e s s s s s s s s s Figure 1: The segmentation problem, illustrated using the sentence “浦东 (Pudong) 开发 (development) 与 (and) 法制 (legal) 建设 (construction)”. Possible segmentation labels are drawn under each character, where b, m, e, s stand for the beginning, middle, end of a multi-character word, and a single character word, respectively. The path shows the correct segmentation by choosing one label for each character. methods are competitive given the same amount of annotation effects (Garrette and Baldridge, 2012; Zhang et al., 2014). However, obtaining manually annotated data can be expensive. On the other hand, there are free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and semiannotated web pages such as Wikipedia. In the last case, word-boundary information is contained in hyperlinks and other markup annotations. Such free data offer a useful alternative for improving the segmentation performance, especially on domains that are not identical to newswire, and for which little annotation is available. In this paper, we in"
D14-1093,I08-1002,0,0.0330375,"kipedia data. (3) along with To make the most use of free annotation, we combine available free lexicon and natural annotation resources by joining the partially annotated sentences derived using each resource, training our CRF model with these partially annotated sentences and the fully annotated PD sentences. The tests are performed on medicine and computer domains. Table 7 shows the results, where further improvements are made on both domains when the two types of resources are combined. 6 Related Work There has been a line of research on making use of unlabeled data for word segmentation. Zhao and Kit (2008) improve segmentation performance by mutual information between characters, collected from large unlabeled data; Li and Sun (2009) use punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words; Sun and Xu (2011) explore several statistical features derived from unlabeled data to help improve character-based word segmentation. These investigations mainly focus on in-domain accuracies. Liu and Zhang (2012) 871 0.78 0.9525 0.9522 0.8775 0.9225 0.8750 0.9200 0.76 0.9175 0.75 0.8725 0.77 0.9519 0.74 0.9150 0.8700 0.73 0.9516 0 50 100 15"
D17-1296,P16-1231,0,0.109656,"ons taken always equals to the number of input sentence for every valid path, it is straightforward to use beam search. We use beamsearch for both training and testing. The early update strategy from Collins and Roark (2004) is applied for training. In particular, each training sequence is decoded, and we keep track of the location of the gold path in the beam. If the gold path falls out of the beam at step t, decoding process is stopped and parameter update is performed using the gold path as a positive example, and beam items as negative examples. We also use the global optimization method (Andor et al., 2016; Zhou et al., 2015) to train our beam-search model. Scheduled Sampling Scheduled sampling (Bengio et al., 2015) can also be used to reduce error propagation. The training goal of the greedy baseline is to maximize the likelihood of each action given the current model state, which means that the correct action is taken at each step. Doing inference, the action predicted by the model itself is taken instead. This discrepancy between training and inference can yield errors that accumulate quickly along the searching process. Scheduled sampling is used to solve the discrepancy by gently changing"
D17-1296,D16-1254,0,0.0289924,"Missing"
D17-1296,N01-1016,0,0.785208,"nnotation layers are provided: one for syntactic bracketing (MRG files), and the other for disfluencies (DPS files). The Switchboard annotation project was not fully completed. Because disfluency annotation is cheaper to produce, many of the DPS training files do not have matching MRG files. Only 619,236 of the 1,482,845 tokens in the DPS disfluency detection training data have gold-standard syntactic parses. To directly compare with transitionbased parsing methods (Honnibal and Johnson, 2014; Wu et al., 2015), we also use the subcorpus of PARSED/MRG/SWBD. Following the experiment settings in Charniak and Johnson (2001), the training subcorpus contains directories 2 and 3 in PARSED/MRG/SWBD and directory 4 is split into test, development sets and others. Following Honnibal and Johnson (2014), we lower-case the text and remove all punctuations and partial words2 . We also discard the ‘um’ and ‘uh’ tokens and merge ‘you know’ and ‘i mean’ into single tokens. Automatic POS-tags generated from pocket crf (Qian and Liu, 2013) are used as POStag in our experiments. For Chinese experiments, we collect 25k spoken sentences from meeting minutes, which are transcribed using the iflyrec toolkit3 , and annotate them wit"
D17-1296,P04-1015,0,0.377117,"As shown in Figure 2, the model state consists of four components: (i) O, a conventional sequential LSTM (Hochreiter and Schmidhuber, 1997) to store the words that have been labeled as fluency. (ii) S, a stack LSTM to represent partial disfluency chunks, which captures chunklevel information. (iii) A, a conventional sequential LSTM to represent history of actions. (iiii) B, a Bi-LSTM to represent words that have not yet been processed. A sequence of transition actions are used to consume input tokens and construct the output from left to right. To reduce error propagation, we use beam-search (Collins and Roark, 2004) and scheduled sampling (Bengio et al., 2015), respectively. We evaluate our model on the commonly used English Switchboard test set and a in-house annotated Chinese data set. Results show that our model outperforms previous state-of-the-art systems. The code is released1 . 2 Background For a background, we briefly introduce transitionbased parsing and its extention for joint disfluency detection. An arc-eager transition-based parsing system consists of a stack σ containing words being processed, a buffer β containing words to be processed and a memory A storing dependency 1 https://github.com"
D17-1296,P15-1033,0,0.413262,"S to boston to denver Figure 2: model state when processing the sentence “want a flight to boston to denver”. arcs which have been generated. There are four types of transition actions (Nivre, 2008) • Shift : Remove the front of the buffer and push it to the stack. • Reduce : Pop the top of the stack. • LeftArc : Pop the top of the stack, and link the popped word to the front of the buffer. • RightArc : Link the front of the buffer to the top of the stack, remove the front of the buffer and push it to the stack. Many neural network parsers have been constructed under this framework, such as (Dyer et al., 2015), who use different LSTM structure to represent information from σ to β. For disfluency detection, the input is a sentence with disfluencies from automatic speech recognition (ASR). We denote the word sequence as w1n = (w1 , ..., wn ). The output of the task is a sequence of binary tags denoted as D1n = (d1 , ..., dn ), where each di corresponds to the word wi , indicating whether wi is a disfluent word or not. Hence the task can be modeled as searching for the best sequenc D∗ given the stream of words w1n D∗ = argmaxD P (D1n |w1n ) Wu et al. (2015) proposes a statistical transitionbased disfl"
D17-1296,N15-1029,0,0.310716,"Missing"
D17-1296,N09-2028,0,0.441731,", it is very important to capture long-range dependencies for disfluency detection. Since there is large parallelism between the reparandum chunk and the following repair chunk (for example, in Figure 1, the reparandum begins with to and ends before another occurrence of to), it is also useful to exploit chunk-level representation, which explicitly makes use of resulted infelicity disfluency chunks. Common approaches take disfluency detection as a sequence labeling problem, where each sentential word is assigned with a label (Zayats et al., 2016; Hough and Schlangen, 2015; Qian and Liu, 2013; Georgila, 2009). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Another drawback of these approaches is that they are unable to exploit chunk2785 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2785–2794 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level features. Semi-CRF (Ferguson et al., 2015) is used to alleviate this issue to some extent. SemiCRF models still have their inefficiencies because they can only use the local chun"
D17-1296,Q14-1011,0,0.60128,"spans or distances. Another drawback of these approaches is that they are unable to exploit chunk2785 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2785–2794 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level features. Semi-CRF (Ferguson et al., 2015) is used to alleviate this issue to some extent. SemiCRF models still have their inefficiencies because they can only use the local chunk information limited by the markov assumption when decoding. A different line of work (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) adopts transition-based parsing models for disfluency detection. This line of work can be seen as a joint of disfluency detection and parsing. The main advantage of the joint models is that they can capture long-range dependency of disfluencies as well as chunk-level information. However, they introduce additional annotated syntactic structure, which is very expensive to produce, and can cause noise by significantly enlarging the output search space. Inspired by the above observations, we investigate a transition-based model without syntactic information. Our model increment"
D17-1296,N16-1030,0,0.0415195,"uencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced a transition-based model for disfluency detection, which does not use any syntax information, learning represention of both chunks and global contexts. Experiments showed that our model achieves the state"
D17-1296,N06-2019,0,0.507578,"lish Switchboard test data. attention-based model can capture a global representation of the input sentence by using a RNN when encoding. It can strongly capture long-range dependencies and achieves good performance, but are also not powerful enough to capture chunklevel information. To capture chunk-level information, Ferguson et al. (2015) try to use semi-CRF for disfluency detection, and reports improved results. Semi-CRF models still have their inefficiencies because they can only use the local chunk information limited by the markov assumption when decoding. Many syntax-based approaches (Lease and Johnson, 2006; Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) have been proposed which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre,"
D17-1296,P14-1038,0,0.0262712,"n, 2014; Wu et al., 2015) have been proposed which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced"
D17-1296,N15-1142,0,0.0289563,"d Liu, 2013) are used as POStag in our experiments. For Chinese experiments, we collect 25k spoken sentences from meeting minutes, which are transcribed using the iflyrec toolkit3 , and annotate them with only disfluency annotations according to the guideline proposed by Meteer et al. (1995). 2 words are recognized as partial words if they are tagged as ‘XX’ or end with ‘-’ 3 the iflyrec toolkit is available at http://www.iflyrec.com/ Neural Network Training Pretrained Word Embeddings. Following Dyer et al. (2015) and Wang et al. (2016), we use a variant of the skip n-gram model introduced by Ling et al. (2015), named “structured skip n-gram”, to create word embeddings. The AFP portion of English Gigaword corpus (version 5) is used as the training corpus. Word embeddings for Chinese are trained on Chinese baike corpus. We use an embedding dimension of 100 for English, 300 for chinese. Hyper-Parameters. Both the Bi-LSTMs and the stack LSTMs have two hidden layers and their dimensions are set to 100. Pretrained word embeddings have 100 dimensions and the learned word embeddings have also 100 dimensions. Pos-tag embeddings have 12 dimensions. The dimension of action embeddings is set to 20. 4.3 Perform"
D17-1296,J93-2004,0,0.0579037,"(wi wi+1 , wi+k wi+k+1 ), −4 ≤ k ≤ +4 and k 6= 0: if wi wi+1 equals wi+k wi+k+1 , the value is 1, others 0 Duplicate(pi pi+1 , pi+k pi+k+1 ), −4 ≤ k ≤ +4 and k 6= 0: if pi pi+1 equals pi+k pi+k+1 , the value is 1, others 0 similarity features f uzzyM atch(wi , wi+k ), k ∈ {−1, +1}: similarity = 2 ∗ num same letters/(len(wi ) + len(wi+k )). if similarity > 0.8, the value is 1, others 0 Table 3: Discrete features used in our transition-based neural networks. p-POS tag. w-word. 4 Experiments 4.1 4.2 Settings Dataset. Our training data include the Switchboard portion of the English Penn Treebank (Marcus et al., 1993) and a in-house Chinese data set. For English, two annotation layers are provided: one for syntactic bracketing (MRG files), and the other for disfluencies (DPS files). The Switchboard annotation project was not fully completed. Because disfluency annotation is cheaper to produce, many of the DPS training files do not have matching MRG files. Only 619,236 of the 1,482,845 tokens in the DPS disfluency detection training data have gold-standard syntactic parses. To directly compare with transitionbased parsing methods (Honnibal and Johnson, 2014; Wu et al., 2015), we also use the subcorpus of PA"
D17-1296,J08-4003,0,0.0365412,"y introduce transitionbased parsing and its extention for joint disfluency detection. An arc-eager transition-based parsing system consists of a stack σ containing words being processed, a buffer β containing words to be processed and a memory A storing dependency 1 https://github.com/hitwsl/transition disfluency OUT DEL et=max{0,W[st;bt;ot;at]+d} O ot at st B bt Bi-LSTM Subtraction want a flight A DEL DEL TOP S to boston to denver Figure 2: model state when processing the sentence “want a flight to boston to denver”. arcs which have been generated. There are four types of transition actions (Nivre, 2008) • Shift : Remove the front of the buffer and push it to the stack. • Reduce : Pop the top of the stack. • LeftArc : Pop the top of the stack, and link the popped word to the front of the buffer. • RightArc : Link the front of the buffer to the top of the stack, remove the front of the buffer and push it to the stack. Many neural network parsers have been constructed under this framework, such as (Dyer et al., 2015), who use different LSTM structure to represent information from σ to β. For disfluency detection, the input is a sentence with disfluencies from automatic speech recognition (ASR)."
D17-1296,N13-1102,0,0.666823,"fifteen words. Hence, it is very important to capture long-range dependencies for disfluency detection. Since there is large parallelism between the reparandum chunk and the following repair chunk (for example, in Figure 1, the reparandum begins with to and ends before another occurrence of to), it is also useful to exploit chunk-level representation, which explicitly makes use of resulted infelicity disfluency chunks. Common approaches take disfluency detection as a sequence labeling problem, where each sentential word is assigned with a label (Zayats et al., 2016; Hough and Schlangen, 2015; Qian and Liu, 2013; Georgila, 2009). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Another drawback of these approaches is that they are unable to exploit chunk2785 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2785–2794 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level features. Semi-CRF (Ferguson et al., 2015) is used to alleviate this issue to some extent. SemiCRF models still have their inefficiencies because they can only u"
D17-1296,D13-1013,0,0.490055,"ated disfluencies with longer spans or distances. Another drawback of these approaches is that they are unable to exploit chunk2785 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2785–2794 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level features. Semi-CRF (Ferguson et al., 2015) is used to alleviate this issue to some extent. SemiCRF models still have their inefficiencies because they can only use the local chunk information limited by the markov assumption when decoding. A different line of work (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) adopts transition-based parsing models for disfluency detection. This line of work can be seen as a joint of disfluency detection and parsing. The main advantage of the joint models is that they can capture long-range dependency of disfluencies as well as chunk-level information. However, they introduce additional annotated syntactic structure, which is very expensive to produce, and can cause noise by significantly enlarging the output search space. Inspired by the above observations, we investigate a transition-based model without syntactic info"
D17-1296,C16-1027,1,0.916833,"t set and a set of in-house annotated Chinese data. 1 RP Figure 1: Sentence with disfluencies annotated in English Switchboard corpus. RM=Reparandum, IM=Interregnum, RP=Repair. The preceding RM is corrected by the following RP. Type repair repair repetition restart Annotation [ I just + I ] enjoy working [ we want + {well} in our area we want ] to [it’s + {uh} it’s ] almost like [ we would like + ] let’s go to the Table 1: Different types of disfluencies. Introduction Disfluency detection is the task of recognizing non-fluent word sequences in spoken language transcripts (Zayats et al., 2016; Wang et al., 2016; Wu et al., 2015). As shown in Figure 1, standard annotation of disfluency structure (Shriberg, 1994) indicates the reparandum (words that are discarded, or corrected by the following words), the interruption point (+) marking the end of the reparandum, the associated repair, and an optional interregnum after the interruption point (filled pauses, discourse cue words, etc.). Ignoring the interregnum, disfluencies can be categorized into three types: restarts, repetitions, and corrections, based on whether the repair is empty, the same as the reparandum or different, respectively. Table 1 give"
D17-1296,P16-1218,0,0.0207443,"Scheduled sampling is used to solve the discrepancy by gently changing the training process from a fully guided scheme using the true previous action, towards a less guided scheme which mostly uses the predicting action instead. We take the action gaining higher p(zt |et ) with a certain probability p, and a probability (1 − p) for the correct action when training. 3.2 State Representation For better capturing non-local context information, we use LSTM structures to represent different components of each state, including buffer, action, stack, and output. In particular, we exploit LSTM-Minus (Wang and Chang, 2016) to model the buffer segment, conventional LSTM to model the action and ouptut segment, and stack LSTM (Dyer et al., 2015) to model the stack segments, which demonstrates highly effectively in parsing task. Buffer Representation In order to construct more informative representation, we use a Bi-LSTM to represent the buffer following the work of Wang and Chang (2016), where the subtraction between a unidirectional 2788 O want a flight S to boston B to denver hb(to) hf(to) hb(denver) hf(denver) to boston to bb = hb(denver) - hb(to) denver bf = hf(to) - hf(denver) Figure 3: Illustration for learn"
D17-1296,P15-1113,0,0.024077,"ngineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced a transition-based model for disfluency detection, which does not use any syntax information, learning represention of both chunks and global contexts. Experiments showed that our model achieves the state-of-the-art F-scores on both the commonly used English Switchboard test set and a in-house annotated Chinese data set. Acknowledgments We thank the anonymous reviewers for their valuable suggestions. This work wa"
D17-1296,P15-1048,0,0.410628,"in-house annotated Chinese data. 1 RP Figure 1: Sentence with disfluencies annotated in English Switchboard corpus. RM=Reparandum, IM=Interregnum, RP=Repair. The preceding RM is corrected by the following RP. Type repair repair repetition restart Annotation [ I just + I ] enjoy working [ we want + {well} in our area we want ] to [it’s + {uh} it’s ] almost like [ we would like + ] let’s go to the Table 1: Different types of disfluencies. Introduction Disfluency detection is the task of recognizing non-fluent word sequences in spoken language transcripts (Zayats et al., 2016; Wang et al., 2016; Wu et al., 2015). As shown in Figure 1, standard annotation of disfluency structure (Shriberg, 1994) indicates the reparandum (words that are discarded, or corrected by the following words), the interruption point (+) marking the end of the reparandum, the associated repair, and an optional interregnum after the interruption point (filled pauses, discourse cue words, etc.). Ignoring the interregnum, disfluencies can be categorized into three types: restarts, repetitions, and corrections, based on whether the repair is empty, the same as the reparandum or different, respectively. Table 1 gives a few examples."
D17-1296,P13-1013,1,0.811416,"which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced a transition-based model for disfluency detect"
D17-1296,P14-1125,1,0.862598,"Missing"
D17-1296,P16-1040,1,0.852594,"luency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced a transition-based model for disfluency detection, which does not use any syntax information, learning represention of both chunks and global contexts. Experiments showed that our model achieves the state-of-the-art F-scores on both the commonly used English Switchboard test set and a in-ho"
D17-1296,P11-2033,1,0.739192,"nd Johnson, 2006; Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) have been proposed which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has"
D17-1296,P15-1117,1,0.930687,"als to the number of input sentence for every valid path, it is straightforward to use beam search. We use beamsearch for both training and testing. The early update strategy from Collins and Roark (2004) is applied for training. In particular, each training sequence is decoded, and we keep track of the location of the gold path in the beam. If the gold path falls out of the beam at step t, decoding process is stopped and parameter update is performed using the gold path as a positive example, and beam items as negative examples. We also use the global optimization method (Andor et al., 2016; Zhou et al., 2015) to train our beam-search model. Scheduled Sampling Scheduled sampling (Bengio et al., 2015) can also be used to reduce error propagation. The training goal of the greedy baseline is to maximize the likelihood of each action given the current model state, which means that the correct action is taken at each step. Doing inference, the action predicted by the model itself is taken instead. This discrepancy between training and inference can yield errors that accumulate quickly along the searching process. Scheduled sampling is used to solve the discrepancy by gently changing the training process"
D17-1296,P13-1043,1,0.849581,"li and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) have been proposed which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigat"
D18-1264,P17-1044,0,0.0254208,"d in our intrinsic evaluation. 8 e.g., “North Koreans” cannot be parsed into (name :op1 &quot;North&quot; :op2 &quot;Korea&quot;) 9 e.g., “Wi Sung - lac” cannot be parsed into (name :op1 &quot;Wi&quot; :op2 &quot;Sung-lac&quot;) 10 e.g. the first “nuclear” aligned to nucleus˜1 in Fig. 1 exp{ga · S TACK LSTM(s) + ba } , a0 exp{ga0 · S TACK LSTM(s) + ba0 } where S TACK LSTM(s) encodes the state s into a vector and ga is the embedding vector of action a. We encourage the reader to refer Ballesteros and Al-Onaizan (2017) for more details. Ensemble. Ensemble has been shown as an effective way of improving the neural model’s performance (He et al., 2017). Since the transitionbased parser directly parse a sentence into its AMR graph, ensemble of several parsers is easier compared to the two-staged AMR parsers. In this paper, we ensemble the parsers trained with different initialization by averaging their probability distribution over the actions. 5 5.1 Alignment Experiments Settings We evaluate our aligner on the LDC2014T12 dataset. Two kinds of evaluations are carried out including the intrinsic and extrinsic evaluations. For the intrinsic evaluation, we follow Flanigan et al. (2014) and evaluate the F1 score of the alignments produced by our"
D18-1264,P17-1014,0,0.380708,"scu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 2014) or adopting the unsupervised word alignment technique from machine translation (Pourdamghani et al.,"
D18-1264,D15-1198,0,0.220272,"Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either gree"
D18-1264,N15-1142,0,0.0374791,"uced by the semantic matching especially by the word embedding similarity component. Without filtering the noise by our oracle parser, just introducing more matching rules will harm the performance. 6 6.1 newswire 73.1 72.7 67.6 71.3 Parsing Experiments Settings We use the same settings in our aligner extrinsic evaluation for the experiments on our transitionbased parser. For the input to the parser, we tried two settings: 1) using only words as input, and 2) using words and POS tags as input. Automatic POS tags are assigned with Stanford POS tagger (Manning et al., 2014). Word embedding from Ling et al. (2015) is used in the same way with Ballesteros and Al-Onaizan (2017). To opt Results Table 6 shows the performance of our transitionbased parser along with comparison to the parsers in the previous works. When compared with our transition-based counterpart (Ballesteros and AlOnaizan, 2017), our word-only model outperforms theirs using the same JAMR alignment. The same trend is witnessed using words and POS tags as input. When replacing the JAMR alignments with ours, the parsing performances are improved in the same way as in Table 4, which further confirms the effectiveness of our aligner. The seco"
D18-1264,D17-1130,0,0.0550349,"ar reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 2014) or adopting the unsupervised word alignment technique"
D18-1264,W13-2322,0,0.262658,"sistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score, which outperforms the parser of Wang and Xue (2017). 1 ARG1 ARG0 country act-02 name name op1 &quot;North&quot; ARG1 reactor mod nucleus~1 quant 2 mod nucleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2"
D18-1264,P13-2131,0,0.475771,"Missing"
D18-1264,P13-1104,0,0.266774,"rst element of the buffer (a word or span) into a concept c. a special form of C ONFIRM that derives the first element into an entity and builds the internal entity AMR fragment. generates a new concept c and pushes it to the front of the buffer. Table 2: The transition system. The letters in monospace font represent the concepts, the italic letters represent the word, and the letters in normal font are either concepts or words. Their work presented the possibility for the oracle parser, but their oracle parser was not touched explicitly. What’s more, in the non-projective dependency parsing, Choi and McCallum (2013)’s extension to the list-based system (Nivre, 2008) with caching mechanism achieves expected linear time complexity and requires fewer actions to parse a non-projective tree than the swap-based system. Their extension to transition-based AMR parsing is worth studying. In this paper, we propose to extend Choi and McCallum (2013)’s transition system to AMR parsing and present the corresponding oracle parser. The oracle parser is used for tuning our aligner and training our parser. We also present a comprehensive comparison of our system with that of Ballesteros and Al-Onaizan (2017) in Section 6"
D18-1264,E17-1051,0,0.619876,"exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 2014) or adopting the uns"
D18-1264,P15-1033,0,0.0258826,"ctions in Table 2 are our ex2426 4. If b0 is a word or span and it aligns to one or more concepts, perform C ONFIRM (c) where c is the concept b0 aligns and has the longest graph distance to the root. 5. If b0 is a concept and its head concept c has the same alignment as b0 , perform N EW (c). 6. If b0 is a concept and there is an unprocessed edge r between s0 and t0 , perform L EFT (r) or R IGHT (r) according to r’s direction. g1 a1 Training Data Aligner .. . an Oracle .. . raw sentence directly into its AMR graph. In this paper, we follow Ballesteros and Al-Onaizan (2017) and use StackLSTM (Dyer et al., 2015) to model the states. The score of a transition action a on state s is calculated as s1 Eval. gn .. . sn p(a|s) = P highest-scored, pick Figure 2: The workflow of tuning the aligner with the oracle parser. ai denotes the i-th alignment, gi denotes the i-th AMR graph, and si denotes the score of the i-th AMR graph. 7. If s0 has unprocessed edge, perform C ACHE. 8. If s0 doesn’t have unprocessed edge, perform R EDUCE. 9. perform S HIFT. We test our oracle parser on the hand-align data created by Flanigan et al. (2014) and it achieves 97.4 Smatch F1 score.7 Besides the errors resulted from incorr"
D18-1264,S16-1186,0,0.252255,"ieves 68.4 Smatch F1 score with only words and POS tags as input (§6) and outperforms the parser of Wang and Xue (2017). Our code and the alignments for LDC2014T12 dataset are publicly available at https:// github.com/Oneplus/tamr 2 Related Work AMR Parsers. AMR parsing maps a natural language sentence into its AMR graph. Most current parsers construct the AMR graph in a two-staged manner which first identifies concepts (nodes in the graph) from the input sentence, then identifies relations (edges in the graph) between the identified concepts. Flanigan et al. (2014) and their follow-up works (Flanigan et al., 2016; Zhou et al., 2016) model the parsing problem as finding the maximum spanning connected graph. Wang et al. (2015b) proposes to greedily transduce the dependency tree into AMR graph and a bunch of works (Wang et al., 2015a; Goodman et al., 2016; Wang and Xue, 2017) further improve the transducer’s performance with rich features and imitation learning.2 Transition-based methods 2 Wang et al. (2015b) and the follow-up works refer their transducing process as “transition-based”. However, to dis2423 that directly parse an input sentence into its AMR graph have also been studied (Ballesteros and Al"
D18-1264,P14-1134,0,0.0948162,"&quot; ARG1 reactor mod nucleus~1 quant 2 mod nucleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In"
D18-1264,P17-1043,0,0.136676,"resentation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 2014) or adopting the unsupervised word alignment technique from machine translation"
D18-1264,P16-1001,0,0.124026,"ts nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 201"
D18-1264,P14-5010,0,0.0056679,"servation to that alignment noise is introduced by the semantic matching especially by the word embedding similarity component. Without filtering the noise by our oracle parser, just introducing more matching rules will harm the performance. 6 6.1 newswire 73.1 72.7 67.6 71.3 Parsing Experiments Settings We use the same settings in our aligner extrinsic evaluation for the experiments on our transitionbased parser. For the input to the parser, we tried two settings: 1) using only words as input, and 2) using words and POS tags as input. Automatic POS tags are assigned with Stanford POS tagger (Manning et al., 2014). Word embedding from Ling et al. (2015) is used in the same way with Ballesteros and Al-Onaizan (2017). To opt Results Table 6 shows the performance of our transitionbased parser along with comparison to the parsers in the previous works. When compared with our transition-based counterpart (Ballesteros and AlOnaizan, 2017), our word-only model outperforms theirs using the same JAMR alignment. The same trend is witnessed using words and POS tags as input. When replacing the JAMR alignments with ours, the parsing performances are improved in the same way as in Table 4, which further confirms th"
D18-1264,J08-4003,0,0.0603115,"special form of C ONFIRM that derives the first element into an entity and builds the internal entity AMR fragment. generates a new concept c and pushes it to the front of the buffer. Table 2: The transition system. The letters in monospace font represent the concepts, the italic letters represent the word, and the letters in normal font are either concepts or words. Their work presented the possibility for the oracle parser, but their oracle parser was not touched explicitly. What’s more, in the non-projective dependency parsing, Choi and McCallum (2013)’s extension to the list-based system (Nivre, 2008) with caching mechanism achieves expected linear time complexity and requires fewer actions to parse a non-projective tree than the swap-based system. Their extension to transition-based AMR parsing is worth studying. In this paper, we propose to extend Choi and McCallum (2013)’s transition system to AMR parsing and present the corresponding oracle parser. The oracle parser is used for tuning our aligner and training our parser. We also present a comprehensive comparison of our system with that of Ballesteros and Al-Onaizan (2017) in Section 6.3. 4.1 tended actions, and they are used to derivi"
D18-1264,K15-1004,0,0.283535,"for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set"
D18-1264,E17-1035,0,0.292067,"; Damonte et al., 2017). In these works, the concept identification and relation identification are performed jointly. An aligner which maps a span of words into its concept serves to the generation of training data for the concept identifier, thus is important to the parser training. Missing or incorrect alignments lead to poor concept identification, which then hurt the overall AMR parsing performance. Besides the typical two-staged methods, the aligner also works in some other AMR parsing algorithms like that using syntax-based machine translation (Pust et al., 2015), sequence-to-sequence (Peng et al., 2017; Konstas et al., 2017), Hyperedge Replacement Grammar (Peng et al., 2015) and Combinatory Category Grammar (Artzi et al., 2015). Previous aligner works solve the alignment problem in two different ways. The rule-based aligner (Flanigan et al., 2014) defines a set of heuristic rules which align a span of words to the graph fragment and greedily applies these rules. The unsupervised aligner (Pourdamghani et al., 2014; Wang and Xue, 2017) uncovers the word-toconcept alignment from the linearized AMR graph through EM. All these approaches yield a single alignment for one sentence and its effect o"
D18-1264,D14-1162,0,0.081,"rs because of the dependencies and mutual exclusions between rules. In the JAMR aligner, rules that recall more alignments but introduce errors are carefully opted out and it influences the aligner’s performance. Our motivation is to use rich semantic resources to recall more alignments. Instead of resolving the resulted conflicts and errors by greedy search, we keep the multiple alignments produced by the aligner and let a parser decide the best alignment. In this paper, we use two kinds of semantic resources to recall more alignments, which include the similarity drawn from Glove embedding (Pennington et al., 2014)3 and the morphosemantic database (Fellbaum et al., 2009) in the WordNet project4 . Two additional matching schemes semantic match and morphological match are proposed as: Semantic Match. Glove embedding encodes a word into its vector representation. We define semantic match of a concept as a word in the sentence that has a cosine similarity greater than 0.7 in the embedding space with the concept striping off trailing number (e.g. run-01 → run). Morphological Match. Morphosemantic is a database that contains links among derivational 3 nlp.stanford.edu/projects/glove/ wordnet.princeton.edu/wor"
D18-1264,D14-1048,0,0.483311,"Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 2014) or adopting the unsupervised word alignment technique from machine translation (Pourdamghani et al., 2014; Wang and Xue, 2017). The rule-based aligner – JAMR aligner proposed by Flanigan et al. (2014) is widely used in previous works thanks to its flexibility of incorporating additional linguistic resources like WordNet. However, achieving good alignments with the JAMR aligner still faces some difficult challenges. The first challenge is deriving an optimal alignment in ambiguous situations. Taking the sentence-AMR-graph pair in Figure 1 for example, the JAMR aligner doesn’t distinguish between the two “nuclear”s in the sentence and can yield sub-optimal alignment in which the first “nuclear” is"
D18-1264,D15-1136,0,0.286353,"cleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extr"
D18-1264,D17-1035,0,0.0145197,"Aligner JAMR Our Alignment F1 (on hand-align) 90.6 95.2 Oracle’s Smatch (on dev. dataset) 91.7 94.7 model JAMR parser + Our aligner - Semantic matching - Oracle Parser Tuning JAMR parser + JAMR aligner Table 3: The intrinsic evaluation results. model newswire JAMR parser: Word, POS, NER, DEP + JAMR aligner 71.3 + Our aligner 73.1 CAMR parser: Word, POS, NER, DEP + JAMR aligner 68.4 + Our aligner 68.8 Table 5: The ablation test results. all out the effect of different initialization in training the neural network, we run 10 differently seeded runs and report their average performance following Reimers and Gurevych (2017). 65.9 67.6 64.6 65.1 6.2 Table 4: The parsing results. Extrinsic Evaluation. Table 4 shows the results. From this table, we can see that our alignment consistently improves all the parsers by a margin ranging from 0.5 to 1.7. Both the intrinsic and the extrinsic evaluations show the effectiveness our aligner. 5.3 Ablation To have a better understanding of our aligner, we conduct ablation test by removing the semantic matching and oracle parser tuning respectively and retrain the JAMR parser on the newswire proportion. The results are shown in Table 5. From this table, we can see that removing"
D18-1264,D17-1129,0,0.460076,"racle parser. Our aligner is further tuned by our oracle parser via picking the alignment that leads to the highestscored achievable AMR graph. Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score, which outperforms the parser of Wang and Xue (2017). 1 ARG1 ARG0 country act-02 name name op1 &quot;North&quot; ARG1 reactor mod nucleus~1 quant 2 mod nucleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent"
D18-1264,P15-2141,0,0.39251,"eus~1 quant 2 mod nucleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, su"
D18-1264,N15-1040,0,0.456906,"eus~1 quant 2 mod nucleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, su"
D18-1264,D16-1065,0,0.64792,"North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules"
D19-1013,P16-1014,0,0.199952,"Gumbel(0, 1)1 and τ is a constant that controls the smoothness of the distribution. approx Tj replaces Tj in equation 1 and goes through the same flattening and expanding process as V to approx0 get vt and the training signal from Seq2Seq generation is passed via the logit approx ot 0 ˜ t, h ˜ 0 ] + vtapprox . = U [h t To make training with Gumbel-Softmax more stable, we first initialize the parameters by pretraining the KB-retriever with distant supervision and further fine-tuning our framework. 1 We sample g by drawing u ∼ Uniform(0, 1) then computing g = − log(− log(u)). 137 4.3 • Ptr-UNK (Gulcehre et al., 2016): Ptr-UNK is the model which augments a sequenceto-sequence architecture with attention-based copy mechanism over the encoder context. Experimental Settings We choose the InCar Assistant dataset (Eric et al., 2017) including three distinct domains: navigation, weather and calendar domain. For weather domain, we follow Wen et al. (2018) to separate the highest temperature, lowest temperature and weather attribute into three different columns. For calendar domain, there are some dialogues without a KB or incomplete KB. In this case, we padding a special token “-” in these incomplete KBs. Our fra"
D19-1013,E17-1001,0,0.0371657,"the corresponding KB to every dialogues. All hyper-parameters are selected according to validation set. We use a three-hop memory network to model our KB-retriever. The dimensionalities of the embedding is selected from {100, 200} and LSTM hidden units is selected from {50, 100, 150, 200, 350}. The dropout we use in our framework is selected from {0.25, 0.5, 0.75} and the batch size we adopt is selected from {1, 2}. L2 regularization is used on our model with a tension of 5 × 10−6 for reducing overfitting. For training the retriever with distant supervision, we adopt the weight typing trick (Liu and Perez, 2017). We use Adam (Kingma and Ba, 2014) to optimize the parameters in our model and adopt the suggested hyper-parameters for optimization. We adopt both the automatic and human evaluations in our experiments. 4.4 • KV Net (Eric et al., 2017): The model adopted and argumented decoder which decodes over the concatenation of vocabulary and KB entities, which allows the model to generate entities. • Mem2Seq (Madotto et al., 2018): Mem2Seq is the model that takes dialogue history and KB entities as input and uses a pointer gate to control either generating a vocabulary word or selecting an input as the"
D19-1013,D15-1166,0,0.798183,"based on Seq2Seq generation, there is a trend in recent study towards modeling the KB query as an attention network over the entire KB entity representations, hoping to learn a model to pay more attention to the relevant entities (Eric et al., 2017; Madotto et al., 2018; Reddy et al., 2018; Wen et al., 2018). Introduction Task-oriented dialogue system, which helps users to achieve specific goals with natural language, is attracting more and more research attention. With the success of the sequence-to-sequence (Seq2Seq) models in text generation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Nallapati et al., 2016b,a), several works tried to model the task-oriented dialogue as the Seq2Seq generation of response from the dialogue Though achieving good end-to-end dialogue generation with over-the-entire-KB attention mechanism, these methods do not guarantee the generation consistency regarding KB entities and ∗ * Email corresponding. 133 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 133–142, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computat"
D19-1013,P18-1136,0,0.488682,"ated at 200 Alester Ave. OK , please give me directions via a route that avoids all heavy traffic. Since there is a road block nearby, I found another route for you and I sent it on your screen. Awesome thank you. Figure 1: An example of a task-oriented dialogue that incorporates a knowledge base (KB). The fourth row in KB supports the second turn of the dialogue. A dialogue system will produce a response with conflict entities if it includes the POI in the fourth row and the address in the fifth row, like “Valero is located at 899 Ames Ct”. history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018). This kind of modeling scheme frees the task-oriented dialogue system from the manually designed pipeline modules and heavy annotation labor for these modules. Different from typical text generation, the successful conversations for task-oriented dialogue system heavily depend on accurate knowledge base (KB) queries. Taking the dialogue in Figure 1 as an example, to answer the driver’s query on the gas station, the dialogue system is required to retrieve the entities like “200 Alester Ave” and “Valero”. For the task-oriented system based on Seq2Seq generation, there is a trend in recent study"
D19-1013,N13-1095,0,0.0304659,"|V |+|E|. In vt , lower |V |is zero and the rest|E |is retrieved entity scores. 4 Training the KB-Retriever As mentioned in section 3.3.1, we adopt the memory network to train our KB-retriever. However, in the Seq2Seq dialogue generation, the training data does not include the annotated KB row retrieval results, which makes supervised training the KBretriever impossible. To tackle this problem, we propose two training methods for our KB-rowretriever. 1) In the first method, inspired by the recent success of distant supervision in information extraction (Zeng et al., 2015; Mintz et al., 2009; Min et al., 2013; Xu et al., 2013), we take advantage of the similarity between the surface string of KB entries and the reference response, and design a set of heuristics to extract training data for the KB-retriever. 2) In the second method, instead of training the KB-retriever as an independent component, we train it along with the training of the Seq2Seq dialogue generation. To make the retrieval process in Equation 1 differentiable, we use Gumbel-Softmax (Jang et al., 2017) as an approximation of the argmax during training. 4.1 4.2 Training with Gumbel-Softmax In addition to treating the row retrieval re"
D19-1013,P09-1113,0,0.0347149,"’s dimensionality is |V |+|E|. In vt , lower |V |is zero and the rest|E |is retrieved entity scores. 4 Training the KB-Retriever As mentioned in section 3.3.1, we adopt the memory network to train our KB-retriever. However, in the Seq2Seq dialogue generation, the training data does not include the annotated KB row retrieval results, which makes supervised training the KBretriever impossible. To tackle this problem, we propose two training methods for our KB-rowretriever. 1) In the first method, inspired by the recent success of distant supervision in information extraction (Zeng et al., 2015; Mintz et al., 2009; Min et al., 2013; Xu et al., 2013), we take advantage of the similarity between the surface string of KB entries and the reference response, and design a set of heuristics to extract training data for the KB-retriever. 2) In the second method, instead of training the KB-retriever as an independent component, we train it along with the training of the Seq2Seq dialogue generation. To make the retrieval process in Equation 1 differentiable, we use Gumbel-Softmax (Jang et al., 2017) as an approximation of the argmax during training. 4.1 4.2 Training with Gumbel-Softmax In addition to treating th"
D19-1013,K16-1028,0,0.0493305,"eration, there is a trend in recent study towards modeling the KB query as an attention network over the entire KB entity representations, hoping to learn a model to pay more attention to the relevant entities (Eric et al., 2017; Madotto et al., 2018; Reddy et al., 2018; Wen et al., 2018). Introduction Task-oriented dialogue system, which helps users to achieve specific goals with natural language, is attracting more and more research attention. With the success of the sequence-to-sequence (Seq2Seq) models in text generation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Nallapati et al., 2016b,a), several works tried to model the task-oriented dialogue as the Seq2Seq generation of response from the dialogue Though achieving good end-to-end dialogue generation with over-the-entire-KB attention mechanism, these methods do not guarantee the generation consistency regarding KB entities and ∗ * Email corresponding. 133 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 133–142, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics tion th"
D19-1013,P17-1045,0,0.0384064,"of neural network in natural language processing, efforts have been made to replacing the discrete and predefined dialogue state with the distributed representation (Bordes and Weston, 2017; Wen et al., 2017b,a; Liu and Lane, 2017). In our framework, our retrieval result can be treated as a numeric representation of the API call return. Instead of interacting with the KB via API calls, more and more recent works tried to incorporate KB query as a part of the model. The most popular way of modeling KB query is treating it as an attention network over the entire KB entities (Eric et al., 2017; Dhingra et al., 2017; Reddy et al., 2018; Raghu et al., 2019; Wu et al., 2019) and the return can be a fuzzy summation of the entity representations. Madotto et al. (2018)’s practice of modeling the KB query with memory network can also be considered as learning an attentive preferHuman Evaluation We provide human evaluation on our framework and the compared models. These responses are based on distinct dialogue history. We hire several human experts and ask them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5. In each judgment, the expert is prese"
D19-1013,N19-1126,0,0.176416,"cessing, efforts have been made to replacing the discrete and predefined dialogue state with the distributed representation (Bordes and Weston, 2017; Wen et al., 2017b,a; Liu and Lane, 2017). In our framework, our retrieval result can be treated as a numeric representation of the API call return. Instead of interacting with the KB via API calls, more and more recent works tried to incorporate KB query as a part of the model. The most popular way of modeling KB query is treating it as an attention network over the entire KB entities (Eric et al., 2017; Dhingra et al., 2017; Reddy et al., 2018; Raghu et al., 2019; Wu et al., 2019) and the return can be a fuzzy summation of the entity representations. Madotto et al. (2018)’s practice of modeling the KB query with memory network can also be considered as learning an attentive preferHuman Evaluation We provide human evaluation on our framework and the compared models. These responses are based on distinct dialogue history. We hire several human experts and ask them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5. In each judgment, the expert is presented with the dialogue history, an outpu"
D19-1013,W17-5506,0,0.0605833,"tion. Valero is located at 200 Alester Ave. OK , please give me directions via a route that avoids all heavy traffic. Since there is a road block nearby, I found another route for you and I sent it on your screen. Awesome thank you. Figure 1: An example of a task-oriented dialogue that incorporates a knowledge base (KB). The fourth row in KB supports the second turn of the dialogue. A dialogue system will produce a response with conflict entities if it includes the POI in the fourth row and the address in the fifth row, like “Valero is located at 899 Ames Ct”. history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018). This kind of modeling scheme frees the task-oriented dialogue system from the manually designed pipeline modules and heavy annotation labor for these modules. Different from typical text generation, the successful conversations for task-oriented dialogue system heavily depend on accurate knowledge base (KB) queries. Taking the dialogue in Figure 1 as an example, to answer the driver’s query on the gas station, the dialogue system is required to retrieve the entities like “200 Alester Ave” and “Valero”. For the task-oriented system based on Seq2Seq generation, there is"
D19-1013,C18-1320,1,0.746102,"or task-oriented dialogue system heavily depend on accurate knowledge base (KB) queries. Taking the dialogue in Figure 1 as an example, to answer the driver’s query on the gas station, the dialogue system is required to retrieve the entities like “200 Alester Ave” and “Valero”. For the task-oriented system based on Seq2Seq generation, there is a trend in recent study towards modeling the KB query as an attention network over the entire KB entity representations, hoping to learn a model to pay more attention to the relevant entities (Eric et al., 2017; Madotto et al., 2018; Reddy et al., 2018; Wen et al., 2018). Introduction Task-oriented dialogue system, which helps users to achieve specific goals with natural language, is attracting more and more research attention. With the success of the sequence-to-sequence (Seq2Seq) models in text generation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Nallapati et al., 2016b,a), several works tried to model the task-oriented dialogue as the Seq2Seq generation of response from the dialogue Though achieving good end-to-end dialogue generation with over-the-entire-KB attention mechanism, these methods do not guarantee the generation consi"
D19-1013,E17-1042,0,0.107666,"Missing"
D19-1013,P13-2117,0,0.0555894,"Missing"
D19-1013,D15-1203,0,0.0161679,"t = U [ h where ot ’s dimensionality is |V |+|E|. In vt , lower |V |is zero and the rest|E |is retrieved entity scores. 4 Training the KB-Retriever As mentioned in section 3.3.1, we adopt the memory network to train our KB-retriever. However, in the Seq2Seq dialogue generation, the training data does not include the annotated KB row retrieval results, which makes supervised training the KBretriever impossible. To tackle this problem, we propose two training methods for our KB-rowretriever. 1) In the first method, inspired by the recent success of distant supervision in information extraction (Zeng et al., 2015; Mintz et al., 2009; Min et al., 2013; Xu et al., 2013), we take advantage of the similarity between the surface string of KB entries and the reference response, and design a set of heuristics to extract training data for the KB-retriever. 2) In the second method, instead of training the KB-retriever as an independent component, we train it along with the training of the Seq2Seq dialogue generation. To make the retrieval process in Equation 1 differentiable, we use Gumbel-Softmax (Jang et al., 2017) as an approximation of the argmax during training. 4.1 4.2 Training with Gumbel-Softmax In add"
D19-1169,P18-1163,0,0.0509824,"Missing"
D19-1169,P17-1055,1,0.877462,"erimental results on two public Chinese reading comprehension datasets show that the proposed cross-lingual approaches yield significant improvements over various baseline systems and set new state-of-the-art performances. 2 Related Works Machine Reading Comprehension (MRC) has been a trending research topic in recent years. Among various types of MRC tasks, spanextraction reading comprehension has been enormously popular (such as SQuAD (Rajpurkar et al., 2016)), and we have seen a great progress on related neural network approaches (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Cui et al., 2017; Hu et al., 2019), especially those were built on pre-trained language models, such as BERT (Devlin et al., 2019). While massive achievements have been made by the community, reading comprehension in other than English has not been well-studied mainly due to the lack of large-scale training data. Asai et al. (2018) proposed to use runtime machine translation for multilingual extractive reading comprehension. They first translate the data from the target language to English and then obtain an answer using an English reading comprehension model. Finally, they recover the corresponding answer in"
D19-1169,D19-1600,1,0.855968,"when there is no or partially available resources in the target language. Then we propose a novel model called Dual BERT to further improve the system performance when there is training data available in the target language. We first translate target language training data into English to form pseudo bilingual parallel data. Then we use multilingual BERT (Devlin et al., 2019) to simultaneously model the &lt;passage, question, answer&gt; in both languages, and fuse the representations of both to generate final predictions. Experimental results on two Chinese reading comprehension dataset CMRC 2018 (Cui et al., 2019) and DRCD (Shao et al., 2018) show that by utilizing English resources could substantially improve system performance and the proposed Dual BERT achieves state-of-the-art performances on both datasets, and even surpass human performance on some metrics. Also, we 1586 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1586–1595, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics conduct experiments on the Japanese and French SQuAD (Asai et al.,"
D19-1169,N19-1423,0,0.681076,"tween &lt;passage, question, answer&gt;. In this paper, we propose the Cross-Lingual Machine Reading Comprehension (CLMRC) task that aims to help reading comprehension in lowresource languages. First, we present several back-translation approaches when there is no or partially available resources in the target language. Then we propose a novel model called Dual BERT to further improve the system performance when there is training data available in the target language. We first translate target language training data into English to form pseudo bilingual parallel data. Then we use multilingual BERT (Devlin et al., 2019) to simultaneously model the &lt;passage, question, answer&gt; in both languages, and fuse the representations of both to generate final predictions. Experimental results on two Chinese reading comprehension dataset CMRC 2018 (Cui et al., 2019) and DRCD (Shao et al., 2018) show that by utilizing English resources could substantially improve system performance and the proposed Dual BERT achieves state-of-the-art performances on both datasets, and even surpass human performance on some metrics. Also, we 1586 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the"
D19-1169,P17-1168,0,0.0185237,"emonstrate the potentials in CLMRC task. 1 1 Introduction Machine Reading Comprehension (MRC) has been a popular task to test the reading ability of the machine, which requires to read text material and answer the questions based on it. Starting from cloze-style reading comprehension, various neural network approaches have been proposed and massive progresses have been made in creating largescale datasets and neural models (Hermann et al., 2015; Hill et al., 2015; Kadlec et al., 2016; Cui 1 Resources available: https://github.com/ ymcui/Cross-Lingual-MRC. et al., 2017; Rajpurkar et al., 2016; Dhingra et al., 2017). Though various types of contributions had been made, most works are dealing with English reading comprehension. Reading comprehension in other than English has not been well-addressed mainly due to the lack of large-scale training data. To enrich the training data, there are two traditional approaches. Firstly, we can annotate data by human experts, which is ideal and high-quality, while it is time-consuming and rather expensive. One can also obtain large-scale automatically generated data (Hermann et al., 2015; Hill et al., 2015; Liu et al., 2017), but the quality is far beyond the usable t"
D19-1169,P16-1086,0,0.0304847,"and DRCD. The results show consistent and significant improvements over various stateof-the-art systems by a large margin, which demonstrate the potentials in CLMRC task. 1 1 Introduction Machine Reading Comprehension (MRC) has been a popular task to test the reading ability of the machine, which requires to read text material and answer the questions based on it. Starting from cloze-style reading comprehension, various neural network approaches have been proposed and massive progresses have been made in creating largescale datasets and neural models (Hermann et al., 2015; Hill et al., 2015; Kadlec et al., 2016; Cui 1 Resources available: https://github.com/ ymcui/Cross-Lingual-MRC. et al., 2017; Rajpurkar et al., 2016; Dhingra et al., 2017). Though various types of contributions had been made, most works are dealing with English reading comprehension. Reading comprehension in other than English has not been well-addressed mainly due to the lack of large-scale training data. To enrich the training data, there are two traditional approaches. Firstly, we can annotate data by human experts, which is ideal and high-quality, while it is time-consuming and rather expensive. One can also obtain large-scale"
D19-1169,P17-1010,1,0.839775,". et al., 2017; Rajpurkar et al., 2016; Dhingra et al., 2017). Though various types of contributions had been made, most works are dealing with English reading comprehension. Reading comprehension in other than English has not been well-addressed mainly due to the lack of large-scale training data. To enrich the training data, there are two traditional approaches. Firstly, we can annotate data by human experts, which is ideal and high-quality, while it is time-consuming and rather expensive. One can also obtain large-scale automatically generated data (Hermann et al., 2015; Hill et al., 2015; Liu et al., 2017), but the quality is far beyond the usable threshold. Another way is to exploit cross-lingual approaches to utilize the data in richresource language to implicitly learn the relations between &lt;passage, question, answer&gt;. In this paper, we propose the Cross-Lingual Machine Reading Comprehension (CLMRC) task that aims to help reading comprehension in lowresource languages. First, we present several back-translation approaches when there is no or partially available resources in the target language. Then we propose a novel model called Dual BERT to further improve the system performance when ther"
D19-1169,D16-1264,0,0.760855,"a large margin, which demonstrate the potentials in CLMRC task. 1 1 Introduction Machine Reading Comprehension (MRC) has been a popular task to test the reading ability of the machine, which requires to read text material and answer the questions based on it. Starting from cloze-style reading comprehension, various neural network approaches have been proposed and massive progresses have been made in creating largescale datasets and neural models (Hermann et al., 2015; Hill et al., 2015; Kadlec et al., 2016; Cui 1 Resources available: https://github.com/ ymcui/Cross-Lingual-MRC. et al., 2017; Rajpurkar et al., 2016; Dhingra et al., 2017). Though various types of contributions had been made, most works are dealing with English reading comprehension. Reading comprehension in other than English has not been well-addressed mainly due to the lack of large-scale training data. To enrich the training data, there are two traditional approaches. Firstly, we can annotate data by human experts, which is ideal and high-quality, while it is time-consuming and rather expensive. One can also obtain large-scale automatically generated data (Hermann et al., 2015; Hill et al., 2015; Liu et al., 2017), but the quality is"
D19-1169,L18-1431,1,0.946224,"ly available resources in the target language. Then we propose a novel model called Dual BERT to further improve the system performance when there is training data available in the target language. We first translate target language training data into English to form pseudo bilingual parallel data. Then we use multilingual BERT (Devlin et al., 2019) to simultaneously model the &lt;passage, question, answer&gt; in both languages, and fuse the representations of both to generate final predictions. Experimental results on two Chinese reading comprehension dataset CMRC 2018 (Cui et al., 2019) and DRCD (Shao et al., 2018) show that by utilizing English resources could substantially improve system performance and the proposed Dual BERT achieves state-of-the-art performances on both datasets, and even surpass human performance on some metrics. Also, we 1586 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1586–1595, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics conduct experiments on the Japanese and French SQuAD (Asai et al., 2018) and achieves substanti"
D19-1169,1983.tc-1.13,0,0.231539,"Missing"
D19-1214,P19-1544,0,0.389705,"Missing"
D19-1214,N18-2118,0,0.12366,"ge understanding (SLU) is a critical component in task-oriented dialogue systems. It usually consists of intent detection to identify users’ intents and slot filling task to extract semantic constituents from the natural language utterances (Tur and De Mori, 2011). As shown in Table 1, given a movie-related utterance “watch action movie”, there are different slot labels for each token and an intent for the whole utterance. Usually, intent detection and slot filling are implemented separately. But intuitively, these two tasks are not independent and the slots often highly depend on the intent (Goo et al., 2018). For example, if the intent of a utterance is * Email corresponding. action movie B-movie name I-movie name WatchMovie Table 1: An example with intent and slot annotation (BIO format), which indicates the slot of movie name from an utterance with an intent WatchMovie. Introduction ∗ watch O WatchMovie, it is more likely to contain the slot movie name rather than the slot music name. Hence, it is promising to incorporate the intent information to guide the slot filling. Considering this strong correlation between the two tasks, some joint models are proposed based on the multi-task learning fr"
D19-1214,H90-1021,0,0.583959,"ointly, the final joint objective is formulated as Lθ = L1 + L2 . (12) Through the joint loss function, the shared representations learned by the shared self-attentive encoder can consider two tasks jointly and further ease the error propagation compared with pipeline models (Zhang and Wang, 2016). 2081 4 Experiments 4.1 • Bi-Model. Wang et al. (2018) proposed the Bi-model to consider the intent and slot filling cross-impact to each other. Experimental Settings To evaluate the efficiency of our proposed model, we conduct experiments on two benchmark datasets. One is the publicly ATIS dataset (Hemphill et al., 1990) containing audio recordings of flight reservations, and the other is the custom-intent-engines collected by Snips (SNIPS dataset) (Coucke et al., 2018). 1 Both datasets used in our paper follows the same format and partition as in Goo et al. (2018). The dimensionalities of the word embedding is 256 for ATIS dataset and 512 for SNIPS dataset. The self-attentive encoder hidden units are set as 256. L2 regularization is used on our model is 1 × 10−6 and dropout ratio is adopted is 0.4 for reducing overfit. We use Adam (Kingma and Ba, 2014) to optimize the parameters in our model and adopted the"
D19-1214,N18-2050,0,0.136863,"lot filling task objection is defined as: nS m X   X ˆ ji,S log yji,S , L2 , − y (11) j=1 i=1 ˆ ji,I y ˆ ji,S are the gold intent label and where and y gold slot label separately; nS is the number of slot labels. To obtain both slot filling and intent detection jointly, the final joint objective is formulated as Lθ = L1 + L2 . (12) Through the joint loss function, the shared representations learned by the shared self-attentive encoder can consider two tasks jointly and further ease the error propagation compared with pipeline models (Zhang and Wang, 2016). 2081 4 Experiments 4.1 • Bi-Model. Wang et al. (2018) proposed the Bi-model to consider the intent and slot filling cross-impact to each other. Experimental Settings To evaluate the efficiency of our proposed model, we conduct experiments on two benchmark datasets. One is the publicly ATIS dataset (Hemphill et al., 1990) containing audio recordings of flight reservations, and the other is the custom-intent-engines collected by Snips (SNIPS dataset) (Coucke et al., 2018). 1 Both datasets used in our paper follows the same format and partition as in Goo et al. (2018). The dimensionalities of the word embedding is 256 for ATIS dataset and 512 for S"
D19-1214,P18-1053,1,0.836806,"e matrix of input vectors X ∈ RT ×d (d represents the mapped dimension) to queries (Q), keys (K) and values (V) matrices by using different linear projections and the self-attention output C ∈ RT ×d is a weighted sum of values:   QK> √ C = softmax V. (1) dk Self-Attentive Encoder In our Stack-Propagation framework, intent detection task and slot filling task share one encoder, In the self-attentive encoder, we use BiLSTM with self-attention mechanism to leverage both advantages of temporal features and contextual information, which are useful for sequence labeling tasks (Zhong et al., 2018; Yin et al., 2018). The BiLSTM (Hochreiter and Schmidhuber, 1997) reads input utterance X = (x1 , x2 , .., xT ) (T is the number of tokens in the input utterance) (2) where E ∈ RT ×2d and ⊕ is concatenation operation. 3.2 Token-Level Intent Detection Decoder In our framework, we perform a token-level intent detection, which can provide token-level intent features for slot filling, different from regarding the intent detection task as the sentence-level classification problem (Liu and Lane, 2016). The token-level intent detection method can be formalized as a sequence labeling problem that maps a input word sequ"
D19-1214,P19-1519,0,0.125076,"twork (RNN) architecture. • Attention BiRNN. Liu and Lane (2016) leveraged the attention mechanism to allow the network to learn the relationship between slot and intent. • Slot-Gated Atten. Goo et al. (2018) proposed the slot-gated joint model to explore the correlation of slot filling and intent detection better. • Self-Attentive Model. Li et al. (2018) proposed a novel self-attentive model with the intent augmented gate mechanism to utilize the semantic correlation between slot and intent. 1 https://github.com/snipsco/ nlu-benchmark/tree/master/ 2017-06-custom-intent-engines • CAPSULE-NLU. Zhang et al. (2019) proposed a capsule-based neural network model with a dynamic routing-by-agreement schema to accomplish slot filling and intent detection. • SF-ID Network. (E et al., 2019) introduced an SF-ID network to establish direct connections for the slot filling and intent detection to help them promote each other mutually. For the Joint Seq, Attention BiRNN, Slot-gated Atten, CAPSULE-NLU and SF-ID Network, we adopt the reported results from Goo et al. (2018); Zhang et al. (2019); E et al. (2019). For the SelfAttentive Model, Bi-Model, we re-implemented the models and obtained the results on the same d"
D19-1214,P16-1147,0,0.0376135,"Joint Conference on Natural Language Processing, pages 2078–2087, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics intent semantic knowledge to guide the slot filling and make our joint model more interpretable. Task B Task A Task B differentiable link Task A Encoder (a) Multi-task framework • We perform the token-level intent detection for Stack-Propagation framework, which improves the intent detection performance and further alleviate the error propagation. Encoder (b) Stack-propagation Figure 1: Multi-task framework vs. Stack-Propagation. proposed by Zhang and Weiss (2016) to leverage the POS tagging features for parsing and achieved good performance, we propose a joint model with Stack-Propagation for SLU tasks. Our framework directly use the output of the intent detection as the input for slot filling to better guide the slot prediction process. In addition, the framework make it easy to design oracle intent experiment to intuitively show how intent information enhances slot filling task. For the second issue, we perform a token-level intent prediction in our framework, which can provide the token-level intent information for slot filling. If some token-level"
D19-1214,P18-1135,0,0.0468615,"l intent information for slot filling by concatenating the output of intent detection decoder and the representations from encoder as the input for slot filling decoder. Both intent detection and slot filling are optimized simultaneously via a joint learning scheme. 3.1 forwardly and backwardly to produce contextsensitive hidden states H = (h1 , h2 , ..., hT ) by repeatedly applying the recurrence hi = BiLSTM φemb (xi ) , hi−1 . Self-attention is a very effective method of leveraging context-aware features over variablelength sequences for natural language processing tasks (Tan et al., 2018; Zhong et al., 2018). In our case, we use self-attention mechanism to capture the contextual information for each token. In this paper, we adopt Vaswani et al. (2017), where we first map the matrix of input vectors X ∈ RT ×d (d represents the mapped dimension) to queries (Q), keys (K) and values (V) matrices by using different linear projections and the self-attention output C ∈ RT ×d is a weighted sum of values:   QK> √ C = softmax V. (1) dk Self-Attentive Encoder In our Stack-Propagation framework, intent detection task and slot filling task share one encoder, In the self-attentive encoder, we use BiLSTM with"
D19-1575,D18-1214,0,0.0435914,"Missing"
D19-1575,Q17-1010,0,0.0684081,"he transformed contextualized embeddings. We train our models using the Adam optimizer (Kingma and Ba, 2015), and most of the them converge within a few thousand epochs in several hours. More implementation details are reported in the Appendix. 4.2 Baseline Systems We compare our method with the following three baseline models: • mBERT (contextualized). Embeddings generated by the mBERT model are directly used in the training and testing procedures. • FT-SVD (Ahmad et al., 2018, off-line, static). SVD-based transformation (Smith et al., 2017) is applied on 300-dimensional FastText embeddings (Bojanowski et al., 2017) to obtain cross-lingual static embeddings, which represents the previous state-ofthe-art. We report results from their paper of the RNNGraph model which used the same architecture as ours. • XLM (Lample and Conneau, 2019, on-line, contextualized). A strong method which learns contextualized cross-lingual embeddings from scratch with cross-lingual data. 6 We also investigated non-linear transformation in our experiments, but didn’t observe any improvements. 7 We found the orthogonal constraint doesn’t help for GD. 8 We tried alternative strategies such as averaging, using the middle or right-m"
D19-1575,K18-2005,1,0.806607,"ethod, where contextualized embeddings of the word canal from Spanish is transformed to the semantic space of English. Introduction Email corresponding Our code is released at https://github.com/ WangYuxuan93/CLBT 2 In this paper, we refer to these embeddings as static as opposed to contextualized ones. 1 He loves the movie Devlin et al., 2018) have demonstrated dramatic superiority over traditional static word embeddings, establishing new state-of-the-arts in various monolingual NLP tasks (Ili´c et al., 2018; Schuster et al., 2018). The success has also been recognized in dependency parsing (Che et al., 2018). The great potential of these contextualized embeddings has inspired us to extend its power to crosslingual scenarios. Several recent works have been proposed to learn contextualized cross-lingual embeddings by training cross-lingual language models from scratch with parallel data as supervision, and has been demonstrated effective in several downstream tasks (Schuster et al., 2018; Mulcaire et al., 2019; Lample and Conneau, 2019). These methods are typically resource-demanding and timeconsuming.3 In this paper, we propose CrossLingual BERT Transformation (CLBT), a simple and efficient off-li"
D19-1575,D18-1024,0,0.0180757,"rces and less training time than XLM, yet achieving highly competitive results. 2 Related Work Static cross-lingual embedding learning methods can be roughly categorized as on-line and off-line methods. Typically, on-line approaches integrate monolingual and cross-lingual objectives to learn cross-lingual word embeddings in a joint manner (Klementiev et al., 2012; Koˇcisk´y et al., 2014; Guo et al., 2016), while off-line approaches take pretrained monolingual word embeddings of different languages as input and retrofit them into a shared semantic space (Xing et al., 2015; Lample et al., 2018; Chen and Cardie, 2018). Several approaches have been proposed recently to connect the rich expressiveness of contextualized word embeddings with cross-lingual transfer. Mulcaire et al. (2019) based their model on ELMo (Peters et al., 2018) and proposed a polyglot contextual representation model by capturing character-level information from multilingual data. Lample and Conneau (2019) adapted the objectives of BERT (Devlin et al., 2018) to incorporate cross-lingual supervision from parallel data to learn cross-lingual language models (XLMs), which have obtained state-of-the-art results on several cross-lingual tasks"
D19-1575,P10-4002,0,0.026285,"cally converges in several hours. To validate the effectiveness of our approach in cross-lingual dependency parsing, we first obtain the CLBT embeddings with the proposed approach, and then use them as input to a modern graph-based neural parser (described in next section), in replacement of the pre-trained static embeddings. Note that BERT produces embeddings in wordpiece-level, so we only use the left-most wordpiece embedding of each word.8 4 4.1 Data and Settings In our experiments, the contextual word pairs are obtained from the Europarl corpora (Koehn, 2005) using the fast align toolkit (Dyer et al., 2010). Only 10,000 sentence pairs are used for each target language. For the parsing datasets, we use the Universal Dependencies(UD) Treebanks (v2.2) (Nivre et al., 2018),9 following the settings of the previous state-of-the-art system (Ahmad et al., 2018). From the 31 languages they have analyzed, we select 18 whose Europarl data is publicly available.10 Statistics of the selected languages and treebanks can be found in the Appendix. We employ the Biaffine Graph-based Parser of Dozat and Manning (2017) and adopt their hyper-parameters for all of our models. In all the experiments, English is used"
D19-1575,S01-1001,0,0.0248893,"d by XLM ranges from 0.2 million (10 million tokens) for Bulgarian to 13.1 million (682 million tokens) for French. In comparison, only 10,000 parallel sentences (0.4 million tokens) are used for each language in CLBT, demonstrating the data-efficiency of our approach. Moreover, given the efficiency in both data and training, CLBT can be readily scaled to new language pairs in hours. 4.5 Analysis 4.5.1 Transformation of Cross-lingual BERT Embedding In order to investigate the properties of contextualized representations before and after the linear transformation, we employ the SENSEVAL2 data (Edmonds and Cotton, 2001),13 where words from different languages are tagged by their word senses in different contexts. We took contextualized representations of the English word nature and its Spanish transla5724 13 www.hipposmond.com/senseval2/ 80 La corrupción , un fenómeno de naturaleza esencialmente política , se ha convertido también en. (Corruption, a phenomenon of an essentially political nature, has also become a spectacle.) 75 70 65 WWF is respected worldwide for its knowledge of wildlife and nature conservation. 60 55 50 Perhaps it's due to the hightech nature of today's shoes, particularly the midsole. 45"
D19-1575,P15-1119,1,0.945046,"ot cross-lingual transfer parsing. Experiments show that our embeddings substantially outperform the previous state-of-the-art that uses static embeddings. We further compare our approach with XLM (Lample and Conneau, 2019), a recently proposed cross-lingual language model trained with massive parallel data, and achieve highly competitive results. 1 1 El canal está marcado por boyas (The channel is marked by buoys) One of the most promising directions for crosslingual dependency parsing, which also remains a challenge, is to bridge the gap of lexical features. Prior works (Xiao and Guo, 2014; Guo et al., 2015) have shown that cross-lingual word embeddings are able to significantly improve the transfer performance compared to delexicalized models (McDonald et al., 2011, 2013). These crosslingual word embeddings are static in the sense that they do not change with the context.2 Recently, contextualized word embeddings derived from large-scale pre-trained language models (McCann et al., 2017; Peters et al., 2017, 2018; ∗ channel WX Y Y The ship went aground in the channel Figure 1: A toy illustration of the method, where contextualized embeddings of the word canal from Spanish is transformed to the se"
D19-1575,W18-6202,0,0.0551155,"Missing"
D19-1575,C12-1089,0,0.0908096,"Missing"
D19-1575,P14-2037,0,0.0417422,"Missing"
D19-1575,2005.mtsummit-papers.11,0,0.0286546,"ach can be trained on a single GPU and typically converges in several hours. To validate the effectiveness of our approach in cross-lingual dependency parsing, we first obtain the CLBT embeddings with the proposed approach, and then use them as input to a modern graph-based neural parser (described in next section), in replacement of the pre-trained static embeddings. Note that BERT produces embeddings in wordpiece-level, so we only use the left-most wordpiece embedding of each word.8 4 4.1 Data and Settings In our experiments, the contextual word pairs are obtained from the Europarl corpora (Koehn, 2005) using the fast align toolkit (Dyer et al., 2010). Only 10,000 sentence pairs are used for each target language. For the parsing datasets, we use the Universal Dependencies(UD) Treebanks (v2.2) (Nivre et al., 2018),9 following the settings of the previous state-of-the-art system (Ahmad et al., 2018). From the 31 languages they have analyzed, we select 18 whose Europarl data is publicly available.10 Statistics of the selected languages and treebanks can be found in the Appendix. We employ the Biaffine Graph-based Parser of Dozat and Manning (2017) and adopt their hyper-parameters for all of our"
D19-1575,P15-1027,0,0.0160464,"ings as an anchor for each word type, and learn a transformation in the anchor space. Our approach, however, learns this transformation directly in the contextual space, and hence is explicitly designed to be word sense-preserving. 3 Cross-Lingual BERT Transformation This section describes our proposed approach, namely CLBT, to transform pre-trained monolingual contextualized embeddings to a shared semantic space. 3.1 Contextual Word Alignment Traditional methods of learning static crosslingual word embeddings have been relying on various sources of supervision such as bilingual dictionaries (Lazaridou et al., 2015; Smith et al., 2017), parallel corpus (Guo et al., 2015) or online Google Translate (Mikolov et al., 2013; Xing et al., 2015). To learn contextualized cross-lingual word embeddings, however, we require supervision at word token-level (or context-level) rather than type-level (i.e. dictionaries). Therefore, we assume a parallel corpus as our supervision, analogous to on-line methods such as XLM (Lample and Conneau, 2019). In our approach, unsupervised bidirectional word alignment is first applied to the parallel corpus to obtain a set of aligned word pairs with their contexts, or contextual wo"
D19-1575,D11-1006,0,0.114886,"e further compare our approach with XLM (Lample and Conneau, 2019), a recently proposed cross-lingual language model trained with massive parallel data, and achieve highly competitive results. 1 1 El canal está marcado por boyas (The channel is marked by buoys) One of the most promising directions for crosslingual dependency parsing, which also remains a challenge, is to bridge the gap of lexical features. Prior works (Xiao and Guo, 2014; Guo et al., 2015) have shown that cross-lingual word embeddings are able to significantly improve the transfer performance compared to delexicalized models (McDonald et al., 2011, 2013). These crosslingual word embeddings are static in the sense that they do not change with the context.2 Recently, contextualized word embeddings derived from large-scale pre-trained language models (McCann et al., 2017; Peters et al., 2017, 2018; ∗ channel WX Y Y The ship went aground in the channel Figure 1: A toy illustration of the method, where contextualized embeddings of the word canal from Spanish is transformed to the semantic space of English. Introduction Email corresponding Our code is released at https://github.com/ WangYuxuan93/CLBT 2 In this paper, we refer to these embedd"
D19-1575,N19-1392,0,0.106687,"tatic word embeddings, establishing new state-of-the-arts in various monolingual NLP tasks (Ili´c et al., 2018; Schuster et al., 2018). The success has also been recognized in dependency parsing (Che et al., 2018). The great potential of these contextualized embeddings has inspired us to extend its power to crosslingual scenarios. Several recent works have been proposed to learn contextualized cross-lingual embeddings by training cross-lingual language models from scratch with parallel data as supervision, and has been demonstrated effective in several downstream tasks (Schuster et al., 2018; Mulcaire et al., 2019; Lample and Conneau, 2019). These methods are typically resource-demanding and timeconsuming.3 In this paper, we propose CrossLingual BERT Transformation (CLBT), a simple and efficient off-line approach that learns a linear transformation from contextual word alignments. With CLBT, contextualized embeddings 3 For instance, XLM was trained on 64 Volta GPUs (Lample and Conneau, 2019). While the time of training is not described in the paper, we may take the statistics from BERT as a reference, e.g., BERTBASE was trained on 4 Cloud TPUs for 4 days (Devlin et al., 2018). 5721 Proceedings of the 2"
D19-1575,P17-1161,0,0.032285,"ked by buoys) One of the most promising directions for crosslingual dependency parsing, which also remains a challenge, is to bridge the gap of lexical features. Prior works (Xiao and Guo, 2014; Guo et al., 2015) have shown that cross-lingual word embeddings are able to significantly improve the transfer performance compared to delexicalized models (McDonald et al., 2011, 2013). These crosslingual word embeddings are static in the sense that they do not change with the context.2 Recently, contextualized word embeddings derived from large-scale pre-trained language models (McCann et al., 2017; Peters et al., 2017, 2018; ∗ channel WX Y Y The ship went aground in the channel Figure 1: A toy illustration of the method, where contextualized embeddings of the word canal from Spanish is transformed to the semantic space of English. Introduction Email corresponding Our code is released at https://github.com/ WangYuxuan93/CLBT 2 In this paper, we refer to these embeddings as static as opposed to contextualized ones. 1 He loves the movie Devlin et al., 2018) have demonstrated dramatic superiority over traditional static word embeddings, establishing new state-of-the-arts in various monolingual NLP tasks (Ili´c"
D19-1575,N18-1202,0,0.0600481,"ne approaches integrate monolingual and cross-lingual objectives to learn cross-lingual word embeddings in a joint manner (Klementiev et al., 2012; Koˇcisk´y et al., 2014; Guo et al., 2016), while off-line approaches take pretrained monolingual word embeddings of different languages as input and retrofit them into a shared semantic space (Xing et al., 2015; Lample et al., 2018; Chen and Cardie, 2018). Several approaches have been proposed recently to connect the rich expressiveness of contextualized word embeddings with cross-lingual transfer. Mulcaire et al. (2019) based their model on ELMo (Peters et al., 2018) and proposed a polyglot contextual representation model by capturing character-level information from multilingual data. Lample and Conneau (2019) adapted the objectives of BERT (Devlin et al., 2018) to incorporate cross-lingual supervision from parallel data to learn cross-lingual language models (XLMs), which have obtained state-of-the-art results on several cross-lingual tasks. Similar to our approach, Schuster et al. (2019) also aligned pretrained contextualized word embeddings through linear transformation in an off-line fashion. They used the averaged contextualized embeddings as an anc"
D19-1575,P19-1493,0,0.078096,"Missing"
D19-1575,N19-1162,0,0.0486122,"een proposed recently to connect the rich expressiveness of contextualized word embeddings with cross-lingual transfer. Mulcaire et al. (2019) based their model on ELMo (Peters et al., 2018) and proposed a polyglot contextual representation model by capturing character-level information from multilingual data. Lample and Conneau (2019) adapted the objectives of BERT (Devlin et al., 2018) to incorporate cross-lingual supervision from parallel data to learn cross-lingual language models (XLMs), which have obtained state-of-the-art results on several cross-lingual tasks. Similar to our approach, Schuster et al. (2019) also aligned pretrained contextualized word embeddings through linear transformation in an off-line fashion. They used the averaged contextualized embeddings as an anchor for each word type, and learn a transformation in the anchor space. Our approach, however, learns this transformation directly in the contextual space, and hence is explicitly designed to be word sense-preserving. 3 Cross-Lingual BERT Transformation This section describes our proposed approach, namely CLBT, to transform pre-trained monolingual contextualized embeddings to a shared semantic space. 3.1 Contextual Word Alignmen"
D19-1575,D19-1077,0,0.0837086,"Missing"
D19-1575,W14-1613,0,0.0249298,"approach on zero-shot cross-lingual transfer parsing. Experiments show that our embeddings substantially outperform the previous state-of-the-art that uses static embeddings. We further compare our approach with XLM (Lample and Conneau, 2019), a recently proposed cross-lingual language model trained with massive parallel data, and achieve highly competitive results. 1 1 El canal está marcado por boyas (The channel is marked by buoys) One of the most promising directions for crosslingual dependency parsing, which also remains a challenge, is to bridge the gap of lexical features. Prior works (Xiao and Guo, 2014; Guo et al., 2015) have shown that cross-lingual word embeddings are able to significantly improve the transfer performance compared to delexicalized models (McDonald et al., 2011, 2013). These crosslingual word embeddings are static in the sense that they do not change with the context.2 Recently, contextualized word embeddings derived from large-scale pre-trained language models (McCann et al., 2017; Peters et al., 2017, 2018; ∗ channel WX Y Y The ship went aground in the channel Figure 1: A toy illustration of the method, where contextualized embeddings of the word canal from Spanish is tr"
D19-1575,N15-1104,0,0.0748489,"Missing"
D19-1600,C10-3004,1,0.797425,"Missing"
D19-1600,P17-1055,1,0.870124,"uch as cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (2016) proposed a Chinese cloze-style reading comprehension dataset: People’s Daily & Children’s Fairy Tale. To add difficulties to the dataset, along with the automatically generated evaluation sets (development and test), they also release a human-annotated evaluation set. Later, Cui et"
D19-1600,L18-1431,1,0.831224,", 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (2016) proposed a Chinese cloze-style reading comprehension dataset: People’s Daily & Children’s Fairy Tale. To add difficulties to the dataset, along with the automatically generated evaluation sets (development and test), they also release a human-annotated evaluation set. Later, Cui et al. (2018) propose another dataset, which is gathered from children’s reading material. To add more diversity and for further investigation on transfer learning, they also provide another evaluation dataset, which is also annotated by human experts, but the query is more natural than the cloze type. The dataset was used in the first evaluation workshop on Chinese machine reading comprehension (CMRC 2017). In opendomain reading comprehension, He et al. (2017) propose a large-scale open-domain Chinese machine reading comprehension dataset (DuReader), which contains 200k queries annotated from the user que"
D19-1600,C16-1167,1,0.900263,"sted the Second Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC 2018). We hope the release of the dataset could further accelerate the Chinese machine reading comprehension research.1 1 Introduction To read and comprehend natural languages is the key to achieve advanced artificial intelligence. Machine Reading Comprehension (MRC) aims to comprehend the context of given articles and answer the questions based on them. Various types of machine reading comprehension datasets have been proposed, such as cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017;"
D19-1600,N19-1423,0,0.0460362,"f three results as the final estimated human performance on this dataset. Estimated Human Performance We also report the estimated human performance in order to measure the difficulty of the proposed dataset. As we have illustrated in the previous section, there are three answers for each question in development, test, and challenge set. Unlike Rajpurkar et al. (2016), we use a cross-validation method to calculate the performance. We regard the first answer as human prediction and treat the rest of the answers as ground truths. In this way, 4 4.1 Experimental Results Baseline System Following Devlin et al. (2019), we adopt BERT for our baseline system. Specifically, we slightly modify the run squad.py script5 for adjusting our dataset, while keeping the most of the original implementation. For the baseline system, we used an initial learning rate of 3e-5 with a batch size of 32 and trained for two epochs. The maximum lengths of document and query are set to 512 and 64. 4.2 Results The results are shown in Table 2. Besides the baseline systems, we also include the participants’ results of CMRC 2018 evaluation. We release the training and development set to the public and accepted submissions from parti"
D19-1600,P17-1168,0,0.0420642,"reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (2016) proposed a Chinese cloze-style reading comprehension dataset: People’s Daily & Children’s Fairy Tale. To add difficulties to the dataset, along with the automatically generated evaluation sets (development and test), they also release a human-annotated evaluation set. Later, Cui et al. (2018) propose an"
D19-1600,P16-1086,0,0.0452388,"have been proposed, such as cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (2016) proposed a Chinese cloze-style reading comprehension dataset: People’s Daily & Children’s Fairy Tale. To add difficulties to the dataset, along with the automatically generated evaluation sets (development and test), they also release a human-annotated evaluation"
D19-1600,D17-1082,0,0.0609544,"anguages is the key to achieve advanced artificial intelligence. Machine Reading Comprehension (MRC) aims to comprehend the context of given articles and answer the questions based on them. Various types of machine reading comprehension datasets have been proposed, such as cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (2016) proposed a Chine"
D19-1600,P17-1010,1,0.86513,"Cui et al., 2016), span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (2016) proposed a Chinese cloze-style reading comprehension dataset: People’s Daily & Children’s Fairy Tale. To add difficulties to the dataset, along with the automatically generated evaluation sets (development and test), they also release a human-annotated evaluation set. Later, Cui et al. (2018) propose another dataset, which is gathered from children’s reading mat"
D19-1600,D16-1264,0,0.489181,"ding Comprehension (CMRC 2018). We hope the release of the dataset could further accelerate the Chinese machine reading comprehension research.1 1 Introduction To read and comprehend natural languages is the key to achieve advanced artificial intelligence. Machine Reading Comprehension (MRC) aims to comprehend the context of given articles and answer the questions based on them. Various types of machine reading comprehension datasets have been proposed, such as cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al"
D19-1600,D13-1020,0,0.138836,"and comprehend natural languages is the key to achieve advanced artificial intelligence. Machine Reading Comprehension (MRC) aims to comprehend the context of given articles and answer the questions based on them. Various types of machine reading comprehension datasets have been proposed, such as cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (201"
D19-1600,P18-1158,0,0.0248253,"ion (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (2016) proposed a Chinese cloze-style reading comprehension dataset: People’s Daily & Children’s Fairy Tale. To add difficulties to the dataset, along with the automatically generated evaluation sets (development and test), they also release a human-annotated evaluation set. Later, Cui et al. (2018) propose another dataset, which is gathered from children’s reading material. To add more diversity and for further investigat"
D19-1600,P17-1018,0,0.0311655,", span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (2016) proposed a Chinese cloze-style reading comprehension dataset: People’s Daily & Children’s Fairy Tale. To add difficulties to the dataset, along with the automatically generated evaluation sets (development and test), they also release a human-annotated evaluation set. Later, Cui et al. (2018) propose another dataset, which is gathered from children’s reading material. To add more"
E14-1062,P08-1085,0,0.0501682,"Missing"
E14-1062,W06-1615,0,0.066482,"omain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods with labeled source domain data and unlabeled target domain data (Dai et al., 2007; Raina et al., 2007). The baseline self-training approach (Liu and Zhang, 2012) belongs to this category. The second considers the differences in the two domains in terms of features (Blitzer et al., 2006; Daume III, 2007), classifying features into domain-independent source domain and target domain groups and training these types consistently. The third considers differences between the distributions of instances in the two domains, treating them differently (Jiang and Zhai, 2007). Our type-supervised method is closer to the second category. However, rather than splitting features into domain-independent and domain-specific types, we use domain-specific dictionaries to capture domain differences, and train a model on the source domain only. Our method can be treated as an approach specific to"
E14-1062,P07-1094,0,0.0159784,"Missing"
E14-1062,D10-1056,0,0.0427273,"Missing"
E14-1062,P12-1110,0,0.0349042,"Missing"
E14-1062,P04-1015,0,0.0111374,"the partial results from the beam to generate new partial results, using two types of actions: (1) Append, which appends ci to the last (partial) word in a partial result; (2) Separate(p), which makes the last word in the partial result as completed and adds ci as a new partial word with a POS tag p. Partial results in the beam are scored globally over all actions used to build them, so that the Nbest can be put back to the agenda for the next step. For each action, features are extracted differently. We use the features from Zhang and Clark (2010). Discriminative learning with early-update (Collins and Roark, 2004; Zhang and Clark, 2011) is used to train the model with beam-search. 2.2 Baseline Unsupervised Adaptation by Self-Training A simple unsupervised approach for POS-tagging with unlabeled data is EM. For a generative model such as HMM, EM can locally maximize the likelihood of training data. Given a good start, EM can result in a competitive HMM tagging model (Goldberg et al., 2008). For discriminative models with source-domain training examples, an initial model can be trained using the source-domain data, and self-training can be applied to find a locally-optimized model using raw target domai"
E14-1062,P11-1061,0,0.0132514,"from ours in several aspects: (1) they focus on in-domain POS-tagging, while our concern is cross-domain tagging; (2) they study POS-tagging on segmented sentences, while we investigate joint segmentation and POStagging for Chinese; (3) their tag-dictionaries are not tag-dictionaries literally, but statistics of wordtag associations. with Garrette et al. (2013), we also find that the type-supervised method is a competitive choice to token-supervised adaptation. There has been a line of work on using graphbased label propagation to expand tag-lexicons for POS-tagging (Subramanya et al., 2010; Das and Petrov, 2011). Similar methods have been applied to character-level Chinese tagging (Zeng et al., 2013). We found that label propagation from neither the source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods wi"
E14-1062,P09-1058,0,0.0311136,"omain data to find improved target domain accuracies over bare CTB training. are available for Chinese. For example, the Chinese Treebank (CTB) (Xue et al., 2005) contains over 50,000 manually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can give moderate improvements (Liu and Zhang, 2012). We show that accuracies can be much more significantly improved by using targetdomain knowledge in the form of lexicons. Both token-supervised and type-supervised domain adaptation rely on a set of source-domain annotations; while the former makes additional use of a small set of target annotations, the latter leverages a target-domain lexicon. We take a feature-based method, analogous to that of Daume III (200"
E14-1062,P07-1033,0,0.154058,"Missing"
E14-1062,C12-2073,1,0.643353,"ually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can give moderate improvements (Liu and Zhang, 2012). We show that accuracies can be much more significantly improved by using targetdomain knowledge in the form of lexicons. Both token-supervised and type-supervised domain adaptation rely on a set of source-domain annotations; while the former makes additional use of a small set of target annotations, the latter leverages a target-domain lexicon. We take a feature-based method, analogous to that of Daume III (2007), which tunes domain-dependent versions of features using domain-specific data. Our method tunes a set of lexicon-based features, so that domain-dependent models are derived from ins"
E14-1062,I05-3025,0,0.271209,"Missing"
E14-1062,D12-1075,0,0.0619082,"Missing"
E14-1062,N13-1014,0,0.0106972,"ctical situation. In particular, we split Chinese words into domain-independent and domain-specific categories, and define unlexicalized features for domain-specific words. We train lexicalized domain-independent and unlexicalized domainspecific features using the source domain annotated sentences and a source-domain lexicon, and then apply the resulting model to the target domain by replacing the source-domain lexicon with a target domain lexicon. Combined with unsupervised learning with unlabeled target-domain of sentences, the conceptually simple method worked highly effectively. Following Garrette and Baldridge (2013), we address practical questions 590 Action Lexicon Feature templates Separate in-lex(w−1 ), l(w−1 ) ◦ in-lex(w−1 ), in-lex(w−1 , t−1 ), l(w−1 ) ◦ in-lex(w−1 , t−1 ) ing words those that occur more than 3 times for words specific to the source domain. We assume that the domain-independent lexicon applies to all target domains also. For some target domains, we can obtain domain-specific terminologies easily from the Internet. However, this can be a very small portion depending on the domain. Thus, it may still be necessary to obtain new lexicons by manual annotation. Table 1: Dictionary feature"
E14-1062,J94-2001,0,0.0592843,"Missing"
E14-1062,W04-3236,0,0.0174807,"Missing"
E14-1062,P13-1057,0,0.109698,"cate that careful considerations need to be given for effective typesupervision. In addition, significant manual work might be required to ensure the quality of lexicons. To compare type- and token-supervised tagging, Garrette and Baldridge (2013) performed a set of experiments by conducting each type of annotation for two hours. They showed that for lowresource languages, a tag-dictionary can be reasonably effective if label propagation (Talukdar and Crammer, 2009) and model minimizations (Ravi and Knight, 2009) are applied to expand and filter the lexicons. Similar findings were reported in Garrette et al. (2013). Do the above findings carry over to the Chinese language? In this paper, we perform an empirical study on the effects of tag-dictionaries for domain adaptation of Chinese POS-tagging. We aim to answer the following research questions: (a) Is domain adaptation feasible with only a target-domain lexicon? (b) Can we further improve type-supervised domain adaptation using unlabeled target-domain sentences? (c) Is crafting a tag dictionary for domain adaptation more effective than manually annotating target domain sentences, given similar efforts? Our investigations are performed under two Chines"
E14-1062,C04-1081,0,0.596785,"Missing"
E14-1062,P13-1076,0,0.0190385,"cross-domain tagging; (2) they study POS-tagging on segmented sentences, while we investigate joint segmentation and POStagging for Chinese; (3) their tag-dictionaries are not tag-dictionaries literally, but statistics of wordtag associations. with Garrette et al. (2013), we also find that the type-supervised method is a competitive choice to token-supervised adaptation. There has been a line of work on using graphbased label propagation to expand tag-lexicons for POS-tagging (Subramanya et al., 2010; Das and Petrov, 2011). Similar methods have been applied to character-level Chinese tagging (Zeng et al., 2013). We found that label propagation from neither the source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods with labeled source domain data and unlabeled target domain data (Dai et al., 2007; Raina et"
E14-1062,petrov-etal-2012-universal,0,0.0207072,"Missing"
E14-1062,P09-1057,0,0.0155387,"s replaced with a raw tag dictionary gleaned from data, without any human intervention. These facts indicate that careful considerations need to be given for effective typesupervision. In addition, significant manual work might be required to ensure the quality of lexicons. To compare type- and token-supervised tagging, Garrette and Baldridge (2013) performed a set of experiments by conducting each type of annotation for two hours. They showed that for lowresource languages, a tag-dictionary can be reasonably effective if label propagation (Talukdar and Crammer, 2009) and model minimizations (Ravi and Knight, 2009) are applied to expand and filter the lexicons. Similar findings were reported in Garrette et al. (2013). Do the above findings carry over to the Chinese language? In this paper, we perform an empirical study on the effects of tag-dictionaries for domain adaptation of Chinese POS-tagging. We aim to answer the following research questions: (a) Is domain adaptation feasible with only a target-domain lexicon? (b) Can we further improve type-supervised domain adaptation using unlabeled target-domain sentences? (c) Is crafting a tag dictionary for domain adaptation more effective than manually anno"
E14-1062,P08-1101,1,0.450623,"Missing"
E14-1062,D10-1017,0,0.0102399,"ver, their efforts differ from ours in several aspects: (1) they focus on in-domain POS-tagging, while our concern is cross-domain tagging; (2) they study POS-tagging on segmented sentences, while we investigate joint segmentation and POStagging for Chinese; (3) their tag-dictionaries are not tag-dictionaries literally, but statistics of wordtag associations. with Garrette et al. (2013), we also find that the type-supervised method is a competitive choice to token-supervised adaptation. There has been a line of work on using graphbased label propagation to expand tag-lexicons for POS-tagging (Subramanya et al., 2010; Das and Petrov, 2011). Similar methods have been applied to character-level Chinese tagging (Zeng et al., 2013). We found that label propagation from neither the source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervi"
E14-1062,D10-1082,1,0.765811,"ed target domain accuracies over bare CTB training. are available for Chinese. For example, the Chinese Treebank (CTB) (Xue et al., 2005) contains over 50,000 manually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can give moderate improvements (Liu and Zhang, 2012). We show that accuracies can be much more significantly improved by using targetdomain knowledge in the form of lexicons. Both token-supervised and type-supervised domain adaptation rely on a set of source-domain annotations; while the former makes additional use of a small set of target annotations, the latter leverages a target-domain lexicon. We take a feature-based method, analogous to that of Daume III (2007), which tunes domain-d"
E14-1062,P11-1139,0,0.023051,"fy CTB words into domainindependent and domain-specific categories. Consisting of semantic information for nearly 100,000 common Chinese words, HowNet can serve as a resource of domain-independent Chinese words. We choose out of all words in the source domain training data those that also occur in HowNet for domain-independent words, and out of the remain4 4.1 Experiments Setting We use annotated sentences from the CTB5 for source-domain training, splitting the corpus into training, development and test sections in the same way as previous work (Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011). Following Liu and Zhang (2012), we use the free Internet novel “Zhuxian” (henceforth referred to as ZX; also known as “Jade dynasty”) as our target domain data. The writing style of the novel is in the literature genre, with the style of Ming and Qing novels, very different from news in CTB. Ex591 CTB sentences ZX sentences 乔石会见俄罗斯议员团 天下之大，无奇不有，山川灵秀，亦多妖魔鬼怪。 (Qiaoshi meets the Russian delegates.) (The world was big. It held everything. There were fascinating 李鹏强调要加快推行公务员制度 landscapes. There were haunting ghosts.) (Lipeng stressed on speeding the reform of official regulations.) 时间无多，我去请出诛仙古剑。"
E14-1062,J11-1005,1,0.489487,"the beam to generate new partial results, using two types of actions: (1) Append, which appends ci to the last (partial) word in a partial result; (2) Separate(p), which makes the last word in the partial result as completed and adds ci as a new partial word with a POS tag p. Partial results in the beam are scored globally over all actions used to build them, so that the Nbest can be put back to the agenda for the next step. For each action, features are extracted differently. We use the features from Zhang and Clark (2010). Discriminative learning with early-update (Collins and Roark, 2004; Zhang and Clark, 2011) is used to train the model with beam-search. 2.2 Baseline Unsupervised Adaptation by Self-Training A simple unsupervised approach for POS-tagging with unlabeled data is EM. For a generative model such as HMM, EM can locally maximize the likelihood of training data. Given a good start, EM can result in a competitive HMM tagging model (Goldberg et al., 2008). For discriminative models with source-domain training examples, an initial model can be trained using the source-domain data, and self-training can be applied to find a locally-optimized model using raw target domain sentences. The trainin"
E14-1062,Q13-1001,0,0.0296522,"Missing"
E14-1062,I11-1035,0,0.0274135,"Missing"
I05-2023,A00-1011,0,0.0173885,"tities. As a kernel function, the Improved-Edit-Distance (IED) is used to calculate the similarity between two Chinese strings. By employing the Voted Perceptron and Support Vector Machine (SVM) kernel machines with the IED kernel as the classifiers, we tested the method by extracting person-affiliation relation from Chinese texts. By comparing with traditional feature-based learning methods, we conclude that our method needs less manual efforts in feature transformation and achieves a better performance. At the beginning, a number of manually engineered systems were developed for RE problem (Aone and Ramos-Santacruz, 2000). The automatic learning methods (Miller et al., 1998; Soderland, 1999) are not necessary to have someone on hand with detailed knowledge of how the RE system works, or how to write rules for it. Usually, the machine learning method represents the NLP objects as feature vectors in the feature extraction step. The methods are named feature-based learning methods. But in many cases, data cannot be easily represented explicitly via feature vectors. For example, in most NLP problems, the feature-based representations produce inherently local representations of objects, for it is computationally in"
I05-2023,1993.tmi-1.4,0,0.0777918,"= 2, the object for the pair 郭 士纳 (people) and IBM公司 (organization) can be written as “接 见 了 ORG 主 席 PEO 。” Through the objects transformed from the original texts, we can calculate the similarity between any two objects by using the kernel (similarity) function. For the Chinese relation extraction problem, we must consider the semantic similarity between words and the structure of strings while computing similarity. Therefore we must consider the kernel function which has a good similarity measure. The methods for computing the similarity between two strings are: the same-word based method (Nirenburg et al., 1993), the thesaurus based method (Qin et al., 2003), the Edit-Distance method (Ristad and Yianilos, 1998) and the statistical method (Chatterjee, 2001). We know that the same-word based method cannot solve the problem of synonyms. The thesaurus based method can overcome this difficulty but does not consider the structure of the text. Although the EditDistance method uses the structure of the text, it also has the same problem of the replacement of synonyms. As for the statistical method, it needs large corpora of similarity text and thus is difficult to use for realistic applications. For the reas"
I08-2109,P06-2010,1,0.900752,"hods, have been extensively studied for SRL (Carreras and M`arquez, 2005). Although feature-based methods are regarded as the state-of-the-art methods and achieve much success in SRL, kernel-based methods are more effective in capturing structured features than featurebased methods. In the meanwhile, the syntactic structure features hidden in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure features via two grammar-driven approximate matching mechanisms over substructures and nodes. Experimental results show that the GTK significant"
I08-2109,P02-1031,0,0.0363188,"verbs or nouns and some constituents of the sentence. In previous work, data-driven techniques, including feature-based and kernel-based learning methods, have been extensively studied for SRL (Carreras and M`arquez, 2005). Although feature-based methods are regarded as the state-of-the-art methods and achieve much success in SRL, kernel-based methods are more effective in capturing structured features than featurebased methods. In the meanwhile, the syntactic structure features hidden in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure"
I08-2109,P04-1043,0,0.078076,"ce. In previous work, data-driven techniques, including feature-based and kernel-based learning methods, have been extensively studied for SRL (Carreras and M`arquez, 2005). Although feature-based methods are regarded as the state-of-the-art methods and achieve much success in SRL, kernel-based methods are more effective in capturing structured features than featurebased methods. In the meanwhile, the syntactic structure features hidden in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure features via two grammar-driven approximate"
I08-2109,N06-2025,0,0.0686209,"= opt([d]) = 1, p = 3. Then according to Eq (14), ∆p (cn1 , cn2 ) can be calculated recursively as Eq. (15) (Please refer to the next page). Finally, we have ∆p (cn1 , cn2 ) = λ1 × ∆0 (a, a) × 0 ∆ (b, b) × ∆0 (c, c) By means of the above algorithm, we can compute the ∆0 (n1 , n2 ) in O(p|cn1 |· |cn2 |2 ) (Lodhi et al., 2002). This means that the worst case complexity of the FGTK-I is O(pρ3 |N1 |· |N2 |2 ), where ρ is the maximum branching factor of the two trees. 3.2 Fast Grammar-driven Convolution Tree Kernel II (FGTK-II) Our FGTK-II algorithm is motivated by the partial trees (PTs) kernel (Moschitti, 2006). The PT kernel algorithm uses the following recursive formulas to evaluate ∆p (cn1 , cn2 ): |cn1 ||cn2 | ∆p (cn1 , cn2 ) = X X ∆0p (cn1 [1 : i], cn2 [1 : j]) (16) i=1 j=1 where cn1 [1 : i] and cn2 [1 : j] are the child subsequences of cn1 and cn2 from 1 to i and from 1 to j, respectively. Given two child node sequences s1 a = cn1 [1 : i] and s2 b = cn2 [1 : j] (a and b are the last children), the PT kernel computes ∆0p (·, ·) as follows:  0 ∆p (s1 a, s2 b) = µ2 ∆0 (a, b)Dp (|s1 |, |s2 |) 0 if a = b else (17) where ∆0 (a, b) is defined in Eq. (7) and Dp is recursively defined as follows: Dp ("
I08-2109,W05-0639,0,0.0418752,"Missing"
I08-2109,P07-1026,1,0.892395,"in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure features via two grammar-driven approximate matching mechanisms over substructures and nodes. Experimental results show that the GTK significantly outperforms the TK (Zhang et al., 2007). Theoretically, the GTK method is applicable to any problem that uses syntax structure features and can be solved by the TK methods, such as parsing, relation extraction, and so on. In this paper, we use SRL as an application to test our proposed algorithms. Although the GTK shows promising results"
I08-2109,W05-0620,0,\N,Missing
I11-1113,E06-1002,0,0.0456043,"mental settings, results and analysis are presented in 1010 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1010–1018, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Section 5. The last section offers some concluding remarks. 2 Related Work Linking name mentions to knowledge base entries has attracted more and more attentions in these years. As an open available resource, Wikipedia is a natural choice of knowledge source for its large scale and good quality. Early work mainly focused on the usage of the structure information in Wikipedia. Bunescu and Pasca (2006) trained a taxonomy kernel on Wikipedia data to disambiguate named entities in open domain. Cucerzan (2007) integrated Wikipedia’s category information in their vector space model for named entity disambiguation. Mihalcea and Csomai (2007) extracted sentences from Wikipedia, regarding the linking information as sense annotation, and used supervised machine learning models to train a classifier for disambiguation. Similarly, Milne and Witten (2008) adopted a learning approach for the disambiguation in Wikipedia. Their method is to balance the prior probability of a sense with its relatedness to"
I11-1113,D07-1074,0,0.922986,"Natural Language Processing, pages 1010–1018, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Section 5. The last section offers some concluding remarks. 2 Related Work Linking name mentions to knowledge base entries has attracted more and more attentions in these years. As an open available resource, Wikipedia is a natural choice of knowledge source for its large scale and good quality. Early work mainly focused on the usage of the structure information in Wikipedia. Bunescu and Pasca (2006) trained a taxonomy kernel on Wikipedia data to disambiguate named entities in open domain. Cucerzan (2007) integrated Wikipedia’s category information in their vector space model for named entity disambiguation. Mihalcea and Csomai (2007) extracted sentences from Wikipedia, regarding the linking information as sense annotation, and used supervised machine learning models to train a classifier for disambiguation. Similarly, Milne and Witten (2008) adopted a learning approach for the disambiguation in Wikipedia. Their method is to balance the prior probability of a sense with its relatedness to the surrounding context. Recently, an Entity Linking task in the Knowledge Base Population (KBP) track eva"
I11-1113,C10-1032,0,0.285777,"ation in Wikipedia. Their method is to balance the prior probability of a sense with its relatedness to the surrounding context. Recently, an Entity Linking task in the Knowledge Base Population (KBP) track evaluation (McNamee and Dang, 2009) provided a benchmark data set. The first KBP track was held at the Text Analysis Conference (TAC)1 , aiming to explore information about entities for Question Answering and Information Extraction. The knowledge base in the evaluation data is also based on Wikipedia. Many information retrieval based models have been proposed on this data set. For example, Dredze et al. (2010) presented a maximum margin approach to rank the candidates. They combined rich features including Wikipedia structure and entity’s popularity. Zheng et al. (2010) proposed learning to rank models for the entity linking problem and obtained high accuracy. One of the most important component of entity linking is to compute the relatedness between entities. Some of the previous works use vector space model and calculate the cosine similarity over the bag-of-word feature vectors (Mihalcea and Csomai, 2007) or the category feature vectors (Cucerzan, 2007). Others take into account citation overlap"
I11-1113,J97-4004,0,0.0148976,": 6: 7: 8: 9: 10: Similarly, for the in-degree measure we build the graph in Algorithm 2, where Cn is the name node set of the candidate entities and Na is the article node set of the neighboring entities. Algorithm 2 In-degree measure based graph construction Require: Cn and Na Ensure: Graph G = (V, E) Disambiguation 1: To build a graph for the disambiguation, we need to extract names from the context of the query (either as the name node or the article node). We use a segmentation technique which is inspired from a Chinese word segmentation algorithm, the forward maximum matching algorithm (Guo, 1997) on the context to find all the names which are included in the Wikipedia title list (i.e. all the name phrases in our Wikipedia graph are the Wikipedia article titles). This algorithm prefers to find the longest names that match with the string. Here we 6 See http://en.wikipedia.org/wiki/Wikipedia:Redirect for detailed instructions V := Ca ∪ Nn E := ∅ for all c ∈ Ca do for all n ∈ Nn do if n ∈ Article(c) then E := E ∪ (c, n) end if end for end for return (V, E) 2: 3: 4: 5: 6: 7: 8: 9: 10: V := Cn ∪ Na E := ∅ for all c ∈ Cn do for all n ∈ Na do if c ∈ Article(n) then E := E ∪ (n, c) end if end"
I11-1113,P10-1138,0,0.0597374,"Missing"
I11-1113,zesch-etal-2008-extracting,0,0.027243,"Missing"
I11-1113,N10-1072,0,0.0186403,"the Knowledge Base Population (KBP) track evaluation (McNamee and Dang, 2009) provided a benchmark data set. The first KBP track was held at the Text Analysis Conference (TAC)1 , aiming to explore information about entities for Question Answering and Information Extraction. The knowledge base in the evaluation data is also based on Wikipedia. Many information retrieval based models have been proposed on this data set. For example, Dredze et al. (2010) presented a maximum margin approach to rank the candidates. They combined rich features including Wikipedia structure and entity’s popularity. Zheng et al. (2010) proposed learning to rank models for the entity linking problem and obtained high accuracy. One of the most important component of entity linking is to compute the relatedness between entities. Some of the previous works use vector space model and calculate the cosine similarity over the bag-of-word feature vectors (Mihalcea and Csomai, 2007) or the category feature vectors (Cucerzan, 2007). Others take into account citation overlap of the relevant Wikipedia entry (Milne and Witten, 2008; Kulkarni et al., 2009; Radford et al., 2010), which implies the co1 http://www.nist.gov/tac occurrence of"
I11-1113,E09-1073,0,\N,Missing
I11-1171,A00-2018,0,0.0775912,"Missing"
I11-1171,E06-1011,0,0.174495,"Missing"
I11-1171,J05-1003,0,0.0537492,"Missing"
I11-1171,P08-1108,0,0.105721,"ndard averaged perceptron to learn the weight vector (Collins, 2002). As a probabilistic model, CRF defines the probability of a sequence to be µ(x, t) = P (t|x) = P ew·f (x,t) w·f (x,t′ ) t′ e Perceptron-based tagger with syntactic features Figure 2: Framework of our method. Based on a guide POS sequence t′ (tC in this paper) and a syntactic tree d, we propose three kinds of features, as shown in Table 1. Our use of guide POS Features fg (x, t′ , t) is mainly inspired by stacked learning, in which results of the firstlevel predicator are used to guide the second (Cohen and de Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). Syntactic features fs (x, d, t) explore features related with the head and children of the focus word. Syntactic features with guide POS tags 1448 Guide POS Features: fg (x, t′ , t) t′i ti t′i−1 t′i ti t′i−1 ti t′i t′i+1 ti t′i+1 ti t′i−1 t′i+1 ti ′ ti ∼ fb (x) t′i−1 t′i t′i+1 ti Syntactic Features: fs (x, d, t) Syntactic Features with Guide POS: fsg (x, t′ , d, t) #lc(i) ti wh(i) ti #lc(i) t′i ti #rc(i) t′i ti t′h(i) ti t′i d(i) ti wlc(i,k) ti d(i) ti t′lc(i,k) t′i ti t′rc(i,k) t′i ti t′h(i) t′i ti t′h(i) d(i) ti #rc(i) ti wh(i) d(i) ti t′lc(i,k) ti t′rc(i,k) ti t′h(i"
I11-1171,W02-1001,0,0.027099,"e to CRF in tagging accuracy but requires much less training time. During training phase, we adopt the 10-fold cross validation strategy to produce both tC and dA for the training set. Input sentence ˆt = arg max µ(x, t) CRF-based tagger tC Dependency Parser dA t We implement two baseline taggers, i.e., a Perceptron-based tagger and a CRF-based tagger. As a linear model, Perceptron defines the score of a tag sequence to be µ(x, t) = w · f (x, t) where f (x, t) refers to the feature vector and w is the corresponding weight vector. We use standard averaged perceptron to learn the weight vector (Collins, 2002). As a probabilistic model, CRF defines the probability of a sequence to be µ(x, t) = P (t|x) = P ew·f (x,t) w·f (x,t′ ) t′ e Perceptron-based tagger with syntactic features Figure 2: Framework of our method. Based on a guide POS sequence t′ (tC in this paper) and a syntactic tree d, we propose three kinds of features, as shown in Table 1. Our use of guide POS Features fg (x, t′ , t) is mainly inspired by stacked learning, in which results of the firstlevel predicator are used to guide the second (Cohen and de Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). Syntactic features"
I11-1171,W96-0213,0,0.369745,"Missing"
I11-1171,I05-3005,0,0.0485469,"Missing"
I11-1171,D07-1117,0,0.0424302,"Missing"
I11-1171,N09-2054,0,0.0424392,"Missing"
I11-1171,P08-1101,0,0.0183868,"ce information can be explored. Experimental results show that this effort is rewarding, and the tagging accuracy is significantly improved. Detailed error analysis confirms the usefulness of these syntactic features. 2 Baseline POS Taggers Given an input sentence x = w1 ...wn , we denote its POS tag sequence by t = t1 ...tn , where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. The goal of POS tagging is to find the highest-scoring sequence: We adopt the exponentiated gradient algorithm to learn the weight vector (Collins et al., 2008). For POS tagging features f (x, t), we follow the work of Zhang and Clark (2008a). Besides standard POS unigram (wi ti ), bigram (ti−1 ti ) and trigram (ti−2 ti−1 ti ) features, they explore many features composed of Chinese characters, such as ci,0 ti and ci,−1 ti , where ci,0 and ci,−1 denote the start and end characters of wi . These characterbased features are very helpful for tagging accuracy. Due to space limitation, we refer to Zhang and Clark (2008a) for the complete feature description. In order to distinguish these features from our proposed syntactic features, we refer to them as the basic features and denote them as fb (x, t). Given w, we adopt the Viterbi al"
I11-1171,D08-1059,0,0.10179,"ce information can be explored. Experimental results show that this effort is rewarding, and the tagging accuracy is significantly improved. Detailed error analysis confirms the usefulness of these syntactic features. 2 Baseline POS Taggers Given an input sentence x = w1 ...wn , we denote its POS tag sequence by t = t1 ...tn , where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. The goal of POS tagging is to find the highest-scoring sequence: We adopt the exponentiated gradient algorithm to learn the weight vector (Collins et al., 2008). For POS tagging features f (x, t), we follow the work of Zhang and Clark (2008a). Besides standard POS unigram (wi ti ), bigram (ti−1 ti ) and trigram (ti−2 ti−1 ti ) features, they explore many features composed of Chinese characters, such as ci,0 ti and ci,−1 ti , where ci,0 and ci,−1 denote the start and end characters of wi . These characterbased features are very helpful for tagging accuracy. Due to space limitation, we refer to Zhang and Clark (2008a) for the complete feature description. In order to distinguish these features from our proposed syntactic features, we refer to them as the basic features and denote them as fb (x, t). Given w, we adopt the Viterbi al"
I11-1171,P08-1102,0,0.0604386,"Missing"
I11-1171,P09-1058,0,0.123831,"Missing"
I11-1171,D08-1017,0,0.0653507,"to learn the weight vector (Collins, 2002). As a probabilistic model, CRF defines the probability of a sequence to be µ(x, t) = P (t|x) = P ew·f (x,t) w·f (x,t′ ) t′ e Perceptron-based tagger with syntactic features Figure 2: Framework of our method. Based on a guide POS sequence t′ (tC in this paper) and a syntactic tree d, we propose three kinds of features, as shown in Table 1. Our use of guide POS Features fg (x, t′ , t) is mainly inspired by stacked learning, in which results of the firstlevel predicator are used to guide the second (Cohen and de Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). Syntactic features fs (x, d, t) explore features related with the head and children of the focus word. Syntactic features with guide POS tags 1448 Guide POS Features: fg (x, t′ , t) t′i ti t′i−1 t′i ti t′i−1 ti t′i t′i+1 ti t′i+1 ti t′i−1 t′i+1 ti ′ ti ∼ fb (x) t′i−1 t′i t′i+1 ti Syntactic Features: fs (x, d, t) Syntactic Features with Guide POS: fsg (x, t′ , d, t) #lc(i) ti wh(i) ti #lc(i) t′i ti #rc(i) t′i ti t′h(i) ti t′i d(i) ti wlc(i,k) ti d(i) ti t′lc(i,k) t′i ti t′rc(i,k) t′i ti t′h(i) t′i ti t′h(i) d(i) ti #rc(i) ti wh(i) d(i) ti t′lc(i,k) ti t′rc(i,k) ti t′h(i) wi ti t′h(i) t′i d(i)"
I11-1176,ambati-etal-2010-active,0,0.0374451,"Missing"
I11-1176,W10-0701,0,0.0352678,"Missing"
I11-1176,C10-3004,1,0.801732,"sers how to use the system. “系统产生的验证码” means “The confirmation codes provided by the system” and “选择正确的意思” means “Please choose the correct word sense”. built a trial system and took advantage of an annotated Chinese all-words WSD corpus consisting of about 10,000 sentences containing about 200,000 words sampled from Chinese news documents. In these words, there are about 78,800 ambiguous words and all of which have been annotated with their corresponding senses by human experts. Among them, we randomly set 5,000 words as unknown questions and remaining as known. 3.2 Thesaurus We use WordMap (Che et al., 2010) as a thesaurus to represent word senses. There are more than 100,000 Chinese words in WordMap. Each word sense belongs to a tree node with five levels. There are 12 top level nodes, such as “entity” and “human beings”. There are about 100 second, 15,000 third, and more fourth and fifth level nodes. Under the fifth level nodes, there are some synonyms which have the same word sense. For instance, the word “材料” has two senses which are represented by five level nodes as follows: 1. 物 (entity) → 统称 (common name) → 物资 (goods) → 物资 (goods) → 材料 (material) 1474 # of times an example is annotated 1"
I11-1176,W04-0811,0,0.0539821,"Missing"
I11-1176,P95-1026,0,0.207491,"Missing"
K17-3005,P13-1104,0,0.0329559,"a. We scale PMI 1 Vietnamese requires word segmentation because white spaces occur both inter- and intra-words. When segmenting Vietnamese, white space-separated tokens are used as inputs, rather than characters as in Chinese and Japanese. In addition, we don’t consider Korean here since the Korean input texts have already been segmented in the corpus provided by the task. Word Segmentation We develop our own word segmentation models particularly for languages which do not have ex53 2.3 The transition-based dependency parsing algorithm with a list-based arc-eager transition system proposed by Choi and McCallum (2013) is used in our parser. We base our parser mainly on the Stack-LSTM model proposed by Dyer et al. (2015), where three Stack-LSTMs are utilized to incrementally obtain the representations of the buffer β, the stack σ and the transition action sequence A. In addition, a dependency-based Recursive Neural Network (RecNN) is used to compute the partially constructed tree representation. However, compared with the arc-standard algorithm (Nivre, 2004) used by Dyer et al. (2015), the list-based arc-eager transition system has an extra component in each configuration, i.e., the deque δ. So we use an ad"
K17-3005,P16-2006,0,0.0510368,"sively combining head-modifier pairs. Whereas in Tree-LSTM, a head is combined with all of its modifiers simultaneously in each LSTM unit. However, our implementation of Tree-LSTM is different from the conventional one. Unlike traditional bottom-up Tree-LSTMs in which each head and all of its modifiers are combined simultaneously, the modifiers are found incrementally during our parsing procedure. Therefore, we propose Incremental Tree-LSTM, which obtains sub-tree representations incrementally. To be more specific, each time a dependency arc is generated, 2016; Kiperwasser and Goldberg, 2016; Cross and Huang, 2016), thus called Bi-LSTM Subtraction. The forward and backward subtractions are calculated independently, i.e., bf = hf (l)−hf (f ) and bb = bb (f ) − bb (l), where hf (f ) and hf (l) are the hidden vectors of the first and the last words in the forward LSTM, hb (f ) and hb (l) are the hidden vectors of the first and the last words in the backward LSTM. Then bf and bb are concatenated as the buffer representation. As illustrated in Figure 5, the forward and backward subtractions for the buffer are bf = hf (here) − hf (nice) and bb = hb (nice) − hb (here) respectively. 55 Target Source we collect"
K17-3005,P15-1033,0,0.103906,". When segmenting Vietnamese, white space-separated tokens are used as inputs, rather than characters as in Chinese and Japanese. In addition, we don’t consider Korean here since the Korean input texts have already been segmented in the corpus provided by the task. Word Segmentation We develop our own word segmentation models particularly for languages which do not have ex53 2.3 The transition-based dependency parsing algorithm with a list-based arc-eager transition system proposed by Choi and McCallum (2013) is used in our parser. We base our parser mainly on the Stack-LSTM model proposed by Dyer et al. (2015), where three Stack-LSTMs are utilized to incrementally obtain the representations of the buffer β, the stack σ and the transition action sequence A. In addition, a dependency-based Recursive Neural Network (RecNN) is used to compute the partially constructed tree representation. However, compared with the arc-standard algorithm (Nivre, 2004) used by Dyer et al. (2015), the list-based arc-eager transition system has an extra component in each configuration, i.e., the deque δ. So we use an additional Stack-LSTM to learn the representation of δ. More importantly, we introduce two LSTM-based tech"
K17-3005,N13-1073,0,0.0392245,"nt data. Target model Unified model Cross-lingual word embeddings train BiTexts Source fine-tuning Target 4.1.2 Figure 7: The cross-lingual transfer approach. Target Source bxr tr & ug & kk kmr fa sme fi ftb & fi We use the provided 100-dimensional multilingual word embeddings5 in our tokenization, POS tagging and parsing models, and use the Wikipedia and CommonCrawl data for training Brown clusters. The number of clusters is set to 256. For cross-lingual transfer parsing of lowresource languages, we use parallel data from OPUS to derive cross-lingual word embeddings.6 The fast align toolkit (Dyer et al., 2013) is used for word alignment.7 We use the Dynet toolkit for the implementation of all our neural models.8 hsb cs Table 2: Cross-lingual delexicalized transfer settings for surprise languages. After that, target language-specific parsers are obtained through fine-tuning on their own treebanks. Figure 7 illustrates the flow of our transfer approach. For the surprise languages in the final test phase, we use the transfer settings in Table 2. We use multi-source delexicalized transfer for surprise language parsing, considering that bilingual parallel data which is required for obtaining crosslingua"
K17-3005,P15-1119,1,0.848142,"raining treebank of each domain, to obtain target domain-specific parsers. In practice, for each language considered here, we treat the largest treebank as our source-domain data, and the rest as target-domain data. Only target-domain models are fine-tuned from the unified parser, while the source-domain parser is trained separately using the source treebank alone. For the new parallel test sets in test phase, we simply use the model trained on source-domain data, without any assumption on the target domain. 3.2 qa en resource, and employ the cross-lingual model transfer approach described in Guo et al. (2015, 2016) to benefit from existing resource-rich languages. The low-resource languages here include Ukrainian (uk), Irish (ga), Uyghur (ug) and Kazakh (kk). We determine their source language (treebank) according to the language families they belong to and their linguistic typological similarity. Specifically, the transfer setting is shown in Table 1. The transfer approach is similar to crossdomain transfer as described above, with one important difference. Here, we use cross-lingual word embeddings and Brown clusters derived by the robust projection approach (Guo et al., 2015) when training the"
K17-3005,L16-1262,0,0.0932897,"Missing"
K17-3005,L16-1680,0,0.0624147,"Missing"
K17-3005,N12-1052,0,0.0885217,"Missing"
K17-3005,P15-1150,0,0.128276,"Missing"
K17-3005,N16-1064,0,0.0315829,"Missing"
K17-3005,P15-1032,0,0.0259162,"or evaluation. Eliyahu Kiperwasser and Yoav Goldberg. 2016. Simple and accurate dependency parsing using bidirectional lstm feature representations. TACL 4:313– 327. 6 Percy Liang. 2005. Semi-supervised learning for natural language. Master thesis, Massachusetts Institute of Technology. Credits There are a few references we would like to give proper credit, especially to data providers: the core Universal Dependencies paper from LREC 2016 (Nivre et al., 2016), the UD version 2.0 datasets (Nivre et al., 2017b,a), the baseline UDPipe models (Straka et al., 2016b), the baseline SyntaxNet models (Weiss et al., 2015) and the evaluation platform TIRA (Potthast et al., 2014). Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. International Conference on Learning Representations (ICLR) Workshop . Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proc. of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together. pages 50–57. Acknowledgments ˇ Joakim Nivre, Zeljko Agi´c, Lars Ahrenberg, Lene Antonsen, Maria Jesus Aranzabe, Masayuki Asahara, Luma Ateyah, Mohammed Attia, Aitziber Atutxa, Ele"
K17-3005,Q16-1023,0,\N,Missing
K18-2005,P15-1001,0,\N,Missing
K18-2005,P15-1119,1,\N,Missing
K18-2005,I17-1007,0,\N,Missing
K18-2005,C16-1002,1,\N,Missing
K18-2005,K17-3005,1,\N,Missing
K18-2005,K17-3002,0,\N,Missing
K18-2005,K17-3004,0,\N,Missing
K18-2005,K17-3003,0,\N,Missing
K18-2005,D17-1002,0,\N,Missing
K18-2005,P18-1129,1,\N,Missing
K18-2005,N18-1088,1,\N,Missing
K19-2007,P13-1023,0,0.367278,"combines formally and linguistically different approaches to meaning representation in graph form in a uniform training and evaluation setup. Recently, a lot of semantic graphbanks arise, which differ in the design of graphs (Kuhlmann and Oepen, 2016), or semantic scheme (Abend and Rappoport, 2017). More specifically, SDP (Oepen et al., 2015), including DM, PSD and PAS, treats the tokens as nodes and connect them with semantic relations; EDS (Flickinger et al., 2017) encodes MRS representations (Copestake et al., 1999) as graphs with the many-to-many relations between tokens and nodes; UCCA (Abend and Rappoport, 2013) represents semantic structures with the multi-layer framework; AMR (Banarescu 1 See http://mrp.nlpl.eu/ for further technical details, information on how to obtain the data, and official results. 76 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 76–85 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2007 post-processing buffer stack transition-based parser with stack LSTM classifier deque transition system action history pos lemma frame tagger tagger tagger BERT Figure 1: A"
K19-2007,W13-2322,0,0.123225,"Missing"
K19-2007,Q16-1023,0,0.0465041,"roposed in in Section 3. 2 We encourage the reader to read Dyer et al. (2015) for more details. 77 operation a a b b c stacks buffer batch LSTM states a state a b state b c state c state a state b state c c Figure 2: When some new I NSERT operations come, the data to be inserted are pushed into corresponding buffers. They will be merged into a batch once batch-processing is triggered. After that, new LSTM states will be pushed to corresponding stacks. 2.2 Batch Training there is already an I NSERT in the buffer; b) operation P OP or Q UERY comes. To clarify, the depth of buffer per data is 1. Kiperwasser and Goldberg (2016) shows that batch training increases the gradient stability and speeds up the training. Delaying the backward to simulate mini-batch update is a simple way to realize batch training, but it fails to compute over data in parallel. To solve this, we propose a method of maintaining stack LSTM structure and using operation buffer. 2.3 2.3.1 BERT-Enhance Word Representation Deep Contextualized Word Representations Neural parsers often use pretrained word embeddings as their primary input, i.e. word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), which assign a single static represen"
K19-2007,P19-4002,0,0.105274,"Missing"
K19-2007,P17-1112,0,0.569697,"are sub-set of surface tokens; in EDS and UCCA, graph nodes are explicitly aligned with the tokens; in AMR, the alignments are implicit. Most semantic parsers are only designed for one or few specific graphbanks, due to the differences in annotation schemes. For example, the currently best parser for SDP is graph-based (Dozat and Manning, 2018), which assumes dependency graphs but cannot be directly applied to UCCA, EDS, and AMR, due the existence of concept node. Hershcovich et al. (2018) parses across different semantic graphbanks (UCCA, DM, AMR), but only works well on UCCA. The system of Buys and Blunsom (2017) is a good data-driven EDS parser, but does poorly on AMR. Lindemann et al. (2019) sets a new SOTA in DM, PAS, PSD, AMR and nearly SOTA in EDS, via representing each graph with the compositional tree structure (Groschwitz et al., 2017), but they do not expand this method to UCCA. Learning from multiple flavors of meaning representation in parallel has hardly been explored, and notable exceptions include the parsers of Peng et al. (2017, 2018); Hershcovich et al. (2018). Therefore, the main challenge in crossframework semantic parsing task is that diverse framework differs in the mapping way be"
K19-2007,D19-1279,0,0.0379668,"2014), which assign a single static representation to each word so that they cannot capture context-dependent meaning. By contrast, deep contextualized word representations, i.e. ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), encode words with respect to the context, which have been proven to be useful for many NLP tasks, achieving state-of-the-art performance in standard Natural Language Understanding (NLU) benchmarks, such as GLUE (Wang et al., 2018a). Che et al. (2018) adopted ELMo in CoNLL 2018 shared task (Zeman et al., 2018) and achieved first prize in terms of LAS metric. (Kondratyuk and Straka, 2019) exceeds the state-ofthe-art in UD with fine-tuning model with BERT. stack LSTM The stack LSTM augments the conventional LSTM with a ‘stack pointer’. And it supports the operation including: a) I NSERT adds elements to the end of the sequence; b) P OP moves the stack pointer to the previous element; c) Q UERY returns the output vector where the stack pointer points. Among these three operation, P OP and Q UERY only manipulates the stack without complex computing, but I NSERT performs lots of computing. Batch Data in Operation-Level Like conventional LSTM can’t form a batch inside a sequence du"
K19-2007,P13-2131,0,0.0357806,"ist the involved results they submitted. Feature DM LF1 MRP PSD LF1 MRP UCCA LF1 MRP EDS EDM MRP GloVe BERT(base) 87.1 94.3 74.1 83.6 56.3 64.3 82.5 87.6 87.3 90.5 73.7 76.7 87.5 92.8 88.2 91.5 AMR SMATCH MRP 64.8 71.0 65.3 71.4 Table 2: HIT-SCIR parser results on MRP split dataset with GloVe or BERT as pretrained word representation. MRP stands for cross-framework evaluation metric. LF1 stands for SDP Labeled F1 (Oepen et al., 2014) in DM/PSD, UCCA Labeled Dependency F1 (Hershcovich et al., 2019) in UCCA. And EDM (Dridan and Oepen, 2011) stands for Elementary Dependency Match in EDS. SMATCH (Cai and Knight, 2013) is an evaluation metric for semantic feature structures in AMR. groups differ in learning rate. For training we use Adam (Kingma and Ba, 2015). Code for our parser and model weights are available at https://github.com/DreamerDeo/ HIT-SCIR-CoNLL2019. as nodes and edges, we need an extra procedure to recognize which nodes should be properties in the final result. Once recognized, node along with the corresponding edge will be converted to the property of its parent node, edge label for the key, and node label for the value. We write some rules to perform the recognizing procedure. Rules come fr"
K19-2007,J16-4009,0,0.296551,"Missing"
K19-2007,K18-2005,1,0.746044,"often use pretrained word embeddings as their primary input, i.e. word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), which assign a single static representation to each word so that they cannot capture context-dependent meaning. By contrast, deep contextualized word representations, i.e. ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), encode words with respect to the context, which have been proven to be useful for many NLP tasks, achieving state-of-the-art performance in standard Natural Language Understanding (NLU) benchmarks, such as GLUE (Wang et al., 2018a). Che et al. (2018) adopted ELMo in CoNLL 2018 shared task (Zeman et al., 2018) and achieved first prize in terms of LAS metric. (Kondratyuk and Straka, 2019) exceeds the state-ofthe-art in UD with fine-tuning model with BERT. stack LSTM The stack LSTM augments the conventional LSTM with a ‘stack pointer’. And it supports the operation including: a) I NSERT adds elements to the end of the sequence; b) P OP moves the stack pointer to the previous element; c) Q UERY returns the output vector where the stack pointer points. Among these three operation, P OP and Q UERY only manipulates the stack without complex comp"
K19-2007,D19-1277,0,0.0864988,"Missing"
K19-2007,P13-1104,0,0.0184641,"ame as DM and PSD. R EDUCE pops the stack, to allow removing a node once all its edges have been created. • N ODE transition creates new non-terminal nodes. For every X ∈ L, N ODEX creates a new node on the buffer as a parent of the first element on the stack, with an X-labeled edge. Transition Systems Building on previous work on parsing reentrancies, discontinuities, and non-terminal nodes, we define an extended set of transitions and features that supports the conjunction of these properties. To solve cross-arc problem, we use list-based arceager algorithm for DM, PSD, and EDS framework as Choi and McCallum (2013); Nivre (2003, 2008); for UCCA framework, we employ S WAP operation to generate cross-arc as Hershcovich et al. (2017).4 3.1 UCCA • L EFT-E DGEX and R IGHT-E DGEX create a new primary X-labeled edge between the first two elements on the stack, where the parent is the left or the right node, respectively. • L EFT-R EMOTEX and R IGHT-R EMOTEX do not have this restriction, and the created edge is additionally marked as remote. DM and PSD • S WAP pops the second node on the stack and adds it to the top of the buffer, as with the similarly named transition in previous work (Maier, 2015; Nivre, 2009"
K19-2007,P19-1450,0,0.241094,"Missing"
K19-2007,N19-1423,0,0.584823,"st-processing buffer stack transition-based parser with stack LSTM classifier deque transition system action history pos lemma frame tagger tagger tagger BERT Figure 1: A unified pipeline for meaning representation parsing, including transition-based parser, BERT-enhanced word representation, and post-processing, along with the additional taggers for label the nodes with pos, frame and lemma. 2.1 pects: 1) Efficient Training Aligning the homogeneous operation in stack LSTM within a batch and then computing them simultaneously; 2) Effective Encoding Fine-tuning the parser with pretrained BERT (Devlin et al., 2019) embedding, which enrich the context information to make accurate local decisions, and global learning for exact search. Together with the post-processing, we developed a unified pipeline for meaning representation parsing. Our contribution can be summarised as follows: In order to design the unified transition-based parser, we refer to the following frameworkspecific parsers: Wang et al. (2018b) for DM and PSD, Hershcovich et al. (2017) for UCCA, Buys and Blunsom (2017) for EDS, Liu et al. (2018) for AMR. Those parsers differ in the design of transition system to generate oracle action sequen"
K19-2007,D18-1264,1,0.946369,"puting them simultaneously; 2) Effective Encoding Fine-tuning the parser with pretrained BERT (Devlin et al., 2019) embedding, which enrich the context information to make accurate local decisions, and global learning for exact search. Together with the post-processing, we developed a unified pipeline for meaning representation parsing. Our contribution can be summarised as follows: In order to design the unified transition-based parser, we refer to the following frameworkspecific parsers: Wang et al. (2018b) for DM and PSD, Hershcovich et al. (2017) for UCCA, Buys and Blunsom (2017) for EDS, Liu et al. (2018) for AMR. Those parsers differ in the design of transition system to generate oracle action sequence, but similar in modeling the parsing state. A tuple (S, L, B, E, V ) is used to represent parsing state, where S is a stack holding processed words, L is a list holding words popped out of S that will be pushed back in the future, and B is a buffer holding unprocessed words. E is a set of labeled dependency arcs. V is a set of graph nodes include concept nodes and surface tokens. The initial state is ([0], [ ], [1, · · · , n], [ ], V ) , where V only contains surface tokens since the concept no"
K19-2007,P15-1116,0,0.0497308,"Choi and McCallum (2013); Nivre (2003, 2008); for UCCA framework, we employ S WAP operation to generate cross-arc as Hershcovich et al. (2017).4 3.1 UCCA • L EFT-E DGEX and R IGHT-E DGEX create a new primary X-labeled edge between the first two elements on the stack, where the parent is the left or the right node, respectively. • L EFT-R EMOTEX and R IGHT-R EMOTEX do not have this restriction, and the created edge is additionally marked as remote. DM and PSD • S WAP pops the second node on the stack and adds it to the top of the buffer, as with the similarly named transition in previous work (Maier, 2015; Nivre, 2009). We follow the work of (Wang et al., 2018b) to design transition system for DM and PSD. 3 For further explanation, please visit the official website:http://mrp.nlpl.eu/index.php?page=5 4 The transition sets for each framework have been introduced with table format in supplementary material. • F INISH pops the root node and marks the state as terminal. 79 special single tokens if needed using operation M ERGE. (b) Then we use operation C ONFIRM to convert a single token on buffer to a graph node(concept). In order to process entity concepts like date-entity better, operation E NT"
K19-2007,P18-2077,0,0.0493883,"ir.hit.edu.cn Abstract et al., 2013) represents the meaning of each word using a concept graph. Koller et al. (2019) classifies these frameworks into three flavors of semantic graphs, based on the degree of alignment between the tokens and the graph nodes. In DM and PSD, nodes are sub-set of surface tokens; in EDS and UCCA, graph nodes are explicitly aligned with the tokens; in AMR, the alignments are implicit. Most semantic parsers are only designed for one or few specific graphbanks, due to the differences in annotation schemes. For example, the currently best parser for SDP is graph-based (Dozat and Manning, 2018), which assumes dependency graphs but cannot be directly applied to UCCA, EDS, and AMR, due the existence of concept node. Hershcovich et al. (2018) parses across different semantic graphbanks (UCCA, DM, AMR), but only works well on UCCA. The system of Buys and Blunsom (2017) is a good data-driven EDS parser, but does poorly on AMR. Lindemann et al. (2019) sets a new SOTA in DM, PAS, PSD, AMR and nearly SOTA in EDS, via representing each graph with the compositional tree structure (Groschwitz et al., 2017), but they do not expand this method to UCCA. Learning from multiple flavors of meaning r"
K19-2007,W11-2927,0,0.134231,"in DM framework, and it ranks 2nd in DM. Amazon achieves 1st in AMR. We only list the involved results they submitted. Feature DM LF1 MRP PSD LF1 MRP UCCA LF1 MRP EDS EDM MRP GloVe BERT(base) 87.1 94.3 74.1 83.6 56.3 64.3 82.5 87.6 87.3 90.5 73.7 76.7 87.5 92.8 88.2 91.5 AMR SMATCH MRP 64.8 71.0 65.3 71.4 Table 2: HIT-SCIR parser results on MRP split dataset with GloVe or BERT as pretrained word representation. MRP stands for cross-framework evaluation metric. LF1 stands for SDP Labeled F1 (Oepen et al., 2014) in DM/PSD, UCCA Labeled Dependency F1 (Hershcovich et al., 2019) in UCCA. And EDM (Dridan and Oepen, 2011) stands for Elementary Dependency Match in EDS. SMATCH (Cai and Knight, 2013) is an evaluation metric for semantic feature structures in AMR. groups differ in learning rate. For training we use Adam (Kingma and Ba, 2015). Code for our parser and model weights are available at https://github.com/DreamerDeo/ HIT-SCIR-CoNLL2019. as nodes and edges, we need an extra procedure to recognize which nodes should be properties in the final result. Once recognized, node along with the corresponding edge will be converted to the property of its parent node, edge label for the key, and node label for the v"
K19-2007,W03-3017,0,0.213811,"pops the stack, to allow removing a node once all its edges have been created. • N ODE transition creates new non-terminal nodes. For every X ∈ L, N ODEX creates a new node on the buffer as a parent of the first element on the stack, with an X-labeled edge. Transition Systems Building on previous work on parsing reentrancies, discontinuities, and non-terminal nodes, we define an extended set of transitions and features that supports the conjunction of these properties. To solve cross-arc problem, we use list-based arceager algorithm for DM, PSD, and EDS framework as Choi and McCallum (2013); Nivre (2003, 2008); for UCCA framework, we employ S WAP operation to generate cross-arc as Hershcovich et al. (2017).4 3.1 UCCA • L EFT-E DGEX and R IGHT-E DGEX create a new primary X-labeled edge between the first two elements on the stack, where the parent is the left or the right node, respectively. • L EFT-R EMOTEX and R IGHT-R EMOTEX do not have this restriction, and the created edge is additionally marked as remote. DM and PSD • S WAP pops the second node on the stack and adds it to the top of the buffer, as with the similarly named transition in previous work (Maier, 2015; Nivre, 2009). We follow"
K19-2007,P15-1033,0,0.0217357,"n 2.3. At last, to label the nodes with pos, frame and lemma, we use additional tagger models to predict these in Section 2.4. The framework-specific transition system is presented in Section 3 and post-processing for each framework is discussed in Section 4. p(a|s) = P exp{ga · STACK LSTM(s) + ba } , a0 exp{ga0 · STACK LSTM(s) + ba0 } where STACK LSTM(s) encodes the state s into a vector, ga and ba are embedding vector, bias vector of action a respectively. The oracle transition action sequence is obtained through transition system, proposed in in Section 3. 2 We encourage the reader to read Dyer et al. (2015) for more details. 77 operation a a b b c stacks buffer batch LSTM states a state a b state b c state c state a state b state c c Figure 2: When some new I NSERT operations come, the data to be inserted are pushed into corresponding buffers. They will be merged into a batch once batch-processing is triggered. After that, new LSTM states will be pushed to corresponding stacks. 2.2 Batch Training there is already an I NSERT in the buffer; b) operation P OP or Q UERY comes. To clarify, the depth of buffer per data is 1. Kiperwasser and Goldberg (2016) shows that batch training increases the gradi"
K19-2007,J08-4003,0,0.0593101,"Missing"
K19-2007,W17-6810,0,0.0838011,"Missing"
K19-2007,K19-2001,0,0.278585,"Missing"
K19-2007,P17-1104,0,0.563505,"g the homogeneous operation in stack LSTM within a batch and then computing them simultaneously; 2) Effective Encoding Fine-tuning the parser with pretrained BERT (Devlin et al., 2019) embedding, which enrich the context information to make accurate local decisions, and global learning for exact search. Together with the post-processing, we developed a unified pipeline for meaning representation parsing. Our contribution can be summarised as follows: In order to design the unified transition-based parser, we refer to the following frameworkspecific parsers: Wang et al. (2018b) for DM and PSD, Hershcovich et al. (2017) for UCCA, Buys and Blunsom (2017) for EDS, Liu et al. (2018) for AMR. Those parsers differ in the design of transition system to generate oracle action sequence, but similar in modeling the parsing state. A tuple (S, L, B, E, V ) is used to represent parsing state, where S is a stack holding processed words, L is a list holding words popped out of S that will be pushed back in the future, and B is a buffer holding unprocessed words. E is a set of labeled dependency arcs. V is a set of graph nodes include concept nodes and surface tokens. The initial state is ([0], [ ], [1, · · · , n], [ ], V"
K19-2007,S15-2153,0,0.237924,"Missing"
K19-2007,P18-1035,0,0.306636,"Missing"
K19-2007,S14-2008,0,0.162509,"Missing"
K19-2007,P17-1186,0,0.0644995,"Missing"
K19-2007,N18-1135,0,0.0708793,"Missing"
K19-2007,D14-1162,0,0.0840185,"depth of buffer per data is 1. Kiperwasser and Goldberg (2016) shows that batch training increases the gradient stability and speeds up the training. Delaying the backward to simulate mini-batch update is a simple way to realize batch training, but it fails to compute over data in parallel. To solve this, we propose a method of maintaining stack LSTM structure and using operation buffer. 2.3 2.3.1 BERT-Enhance Word Representation Deep Contextualized Word Representations Neural parsers often use pretrained word embeddings as their primary input, i.e. word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), which assign a single static representation to each word so that they cannot capture context-dependent meaning. By contrast, deep contextualized word representations, i.e. ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), encode words with respect to the context, which have been proven to be useful for many NLP tasks, achieving state-of-the-art performance in standard Natural Language Understanding (NLU) benchmarks, such as GLUE (Wang et al., 2018a). Che et al. (2018) adopted ELMo in CoNLL 2018 shared task (Zeman et al., 2018) and achieved first prize in terms of LAS metric. (Kondra"
K19-2007,N18-1202,0,0.0436837,"s a simple way to realize batch training, but it fails to compute over data in parallel. To solve this, we propose a method of maintaining stack LSTM structure and using operation buffer. 2.3 2.3.1 BERT-Enhance Word Representation Deep Contextualized Word Representations Neural parsers often use pretrained word embeddings as their primary input, i.e. word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), which assign a single static representation to each word so that they cannot capture context-dependent meaning. By contrast, deep contextualized word representations, i.e. ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), encode words with respect to the context, which have been proven to be useful for many NLP tasks, achieving state-of-the-art performance in standard Natural Language Understanding (NLU) benchmarks, such as GLUE (Wang et al., 2018a). Che et al. (2018) adopted ELMo in CoNLL 2018 shared task (Zeman et al., 2018) and achieved first prize in terms of LAS metric. (Kondratyuk and Straka, 2019) exceeds the state-ofthe-art in UD with fine-tuning model with BERT. stack LSTM The stack LSTM augments the conventional LSTM with a ‘stack pointer’. And it supports the operatio"
K19-2007,C08-1095,0,0.238116,"ous concepts. If there is a concept node on top of buffer, operation N EW can be performed to parse this kind of concept nodes. After solving the problem of parsing concept nodes from surface string, the basic transition set used in DM and PSD is able to predict edges between concept nodes. As a UCCA node may only have one incoming primary edge, E DGE transitions are disallowed if the child node already has an incoming primary edge. To support the prediction of multiple parents, node and edge transitions leave the stack unchanged, as in other work on transition-based dependency graph parsing (Sagae and Tsujii, 2008). 3.3 EDS Based on the work of (Buys and Blunsom, 2017), we extended N ODE -S TARTL and N ODE -E ND actions for generating concept node and realizing node alignment. To clarify, wi is the top element in stack and wj is the top element in buffer. Moreover, wi could only be concept node (stack and list only contain concept node), and wj could be concept node or surface token. • R EDUCE and PASS operations are the same as DM and PSD. • S HIFT, L EFT-E DGEX and R IGHT-E DGEX are similar to operations in DM and PSD, but they can be performed only when the top of buffer is a concept node. • S HIFT a"
K19-2007,W18-5446,0,0.259079,"pects: 1) Efficient Training Aligning the homogeneous operation in stack LSTM within a batch and then computing them simultaneously; 2) Effective Encoding Fine-tuning the parser with pretrained BERT (Devlin et al., 2019) embedding, which enrich the context information to make accurate local decisions, and global learning for exact search. Together with the post-processing, we developed a unified pipeline for meaning representation parsing. Our contribution can be summarised as follows: In order to design the unified transition-based parser, we refer to the following frameworkspecific parsers: Wang et al. (2018b) for DM and PSD, Hershcovich et al. (2017) for UCCA, Buys and Blunsom (2017) for EDS, Liu et al. (2018) for AMR. Those parsers differ in the design of transition system to generate oracle action sequence, but similar in modeling the parsing state. A tuple (S, L, B, E, V ) is used to represent parsing state, where S is a stack holding processed words, L is a list holding words popped out of S that will be pushed back in the future, and B is a buffer holding unprocessed words. E is a set of labeled dependency arcs. V is a set of graph nodes include concept nodes and surface tokens. The initial"
K19-2007,K18-2001,0,0.0649206,"Missing"
N10-1030,J96-1002,0,0.0190661,"an Chapter of the ACL, pages 246–249, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics trees into dependence trees with an Constituent-toDependency Conversion Tool2 . In addition, we also convert the OntoNotes sense of each polysemant into WordNet sense using sense inventory file provided by OntoNotes 2.0. For an OntoNotes sense with more than one WordNet sense, we simply use the foremost (more popular) one. 3 Semantic Role Labeling System Our baseline is a state-of-the-art SRL system based on dependency syntactic tree (Che et al., 2009). A maximum entropy (Berger et al., 1996) classifier is used to predict the probabilities of a word in the sentence to be each semantic role. A virtual role “NULL” (presenting none of roles is assigned) is added to the roles set, so it does not need semantic role identification stage anymore. For a predicate, two classifiers (one for noun predicates, and the other for verb predicates) predict probabilities of each word in a sentence to be each semantic role (including virtual role “NULL”). The features used in this stage are listed in Table 1. problem. As for the POS, it represents the syntactic information, but is not enough to dist"
N10-1030,D07-1007,0,0.0482077,"a hot task in natural language processing. SRL aims at identifying the relations between the predicates in a sentence and their associated arguments. At present, the main stream researches are focusing on feature engineering or combination of multiple results. Word senses are important information for recognizing semantic roles. For example, if we know “cat” is an “agent” of the predicate “eat” in a sentence, we can guess that “dog” can also be an “agent” of “eat”. Word sense has been successfully used in many natural language processing tasks, such as machine translation (Chan et al., 2007; Carpuat and Wu, 2007). CoNLL 2008 shared task (Surdeanu et al., 2008) first introduced the predicate classification task, which can be regarded as the predicate sense disambiguation. Meza-Ruiz and Riedel (2009) has shown that the predicate sense can improve the final SRL performance. However, there is few discussion about the concrete influence of all word senses, i.e. the words besides predicates. The major reason is lacking the corpus, which is both annotated with all word senses and semantic roles. The release of OntoNotes corpus provides an opportunity for us to verify whether all word senses can help SRL. Ont"
N10-1030,P07-1005,0,0.0880655,"Missing"
N10-1030,W09-1207,1,0.868361,"Annual Conference of the North American Chapter of the ACL, pages 246–249, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics trees into dependence trees with an Constituent-toDependency Conversion Tool2 . In addition, we also convert the OntoNotes sense of each polysemant into WordNet sense using sense inventory file provided by OntoNotes 2.0. For an OntoNotes sense with more than one WordNet sense, we simply use the foremost (more popular) one. 3 Semantic Role Labeling System Our baseline is a state-of-the-art SRL system based on dependency syntactic tree (Che et al., 2009). A maximum entropy (Berger et al., 1996) classifier is used to predict the probabilities of a word in the sentence to be each semantic role. A virtual role “NULL” (presenting none of roles is assigned) is added to the roles set, so it does not need semantic role identification stage anymore. For a predicate, two classifiers (one for noun predicates, and the other for verb predicates) predict probabilities of each word in a sentence to be each semantic role (including virtual role “NULL”). The features used in this stage are listed in Table 1. problem. As for the POS, it represents the syntact"
N10-1030,N09-1037,0,0.0933089,"Missing"
N10-1030,N06-2015,0,0.181376,"Missing"
N10-1030,N09-1018,0,0.0452269,"arches are focusing on feature engineering or combination of multiple results. Word senses are important information for recognizing semantic roles. For example, if we know “cat” is an “agent” of the predicate “eat” in a sentence, we can guess that “dog” can also be an “agent” of “eat”. Word sense has been successfully used in many natural language processing tasks, such as machine translation (Chan et al., 2007; Carpuat and Wu, 2007). CoNLL 2008 shared task (Surdeanu et al., 2008) first introduced the predicate classification task, which can be regarded as the predicate sense disambiguation. Meza-Ruiz and Riedel (2009) has shown that the predicate sense can improve the final SRL performance. However, there is few discussion about the concrete influence of all word senses, i.e. the words besides predicates. The major reason is lacking the corpus, which is both annotated with all word senses and semantic roles. The release of OntoNotes corpus provides an opportunity for us to verify whether all word senses can help SRL. OntoNotes is a large corpus annotated with constituency trees (based on Penn Treebank), predicate argument structures (based on Penn PropBank) and word senses. It has been used in some natural"
N10-1030,W08-2121,0,0.032221,"Missing"
N10-1030,D08-1105,0,0.126758,"Missing"
N13-1006,P11-1062,0,0.0227296,"e complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Dependency Parsing (Martins et al., 2009) and Textual Entailment (Berant et al., 2011). Zhuang and Zong (2010) proposed a joint inference method for bilingual semantic role labeling with ILP. However, their approach requires training an alignment model with a manually annotated corpus. 60 9 Conclusions We proposed a novel ILP based inference algorithm with bilingual constraints for NER. This method can jointly infer bilingual named entities without using any annotated bilingual corpus. We investigate various bilingual constraints: hard and soft constraints. Out empirical study on largescale OntoNotes Chinese-English parallel NER data showed that Soft-align method, which allows"
N13-1006,J92-4003,0,0.0240595,"rson), LOC (Location), Chinese NER Templates 00: 1 (class bias param) 01: wi+k , −1 ≤ k ≤ 1 02: wi+k−1 ◦ wi+k , 0 ≤ k ≤ 1 03: shape(wi+k ), −4 ≤ k ≤ 4 04: prefix(wi , k), 1 ≤ k ≤ 4 05: prefix(wi−1 , k), 1 ≤ k ≤ 4 06: suffix(wi , k), 1 ≤ k ≤ 4 07: suffix(wi−1 , k), 1 ≤ k ≤ 4 08: radical(wi , k), 1 ≤ k ≤ len(wi ) Unigram Features yi ◦ 00 – 08 Bigram Features yi−1 ◦ yi ◦ 00 – 08 NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). To our knowledge, this work is the first use of word clustering features for Chinese NER. A C++ implementation of the Brown word clustering algorithms (Brown et al., 1992) was used to obtain the word clusters (Liang, 2005).8 Raw text was obtained from the fifth edition of Chinese Gigaword (LDC2011T13). One million paragraphs from Xinhua news section were randomly selected, and the Stanford Word Segmenter with LDC standard was applied to segment Chinese text into words.9 About 46 million words were obtained which were clustered into 1,000 word classes. Table 1: Basic features of Chinese NER. 5 ORG (Organization) and GPE (Geo-Political Entities), and discarded the others. Since the bilingual corpus is only aligned at the document level, we performed sentence alig"
N13-1006,W10-2906,0,0.0980242,"ual cues can help to recognize errors a monolingual tagger would make, allowing us to produce more accurately tagged bitext. Each side of the tagged bitext can then be used to expand the original monolingual training dataset, which may lead to higher accuracy in the monolingual taggers. Previous work such as Li et al. (2012) and Kim et al. (2012) demonstrated that bilingual corpus annotated with NER labels can be used to improve monolingual tagger performance. But a major drawback of their approaches are the need for manual annotation efforts to create such corpora. To avoid this requirement, Burkett et al. (2010) suggested a “multi-view” learning scheme based on re-ranking. Noisy output of a “strong” tagger is used as training data to learn parameters of a log-linear re-ranking model with additional bilingual features, simulated by a “weak” tagger. The learned parameters are then reused with the “strong” tagger to re-rank its own outputs for unseen inputs. Designing good “weak” taggers so that they complement the “view” of bilingual features in the log-linear re-ranker is crucial to the success of this algorithm. Unfortunately there is no principled way of designing such “weak” taggers. In this paper,"
N13-1006,P10-1065,0,0.0271909,"d the difference discussed in Section 1, their re-ranking strategy may lose the correct named entity results if they are not included in the top-N outputs. Furthermore, we consider the word alignment probabilities in our method which can reduce the influence of word alignment errors. Finally, we test our method on a large standard publicly available corpus (8,249 sentences), while they used a much smaller (200 sentences) manually annotated bilingual NER corpus for results validation. In addition to bilingual corpora, bilingual dictionaries are also useful resources. Huang and Vogel (2002) and Chen et al. (2010) proposed approaches for extracting bilingual named entity pairs from unannotated bitext, in which verification is based on bilingual named entity dictionaries. However, large-scale bilingual named entity dictionaries are difficult to obtain for most language pairs. Yarowsky and Ngai (2001) proposed a projection method that transforms high-quality analysis results of one language, such as English, into other languages on the basis of word alignment. Das and Petrov (2011) applied the above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities"
N13-1006,P11-1061,0,0.0689837,"ults validation. In addition to bilingual corpora, bilingual dictionaries are also useful resources. Huang and Vogel (2002) and Chen et al. (2010) proposed approaches for extracting bilingual named entity pairs from unannotated bitext, in which verification is based on bilingual named entity dictionaries. However, large-scale bilingual named entity dictionaries are difficult to obtain for most language pairs. Yarowsky and Ngai (2001) proposed a projection method that transforms high-quality analysis results of one language, such as English, into other languages on the basis of word alignment. Das and Petrov (2011) applied the above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Depende"
N13-1006,1993.eamt-1.1,0,0.475415,"Missing"
N13-1006,P05-1045,1,0.0203067,"Geo-Political Entities), and discarded the others. Since the bilingual corpus is only aligned at the document level, we performed sentence alignment using the Champollion Tool Kit (CTK).4 After removing sentences with no aligned sentence, a total of 8,249 sentence pairs were retained. We used the BerkeleyAligner,5 to produce word alignments over the sentence-aligned datasets. BerkeleyAligner also gives posterior probabilities Pa for each aligned word pair. We used the CRF-based Stanford NER tagger (using Viterbi decoding) as our baseline monolingual NER tool.6 English features were taken from Finkel et al. (2005). Table 1 lists the basic features of Chinese NER, where ◦ means string concatenation and yi is the named entity tag of the ith word wi . Moreover, shape(wi ) is the shape of wi , such as date and number. prefix/suffix(wi , k) denotes the k-characters prefix/suffix of wi . radical(wi , k) denotes the radical of the k th Chinese character of wi .7 len(wi ) is the number of Chinese characters in wi . To make the baseline CRF taggers stronger, we added word clustering features to improve generalization over unseen data for both Chinese and English. Word clustering features have been successfully"
N13-1006,I11-1030,1,0.656986,"es. Huang and Vogel (2002) and Chen et al. (2010) proposed approaches for extracting bilingual named entity pairs from unannotated bitext, in which verification is based on bilingual named entity dictionaries. However, large-scale bilingual named entity dictionaries are difficult to obtain for most language pairs. Yarowsky and Ngai (2001) proposed a projection method that transforms high-quality analysis results of one language, such as English, into other languages on the basis of word alignment. Das and Petrov (2011) applied the above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Dependency Parsing (Martins et al., 2009) and Textual Entailment (Berant et al., 2011). Zhuang and"
N13-1006,N06-2015,0,0.0588506,"al constraints. We can re-express Eq. (13) as follows: Y Y (λayc ye )Ia (18) λyac ye = X X X XX j zjy log Pjy y∈Y zayc ye Pa log λyac ye (19) a∈A yc ∈Y ye ∈Y We name the set of constraints above Soft-align, which has the same constraints as Soft-tag, i.e., Eqs. (8), (9), (15) and (16). 4 Experimental Setup We conduct experiments on the latest OntoNotes 4.0 corpus (LDC2011T03). OntoNotes is a large, manually annotated corpus that contains various text genres and annotations, such as part-of-speech tags, named entity labels, syntactic parse trees, predicateargument structures and co-references (Hovy et al., 2006). Aside from English, this corpus also contains several Chinese and Arabic corpora. Some of these corpora contain bilingual parallel documents. We used the Chinese-English parallel corpus with named entity labels as our development and test data. This corpus includes about 400 document pairs (chtb 0001-0325, ectb 1001-1078). We used oddnumbered documents as development data and evennumbered documents as test data. We used all other portions of the named entity annotated corpus as training data for the monolingual systems. There were a total of ∼660 Chinese documents (∼16k sentences) and ∼1,400"
N13-1006,D09-1127,0,0.013019,"Missing"
N13-1006,P12-1073,0,0.292443,"ut entities. For example, in Figure 1, the word “本 (Ben)” is common in Chinese but rarely appears as a translated foreign name. However, its aligned word on the English side (“Ben”) provides a strong clue that this is a person name. Judicious use of this type of bilingual cues can help to recognize errors a monolingual tagger would make, allowing us to produce more accurately tagged bitext. Each side of the tagged bitext can then be used to expand the original monolingual training dataset, which may lead to higher accuracy in the monolingual taggers. Previous work such as Li et al. (2012) and Kim et al. (2012) demonstrated that bilingual corpus annotated with NER labels can be used to improve monolingual tagger performance. But a major drawback of their approaches are the need for manual annotation efforts to create such corpora. To avoid this requirement, Burkett et al. (2010) suggested a “multi-view” learning scheme based on re-ranking. Noisy output of a “strong” tagger is used as training data to learn parameters of a log-linear re-ranking model with additional bilingual features, simulated by a “weak” tagger. The learned parameters are then reused with the “strong” tagger to re-rank its own out"
N13-1006,P08-1068,0,0.0356057,"es). OntoNotes annotates 18 named entity types, such as person, location, date and money. In this paper, we selected the four most common named entity types, i.e., PER (Person), LOC (Location), Chinese NER Templates 00: 1 (class bias param) 01: wi+k , −1 ≤ k ≤ 1 02: wi+k−1 ◦ wi+k , 0 ≤ k ≤ 1 03: shape(wi+k ), −4 ≤ k ≤ 4 04: prefix(wi , k), 1 ≤ k ≤ 4 05: prefix(wi−1 , k), 1 ≤ k ≤ 4 06: suffix(wi , k), 1 ≤ k ≤ 4 07: suffix(wi−1 , k), 1 ≤ k ≤ 4 08: radical(wi , k), 1 ≤ k ≤ len(wi ) Unigram Features yi ◦ 00 – 08 Bigram Features yi−1 ◦ yi ◦ 00 – 08 NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). To our knowledge, this work is the first use of word clustering features for Chinese NER. A C++ implementation of the Brown word clustering algorithms (Brown et al., 1992) was used to obtain the word clusters (Liang, 2005).8 Raw text was obtained from the fifth edition of Chinese Gigaword (LDC2011T13). One million paragraphs from Xinhua news section were randomly selected, and the Stanford Word Segmenter with LDC standard was applied to segment Chinese text into words.9 About 46 million words were obtained which were clustered into 1,000 word classes. Table 1: Basic features of Chinese NER."
N13-1006,P09-1039,0,0.0137924,"above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Dependency Parsing (Martins et al., 2009) and Textual Entailment (Berant et al., 2011). Zhuang and Zong (2010) proposed a joint inference method for bilingual semantic role labeling with ILP. However, their approach requires training an alignment model with a manually annotated corpus. 60 9 Conclusions We proposed a novel ILP based inference algorithm with bilingual constraints for NER. This method can jointly infer bilingual named entities without using any annotated bilingual corpus. We investigate various bilingual constraints: hard and soft constraints. Out empirical study on largescale OntoNotes Chinese-English parallel NER data"
N13-1006,J94-2001,0,0.580131,"Missing"
N13-1006,N04-1043,0,0.0626188,"s) and ∼1,400 English documents (∼39k sentences). OntoNotes annotates 18 named entity types, such as person, location, date and money. In this paper, we selected the four most common named entity types, i.e., PER (Person), LOC (Location), Chinese NER Templates 00: 1 (class bias param) 01: wi+k , −1 ≤ k ≤ 1 02: wi+k−1 ◦ wi+k , 0 ≤ k ≤ 1 03: shape(wi+k ), −4 ≤ k ≤ 4 04: prefix(wi , k), 1 ≤ k ≤ 4 05: prefix(wi−1 , k), 1 ≤ k ≤ 4 06: suffix(wi , k), 1 ≤ k ≤ 4 07: suffix(wi−1 , k), 1 ≤ k ≤ 4 08: radical(wi , k), 1 ≤ k ≤ len(wi ) Unigram Features yi ◦ 00 – 08 Bigram Features yi−1 ◦ yi ◦ 00 – 08 NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). To our knowledge, this work is the first use of word clustering features for Chinese NER. A C++ implementation of the Brown word clustering algorithms (Brown et al., 1992) was used to obtain the word clusters (Liang, 2005).8 Raw text was obtained from the fifth edition of Chinese Gigaword (LDC2011T13). One million paragraphs from Xinhua news section were randomly selected, and the Stanford Word Segmenter with LDC standard was applied to segment Chinese text into words.9 About 46 million words were obtained which were clustered into 1,000 word classes"
N13-1006,D10-1069,0,0.0403525,"Missing"
N13-1006,C04-1197,0,0.0188073,"alignment. Das and Petrov (2011) applied the above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Dependency Parsing (Martins et al., 2009) and Textual Entailment (Berant et al., 2011). Zhuang and Zong (2010) proposed a joint inference method for bilingual semantic role labeling with ILP. However, their approach requires training an alignment model with a manually annotated corpus. 60 9 Conclusions We proposed a novel ILP based inference algorithm with bilingual constraints for NER. This method can jointly infer bilingual named entities without using any annotated bilingual corpus. We investigate various bilingual constraints: hard and soft constraints. Out empirical study on largescale"
N13-1006,N01-1026,0,0.135301,"Finally, we test our method on a large standard publicly available corpus (8,249 sentences), while they used a much smaller (200 sentences) manually annotated bilingual NER corpus for results validation. In addition to bilingual corpora, bilingual dictionaries are also useful resources. Huang and Vogel (2002) and Chen et al. (2010) proposed approaches for extracting bilingual named entity pairs from unannotated bitext, in which verification is based on bilingual named entity dictionaries. However, large-scale bilingual named entity dictionaries are difficult to obtain for most language pairs. Yarowsky and Ngai (2001) proposed a projection method that transforms high-quality analysis results of one language, such as English, into other languages on the basis of word alignment. Das and Petrov (2011) applied the above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Const"
N13-1006,D10-1030,0,0.0481815,"al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Dependency Parsing (Martins et al., 2009) and Textual Entailment (Berant et al., 2011). Zhuang and Zong (2010) proposed a joint inference method for bilingual semantic role labeling with ILP. However, their approach requires training an alignment model with a manually annotated corpus. 60 9 Conclusions We proposed a novel ILP based inference algorithm with bilingual constraints for NER. This method can jointly infer bilingual named entities without using any annotated bilingual corpus. We investigate various bilingual constraints: hard and soft constraints. Out empirical study on largescale OntoNotes Chinese-English parallel NER data showed that Soft-align method, which allows inconsistent named entit"
N15-1012,W11-2832,0,0.451539,"follow previous work and conduct experiments on the Penn Treebank (PTB), using Wall Street Jour118 nal sections 2–21 for training, 22 for development testing and 23 for final testing. Gold-standard dependency trees are derived from bracketed sentences in the treebank using Penn2Malt1 , and base noun phrases are treated as a single word (Wan et al., 2009; Zhang, 2013). The BLEU score (Papineni et al., 2002) is used to evaluate the performance of linearization, which has been adopted in former literals (Wan et al., 2009; White and Rajkumar, 2009; Zhang and Clark, 2011b) and recent shared-tasks (Belz et al., 2011). We use our implementation of the best-first system of Zhang (2013), which gives the state-of-the-art results, as the baseline. 4.1 Influence of Beam size We first study the influence of beam size by performing free word ordering on the development test data. BLEU score curves with different beam sizes are shown in Figure 7. From this figure, we can see that the systems with beam 64 and 128 achieve the best results. However, the 128-beam system does not improve the performance significantly (48.2 vs 47.5), but runs twice slower. As a result, we set the beam size to 64 in the remaining experim"
N15-1012,C10-1012,0,0.414105,"([σ|j i], β, A) ([σ|j], β, A ∪ {j → i}) Figure 1: The arc-standard parsing algorithm. Introduction Linearization is the task of ordering a bag of words into a grammatical and fluent sentence. Syntaxbased linearization algorithms generate a sentence along with its syntactic structure. Depending on how much syntactic information is available as inputs, recent work on syntactic linearization can be classified into free word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), which orders a bag of words without syntactic constraints, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by most previous work, and the best results have been achieved by a time-constrained best-first search framework (White, 2004a; White and Rajkumar, 20"
N15-1012,P04-1015,0,0.171585,"and their modifiers. The original feature templates of Zhang and Nivre (2011) also contain information of the front words on the buffer. However, since the buffer is unordered for linearization, we do not include these features. The linearization feature templates are specific for linearization, and captures surface ngram information. Each search state represents a partially linearized sentence. We represents the last word in the partially linearized sentence as w0 and the second last as w−1 . Given a set of labeled training examples, the averaged perceptron (Collins, 2002) with early update (Collins and Roark, 2004; Zhang and Nivre, 2011) is used to train the parameters θ of the model. 3.2 Input Syntactic Constraints The use of syntactic constraints to achieve better linearization performance has been studied in previous work. Wan et al. (2009) employ POS constraints 116 NP . VBD . Dr. Talcott . .2 1 led NP . . NP . .IN a team . 3 of.4 Harvard University . 5 Figure 4: Example partial tree. Words in the same sub dependency trees are grouped by rounded boxes. Word indices do not specify their orders. Base phrases (e.g. Dr. Talcott) are treated as single words. in learning a dependency language model. Zha"
N15-1012,W02-1001,0,0.0628857,"e context information for S0 , S1 and their modifiers. The original feature templates of Zhang and Nivre (2011) also contain information of the front words on the buffer. However, since the buffer is unordered for linearization, we do not include these features. The linearization feature templates are specific for linearization, and captures surface ngram information. Each search state represents a partially linearized sentence. We represents the last word in the partially linearized sentence as w0 and the second last as w−1 . Given a set of labeled training examples, the averaged perceptron (Collins, 2002) with early update (Collins and Roark, 2004; Zhang and Nivre, 2011) is used to train the parameters θ of the model. 3.2 Input Syntactic Constraints The use of syntactic constraints to achieve better linearization performance has been studied in previous work. Wan et al. (2009) employ POS constraints 116 NP . VBD . Dr. Talcott . .2 1 led NP . . NP . .IN a team . 3 of.4 Harvard University . 5 Figure 4: Example partial tree. Words in the same sub dependency trees are grouped by rounded boxes. Word indices do not specify their orders. Base phrases (e.g. Dr. Talcott) are treated as single words. i"
N15-1012,E14-1028,0,0.228722,"Missing"
N15-1012,P09-1091,0,0.647686,"β, A ∪ {j ← i}) ([σ|j i], β, A) ([σ|j], β, A ∪ {j → i}) Figure 1: The arc-standard parsing algorithm. Introduction Linearization is the task of ordering a bag of words into a grammatical and fluent sentence. Syntaxbased linearization algorithms generate a sentence along with its syntactic structure. Depending on how much syntactic information is available as inputs, recent work on syntactic linearization can be classified into free word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), which orders a bag of words without syntactic constraints, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by most previous work, and the best results have been achieved by a time-constrained best-first search framework (White, 2004a; W"
N15-1012,P10-1001,0,0.0355131,"connection between syntactic linearization and syntactic parsing: both build a syntactic tree over a sentence, with the former performing word ordering in addition to derivation construction. As a result, syntactic linearization can be treated as a generalized form of parsing, for which there is no input word order, and therefore extensions to parsing algorithms can be used to perform linearization. For syntactic parsing, the algorithm of Zhang and Nivre (2011) gives competitive accuracies under linear complexity. Compared with parsers that use dynamic programming (McDonald and Pereira, 2006; Koo and Collins, 2010), the efficient beam-search system is more suitable for the NP-hard linearization task. We extend the parser of Zhang and Nivre (2011), so that word ordering is performed in addition to syntactic tree construction. Experimental results show that the transition-based linearization system runs an order of magnitude faster than a state-ofthe-art best-first baseline, with improved accuracies in standard evaluation. Our linearizer is publicly available under GPL at http://sourceforge. net/projects/zgen/. 2 Transition-Based Parsing The task of dependency parsing is to find a dependency tree given an"
N15-1012,E06-1011,0,0.044596,"r method is inspired by the connection between syntactic linearization and syntactic parsing: both build a syntactic tree over a sentence, with the former performing word ordering in addition to derivation construction. As a result, syntactic linearization can be treated as a generalized form of parsing, for which there is no input word order, and therefore extensions to parsing algorithms can be used to perform linearization. For syntactic parsing, the algorithm of Zhang and Nivre (2011) gives competitive accuracies under linear complexity. Compared with parsers that use dynamic programming (McDonald and Pereira, 2006; Koo and Collins, 2010), the efficient beam-search system is more suitable for the NP-hard linearization task. We extend the parser of Zhang and Nivre (2011), so that word ordering is performed in addition to syntactic tree construction. Experimental results show that the transition-based linearization system runs an order of magnitude faster than a state-ofthe-art best-first baseline, with improved accuracies in standard evaluation. Our linearizer is publicly available under GPL at http://sourceforge. net/projects/zgen/. 2 Transition-Based Parsing The task of dependency parsing is to find a"
N15-1012,J08-4003,0,0.0934826,"onstruction. Experimental results show that the transition-based linearization system runs an order of magnitude faster than a state-ofthe-art best-first baseline, with improved accuracies in standard evaluation. Our linearizer is publicly available under GPL at http://sourceforge. net/projects/zgen/. 2 Transition-Based Parsing The task of dependency parsing is to find a dependency tree given an input sentence. Figure 2 shows an example dependency tree, which consists of dependency arcs that represent syntactic relations between pairs of words. A transition-based dependency parsing algorithm (Nivre, 2008) can be formalized as a transition system, S = (C, T, cs , Ct ), where C is the set of states, T is a set of transition actions, cs is the initial state and Ct is a set of terminal states. The parsing process is modeled as an application of a sequence of actions, transducing the initial state into a final state, while constructing de114 0 1 2 3 4 5 6 7 8 9 10 11 Transition S HIFT S HIFT S HIFT S HIFT S HIFT R IGHTA RC R IGHTA RC R IGHTA RC S HIFT R IGHTA RC L EFTA RC σ [] [1] [1 2] [1 2 3] [1 2 3 4] [1 2 3 4 5] [1 2 3 4] [1 2 3] [1 2] [1 2 6] [1 2] [2] β [1...6] [2...6] [3...6] [4...6] [5,6] ["
N15-1012,P02-1040,0,0.0922216,"build arcs between top two words i and j on the stack (line 10-13). If no arc exists between i and j, the next action should shift the parent word of i or a word in i’s sibling tree (line 14-16). 4 Experiments We follow previous work and conduct experiments on the Penn Treebank (PTB), using Wall Street Jour118 nal sections 2–21 for training, 22 for development testing and 23 for final testing. Gold-standard dependency trees are derived from bracketed sentences in the treebank using Penn2Malt1 , and base noun phrases are treated as a single word (Wan et al., 2009; Zhang, 2013). The BLEU score (Papineni et al., 2002) is used to evaluate the performance of linearization, which has been adopted in former literals (Wan et al., 2009; White and Rajkumar, 2009; Zhang and Clark, 2011b) and recent shared-tasks (Belz et al., 2011). We use our implementation of the best-first system of Zhang (2013), which gives the state-of-the-art results, as the baseline. 4.1 Influence of Beam size We first study the influence of beam size by performing free word ordering on the development test data. BLEU score curves with different beam sizes are shown in Figure 7. From this figure, we can see that the systems with beam 64 and"
N15-1012,P05-1025,0,0.015265,"nlabeled and labeled dependency trees (He et al., 2009; Zhang, 2013). These methods mostly use greedy or best-first algorithms to order each tree node. Our work is different by performing word ordering using a transition process. Besides dependency grammar, linearization with other syntactic grammars, such as CFG and CCG (White and Rajkumar, 2009; Zhang and Clark, 2011b), has also been studied. In this paper, we adopt the dependency grammar for transition-based linearization. However, since transition-based parsing algorithms has been successfully applied to different grammars, including CFG (Sagae et al., 2005) and CCG (Xu et al., 2014), our linearization method can be applied to these grammars. 6 Conclusion We studied transition-based syntactic linearization as an extension to transition-based parsing. Compared with best-first systems, the advantage of our transition-based algorithm includes bounded time complexity, and the guarantee to yield full sentences when given a bag of words. Experimental results show that our algorithm achieves improved accuracies, with significantly faster decoding speed compared with a state-of-the-art best-first baseline. We publicly release our code at http: //sourcefo"
N15-1012,E09-1097,0,0.510304,"State ([ ], [1...n], ∅) Final State ([ ], [ ], A) Induction Rules: S HIFT L EFTA RC R IGHTA RC (σ, [i|β], A) ([σ |i], β, A) ([σ|j i], β, A) ([σ|i], β, A ∪ {j ← i}) ([σ|j i], β, A) ([σ|j], β, A ∪ {j → i}) Figure 1: The arc-standard parsing algorithm. Introduction Linearization is the task of ordering a bag of words into a grammatical and fluent sentence. Syntaxbased linearization algorithms generate a sentence along with its syntactic structure. Depending on how much syntactic information is available as inputs, recent work on syntactic linearization can be classified into free word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), which orders a bag of words without syntactic constraints, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search"
N15-1012,D09-1043,0,0.751283,"9; Bohnet et al., 2010; Song et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by most previous work, and the best results have been achieved by a time-constrained best-first search framework (White, 2004a; White and Rajkumar, 2009; Zhang and Clark, 2011b; Song et al., 2014). Though empirically highly accurate, one drawback of this approach is that there is no asymptotic upper bound on the time complexity of finding the first full sentence. As a result, it can take 5–10 seconds to process a sentence, and sometimes fail to yield a full sentence at timeout. This issue is more severe for larger bags of words, and makes the algorithms practically less useful. We study the effect of an alternative learning and search framework for the linearization prob113 Human Language Technologies: The 2015 Annual Conference of the North"
N15-1012,P14-1021,1,0.82756,"cy trees (He et al., 2009; Zhang, 2013). These methods mostly use greedy or best-first algorithms to order each tree node. Our work is different by performing word ordering using a transition process. Besides dependency grammar, linearization with other syntactic grammars, such as CFG and CCG (White and Rajkumar, 2009; Zhang and Clark, 2011b), has also been studied. In this paper, we adopt the dependency grammar for transition-based linearization. However, since transition-based parsing algorithms has been successfully applied to different grammars, including CFG (Sagae et al., 2005) and CCG (Xu et al., 2014), our linearization method can be applied to these grammars. 6 Conclusion We studied transition-based syntactic linearization as an extension to transition-based parsing. Compared with best-first systems, the advantage of our transition-based algorithm includes bounded time complexity, and the guarantee to yield full sentences when given a bag of words. Experimental results show that our algorithm achieves improved accuracies, with significantly faster decoding speed compared with a state-of-the-art best-first baseline. We publicly release our code at http: //sourceforge.net/projects/zgen/. Fo"
N15-1012,J11-1005,1,0.277872,"ng et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by most previous work, and the best results have been achieved by a time-constrained best-first search framework (White, 2004a; White and Rajkumar, 2009; Zhang and Clark, 2011b; Song et al., 2014). Though empirically highly accurate, one drawback of this approach is that there is no asymptotic upper bound on the time complexity of finding the first full sentence. As a result, it can take 5–10 seconds to process a sentence, and sometimes fail to yield a full sentence at timeout. This issue is more severe for larger bags of words, and makes the algorithms practically less useful. We study the effect of an alternative learning and search framework for the linearization prob113 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the"
N15-1012,D11-1106,1,0.31725,"ng et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by most previous work, and the best results have been achieved by a time-constrained best-first search framework (White, 2004a; White and Rajkumar, 2009; Zhang and Clark, 2011b; Song et al., 2014). Though empirically highly accurate, one drawback of this approach is that there is no asymptotic upper bound on the time complexity of finding the first full sentence. As a result, it can take 5–10 seconds to process a sentence, and sometimes fail to yield a full sentence at timeout. This issue is more severe for larger bags of words, and makes the algorithms practically less useful. We study the effect of an alternative learning and search framework for the linearization prob113 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the"
N15-1012,P11-2033,1,0.955294,"cy tree. lem, which has a theoretical upper bound on the time complexity, and always yields a full sentence in quadratic time. Our method is inspired by the connection between syntactic linearization and syntactic parsing: both build a syntactic tree over a sentence, with the former performing word ordering in addition to derivation construction. As a result, syntactic linearization can be treated as a generalized form of parsing, for which there is no input word order, and therefore extensions to parsing algorithms can be used to perform linearization. For syntactic parsing, the algorithm of Zhang and Nivre (2011) gives competitive accuracies under linear complexity. Compared with parsers that use dynamic programming (McDonald and Pereira, 2006; Koo and Collins, 2010), the efficient beam-search system is more suitable for the NP-hard linearization task. We extend the parser of Zhang and Nivre (2011), so that word ordering is performed in addition to syntactic tree construction. Experimental results show that the transition-based linearization system runs an order of magnitude faster than a state-ofthe-art best-first baseline, with improved accuracies in standard evaluation. Our linearizer is publicly a"
N15-1012,E12-1075,1,0.781136,"n], ∅) Final State ([ ], [ ], A) Induction Rules: S HIFT L EFTA RC R IGHTA RC (σ, [i|β], A) ([σ |i], β, A) ([σ|j i], β, A) ([σ|i], β, A ∪ {j ← i}) ([σ|j i], β, A) ([σ|j], β, A ∪ {j → i}) Figure 1: The arc-standard parsing algorithm. Introduction Linearization is the task of ordering a bag of words into a grammatical and fluent sentence. Syntaxbased linearization algorithms generate a sentence along with its syntactic structure. Depending on how much syntactic information is available as inputs, recent work on syntactic linearization can be classified into free word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), which orders a bag of words without syntactic constraints, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by"
N18-1088,D15-1041,1,0.919347,"overed by UD guidelines.1 Our annotation includes tokenization, part-of-speech (POS) tags, and (labeled) Universal Dependencies. We characterize the disagreements among our annotators and find that consistent annotation is still challenging to deliver even with the extended guidelines. Based on these annotations, we nonetheless designed a pipeline to parse raw tweets into Universal Dependencies. Our pipeline includes: a bidirectional LSTM (bi-LSTM) tokenizer, a word cluster–enhanced POS tagger (following Owoputi et al., 2013), and a stack LSTM parser with character-based word representations (Ballesteros et al., 2015), which we refer to as our “baseline” parser. To overcome the noise in our annotated We study the problem of analyzing tweets with Universal Dependencies (UD; Nivre et al., 2016). We extend the UD guidelines to cover special constructions in tweets that affect tokenization, part-ofspeech tagging, and labeled dependencies. Using the extended guidelines, we create a new tweet treebank for English (T WEEBANK V 2) that is four times larger than the (unlabeled) T WEEBANK V 1 introduced by Kong et al. (2014). We characterize the disagreements between our annotators and show that it is challenging to"
N18-1088,D16-1211,1,0.907114,"Missing"
N18-1088,J92-4003,0,0.519758,"Missing"
N18-1088,de-marneffe-etal-2014-universal,0,0.0901755,"Missing"
N18-1088,R13-1026,0,0.167981,"Missing"
N18-1088,D16-1180,1,0.877777,"e updated version of Twokenizer from Owoputi et al. (2013). 13 http://nlp.stanford.edu/data/glove.twitter. 27B.zip 971 System Kong et al. (2014) Dozat et al. (2017) Ballesteros et al. (2015) Ensemble (20) Distillation (α = 1.0) Distillation (α = 0.9) Distillation w/ exploration UAS 81.4 81.8 80.2 83.4 81.8 82.0 82.1 LAS 76.9 77.7 75.7 79.4 77.6 77.8 77.9 Kt/s 0.3 1.7 2.3 0.2 2.3 2.3 2.3 tems above (Table 6). Distillation. The shortcoming of the 20-parser ensemble is, of course, that it requires twenty times the runtime of a single greedy parser, making it the slowest system in our comparison. Kuncoro et al. (2016) proposed the distillation of 20 greedy transition-based parser into a single graphbased parser; they transformed the votes of the ensemble into a structured loss function. However, as Kuncoro et al. pointed out, it is not straightforward to use a structured loss in a transition-based parsing algorithm. Because fast runtime is so important for NLP on social media, we introduce a new way to distill our greedy ensemble into a single transition-based parser (the first such attempt, to our knowledge). Our approach applies techniques from Hinton et al. (2015) and Kim and Rush (2016) to parsing. Not"
N18-1088,K17-3002,0,0.103467,"ination of UD_English-EWT and T WEE BANK V 2 training sets. Gold-standard tokenization and automatic POS tags are used. Automatic POS tags are assigned with 5-fold jackknifing. Hyperparameters are tuned on the T WEEBANK V 2 development set. Unlabeled attachment score and labeled attachment score (including punctuation) are reported. All the experiments were run on a Xeon E5-2670 2.6 GHz machine. Reimers and Gurevych (2017) and others have 12 We use the updated version of Twokenizer from Owoputi et al. (2013). 13 http://nlp.stanford.edu/data/glove.twitter. 27B.zip 971 System Kong et al. (2014) Dozat et al. (2017) Ballesteros et al. (2015) Ensemble (20) Distillation (α = 1.0) Distillation (α = 0.9) Distillation w/ exploration UAS 81.4 81.8 80.2 83.4 81.8 82.0 82.1 LAS 76.9 77.7 75.7 79.4 77.6 77.8 77.9 Kt/s 0.3 1.7 2.3 0.2 2.3 2.3 2.3 tems above (Table 6). Distillation. The shortcoming of the 20-parser ensemble is, of course, that it requires twenty times the runtime of a single greedy parser, making it the slowest system in our comparison. Kuncoro et al. (2016) proposed the distillation of 20 greedy transition-based parser into a single graphbased parser; they transformed the votes of the ensemble int"
N18-1088,K17-3001,0,0.199799,"a tweet. We therefore treat the whole expression as non-syntactic, including assigning the other (X) part of speech to both RT and @coldplay, attaching the at-mention to RT with the discourse label and the colon to RT with the punct(uation) label, and attaching RT to the predicate of the following sentence. course while they used parataxis. In referential URLs, we use list (following the precedent of UD_English-EWT) while they used dep. Our choice of discourse for sentiment emoticons is inspired by the observation that emoticons are annotated as discourse by UD_English-EWT; Sanguinetti et al. (2017) used the same relation for the emoticons. Retweet constructions and truncated words were not explicitly touched by Sanguinetti et al. (2017). Judging from the released treebank8 , the RT marker, at-mention, and colon in the retweet construction are all attached to the predicate of the following sentence with dep, vocative:mention and punct. We expect that the official UD guidelines will eventually adopt standards for these constructions so the treebanks can be harmonized. Constructions handled by UD. A number of constructions that are especially common in tweets are handled by UD conventions:"
N18-1088,N13-1037,0,0.0477398,"Missing"
N18-1088,P16-1101,0,0.076965,"Missing"
N18-1088,P14-5010,0,0.00286153,"s the tokenization results, 11 Manual annotation was done with Arborator (Gerdes, 2013), a web platform for drawing dependency trees. 970 System Stanford CoreNLP Twokenizer UDPipe v1.2 our bi-LSTM tokenizer System Stanford CoreNLP Owoputi et al., 2013 (greedy) Owoputi et al., 2013 (CRF) Ma and Hovy, 2016 F1 97.3 94.6 97.4 98.3 Table 3: Tokenizer comparison on the T WEEBANK V 2 test set. Table 4: POS tagger comparison on gold-standard tokens in the T WEEBANK V 2 test set. Tokenization System Stanford CoreNLP our bi-LSTM tokenizer (§3.1) compared to other available tokenizers. Stanford CoreNLP (Manning et al., 2014) and Twokenizer (O’Connor et al., 2010)12 are rule-based systems and were not adapted to the UD tokenization scheme. The UDPipe v1.2 (Straka and Straková, 2017) model was re-trained on the same data as our system. Compared with UDPipe, we use an LSTM instead of a GRU in our model and we also use a larger size for hidden units (64 vs. 20), which has stronger representational power. Our bi-LSTM tokenizer achieves the best accuracy among all these tokenizers. These results speak to the value of statistical modeling in tokenization for informal texts. 3.2 Accuracy 90.6 93.7 94.6 92.5 F1 92.3 93.3"
N18-1088,J93-2004,0,0.062664,"to distill an ensemble of 20 transition-based parsers into a single one. Our parser achieves an improvement of 2.2 in LAS over the un-ensembled baseline and outperforms parsers that are state-ofthe-art on other treebanks in both accuracy and speed. 1 Noah A. Smith University of Washington Introduction NLP for social media messages is challenging, requiring domain adaptation and annotated datasets (e.g., treebanks) for training and evaluation. Pioneering work by Foster et al. (2011) annotated 7,630 tokens’ worth of tweets according to the phrase-structure conventions of the Penn Treebank (PTB; Marcus et al., 1993), enabling conversion to Stanford Dependencies. Kong et al. (2014) further studied the challenges in annotating tweets and 1 We developed our treebank independently of a similar effort for Italian tweets (Sanguinetti et al., 2017). See §2.5 for a comparison. 965 Proceedings of NAACL-HLT 2018, pages 965–975 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Their aim was the rapid development of a dependency parser for tweets, and to that end they contributed a new annotated corpus, T WEEBANK, consisting of 12,149 tokens. Their annotations added unlabeled"
N18-1088,W13-3711,0,0.0175481,"rimental results. Our preliminary results showed that our model trained on the combination of UD_English-EWT and T WEEBANK V 2 outperformed the one trained only on the UD_EnglishEWT or T WEEBANK V 2, consistent with previous work on dialect treebank parsing (Wang et al., 2017). So we trained our tokenizer on the training portion of T WEEBANK V 2 combined with the UD_English-EWT training set and tested on the T WEEBANK V 2 test set. We report F1 scores, combining precision and recall for token identification. Table 3 shows the tokenization results, 11 Manual annotation was done with Arborator (Gerdes, 2013), a web platform for drawing dependency trees. 970 System Stanford CoreNLP Twokenizer UDPipe v1.2 our bi-LSTM tokenizer System Stanford CoreNLP Owoputi et al., 2013 (greedy) Owoputi et al., 2013 (CRF) Ma and Hovy, 2016 F1 97.3 94.6 97.4 98.3 Table 3: Tokenizer comparison on the T WEEBANK V 2 test set. Table 4: POS tagger comparison on gold-standard tokens in the T WEEBANK V 2 test set. Tokenization System Stanford CoreNLP our bi-LSTM tokenizer (§3.1) compared to other available tokenizers. Stanford CoreNLP (Manning et al., 2014) and Twokenizer (O’Connor et al., 2010)12 are rule-based systems a"
N18-1088,P11-2008,1,0.897627,"Missing"
N18-1088,C12-1059,0,0.0394777,"Missing"
N18-1088,N13-1039,1,0.950015,"Missing"
N18-1088,Q13-1033,0,0.0408296,"Missing"
N18-1088,D14-1162,0,0.0814147,"Missing"
N18-1088,D17-1256,0,0.437481,"Missing"
N18-1088,petrov-etal-2012-universal,0,0.0767282,"Missing"
N18-1088,P15-1119,1,0.752001,"ggested by Schneider et al. (2013), fairly close to Yamada and Matsumoto (2003) dependencies (without labels). Both annotation efforts were highly influenced by the PTB, whose guidelines have good grammatical coverage on newswire. However, when it comes to informal, unedited, user-generated text, the guidelines may leave many annotation decisions unspecified. Universal Dependencies (Nivre et al., 2016, UD) were introduced to enable consistent annotation across different languages. To allow such consistency, UD was designed to be adaptable to different genres (Wang et al., 2017) and languages (Guo et al., 2015; Ammar et al., 2016). We propose that analyzing the syntax of tweets can benefit from such adaptability. In this paper, we introduce a new English tweet treebank of 55,607 tokens that follows the UD guidelines, but also contends with social media-specific challenges that were not covered by UD guidelines.1 Our annotation includes tokenization, part-of-speech (POS) tags, and (labeled) Universal Dependencies. We characterize the disagreements among our annotators and find that consistent annotation is still challenging to deliver even with the extended guidelines. Based on these annotations, we"
N18-1088,D17-1035,0,0.0171475,"hich should help mitigate the challenge of spelling variation. We encourage the reader to refer their paper for more details about the model. In our initial experiments, we train our parser on the combination of UD_English-EWT and T WEE BANK V 2 training sets. Gold-standard tokenization and automatic POS tags are used. Automatic POS tags are assigned with 5-fold jackknifing. Hyperparameters are tuned on the T WEEBANK V 2 development set. Unlabeled attachment score and labeled attachment score (including punctuation) are reported. All the experiments were run on a Xeon E5-2670 2.6 GHz machine. Reimers and Gurevych (2017) and others have 12 We use the updated version of Twokenizer from Owoputi et al. (2013). 13 http://nlp.stanford.edu/data/glove.twitter. 27B.zip 971 System Kong et al. (2014) Dozat et al. (2017) Ballesteros et al. (2015) Ensemble (20) Distillation (α = 1.0) Distillation (α = 0.9) Distillation w/ exploration UAS 81.4 81.8 80.2 83.4 81.8 82.0 82.1 LAS 76.9 77.7 75.7 79.4 77.6 77.8 77.9 Kt/s 0.3 1.7 2.3 0.2 2.3 2.3 2.3 tems above (Table 6). Distillation. The shortcoming of the 20-parser ensemble is, of course, that it requires twenty times the runtime of a single greedy parser, making it the slowe"
N18-1088,D11-1141,0,0.495674,"Missing"
N18-1088,W17-6526,0,0.480518,"oth accuracy and speed. 1 Noah A. Smith University of Washington Introduction NLP for social media messages is challenging, requiring domain adaptation and annotated datasets (e.g., treebanks) for training and evaluation. Pioneering work by Foster et al. (2011) annotated 7,630 tokens’ worth of tweets according to the phrase-structure conventions of the Penn Treebank (PTB; Marcus et al., 1993), enabling conversion to Stanford Dependencies. Kong et al. (2014) further studied the challenges in annotating tweets and 1 We developed our treebank independently of a similar effort for Italian tweets (Sanguinetti et al., 2017). See §2.5 for a comparison. 965 Proceedings of NAACL-HLT 2018, pages 965–975 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Their aim was the rapid development of a dependency parser for tweets, and to that end they contributed a new annotated corpus, T WEEBANK, consisting of 12,149 tokens. Their annotations added unlabeled dependencies to a portion of the data annotated with POS tags by Gimpel et al. (2011) and Owoputi et al. (2013) after rule-based tokenization (O’Connor et al., 2010). Kong et al. also contributed a system for parsing; we defer th"
N18-1088,D16-1139,0,0.0215692,"r comparison. Kuncoro et al. (2016) proposed the distillation of 20 greedy transition-based parser into a single graphbased parser; they transformed the votes of the ensemble into a structured loss function. However, as Kuncoro et al. pointed out, it is not straightforward to use a structured loss in a transition-based parsing algorithm. Because fast runtime is so important for NLP on social media, we introduce a new way to distill our greedy ensemble into a single transition-based parser (the first such attempt, to our knowledge). Our approach applies techniques from Hinton et al. (2015) and Kim and Rush (2016) to parsing. Note that training a transition-based parser typically involves the transformation of the training data into a sequence of “oracle” state-action pairs. Let q(a |s) denote the distilled model’s probability of an action a given parser state s; let p(a |s) be the probability under the ensemble (i.e., the average of the 20 separately-trained ensemble members). To train the distilled model, we minimize the interpolation between their distillation loss and the conventional log loss: XX argminq α −p(a |si ) · log q(a |si ) Table 6: Dependency parser comparison on T WEE BANK V 2 test set,"
N18-1088,Q16-1023,0,0.108829,"Missing"
N18-1088,W13-2307,1,0.929727,"Missing"
N18-1088,D14-1108,1,0.404607,"uti et al., 2013), and a stack LSTM parser with character-based word representations (Ballesteros et al., 2015), which we refer to as our “baseline” parser. To overcome the noise in our annotated We study the problem of analyzing tweets with Universal Dependencies (UD; Nivre et al., 2016). We extend the UD guidelines to cover special constructions in tweets that affect tokenization, part-ofspeech tagging, and labeled dependencies. Using the extended guidelines, we create a new tweet treebank for English (T WEEBANK V 2) that is four times larger than the (unlabeled) T WEEBANK V 1 introduced by Kong et al. (2014). We characterize the disagreements between our annotators and show that it is challenging to deliver consistent annotation due to ambiguity in understanding and explaining tweets. Nonetheless, using the new treebank, we build a pipeline system to parse raw tweets into UD. To overcome annotation noise without sacrificing computational efficiency, we propose a new method to distill an ensemble of 20 transition-based parsers into a single one. Our parser achieves an improvement of 2.2 in LAS over the un-ensembled baseline and outperforms parsers that are state-ofthe-art on other treebanks in bot"
N18-1088,K17-3009,0,0.0304172,"oreNLP Twokenizer UDPipe v1.2 our bi-LSTM tokenizer System Stanford CoreNLP Owoputi et al., 2013 (greedy) Owoputi et al., 2013 (CRF) Ma and Hovy, 2016 F1 97.3 94.6 97.4 98.3 Table 3: Tokenizer comparison on the T WEEBANK V 2 test set. Table 4: POS tagger comparison on gold-standard tokens in the T WEEBANK V 2 test set. Tokenization System Stanford CoreNLP our bi-LSTM tokenizer (§3.1) compared to other available tokenizers. Stanford CoreNLP (Manning et al., 2014) and Twokenizer (O’Connor et al., 2010)12 are rule-based systems and were not adapted to the UD tokenization scheme. The UDPipe v1.2 (Straka and Straková, 2017) model was re-trained on the same data as our system. Compared with UDPipe, we use an LSTM instead of a GRU in our model and we also use a larger size for hidden units (64 vs. 20), which has stronger representational power. Our bi-LSTM tokenizer achieves the best accuracy among all these tokenizers. These results speak to the value of statistical modeling in tokenization for informal texts. 3.2 Accuracy 90.6 93.7 94.6 92.5 F1 92.3 93.3 Table 5: Owoputi et al. (2013) POS tagging performance with automatic tokenization on the T WEEBANK V 2 test set. Experimental results. We tested the POS tagger"
N18-1088,P17-1159,0,0.100495,"d largely following conventions suggested by Schneider et al. (2013), fairly close to Yamada and Matsumoto (2003) dependencies (without labels). Both annotation efforts were highly influenced by the PTB, whose guidelines have good grammatical coverage on newswire. However, when it comes to informal, unedited, user-generated text, the guidelines may leave many annotation decisions unspecified. Universal Dependencies (Nivre et al., 2016, UD) were introduced to enable consistent annotation across different languages. To allow such consistency, UD was designed to be adaptable to different genres (Wang et al., 2017) and languages (Guo et al., 2015; Ammar et al., 2016). We propose that analyzing the syntax of tweets can benefit from such adaptability. In this paper, we introduce a new English tweet treebank of 55,607 tokens that follows the UD guidelines, but also contends with social media-specific challenges that were not covered by UD guidelines.1 Our annotation includes tokenization, part-of-speech (POS) tags, and (labeled) Universal Dependencies. We characterize the disagreements among our annotators and find that consistent annotation is still challenging to deliver even with the extended guidelines"
N18-1088,W03-3023,0,0.271111,"Missing"
N18-1088,L16-1262,0,\N,Missing
P06-2010,W05-0627,1,0.831541,"based method use a large number of hand-craft diverse features, from word, POS, syntax and semantics, NER, etc. The standard features with polynomial kernel gets the best performance. The reason is that the arbitrary binary combination among features implicated by the polynomial kernel is useful to SRL. We believe that combining the two methods can perform better. In order to make full use of the syntactic information and the standard flat features, we present a composite kernel between hybrid kernel (Khybrid ) and standard features with polynomial Stage 4: A rule-based post-processing stage (Liu et al., 2005) is used to handle some unmatched arguments with constituents, such as AM-MOD, AM-NEG. 5.1.4 Classifier We use the Voted Perceptron (Freund and Schapire, 1998) algorithm as the kernel machine. The performance of the Voted Perceptron is close to, but not as good as, the performance of SVM on the same problem, while saving computation time and programming effort significantly. SVM is too slow to finish our experiments for tuning parameters. The Voted Perceptron is a binary classifier. In order to handle multi-classification problems, we adopt the one vs. others strategy and select the one with t"
P06-2010,P98-1013,0,0.0108697,"l for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004) are proposed and explored in NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model th"
P06-2010,W04-2412,0,0.0431802,"Missing"
P06-2010,J93-2004,0,0.0306892,"hybrid convolution tree kernel, Khybrid . The aim of our experiments is to verify the effectiveness of our hybrid convolution tree kernel and and its combination with the standard flat features. Since the size of a parse tree is not constant, we normalize K(T1 , T2 ) by dividing it by p K(T1 , T1 ) · K(T2 , T2 ) 5.1 Experimental Setting 5.1.1 Corpus We use the benchmark corpus provided by CoNLL-2005 SRL shared task (Carreras and M`arquez, 2005) provided corpus as our training, development, and test sets. The data consist of sections of the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1993), with information on predicate-argument structures extracted from the PropBank corpus (Palmer et al., 2005). We followed the standard partition used in syntactic parsing: sections 02-21 for training, section 24 for development, and section 23 for test. In addition, the test set of the shared task includes three sections of the Brown corpus. Table 2 provides counts of sentences, tokens, annotated propositions, and arguments in the four data sets. 4.3 Comparison with Previous Work It would be interesting to investigate the differences between our method and the feature-based methods. The basic"
P06-2010,W05-0620,0,0.262008,"Missing"
P06-2010,P04-1043,0,0.365021,"and Duffy, 2001) provide an elegant kernel-based solution to implicitly explore tree structure features by directly computing the similarity between two trees. In addition, some machine learning algorithms with dual form, such as Perceptron and Support Vector Machines (SVM) (Cristianini and Shawe-Taylor, 2000), which do not need know the exact presentation of objects and only need compute their kernel functions during the process of learning and prediction. They can be well used as learning algorithms in the kernel-based methods. They are named kernel machines. In this paper, we decompose the Moschitti (2004)’s predicate-argument feature (PAF) kernel into a Path kernel and a Constituent Structure kerIntroduction In the last few years there has been increasing interest in Semantic Role Labeling (SRL). It is currently a well defined task with a substantial body of work and comparative evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows"
P06-2010,A00-2018,0,0.0820962,"Missing"
P06-2010,W03-0423,0,0.026918,"sentation in our feature space is more robust than the Parse Tree Path feature in the flat feature set since the Path feature is sensitive to small changes of the parse trees and it also does not maintain the hierarchical information of a parse tree. Sentences Tokens Propositions Arguments Train 39,832 950,028 90,750 239,858 Devel 1,346 32,853 3,248 8,346 tWSJ 2,416 56,684 5,267 14,077 tBrown 426 7,159 804 2,177 Table 2: Counts on the data set The preprocessing modules used in CONLL2005 include an SVM based POS tagger (Gim´enez and M`arquez, 2003), Charniak (2000)’s full syntactic parser, and Chieu and Ng (2003)’s Named Entity recognizer. 5.1.2 Evaluation The system is evaluated with respect to precision, recall, and Fβ=1 of the predicted arguments. P recision (p) is the proportion of arguments predicted by a system which are correct. Recall (r) is the proportion of correct arguments which are predicted by a system. Fβ=1 computes the harmonic mean of precision and recall, which is the final measure to evaluate the performances of systems. It is formulated as: Fβ=1 = 2pr/(p + r). srl-eval.pl2 is the official program of the CoNLL-2005 SRL shared task to evaluate a system performance. It is also worth c"
P06-2010,J05-1004,0,0.316121,"d a Constituent Structure kerIntroduction In the last few years there has been increasing interest in Semantic Role Labeling (SRL). It is currently a well defined task with a substantial body of work and comparative evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al., 2005). The PropBank defines 6 main arguments, Arg0 is the Agent, Arg1 is Patient, etc. ArgMmay indicate adjunct arguments, such as Locative, Temporal. Many researchers (Gildea and Jurafsky, 2002; Pradhan et al., 2005a) use feature-based methods 1 http://www.cs.unt.edu/∼rada/senseval/senseval3/ 73 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73–80, c Sydney, July 2006. 2006 Association for Computational Linguistics nel, and then compose them into a hybrid convolution tree kernel. Our hybrid kernel method using Voted Perceptron kernel machine outperforms the PAF kernel in"
P06-2010,P05-1072,0,0.24175,"tive evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al., 2005). The PropBank defines 6 main arguments, Arg0 is the Agent, Arg1 is Patient, etc. ArgMmay indicate adjunct arguments, such as Locative, Temporal. Many researchers (Gildea and Jurafsky, 2002; Pradhan et al., 2005a) use feature-based methods 1 http://www.cs.unt.edu/∼rada/senseval/senseval3/ 73 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73–80, c Sydney, July 2006. 2006 Association for Computational Linguistics nel, and then compose them into a hybrid convolution tree kernel. Our hybrid kernel method using Voted Perceptron kernel machine outperforms the PAF kernel in the development sets of CoNLL-2005 SRL shared task. In addition, the final composing kernel between hybrid convolution tree kernel and standard features’ polynomial kernel outperforms each of them individually."
P06-2010,P04-1054,0,0.121072,"hod. Section 5 shows the experimental results. We conclude our work in Section 6. 2 Many kernel functions have been proposed in machine learning community and have been applied to NLP study. In particular, Haussler (1999) and Watkins (1999) proposed the best-known convolution kernels for a discrete structure. In the context of convolution kernels, more and more kernels for restricted syntaxes or specific domains, such as string kernel for text categorization (Lodhi et al., 2002), tree kernel for syntactic parsing (Collins and Duffy, 2001), kernel for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004) are proposed and explored in NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features"
P06-2010,C04-1197,0,0.0455194,"ing was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model the syntactic structured information. It is sensitive to small changes of the syntactic structure features. This can give rise to a data sparseness problem and prevent the learning algorithms from generalizing unseen data well. As an alternative to the standard feature-based methods, kernel-based methods have been proposed to implicitly explore features in a highdimension space by directly calculating the simi"
P06-2010,W05-0639,0,0.113832,"Missing"
P06-2010,J02-3001,0,0.676474,"al body of work and comparative evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al., 2005). The PropBank defines 6 main arguments, Arg0 is the Agent, Arg1 is Patient, etc. ArgMmay indicate adjunct arguments, such as Locative, Temporal. Many researchers (Gildea and Jurafsky, 2002; Pradhan et al., 2005a) use feature-based methods 1 http://www.cs.unt.edu/∼rada/senseval/senseval3/ 73 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73–80, c Sydney, July 2006. 2006 Association for Computational Linguistics nel, and then compose them into a hybrid convolution tree kernel. Our hybrid kernel method using Voted Perceptron kernel machine outperforms the PAF kernel in the development sets of CoNLL-2005 SRL shared task. In addition, the final composing kernel between hybrid convolution tree kernel and standard features’ polynomial kernel outperforms each"
P06-2010,P02-1031,0,0.0644078,"cate-Constituent related features parse tree path from the predicate to the constituent the relative position of the constituent and the predicate, before or after the nodes number on the parse tree path some part on the parse tree path the clause layers from the constituent to the predicate Table 1: Standard flat features However, to find relevant features is, as usual, a complex task. In addition, according to the description of the standard features, we can see that the syntactic features, such as Path, Path Length, bulk large among all features. On the other hand, the previous researches (Gildea and Palmer, 2002; Punyakanok et al., 2005) have also recognized the 74 Firstly, a parse tree T can be represented by a vector of integer counts of each sub-tree type (regardless of its ancestors): Φ(T ) = (# of sub-trees of type 1, . . . , # of sub-trees of type i, . . . , # of sub-trees of type n) This results in a very high dimension since the number of different subtrees is exponential to the tree’s size. Thus it is computationally infeasible to use the feature vector Φ(T ) directly. To solve this problem, we introduce the tree kernel function which is able to calculate the dot product between the above hi"
P06-2010,W04-3212,0,0.270004,"ent Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model the syntactic structured information. It is sensitive to small changes of the syntactic structure features. This can give rise to a data sparseness problem and prevent the learning algorit"
P06-2010,W04-3211,0,\N,Missing
P06-2010,C98-1013,0,\N,Missing
P07-1026,P98-1013,0,0.0102809,"nvolution tree kernel on the data set of the CoNLL-2005 SRL shared task. The remainder of the paper is organized as follows: Section 2 reviews the previous work and Section 3 discusses our grammar-driven convolution tree kernel. Section 4 shows the experimental results. We conclude our work in Section 5. 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treeba"
P07-1026,W05-0620,0,0.371187,"classification are regarded as two key steps in semantic role labeling. Semantic role identification involves classifying each syntactic element in a sentence into either a semantic argument or a non-argument while semantic role classification involves classifying each semantic argument identified into a specific semantic role. This paper focuses on semantic role classification task with the assumption that the semantic arguments have been identified correctly. Both feature-based and kernel-based learning methods have been studied for semantic role classification (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). In feature-based methods, a flat feature vector is used to represent a predicateargument structure while, in kernel-based methods, a kernel function is used to measure directly the similarity between two predicate-argument structures. As we know, kernel methods are more effective in capturing structured features. Moschitti (2004) and Che et al. (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. The convolution tree kernel takes sub-tree as its feature and counts the number of common sub-trees as the similarity between two predicate-arguments. Th"
P07-1026,A00-2018,0,0.0600695,"Missing"
P07-1026,J05-1004,0,0.0606769,"Missing"
P07-1026,W04-3212,0,0.110658,"ork and Section 3 discusses our grammar-driven convolution tree kernel. Section 4 shows the experimental results. We conclude our work in Section 5. 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treebank/ for the detailed definitions of the grammar tags used in the paper. 2 Some rewrite rules in English grammar are generalizations of others: for example, “N"
P07-1026,P06-1104,1,0.842473,"hods for SRL: as an alternative, kernel methods are more effective in modeling structured objects. This is because a kernel can measure the similarity between two structured objects using the original representation of the objects instead of explicitly enumerating their features. Many kernels have been proposed and applied to the NLP study. In particular, Haussler (1999) proposed the well-known convolution kernels for a discrete structure. In the context of it, more and more kernels for restricted syntaxes or specific domains (Collins and Duffy, 2001; Lodhi et al., 2002; Zelenko et al., 2003; Zhang et al., 2006) are proposed and explored in the NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel for SRL under the framework of convolution tree kernel. He selected portions of syntactic parse trees as predicateargument feature spaces, which include salient substructures of predicate-arguments, to define convolution kernels for the task of semantic role classification. Under the same framework, Che et al. (2006) proposed a hybrid convolution tree kernel, which consists of two individual convolution kernels: a Path kernel and a Constituent Structure kern"
P07-1026,W04-3211,0,\N,Missing
P07-1026,J93-2004,0,\N,Missing
P07-1026,N06-2025,0,\N,Missing
P07-1026,W03-1012,0,\N,Missing
P07-1026,C04-1197,0,\N,Missing
P07-1026,P05-1072,0,\N,Missing
P07-1026,C98-1013,0,\N,Missing
P07-1026,P04-1043,0,\N,Missing
P07-1026,J02-3001,0,\N,Missing
P07-1026,P04-1016,0,\N,Missing
P07-1026,P06-2010,1,\N,Missing
P07-1026,W04-2412,0,\N,Missing
P12-1071,P11-1070,0,0.0244387,"Missing"
P12-1071,W09-1210,0,0.0236959,"Missing"
P12-1071,D08-1092,0,0.0423625,"ect comparison indicates that our approach also outperforms previous work based on treebank conversion. # of Words 0.51 million 0.78 million 1.11 million 0.36 million about 1 million Grammar Phrase structure Phrase structure Dependency structure Phrase structure Phrase structure Table 1: Several publicly available Chinese treebanks. Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). 1 Introduction The scale of available labeled data significantly affects the performance of statistical data-driven models. As a structural classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 ∗ Corresp"
P12-1071,W10-2906,0,0.0128754,"so outperforms previous work based on treebank conversion. # of Words 0.51 million 0.78 million 1.11 million 0.36 million about 1 million Grammar Phrase structure Phrase structure Dependency structure Phrase structure Phrase structure Table 1: Several publicly available Chinese treebanks. Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). 1 Introduction The scale of available labeled data significantly affects the performance of statistical data-driven models. As a structural classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 ∗ Correspondence author: tliu@ir.hit.edu.cn Incorpo"
P12-1071,P05-1022,0,0.228193,"Missing"
P12-1071,A00-2018,0,0.417901,"Missing"
P12-1071,W09-1207,1,0.900322,"Missing"
P12-1071,D09-1060,0,0.078901,"Missing"
P12-1071,P10-1003,0,0.0547958,"s work based on treebank conversion. # of Words 0.51 million 0.78 million 1.11 million 0.36 million about 1 million Grammar Phrase structure Phrase structure Dependency structure Phrase structure Phrase structure Table 1: Several publicly available Chinese treebanks. Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). 1 Introduction The scale of available labeled data significantly affects the performance of statistical data-driven models. As a structural classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 ∗ Correspondence author: tliu@ir.hit.edu.cn Incorporating an increased"
P12-1071,P99-1065,0,0.119653,"In contrast, we experiment with two real large-scale treebanks, and boost the stateof-the-art parsing accuracy using QG features. Second, we explore much richer QG features to fully exploit the knowledge of the source treebank. These features are tailored to the dependency parsing problem. In summary, the present work makes substantial progress in modeling structural annotation inconsistencies with QG features for parsing. Previous work on treebank conversion primarily focuses on converting one grammar formalism of a treebank into another and then conducting a study on the converted treebank (Collins et al., 1999; Xia et al., 2008). The work by Niu et al. (2009) is, to our knowledge, the only study to date that combines the converted treebank with the existing target treebank. They automatically convert the dependency-structure CDT into the phrase-structure style of CTB5 using a statistical constituency parser trained on CTB5. Their experiments show that the combined treebank can significantly improve the performance of constituency parsers. However, their method requires several sophisticated strategies, such as corpus weighting and score interpolation, to reduce the influence of conversion errors. I"
P12-1071,W02-1001,0,0.0341419,"Missing"
P12-1071,W09-1205,0,0.0613185,"Missing"
P12-1071,D11-1044,0,0.0868094,"ions, and the score of a target dependency tree becomes Source Treebank S={(xi, di)}i Train Target Treebank T={(xj, dj)}j Source Parser ParserS Parse Out Parsed Treebank TS={(xj, djS)}j Target Treebank with Source Annotations T+S={(xj, djS, dj)}j Train Target Parser ParserT Score(x, t, d′ , d) =Scorebs (x, t, d) Figure 3: Framework of our approach. +Scoreqg (x, t, d′ , d) portion of d′ , and the construction of d can be inspired by arbitrary substructures of d′ . To date, QGs have been successfully applied to various tasks, such as word alignment (Smith and Eisner, 2006), machine translation (Gimpel and Smith, 2011), question answering (Wang et al., 2007), and sentence simplification (Woodsend and Lapata, 2011). In the present work, we utilize the idea of the QG for the exploitation of multiple monolingual treebanks. The key idea is to let the parse tree of one style inspire the parsing process of another style. Different from a MT process, our problem consid678 The first part corresponds to the baseline model, whereas the second part is affected by the source tree d′ and can be rewritten as Scoreqg (x, t, d′ , d) = wqg · fqg (x, t, d′ , d) where fqg (.) denotes the QG features. We expect the QG features"
P12-1071,D09-1127,0,0.176739,"Missing"
P12-1071,P09-1059,0,0.148157,". We conduct extensive experiments using CDT as the source treebank to enhance two target treebanks (CTB5 and CTB6). Results show that our approach can significantly boost state-of-the-art parsing accuracy. Moreover, an indirect comparison indicates that our ap2 CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). Notably, converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules. 676 The present work is primarily inspired by Jiang et al. (2009) and Smith and Eisner (2009). Jiang et al. (2009) improve the performance of word segmentation and part-of-speech (POS) tagging on CTB5 using another large-scale corpus of different annotation standards (People’s Daily). Their framework is similar to ours. However, handling syntactic annotation inconsistencies is significantly more challenging in our case of parsing. Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. The first part of their work is closely connected with our work, but with a few important differences. First, they conduct simulated exper"
P12-1071,P10-1001,0,0.129047,"en models. As a structural classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 ∗ Correspondence author: tliu@ir.hit.edu.cn Incorporating an increased number of global features, such as third-order features in graph-based parsers, slightly affects parsing accuracy (Koo and Collins, 2010; Li et al., 2011). 1 The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT) (Liu et al., 2006). The Sinica treebank (Chen et al., 2003) and the Tsinghua Chinese Treebank (TCT) (Qiang, 2004) can be similarly exploited with our proposed approach, which w"
P12-1071,P08-1068,0,0.10649,"Missing"
P12-1071,D11-1109,1,0.951214,"ral classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 ∗ Correspondence author: tliu@ir.hit.edu.cn Incorporating an increased number of global features, such as third-order features in graph-based parsers, slightly affects parsing accuracy (Koo and Collins, 2010; Li et al., 2011). 1 The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT) (Liu et al., 2006). The Sinica treebank (Chen et al., 2003) and the Tsinghua Chinese Treebank (TCT) (Qiang, 2004) can be similarly exploited with our proposed approach, which we leave as future"
P12-1071,D08-1017,0,0.185678,"ch as corpus weighting and score interpolation, to reduce the influence of conversion errors. Instead of using the noisy converted treebank as additional training data, our approach allows the QGenhanced parsing models to softly learn the systematic inconsistencies based on QG features, making our approach simpler and more robust. Our approach is also intuitively related to stacked learning (SL), a machine learning framework that has recently been applied to dependency parsing to integrate two main-stream parsing models, i.e., graph-based and transition-based models (Nivre and McDonald, 2008; Martins et al., 2008). However, the SL framework trains two parsers on the same treebank and therefore does not need to consider the problem of annotation inconsistencies. 3 Dependency Parsing Given an input sentence x = w0 w1 ...wn and its POS tag sequence t = t0 t1 ...tn , the goal of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m, l) : 0 ≤ h ≤ n, 0 < m ≤ n, l ∈ L}, where (h, m, l) indicates an directed arc from the head word (also called father) wh to the modifier (also called child or dependent) wm with a dependency label l, and L is the label set. We omit the l"
P12-1071,E06-1011,0,0.0258575,"the highest scoring tree from a directed graph. To guarantee the efficiency of the decoding algorithms, the score of a dependency tree is factored into the scores of some small parts (subtrees). Scorebs (x, t, d) = wbs · fbs (x, t, d) X = wpart · fpart (x, t, p) h g h h m s dependency m m grandparent sibling Figure 2: Scoring parts used in our graph-based parsing models. • The first-order model (O1) only incorporates dependency parts (McDonald et al., 2005), and requires O(n3 ) parsing time. • The second-order model using only sibling parts (O2sib) includes both dependency and sibling parts (McDonald and Pereira, 2006), and needs O(n3 ) parsing time. • The second-order model (O2) uses all the scoring parts in Figure 2 (Koo and Collins, 2010). The time complexity of the decoding algorithm is O(n4 ).4 For the O2 model, the score function is rewritten as: X Scorebs (x, t, d) = wdep · fdep (x, t, h, m) {(h,m)}⊆d + X wsib · fsib (x, t, h, s, m) {(h,s),(h,m)}⊆d + X wgrd · fgrd (x, t, g, h, m) {(g,h),(h,m)}⊆d where fdep (.), fsib (.) and fgrd (.) correspond to the features for the three kinds of scoring parts. We adopt the standard features following Li et al. (2011). For the O1 and O2sib models, the above formula"
P12-1071,P05-1012,0,0.141192,"earch, we adopt the graph-based parsing models for their state-of-the-art performance in a variety of languages.3 Graph-based models view the problem as finding the highest scoring tree from a directed graph. To guarantee the efficiency of the decoding algorithms, the score of a dependency tree is factored into the scores of some small parts (subtrees). Scorebs (x, t, d) = wbs · fbs (x, t, d) X = wpart · fpart (x, t, p) h g h h m s dependency m m grandparent sibling Figure 2: Scoring parts used in our graph-based parsing models. • The first-order model (O1) only incorporates dependency parts (McDonald et al., 2005), and requires O(n3 ) parsing time. • The second-order model using only sibling parts (O2sib) includes both dependency and sibling parts (McDonald and Pereira, 2006), and needs O(n3 ) parsing time. • The second-order model (O2) uses all the scoring parts in Figure 2 (Koo and Collins, 2010). The time complexity of the decoding algorithm is O(n4 ).4 For the O2 model, the score function is rewritten as: X Scorebs (x, t, d) = wdep · fdep (x, t, h, m) {(h,m)}⊆d + X wsib · fsib (x, t, h, s, m) {(h,s),(h,m)}⊆d + X wgrd · fgrd (x, t, g, h, m) {(g,h),(h,m)}⊆d where fdep (.), fsib (.) and fgrd (.) corre"
P12-1071,P09-1006,0,0.330964,"these treebanks contain rich human knowledge on the Chinese syntax, thereby having a great deal of common ground. Therefore, exploiting multiple treebanks is very attractive for boosting parsing accuracy. Figure 1 gives an example with different an675 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 675–684, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics OBJ NMOD NMOD ROOT w0 VV NN CC NN 促进1 贸易2 和3 工业4 promote v trade n and c industry n ROOT VOB proach also outperforms the treebank conversion approach of Niu et al. (2009). 2 Related Work LAD COO Figure 1: Example with annotations from CTB5 (upper) and CDT (under). notations from CTB5 and CDT.2 This example illustrates that the two treebanks annotate coordination constructions differently. In CTB5, the last noun is the head, whereas the first noun is the head in CDT. One natural idea for multiple treebank exploitation is treebank conversion. First, the annotations in the source treebank are converted into the style of the target treebank. Then, both the converted treebank and the target treebank are combined. Finally, the combined treebank are used to train a b"
P12-1071,P08-1108,0,0.119102,"phisticated strategies, such as corpus weighting and score interpolation, to reduce the influence of conversion errors. Instead of using the noisy converted treebank as additional training data, our approach allows the QGenhanced parsing models to softly learn the systematic inconsistencies based on QG features, making our approach simpler and more robust. Our approach is also intuitively related to stacked learning (SL), a machine learning framework that has recently been applied to dependency parsing to integrate two main-stream parsing models, i.e., graph-based and transition-based models (Nivre and McDonald, 2008; Martins et al., 2008). However, the SL framework trains two parsers on the same treebank and therefore does not need to consider the problem of annotation inconsistencies. 3 Dependency Parsing Given an input sentence x = w0 w1 ...wn and its POS tag sequence t = t0 t1 ...tn , the goal of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m, l) : 0 ≤ h ≤ n, 0 < m ≤ n, l ∈ L}, where (h, m, l) indicates an directed arc from the head word (also called father) wh to the modifier (also called child or dependent) wm with a dependency label l, and L is the l"
P12-1071,W03-3017,0,0.0228695,"2sib models, the above formula is modified by deactivating the extra parts. 4 Dependency Parsing with QG Features p⊆d where p is a scoring part which contains one or more dependencies of d, and fbs (.) denotes the basic parsing features, as opposed to the QG features. Figure 2 lists the scoring parts used in our work, where g, h, m, and s, are word indices. We implement three parsing models of varying strengths in capturing features to better understand the effect of the proposed QG features. 3 Our approach can equally be applied to transition-based parsing models (Yamada and Matsumoto, 2003; Nivre, 2003) with minor modifications. 677 Smith and Eisner (2006) propose the QG for machine translation (MT) problems, allowing greater syntactic divergences between the two languages. Given a source sentence x′ and its syntactic tree d′ , a QG defines a monolingual grammar that generates translations of x′ , which can be denoted by p(x, d, a|x′ , d′ ), where x and d refer to a translation and its parse, and a is a cross-language alignment. Under a QG, any portion of d can be aligned to any 4 We use the coarse-to-fine strategy to prune the search space, which largely accelerates the decoding procedure ("
P12-1071,W06-3104,0,0.253692,"by deactivating the extra parts. 4 Dependency Parsing with QG Features p⊆d where p is a scoring part which contains one or more dependencies of d, and fbs (.) denotes the basic parsing features, as opposed to the QG features. Figure 2 lists the scoring parts used in our work, where g, h, m, and s, are word indices. We implement three parsing models of varying strengths in capturing features to better understand the effect of the proposed QG features. 3 Our approach can equally be applied to transition-based parsing models (Yamada and Matsumoto, 2003; Nivre, 2003) with minor modifications. 677 Smith and Eisner (2006) propose the QG for machine translation (MT) problems, allowing greater syntactic divergences between the two languages. Given a source sentence x′ and its syntactic tree d′ , a QG defines a monolingual grammar that generates translations of x′ , which can be denoted by p(x, d, a|x′ , d′ ), where x and d refer to a translation and its parse, and a is a cross-language alignment. Under a QG, any portion of d can be aligned to any 4 We use the coarse-to-fine strategy to prune the search space, which largely accelerates the decoding procedure (Koo and Collins, 2010). Target Side h Syntactic Struct"
P12-1071,D09-1086,0,0.108437,"xperiments using CDT as the source treebank to enhance two target treebanks (CTB5 and CTB6). Results show that our approach can significantly boost state-of-the-art parsing accuracy. Moreover, an indirect comparison indicates that our ap2 CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). Notably, converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules. 676 The present work is primarily inspired by Jiang et al. (2009) and Smith and Eisner (2009). Jiang et al. (2009) improve the performance of word segmentation and part-of-speech (POS) tagging on CTB5 using another large-scale corpus of different annotation standards (People’s Daily). Their framework is similar to ours. However, handling syntactic annotation inconsistencies is significantly more challenging in our case of parsing. Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. The first part of their work is closely connected with our work, but with a few important differences. First, they conduct simulated experiments on one treebank by ma"
P12-1071,D07-1003,0,0.0514962,"ee becomes Source Treebank S={(xi, di)}i Train Target Treebank T={(xj, dj)}j Source Parser ParserS Parse Out Parsed Treebank TS={(xj, djS)}j Target Treebank with Source Annotations T+S={(xj, djS, dj)}j Train Target Parser ParserT Score(x, t, d′ , d) =Scorebs (x, t, d) Figure 3: Framework of our approach. +Scoreqg (x, t, d′ , d) portion of d′ , and the construction of d can be inspired by arbitrary substructures of d′ . To date, QGs have been successfully applied to various tasks, such as word alignment (Smith and Eisner, 2006), machine translation (Gimpel and Smith, 2011), question answering (Wang et al., 2007), and sentence simplification (Woodsend and Lapata, 2011). In the present work, we utilize the idea of the QG for the exploitation of multiple monolingual treebanks. The key idea is to let the parse tree of one style inspire the parsing process of another style. Different from a MT process, our problem consid678 The first part corresponds to the baseline model, whereas the second part is affected by the source tree d′ and can be rewritten as Scoreqg (x, t, d′ , d) = wqg · fqg (x, t, d′ , d) where fqg (.) denotes the QG features. We expect the QG features to encourage or penalize certain scorin"
P12-1071,D11-1038,0,0.0160561,"rget Treebank T={(xj, dj)}j Source Parser ParserS Parse Out Parsed Treebank TS={(xj, djS)}j Target Treebank with Source Annotations T+S={(xj, djS, dj)}j Train Target Parser ParserT Score(x, t, d′ , d) =Scorebs (x, t, d) Figure 3: Framework of our approach. +Scoreqg (x, t, d′ , d) portion of d′ , and the construction of d can be inspired by arbitrary substructures of d′ . To date, QGs have been successfully applied to various tasks, such as word alignment (Smith and Eisner, 2006), machine translation (Gimpel and Smith, 2011), question answering (Wang et al., 2007), and sentence simplification (Woodsend and Lapata, 2011). In the present work, we utilize the idea of the QG for the exploitation of multiple monolingual treebanks. The key idea is to let the parse tree of one style inspire the parsing process of another style. Different from a MT process, our problem consid678 The first part corresponds to the baseline model, whereas the second part is affected by the source tree d′ and can be rewritten as Scoreqg (x, t, d′ , d) = wqg · fqg (x, t, d′ , d) where fqg (.) denotes the QG features. We expect the QG features to encourage or penalize certain scoring parts in the target side according to the source tree d"
P12-1071,W03-3023,0,0.0296712,"al. (2011). For the O1 and O2sib models, the above formula is modified by deactivating the extra parts. 4 Dependency Parsing with QG Features p⊆d where p is a scoring part which contains one or more dependencies of d, and fbs (.) denotes the basic parsing features, as opposed to the QG features. Figure 2 lists the scoring parts used in our work, where g, h, m, and s, are word indices. We implement three parsing models of varying strengths in capturing features to better understand the effect of the proposed QG features. 3 Our approach can equally be applied to transition-based parsing models (Yamada and Matsumoto, 2003; Nivre, 2003) with minor modifications. 677 Smith and Eisner (2006) propose the QG for machine translation (MT) problems, allowing greater syntactic divergences between the two languages. Given a source sentence x′ and its syntactic tree d′ , a QG defines a monolingual grammar that generates translations of x′ , which can be denoted by p(x, d, a|x′ , d′ ), where x and d refer to a translation and its parse, and a is a cross-language alignment. Under a QG, any portion of d can be aligned to any 4 We use the coarse-to-fine strategy to prune the search space, which largely accelerates the decodi"
P12-1071,P08-1101,0,0.261243,"ling the annotation inconsistencies using transformation patterns (TP). The TPs are used to compose quasi-synchronous grammar (QG) features, such that the knowledge of the source treebank can inspire the target parser to build better trees. We conduct extensive experiments using CDT as the source treebank to enhance two target treebanks (CTB5 and CTB6). Results show that our approach can significantly boost state-of-the-art parsing accuracy. Moreover, an indirect comparison indicates that our ap2 CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). Notably, converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules. 676 The present work is primarily inspired by Jiang et al. (2009) and Smith and Eisner (2009). Jiang et al. (2009) improve the performance of word segmentation and part-of-speech (POS) tagging on CTB5 using another large-scale corpus of different annotation standards (People’s Daily). Their framework is similar to ours. However, handling syntactic annotation inconsistencies is significantly more challenging in our case of pa"
P12-1071,D08-1059,0,0.394174,"ling the annotation inconsistencies using transformation patterns (TP). The TPs are used to compose quasi-synchronous grammar (QG) features, such that the knowledge of the source treebank can inspire the target parser to build better trees. We conduct extensive experiments using CDT as the source treebank to enhance two target treebanks (CTB5 and CTB6). Results show that our approach can significantly boost state-of-the-art parsing accuracy. Moreover, an indirect comparison indicates that our ap2 CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). Notably, converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules. 676 The present work is primarily inspired by Jiang et al. (2009) and Smith and Eisner (2009). Jiang et al. (2009) improve the performance of word segmentation and part-of-speech (POS) tagging on CTB5 using another large-scale corpus of different annotation standards (People’s Daily). Their framework is similar to ours. However, handling syntactic annotation inconsistencies is significantly more challenging in our case of pa"
P12-1071,P11-2033,0,0.0579417,"m) ◦ dist(h, m) to form new features. Corpus Train Dev Test PD 281,311 5,000 10,000 CDT 55,500 1,500 3,000 CTB5 16,091 803 1,910 352 348 CTB5X 18,104 CTB6 22,277 1,762 2,556 Models without QG with QG O2 86.13 86.44 (+0.31, p = 0.06) O2sib 85.63 86.17 (+0.54, p = 0.003) O1 83.16 84.40 (+1.24, p < 10−5 ) Li11 86.18 — Z&N11 86.00 — Table 3: Data used in this work (in sentence number). Table 4: Parsing accuracy (UAS) comparison on CTB5test with gold-standard POS tags. Li11 refers to the second-order graph-based model of Li et al. (2011), whereas Z&N11 is the feature-rich transition-based model of Zhang and Nivre (2011). We adopt unlabeled attachment score (UAS) as the primary evaluation metric. We also use Root accuracy (RA) and complete match rate (CM) to give more insights. All metrics exclude punctuation. We adopt Dan Bikel’s randomized parsing evaluation comparator for significance test (Noreen, 1989).7 For all models used in current work (POS tagging and parsing), we adopt averaged perceptron to train the feature weights (Collins, 2002). We train each model for 10 iterations and select the parameters that perform best on the development set. 5.1 Preliminaries This subsection describes how we project th"
P12-1071,P11-1156,0,0.0529369,"Missing"
P12-1071,W09-1201,0,\N,Missing
P12-2003,W04-3224,0,0.0174745,"arsing algorithms specifically to Chinese. Number of in files sentences tokens Train 2,083 46,572 1,039,942 Dev 160 2,079 59,955 Test 205 2,796 81,578 Total 2,448 51,447 1,181,475 Table 2: Statistics for Chinese TreeBank (CTB) 7.0 data. 2 Methodology We compared seven popular open source constituent and dependency parsers, focusing on both accuracy and parsing speed. We hope that our analysis will help end-users select a suitable method for parsing to Stanford dependencies in their own applications. 2.1 Parsers We considered four constituent parsers. They are: Berkeley (Petrov et al., 2006), Bikel (2004), Charniak (2000) and Stanford (Klein and Manning, 2003) chineseFactored, which is also the default used by Stanford dependencies. The three dependency parsers are: MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010)2 and MSTParser (McDonald and Pereira, 2006). Table 1 has more information. 2 A second-order MST parser (with the speed optimization). 12 2.2 Corpus We used the latest Chinese TreeBank (CTB) 7.0 in all experiments.3 CTB 7.0 is larger and has more sources (e.g., web text), compared to previous versions. We split the data into train/development/test sets (see Table 2), with gold wor"
P12-2003,C10-1011,0,0.0646138,"eBank (CTB) 7.0 data. 2 Methodology We compared seven popular open source constituent and dependency parsers, focusing on both accuracy and parsing speed. We hope that our analysis will help end-users select a suitable method for parsing to Stanford dependencies in their own applications. 2.1 Parsers We considered four constituent parsers. They are: Berkeley (Petrov et al., 2006), Bikel (2004), Charniak (2000) and Stanford (Klein and Manning, 2003) chineseFactored, which is also the default used by Stanford dependencies. The three dependency parsers are: MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010)2 and MSTParser (McDonald and Pereira, 2006). Table 1 has more information. 2 A second-order MST parser (with the speed optimization). 12 2.2 Corpus We used the latest Chinese TreeBank (CTB) 7.0 in all experiments.3 CTB 7.0 is larger and has more sources (e.g., web text), compared to previous versions. We split the data into train/development/test sets (see Table 2), with gold word segmentation, following the guidelines suggested in documentation. 2.3 Settings Every parser was run with its own default options. However, since the default classifier used by MaltParser is libsvm (Chang and Lin, 2"
P12-2003,W06-2920,0,0.135444,"Missing"
P12-2003,cer-etal-2010-parsing,0,0.0769393,"Missing"
P12-2003,W09-2307,0,0.205061,"ndencies (de Marneffe and Manning, 2008) provide a simple description of relations between pairs of words in a sentence. This semantically-oriented representation is intuitive and easy to apply, requiring little linguistic expertise. Consequently, Stanford dependencies are widely used: in biomedical text mining (Kim et al., 2009), as well as in textual entailment (Androutsopoulos and Malakasiotis, 2010), information extraction (Wu and Weld, 2010; Banko et al., 2007) and sentiment analysis (Meena and Prabhakar, 2007). In addition to English, there is a Chinese version of Stanford dependencies (Chang et al., 2009), which is also useful for many applications, such as Chinese sentiment analysis (Wu et al., 2011; Wu et al., 2009; Zhuang et al., 2006) and relation extraction (Huang et al., 2008). Figure 1 shows a sample constituent parse tree and the corresponding Stanford dependencies for a sentence in Chinese. Although there are several variants of Stanford dependencies for English,1 so far only a basic version (i.e, dependency tree structures) is available for Chinese. Stanford dependencies were originally obtained from constituent trees, using rules (de Marneffe et al., 2006). But as dependency parsing"
P12-2003,P05-1022,0,0.271574,"Missing"
P12-2003,A00-2018,0,0.645992,"fically to Chinese. Number of in files sentences tokens Train 2,083 46,572 1,039,942 Dev 160 2,079 59,955 Test 205 2,796 81,578 Total 2,448 51,447 1,181,475 Table 2: Statistics for Chinese TreeBank (CTB) 7.0 data. 2 Methodology We compared seven popular open source constituent and dependency parsers, focusing on both accuracy and parsing speed. We hope that our analysis will help end-users select a suitable method for parsing to Stanford dependencies in their own applications. 2.1 Parsers We considered four constituent parsers. They are: Berkeley (Petrov et al., 2006), Bikel (2004), Charniak (2000) and Stanford (Klein and Manning, 2003) chineseFactored, which is also the default used by Stanford dependencies. The three dependency parsers are: MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010)2 and MSTParser (McDonald and Pereira, 2006). Table 1 has more information. 2 A second-order MST parser (with the speed optimization). 12 2.2 Corpus We used the latest Chinese TreeBank (CTB) 7.0 in all experiments.3 CTB 7.0 is larger and has more sources (e.g., web text), compared to previous versions. We split the data into train/development/test sets (see Table 2), with gold word segmentation, f"
P12-2003,W08-1301,0,0.169556,"Missing"
P12-2003,de-marneffe-etal-2006-generating,0,0.0727123,"Missing"
P12-2003,W09-1401,0,0.0111235,"ord dependencies. Figure 1: A sample Chinese constituent parse tree and its corresponding Stanford dependencies for the sentence China (中国) encourages (鼓励) private (民营) entrepreneurs (企业家) to invest (投资) in national (国家) infrastructure (基础) construction (建设). Introduction Stanford dependencies (de Marneffe and Manning, 2008) provide a simple description of relations between pairs of words in a sentence. This semantically-oriented representation is intuitive and easy to apply, requiring little linguistic expertise. Consequently, Stanford dependencies are widely used: in biomedical text mining (Kim et al., 2009), as well as in textual entailment (Androutsopoulos and Malakasiotis, 2010), information extraction (Wu and Weld, 2010; Banko et al., 2007) and sentiment analysis (Meena and Prabhakar, 2007). In addition to English, there is a Chinese version of Stanford dependencies (Chang et al., 2009), which is also useful for many applications, such as Chinese sentiment analysis (Wu et al., 2011; Wu et al., 2009; Zhuang et al., 2006) and relation extraction (Huang et al., 2008). Figure 1 shows a sample constituent parse tree and the corresponding Stanford dependencies for a sentence in Chinese. Although th"
P12-2003,P03-1054,0,0.13024,"ituent parse tree and the corresponding Stanford dependencies for a sentence in Chinese. Although there are several variants of Stanford dependencies for English,1 so far only a basic version (i.e, dependency tree structures) is available for Chinese. Stanford dependencies were originally obtained from constituent trees, using rules (de Marneffe et al., 2006). But as dependency parsing technologies mature (K¨ubler et al., 2009), they offer increasingly attractive alternatives that eliminate the need for an intermediate representation. Cer et al. (2010) reported that Stanford’s implementation (Klein and Manning, 2003) underperforms other constituent 1 nlp.stanford.edu/software/dependencies_manual.pdf 11 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 11–16, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Type Constituent Dependency Parser Version Algorithm URL Berkeley Bikel Charniak Stanford MaltParser Mate MSTParser 1.1 1.2 Nov. 2009 2.0 1.6.1 2.0 0.5 PCFG PCFG PCFG Factored Arc-Eager 2nd-order MST MST code.google.com/p/berkeleyparser www.cis.upenn.edu/˜dbikel/download.html www.cog.brown.edu/˜mj/Software.htm nlp.stan"
P12-2003,P10-1001,0,0.0421575,"mj/Software.htm nlp.stanford.edu/software/lex-parser.shtml maltparser.org code.google.com/p/mate-tools sourceforge.net/projects/mstparser Table 1: Basic information for the seven parsers included in our experiments. parsers, for English, on both accuracy and speed. Their thorough investigation also showed that constituent parsers systematically outperform parsing directly to Stanford dependencies. Nevertheless, relative standings could have changed in recent years: dependency parsers are now significantly more accurate, thanks to advances like the high-order maximum spanning tree (MST) model (Koo and Collins, 2010) for graph-based dependency parsing (McDonald and Pereira, 2006). Therefore, we deemed it important to re-evaluate the performance of constituent and dependency parsers. But the main purpose of our work is to apply the more sophisticated dependency parsing algorithms specifically to Chinese. Number of in files sentences tokens Train 2,083 46,572 1,039,942 Dev 160 2,079 59,955 Test 205 2,796 81,578 Total 2,448 51,447 1,181,475 Table 2: Statistics for Chinese TreeBank (CTB) 7.0 data. 2 Methodology We compared seven popular open source constituent and dependency parsers, focusing on both accurac"
P12-2003,P08-1068,0,0.156884,"Missing"
P12-2003,D11-1109,1,0.835978,"e difference between their accuracies is not statistically significant (p &gt; 0.05).9 Table 4 highlights performance (F1 scores) for the most frequent relation labels. Mate does better on most relations, noun compound modifiers (nn) and adjectival modifiers (amod) in particular; and the Berkeley parser is better at root and dep.10 Mate seems to excel at short-distance dependencies, possibly because it uses more local features (even with a second-order model) than the Berkeley parser, whose PCFG can capture longer-distance rules. Since POS-tags are especially informative of Chinese dependencies (Li et al., 2011), we harmonized training and test data, using 10-way jackknifing (see §2.4). This method is more robust than training a 7 One (small) factor contributing to the difference between the two languages is that in the Chinese setup we stop with basic Stanford dependencies — there is no penalty for further conversion; another is not using discriminative reranking for Chinese. 8 sites.google.com/site/sancl2012/home/shared-task 9 For LAS, p ≈ 0.11; and for UAS, p ≈ 0.25, according to www.cis.upenn.edu/˜dbikel/download/compare.pl 6 10 ilk.uvt.nl/conll/software/eval.pl 13 An unmatched (default) relation"
P12-2003,E06-1011,0,0.0748711,"maltparser.org code.google.com/p/mate-tools sourceforge.net/projects/mstparser Table 1: Basic information for the seven parsers included in our experiments. parsers, for English, on both accuracy and speed. Their thorough investigation also showed that constituent parsers systematically outperform parsing directly to Stanford dependencies. Nevertheless, relative standings could have changed in recent years: dependency parsers are now significantly more accurate, thanks to advances like the high-order maximum spanning tree (MST) model (Koo and Collins, 2010) for graph-based dependency parsing (McDonald and Pereira, 2006). Therefore, we deemed it important to re-evaluate the performance of constituent and dependency parsers. But the main purpose of our work is to apply the more sophisticated dependency parsing algorithms specifically to Chinese. Number of in files sentences tokens Train 2,083 46,572 1,039,942 Dev 160 2,079 59,955 Test 205 2,796 81,578 Total 2,448 51,447 1,181,475 Table 2: Statistics for Chinese TreeBank (CTB) 7.0 data. 2 Methodology We compared seven popular open source constituent and dependency parsers, focusing on both accuracy and parsing speed. We hope that our analysis will help end-use"
P12-2003,P08-1108,0,0.220944,"Missing"
P12-2003,nivre-etal-2006-maltparser,0,0.070012,"Statistics for Chinese TreeBank (CTB) 7.0 data. 2 Methodology We compared seven popular open source constituent and dependency parsers, focusing on both accuracy and parsing speed. We hope that our analysis will help end-users select a suitable method for parsing to Stanford dependencies in their own applications. 2.1 Parsers We considered four constituent parsers. They are: Berkeley (Petrov et al., 2006), Bikel (2004), Charniak (2000) and Stanford (Klein and Manning, 2003) chineseFactored, which is also the default used by Stanford dependencies. The three dependency parsers are: MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010)2 and MSTParser (McDonald and Pereira, 2006). Table 1 has more information. 2 A second-order MST parser (with the speed optimization). 12 2.2 Corpus We used the latest Chinese TreeBank (CTB) 7.0 in all experiments.3 CTB 7.0 is larger and has more sources (e.g., web text), compared to previous versions. We split the data into train/development/test sets (see Table 2), with gold word segmentation, following the guidelines suggested in documentation. 2.3 Settings Every parser was run with its own default options. However, since the default classifier used by MaltParser is lib"
P12-2003,P06-1055,0,0.13597,"isticated dependency parsing algorithms specifically to Chinese. Number of in files sentences tokens Train 2,083 46,572 1,039,942 Dev 160 2,079 59,955 Test 205 2,796 81,578 Total 2,448 51,447 1,181,475 Table 2: Statistics for Chinese TreeBank (CTB) 7.0 data. 2 Methodology We compared seven popular open source constituent and dependency parsers, focusing on both accuracy and parsing speed. We hope that our analysis will help end-users select a suitable method for parsing to Stanford dependencies in their own applications. 2.1 Parsers We considered four constituent parsers. They are: Berkeley (Petrov et al., 2006), Bikel (2004), Charniak (2000) and Stanford (Klein and Manning, 2003) chineseFactored, which is also the default used by Stanford dependencies. The three dependency parsers are: MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010)2 and MSTParser (McDonald and Pereira, 2006). Table 1 has more information. 2 A second-order MST parser (with the speed optimization). 12 2.2 Corpus We used the latest Chinese TreeBank (CTB) 7.0 in all experiments.3 CTB 7.0 is larger and has more sources (e.g., web text), compared to previous versions. We split the data into train/development/test sets (see Table 2),"
P12-2003,D10-1001,0,0.0423331,"open source parsers — four constituent and three dependency — for generating Stanford dependencies in Chinese. Mate, a high-order MST dependency parser, with lemmatization and jackknifed POS-tags, appears most accurate; but Berkeley’s faster constituent parser, with jointly-inferred tags, is statistically no worse. This outcome is different from English, where constituent parsers systematically outperform direct methods. Though Mate scored higher overall, Berkeley’s parser was better at recovering longer-distance relations, suggesting that a combined approach could perhaps work better still (Rush et al., 2010, §4.2). Acknowledgments We thank Daniel Cer, for helping us replicate the English experimental setup and for suggesting that we explore jackknifing methods, and the anonymous reviewers, for valuable comments. Supported in part by the National Natural Science Foundation of China (NSFC) via grant 61133012, the National “863” Major Project grant 2011AA01A207, and the National “863” Leading Technology Research Project grant 2012AA011102. 13 w3.msi.vxu.se/˜nivre/research/Penn2Malt.html Second author gratefully acknowledges the continued help and support of his advisor, Dan Jurafsky, and of the Def"
P12-2003,N10-1091,0,0.0796641,"Missing"
P12-2003,N03-1033,0,0.011567,"its own default options. However, since the default classifier used by MaltParser is libsvm (Chang and Lin, 2011) with a polynomial kernel, it may be too slow for training models on all of CTB 7.0 training data in acceptable time. Therefore, we also tested this particular parser with the faster liblinear (Fan et al., 2008) classifier. All experiments were performed on a machine with Intel’s Xeon E5620 2.40GHz CPU and 24GB RAM. 2.4 Features Unlike constituent parsers, dependency models require exogenous part-of-speech (POS) tags, both in training and in inference. We used the Stanford tagger (Toutanova et al., 2003) v3.1, with the MEMM model,4 in combination with 10-way jackknifing.5 Word lemmas — which are generalizations of words — are another feature known to be useful for dependency parsing. Here we lemmatized each Chinese word down to its last character, since — in contrast to English — a Chinese word’s suffix often carries that word’s core sense (Tseng et al., 2005). For example, bicycle (自 行 车 ), car (汽 车 ) and train (火车 车) are all various kinds of vehicle (车). 3 www.ldc.upenn.edu/Catalog/CatalogEntry.jsp? catalogId=LDC2010T07 4 nlp.stanford.edu/software/tagger.shtml 5 Training sentences in each f"
P12-2003,I05-3005,0,0.0197428,"d on a machine with Intel’s Xeon E5620 2.40GHz CPU and 24GB RAM. 2.4 Features Unlike constituent parsers, dependency models require exogenous part-of-speech (POS) tags, both in training and in inference. We used the Stanford tagger (Toutanova et al., 2003) v3.1, with the MEMM model,4 in combination with 10-way jackknifing.5 Word lemmas — which are generalizations of words — are another feature known to be useful for dependency parsing. Here we lemmatized each Chinese word down to its last character, since — in contrast to English — a Chinese word’s suffix often carries that word’s core sense (Tseng et al., 2005). For example, bicycle (自 行 车 ), car (汽 车 ) and train (火车 车) are all various kinds of vehicle (车). 3 www.ldc.upenn.edu/Catalog/CatalogEntry.jsp? catalogId=LDC2010T07 4 nlp.stanford.edu/software/tagger.shtml 5 Training sentences in each fold were tagged using a model based on the other nine folds; development and test sentences were tagged using a model based on all ten of the training folds. Type Constituent Dependency Parser Dev UAS LAS Test UAS LAS 82.0 79.4 77.8 76.9 76.0 77.3 82.8 78.8 82.9 80.0 78.3 77.3 76.3 78.0 83.1 78.9 Berkeley Bikel Charniak Stanford MaltParser (liblinear) MaltParse"
P12-2003,P10-1013,0,0.0281581,"e sentence China (中国) encourages (鼓励) private (民营) entrepreneurs (企业家) to invest (投资) in national (国家) infrastructure (基础) construction (建设). Introduction Stanford dependencies (de Marneffe and Manning, 2008) provide a simple description of relations between pairs of words in a sentence. This semantically-oriented representation is intuitive and easy to apply, requiring little linguistic expertise. Consequently, Stanford dependencies are widely used: in biomedical text mining (Kim et al., 2009), as well as in textual entailment (Androutsopoulos and Malakasiotis, 2010), information extraction (Wu and Weld, 2010; Banko et al., 2007) and sentiment analysis (Meena and Prabhakar, 2007). In addition to English, there is a Chinese version of Stanford dependencies (Chang et al., 2009), which is also useful for many applications, such as Chinese sentiment analysis (Wu et al., 2011; Wu et al., 2009; Zhuang et al., 2006) and relation extraction (Huang et al., 2008). Figure 1 shows a sample constituent parse tree and the corresponding Stanford dependencies for a sentence in Chinese. Although there are several variants of Stanford dependencies for English,1 so far only a basic version (i.e, dependency tree stru"
P12-2003,D09-1159,0,0.0350361,"Missing"
P12-2003,D11-1123,0,0.0367332,"Missing"
P12-2003,P11-2033,0,0.190951,"Missing"
P12-2003,C10-2013,0,\N,Missing
P12-2003,W08-2121,0,\N,Missing
P12-2003,D07-1096,0,\N,Missing
P13-1013,P12-1110,0,0.289433,"syntax trees (Figure 1(b)). With richer information than word-level trees, this form of parse trees can be useful for all the aforementioned Chinese NLP applications. (d) modifier-noun. Figure 2: Inner word structures of “库存 (repertory)”,“考古 (archaeology)”, “科技 (science and technology)” and “败类 (degenerate)”. art joint segmenter and POS tagger, and our baseline word-based parser. Our word annotations lead to further improvements to the joint system, especially for phrase-structure parsing accuracy. Our parser work falls in line with recent work of joint segmentation, POS tagging and parsing (Hatori et al., 2012; Li and Zhou, 2012; Qian and Liu, 2012). Compared with related work, our model gives the best published results for joint segmentation and POS tagging, as well as joint phrase-structure parsing on standard CTB5 evaluations. With linear-time complexity, our parser is highly efficient, processing over 30 sentences per second with a beam size of 16. An open release of the parser is freely available at http://sourceforge.net/projects/zpar/, version 0.6. With regard to task of parsing itself, an important advantage of the character-level syntax trees is that they allow word segmentation, part-of-s"
P13-1013,P09-1058,0,0.179797,"Missing"
P13-1013,D12-1132,0,0.68711,"1(b)). With richer information than word-level trees, this form of parse trees can be useful for all the aforementioned Chinese NLP applications. (d) modifier-noun. Figure 2: Inner word structures of “库存 (repertory)”,“考古 (archaeology)”, “科技 (science and technology)” and “败类 (degenerate)”. art joint segmenter and POS tagger, and our baseline word-based parser. Our word annotations lead to further improvements to the joint system, especially for phrase-structure parsing accuracy. Our parser work falls in line with recent work of joint segmentation, POS tagging and parsing (Hatori et al., 2012; Li and Zhou, 2012; Qian and Liu, 2012). Compared with related work, our model gives the best published results for joint segmentation and POS tagging, as well as joint phrase-structure parsing on standard CTB5 evaluations. With linear-time complexity, our parser is highly efficient, processing over 30 sentences per second with a beam size of 16. An open release of the parser is freely available at http://sourceforge.net/projects/zpar/, version 0.6. With regard to task of parsing itself, an important advantage of the character-level syntax trees is that they allow word segmentation, part-of-speech (POS) tagging"
P13-1013,P11-1141,0,0.689122,"c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (constituent) trees, adding recursive structures of characters for words. We manually annotate the structures of 37,382 words, which cover the entire CTB5. Using these annotations, we transform CTB-style constituent trees into character-level trees (Figure 1(b)). Our word structure corpus, together with a set of tools to transform CTB-style trees into character-level trees, is released at https://github.com/zhangmeishan/wordstructures. Our annotation work is in line with the work of Vadas and Curran (2007) and Li (2011), which provide extended annotations of Penn Treebank (PTB) noun phrases and CTB words (on the morphological level), respectively. NN-rNN-r NN-r NN-r NN-bNN-b NN-iNN-i NN-b NN-b NN-i NN-i NN-lNN-l NN-l NN-l VV-bVV-b VV-iVV-i VV-b VV-b VV-i VV-i 库 库 存 存 考 考 古 古 (repository) (saving) (investigate) (ancient) 库 存 考 古 (repository) (saving) (investigate) (ancient) 库 存 考 古 (repository) (saving) (saving) (investigate) (investigate)(ancient) (ancient) (repository) (a) subject-predicate. (b) verb-object. NN-rNN-r NN-cNN-c NNc NN-r NN-r NN-c NN-bNN-b NN-iNN-i NN-bNN-b NN-iNN-i NN-b NN-b NN-b NN-i NN-i NN"
P13-1013,W03-1025,0,0.0140119,"for joint segmentation and POS tagging, as well as joint phrase-structure parsing on standard CTB5 evaluations. With linear-time complexity, our parser is highly efficient, processing over 30 sentences per second with a beam size of 16. An open release of the parser is freely available at http://sourceforge.net/projects/zpar/, version 0.6. With regard to task of parsing itself, an important advantage of the character-level syntax trees is that they allow word segmentation, part-of-speech (POS) tagging and parsing to be performed jointly, using an efficient CKY-style or shift-reduce algorithm. Luo (2003) exploited this advantage by adding flat word structures without manually annotation to CTB trees, and building a generative character-based parser. Compared to a pipeline system, the advantages of a joint system include reduction of error propagation, and the integration of segmentation, POS tagging and syntax features. With hierarchical structures and head character information, our annotated words are more informative than flat word structures, and hence can bring further improvements to phrase-structure parsing. 2 Word Structures and Syntax Trees The Chinese language is a character-based l"
P13-1013,W12-6304,0,0.0342964,"ore informative than flat word structures, and hence can bring further improvements to phrase-structure parsing. 2 Word Structures and Syntax Trees The Chinese language is a character-based language. Unlike alphabetical languages, Chinese characters convey meanings, and the meaning of most Chinese words takes roots in their character. For example, the word “计算机 (computer)” is composed of the characters “计 (count)”, “算 (calculate)” and “机 (machine)”. An informal name of “computer” is “电脑”, which is composed of “电 (electronic)” and “脑 (brain)”. Chinese words have internal structures (Xue, 2001; Ma et al., 2012). The way characters interact within words can be similar to the way words interact within phrases. Figure 2 shows the structures of the four words “库存 (repertory)”, “ 考古 To analyze word structures in addition to phrase structures, our character-based parser naturally performs joint word segmentation, POS tagging and parsing jointly. Our model is based on the discriminative shift-reduce parser of Zhang and Clark (2009; 2011), which is a state-of-the-art word-based phrase-structure parser for Chinese. We extend their shift-reduce framework, adding more transition actions for word segmentation a"
P13-1013,W04-3236,0,0.0796187,"and morphological-level word structures. 朋友 (friend) 们 (plural) 教育 (education) 界 (field) structures. For each word or subword, we specify its POS and head direction. We use “l”, “r” and “c” to indicate the “left”, “right” and “coordination” head directions, respectively. The “coordination” direction is mostly used in coordination structures, while a very small number of transliteration words, such as “奥巴马 (Obama)” and “洛 杉矶 (Los Angeles)”, have flat structures, and we use “coordination” for their left binarization. For leaf characters, we follow previous work on word segmentation (Xue, 2003; Ng and Low, 2004), and use “b” and “i” to indicate the beginning and nonbeginning characters of a word, respectively. The vast majority of words do not have structural ambiguities. However, the structures of some words may vary according to different POS. For example, “制 服” means “dominate” when it is tagged as a verb, of which the head is the left character; the same word means “uniform dress” when tagged as a noun, of which the head is the right character. Thus the input of the word structure annotation is a word together with its POS. The annotation work was conducted by three persons, with one person annot"
P13-1013,P04-1015,0,0.337377,"erform word segmentation, POS tagging and phrase-structure parsing. To our knowledge, this is the first work to develop a transition-based system that jointly performs the above three tasks. Trained using annotated word structures, our parser also analyzes the internal structures of Chinese words. Our character-based Chinese parsing model is based on the work of Zhang and Clark (2009), which is a transition-based model for lexicalized constituent parsing. They use a beam-search decoder so that the transition action sequence can be globally optimized. The averaged perceptron with early-update (Collins and Roark, 2004) is used to train the model parameters. Their transition system contains four kinds of actions: (1) SHIFT, (2) REDUCE-UNARY, (3) REDUCE-BINARY and (4) TERMINATE. The system can provide binarzied CFG trees in Chomsky Norm Form, and they present a reversible conversion procedure to map arbitrary CFG trees into binarized trees. In this work, we remain consistent with their work, using the head-finding rules of Zhang and Clark (2008), and the same binarization algorithm.1 We apply the same beam-search algorithm for decoding, and employ the averaged perceptron with early-update to train our model."
P13-1013,D10-1082,1,0.605411,"a word is in a tag dictionary, which is collected by extracting all multi-character subwords that occur more than five times in the training corpus. For string features, c0 , c−1 and c−2 represent the current character and its previous two characters, respectively; w−1 and w−2 represent the previous two words to the current character, respectively; t0 , t−1 and t−2 represent the POS tags of the current word and the previous two words, respectively. The string features are used for word segmentation and POS tagging, and are adapted from a state-of-the-art joint segmentation and tagging model (Zhang and Clark, 2010). In summary, our character-based parser contains the word-based features of constituent parser presented in Zhang and Clark (2009), the wordbased and shallow character-based features of joint word segmentation and POS tagging presented in Zhang and Clark (2010), and additionally the deep character-based features that encode word structure information, which are the first presented by this paper. 4 4.1 95 90 90 80 85 70 80 64b 16b 4b 1b 75 70 60 64b 16b 4b 1b 50 40 65 30 0 10 20 30 40 (a) Joint segmentation and POS tagging F-scores. 0 10 20 30 40 (b) Joint constituent parsing F-scores. Figure"
P13-1013,D12-1046,0,0.0993978,"information than word-level trees, this form of parse trees can be useful for all the aforementioned Chinese NLP applications. (d) modifier-noun. Figure 2: Inner word structures of “库存 (repertory)”,“考古 (archaeology)”, “科技 (science and technology)” and “败类 (degenerate)”. art joint segmenter and POS tagger, and our baseline word-based parser. Our word annotations lead to further improvements to the joint system, especially for phrase-structure parsing accuracy. Our parser work falls in line with recent work of joint segmentation, POS tagging and parsing (Hatori et al., 2012; Li and Zhou, 2012; Qian and Liu, 2012). Compared with related work, our model gives the best published results for joint segmentation and POS tagging, as well as joint phrase-structure parsing on standard CTB5 evaluations. With linear-time complexity, our parser is highly efficient, processing over 30 sentences per second with a beam size of 16. An open release of the parser is freely available at http://sourceforge.net/projects/zpar/, version 0.6. With regard to task of parsing itself, an important advantage of the character-level syntax trees is that they allow word segmentation, part-of-speech (POS) tagging and parsing to be pe"
P13-1013,J11-1005,1,0.212902,"Missing"
P13-1013,P11-1139,0,0.0424243,"Missing"
P13-1013,E09-1100,0,0.443661,"Missing"
P13-1013,P07-1031,0,0.0309607,"Linguistics, pages 125–134, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (constituent) trees, adding recursive structures of characters for words. We manually annotate the structures of 37,382 words, which cover the entire CTB5. Using these annotations, we transform CTB-style constituent trees into character-level trees (Figure 1(b)). Our word structure corpus, together with a set of tools to transform CTB-style trees into character-level trees, is released at https://github.com/zhangmeishan/wordstructures. Our annotation work is in line with the work of Vadas and Curran (2007) and Li (2011), which provide extended annotations of Penn Treebank (PTB) noun phrases and CTB words (on the morphological level), respectively. NN-rNN-r NN-r NN-r NN-bNN-b NN-iNN-i NN-b NN-b NN-i NN-i NN-lNN-l NN-l NN-l VV-bVV-b VV-iVV-i VV-b VV-b VV-i VV-i 库 库 存 存 考 考 古 古 (repository) (saving) (investigate) (ancient) 库 存 考 古 (repository) (saving) (investigate) (ancient) 库 存 考 古 (repository) (saving) (saving) (investigate) (investigate)(ancient) (ancient) (repository) (a) subject-predicate. (b) verb-object. NN-rNN-r NN-cNN-c NNc NN-r NN-r NN-c NN-bNN-b NN-iNN-i NN-bNN-b NN-iNN-i NN-b NN-b NN-"
P13-1013,I11-1035,0,0.0680725,"Missing"
P13-1013,O03-4002,0,0.0938348,"f NN-f NN-f and morphological-level word structures. 朋友 (friend) 们 (plural) 教育 (education) 界 (field) structures. For each word or subword, we specify its POS and head direction. We use “l”, “r” and “c” to indicate the “left”, “right” and “coordination” head directions, respectively. The “coordination” direction is mostly used in coordination structures, while a very small number of transliteration words, such as “奥巴马 (Obama)” and “洛 杉矶 (Los Angeles)”, have flat structures, and we use “coordination” for their left binarization. For leaf characters, we follow previous work on word segmentation (Xue, 2003; Ng and Low, 2004), and use “b” and “i” to indicate the beginning and nonbeginning characters of a word, respectively. The vast majority of words do not have structural ambiguities. However, the structures of some words may vary according to different POS. For example, “制 服” means “dominate” when it is tagged as a verb, of which the head is the left character; the same word means “uniform dress” when tagged as a noun, of which the head is the right character. Thus the input of the word structure annotation is a word together with its POS. The annotation work was conducted by three persons, wi"
P13-1013,D08-1059,1,0.600383,"zed constituent parsing. They use a beam-search decoder so that the transition action sequence can be globally optimized. The averaged perceptron with early-update (Collins and Roark, 2004) is used to train the model parameters. Their transition system contains four kinds of actions: (1) SHIFT, (2) REDUCE-UNARY, (3) REDUCE-BINARY and (4) TERMINATE. The system can provide binarzied CFG trees in Chomsky Norm Form, and they present a reversible conversion procedure to map arbitrary CFG trees into binarized trees. In this work, we remain consistent with their work, using the head-finding rules of Zhang and Clark (2008), and the same binarization algorithm.1 We apply the same beam-search algorithm for decoding, and employ the averaged perceptron with early-update to train our model. We make two extensions to their work to enable joint segmentation, POS tagging and phrasestructure parsing from the character level. First, we modify the actions of the transition system for • SHIFT-SEPARATE(t): remove the head character cj from Q, pushing a subword node S0 2 0 cj onto S, assigning S .t = t. Note that the parse tree S0 must correspond to a full-word or a phrase node, and the character cj is the first character of"
P13-1013,W09-3825,1,0.954764,"”, “算 (calculate)” and “机 (machine)”. An informal name of “computer” is “电脑”, which is composed of “电 (electronic)” and “脑 (brain)”. Chinese words have internal structures (Xue, 2001; Ma et al., 2012). The way characters interact within words can be similar to the way words interact within phrases. Figure 2 shows the structures of the four words “库存 (repertory)”, “ 考古 To analyze word structures in addition to phrase structures, our character-based parser naturally performs joint word segmentation, POS tagging and parsing jointly. Our model is based on the discriminative shift-reduce parser of Zhang and Clark (2009; 2011), which is a state-of-the-art word-based phrase-structure parser for Chinese. We extend their shift-reduce framework, adding more transition actions for word segmentation and POS tagging, and defining novel features that capture character information. Even when trained using character-level syntax trees with flat word structures, our joint parser outperforms a strong pipelined baseline that consists of a state-of-the126 VV-lV VV-l VV-bVV-b VV-b VV-b 烧 烧 烧 (burn) 烧(burn) (burn)(burn) AD-lA AD-l AD-bAD-b AD-b AD-b 徒 徒 徒 (vain) 徒(vain) (vain)(vain) V-l VV-b 横 (fiercely) 横 (fiercely) VV-l V"
P13-1013,N07-1051,0,\N,Missing
P13-1106,P02-1051,0,0.0382439,"in, China, 150001 Stanford, CA 94305 mengqiu@cs.stanford.edu car@ir.hit.edu.cn manning@cs.stanford.edu Abstract sentence. Recognizing them provides useful information for phrase detection and word sense disambiguation (e.g., “melody” as in a female name has a different translation from the word “melody” in a musical sense), and can be directly leveraged to improve translation quality (Babych and Hartley, 2003). We can also automatically construct a named entity translation lexicon by annotating and extracting entities from bi-texts, and use it to improve MT performance (Huang and Vogel, 2002; Al-Onaizan and Knight, 2002). Previous work such as Burkett et al. (2010b), Li et al. (2012) and Kim et al. (2012) have also demonstrated that bitexts annotated with NER tags can provide useful additional training sources for improving the performance of standalone monolingual taggers. Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as"
P13-1106,W03-2201,0,0.0168632,"ment and Bilingual Named Entity Recognition Using Dual Decomposition Mengqiu Wang Wanxiang Che Christopher D. Manning Stanford University Harbin Institute of Technology Stanford University Stanford, CA 94305 Harbin, China, 150001 Stanford, CA 94305 mengqiu@cs.stanford.edu car@ir.hit.edu.cn manning@cs.stanford.edu Abstract sentence. Recognizing them provides useful information for phrase detection and word sense disambiguation (e.g., “melody” as in a female name has a different translation from the word “melody” in a musical sense), and can be directly leveraged to improve translation quality (Babych and Hartley, 2003). We can also automatically construct a named entity translation lexicon by annotating and extracting entities from bi-texts, and use it to improve MT performance (Huang and Vogel, 2002; Al-Onaizan and Knight, 2002). Previous work such as Burkett et al. (2010b), Li et al. (2012) and Kim et al. (2012) have also demonstrated that bitexts annotated with NER tags can provide useful additional training sources for improving the performance of standalone monolingual taggers. Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated"
P13-1106,P91-1034,0,0.0787714,"and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines. 1 Because human translation in general preserves semantic equivalence, bi-texts represent two perspectives on the same semantic content (Burkett et al., 2010b). As a result, we can find complementary cues in the two languages that help to disambiguate named entity mentions (Brown et al., 1991). For example, the English word “Jordan” can be either a last name or a country. Without sufficient context it can be difficult to distinguish the two; however, in Chinese, these two senses are disambiguated: “乔丹” as a last name, and “约旦” as a country name. Introduction We study the problem of Named Entity Recognition (NER) in a bilingual context, where the goal is to annotate parallel bi-texts with named entity tags. This is a particularly important problem for machine translation (MT) since entities such as person names, locations, organizations, etc. carry much of the information expressed"
P13-1106,P04-1056,0,0.0200656,"how we enforce agreement between every aligned word 1080 pair. Martins et al. (2011b) presented a new DD method that combines the power of DD with the augmented Lagrangian method. They showed that their method can achieve faster convergence than traditional sub-gradient methods in models with many overlapping components (Martins et al., 2011a). This method is directly applicable to our work. Another promising direction for improving NER performance is in enforcing global label consistency across documents, which is an idea that has been greatly explored in the past (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005). More recently, Rush et al. (2012) and Chieu and Teow (2012) have shown that combining local prediction models with global consistency models, and enforcing agreement via DD is very effective. It is straightforward to incorporate an additional global consistency model into our model for further improvements. Our joint alignment and NER decoding approach is inspired by prior work on improving alignment quality through encouraging agreement between bi-directional models (Liang et al., 2006; DeNero and Macherey, 2011). Instead of enforcing agreement in the alignment space b"
P13-1106,N10-1015,0,0.0814644,"ford.edu car@ir.hit.edu.cn manning@cs.stanford.edu Abstract sentence. Recognizing them provides useful information for phrase detection and word sense disambiguation (e.g., “melody” as in a female name has a different translation from the word “melody” in a musical sense), and can be directly leveraged to improve translation quality (Babych and Hartley, 2003). We can also automatically construct a named entity translation lexicon by annotating and extracting entities from bi-texts, and use it to improve MT performance (Huang and Vogel, 2002; Al-Onaizan and Knight, 2002). Previous work such as Burkett et al. (2010b), Li et al. (2012) and Kim et al. (2012) have also demonstrated that bitexts annotated with NER tags can provide useful additional training sources for improving the performance of standalone monolingual taggers. Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading error"
P13-1106,W10-2906,0,0.160361,"ford.edu car@ir.hit.edu.cn manning@cs.stanford.edu Abstract sentence. Recognizing them provides useful information for phrase detection and word sense disambiguation (e.g., “melody” as in a female name has a different translation from the word “melody” in a musical sense), and can be directly leveraged to improve translation quality (Babych and Hartley, 2003). We can also automatically construct a named entity translation lexicon by annotating and extracting entities from bi-texts, and use it to improve MT performance (Huang and Vogel, 2002; Al-Onaizan and Knight, 2002). Previous work such as Burkett et al. (2010b), Li et al. (2012) and Kim et al. (2012) have also demonstrated that bitexts annotated with NER tags can provide useful additional training sources for improving the performance of standalone monolingual taggers. Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading error"
P13-1106,P10-1065,0,0.127673,"asal constraints are indirectly imposed by entity spans. We also differ in the implementation details, where in their case belief propagation is used in both training and Viterbi inference. Burkett et al. (2010a) presented a supervised learning method for performing joint parsing and word alignment using log-linear models over parse trees and an ITG model over alignment. The model demonstrates performance improvements in both parsing and alignment, but shares the common limitations of other supervised work in that it requires manually annotated bilingual joint parsing and word alignment data. Chen et al. (2010) also tackled the problem of joint alignment and NER. Their method employs a set of heuristic rules to expand a candidate named entity set generated by monolingual taggers, and then rank those candidates using a bilingual named entity dictionary. Our approach differs in that we provide a probabilistic formulation of the problem and do not require pre-existing NE dictionaries. 9 Conclusion We introduced a graphical model that combines two HMM word aligners and two CRF NER taggers into a joint model, and presented a dual decomposition inference method for performing efficient decoding over this"
P13-1106,E09-1020,0,0.0436736,"Missing"
P13-1106,P08-2007,0,0.0200217,"ani, 1993). For alignment experiments, we train two uni1 The exact feature set and the CRF implementation can be found here: http://nlp.stanford.edu/ software/CRF-NER.shtml directional HMM models as our baseline and monolingual alignment models. The parameters of the HMM were initialized by IBM Model 1 using the agreement-based EM training algorithms from Liang et al. (2006). Each model is trained for 2 iterations over a parallel corpus of 12 million English words and Chinese words, almost twice as much data as used in previous work that yields state-of-the-art unsupervised alignment results (DeNero and Klein, 2008; Haghighi et al., 2009; DeNero and Macherey, 2011). Word alignment evaluation is done over the sections of OntoNotes that have matching goldstandard word alignment annotations from GALE Y1Q4 dataset.2 This subset contains 288 documents and 3,391 sentence pairs. We will refer to this subset as wa-subset. This evaluation set is over 20 times larger than the 150 sentences set used in most past evaluations (DeNero and Klein, 2008; Haghighi et al., 2009; DeNero and Macherey, 2011). Alignments input to the B I -NER model are produced by thresholding the averaged posterior probability at 0.5. In joi"
P13-1106,P11-1043,0,0.160678,"4-9 2013. 2013 Association for Computational Linguistics B-ORG I-ORG I-ORG [O] B-LOC O O Xinhua News Agency Beijing Feb 16 e1 e2 e3 e4 e5 e6 f1 f2 f3 f4 f5 f6 新华社 ， 北京 ， 二月 十六 O O B-ORG O B-GPE O Figure 1: Example of NER labels between two word-aligned bilingual parallel sentences. The [O] tag is an example of a wrong tag assignment. The dashed alignment link between e3 and f2 is an example of alignment error. previous applications of the DD method in NLP, where the model typically factors over two components and agreement is to be sought between the two (Rush et al., 2010; Koo et al., 2010; DeNero and Macherey, 2011; Chieu and Teow, 2012), our method decomposes the larger graphical model into many overlapping components where each alignment edge forms a separate factor. We design clique potentials over the alignment-based edges to encourage entity tag agreements. Our method does not require any manual annotation of word alignments or named entities over the bilingual training data. The aforementioned B I -NER model assumes fixed alignment input given by an underlying word aligner. But the entity span and type predictions given by the NER models contain complementary information for correcting alignment e"
P13-1106,D09-1127,0,0.0229834,") are baseline output. distance swapping phenomena. The two unidirectional HMMs also have strong disagreements over the alignments, and the resulting baseline aligner output only recovers two links. If we were to take this alignment as fixed input, most likely we would not be able to recover the error over e11 , but the joint decoding method successfully recovered 4 more links, and indirectly resulted in the NER tagging improvement discussed above. 8 Related Work The idea of employing bilingual resources to improve over monolingual systems has been explored by much previous work. For example, Huang et al. (2009) improved parsing performance using a bilingual parallel corpus. In the NER domain, Li et al. (2012) presented a cyclic CRF model very similar to our B I -NER model, and performed approximate inference using loopy belief propagation. The feature-rich CRF formulation of bilingual edge potentials in their model is much more powerful than our simple PMI-based bilingual edge model. Adding a richer bilingual edge model might well further improve our results, and this is a possible direction for further experimentation. However, a big drawback of this approach is that training such a feature-rich mo"
P13-1106,P12-1073,0,0.193826,"d.edu Abstract sentence. Recognizing them provides useful information for phrase detection and word sense disambiguation (e.g., “melody” as in a female name has a different translation from the word “melody” in a musical sense), and can be directly leveraged to improve translation quality (Babych and Hartley, 2003). We can also automatically construct a named entity translation lexicon by annotating and extracting entities from bi-texts, and use it to improve MT performance (Huang and Vogel, 2002; Al-Onaizan and Knight, 2002). Previous work such as Burkett et al. (2010b), Li et al. (2012) and Kim et al. (2012) have also demonstrated that bitexts annotated with NER tags can provide useful additional training sources for improving the performance of standalone monolingual taggers. Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. We observe that NER label information c"
P13-1106,D10-1125,0,0.131861,", Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics B-ORG I-ORG I-ORG [O] B-LOC O O Xinhua News Agency Beijing Feb 16 e1 e2 e3 e4 e5 e6 f1 f2 f3 f4 f5 f6 新华社 ， 北京 ， 二月 十六 O O B-ORG O B-GPE O Figure 1: Example of NER labels between two word-aligned bilingual parallel sentences. The [O] tag is an example of a wrong tag assignment. The dashed alignment link between e3 and f2 is an example of alignment error. previous applications of the DD method in NLP, where the model typically factors over two components and agreement is to be sought between the two (Rush et al., 2010; Koo et al., 2010; DeNero and Macherey, 2011; Chieu and Teow, 2012), our method decomposes the larger graphical model into many overlapping components where each alignment edge forms a separate factor. We design clique potentials over the alignment-based edges to encourage entity tag agreements. Our method does not require any manual annotation of word alignments or named entities over the bilingual training data. The aforementioned B I -NER model assumes fixed alignment input given by an underlying word aligner. But the entity span and type predictions given by the NER models contain complementary information"
P13-1106,N06-1014,0,0.234913,"ore the distinction between B- and Itags. We report standard NER measures (entity precision (P), recall (R) and F1 score) on the test set. Statistical significance tests are done using the paired bootstrap resampling method (Efron and Tibshirani, 1993). For alignment experiments, we train two uni1 The exact feature set and the CRF implementation can be found here: http://nlp.stanford.edu/ software/CRF-NER.shtml directional HMM models as our baseline and monolingual alignment models. The parameters of the HMM were initialized by IBM Model 1 using the agreement-based EM training algorithms from Liang et al. (2006). Each model is trained for 2 iterations over a parallel corpus of 12 million English words and Chinese words, almost twice as much data as used in previous work that yields state-of-the-art unsupervised alignment results (DeNero and Klein, 2008; Haghighi et al., 2009; DeNero and Macherey, 2011). Word alignment evaluation is done over the sections of OntoNotes that have matching goldstandard word alignment annotations from GALE Y1Q4 dataset.2 This subset contains 288 documents and 3,391 sentence pairs. We will refer to this subset as wa-subset. This evaluation set is over 20 times larger than"
P13-1106,ma-2006-champollion,0,0.027723,"such as the neighborhood constraint proposed by DeNero and Macherey (2011) can be easily integrated into our model. The neighborhood constraint enforces that if fj is aligned to ei , then fj can only be aligned to ei+1 or ei−1 (with a small penalty), but not any other word position. We report results of adding neighborhood constraints to our model in Section 6. 4 Experimental Setup We evaluate on the large OntoNotes (v4.0) corpus (Hovy et al., 2006) which contains manually 1077 annotated NER tags for both Chinese and English. Document pairs are sentence aligned using the Champollion Tool Kit (Ma, 2006). After discarding sentences with no aligned counterpart, a total of 402 documents and 8,249 parallel sentence pairs were used for evaluation. We will refer to this evaluation set as full-set. We use odd-numbered documents as the dev set and evennumbered documents as the blind test set. We did not perform parameter tuning on the dev set to optimize performance, instead we fix the initial learning rate to 0.5 and maximum iterations to 1,000 in all DD experiments. We only use the dev set for model development. The Stanford CRF-based NER tagger was used as the monolingual component in our models"
P13-1106,D11-1022,0,0.0542004,"Missing"
P13-1106,D11-1001,0,0.0193111,"lored an “up-training” mechanism by using the outputs from a strong monolingual model as ground-truth, and simulated a learning environment where a bilingual model is trained to help a “weakened” monolingual model to recover the results of the strong model. It is worth mentioning that since our method does not require additional training and can take pretty much any existing model as “black-box” during decoding, the richer and more accurate bilingual model learned from Burkett et al. (2010b) can be directly plugged into our model. A similar dual decomposition algorithm to ours was proposed by Riedel and McCallum (2011) for biomedical event detection. In their Model 3, the trigger and argument extraction models are reminiscent of the two monolingual CRFs in our model; additional binding agreements are enforced over every protein pair, similar to how we enforce agreement between every aligned word 1080 pair. Martins et al. (2011b) presented a new DD method that combines the power of DD with the augmented Lagrangian method. They showed that their method can achieve faster convergence than traditional sub-gradient methods in models with many overlapping components (Martins et al., 2011a). This method is directl"
P13-1106,1993.eamt-1.1,0,0.272285,"so on, we select the four most commonly seen named entity types for evaluation. They are person, location, organization and GPE. All entities of these four types are converted to the standard BIO format, and background tokens and all other entity types are marked with tag O. When we consider label agreements over aligned word pairs in all bilingual agreement models, we ignore the distinction between B- and Itags. We report standard NER measures (entity precision (P), recall (R) and F1 score) on the test set. Statistical significance tests are done using the paired bootstrap resampling method (Efron and Tibshirani, 1993). For alignment experiments, we train two uni1 The exact feature set and the CRF implementation can be found here: http://nlp.stanford.edu/ software/CRF-NER.shtml directional HMM models as our baseline and monolingual alignment models. The parameters of the HMM were initialized by IBM Model 1 using the agreement-based EM training algorithms from Liang et al. (2006). Each model is trained for 2 iterations over a parallel corpus of 12 million English words and Chinese words, almost twice as much data as used in previous work that yields state-of-the-art unsupervised alignment results (DeNero and"
P13-1106,P05-1045,1,0.0630441,"After discarding sentences with no aligned counterpart, a total of 402 documents and 8,249 parallel sentence pairs were used for evaluation. We will refer to this evaluation set as full-set. We use odd-numbered documents as the dev set and evennumbered documents as the blind test set. We did not perform parameter tuning on the dev set to optimize performance, instead we fix the initial learning rate to 0.5 and maximum iterations to 1,000 in all DD experiments. We only use the dev set for model development. The Stanford CRF-based NER tagger was used as the monolingual component in our models (Finkel et al., 2005). It also serves as a stateof-the-art monolingual baseline for both English and Chinese. For English, we use the default tagger setting from Finkel et al. (2005). For Chinese, we use an improved set of features over the default tagger, which includes distributional similarity features trained on large amounts of nonoverlapping data.1 We train the two CRF models on all portions of the OntoNotes corpus that are annotated with named entity tags, except the parallel-aligned portion which we reserve for development and test purposes. In total, there are about 660 training documents (∼16k sentences)"
P13-1106,D10-1001,0,0.394667,"ags. This is a particularly important problem for machine translation (MT) since entities such as person names, locations, organizations, etc. carry much of the information expressed in the source In this work, we first develop a bilingual NER model (denoted as B I -NER) by embedding two monolingual CRF-based NER models into a larger undirected graphical model, and introduce additional edge factors based on word alignment (WA). Because the new bilingual model contains many cyclic cliques, exact inference is intractable. We employ a dual decomposition (DD) inference algorithm (Bertsekas, 1999; Rush et al., 2010) for performing approximate inference. Unlike most 1073 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1073–1082, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics B-ORG I-ORG I-ORG [O] B-LOC O O Xinhua News Agency Beijing Feb 16 e1 e2 e3 e4 e5 e6 f1 f2 f3 f4 f5 f6 新华社 ， 北京 ， 二月 十六 O O B-ORG O B-GPE O Figure 1: Example of NER labels between two word-aligned bilingual parallel sentences. The [O] tag is an example of a wrong tag assignment. The dashed alignment link between e3 and f2 is an example of alignment error"
P13-1106,D12-1131,0,0.0138707,"Martins et al. (2011b) presented a new DD method that combines the power of DD with the augmented Lagrangian method. They showed that their method can achieve faster convergence than traditional sub-gradient methods in models with many overlapping components (Martins et al., 2011a). This method is directly applicable to our work. Another promising direction for improving NER performance is in enforcing global label consistency across documents, which is an idea that has been greatly explored in the past (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005). More recently, Rush et al. (2012) and Chieu and Teow (2012) have shown that combining local prediction models with global consistency models, and enforcing agreement via DD is very effective. It is straightforward to incorporate an additional global consistency model into our model for further improvements. Our joint alignment and NER decoding approach is inspired by prior work on improving alignment quality through encouraging agreement between bi-directional models (Liang et al., 2006; DeNero and Macherey, 2011). Instead of enforcing agreement in the alignment space based on best sequences found by Viterbi, we could opt to"
P13-1106,P09-1104,0,0.0110565,"t experiments, we train two uni1 The exact feature set and the CRF implementation can be found here: http://nlp.stanford.edu/ software/CRF-NER.shtml directional HMM models as our baseline and monolingual alignment models. The parameters of the HMM were initialized by IBM Model 1 using the agreement-based EM training algorithms from Liang et al. (2006). Each model is trained for 2 iterations over a parallel corpus of 12 million English words and Chinese words, almost twice as much data as used in previous work that yields state-of-the-art unsupervised alignment results (DeNero and Klein, 2008; Haghighi et al., 2009; DeNero and Macherey, 2011). Word alignment evaluation is done over the sections of OntoNotes that have matching goldstandard word alignment annotations from GALE Y1Q4 dataset.2 This subset contains 288 documents and 3,391 sentence pairs. We will refer to this subset as wa-subset. This evaluation set is over 20 times larger than the 150 sentences set used in most past evaluations (DeNero and Klein, 2008; Haghighi et al., 2009; DeNero and Macherey, 2011). Alignments input to the B I -NER model are produced by thresholding the averaged posterior probability at 0.5. In joint NER and alignment ex"
P13-1106,N06-2015,0,0.0268304,"her by having them all agree with the edge cliques. It is also worth noting that since we decode the alignment models with Viterbi inference, additional constraints such as the neighborhood constraint proposed by DeNero and Macherey (2011) can be easily integrated into our model. The neighborhood constraint enforces that if fj is aligned to ei , then fj can only be aligned to ei+1 or ei−1 (with a small penalty), but not any other word position. We report results of adding neighborhood constraints to our model in Section 6. 4 Experimental Setup We evaluate on the large OntoNotes (v4.0) corpus (Hovy et al., 2006) which contains manually 1077 annotated NER tags for both Chinese and English. Document pairs are sentence aligned using the Champollion Tool Kit (Ma, 2006). After discarding sentences with no aligned counterpart, a total of 402 documents and 8,249 parallel sentence pairs were used for evaluation. We will refer to this evaluation set as full-set. We use odd-numbered documents as the dev set and evennumbered documents as the blind test set. We did not perform parameter tuning on the dev set to optimize performance, instead we fix the initial learning rate to 0.5 and maximum iterations to 1,000"
P14-1113,D07-1017,0,0.0489721,"of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chu"
P14-1113,C10-3004,1,0.465681,"uned on a development dataset. 3.3.3 Training Data To learn the projection matrices, we extract training data from a Chinese semantic thesaurus, Tongyi Cilin (Extended) (CilinE for short) which 1202 δ Root 物 object B 昆虫 : 动物 (insect : animal) 蜻蜓 : 动物 (dragonfly : animal) 动物 animal … Sense Code: Bi 昆虫 insect 18 -- … 蜻蜓 : 昆虫 Sense Code: Bi18A 蜻蜓 dragonfly Sense Code: Bi18A06@ A … … Level 3 x Figure 4: In this example, Φk x is located in the circle with center y and radius δ. So y is considered as a hypernym of x. Conversely, y is not a hypernym of x0 . Level 5 06@ CilinE contains 100,093 words (Che et al., 2010).3 CilinE is organized as a hierarchy of five levels, in which the words are linked by hypernym–hyponym relations (right panel, Figure 3). Each word in CilinE has one or more sense codes (some words are polysemous) that indicate its position in the hierarchy. The senses of words in the first level, such as “物 (object)” and “时间 (time),” are very general. The fourth level only has sense codes without real words. Therefore, we extract words in the second, third and fifth levels to constitute hypernym– hyponym pairs (left panel, Figure 3). Note that mapping one hyponym to multiple hypernyms with t"
P14-1113,W09-0215,0,0.0162212,"ance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (T"
P14-1113,D13-1122,1,0.267748,"versely, “dog” is a hyponym of “canine.” As key sources of knowledge, semantic thesauri and ontologies can support many natural language processing applications. However, these semantic resources are limited in its scope and domain, and their manual construction is knowledge intensive and time consuming. Therefore, many researchers Email correspondence. 毛茛科 Ranunculaceae 乌头属 Aconitum Introduction ∗ 生物 organism 植物 plant have attempted to automatically extract semantic relations or to construct taxonomies. A major challenge for this task is the automatic discovery of hypernym-hyponym relations. Fu et al. (2013) propose a distant supervision method to extract hypernyms for entities from multiple sources. The output of their model is a list of hypernyms for a given enity (left panel, Figure 1). However, there usually also exists hypernym–hyponym relations among these hypernyms. For instance, “植 物 (plant)” and “毛 茛 科 (Ranunculaceae)” are both hypernyms of the entity “乌 头 (aconit),” and “植 物 (plant)” is also a hypernym of “毛 茛 科 (Ranunculaceae).” Given a list of hypernyms of an entity, our goal in the present work is to construct a semantic hierarchy of these hypernyms (right panel, Figure 1).1 Some pre"
P14-1113,P05-1014,0,0.450387,"hod to extract hypernyms of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Soche"
P14-1113,C92-2082,0,0.775886,"物 (plant)” is also a hypernym of “毛 茛 科 (Ranunculaceae).” Given a list of hypernyms of an entity, our goal in the present work is to construct a semantic hierarchy of these hypernyms (right panel, Figure 1).1 Some previous works extend and refine manually-built semantic hierarchies by using other resources (e.g., Wikipedia) (Suchanek et al., 2008). However, the coverage is limited by the scope of the resources. Several other works relied heavily on lexical patterns, which would suffer from deficiency because such patterns can only cover a small proportion of complex linguistic circumstances (Hearst, 1992; Snow et al., 2005). 1 In this study, we focus on Chinese semantic hierarchy construction. The proposed method can be easily adapted to other languages. 1199 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1199–1209, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Besides, distributional similarity methods (Kotlerman et al., 2010; Lenci and Benotto, 2012) are based on the assumption that a term can only be used in contexts where its hypernyms can be used and that a term might be used in any contexts whe"
P14-1113,S12-1012,0,0.493654,"icult to guarantee. Generally speaking, these pattern-based methods often suffer from low recall or precision because of the coverage or the quality of the patterns. The distributional methods assume that the contexts of hypernyms are broader than the ones of their hyponyms. For distributional similarity computing, each word is represented as a semantic vector composed of the pointwise mutual information (PMI) with its contexts. Kotlerman et al. (2010) design a directional distributional measure to infer hypernym–hyponym relations based on the standard IR Average Precision evaluation measure. Lenci and Benotto (2012) propose another measure focusing on the contexts that hypernyms do not share with their hyponyms. However, broader semantics may not always infer broader contexts. For example, for terms “Obama’ and 1200 “American people”, it is hard to say whose contexts are broader. Our previous work (Fu et al., 2013) applies a web mining method to discover the hypernyms of Chinese entities from multiple sources. We assume that the hypernyms of an entity co-occur with it frequently. It works well for named entities. But for class names (e.g., singers in Hong Kong, tropical fruits) with wider range of meanin"
P14-1113,I08-2112,0,0.0347144,". One possible solution may be adding more data of this kind to the training set. 6 Related Work In addition to the works mentioned in Section 2, we introduce another set of related studies in this section. Evans (2004), Ortega-Mendoza et al. (2007), and Sang (2007) consider web data as a large corpus and use search engines to identify hypernyms based on the lexical patterns of Hearst (1992). However, the low quality of the sentences in the search results negatively influence the precision of hypernym extraction. Following the method for discovering patterns automatically (Snow et al., 2005), McNamee et al. (2008) apply the same method to extract hypernyms of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (W"
P14-1113,N13-1090,0,0.480991,"ks well for hypernym extraction of entity names, but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics. Moreover, all of these methods do not use the word semantics effectively. This paper proposes a novel approach for semantic hierarchy construction based on word embeddings. Word embeddings, also known as distributed word representations, typically represent words with dense, low-dimensional and realvalued vectors. Word embeddings have been empirically shown to preserve linguistic regularities, such as the semantic relationship between words (Mikolov et al., 2013b). For example, v(king) − v(queen) ≈ v(man) − v(woman), where v(w) is the embedding of the word w. We observe that a similar property also applies to the hypernym–hyponym relationship (Section 3.3), which is the main inspiration of the present study. However, we further observe that hypernym– hyponym relations are more complicated than a single offset can represent. To address this challenge, we propose a more sophisticated and general method — learning a linear projection which maps words to their hypernyms (Section 3.3.1). Furthermore, we propose a piecewise linear projection method based o"
P14-1113,P07-2042,0,0.242856,"Missing"
P14-1113,P06-1101,0,0.241765,"tes that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the"
P14-1113,D11-1014,0,0.0421,"ethods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors. Their work indicates that word embeddings preserve some interesting linguistic regularities, which might provide support for many applications. In this paper, we im"
P14-1113,P07-1058,0,0.026901,"to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity"
P14-1113,P10-1040,0,0.084559,"). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors. Their work indicates that word embeddings preserve some interesting linguistic regularities, which might provide support for many applications. In this paper, we improve on their work by learning multiple linear projections in the embedding space, to model hypernym–hypony"
P14-1113,C04-1146,0,0.0870413,"Missing"
P14-1125,D12-1133,0,0.10455,"d of “大法官 (chief lawyer)” is “法官 (lawyer)”, and the smallest right subword 1327 大 big 法 law 官 officer (a) smallest left subword 合 agree with 法 law 化 ize (b) smallest right subword Figure 2: An example to illustrate the innermost left/right subwords. of “合法化 (legalize)” is “合法 (legal)”. 3 Character-Level Dependency Parsing A transition-based framework with global learning and beam search decoding (Zhang and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhan"
P14-1125,W06-2925,0,0.0254782,"Missing"
P14-1125,W08-0336,0,0.0274826,"flexible segmentation standards using this formalism. In Figure 1(c), for example, “副局长 (deputy 1326 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326–1336, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics director)” can be segmented as both “副 (deputy) |局 长 (director)” and “副 局 长 (deputy director)”, but not “副 (deputy) 局 (office) |长 (manager)”, by dependency coherence. Chinese language processing tasks, such as machine translation, can benefit from flexible segmentation standards (Zhang et al., 2008; Chang et al., 2008). Second, word internal structures can also be useful for syntactic parsing. Zhang et al. (2013) have shown the usefulness of word structures in Chinese constituent parsing. Their results on the Chinese Treebank (CTB) showed that characterlevel constituent parsing can bring increased performances even with the pseudo word structures. They further showed that better performances can be achieved when manually annotated word structures are used instead of pseudo structures. In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al. (2013)’s annotation"
P14-1125,W09-2307,0,0.0174117,"ting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependency tree Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 ("
P14-1125,D09-1127,0,0.015966,"Missing"
P14-1125,P10-1001,0,0.0106105,"ake speech (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows"
P14-1125,D12-1132,0,0.15249,"his work, we extend their formulation, making use of largescale annotations of Zhang et al. (2013), so that the syntactic word-level dependencies can be parsed together with intra-word dependencies. Hatori et al. (2012) proposed a joint model for Chinese word segmentation, POS-tagging and dependency parsing, studying the influence of joint model and character features for parsing, Their model is extended from the arc-standard transition-based model, and can be regarded as an alternative to the arc-standard model of our work when pseudo intra-word dependencies are used. Similar work is done by Li and Zhou (2012). Our proposed arc-standard model is more concise while obtaining better performance than Hatori et al. (2012)’s work. With respect to word structures, real intra-word dependencies are often more complicated, while pseudo word structures cannot be used to correctly guide segmentation. Zhao (2009), Hatori et al. (2012) and our work all study character-level dependency parsing. While Zhao (2009) focus on word internal structures using pseudo inter-word dependencies, Hatori et al. (2012) investigate a joint model using pseudo intra-word dependencies. We use manual dependencies for both inner- and"
P14-1125,P12-1071,1,0.880086,"(c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependency tree Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 (The deputy directo"
P14-1125,P11-1141,0,0.0156388,"d increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependency tree Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 (The deputy director of forestry administration make a speech in the meeting)”. a character-level dependency tree, where the leaf nodes are Chinese characters. Character-level dependency parsing is interesting in at least two aspects. First, character-level t"
P14-1125,P13-1104,0,0.0118312,"Missing"
P14-1125,P05-1012,0,0.0232684,"make a speech 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on"
P14-1125,P04-1015,0,0.0960443,"011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhang and Clark, 2008) or the arc-standard (Huang et al., 2009) transition systems. When the internal structures of words are annotated, character-level dependency parsing can be treated as a special case of word-level dependency parsing, with “words” being “characters”. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to well-established features fo"
P14-1125,P05-1013,0,0.00967053,"长 会 上 发 言 woods industry office deputy office manager meeting in make speech (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, bui"
P14-1125,W02-1001,0,0.090003,"ng and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhang and Clark, 2008) or the arc-standard (Huang et al., 2009) transition systems. When the internal structures of words are annotated, character-level dependency parsing can be treated as a special case of word-level dependency parsing, with “words” being “characters”. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to we"
P14-1125,J08-4003,0,0.437215,"ee by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author."
P14-1125,tsarfaty-goldberg-2008-word,0,0.311687,"el dependency parsing. While Zhao (2009) focus on word internal structures using pseudo inter-word dependencies, Hatori et al. (2012) investigate a joint model using pseudo intra-word dependencies. We use manual dependencies for both inner- and inter-word structures, studying their influences on each other. Zhang et al. (2013) was the first to perform Chinese syntactic parsing over characters. They extended word-level constituent trees by annotated word structures, and proposed a transition-based approach to parse intra-word structures and wordlevel constituent structures jointly. For Hebrew, Tsarfaty and Goldberg (2008) investigated joint segmentation and parsing over characters using a graph-based method. Our work is similar in exploiting character-level syntax. We study the dependency grammar, another popular syntactic representation, and propose two novel transition systems for character-level dependency parsing. Nivre (2008) gave a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing. We extend both algorithms to character-level joint word segmentation, POS-tagging and dependency parsing. To our knowl"
P14-1125,I05-3017,0,0.0666397,"ee Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 (The deputy director of forestry administration make a speech in the meeting)”. a character-level dependency tree, where the leaf nodes are Chinese characters. Character-level dependency parsing is interesting in at least two aspects. First, character-level trees circumvent the issue that no universal standard exists for Chinese word segmentation. In the well-known Chinese word segmentation bakeoff tasks, for example, different segmentation standards have been used by different data sets (Emerson, 2005). On the other hand, most disagreement on segmentation standards boils down to disagreement on segmentation granularity. As demonstrated by Zhao (2009), one can extract both finegrained and coarse-grained words from characterlevel dependency trees, and hence can adapt to flexible segmentation standards using this formalism. In Figure 1(c), for example, “副局长 (deputy 1326 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326–1336, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics director)” can be segmented a"
P14-1125,I11-1035,0,0.0124878,"a-word dependencies and pseudo inter-word dependencies; same as those of the character-level arc-standard model, shown in Table 1. 4 • STD (pseudo, real): the arc-standard model with pseudo intra-word dependencies and real inter-word dependencies; Experiments 4.1 Experimental Settings We use the Chinese Penn Treebank 5.0, 6.0 and 7.0 to conduct the experiments, splitting the corpora into training, development and test sets according to previous work. Three different splitting methods are used, namely CTB50 by Zhang and Clark (2010), CTB60 by the official documentation of CTB 6.0, and CTB70 by Wang et al. (2011). The dataset statistics are shown in Table 2. We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. The intra-word dependencies are extracted from the annotations of Zhang et al. (2013)2 . The standard measures of word-level precision, recall and F1 score are used to evaluate word segmentation, POS-tagging and dependency parsing, following Hatori et al. (2012). In addition, we use the same measures to evaluate intra-word dependencies, which indicate the performance of predicting word structures. A word’s structure is correct only if all the i"
P14-1125,P12-1110,0,0.264853,"ency parsing, with “words” being “characters”. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to well-established features for word segmentation, POS-tagging and syntactic parsing. In this section, we introduce novel extensions to the arc-standard and the arc-eager transition systems, so that word-based and characterbased features can be used simultaneously for character-level dependency parsing. 3.1 The Arc-Standard Model The arc-standard model has been applied to joint segmentation, POS-tagging and dependency parsing (Hatori et al., 2012), but with pseudo word structures. For unified processing of annotated word structures and fair comparison between character-level arc-eager and arc-standard systems, we define a different arc-standard transition system, consistent with our character-level arceager system. In the word-based arc-standard model, the transition state includes a stack and a queue, where the stack contains a sequence of partially-parsed dependency trees, and the queue consists of unprocessed input words. Four actions are defined for state transition, including arc-left (AL, which creates a left arc between the top"
P14-1125,D08-1059,1,0.940802,"ter-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Correspo"
P14-1125,P10-1110,0,0.021232,"he smallest left subword of “大法官 (chief lawyer)” is “法官 (lawyer)”, and the smallest right subword 1327 大 big 法 law 官 officer (a) smallest left subword 合 agree with 法 law 化 ize (b) smallest right subword Figure 2: An example to illustrate the innermost left/right subwords. of “合法化 (legalize)” is “合法 (legal)”. 3 Character-Level Dependency Parsing A transition-based framework with global learning and beam search decoding (Zhang and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be ei"
P14-1125,D10-1082,1,0.927638,"ows an example, where the smallest left subword of “大法官 (chief lawyer)” is “法官 (lawyer)”, and the smallest right subword 1327 大 big 法 law 官 officer (a) smallest left subword 合 agree with 法 law 化 ize (b) smallest right subword Figure 2: An example to illustrate the innermost left/right subwords. of “合法化 (legalize)” is “合法 (legal)”. 3 Character-Level Dependency Parsing A transition-based framework with global learning and beam search decoding (Zhang and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the s"
P14-1125,J11-1005,1,0.727336,"for syntactic parsing. Zhang et al. (2013) have shown the usefulness of word structures in Chinese constituent parsing. Their results on the Chinese Treebank (CTB) showed that characterlevel constituent parsing can bring increased performances even with the pseudo word structures. They further showed that better performances can be achieved when manually annotated word structures are used instead of pseudo structures. In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al. (2013)’s annotations and based on a transition-based parsing framework (Zhang and Clark, 2011). There are two dominant transitionbased dependency parsing systems, namely the arc-standard and the arc-eager parsers (Nivre, 2008). We study both algorithms for characterlevel dependency parsing in order to make a comprehensive investigation. For direct comparison with word-based parsers, we incorporate the traditional word segmentation, POS-tagging and dependency parsing stages in our joint parsing models. We make changes to the original transition systems, and arrive at two novel transition-based character-level parsers. We conduct experiments on three data sets, including CTB 5.0, CTB 6.0"
P14-1125,P11-2033,1,0.953288,"intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependenc"
P14-1125,W08-0335,0,0.0212139,"hence can adapt to flexible segmentation standards using this formalism. In Figure 1(c), for example, “副局长 (deputy 1326 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326–1336, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics director)” can be segmented as both “副 (deputy) |局 长 (director)” and “副 局 长 (deputy director)”, but not “副 (deputy) 局 (office) |长 (manager)”, by dependency coherence. Chinese language processing tasks, such as machine translation, can benefit from flexible segmentation standards (Zhang et al., 2008; Chang et al., 2008). Second, word internal structures can also be useful for syntactic parsing. Zhang et al. (2013) have shown the usefulness of word structures in Chinese constituent parsing. Their results on the Chinese Treebank (CTB) showed that characterlevel constituent parsing can bring increased performances even with the pseudo word structures. They further showed that better performances can be achieved when manually annotated word structures are used instead of pseudo structures. In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al"
P14-1125,P13-1013,1,0.528047,"ng interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependency tree Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 (The deputy director of forestry administration make a speech in the meeting)”. a character-level dependency tree, where the leaf nodes are Chinese characters. Character-level dependency parsing is interesting in at least two aspects. First, character-level trees circumvent the i"
P14-1125,E09-1100,0,0.097413,"over characters. Character-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving wordlevel dependency parsing accuracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods. 1 林业局 forestry administration 上 in 发言 make a speech 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohn"
P14-1125,P13-1043,1,0.251531,"ion is performed by the SHw action. The actions for intra-word dependencies include intra-word arc-left (ALc ), intra-word arcright (ARc ), pop-word (PW) and inter-word shift (SHc ). The definitions of ALc , ARc and SHc are the same as the word-based arc-standard model, while PW changes the top element on the stack into a full-word node, which can only take interword dependencies. One thing to note is that, due to variable word sizes in character-level parsing, the number of actions can vary between different sequences of actions corresponding to different analyses. We use the padding method (Zhu et al., 2013), adding an IDLE action to finished transition action sequences, for better alignments between states in the beam. 1328 In the character-level arc-standard transition step 0 1 2 3 4 5 6 7 ··· 12 13 ··· step 0 1 2 3 4 5 6 7 ··· 13 14 ··· action SHw (NR) SHc ALc SHc ALc PW SHw (NN) ··· PW ALw ··· (a) action SHc (NR) ALc SHc ALc SHc PW SHw ··· PW ALw ··· stack queue dependencies φ 林 业 ··· φ 林/NR 业 局 ··· φ 林/NR 业/NR 局 副 ··· φ x 业/NR 局 副 ··· A1 = {林 业} 业/NR 局/NR 副 局 ··· A1 S x 局/NR 副 局 ··· A2 = A1 {业 局} 林业局/NR 副 局 ··· A2 林业局/NR 副/NN 局 长 ··· A2 ··· ··· ··· 林业局/NR 副局长/NN 会 上 · · · Ai S x 副局长/NN 会 上 ·"
P15-1119,J92-4003,0,0.140167,"n Section 4 to induce the cross-lingual word clusters. We re-implement the PROJECTED cluster approach in T¨ackstr¨om et al. (2012), which assigns a target word to the cluster with which it is most often aligned: c(wiT ) = arg max k ∑ (i,j)∈AT ∣S ci,j ⋅ 1[c(wjS ) = k] This method also has the drawback that words that do not occur in the alignment dictionary (OOV) cannot be assigned a cluster. Therefore, we use the same strategy as described in Section 4.2 to find the most likely clusters for the OOV words. Instead of the clustering model of Uszkoreit and Brants (2008), we use Brown clustering (Brown et al., 1992) to induce hierarchical word clusters, where each word is represented as a bit-string. We use the same word cluster feature templates from T¨ackstr¨om et al. (2012), and set the number of Brown clusters to 256. 5.3 Experimental Results All of the parsing models are trained using the development data from English for early-stopping. Table 3 lists the results of the cross-lingual transfer experiments for dependency parsing. Table 4 further summarizes each of the experimental gains detailed in Table 3. Our delexicalized system obtains slightly lower performance than those reported in McDonald et"
P15-1119,W06-2920,0,0.0171356,"e templates from T¨ackstr¨om et al. (2012), and set the number of Brown clusters to 256. 5.3 Experimental Results All of the parsing models are trained using the development data from English for early-stopping. Table 3 lists the results of the cross-lingual transfer experiments for dependency parsing. Table 4 further summarizes each of the experimental gains detailed in Table 3. Our delexicalized system obtains slightly lower performance than those reported in McDonald et al. (2013) (McD13), because we’re using Before this dataset was carried out, the CoNLL multilingual dependency treebanks (Buchholz and Marsi, 2006) were often used for evaluation. However, the major problem is that the dependency annotations vary for different languages (e.g. the choice of lexical versus functional head), which makes it impossible to evaluate the LAS. 1239 D ELEX P ROJ P ROJ+Cluster CCA CCA+Cluster Unlabeled Attachment Score (UAS) EN DE ES FR AVG 83.67 57.01 68.05 68.85 64.64 91.96 60.07 71.42 71.36 67.62 92.33 60.35 71.90 72.93 68.39 90.62† 59.42 68.87 69.58 65.96 92.03† 60.66 71.33 70.87 67.62 EN 79.42 90.48 90.91 88.88† 90.49† M C D13 83.33 58.50 68.07 70.14 65.57 78.54 48.11 56.86 58.20 54.39 84.44 90.21 57.30 60.55"
P15-1119,P14-1063,0,0.0232538,"ture templates used in our system are shown in Table 1. Then, feature compositions are performed at the hidden layer via a cube activation function: g(x) = x3 . The cube activation function can be viewed as a special case of low-rank tensor. Formally, g(x) can be expanded as: g(w1 x1 + ... + wm xm + b) = ∑ (wi wj wk )xi xj xk + ∑ b(wi wj )xi xj + ... i,j,k i,j If we treat the bias term as b × x0 where x0 = 1, then the weight corresponding to each feature combination xi xj xk is wi wj wk , which is exactly the same as a rank-1 component tensor in the lowrank form using CP tensor decomposition (Cao and Khudanpur, 2014). Consequently, the cube activation function implicitly derives full feature combinations. An advantage of the cube activation function is that it is flexible for adding extra features to the input. In fact, we can add as many features as possible to the input layer to improve the parsing accuracy. We will show in Section 5.2 that the Brown cluster features can be readily incorporated into our model. Cross-lingual Transfer. The idea of crosslingual transfer using the parser we examined above is straightforward. In contrast to traditional approaches that have to discard rich lexical features (d"
P15-1119,N13-1006,1,0.167482,"(Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao"
P15-1119,D14-1082,0,0.779303,"atures used in traditional dependency parsers, distributed representations map symbolic features into a continuous representation space, that can be shared across languages. Therefore, our model has the ability to utilize both lexical and non-lexical features naturally. Specifically, our framework contains two primary components: • A neural network-based dependency parser. We expect a non-linear model for dependency parsing in our study, because distributed feature representations are shown to be more effective in non-linear architectures than in linear architectures (Wang and Manning, 2013). Chen and Manning (2014) propose a transition-based dependency parser using a neural network architecture, which is simple but works well on several datasets. Briefly, this model simply replaces the predictor in transition-based dependency parser with a well-designed neural network classifier. We will provide explanations for the merits of this model in Section 3, as well as how we adapt it to the cross-lingual task. • Cross-lingual word representation learning. The key to filling the lexical feature gap is to project the representations of these features from different languages into a common vector space, preservin"
P15-1119,P11-1061,0,0.0110737,"ion method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Klementiev et al., 2012)‡ B IAE (Chandar et al., 2014)‡ B ICVM (Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49"
P15-1119,W08-1301,0,0.0204693,"Missing"
P15-1119,de-marneffe-etal-2006-generating,0,0.034227,"Missing"
P15-1119,D12-1001,0,0.0275469,"uding NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-"
P15-1119,P10-4002,0,0.0314634,"r-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network dependency parser, we set the number of hidden units to 400. The dimension of embeddings for different features are shown in Table 2. Dim. Word 50 POS 50 Label 50 Dist. 5 Val. 5 Cluster 8 Table 2: Dimensions of feature embeddings. Adaptive stochastic gradient descent (AdaGrad) (Duchi et al., 2011) is used for optimization. For the CCA approach, we use the implementation of Faruqui and Dyer (2014). The dimensions of the monolin"
P15-1119,E14-1049,0,0.441226,"which is typically smaller than the monolingual datasets. Therefore, in order to improve the robustness of projection, we utilize a morphology-inspired mechanism, to propagate embeddings from in-vocabulary words to out-ofvocabulary (OOV) words. Specifically, for each T , we extract a list of candidate OOV word woov words that is similar to it in terms of edit distance, and then set the averaged vector as the embedding T of woov . Formally, T v(woov ) = Avg (v(w′ )) w′ ∈C T where C = {w∣EditDist(woov , w) ≤ τ } 4.3 ?1 Canonical Correlation Analysis The second approach we consider is similar to Faruqui and Dyer (2014), which use CCA to improve monolingual word embeddings with multilingual correlation. CCA is a way of measur?2 Σ ?1 Ω′ Ω ? ? ? ? ?2 ?2 Ω∗ CCA ?1 Σ∗ ? ? Figure 3: CCA for cross-lingual word representation learning. ing the linear relationship between multidimensional variables. For two multidimensional variables, CCA aims to find two projection matrices to map the original variables to a new basis (lowerdimensional), such that the correlation between the two variables is maximized. Let’s treat CCA as a blackbox here, and see how to apply CCA for inducing bilingual word embeddings. Suppose there"
P15-1119,P09-1042,0,0.0255939,"Missing"
P15-1119,P10-1156,0,0.0165341,"46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, whi"
P15-1119,D14-1012,1,0.642138,"major ways of applying distributed representations to NLP tasks. First, they can be fed into existing supervised NLP systems as augmented features in a semi-supervised manner. This kind of approach has been adopted in a variety of applications (Turian et al., 2010). Despite its simplicity and effectiveness, it has been shown that the potential of distributed representations cannot be fully exploited in the generalized linear models which are adopted in most of the existing NLP systems (Wang and Manning, 2013). One remedy is to discretize the distributed feature representations, as studied in Guo et al. (2014). However, we believe that a non-linear system, e.g. a neural network, is a more powerful and effective solution. Some decent progress has already been made in this paradigm of NLP on various tasks (Collobert et al., 2011; Chen and Manning, 2014; Sutskever et al., 2014). 3 Transition actions In this paper, these two terms are used interchangeably. Stack Buffer ROOT has_VBZ good_JJ control_NN ._. nsubj He_PRP Configuration Figure 2: Neural network model for dependency parsing. The Cluster features are introduced in Section 5.2. which typically consists of a stack S, a buffer B, and a partially"
P15-1119,P08-1088,0,0.0084253,"projection approach, CCA assigns embeddings for every word in the monolingual vocabulary. However, one potential limitation is that CCA assumes linear transformation of word embeddings, which is difficult to satisfy. 4 T ∣S A is also worth trying, but we observed slight performance degradation in our experimental setting. 1238 Note that both approaches can be generalize to lower-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network dependency parser, we set the number of hidden units"
P15-1119,P14-1006,0,0.0265905,"Missing"
P15-1119,D11-1110,0,0.0181767,"cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the b"
P15-1119,C10-1063,0,0.0172979,".). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Klementiev et al., 2012)‡ B IAE (Chandar et al., 2014)‡ B ICVM (Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingua"
P15-1119,P12-2010,0,0.021569,"oposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Klementiev et al., 2012)‡ B IAE (Chandar et al., 2014)‡ B ICVM (Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES U"
P15-1119,P12-1073,0,0.017508,"tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Klementiev et al., 2012)‡ B IAE (Chandar et al., 2014)‡ B ICVM (Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison"
P15-1119,P04-1061,0,0.0694958,"ng central problems. The majority of work on dependency parsing has been dedicated to resource-rich languages, such as English and Chinese. For these languages, there exist large-scale ∗ This work was done while the author was visiting JHU. annotated treebanks that can be used for supervised training of dependency parsers. However, for most of the languages in the world, there are few or even no labeled training data for parsing, and it is labor intensive and time-consuming to manually build treebanks for all languages. This fact has given rise to a number of research on unsupervised methods (Klein and Manning, 2004), annotation projection methods (Hwa et al., 2005), and model transfer methods (McDonald et al., 2011) for predicting linguistic structures. In this study, we focus on the model transfer methods, which attempt to build parsers for low-resource languages by exploiting treebanks from resourcerich languages. The major obstacle in transferring a parsing system from one language to another is the lexical features, e.g. words, which are not directly transferable across languages. To solve this problem, McDonald et al. (2011) build a delexicalized parser - a parser that only has non-lexical features."
P15-1119,C12-1089,0,0.351816,"Missing"
P15-1119,W02-0902,0,0.0119728,"dings for our cross-lingual task. Contrary to the projection approach, CCA assigns embeddings for every word in the monolingual vocabulary. However, one potential limitation is that CCA assumes linear transformation of word embeddings, which is difficult to satisfy. 4 T ∣S A is also worth trying, but we observed slight performance degradation in our experimental setting. 1238 Note that both approaches can be generalize to lower-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network depe"
P15-1119,P13-1105,0,0.0787465,"Missing"
P15-1119,W14-1614,0,0.05557,"-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-resource languages. That said, these methods have the advantage that they are capable of capturing some language-specific syntactic patterns which our approach cannot.15 These two kinds of approaches 15 For example, in Spanish and French, adjectives often appears after nouns, thus forming a right-directed arc labeled by amod, whereas in English, the amod"
P15-1119,P14-1126,0,0.19646,"pplied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-resource languages. That said, these methods have the advantage that they are capable of capturing some language-specific syntactic patterns which our approach cannot.15 These two kinds of approaches 15 For example, in Spanish and French, adjectives often appears after nouns, thus forming a right-directed arc labeled by amod, whereas in English, the amod arcs are mostly leftdirected. ‡ FR LAS 58.78 46.66 58.08"
P15-1119,C14-1175,0,0.133083,"dency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel"
P15-1119,N01-1020,1,0.605674,"ual task. Contrary to the projection approach, CCA assigns embeddings for every word in the monolingual vocabulary. However, one potential limitation is that CCA assumes linear transformation of word embeddings, which is difficult to satisfy. 4 T ∣S A is also worth trying, but we observed slight performance degradation in our experimental setting. 1238 Note that both approaches can be generalize to lower-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network dependency parser, we set the"
P15-1119,D07-1013,0,0.00913989,"d cluster features can be effectively embedded into our model, leading to significant additive improvements. 2 2.1 Background Dependency Parsing Given an input sentence x = w0 w1 ...wn , the goal of dependency parsing is to build a dependency tree (Figure 1), which can be denoted by d = {(h, m, l) ∶ 0 ≤ h ≤ n; 0 &lt; m ≤ n, l ∈ L}. (h, m, l) indicates a directed arc from the head word wh to the modifier wm with a dependency label l, and L is the label set. The mainstream models that have been proposed for dependency parsing can be described as either graph-based models or transitionbased models (McDonald and Nivre, 2007). Graph-based models view the parsing problem as finding the highest scoring tree from a directed graph. The score of a dependency tree is typically factored into scores of some small structures (e.g. arcs) depending on the order of a model. Transition-based models aim to predict a transition sequence from an initial parser state to some terminal states, depending on the parsing history. This approach has a lot of interest since it is fast (linear time) and can incorporate rich non-local features (Zhang and Nivre, 2011). It has been considered that simple transitionbased parsing using greedy d"
P15-1119,D11-1006,0,0.219143,"uages, such as English and Chinese. For these languages, there exist large-scale ∗ This work was done while the author was visiting JHU. annotated treebanks that can be used for supervised training of dependency parsers. However, for most of the languages in the world, there are few or even no labeled training data for parsing, and it is labor intensive and time-consuming to manually build treebanks for all languages. This fact has given rise to a number of research on unsupervised methods (Klein and Manning, 2004), annotation projection methods (Hwa et al., 2005), and model transfer methods (McDonald et al., 2011) for predicting linguistic structures. In this study, we focus on the model transfer methods, which attempt to build parsers for low-resource languages by exploiting treebanks from resourcerich languages. The major obstacle in transferring a parsing system from one language to another is the lexical features, e.g. words, which are not directly transferable across languages. To solve this problem, McDonald et al. (2011) build a delexicalized parser - a parser that only has non-lexical features. A delexicalized parser makes sense in that POS tag features are significantly predictive for unlabele"
P15-1119,D10-1120,0,0.021069,"Missing"
P15-1119,P12-1066,0,0.0410584,"Missing"
P15-1119,W04-0308,0,0.0244311,"ROOT has_VBZ good_JJ control_NN ._. nsubj He_PRP Configuration Figure 2: Neural network model for dependency parsing. The Cluster features are introduced in Section 5.2. which typically consists of a stack S, a buffer B, and a partially derived forest, i.e. a set of dependency arcs A. Given an input word sequence x = w1 w2 , ..., wn , the initial configuration can be represented as a tuple: ⟨[w0 ]S , [w1 w2 , ..., wn ]B , ∅⟩, and the terminal configuration is ⟨[w0 ]S , []B , A⟩, where w0 is a pseudo word indicating the root of the whole dependency tree. We consider the arc-standard algorithm (Nivre, 2004) in this paper, which defines three types of transition actions: L EFT-A RC(l), R IGHT-A RC(l), and S HIFT, l is the dependency label. The typical approach for greedy arc-standard parsing is to build a multi-class classifier (e.g., SVM, MaxEnt) of predicting the transition action given a feature vector extracted from a specific configuration. While conventional feature engineering suffers from the problem of sparsity, incompleteness and expensive feature computation (Chen and Manning, 2014), the neural network model provides a potential solution. The architecture of the neural network-based de"
P15-1119,petrov-etal-2012-universal,0,0.0336392,"Missing"
P15-1119,D09-1086,0,0.00730072,". 6 Related Studies Existing approaches for cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized"
P15-1119,W15-1824,0,0.0443701,"be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Kle"
P15-1119,P12-1068,0,0.00760257,"UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser"
P15-1119,P10-1040,0,0.0337388,"in the NLP research community of learning distributed representations for different natural language units, from morphemes, words and phrases, to sentences and documents. Using distributed representations, these symbolic units are embedded into a lowdimensional and continuous space, thus it is often referred to as embeddings.1 In general, there are two major ways of applying distributed representations to NLP tasks. First, they can be fed into existing supervised NLP systems as augmented features in a semi-supervised manner. This kind of approach has been adopted in a variety of applications (Turian et al., 2010). Despite its simplicity and effectiveness, it has been shown that the potential of distributed representations cannot be fully exploited in the generalized linear models which are adopted in most of the existing NLP systems (Wang and Manning, 2013). One remedy is to discretize the distributed feature representations, as studied in Guo et al. (2014). However, we believe that a non-linear system, e.g. a neural network, is a more powerful and effective solution. Some decent progress has already been made in this paradigm of NLP on various tasks (Collobert et al., 2011; Chen and Manning, 2014; Su"
P15-1119,P08-1086,0,0.0144334,"). We use the same alignment dictionary as described in Section 4 to induce the cross-lingual word clusters. We re-implement the PROJECTED cluster approach in T¨ackstr¨om et al. (2012), which assigns a target word to the cluster with which it is most often aligned: c(wiT ) = arg max k ∑ (i,j)∈AT ∣S ci,j ⋅ 1[c(wjS ) = k] This method also has the drawback that words that do not occur in the alignment dictionary (OOV) cannot be assigned a cluster. Therefore, we use the same strategy as described in Section 4.2 to find the most likely clusters for the OOV words. Instead of the clustering model of Uszkoreit and Brants (2008), we use Brown clustering (Brown et al., 1992) to induce hierarchical word clusters, where each word is represented as a bit-string. We use the same word cluster feature templates from T¨ackstr¨om et al. (2012), and set the number of Brown clusters to 256. 5.3 Experimental Results All of the parsing models are trained using the development data from English for early-stopping. Table 3 lists the results of the cross-lingual transfer experiments for dependency parsing. Table 4 further summarizes each of the experimental gains detailed in Table 3. Our delexicalized system obtains slightly lower p"
P15-1119,I13-1183,0,0.0132607,"Missing"
P15-1119,Q14-1005,0,0.032741,"om, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cr"
P15-1119,W14-1613,0,0.186749,"2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-resource languages. T"
P15-1119,H01-1035,1,0.652706,"ks. It is worth noting that we don’t assume/require bilingual parallel data in CCA and P ROJ. What we need in practice is a bilingual lexicon for each paired languages. This is especially important for generalizing our approaches to lower-resource languages, where parallel texts are not available. 6 Related Studies Existing approaches for cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorp"
P15-1119,P11-2033,0,0.022,"an be described as either graph-based models or transitionbased models (McDonald and Nivre, 2007). Graph-based models view the parsing problem as finding the highest scoring tree from a directed graph. The score of a dependency tree is typically factored into scores of some small structures (e.g. arcs) depending on the order of a model. Transition-based models aim to predict a transition sequence from an initial parser state to some terminal states, depending on the parsing history. This approach has a lot of interest since it is fast (linear time) and can incorporate rich non-local features (Zhang and Nivre, 2011). It has been considered that simple transitionbased parsing using greedy decoding and local training is not as accurate as graph-based parsers or transition-based parsers with beam-search and 1235 global training (Zhang and Clark, 2011). Recently, Chen and Manning (2014) show that greedy transition-based parsers can be greatly improved by using a well-designed neural network architecture. This approach can be considered as a new paradigm of parsing, in that it is based on pure distributed feature representations. In this study, we choose Chen and Manning’s architecture to build our basic depe"
P15-1119,P09-1007,0,0.00764621,"ting approaches for cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B"
P15-1119,D10-1030,0,0.0157323,"l., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings"
P15-1119,N12-1052,0,0.445863,"Missing"
P15-1119,J11-1005,0,\N,Missing
P15-1119,P13-2017,0,\N,Missing
P17-4003,D09-1026,0,0.0138392,"4: The framework of the proposed LTS model for response generation. versation state tracking as inputs and estimates the triggered domain distribution using a convolutional neural network. In Benben, we proposed a topic augmented convolutional neural network to integrate the continuous word representations and the discrete topic information into a unified framework for domain selection. Figure 3 shows the framework of the proposed topic augmented convolutional neural network for domain selection. The word embedding matrix and the topic matrix are obtained using the word2vec6 and Labeled LDA (Ramage et al., 2009), respectively. The two representations of the input conversation utterance are combined in the full connection layer and output the domain triggered distribution. At last, the domains whose triggered probabilities are larger than a threshold are selected to execute the following domain processing step. Note that after the domain selection step, there may be one or more triggered domains. If there is no domain to be triggered, the conversation state is updated and then sent to the response generation module. tracker records the historical content, the current domain and the historically trigge"
P17-4003,C10-3004,1,0.744957,"e seen, the architecture of Benben can be corresponded to the classic architecture of spoken dialogue systems (Young et al., 2013). Concretely, the natural language understanding, dialogue management and natural language generation in spoken dialogue systems are corresponding to the 1), 2) and 3), 4) components of the Benben architecture, respectively. We will next detail each component in the following sections. 2.1 Language Understanding The user input can be either text or speech. Therefore, the first step is to understand both the speech transcription and text. In Benben, the LTP toolkit (Che et al., 2010) is utilized to the basic language processing, including Chinese word segmentation, part-of-speech tagging, word sense disambiguation, named entity recognition, dependency parsing, semantic role labelling and semantic 5 2.2 Conversation State Tracking After the language understanding step, an input sentence is transferred to several feature representations. These feature representations are then taken as the inputs of the conversation state tracking and domain selection. The conversation state http://www.ltp-cloud.com 14 Word Embedding Matrix yt yt-1 y2 y1 st st-1 s2 s1 Word2Vec y0 =g(c,E) Ini"
P17-4003,P17-1010,1,\N,Missing
P18-1129,D16-1211,0,0.0161719,"ation and the results are shown in Figure 3. Sharpen the distribution during the sampling process generally performs better on development set. Our distillation from exploration model gets almost the same performance as that from reference, but simply combing these two sets of data outperform both models by achieving an LAS of 92.14. We also compare our parser with the other parsers in Table 2. The second group shows the greedy transition-based parsers in previous literatures. Andor et al. (2016) presented an alternative state representation and explored both greedy and beam search decoding. (Ballesteros et al., 2016) explores training the greedy parser with dynamic oracle. Our distillation parser outperforms all these greedy counterparts. The third group shows 92.14 92 89 BLEU on dev. set 4.2 BLEU 22.79 26.26 24.76 24.64 25.44 20.73 22.53 23.83 Baseline Ensemble (10) Distill (reference, α=0.8) Distill (exploration, T =0.1) Distill (both) MIXER BSO (local, B=1) BSO (global, B=1) LAS on dev. set Baseline Ensemble (20) Distill (reference, α=1.0) Distill (exploration, T =1.0) Distill (both) Ballesteros et al. (2016) (dyn. oracle) Andor et al. (2016) (local, B=1) Buckman et al. (2016) (local, B=8) Andor et al."
P18-1129,D16-1254,0,0.0321506,"Missing"
P18-1129,P04-1015,0,0.387277,"ch-based structured prediction tasks – transition-based dependency parsing and neural machine translation show that distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures. 1 Reference states Search-based structured prediction models the generation of natural language structure (part-ofspeech tags, syntax tree, translations, semantic graphs, etc.) as a search problem (Collins and Roark, 2004; Liang et al., 2006; Zhang and Clark, 2008; Huang et al., 2012; Sutskever et al., 2014; Goodman et al., 2016). It has drawn a lot of research attention in recent years thanks to its competitive performance on both accuracy and running time. A stochastic policy that controls the whole search process is usually learned by imitating a reference policy. The imitation is usually addressed as training a classifier to predict the ref* Email corresponding. Distillation loss Exploration states Reference policy Training data Exploration policy model1 Ensemble model ... modelM Figure 1: Workflow of our"
P18-1129,P81-1022,0,0.63388,"Missing"
P18-1129,P15-1033,0,0.025449,"and exploration with the following manner: we use πR and πE to generate a set of training states. Then, we learn p(a |s) on the generated states. If one state was generated by the reference policy, we minimize the interpretation of distillation and NLL loss. Otherwise, we minimize the distillation loss only. 4 Experiments We perform experiments on two tasks: transitionbased dependency parsing and neural machine translation. Both these two tasks are converted to search-based structured prediction as Section 2.1. For the transition-based parsing, we use the stack-lstm parsing model proposed by Dyer et al. (2015) to parameterize the classifier.1 For the neural machine translation, we parameterize the classifier as an LSTM encoder-decoder model by following Luong et al. (2015).2 We encourage the reader of this paper to refer corresponding papers for more details. 4.1 Settings 4.1.1 Transition-based Dependency Parsing We perform experiments on Penn Treebank (PTB) dataset with standard data split (Section 2-21 for training, Section 22 for development, and Section 23 for testing). Stanford dependencies are converted from the original constituent trees using Stanford CoreNLP 3.3.03 by following Dyer et al."
P18-1129,P16-1231,0,0.0231506,".0, best performance on development set is achieved and the test LAS is 91.99. We tune the temperature T during exploration and the results are shown in Figure 3. Sharpen the distribution during the sampling process generally performs better on development set. Our distillation from exploration model gets almost the same performance as that from reference, but simply combing these two sets of data outperform both models by achieving an LAS of 92.14. We also compare our parser with the other parsers in Table 2. The second group shows the greedy transition-based parsers in previous literatures. Andor et al. (2016) presented an alternative state representation and explored both greedy and beam search decoding. (Ballesteros et al., 2016) explores training the greedy parser with dynamic oracle. Our distillation parser outperforms all these greedy counterparts. The third group shows 92.14 92 89 BLEU on dev. set 4.2 BLEU 22.79 26.26 24.76 24.64 25.44 20.73 22.53 23.83 Baseline Ensemble (10) Distill (reference, α=0.8) Distill (exploration, T =0.1) Distill (both) MIXER BSO (local, B=1) BSO (global, B=1) LAS on dev. set Baseline Ensemble (20) Distill (reference, α=1.0) Distill (exploration, T =1.0) Distill (bo"
P18-1129,C12-1059,0,0.258443,"ructured prediction. The yellow bracket represents the ensemble of multiple models trained with different initialization. The dashed red line shows our distillation from reference (§3.2). The solid blue line shows our distillation from exploration (§3.3). Introduction ∗ Distilled model erence policy’s search action on the encountered states when performing the reference policy. Such imitation process can sometimes be problematic. One problem is the ambiguities of the reference policy, in which multiple actions lead to the optimal structure but usually, only one is chosen as training instance (Goldberg and Nivre, 2012). Another problem is the discrepancy between training and testing, in which during the test phase, the learned policy enters non-optimal states whose search action is never learned (Ross and Bagnell, 2010; Ross et al., 2011). All these problems harm the generalization ability of search-based structured prediction and lead to poor performance. Previous works tackle these problems from two directions. To overcome the ambiguities in data, techniques like ensemble are often adopted (Di1393 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages"
P18-1129,Q14-1010,0,0.0138696,"imal 89.59 90.90 91.38 Table 4: The ranking performance of parsers’ output distributions evaluated in MAP on “problematic” states. 4.3.1 Ensemble on “Problematic” States As mentioned in previous sections, “problematic” states which is either ambiguous or non-optimal harm structured prediciton’s performance. Ensemble shows to improve the performance in Section 4.2, which indicates it does better on these states. To empirically testify this, we use dependency parsing as a testbed and study the ensemble’s output distribution using the dynamic oracle. The dynamic oracle (Goldberg and Nivre, 2012; Goldberg et al., 2014) can be used to efficiently determine, given any state s, which transition action leads to the best achievable parse from s; if some errors may have already made, what is the best the parser can do, going forward? This allows us to analyze the accuracy of each parser’s individual decisions, in the “problematic” states. In this paper, we evaluate the output distributions of the baseline and ensemble parser against the reference actions suggested by the dynamic oracle. Since dynamic oracle yields more than one reference actions due to ambiguities and previous mistakes and the output distribution"
P18-1129,P16-1001,0,0.145512,"hat distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures. 1 Reference states Search-based structured prediction models the generation of natural language structure (part-ofspeech tags, syntax tree, translations, semantic graphs, etc.) as a search problem (Collins and Roark, 2004; Liang et al., 2006; Zhang and Clark, 2008; Huang et al., 2012; Sutskever et al., 2014; Goodman et al., 2016). It has drawn a lot of research attention in recent years thanks to its competitive performance on both accuracy and running time. A stochastic policy that controls the whole search process is usually learned by imitating a reference policy. The imitation is usually addressed as training a classifier to predict the ref* Email corresponding. Distillation loss Exploration states Reference policy Training data Exploration policy model1 Ensemble model ... modelM Figure 1: Workflow of our knowledge distillation for search-based structured prediction. The yellow bracket represents the ensemble of m"
P18-1129,nilsson-nivre-2008-malteval,0,0.0401907,"Missing"
P18-1129,J08-4003,0,0.595658,"parsing (σ, β, A), where σ is a stack, β is a buffer, and A is the partially generated tree {S HIFT, L EFT, R IGHT} {([ ], [1, .., n], ∅)} {([ROOT], [ ], A)} • S HIFT: (σ, j|β) → (σ|j, β) • L EFT: (σ|i j, β) → (σ|j, β) A ← A ∪ {i ← j} • R IGHT: (σ|i j, β) → (σ|i, β) A ← A ∪ {i → j} Neural machine translation ($, y1 , y2 , ..., yt ), where $ is the start symbol. pick one word w from the target side vocabulary W. {($)} {($, y1 , y2 , ..., ym )} ($, y1 , y2 , ..., yt ) → ($, y1 , y2 , ..., yt , yt+1 = w) Table 1: The search-based structured prediction view of transition-based dependency parsing (Nivre, 2008) and neural machine translation (Sutskever et al., 2014). etterich, 2000). To mitigate the discrepancy, exploration is encouraged during the training process (Ross and Bagnell, 2010; Ross et al., 2011; Goldberg and Nivre, 2012; Bengio et al., 2015; Goodman et al., 2016). In this paper, we propose to consider these two problems in an integrated knowledge distillation manner (Hinton et al., 2015). We distill a single model from the ensemble of several baselines trained with different initialization by matching the ensemble’s output distribution on the reference states. We also let the ensemble r"
P18-1129,P02-1040,0,0.10334,"t n = 20. 4.1.2 Neural Machine Translation We conduct our experiments on a small machine translation dataset, which is the Germanto-English portion of the IWSLT 2014 machine translation evaluation campaign. The dataset contains around 153K training sentence pairs, 7K development sentence pairs, and 7K testing sentence pairs. We use the same preprocessing as Ranzato et al. (2015), which leads to a German vocabulary of about 30K entries and an English vocabulary of 25K entries. One-layer LSTM for both encoder and decoder with 256 hidden units are used by following Wiseman and Rush (2016). BLEU (Papineni et al., 2002) was used to evaluate the translator’s performance.4 Like in the dependency parsing experiments, we run M = 10 differentlyseeded runs and report the averaged score. Optimizing the distillation loss in Equation 1 requires enumerating over the action space. It is expensive for machine translation since the size of the action space (vocabulary) is considerably large (25K in our experiments). In this paper, we use the K-most probable actions (translations on target side) on one state to approximate the P whole probability distribution of q(a |s) as a q(a | P s) · log p(a |s) ≈ K ak |s) · log p(ˆ a"
P18-1129,N12-1015,0,0.0257324,"arsing and neural machine translation show that distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures. 1 Reference states Search-based structured prediction models the generation of natural language structure (part-ofspeech tags, syntax tree, translations, semantic graphs, etc.) as a search problem (Collins and Roark, 2004; Liang et al., 2006; Zhang and Clark, 2008; Huang et al., 2012; Sutskever et al., 2014; Goodman et al., 2016). It has drawn a lot of research attention in recent years thanks to its competitive performance on both accuracy and running time. A stochastic policy that controls the whole search process is usually learned by imitating a reference policy. The imitation is usually addressed as training a classifier to predict the ref* Email corresponding. Distillation loss Exploration states Reference policy Training data Exploration policy model1 Ensemble model ... modelM Figure 1: Workflow of our knowledge distillation for search-based structured prediction."
P18-1129,D17-1035,0,0.0240206,"tion and α is determined on the development set. 1 The code for parsing experiments is available at: https://github.com/Oneplus/twpipe. 2 We based our NMT experiments on OpenNMT (Klein et al., 2017). The code for NMT experiments is available at: https://github.com/Oneplus/OpenNMT-py. 3 stanfordnlp.github.io/CoreNLP/ history.html BLEU score on dev. set 3.4 27.25 27.13 27.06 27.00 26.934 26.926 10 20 26.86 26.99 26.99 50 100 26.75 26.50 26.25 1 2 5 Figure 2: The effect of using different Ks when approximating distillation loss with K-most probable actions in the machine translation experiments. Reimers and Gurevych (2017) and others have pointed out that neural network training is nondeterministic and depends on the seed for the random number generator. To control for this effect, they suggest to report the average of M differentlyseeded runs. In all our dependency parsing, we set n = 20. 4.1.2 Neural Machine Translation We conduct our experiments on a small machine translation dataset, which is the Germanto-English portion of the IWSLT 2014 machine translation evaluation campaign. The dataset contains around 153K training sentence pairs, 7K development sentence pairs, and 7K testing sentence pairs. We use the"
P18-1129,D16-1139,0,0.0931068,"ons are achieved by our distillation methods. As Keskar et al. (2016) pointed out that the generalmin max σ 90.45 92.00 91.14 92.37 0.17 0.09 21.63 24.22 23.67 25.65 0.55 0.12 Table 5: The minimal, maximum, and standard derivation values on differently-seeded runs. ization gap is not due to overfit, but due to the network converge to sharp minimizer which generalizes worse, we attribute the more stable training from our distillation model as the distillation loss presents less sharp minimizers. 5 Related Work Several works have been proposed to applying knowledge distillation to NLP problems. Kim and Rush (2016) presented a distillation model which focus on distilling the structured loss from a large model into a small one which works on sequencelevel. In contrast to their work, we pay more attention to action-level distillation and propose to do better action-level distillation by both from reference and exploration. Freitag et al. (2017) used an ensemble of 6translators to generate training reference. Exploration was tried in their work with beam-search. We differ their work by training the single model 1400 to match the distribution of the ensemble. Using ensemble in exploration was also studied i"
P18-1129,P17-4012,0,0.0724743,"Missing"
P18-1129,W04-3250,0,0.0171869,"a q(a | P s) · log p(a |s) ≈ K ak |s) · log p(ˆ ak |s), k q(ˆ where a ˆk is the k-th probable action. We fix α to 4 We use multi-bleu.perl to evaluate our model’s performance 1397 LAS 90.83 92.73 91.99 92.00 92.14 91.42 91.02 91.19 91.70 92.79 94.08 92.06 94.60 Table 2: The dependency parsing results. Significance test (Nilsson and Nivre, 2008) shows the improvement of our Distill (both) over Baseline is statistically significant with p < 0.01. Table 3: The machine translation results. MIXER denotes that of Ranzato et al. (2015), BSO denotes that of Wiseman and Rush (2016). Significance test (Koehn, 2004) shows the improvement of our Distill (both) over Baseline is statistically significant with p < 0.01. 92.11 1 and vary K and evaluate the distillation model’s performance. These results are shown in Figure 2 where there is no significant difference between different Ks and in speed consideration, we set K to 1 in the following experiments. 4.2.1 92.05 92.09 92.11 0.5 0.67 1 91.98 91.85 91 90 89.04 0.1 Results Transition-based Dependency Parsing Table 2 shows our PTB experimental results. From this result, we can see that the ensemble model outperforms the baseline model by 1.90 in LAS. For ou"
P18-1129,D17-1208,0,0.0231132,"transfer knowledge of cumbersome model into a simple one on the unlabeled data (Liang et al., 2008; Li et al., 2014). Their extensions to knowledge distillation call for further study. Kuncoro et al. (2016) proposed to compile the knowledge from an ensemble of 20 transitionbased parsers into a voting and distill the knowledge by introducing the voting results as a regularizer in learning a graph-based parser. Different from their work, we directly do the distillation on the classifier of the transition-based parser. Besides the attempts for directly using the knowledge distillation technique, Stahlberg and Byrne (2017) propose to first build the ensemble of several machine translators into one network by unfolding and then use SVD to shrink its parameters, which can be treated as another kind of knowledge distillation. 6 Conclusion In this paper, we study knowledge distillation for search-based structured prediction and propose to distill an ensemble into a single model both from reference and exploration states. Experiments on transition-based dependency parsing and machine translation show that our distillation method significantly improves the single model’s performance. Comparison analysis gives empiric"
P18-1129,E17-1117,0,0.0735766,"Missing"
P18-1129,D16-1180,0,0.140531,"outperforms all these greedy counterparts. The third group shows 92.14 92 89 BLEU on dev. set 4.2 BLEU 22.79 26.26 24.76 24.64 25.44 20.73 22.53 23.83 Baseline Ensemble (10) Distill (reference, α=0.8) Distill (exploration, T =0.1) Distill (both) MIXER BSO (local, B=1) BSO (global, B=1) LAS on dev. set Baseline Ensemble (20) Distill (reference, α=1.0) Distill (exploration, T =1.0) Distill (both) Ballesteros et al. (2016) (dyn. oracle) Andor et al. (2016) (local, B=1) Buckman et al. (2016) (local, B=8) Andor et al. (2016) (local, B=32) Andor et al. (2016) (global, B=32) Dozat and Manning (2016) Kuncoro et al. (2016) Kuncoro et al. (2017) 27.00 26.99 0.2 26.97 1.5 2 5 26.93 26.7 26.75 26.50 26.24 26.25 0.1 0.2 0.5 0.67 1 Figure 3: The effect of T on PTB (above) and IWSLT 2014 (below) development set. parsers trained on different techniques including decoding with beam search (Buckman et al., 2016; Andor et al., 2016), training transitionbased parser with beam search (Andor et al., 2016), graph-based parsing (Dozat and Manning, 2016), distilling a graph-based parser from the output of 20 parsers (Kuncoro et al., 2016), and converting constituent parsing results to dependencies (Kuncoro et al., 2017). Our d"
P18-1129,P14-1043,0,0.0301015,"and exploration. Freitag et al. (2017) used an ensemble of 6translators to generate training reference. Exploration was tried in their work with beam-search. We differ their work by training the single model 1400 to match the distribution of the ensemble. Using ensemble in exploration was also studied in reinforcement learning community (Osband et al., 2016). In addition to distilling the ensemble on the labeled training data, a line of semisupervised learning works show that it’s effective to transfer knowledge of cumbersome model into a simple one on the unlabeled data (Liang et al., 2008; Li et al., 2014). Their extensions to knowledge distillation call for further study. Kuncoro et al. (2016) proposed to compile the knowledge from an ensemble of 20 transitionbased parsers into a voting and distill the knowledge by introducing the voting results as a regularizer in learning a graph-based parser. Different from their work, we directly do the distillation on the classifier of the transition-based parser. Besides the attempts for directly using the knowledge distillation technique, Stahlberg and Byrne (2017) propose to first build the ensemble of several machine translators into one network by un"
P18-1129,Q14-1042,0,0.0137791,"d model significantly improves over strong baselines and outperforms other greedy structured prediction (§4.2). Comprehensive analysis empirically shows the feasibility of our distillation method (§4.3). 2 2.1 Background Search-based Structured Prediction Structured prediction maps an input x = (x1 , x2 , ..., xn ) to its structural output y = (y1 , y2 , ..., ym ), where each component of y has some internal dependencies. Search-based structured prediction (Collins and Roark, 2004; Daum´e III et al., 2005; Daum´e III et al., 2009; Ross and Bagnell, 2010; Ross et al., 2011; Doppa et al., 2014; Vlachos and Clark, 2014; Chang et al., 2015) models the generation of the structure as a search problem and it can be formalized as a tuple (S, A, T (s, a), S0 , ST ), in which S is a set of states, A is a set of actions, T is a function that maps S × A → S, S0 is a set of initial states, and ST is a set of terminal states. Starting from an initial state s0 ∈ S0 , the structured prediction model repeatably chooses an action at ∈ A by following a policy π(s) and applies at to st and enter a new state st+1 as st+1 ← T (st , at ), until a final state sT ∈ ST is achieved. Several natural language structured prediction p"
P18-1129,D16-1137,0,0.0952854,"l our dependency parsing, we set n = 20. 4.1.2 Neural Machine Translation We conduct our experiments on a small machine translation dataset, which is the Germanto-English portion of the IWSLT 2014 machine translation evaluation campaign. The dataset contains around 153K training sentence pairs, 7K development sentence pairs, and 7K testing sentence pairs. We use the same preprocessing as Ranzato et al. (2015), which leads to a German vocabulary of about 30K entries and an English vocabulary of 25K entries. One-layer LSTM for both encoder and decoder with 256 hidden units are used by following Wiseman and Rush (2016). BLEU (Papineni et al., 2002) was used to evaluate the translator’s performance.4 Like in the dependency parsing experiments, we run M = 10 differentlyseeded runs and report the averaged score. Optimizing the distillation loss in Equation 1 requires enumerating over the action space. It is expensive for machine translation since the size of the action space (vocabulary) is considerably large (25K in our experiments). In this paper, we use the K-most probable actions (translations on target side) on one state to approximate the P whole probability distribution of q(a |s) as a q(a | P s) · log"
P18-1129,D08-1059,0,0.0483245,"tion-based dependency parsing and neural machine translation show that distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures. 1 Reference states Search-based structured prediction models the generation of natural language structure (part-ofspeech tags, syntax tree, translations, semantic graphs, etc.) as a search problem (Collins and Roark, 2004; Liang et al., 2006; Zhang and Clark, 2008; Huang et al., 2012; Sutskever et al., 2014; Goodman et al., 2016). It has drawn a lot of research attention in recent years thanks to its competitive performance on both accuracy and running time. A stochastic policy that controls the whole search process is usually learned by imitating a reference policy. The imitation is usually addressed as training a classifier to predict the ref* Email corresponding. Distillation loss Exploration states Reference policy Training data Exploration policy model1 Ensemble model ... modelM Figure 1: Workflow of our knowledge distillation for search-based str"
P18-1129,P06-1096,0,0.162632,"Missing"
P18-1129,D15-1166,0,0.0503662,"Missing"
S07-1034,W02-1006,0,0.0879164,"by their morphological root forms. The implementation for getting the morphological root forms is WordNet (morph). 4.2 We used 4 kinds of features of the target word and its context as shown in Table 1. Part of the original text of an example is “… This is the <head>age</head> of new media , the era of …”. Name MaltParser4 this_0, be_0, the_0, age_t, of_1, new_1, medium_1, ,_1, the_1 SYN_HEAD_is SYN_HEADPOS_VBZ SYN_RELATION_PRD SYN_HEADRIGHT Table 1: Features the system extracted The next 4 subsections elaborate these features. Learning Algorithm SVM is an effective learning algorithm to WSD (Lee and Ng, 2002). The SVM tries to find a hyperplane with the largest margin separating the training samples into two classes. The instances in the same side of the hyperplane have the same class label. A test instance’s feature decides the position where the sample is in the feature space and which side of the hyperplane it is. In this way, it leads to get a prediction. SVM could be extended to tackle multi-classes problems by using oneagainst-one or one-against-rest strategy. In the WSD problem, input of SVM is the feature vector of the instance. Features that appear in all the training samples are arranged"
S07-1034,gimenez-marquez-2004-svmtool,0,0.0141843,"; the instance is “x2 x6 x5 x7”. The feature vector of this sample should be <0, 1, 0, 0, 1, 1, 1>. The implementation of SVM here is libsvm 1 (Chang and Lin, 2001) for multi-classes. 4 Collocation Part-of-Speechs of Neighboring Words As mentioned above, the data sparseness is a serious problem in WSD. Besides changing tokens to their morphological root forms, part-of-speech is a good choice too. The size of POS tag set is much smaller than the size of surrounding words set. And the neighboring words’ part-of-speeches also contain useful information for WSD. In this part, we use a POS tagger (Giménez and Márquez, 2004) to assign POS tags to those tokens. We get the left and right 3 words’ POS tags together with their position information in the target words’ sentence. For example, the word age is to be disambiguated in the sentence of “… This is the 2 4 166 http://w3.msi.vxu.se/~nivre/research/MaltParser.html <head>age</head> of new media , the era of …”. The features then will be added to the feature vector are “DT_0, VBZ_0, DT_0, NN_t, IN_1, JJ_1, NNS_1”, in which _0/_1 stands for the word with current POS tag is in the left/right side of the target word. The POS tag set in use here is Penn Treebank Tagse"
S07-1034,W06-2933,0,0.0267094,"removed. Take the same instance last subsection has mentioned as example. The features we extracted are “this_0, be_0, the_0, age_t, of_1, new_1, medium_1”. Like POS, _0/_1 stands for the word is in the left/right side of the target word. Then the features were added to the feature vector space. 4.4 Syntactic Relations Many effective context words are not in a short distance to the target word, but we shouldn’t enlarge the window size too much in case of including too many noises. A solution to this problem is to use the syntactic relations of the target word and its parent head word. We use Nivre et al., (2006)’s dependency parser. In this part, we get 4 features from every instance: head word of the target word, the head word’s POS, the head word’s dependency relation with the target word and the relative position of the head word to the target word. Still take the same instance which has been mentioned in the las subsection as example. The features we extracted are “SYN_HEAD_is, SYN_HEADPOS_VBZ, SYN_RELATION_PRD, SYN_HEADRIGHT”, in which SYN_HEAD_is stands for is is the head word of age; SYN_HEADPOS_VBZ stands for the POS of the 5 6 5 http://www.lsi.upc.es/~nlp/SVMTool/PennTreebank.html 167 Data S"
S10-1091,P05-1074,0,0.0367893,"e can be translated to columna, columna vertebral, pilar and convicciones etc. in Spanish, and these words also have other relevant translations in English, such as vertebral column, column, pillar and convictions etc., which are semantically related to the target word backbone. We use a statistical machine translation system to calculate the translation probability from English to another language (called as pivot language) as well as the translation probability from that language to English. By multiplying these two probabilities, we get a paraphrase probability. This method was defined in (Bannard and Callison-Burch, 2005). In our system, we choose the top k paraphrases rs(w, nj ) = ∑ p(f |w)p(nj |f ), (4) f where f is the pivot language word. We use the English-Spanish parallel text from Europarl (Koehn, 2005). We choose Spanish as the pivot language because in the both directions the BLEU score of the translation between English and Spanish is relatively higher than other English and other languages (Koehn, 2005). 4 Data set and System Settings The organizers of the SemEval-2 specific domain WSD task provide no training data but raw background data in the environmental domain. The English background data is o"
S10-1091,2005.mtsummit-papers.11,0,0.0436287,"Missing"
S10-1091,H05-1053,0,0.0496578,"Missing"
S10-1091,J07-4005,0,0.106253,"ems when the training data is inadequate. In past evaluations, MFS from WordNet performed even better than most of the unsupervised systems (Snyder and Palmer, 2004; Navigli et al., 2007). MFS is usually obtained from a large scale sense tagged corpus, such as SemCor (Miller et al., 1994). However, some polysemous words have different MFS in different domains. For example, in the Koeling et al. (2005) corpus, target word coach means “manager” mostly in the S PORTS domain but means “bus” mostly in the F INANCE domain. So when the MFS is applied to specific domains, it needs to be re-estimated. McCarthy et al. (2007) proposed an unsupervised predominant word sense acquisition method which obtains domain specific MFS without sense tagged corpus. In their method, a thesaurus, in which words are connected with their distributional similarity, is constructed from the domain raw text. Word senses are ranked by their prevalence score which is calculated using the thesaurus and the sense inventory. In this paper, we propose another way to construct the thesaurus. We use statistical machine Figure 1: The architecture of HIT-CIR translation (SMT) techniques to extract paraphrase pairs from bilingual parallel text."
S10-1091,S07-1006,0,0.0286945,"Missing"
S10-1091,P07-1096,0,0.0354263,"Missing"
S10-1091,W04-0811,0,0.0259013,"Missing"
S10-1091,S10-1013,0,\N,Missing
S10-1091,H93-1061,0,\N,Missing
S12-1050,W06-2920,0,0.0898414,"the three sections. Data Set CTB files 1-10; 36-65;81-121; Training 1001-1078; 1100-1119; 1126-1140 Devel 66-80; 1120-1125 Test 11-35; 1141-1151 Total 1-121; 1001-1078 1100-1151 # sent. # words. 8301 250311 534 15329 1233 34311 10068 299951 Table 3: Statistics of training, development and test data. 3.2 Data Format The data format is identical to that of a syntactic dependency parsing shared task. All the sentences are in one text file, with each sentence separated by a blank line. Each sentence consists of one or more tokens, and each token is represented on one line consisting of 10 fields. Buchholz and Marsi (2006) provide more detailed information on the format. Fields are separated from each other by a tab. Only five of the 10 fields are used: token id, form, pos tagger, head, and deprel. Head denotes the semantic dependency of each word, and deprel denotes the corresponding semantic relations of the dependency. In the data, the lemma column is filled with the form and the cpostag column with the postag. Figure 2 shows an example. 3.3 Evaluation Method LAS, which is a method widely used in syntactic dependency parsing, is used to evaluate the performance of the semantic dependency parsing system. LAS"
S12-1050,P10-1110,0,0.0245126,"vel-label is introduced into the semantic relation, for example, the r-{Main Semantic Roles}, with NJUParser exploiting special strategies to handle it. However, this third method does not show positive performance. 3. Zhijun Wu-1 This system extends the second-order of the MSTParser by adding third-order features, and then applying this model to Chinese semantic dependency parsing. In contrast to Koo and Collins (2010) this system does not implement 382 • Parsing each sentence using Nivre’s arc standard, Nivre’s arc eager (Nivre and Nilsson, 2005; Nivre, 2008), and Liang’s dynamic algorithm (Huang and Sagae, 2010); • Combining parses given by the three parsers into a weighted directed graph; • Using the Chu-Liu-Edmonds algorithm to search for the final parse for each sentence. 5. Giuseppe Attardi-SVM-1-R, Giuseppe AttardiSVM-1-rev We didn’t receive the system description of these two systems. 5 Results & Analysis LAS is the main evaluation metric in Chinese Semantic Dependency Parsing, whereas UAS is the secondary metric. Table 4 shows the results for these two indicators in all participating systems. As shown in Table 4, the Zhou Qiaoli-3 system achieved the best results with LAS of 61.84. The LAS val"
S12-1050,P10-1001,0,0.0410518,"semicolon should be used to split the sentence. Second, the last character in a Chinese word is extracted as the lemma, since it usually contains the main sense or semantic class. Third, the multilevel-label is introduced into the semantic relation, for example, the r-{Main Semantic Roles}, with NJUParser exploiting special strategies to handle it. However, this third method does not show positive performance. 3. Zhijun Wu-1 This system extends the second-order of the MSTParser by adding third-order features, and then applying this model to Chinese semantic dependency parsing. In contrast to Koo and Collins (2010) this system does not implement 382 • Parsing each sentence using Nivre’s arc standard, Nivre’s arc eager (Nivre and Nilsson, 2005; Nivre, 2008), and Liang’s dynamic algorithm (Huang and Sagae, 2010); • Combining parses given by the three parsers into a weighted directed graph; • Using the Chu-Liu-Edmonds algorithm to search for the final parse for each sentence. 5. Giuseppe Attardi-SVM-1-R, Giuseppe AttardiSVM-1-rev We didn’t receive the system description of these two systems. 5 Results & Analysis LAS is the main evaluation metric in Chinese Semantic Dependency Parsing, whereas UAS is the se"
S12-1050,W03-1712,0,0.174219,"lization, easy comprehension, high efficiency, and so on. Dependency parsing has been studied intensively in recent decades, with most related work focusing on syntactic structure. Many research papers on Chinese linguistics demonstrate the remarkable difference between semantics and syntax (Jin, 2001; Zhou and Zhang, 2003). Chinese is a meaning-combined language with very flexible syntax, and semantics are more stable than syntax. The word is the basic unit of semantics, and the structure and meaning of a sentence consists mainly of a series of semantic dependencies between individual words (Li et al., 2003). Thus, a reasonable endeavor is to exploit dependency parsing for semantic analysis of Chinese languages. Figure 1 shows an example of Chinese semantic dependency parsing. 378 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 378–384, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics root d-genetive d-restrictive content d-restrictive d-genetive 国际 货币 International Monetary 基金 Fund d-restrictive prep-depend agent 组织 调低 organization turn down 对 for 全球 global d-domain 经济 economy aux-depend 增长 increasing 的 of 预测 prediction Figure 1: An"
S12-1050,D11-1109,1,0.841365,"h dependency. The system-combining strategy involves three steps: 1. Zhou Qiaoli-1, Zhou Qiaoli-2, Zhou Qiaoli-3 These three systems propose a divide-andconquer strategy for semantic dependency parsing. The Semantic Role (SR) phrases are identified (Cai et al., 2011) and then replaced by their head or the SR of the head. The original sentence is thus divided into two types of parts that can be parsed separately. The first type is SR phrase parsing, and the second involves the replacement of SR phrases with either their head or the SR of the head. Finally, the paper takes a graph-based parser (Li et al., 2011) as the semantic dependency parser for all parts. These three systems differ in their phrase identification strategies. 2. NJU-Parser-1, NJU-Parser-2 The NJU-Parser is based on the state-of-theart MSTParser (McDonald, 2006). NJU-Parser applies three methods to enhance semantic dependency parsing. First, sentences are split into sub-sentences using commas and semicolons: (a) sentences are split using only commas and semicolons, as in the primary system, and (b) classifiers are used to determine whether a comma or semicolon should be used to split the sentence. Second, the last character in a Ch"
S12-1050,P05-1013,0,0.0275768,"usually contains the main sense or semantic class. Third, the multilevel-label is introduced into the semantic relation, for example, the r-{Main Semantic Roles}, with NJUParser exploiting special strategies to handle it. However, this third method does not show positive performance. 3. Zhijun Wu-1 This system extends the second-order of the MSTParser by adding third-order features, and then applying this model to Chinese semantic dependency parsing. In contrast to Koo and Collins (2010) this system does not implement 382 • Parsing each sentence using Nivre’s arc standard, Nivre’s arc eager (Nivre and Nilsson, 2005; Nivre, 2008), and Liang’s dynamic algorithm (Huang and Sagae, 2010); • Combining parses given by the three parsers into a weighted directed graph; • Using the Chu-Liu-Edmonds algorithm to search for the final parse for each sentence. 5. Giuseppe Attardi-SVM-1-R, Giuseppe AttardiSVM-1-rev We didn’t receive the system description of these two systems. 5 Results & Analysis LAS is the main evaluation metric in Chinese Semantic Dependency Parsing, whereas UAS is the secondary metric. Table 4 shows the results for these two indicators in all participating systems. As shown in Table 4, the Zhou Qia"
S12-1050,J08-4003,0,0.0232911,"n sense or semantic class. Third, the multilevel-label is introduced into the semantic relation, for example, the r-{Main Semantic Roles}, with NJUParser exploiting special strategies to handle it. However, this third method does not show positive performance. 3. Zhijun Wu-1 This system extends the second-order of the MSTParser by adding third-order features, and then applying this model to Chinese semantic dependency parsing. In contrast to Koo and Collins (2010) this system does not implement 382 • Parsing each sentence using Nivre’s arc standard, Nivre’s arc eager (Nivre and Nilsson, 2005; Nivre, 2008), and Liang’s dynamic algorithm (Huang and Sagae, 2010); • Combining parses given by the three parsers into a weighted directed graph; • Using the Chu-Liu-Edmonds algorithm to search for the final parse for each sentence. 5. Giuseppe Attardi-SVM-1-R, Giuseppe AttardiSVM-1-rev We didn’t receive the system description of these two systems. 5 Results & Analysis LAS is the main evaluation metric in Chinese Semantic Dependency Parsing, whereas UAS is the secondary metric. Table 4 shows the results for these two indicators in all participating systems. As shown in Table 4, the Zhou Qiaoli-3 system a"
S12-1050,W03-1707,0,0.242389,"Missing"
S12-1050,J03-4003,0,\N,Missing
S16-1167,W06-2920,0,0.0405957,"XT #sent #word #sent #word Train 8,301 250,249 10,817 128,095 Dev 534 15,325 1,546 18,257 Test 1,233 34,305 3,096 36,097 Table 2: Statics of the corpus. 3.2 • Labeled precision (LP), recall (LR), F1 (LR) and recall for non-local dependencies (NLR); Data Format All data provided for the task uses a columnbased file format, similar to the one of the 2006 CoNLL Shared Task (Table 3). Each training/developing/testing set is a text file, containing sentences separated by a blank line. Each sentence consists of more than one tokens, and each token is represented on one line consisting of 10 fields. Buchholz and Marsi (2006) provide more detailed information on the format. It’s worth noting that if one word has more than one heads, it will appear in more than one lines in the training/developing/testing files continuously. Fields are separated from each other by a tab. Only five of the 10 fields are used: token id, form, pos tagger, head, and deprel. Head denotes the semantic dependency of each word, and deprel denotes the corresponding semantic relations of the dependency. In the data, the lemma column is filled with the form and the cpostag column with the postag. 3.3 files containing sentences in two domains w"
S16-1167,S12-1050,1,0.655272,"aphs that can capture richer latent semantics, and the goal of this task is to identify such semantic structures from a corpus of Chinese sentences. We provide two distinguished corpora in the NEWS domain with 10,068 sentences and the TEXTBOOKS domain with 14,793 sentences respectively. We will first introduce the motivation for this task, and then present the task in detail including data preparation, data format, task evaluation and so on. At last, we briefly describe the submitted systems and analyze these results. 1 Yu Ding† Introduction This task is a rerun of the task 5 at SemEval 2012 (Che et al., 2012), named Chinese semantic dependency parsing (SDP). In the previous task, we aimed at investigating “deep” semantic relations within sentences through tree-structured dependencies. As traditionally defined, syntactic dependency parsing results are connected trees defined over all words of a sentence and language-specific grammatical functions. On the contrary, in semantic dependency parsing, each head-dependent arc instead bears a semantic relation, rather than grammatical relation. In this way, semantic dependency parsing results can be used to answer questions directly, like who did what to w"
S16-1167,S14-2008,0,0.0393196,"head-dependent arc instead bears a semantic relation, rather than grammatical relation. In this way, semantic dependency parsing results can be used to answer questions directly, like who did what to whom when and where. Figure 1 illustrates an example of semantic dependency graph. Here, “她 (she)” is the argument of “脸色 (face)” and at the same time it is an argument of “病 (disease)”. Researchers in dependency parsing community realized dependency parsing restricted in a tree structure is still too shallow, so they explored semantic information beyond tree structure in task 8 at SemEval 2014 (Oepen et al., 2014) and task 18 at SemEval 2015 (Oepen et al., 2015). They provided data in similar structure with what we are going to provide, but in distinct semantic representation systems. Once again we propose this task to promote research that will lead to deeper understanding of Chinese sentences, and we believe that freely available and well annotated corpora which can be used as common testbed is necessary to promote research in data-driven statistical dependency parsing. 1074 Proceedings of SemEval-2016, pages 1074–1080, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Lin"
S16-1167,S15-2153,0,0.0880504,"tion, rather than grammatical relation. In this way, semantic dependency parsing results can be used to answer questions directly, like who did what to whom when and where. Figure 1 illustrates an example of semantic dependency graph. Here, “她 (she)” is the argument of “脸色 (face)” and at the same time it is an argument of “病 (disease)”. Researchers in dependency parsing community realized dependency parsing restricted in a tree structure is still too shallow, so they explored semantic information beyond tree structure in task 8 at SemEval 2014 (Oepen et al., 2014) and task 18 at SemEval 2015 (Oepen et al., 2015). They provided data in similar structure with what we are going to provide, but in distinct semantic representation systems. Once again we propose this task to promote research that will lead to deeper understanding of Chinese sentences, and we believe that freely available and well annotated corpora which can be used as common testbed is necessary to promote research in data-driven statistical dependency parsing. 1074 Proceedings of SemEval-2016, pages 1074–1080, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics Exp ROOT time Poss ROOT 现在 now 她 she 脸色"
S16-1167,P14-1042,0,0.0275411,"ins will be released separately. The final rankings will refer to the average results of the two testing files (taking training data size into consideration). We compare predicted dependencies (predicate-roleargument triples, and some of them contain roots of the whole sentences) with our human-annotated ones, which are regarded as gold dependencies. Our evaluate measures are on two granularity, dependency arc and the complete sentence. Labeled and unlabeled precision and recall with respect to predicted dependencies will be used as evaluation measures. Since non-local dependencies (following Sun et al. (2014), we call these dependency arcs making dependency trees collapsed non-local ones) discovery is extremely difficult, we will evaluate non-local dependencies separately. For sentences level, we will use labeled and unlabeled exact match to measure sentence parsing accuracy. Following Task 8 at SemEval 2014, below and in other taskrelated contexts, we abbreviate these metrics as: Evaluation During the phase of evaluation, each system should propose parsing results on the previously unseen testing data. Similar with training phase, testing 1078 • Unlabeled precision (UP), recall (UR), F1 (UF) and"
S16-1167,W03-1707,0,0.0686364,"he closed track encourages participates to focus on building dependency 1077 3.1 Corpus Statistics Since texts in rich categories have different linguistic properties with different communication purpose. This task provides two distinguished corpora in appreciable quantity respectively in the domain of NEWS and TEXTBOOKS (from primary school textbooks). Each corpus contains particular linguistic phenomena. We provide 10,068 sentences of NEWS and 14,793 sentence of TEXTBOOKS. The sentences of news keep the same with the data in task 5 at SemEval 2012, which come from the Chinese PropBank 6.01 (Xue and Palmer, 2003) as the raw corpus to create the Chinese semantic dependency corpus. Sentences were selected by index: 1-121, 1001-1078, 1100-1151. TEXTBOOKS refer to shorter sentences with various ways of expressions, i.e., colloquial sentences (3,000), primary school texts (11,793). Detailed statics are described in Table 2. NEWS TEXT #sent #word #sent #word Train 8,301 250,249 10,817 128,095 Dev 534 15,325 1,546 18,257 Test 1,233 34,305 3,096 36,097 Table 2: Statics of the corpus. 3.2 • Labeled precision (LP), recall (LR), F1 (LR) and recall for non-local dependencies (NLR); Data Format All data provided f"
W04-1120,C94-1032,0,0.0235379,"ayer components; on the other hand the incorrect analysis of lower layers must reduce the accuracy of higher layers. In Chinese Word-Seg component, many segmentation ambiguities which cannot be solved using only lexical information. In order to improve the performance of Word-Seg, we have to use some syntax and even semantic information. Without correct Word-Seg results, however the syntax and semantic parser cannot obtain a correct analysis. It is a chain debts problem. People have tried to solve the error-multiplied problem by integrating multi-layers into a uniform model (Gao et al., 2001; Nagata, 1994). But with the increasing number of integrated layers, the model becomes too complex to build or solve. The feedback mechanism (Wu and Jiang, 1998) helps to use the information of high layers to control the final result. If the analysis at feedback point cannot be passed, the whole analysis will be denied. This mechanism places too much burden on the function of feedback point. This leads to the problems that a correct lower layer result may be rejected or an error result may be accepted. We propose a new Multilayer Search Mechanism (MSM) to solve the problems mentioned above. Based on the mec"
W05-0627,J96-1002,0,0.0317627,"ntic interpretation is needed, such as question and answering, information extraction, machine translation, paraphrasing, and so on. ∗ This research was supported by National Natural Science Foundation of China via grant 60435020 Last year, CoNLL-2004 hold a semantic role labeling shared task (Carreras and M`arquez, 2004) to test the participant systems’ performance based on shallow syntactic parser results. In 2005, SRL shared task is continued (Carreras and M`arquez, 2005), because it is a complex task and now it is far from desired performance. In our SRL system, we select maximum entropy (Berger et al., 1996) as a classifier to implement the semantic role labeling system. Different from the best classifier reported in literatures (Pradhan et al., 2005) – support vector machines (SVMs) (Vapnik, 1995), it is much easier for maximum entropy classifier to handle the multi-class classification problem without additional post-processing steps. The classifier is much faster than training SVMs classifiers. In addition, maximum entropy classifier can be tuned to minimize over-fitting by adjusting gaussian prior. Xue and Palmer (2004; 2005) and Kwon et al. (2004) have applied the maximum entropy classifier"
W05-0627,W04-2412,0,0.0947866,"Missing"
W05-0627,W05-0620,0,0.112708,"Missing"
W05-0627,C04-1179,0,0.0311086,"Missing"
W05-0627,W04-3212,0,0.147832,"Missing"
W08-2134,W05-0627,1,0.833091,"o the predicate (negative for being left to the predicate and positive for right), another feature is formed, namely “Bag of POS (Numbered)”. WIND5 BIGRAM (b3): 5 closest words from both left and right plus the predicate itself, in total 11 words form a “window”, within which bigrams are enumerated. The final optimized feature set for the task of predicate classification is (a1, a21, a23, a71, a72, a73, a74, a81, a82, a83, a84, a9, b11, b12, b22, b3, a71+a9). 3.4 Semantic Role Classification In our system, the identification and classification of semantic roles are achieved in a single stage (Liu et al., 2005) through one single classifier (actually two, one for noun predicates, and the other for verb predicates). Each word in a sentence is given probabilities to be each semantic role (including none of the these roles) for a predicate. Features introduced in addition to those of the previous subsections are the following: POS PATH (c11), REL PATH (c12): The “POS Path” feature consists of POS tags of the words along the path from a word to the predicate. Other than “Up” and “Down”, the “Left” and “Right” direction of the path is added. Similarly, the “Relation Path” feature consists of the relation"
W08-2134,C04-1197,0,0.0272563,"ost Inference. During the Predicate Identification stage we examine each word in a sentence to discover target predicates, including both noun predicates (from NomBank) and verb predicates (from PropBank). In the Predicate Classification stage, each predicate is assigned a certain sense number. For each predicate, the probabilities of a word in the sentence to be each semantic role are predicted in the Semantic Role Classification stage. Maximum entropy model is selected as our classifiers in these stages. Finally an ILP (Integer Linear Programming) based method is adopted for post inference (Punyakanok et al., 2004). 3.2 Predicate Identification The predicate identification is treated as a binary classification problem. Each word in a sentence is predicted to be a predicate or not to be. A set of features are extracted for each word, and an optimized subset of them are adopted in our final system. The following is a full list of the features: DEPREL (a1): Type of relation to the parent. WORD (a21), POS (a22), LEMMA (a23), HEAD (a31), HEAD POS (a32), HEAD LEMMA (a33): The forms, POS tags and lemmas of a word and it’s headword (parent) . FIRST WORD (a41), FIRST POS (a42), FIRST LEMMA (a43), LAST WORD (a51)"
W08-2134,W08-2121,0,0.0848569,"Missing"
W09-1207,W09-1201,0,0.118581,"Missing"
W09-1207,D08-1008,0,0.0936805,"Missing"
W09-1207,kawahara-etal-2002-construction,0,0.0239566,"ntropy classifier is implemented with Maximum Entropy Modeling Toolkit1 . The classifier parameters are tuned with the development data for different languages respectively. lp solve 5.52 is chosen as our ILP problem solver. 1 2 http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html http://sourceforge.net/projects/lpsolve 5 Experiments 5.1 Experimental Setup We participate in the CoNLL 2009 shared task with all 7 languages: Catalan (Taul´e et al., 2008), Chinese (Palmer and Xue, 2009), Czech (Hajiˇc et al., 2006), English (Surdeanu et al., 2008), German (Burchardt et al., 2006), Japanese (Kawahara et al., 2002), and Spanish (Taul´e et al., 2008). Besides the closed challenge, we also submitted the open challenge results. Our open challenge strategy is very simple. We add the SRL development data of each language into their training data. The purpose is to examine the effect of the additional data, especially for out-of-domain (ood) data. Three machines (with 2.5GHz Xeon CPU and 16G memory) were used to train our models. During the peak time, Amazon’s EC2 (Elastic Compute Cloud)3 was used, too. Our system requires 15G memory at most and the longest training time is about 36 hours. During training the"
W09-1207,W02-1006,0,0.0162974,"dicate Classification The predicate classification is regarded as a supervised word sense disambiguation (WSD) task here. The task is divided into four steps: 1. Target words selection: predicates with multiple senses appearing in the training data are selected as target words. 2. Feature extraction: features in the context around these target words are extracted as shown in Table 4. The detailed explanation about these features can be found from (Che et al., 2008). 3. Classification: for each target word, a Support Vector Machine (SVM) classifier is used to classify its sense. As reported by Lee and Ng (2002) and Guo et al. (2007), SVM shows good performance on the WSD task. Here libsvm (Chang and Lin, 2001) is used. The linear kernel function is used and the trade off parameter C is 1. 4. Post processing: for each predicate in the test data which does not appear in the training data, its first sense in the frame files is used. 4 Semantic Role Labeling The semantic role labeling (SRL) can be divided into two separate stages: semantic role classification (SRC) and post inference (PI). During the SRC stage, a Maximum entropy (Berger et al., 1996) classifier is used to predict the probabilities of a"
W09-1207,P05-1013,0,0.0387666,"role labeling. 2 Syntactic Dependency Parsing We extend our CoNLL 2008 graph-based model (Che et al., 2008) in four ways: 1. We use bigram features to choose multiple possible syntactic labels for one arc, and decide the optimal label during decoding. 2. We extend the model with sibling features (McDonald, 2006). 3. We extend the model with grandchildren features. Rather than only using the left-most and rightmost grandchildren as Carreras (2007) and Johansson and Nugues (2008) did, we use all left and right grandchildren in our model. 4. We adopt the pseudo-projective approach introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English. 49 For each arc, we firstly use unigram features to choose the K1 -best labels. The second parameter of f1lbl (·) indicates whether the node is the head of the arc, and the third parameter indicates the direction. L denotes the whole label set. Then we re-rank the labels by combining the bigram features, and choose K2 -best labels. During decoding, we only use the K2 labels chosen for each arc (K2 ¿ K1 < |L|). 2.2 High-order Model and Algorithm Following the Eisner (2000) algorithm, we use spans as the basic unit. A s"
W09-1207,C04-1197,0,0.029135,"tage anymore. For a predicate of each language, two classifiers (one for noun predicates, and the other for verb predicates) predict probabilities of each word in a sentence to be each semantic role (including virtual role “NULL”). The features used in this stage are listed in Table 4. The probability of each word to be a semantic role for a predicate is given by the SRC stage. The results generated by selecting the roles with the largest probabilities, however, do not satisfy some constrains. As we did in the last year’s system (Che et al., 2008), we use the ILP (Integer Linear Programming) (Punyakanok et al., 2004) to get the global optimization, which is satisfied with three constrains: C1: Each word should be labeled with one and only one label (including the virtual label “NULL”). C2: Roles with a small probability should never be labeled (except for the virtual role “NULL”). The threshold we use in our system is 0.3. C3: Statistics show that some roles (except for the virtual role “NULL”) usually appear once for a predicate. We impose a no-duplicate-roles constraint with a no-duplicate-roles list, which is constructed according to the times of semantic roles’ duplication for each single predicate. T"
W09-1207,W08-2121,0,0.165854,"Missing"
W09-1207,taule-etal-2008-ancora,0,0.0539562,"Missing"
W09-1207,burchardt-etal-2006-salsa,0,\N,Missing
W09-1207,S07-1034,1,\N,Missing
W09-1207,J96-1002,0,\N,Missing
W09-1207,W08-2134,1,\N,Missing
W09-1207,D07-1101,0,\N,Missing
W12-6316,C10-3004,1,0.878455,"Missing"
W12-6316,D11-1090,0,0.0789906,"ow. Information of unlabeled data can be easily computed and benefit the word segmentation model. When integrated into machine learning framework, it will help reduce sparsity issue caused by the out of vocabulary words. 2.3.1 Mutual Information In probability theory, mutual information measures the mutual dependency of two random variables. Empirical study shows that observation of high mutual information between two characters may indicates real association of these two characters in a word, while low mutual information usually means they belongs to different words. In this paper, we follow Sun and Xu (2011)’s definition of mutual information. For a character bigram ci ci+1 , their mutual information is computed as follow: • character unigram: cs (i − 2 ≤ s ≤ i + 2) • character bigram: cs cs+1 (i − 2 ≤ s ≤ i + 1), cs cs+2 (i − 2 ≤ s ≤ i) • character trigram: cs−1 cs cs+1 (s = i) • repetition of characters: is cs equals cs+1 (i− 1 ≤ s ≤ i), is cs equals cs+2 (i − 2 ≤ s ≤ i) • character type: is ci an alphabet, digit, punctuation or others M I(ci ci+1 ) = log 2.2 Rule Detection Features p(ci ci+1 ) p(ci )p(ci+1 ) For each character ci , M I(ci ci+1 ) and M I(ci−1 ci ) are computed and rounded down"
W12-6316,I11-1035,0,0.0608607,"Missing"
W12-6330,N06-2033,0,0.0369784,"s been investigated by many researchers. Most methods of PSG parsing exploited some manly annotated corpus and proposed a single statistical model (Petrov and Klein, 2007; Zhang and Clark, 2009) based on the corpus. For Chinese, Tsinghua Chinese Treebank (TCT) (Qiang, 2004) and Penn Chinese TreeBank (CTB) (Xue et al., 2005) are two most popular manly annotated corpus. In this paper, we are especially interested in parser combination. Many past works have suggest a number of methods for parser combination. These methods concern on combing different parsers which are trained on the same corpus. Sagae and Lavie (2006) proposed a constituent reparsing method for multiple parsers combina2. The grammars of TCT corpus are very different that of CTB corpus. We should transform CTB grammars into TCT grammars before final combination. If these two issues have been done already, we can apply CKY reparsing algorithm and get the final parsing result. The rest of the paper is organized as follows. Section 2 introduces the overall system architecture. And then we introduce our method in detail. In section 3 we present the binarization algorithm used in the system. Section 4 describes the CKY reparsing algorithm. Secti"
W12-6330,D08-1018,0,0.0588326,"Missing"
W12-6330,W09-3825,0,0.0614032,"Missing"
W12-6330,D09-1161,0,0.0291345,"Missing"
W12-6330,P06-1055,0,0.0839699,"Missing"
W12-6330,N07-1051,0,\N,Missing
W12-6330,J03-4003,0,\N,Missing
W16-4907,N16-1030,0,0.0275254,"e last word “损害(damage)” shows up. Traditional models with features extracted from a limited context window may not be able to handle these situations. Neural network-based models have been extensively used in natural language processing (NLP) during recent years, due to their strong capability of automatical feature learning. In particular, the long shortterm memory (LSTM) (Hochreiter and Schmidhuber, 1997) based recurrent neural networks (RNN) have been proved to be highly effective in various applications that involves sequence modeling, such as language modeling, named entity recognition (Lample et al., 2016) and parsing (Vinyals et al., 2015), etc. Therefore, in this paper, we propose to use LSTM-based RNNs to solve the CGED problem. In order to leverage both the merits of CRF models and LSTM models, we further present an ensemble model using Stacking (Nivre and McDonald, 2008). Evaluations on the NLP-TEA-3 shared task for CGED show that our models achieve the best F1-scores in all levels and the best recall in two levels. The rest of the paper is organized as follows: Section 2 gives the definition of the CEGD task. Section 3 describes how LSTM network is used to predict errors and what other wo"
W16-4907,C14-2015,0,0.147888,"n traditional Chinese texts rather than simplified Chinese, and one sentence includes one error at most in last two years. There have been several studies focused on Chinese grammatical error detection. Wu et al. (2010) proposed a method using both Relative Position Language Model and Parse Template Language Model to detect Chinese errors written by US learner. Yu and Chen (2012) proposed a classifier to detect wordordering errors in Chinese sentences from the HSK dynamic composition corpus. Lee et al. (2013) proposed linguistic rule based Chinese error detection for second language learning. Lee et al. (2014) developed a sentence judgment system using both rule-based and n-gram statistical methods to detect grammatical errors in Chinese sentences. However, all of these previous works used hand-crafted features which may be incomplete and cause the loss of some important information. Comparatively, our neural network approaches have strong capability of automatical feature learning and are completely data-driven. 6 Conclusion This paper describes our system in the NLP-TEA-3 task for CGED-HSK. We explored the CRF-based model, the LSTM-based model and further used stacking to combine the two models."
W16-4907,W15-4401,0,0.10525,"m presents the best F1 scores in all three levels and also the best recall rates in the last two levels on evaluation dataset. However, the results of this task are not that credible because there are many ways to correct a wrong Chinese sentence. For example, deleting some redundant words may replace errors of missing words. 5 Related Works In NLP-TEA-1 (Yu et al., 2014) shared task for CGED, there were four types of errors, which were the same as the task of this year. The evaluation was only based on detection of error occurrence, disregarding the recognization of boundaries. In NLP-TEA-2 (Lee et al., 2015) shared task for CGED, the participating systems are required to not only detect the errors but also locate them. Evaluations were focused on traditional Chinese texts rather than simplified Chinese, and one sentence includes one error at most in last two years. There have been several studies focused on Chinese grammatical error detection. Wu et al. (2010) proposed a method using both Relative Position Language Model and Parse Template Language Model to detect Chinese errors written by US learner. Yu and Chen (2012) proposed a classifier to detect wordordering errors in Chinese sentences from"
W16-4907,P08-1108,0,0.167278,"o their strong capability of automatical feature learning. In particular, the long shortterm memory (LSTM) (Hochreiter and Schmidhuber, 1997) based recurrent neural networks (RNN) have been proved to be highly effective in various applications that involves sequence modeling, such as language modeling, named entity recognition (Lample et al., 2016) and parsing (Vinyals et al., 2015), etc. Therefore, in this paper, we propose to use LSTM-based RNNs to solve the CGED problem. In order to leverage both the merits of CRF models and LSTM models, we further present an ensemble model using Stacking (Nivre and McDonald, 2008). Evaluations on the NLP-TEA-3 shared task for CGED show that our models achieve the best F1-scores in all levels and the best recall in two levels. The rest of the paper is organized as follows: Section 2 gives the definition of the CEGD task. Section 3 describes how LSTM network is used to predict errors and what other works we have done. Section 4 ∗ Email correspondence. 49 Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications, pages 49–56, Osaka, Japan, December 12 2016. shows the evaluation results. Section 5 gives some related works. Secti"
W16-4907,C12-1184,0,0.199883,"error occurrence, disregarding the recognization of boundaries. In NLP-TEA-2 (Lee et al., 2015) shared task for CGED, the participating systems are required to not only detect the errors but also locate them. Evaluations were focused on traditional Chinese texts rather than simplified Chinese, and one sentence includes one error at most in last two years. There have been several studies focused on Chinese grammatical error detection. Wu et al. (2010) proposed a method using both Relative Position Language Model and Parse Template Language Model to detect Chinese errors written by US learner. Yu and Chen (2012) proposed a classifier to detect wordordering errors in Chinese sentences from the HSK dynamic composition corpus. Lee et al. (2013) proposed linguistic rule based Chinese error detection for second language learning. Lee et al. (2014) developed a sentence judgment system using both rule-based and n-gram statistical methods to detect grammatical errors in Chinese sentences. However, all of these previous works used hand-crafted features which may be incomplete and cause the loss of some important information. Comparatively, our neural network approaches have strong capability of automatical fe"
W18-3707,J90-1003,0,0.23071,"Missing"
W18-3707,W16-4906,0,0.103894,"Missing"
W18-3707,I17-4006,0,0.569554,"rmance. For a target word, we compute ePMI together with neighbor words and map them to discrete value internals as features. Gaussian ePMI. we use trainable weighted Gaussian distribution to leverage words’ distance. (4) Novel Features The task heavily depends on the prior knowledge that can be represented by the selection of features. In practice, feature selection is straightforward phase to affects the model’s performance. Better task-specific features simplify the complexity of a model, whereby improve the performance in all levels. Besides the feature engineering introduced by ALI team (Yang et al., 2017), we design several additional features that will be discussed next. Word Segmentation. we found that sentences in segments are essential to solving the grammatical task due to Chinese’s words being combined without segmented spaces that help to indicate the exact meaning of the sentence without ambiCombination of POS and PMI. our intuition is that the efficiency of PMI-score (Church and Hanks, 1 54 http://www.ltp-cloud.com/ 1990) between words is more relevant to what their POSs (Ferraro et al., 2014) exactly are; PMI-scores for different POS-pairs have different meaning, even though the POS-"
W18-3707,C12-1184,0,0.0221177,"hat are correctly identified; FP (False Positive) is the number of error-sentences that are incorrectly identified as correct sentences; TN (True Negative) is the number of correctsentences that are correctly identified; FN (False Negative) is the number of correct-sentences that are incorrectly identified as containing grammatical errors. The metrics that are used to measure a system’s performance has three levels: detection, identification, and position. Each level is evaluated with the help of the confusion matrix based on these metrics (Lee et al.,2016):      dom field (CRF) network (Yu and Chen, 2012) to form a BiLSTM-CRF model can efficiently use past and future information via a Bi-LSTM layer and connecting consecutive output layers from Bi-LSTM via a CRF layer such that the sequence tagging problems can be solved better. Two kinds of potentials are defined in the BiLSTM-CRF model (Huang et al.,2015): emission and transition potentials. The emission potential P is the matrix of scores output by the BiLSTM network, of size , where k in the size of distinct tags. Specifically, represents the emission score of the word to the tag in an input sequence. The transition potential A is the matri"
W18-3707,W16-4907,1,0.823817,"Missing"
