2021.jeptalnrecital-taln.30,Simplification automatique de textes biom{\\'e}dicaux en fran{\\c{c}}ais: lorsque des donn{\\'e}es pr{\\'e}cises de petite taille aident ({F}rench Biomedical Text Simplification : When Small and Precise Helps ),2021,-1,-1,2,1,5648,remi cardon,Actes de la 28e Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf{\\'e}rence principale,0,"Nous pr{\'e}sentons un r{\'e}sum{\'e} en fran{\c{c}}ais et un r{\'e}sum{\'e} en anglais de l{'}article (Cardon {\&} Grabar, 2020), publi{\'e} dans les actes de la conf{\'e}rence 28th International Conference on Computational Linguistics (COLING 2020)."
2021.jeptalnrecital-deft.1,Classification de cas cliniques et {\\'e}valuation automatique de r{\\'e}ponses d{'}{\\'e}tudiants : pr{\\'e}sentation de la campagne {DEFT} 2021 (Clinical cases classification and automatic evaluation of student answers : Presentation of the {DEFT} 2021 Challenge),2021,-1,-1,2,0,5675,cyril grouin,Actes de la 28e Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Atelier D{\\'E}fi Fouille de Textes (DEFT),0,"Le d{\'e}fi fouille de textes (DEFT) est une campagne d{'}{\'e}valuation annuelle francophone. Nous pr{\'e}sentons les corpus et baselines {\'e}labor{\'e}es pour trois t{\^a}ches : (i) identifier le profil clinique de patients d{\'e}crits dans des cas cliniques, (ii) {\'e}valuer automatiquement les r{\'e}ponses d{'}{\'e}tudiants sur des questionnaires en ligne (Moodle) {\`a} partir de la correction de l{'}enseignant, et (iii) poursuivre une {\'e}valuation de r{\'e}ponses d{'}{\'e}tudiants {\`a} partir de r{\'e}ponses d{\'e}j{\`a} {\'e}valu{\'e}es par l{'}enseignant. Les r{\'e}sultats varient de 0,394 {\`a} 0,814 de F-mesure sur la premi{\`e}re t{\^a}che (7 {\'e}quipes), de 0,448 {\`a} 0,682 de pr{\'e}cision sur la deuxi{\`e}me (3 {\'e}quipes), et de 0,133 {\`a} 0,510 de pr{\'e}cision sur la derni{\`e}re (3 {\'e}quipes)."
2020.lrec-1.851,A {F}rench Corpus for Semantic Similarity,2020,-1,-1,2,1,5648,remi cardon,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Semantic similarity is an area of Natural Language Processing that is useful for several downstream applications, such as machine translation, natural language generation, information retrieval, or question answering. The task consists in assessing the extent to which two sentences express or do not express the same meaning. To do so, corpora with graded pairs of sentences are required. The grade is positioned on a given scale, usually going from 0 (completely unrelated) to 5 (equivalent semantics). In this work, we introduce such a corpus for French, the first that we know of. It is comprised of 1,010 sentence pairs with grades from five annotators. We describe the annotation process, analyse these data, and perform a few experiments for the automatic grading of semantic similarity."
2020.jeptalnrecital-taln.21,Pr{\\'e}dire le niveau de langue d{'}apprenants d{'}anglais (Predict the language level for {E}nglish learners),2020,-1,-1,1,1,5649,natalia grabar,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 2 : Traitement Automatique des Langues Naturelles",0,"L{'}apprentissage de la deuxi{\`e}me langue (L2) est un processus progressif dans lequel l{'}apprenant am{\'e}liore sa ma{\^\i}trise au fur et {\`a} mesure de l{'}apprentissage. L{'}analyse de productions d{'}apprenants int{\'e}resse les chercheurs et les enseignants car cela permet d{'}avoir une meilleure id{\'e}e des difficult{\'e}s et les facilit{\'e}s d{'}apprentissage et de faire des programmes didactiques plus adapt{\'e}s. Cela peut {\'e}galement donner des indications sur les difficult{\'e}s cognitives {\`a} ma{\^\i}triser les notions grammaticales abstraites dans une nouvelle langue. Nous proposons de travailler sur un corpus de productions langagi{\`e}res d{'}apprenants d{'}anglais provenant de diff{\'e}rents pays et donc ayant diff{\'e}rentes langues maternelles (L1). Notre objectif consiste {\`a} cat{\'e}goriser ces productions langagi{\`e}res selon six niveaux de langue (A1, A2, B1, B2, C1, C2). Nous utilisons diff{\'e}rents ensembles de descripteurs, y compris les verbes et expressions modaux. Nous obtenons des r{\'e}sultats int{\'e}ressants pour cette cat{\'e}gorisation multiclasse, ce qui indique qu{'}il existe des diff{\'e}rences linguistiques inh{\'e}rentes entre les diff{\'e}rents niveaux."
2020.jeptalnrecital-deft.1,Pr{\\'e}sentation de la campagne d{'}{\\'e}valuation {DEFT} 2020 : similarit{\\'e} textuelle en domaine ouvert et extraction d{'}information pr{\\'e}cise dans des cas cliniques (Presentation of the {DEFT} 2020 Challenge : open domain textual similarity and precise information extraction from clinical cases ),2020,-1,-1,2,1,5648,remi cardon,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Atelier D{\\'E}fi Fouille de Textes",0,"L{'}{\'e}dition 2020 du d{\'e}fi fouille de texte (DEFT) a propos{\'e} deux t{\^a}ches autour de la similarit{\'e} textuelle et une t{\^a}che d{'}extraction d{'}information. La premi{\`e}re t{\^a}che vise {\`a} identifier le degr{\'e} de similarit{\'e} entre paires de phrases sur une {\'e}chelle de 0 (le moins similaire) {\`a} 5 (le plus similaire). Les r{\'e}sultats varient de 0,65 {\`a} 0,82 d{'}EDRM. La deuxi{\`e}me t{\^a}che consiste {\`a} d{\'e}terminer la phrase la plus proche d{'}une phrase source parmi trois phrases cibles fournies, avec des r{\'e}sultats tr{\`e}s {\'e}lev{\'e}s, variant de 0,94 {\`a} 0,99 de pr{\'e}cision. Ces deux t{\^a}ches reposent sur un corpus du domaine g{\'e}n{\'e}ral et de sant{\'e}. La troisi{\`e}me t{\^a}che propose d{'}extraire dix cat{\'e}gories d{'}informations du domaine m{\'e}dical depuis le corpus de cas cliniques de DEFT 2019. Les r{\'e}sultats varient de 0,07 {\`a} 0,66 de F-mesure globale pour la sous-t{\^a}che des pathologies et signes ou sympt{\^o}mes, et de 0,14 {\`a} 0,76 pour la sous-t{\^a}che sur huit cat{\'e}gories m{\'e}dicales. Les m{\'e}thodes utilis{\'e}es reposent sur des CRF et des r{\'e}seaux de neurones."
2020.coling-main.62,{F}rench Biomedical Text Simplification: When Small and Precise Helps,2020,-1,-1,2,1,5648,remi cardon,Proceedings of the 28th International Conference on Computational Linguistics,0,"We present experiments on biomedical text simplification in French. We use two kinds of corpora {--} parallel sentences extracted from existing health comparable corpora in French and WikiLarge corpus translated from English to French {--} and a lexicon that associates medical terms with paraphrases. Then, we train neural models on these parallel corpora using different ratios of general and specialized sentences. We evaluate the results with BLEU, SARI and Kandel scores. The results point out that little specialized data helps significantly the simplification."
2020.bucc-1.7,Reducing the Search Space for Parallel Sentences in Comparable Corpora,2020,-1,-1,2,1,5648,remi cardon,Proceedings of the 13th Workshop on Building and Using Comparable Corpora,0,"This paper describes and evaluates simple techniques for reducing the research space for parallel sentences in monolingual comparable corpora. Initially, when searching for parallel sentences between two comparable documents, all the possible sentence pairs between the documents have to be considered, which introduces a great degree of imbalance between parallel pairs and non-parallel pairs. This is a problem because even with a high performing algorithm, a lot of noise will be present in the extracted results, thus introducing a need for an extensive and costly manual check phase. We work on a manually annotated subset obtained from a French comparable corpus and show how we can drastically reduce the number of sentence pairs that have to be fed to a classifier so that the results can be manually handled."
W19-5011,{RNN} Embeddings for Identifying Difficult to Understand Medical Words,2019,0,1,3,0,23936,hanna pylieva,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"Patients and their families often require a better understanding of medical information provided by doctors. We currently address this issue by improving the identification of difficult to understand medical words. We introduce novel embeddings received from RNN - FrnnMUTE (French RNN Medical Understandability Text Embeddings) which allow to reach up to 87.0 F1 score in identification of difficult words. We also note that adding pre-trained FastText word embeddings to the feature set substantially improves the performance of the model which classifies words according to their difficulty. We study the generalizability of different models through three cross-validation scenarios which allow testing classifiers in real-world conditions: understanding of medical words by new users, and classification of new unseen words by the automatic models. The RNN - FrnnMUTE embeddings and the categorization code are being made available for the research."
W19-5013,Query selection methods for automated corpora construction with a use case in food-drug interactions,2019,0,0,4,0,17236,georgeta bordea,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"In this paper, we address the problem of automatically constructing a relevant corpus of scientific articles about food-drug interactions. There is a growing number of scientific publications that describe food-drug interactions but currently building a high-coverage corpus that can be used for information extraction purposes is not trivial. We investigate several methods for automating the query selection process using an expert-curated corpus of food-drug interactions. Our experiments show that index term features along with a decision tree classifier are the best approach for this task and that feature selection approaches and in particular gain ratio outperform frequency-based methods for query selection."
W19-5029,Clinical Case Reports for {NLP},2019,0,1,2,0,5675,cyril grouin,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"Textual data are useful for accessing expert information. Yet, since the texts are representative of distinct language uses, it is necessary to build specific corpora in order to be able to design suitable NLP tools. In some domains, such as medical domain, it may be complicated to access the representative textual data and their semantic annotations, while there exists a real need for providing efficient tools and methods. Our paper presents a corpus of clinical cases written in French, and their semantic annotations. Thus, we manually annotated a set of 717 files into four general categories (age, gender, outcome, and origin) for a total number of 2,835 annotations. The values of age, gender, and outcome are normalized. A subset with 70 files has been additionally manually annotated into 27 categories for a total number of 5,198 annotations."
W19-5033,Simplification-induced transformations: typology and some characteristics,2019,0,0,3,0,18610,anais koptient,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"The purpose of automatic text simplification is to transform technical or difficult to understand texts into a more friendly version. The semantics must be preserved during this transformation. Automatic text simplification can be done at different levels (lexical, syntactic, semantic, stylistic...) and relies on the corresponding knowledge and resources (lexicon, rules...). Our objective is to propose methods and material for the creation of transformation rules from a small set of parallel sentences differentiated by their technicity. We also propose a typology of transformations and quantify them. We work with French-language data related to the medical domain, although we assume that the method can be exploited on texts in any language and from any domain."
R19-1020,Parallel Sentence Retrieval From Comparable Corpora for Biomedical Text Simplification,2019,0,0,2,1,5648,remi cardon,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Parallel sentences provide semantically similar information which can vary on a given dimension, such as language or register. Parallel sentences with register variation (like expert and non-expert documents) can be exploited for the automatic text simplification. The aim of automatic text simplification is to better access and understand a given information. In the biomedical field, simplification may permit patients to understand medical and health texts. Yet, there is currently no such available resources. We propose to exploit comparable corpora which are distinguished by their registers (specialized and simplified versions) to detect and align parallel sentences. These corpora are in French and are related to the biomedical area. Manually created reference data show 0.76 inter-annotator agreement. Our purpose is to state whether a given pair of specialized and simplified sentences is parallel and can be aligned or not. We treat this task as binary classification (alignment/non-alignment). We perform experiments with a controlled ratio of imbalance and on the highly unbalanced real data. Our results show that the method we present here can be used to automatically generate a corpus of parallel sentences from our comparable corpus."
R19-1026,Speculation and Negation detection in {F}rench biomedical corpora,2019,0,0,3,1,25287,clement dalloux,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"In this work, we propose to address the detection of negation and speculation, and of their scope, in French biomedical documents. It has been indeed observed that they play an important role and provide crucial clues for other NLP applications. Our methods are based on CRFs and BiLSTM. We reach up to 97.21 {\%} and 91.30 {\%} F-measure for the detection of negation and speculation cues, respectively, using CRFs. For the computing of scope, we reach up to 90.81 {\%} and 86.73 {\%} F-measure on negation and speculation, respectively, using BiLSTM-CRF fed with word embeddings."
2019.jeptalnrecital-long.5,Corpus annot{\\'e} de cas cliniques en fran{\\c{c}}ais (Annotated corpus with clinical cases in {F}rench),2019,-1,-1,1,1,5649,natalia grabar,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume I : Articles longs,0,"Les corpus textuels sont utiles pour diverses applications de traitement automatique des langues (TAL) en fournissant les donn{\'e}es n{\'e}cessaires pour leur cr{\'e}ation, adaptation ou {\'e}valuation. Cependant, dans certains domaines comme le domaine m{\'e}dical, l{'}acc{\`e}s aux donn{\'e}es est rendu compliqu{\'e}, voire impossible, pour des raisons de confidentialit{\'e} et d{'}{\'e}thique. Il existe n{\'e}anmoins de r{\'e}els besoins en corpus cliniques pour l{'}enseignement et la recherche. Pour r{\'e}pondre {\`a} ce d{\'e}fi, nous pr{\'e}sentons dans cet article le corpus CAS contenant des cas cliniques de patients, r{\'e}els ou fictifs, que nous avons compil{\'e}s. Ces cas cliniques en fran{\c{c}}ais couvrent plusieurs sp{\'e}cialit{\'e}s m{\'e}dicales et focalisent donc sur diff{\'e}rentes situations cliniques. Actuellement, le corpus contient 4 300 cas (environ 1,5M d{'}occurrences de mots). Il est accompagn{\'e} d{'}informations (discussions des cas cliniques, mots-cl{\'e}s, etc.) et d{'}annotations que nous avons effectu{\'e}es au regard des besoins de la recherche en TAL dans ce domaine. Nous pr{\'e}sentons {\'e}galement les r{\'e}sultats de premi{\`e}res exp{\'e}riences de recherche et d{'}extraction d{'}information qui ont {\'e}t{\'e} effectu{\'e}es avec ce corpus annot{\'e}. Ces exp{\'e}riences peuvent fournir une baseline {\`a} d{'}autres chercheurs souhaitant travailler avec les donn{\'e}es."
2019.jeptalnrecital-deft.1,Recherche et extraction d{'}information dans des cas cliniques. Pr{\\'e}sentation de la campagne d{'}{\\'e}valuation {DEFT} 2019 (Information Retrieval and Information Extraction from Clinical Cases),2019,-1,-1,1,1,5649,natalia grabar,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. D{\\'e}fi Fouille de Textes (atelier TALN-RECITAL),0,"Cet article pr{\'e}sente la campagne d{'}{\'e}valuation DEFT 2019 sur l{'}analyse de textes cliniques r{\'e}dig{\'e}s en fran{\c{c}}ais. Le corpus se compose de cas cliniques publi{\'e}s et discut{\'e}s dans des articles scientifiques, et index{\'e}s par des mots-cl{\'e}s. Nous proposons trois t{\^a}ches ind{\'e}pendantes : l{'}indexation des cas cliniques et discussions, {\'e}valu{\'e}e prioritairement par la MAP (mean average precision), l{'}appariement entre cas cliniques et discussions, {\'e}valu{\'e} au moyen d{'}une pr{\'e}cision, et l{'}extraction d{'}information parmi quatre cat{\'e}gories ({\^a}ge, genre, origine de la consultation, issue), {\'e}valu{\'e}e en termes de rappel, pr{\'e}cision et F-mesure. Nous pr{\'e}sentons les r{\'e}sultats obtenus par les participants sur chaque t{\^a}che."
2019.jeptalnrecital-court.10,D{\\'e}tecter la non-adh{\\'e}rence m{\\'e}dicamenteuse dans les forums de discussion avec les m{\\'e}thodes de recherche d{'}information (Detect drug non-compliance in {I}nternet fora using Information Retrieval methods ),2019,-1,-1,2,1,27342,elise bigeard,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume II : Articles courts,0,"Les m{\'e}thodes de recherche d{'}information permettent d{'}explorer les donn{\'e}es textuelles. Nous les exploitons pour la d{\'e}tection de messages avec la non-adh{\'e}rence m{\'e}dicamenteuse dans les forums de discussion. La non-adh{\'e}rence m{\'e}dicamenteuse correspond aux cas lorsqu{'}un patient ne respecte pas les indications de son m{\'e}decin et modifie les prises de m{\'e}dicaments (augmente ou diminue les doses, par exemple). Le moteur de recherche exploit{\'e} montre 0,9 de pr{\'e}cision sur les 10 premiers r{\'e}sultats avec un corpus {\'e}quilibr{\'e}, et 0,4 avec un corpus respectant la distribution naturelle des messages, qui est tr{\`e}s d{\'e}s{\'e}quilibr{\'e}e en d{\'e}faveur de la cat{\'e}gorie recherch{\'e}e. La pr{\'e}cision diminue avec l{'}augmentation du nombre de r{\'e}sultats consid{\'e}r{\'e}s alors que le rappel augmente. Nous exploitons {\'e}galement le moteur de recherche sur de nouvelles donn{\'e}es et avec des types pr{\'e}cis de non-adh{\'e}rence."
2019.jeptalnrecital-court.11,D{\\'e}tection automatique de phrases parall{\\`e}les dans un corpus biom{\\'e}dical comparable technique / simplifi{\\'e} (Automatic detection of parallel sentences in comparable biomedical corpora),2019,-1,-1,2,1,5648,remi cardon,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume II : Articles courts,0,"Les phrases parall{\`e}les contiennent des informations identiques ou tr{\`e}s proches s{\'e}mantiquement et offrent des indications importantes sur le fonctionnement de la langue. Lorsque les phrases sont diff{\'e}renci{\'e}es par leur registre (comme expert vs. non-expert), elles peuvent {\^e}tre exploit{\'e}es pour la simplification automatique de textes. Le but de la simplification automatique est d{'}am{\'e}liorer la compr{\'e}hension de textes. Par exemple, dans le domaine biom{\'e}dical, la simplification peut permettre aux patients de mieux comprendre les textes relatifs {\`a} leur sant{\'e}. Il existe cependant tr{\`e}s peu de ressources pour la simplification en fran{\c{c}}ais. Nous proposons donc d{'}exploiter des corpus comparables, diff{\'e}renci{\'e}s par leur technicit{\'e}, pour y d{\'e}tecter des phrases parall{\`e}les et les aligner. Les donn{\'e}es de r{\'e}f{\'e}rence sont cr{\'e}{\'e}es manuellement et montrent un accord inter-annotateur de 0,76. Nous exp{\'e}rimentons sur des donn{\'e}es {\'e}quilibr{\'e}es et d{\'e}s{\'e}quilibr{\'e}es. La F-mesure sur les donn{\'e}es {\'e}quilibr{\'e}es atteint jusqu{'}{\`a} 0,94. Sur les donn{\'e}es d{\'e}s{\'e}quilibr{\'e}es, les r{\'e}sultats sont plus faibles (jusqu{'}{\`a} 0,92 de F-mesure) mais restent comp{\'e}titifs lorsque les mod{\`e}les sont entra{\^\i}n{\'e}s sur les donn{\'e}es {\'e}quilibr{\'e}es."
W18-7002,{CLEAR} {--} Simple Corpus for Medical {F}rench,2018,0,0,1,1,5649,natalia grabar,Proceedings of the 1st Workshop on Automatic Text Adaptation ({ATA}),0,"Availability of corpora with technical and simplified contents is crucial for the development and test of methods for text simplification. We describe this kind of corpus for the French medical language. The corpus contains texts from three sources: encyclopedia, drug leaflets and scientific summaries. Each source proposes comparable information in specialized and plain languages. A subset of this corpus has been processed manually in order to find and align parallel sentences. This subset currently contains 663 pairs with parallel sentences. Alignment has been done by two annota-tors and shows 0.76 inter-annotator agreement ."
W18-7003,Study of Readability of Health Documents with Eye-tracking Approaches,2018,0,0,1,1,5649,natalia grabar,Proceedings of the 1st Workshop on Automatic Text Adaptation ({ATA}),0,"Medical area is an integral part of our lives due to health concerns, but the availability of medical information does not guarantee its correct understanding by patients. Several studies addressed this issue and pointed out real difficulties in the understanding of health contents by patients. We propose to use eye-tracking methods for studying this issue. For this, original technical and simplified versions of a deidentified clinical document are exploited. Eye-tracking methods permit to follow and to record the gaze of participants and to detect reading indicators such as duration of fixations, regressions and saccades. These indicators are correlated with answers to questionnaires submitted to participants after the reading. Our results indicate that there is statistically significant difference in reading and understanding of original and simplified versions of health documents. These results, in combination with another experiment, permit to propose a typology of medical words which need to be explained or simplified to non-expert readers."
W18-5610,Identification of Parallel Sentences in Comparable Monolingual Corpora from Different Registers,2018,0,0,2,1,5648,remi cardon,Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis,0,"Parallel aligned sentences provide useful information for different NLP applications. Yet, this kind of data is seldom available, especially for languages other than English. We propose to exploit comparable corpora in French which are distinguished by their registers (specialized and simplified versions) to detect and align parallel sentences. These corpora are related to the biomedical area. Our purpose is to state whether a given pair of specialized and simplified sentences is to be aligned or not. Manually created reference data show 0.76 inter-annotator agreement. We exploit a set of features and several automatic classifiers. The automatic alignment reaches up to 0.93 Precision, Recall and F-measure. In order to better evaluate the method, it is applied to data in English from the \textit{SemEval} STS competitions. The same features and models are applied in monolingual and cross-lingual contexts, in which they show up to 0.90 and 0.73 F-measure, respectively."
W18-5614,{CAS}: {F}rench Corpus with Clinical Cases,2018,0,0,1,1,5649,natalia grabar,Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis,0,"Textual corpora are extremely important for various NLP applications as they provide information necessary for creating, setting and testing these applications and the corresponding tools. They are also crucial for designing reliable methods and reproducible results. Yet, in some areas, such as the medical area, due to confidentiality or to ethical reasons, it is complicated and even impossible to access textual data representative of those produced in these areas. We propose the CAS corpus built with clinical cases, such as they are reported in the published scientific literature in French. We describe this corpus, currently containing over 397,000 word occurrences, and the existing linguistic and semantic annotations."
2018.jeptalnrecital-long.1,{\\'E}tude de la lisibilit{\\'e} des documents de sant{\\'e} avec des m{\\'e}thodes d{'}oculom{\\'e}trie (Study of readability of health documents with eye-tracking methods),2018,-1,-1,1,1,5649,natalia grabar,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"Le domaine m{\'e}dical fait partie de la vie quotidienne pour des raisons de sant{\'e}, mais la disponibilit{\'e} des informations m{\'e}dicales ne garantit pas leur compr{\'e}hension correcte par les patients. Plusieurs {\'e}tudes ont d{\'e}montr{\'e} qu{'}il existe une difficult{\'e} r{\'e}elle dans la compr{\'e}hension de contenus m{\'e}dicaux par les patients. Nous proposons d{'}exploiter les m{\'e}thodes d{'}oculom{\'e}trie pour {\'e}tudier ces questions et pour d{\'e}tecter quelles unit{\'e}s linguistiques posent des difficult{\'e}s de compr{\'e}hension. Pour cela, des textes m{\'e}dicaux en version originale et simplifi{\'e}e sont exploit{\'e}s. L{'}oculom{\'e}trie permet de suivre le regard des participants de l{'}{\'e}tude et de r{\'e}v{\'e}ler les indicateurs de lecture, comme la dur{\'e}e des fixations, les r{\'e}gressions et les saccades. Les r{\'e}sultats indiquent qu{'}il existe une diff{\'e}rence statistiquement significative lors de la lecture des versions originales et simplifi{\'e}es des documents de sant{\'e} test{\'e}s."
2018.jeptalnrecital-court.16,D{\\'e}tection de m{\\'e}susages de m{\\'e}dicaments dans les r{\\'e}seaux sociaux (Detection of drug misuse in social media),2018,-1,-1,2,1,27342,elise bigeard,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"Un m{\'e}susage appara{\^\i}t lorsqu{'}un patient ne respecte pas sa prescription et fait des actions pouvant mener {\`a} des effets nocifs. Bien que ces situations soient dangereuses, les patients ne signalent g{\'e}n{\'e}ralement pas les m{\'e}susages {\`a} leurs m{\'e}decins. Il est donc n{\'e}cessaire d{'}{\'e}tudier d{'}autres sources d{'}information pour d{\'e}couvrir ce qui se passe en r{\'e}alit{\'e}. Nous proposons d{'}{\'e}tudier les forums de sant{\'e} en ligne. L{'}objectif de notre travail consiste {\`a} explorer les forums de sant{\'e} avec des m{\'e}thodes de classification supervis{\'e}e afin d{'}identifier les messages contenant un m{\'e}susage de m{\'e}dicament. Notre m{\'e}thode permet de d{\'e}tecter les m{\'e}susages avec une F-mesure allant jusqu{'}{\`a} 0,810. Cette m{\'e}thode peut aider dans la d{\'e}tection de m{\'e}susages et la construction d{'}un corpus exploitable par les experts pour {\'e}tudier les types de m{\'e}susages commis par les patients."
2018.jeptalnrecital-court.24,Port{\\'e}e de la n{\\'e}gation : d{\\'e}tection par apprentissage supervis{\\'e} en fran{\\c{c}}ais et portugais br{\\'e}silien (Negation scope : sequence labeling by supervised learning in {F}rench and {B}razilian-{P}ortuguese),2018,-1,-1,3,1,25287,clement dalloux,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"La d{\'e}tection automatique de la n{\'e}gation fait souvent partie des pr{\'e}-requis dans les syst{\`e}mes d{'}extraction d{'}information, notamment dans le domaine biom{\'e}dical. Cet article pr{\'e}sente nos contributions concernant la d{\'e}tection de la port{\'e}e de la n{\'e}gation en fran{\c{c}}ais et portugais br{\'e}silien. Nous pr{\'e}sentons d{'}une part deux corpus principalement constitu{\'e}s d{'}extraits de protocoles d{'}essais cliniques en fran{\c{c}}ais et portugais br{\'e}silien, d{\'e}di{\'e}s aux crit{\`e}res d{'}inclusion de patients. Les marqueurs de n{\'e}gation et leurs port{\'e}es y ont {\'e}t{\'e} annot{\'e}s manuellement. Nous pr{\'e}sentons d{'}autre part une approche par r{\'e}seau de neurones r{\'e}currents pour extraire les port{\'e}es."
grabar-hamon-2017-understanding,Understanding of unknown medical words,2017,0,1,1,1,5649,natalia grabar,Proceedings of the Biomedical {NLP} Workshop associated with {RANLP} 2017,0,"We assume that unknown words with internal structure (affixed words or compounds) can provide speakers with linguistic cues as for their meaning, and thus help their decoding and understanding. To verify this hypothesis, we propose to work with a set of French medical words. These words are annotated by five annotators. Then, two kinds of analysis are performed: analysis of the evolution of understandable and non-understandable words (globally and according to some suffixes) and analysis of clusters created with unsupervised algorithms on basis of linguistic and extra-linguistic features of the studied words. Our results suggest that, according to linguistic sensitivity of annotators, technical words can be decoded and become understandable. As for the clusters, some of them distinguish between understandable and non-understandable words. Resources built in this work will be made freely available for the research purposes."
hamon-etal-2017-pomelo,{POMELO}: {M}edline corpus with manually annotated food-drug interactions,2017,0,0,4,0,18582,thierry hamon,Proceedings of the Biomedical {NLP} Workshop associated with {RANLP} 2017,0,"When patients take more than one medication, they may be at risk of drug interactions, which means that a given drug can cause unexpected effects when taken in combination with other drugs. Similar effects may occur when drugs are taken together with some food or beverages. For instance, grapefruit has interactions with several drugs, because its active ingredients inhibit enzymes involved in the drugs metabolism and can then cause an excessive dosage of these drugs. Yet, information on food/drug interactions is poorly researched. The current research is mainly provided by the medical domain and a very tentative work is provided by computer sciences and NLP domains. One factor that motivates the research is related to the availability of the annotated corpora and the reference data. The purpose of our work is to describe the rationale and approach for creation and annotation of scientific corpus with information on food/drug interactions. This corpus contains 639 MEDLINE citations (titles and abstracts), corresponding to 5,752 sentences. It is manually annotated by two experts. The corpus is named POMELO. This annotated corpus will be made available for the research purposes."
2017.jeptalnrecital-long.14,Analyse et {\\'e}volution de la compr{\\'e}hension de termes techniques (Analysis and Evolution of Understanding of Technical Terms),2017,-1,-1,1,1,5649,natalia grabar,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 - Articles longs,0,"Nous faisons l{'}hypoth{\`e}se que les mots techniques inconnus dot{\'e}s d{'}une structure interne (mots affix{\'e}s ou compos{\'e}s) peuvent fournir des indices linguistiques {\`a} un locuteur, ce qui peut l{'}aider {\`a} analyser et {\`a} comprendre ces mots. Afin de tester notre hypoth{\`e}se, nous proposons de travailler sur un ensemble de mots techniques provenant du domaine m{\'e}dical. Un grand ensemble de mots techniques est annot{\'e} par cinq annotateurs. Nous effectuons deux types d{'}analyses : l{'}analyse de l{'}{\'e}volution des mots compr{\'e}hensibles et incompr{\'e}hensibles (de mani{\`e}re g{\'e}n{\'e}rale et en fonction de certains suffixes) et l{'}analyse des clusters avec ces mots cr{\'e}{\'e}s par apprentissage non-supervis{\'e}, sur la base des descripteurs linguistiques et extra-linguistiques. Nos r{\'e}sultats indiquent que, selon la sensibilit{\'e} linguistique des annotateurs, les mots techniques peuvent devenir d{\'e}codables et compr{\'e}hensibles. Quant aux clusters, le contenu de certains refl{\`e}te la difficult{\'e} des mots qui les composent et montre {\'e}galement la progression des annotateurs dans leur compr{\'e}hension. La ressource construite est disponible pour la recherche : http://natalia.grabar.free.fr/rated-lexicon.html."
2017.jeptalnrecital-court.5,"Crit{\\`e}res num{\\'e}riques dans les essais cliniques : annotation, d{\\'e}tection et normalisation (Numerical criteria in clinical trials : annotation, detection and normalization)",2017,-1,-1,1,1,5649,natalia grabar,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 2 - Articles courts,0,"Les essais cliniques sont un {\'e}l{\'e}ment fondamental pour l{'}{\'e}valuation de nouvelles th{\'e}rapies ou techniques de diagnostic, de leur s{\'e}curit{\'e} et efficacit{\'e}. Ils exigent d{'}avoir un {\'e}chantillon convenable de la population. Le d{\'e}fi consiste alors {\`a} recruter le nombre suffisant de participants avec des caract{\'e}ristiques similaires pour garantir que les r{\'e}sultats des essais sont bien contr{\^o}l{\'e}s et dus aux facteurs {\'e}tudi{\'e}s. C{'}est une t{\^a}che difficile, effectu{\'e}e essentiellement manuellement. Comme les valeurs num{\'e}riques sont une information tr{\`e}s fr{\'e}quente et importante, nous proposons un syst{\`e}me automatique qui vise leur extraction et normalisation."
L16-1420,A Large Rated Lexicon with {F}rench Medical Words,2016,22,1,1,1,5649,natalia grabar,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Patients are often exposed to medical terms, such as anosognosia, myelodysplastic, or hepatojejunostomy, that can be semantically complex and hardly understandable by non-experts in medicine. Hence, it is important to assess which words are potentially non-understandable and require further explanations. The purpose of our work is to build specific lexicon in which the words are rated according to whether they are understandable or non-understandable. We propose to work with medical words in French such as provided by an international medical terminology. The terms are segmented in single words and then each word is manually processed by three annotators. The objective is to assign each word into one of the three categories: I can understand, I am not sure, I cannot understand. The annotators do not have medical training nor they present specific medical problems. They are supposed to represent an average patient. The inter-annotator agreement is then computed. The content of the categories is analyzed. Possible applications in which this lexicon can be helpful are proposed and discussed. The rated lexicon is freely available for the research purposes. It is accessible online at http://natalia.grabar.perso.sfr.fr/rated-lexicon.html"
L16-1596,Detection of Reformulations in Spoken {F}rench,2016,28,0,1,1,5649,natalia grabar,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Our work addresses automatic detection of enunciations and segments with reformulations in French spoken corpora. The proposed approach is syntagmatic. It is based on reformulation markers and specificities of spoken language. The reference data are built manually and have gone through consensus. Automatic methods, based on rules and CRF machine learning, are proposed in order to detect the enunciations and segments that contain reformulations. With the CRF models, different features are exploited within a window of various sizes. Detection of enunciations with reformulations shows up to 0.66 precision. The tests performed for the detection of reformulated segments indicate that the task remains difficult. The best average performance values reach up to 0.65 F-measure, 0.75 precision, and 0.63 recall. We have several perspectives to this work for improving the detection of reformulated segments and for studying the data from other points of view."
2016.jeptalnrecital-poster.31,Vers une analyse des diff{\\'e}rences interlinguistiques entre les genres textuels : {\\'e}tude de cas bas{\\'e}e sur les n-grammes et l{'}analyse factorielle des correspondances (Towards a cross-linguistic analysis of genres: A case study based on n-grams and Correspondence Analysis),2016,-1,-1,3,1,35942,marieaude lefer,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters),0,"L{'}objectif de notre travail est d{'}{\'e}valuer l{'}int{\'e}r{\^e}t d{'}employer les n-grammes et l{'}analyse factorielle des correspondances (AFC) pour comparer les genres textuels dans les {\'e}tudes contrastives interlinguistiques. Nous exploitons un corpus bilingue anglais-fran{\c{c}}ais constitu{\'e} de textes originaux comparables. Le corpus r{\'e}unit trois genres : les d{\'e}bats parlementaires europ{\'e}ens, les {\'e}ditoriaux de presse et les articles scientifiques. Dans un premier temps, les n-grammes d{'}une longueur de 2 {\`a} 4 mots sont extraits dans chaque langue. Ensuite, pour chaque longueur, les 1 000 n-grammes les plus fr{\'e}quents dans chaque langue sont trait{\'e}s par l{'}AFC pour d{\'e}terminer quels n-grammes sont particuli{\`e}rement saillants dans les genres {\'e}tudi{\'e}s. Enfin, les n-grammes sont cat{\'e}goris{\'e}s manuellement en distinguant les expressions d{'}opinion et de certitude, les marqueurs discursifs et les expressions r{\'e}f{\'e}rentielles. Les r{\'e}sultats montrent que les n-grammes permettent de mettre au jour des caract{\'e}ristiques typiques des genres {\'e}tudi{\'e}s, de m{\^e}me que des contrastes interlangues int{\'e}ressants."
2016.jeptalnrecital-long.12,Exploitation de reformulations pour l{'}acquisition d{'}un vocabulaire expert/non expert (Exploitation of reformulations for the acquisition of expert/non-expert vocabulary),2016,-1,-1,2,0,35946,edwige antoine,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Articles longs),0,"Les notions de domaines techniques, comme les notions m{\'e}dicales, pr{\'e}sentent souvent des difficult{\'e}s de compr{\'e}hension par les non experts. Un vocabulaire qui associe les termes techniques aux expressions grand public peut aider {\`a} rendre les textes techniques mieux compr{\'e}hensibles. L{'}objectif de notre travail est de construire un tel vocabulaire. Nous proposons d{'}exploiter la notion de reformulation gr{\^a}ce {\`a} trois m{\'e}thodes : extraction d{'}abr{\'e}viations, exploitation de marqueurs de reformulation et de parenth{\`e}ses. Les segments associ{\'e}s gr{\^a}ce {\`a} ces m{\'e}thodes sont align{\'e}s avec les terminologies m{\'e}dicales. Nos r{\'e}sultats permettent de couvrir un grand nombre de termes m{\'e}dicaux et montrent une pr{\'e}cision d{'}extraction entre 0,68 et 0,98. Au total, plusieurs dizaines de milliers de paires sont propos{\'e}s. Ces r{\'e}sultats sont analys{\'e}s et compar{\'e}s avec les travaux existants."
2016.jeptalnrecital-long.20,Pr{\\'e}diction automatique de fonctions pragmatiques dans les reformulations (Automatic prediction of pragmatic functions in reformulations),2016,-1,-1,1,1,5649,natalia grabar,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Articles longs),0,"La reformulation participe {\`a} la structuration du discours, notamment dans le cas des dialogues, et contribue {\'e}galement {\`a} la dynamique du discours. Reformuler est un acte significatif qui poursuit des objectifs pr{\'e}cis. L{'}objectif de notre travail est de pr{\'e}dire automatiquement la raison pour laquelle un locuteur effectue une reformulation. Nous utilisons une classification de onze fonctions pragmatiques inspir{\'e}es des travaux existants et des donn{\'e}es analys{\'e}es. Les donn{\'e}es de r{\'e}f{\'e}rence sont issues d{'}annotations manuelles et consensuelles des reformulations spontan{\'e}es form{\'e}es autour de trois marqueurs (c{'}est-{\`a}-dire, je veux dire, disons). Les donn{\'e}es proviennent d{'}un corpus oral et d{'}un corpus de discussions sur les forums de sant{\'e}. Nous exploitons des algorithmes de cat{\'e}gorisation supervis{\'e}e et un ensemble de plusieurs descripteurs (syntaxiques, formels, s{\'e}mantiques et discursifs) pour pr{\'e}dire les cat{\'e}gories de reformulation. La distribution des {\'e}nonc{\'e}s et phrases selon les cat{\'e}gories n{'}est pas homog{\`e}ne. Les exp{\'e}riences sont positionn{\'e}es {\`a} deux niveaux : g{\'e}n{\'e}rique et sp{\'e}cifique. Nos r{\'e}sultats indiquent qu{'}il est plus facile de pr{\'e}dire les types de fonctions au niveau g{\'e}n{\'e}rique (la moyenne des F-mesures est autour de 0,80), qu{'}au niveau des cat{\'e}gories individuelles (la moyenne des F-mesures est autour de 0,40). L{'}influence de diff{\'e}rents param{\`e}tres est {\'e}tudi{\'e}e."
2015.jeptalnrecital-long.16,Extraction automatique de paraphrases grand public pour les termes m{\\'e}dicaux,2015,-1,-1,1,1,5649,natalia grabar,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous sommes tous concern{\'e}s par notre {\'e}tat de sant{\'e} et restons sensibles aux informations de sant{\'e} disponibles dans la soci{\'e}t{\'e} moderne {\`a} travers par exemple les r{\'e}sultats des recherches scientifiques, les m{\'e}dias sociaux de sant{\'e}, les documents cliniques, les {\'e}missions de t{\'e}l{\'e} et de radio ou les nouvelles. Cependant, il est commun de rencontrer dans le domaine m{\'e}dical des termes tr{\`e}s sp{\'e}cifiques (e.g., bl{\'e}pharospasme, alexitymie, appendicectomie), qui restent difficiles {\`a} comprendre par les non sp{\'e}cialistes. Nous proposons une m{\'e}thode automatique qui vise l{'}acquisition de paraphrases pour les termes m{\'e}dicaux, qui soient plus faciles {\`a} comprendre que les termes originaux. La m{\'e}thode est bas{\'e}e sur l{'}analyse morphologique des termes, l{'}analyse syntaxique et la fouille de textes non sp{\'e}cialis{\'e}s. L{'}analyse et l{'}{\'e}valuation des r{\'e}sultats indiquent que de telles paraphrases peuvent {\^e}tre trouv{\'e}es dans les documents non sp{\'e}cialis{\'e}s et pr{\'e}sentent une compr{\'e}hension plus facile. En fonction des param{\`e}tres de la m{\'e}thode, la pr{\'e}cision varie entre 86 et 55{\%}. Ce type de ressources est utile pour plusieurs applications de TAL (e.g., recherche d{'}information grand public, lisibilit{\'e} et simplification de textes, syst{\`e}mes de question-r{\'e}ponses)."
2015.jeptalnrecital-long.26,...des conf{\\'e}rences enfin disons des causeries... D{\\'e}tection automatique de segments en relation de paraphrase dans les reformulations de corpus oraux,2015,0,1,1,1,5649,natalia grabar,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Notre travail porte sur la d{\'e}tection automatique des segments en relation de reformulation paraphrastique dans les corpus oraux. L{'}approche propos{\'e}e est une approche syntagmatique qui tient compte des marqueurs de reformulation paraphrastique et des sp{\'e}cificit{\'e}s de l{'}oral. Les donn{\'e}es de r{\'e}f{\'e}rence sont consensuelles. Une m{\'e}thode automatique fond{\'e}e sur l{'}apprentissage avec les CRF est propos{\'e}e afin de d{\'e}tecter les segments paraphras{\'e}s. Diff{\'e}rents descripteurs sont exploit{\'e}s dans une fen{\^e}tre de taille variable. Les tests effectu{\'e}s montrent que les segments en relation de paraphrase sont assez difficiles {\`a} d{\'e}tecter, surtout avec leurs fronti{\`e}res correctes. Les meilleures moyennes atteignent 0,65 de F-mesure, 0,75 de pr{\'e}cision et 0,63 de rappel. Nous avons plusieurs perspectives {\`a} ce travail pour am{\'e}liorer la d{\'e}tection des segments en relation de paraphrase et pour {\'e}tudier les donn{\'e}es depuis d{'}autres points de vue."
W14-4812,Unsupervised Method for the Acquisition of General Language Paraphrases for Medical Compounds,2014,-1,-1,1,1,5649,natalia grabar,Proceedings of the 4th International Workshop on Computational Terminology (Computerm),0,None
W14-4814,Towards Automatic Distinction between Specialized and Non-Specialized Occurrences of Verbs in Medical Corpora,2014,31,3,2,0,38365,ornella tchami,Proceedings of the 4th International Workshop on Computational Terminology (Computerm),0,"The medical field gathers people of different social statuses, such as students, pharmacists, managers, biologists, nurses and mainly medical doctors and patients, who represent the main actors. Despite their different levels of expertise, these actors need to interact and understand each other but the communication is not always easy and effective. This paper describes a method for a contrastive automatic analysis of verbs in medical corpora, based on the semantic annotation of the verbs nominal co-occurents. The corpora used are specialized in cardiology and distinguished according to their levels of expertise (high and low). The semantic annotation of these corpora is performed by using an existing medical terminology. The results indicate that the same verbs occurring in the two corpora show different specialization levels, which are indicated by the words (nouns and adjectives derived from medical terms) they occur with."
W14-1202,Automatic diagnosis of understanding of medical words,2014,42,2,1,1,5649,natalia grabar,Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations ({PITR}),0,"Within the medical field, very specialized terms are commonly used, while their understanding by laymen is not always successful. We propose to study the understandability of medical words by laymen. Three annotators are involved in the creation of the reference data used for training and testing. The features of the words may be linguistic (i.e., number of characters, syllables, number of morphological bases and affixes) and extra-linguistic (i.e., their presence in a reference lexicon, frequency on a search engine). The automatic categorization results show between 0.806 and 0.947 F-measure values. It appears that several features and their combinations are relevant for the analysis of understandability (i.e., syntactic categories, presence in reference lexica, frequency on the general search engine, final substring)."
W14-1116,Tuning {H}eidel{T}ime for identifying time expressions in clinical texts in {E}nglish and {F}rench,2014,18,5,2,0.530034,18582,thierry hamon,Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi),0,"We present work on tuning the Heideltime system for identifying time expressions in clinical texts in English and French languages. The main amount of the method is related to the enrichment and adaptation of linguistic resources to identify Timex3 clinical expressions and to normalize them. The test of the adapted versions have been done on the i2b2/VA 2012 corpus for English and a collection of clinical texts for French, which have been annotated for the purpose of this study. We achieve a 0.8500 F-measure on the recognition and normalization of temporal expressions in English, and up to 0.9431 in French. Future work will allow to improve and consolidate the results."
F14-1027,Detection and Analysis of Paraphrastic Reformulations in Spoken Corpora (Rep{\\'e}rage et analyse de la reformulation paraphrastique dans les corpus oraux) [in {F}rench],2014,-1,-1,2,0,5639,iris eshkoltaravella,Proceedings of TALN 2014 (Volume 1: Long Papers),0,None
2014.lilt-11.7,Evaluative prefixes in translation: From automatic alignment to semantic categorization,2014,30,2,2,1,35942,marieaude lefer,"Linguistic Issues in Language Technology, Volume 11, 2014 - Theoretical and Computational Morphology: New Trends and Synergies",0,"This article aims to assess to what extent translation can shed light on the semantics of French evaluative prefixation by adopting No Ìel (2003){'}s {`}translations as evidence for semantics{'} approach. In French, evaluative prefixes can be classified along two dimensions (cf. (Fradin and Montermini 2009)): (1) a quantity dimension along a maximum/minimum axis and the semantic values big and small, and (2) a quality dimension along a positive/negative axis and the values good (excess; higher degree) and bad (lack; lower degree). In order to provide corpus-based insights into this semantic categorization, we analyze French evaluative prefixes alongside their English translation equivalents in a parallel corpus. To do so, we focus on periphrastic translations, as they are likely to {`}spell out{'} the meaning of the French prefixes. The data used were extracted from the Europarl parallel corpus (Koehn 2005; Cartoni and Meyer 2012). Using a tailormade program, we first aligned the French prefixed words with the corresponding word(s) in English target sentences, before proceeding to the evaluation of the aligned sequences and the manual analysis of the bilingual data. Results confirm that translation data can be used as evidence for semantics in morphological research and help refine existing semantic descriptions of evaluative prefixes."
F13-1005,Grouping of terms based on linguistic and semantic regularities in a cross-lingual context (Groupement de termes bas{\\'e} sur des r{\\'e}gularit{\\'e}s linguistiques et s{\\'e}mantiques dans un contexte cross-langue) [in {F}rench],2013,-1,-1,3,0,41761,marie dupuch,Proceedings of TALN 2013 (Volume 1: Long Papers),0,None
W12-2403,Semantic distance and terminology structuring methods for the detection of semantically close terms,2012,19,2,4,0,41761,marie dupuch,{B}io{NLP}: Proceedings of the 2012 Workshop on Biomedical Natural Language Processing,0,"The identification of semantically similar linguistic expressions despite their formal difference is an important task within NLP applications (information retrieval and extraction, terminology structuring...) We propose to detect the semantic relatedness between biomedical terms from the pharmacovigilance area. Two approaches are exploited: semantic distance within structured resources and terminology structuring methods applied to a raw list of terms. We compare these methods and study their complementarity. The results are evaluated against the reference pharmacovigilance data and manually by an expert."
W12-2413,Combining Compositionality and Pagerank for the Identification of Semantic Relations between Biomedical Words,2012,25,0,5,0.933596,18582,thierry hamon,{B}io{NLP}: Proceedings of the 2012 Workshop on Biomedical Natural Language Processing,0,"The acquisition of semantic resources and relations is an important task for several applications, such as query expansion, information retrieval and extraction, machine translation. However, their validity should also be computed and indicated, especially for automatic systems and applications. We exploit the compositionality based methods for the acquisition of synonymy relations and of indicators of these synonyms. We then apply pagerank-derived algorithm to the obtained semantic graph in order to filter out the acquired synonyms. Evaluation performed with two independent experts indicates that the quality of synonyms is systematically improved by 10 to 15% after their filtering."
W09-1311,Exploring Graph Structure for Detection of Reliability Zones within Synonym Resources: Experiment with the Gene Ontology,2009,18,1,2,0.933596,18582,thierry hamon,Proceedings of the {B}io{NLP} 2009 Workshop,0,"Computing the semantic similarity between terms relies on existence and usage of semantic resources. However, these resources, often composed of equivalent units, or synonyms, must be first analyzed and weighted in order to define within them the reliability zones where the semantic cohesiveness is stronger. We propose an original method for acquisition of elementary synonyms based on exploitation of structured terminologies, analysis of syntactic structure of complex (multi-unit) terms and their compositionality. The acquired synonyms are then profiled thanks to endogenous lexical and linguistic indicators (other types of relations, lexical inclusions, productivity), which are automatically inferred within the same terminologies. Additionally, synonymy relations are observed within graph, and its structure is analyzed. Particularly, we explore the usefulness of the graph theory notions such as connected component, clique, density, bridge, articulation vertex, and centrality of vertices."
2009.jeptalnrecital-court.31,Profilage s{\\'e}mantique endog{\\`e}ne des relations de synonymie au sein de Gene Ontology,2009,-1,-1,2,0.933596,18582,thierry hamon,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Le calcul de la similarit{\'e} s{\'e}mantique entre les termes repose sur l{'}existence et l{'}utilisation de ressources s{\'e}mantiques. Cependant de telles ressources, qui proposent des {\'e}quivalences entre entit{\'e}s, souvent des relations de synonymie, doivent elles-m{\^e}mes {\^e}tre d{'}abord analys{\'e}es afin de d{\'e}finir des zones de fiabilit{\'e} o{\`u} la similarit{\'e} s{\'e}mantique est plus forte. Nous proposons une m{\'e}thode d{'}acquisition de synonymes {\'e}l{\'e}mentaires gr{\^a}ce {\`a} l{'}exploitation des terminologies structur{\'e}es au travers l{'}analyse de la structure syntaxique des termes complexes et de leur compositionnalit{\'e}. Les synonymes acquis sont ensuite profil{\'e}s gr{\^a}ce aux indicateurs endog{\`e}nes inf{\'e}r{\'e}s automatiquement {\`a} partir de ces m{\^e}mes terminologies (d{'}autres types de relations, inclusions lexicales, productivit{\'e}, forme des composantes connexes). Dans le domaine biom{\'e}dical, il existe de nombreuses terminologies structur{\'e}es qui peuvent {\^e}tre exploit{\'e}es pour la constitution de ressources s{\'e}mantiques. Le travail pr{\'e}sent{\'e} ici exploite une de ces terminologies, Gene Ontology."
goeuriot-etal-2008-characterization,"Characterization of Scientific and Popular Science Discourse in {F}rench, {J}apanese and {R}ussian",2008,1,2,2,1,5695,lorraine goeuriot,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We aim to characterize the comparability of corpora, we address this issue in the trilingual context through the distinction of expert and non expert documents. We work separately with corpora composed of documents from the medical domain in three languages (French, Japanese and Russian) which present an important linguistic distance between them. In our approach, documents are characterized in each language by their topic and by a discursive typology positioned at three levels of document analysis: structural, modal and lexical. The document typology is implemented with two learning algorithms (SVMlight and C4.5). Evaluation of results shows that the proposed discursive typology can be transposed from one language to another, as it indeed allows to distinguish the two aimed discourses (science and popular science). However, we observe that performances vary a lot according to languages, algorithms and types of discursive characteristics."
2007.jeptalnrecital-poster.9,"Caract{\\'e}risation des discours scientifiques et vulgaris{\\'e}s en fran{\\c{c}}ais, japonais et russe",2007,-1,-1,2,1,5695,lorraine goeuriot,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"L{'}objectif principal de notre travail consiste {\`a} {\'e}tudier la notion de comparabilit{\'e} des corpus, et nous abordons cette question dans un contexte monolingue en cherchant {\`a} distinguer les documents scientifiques et vulgaris{\'e}s. Nous travaillons s{\'e}par{\'e}ment sur des corpus compos{\'e}s de documents du domaine m{\'e}dical dans trois langues {\`a} forte distance linguistique (le fran{\c{c}}ais, le japonais et le russe). Dans notre approche, les documents sont caract{\'e}ris{\'e}s dans chaque langue selon leur th{\'e}matique et une typologie discursive qui se situe {\`a} trois niveaux de l{'}analyse des documents : structurel, modal et lexical. Le typage des documents est impl{\'e}ment{\'e} avec deux algorithmes d{'}apprentissage (SVMlight et C4.5). L{'}{\'e}valuation des r{\'e}sultats montre que la typologie discursive propos{\'e}e est portable d{'}une langue {\`a} l{'}autre car elle permet en effet de distinguer les deux discours. Nous constatons n{\'e}anmoins des performances tr{\`e}s vari{\'e}es selon les langues, les algorithmes et les types de caract{\'e}ristiques discursives."
2006.jeptalnrecital-poster.16,Relever des crit{\\`e}res pour la distinction automatique entre les documents m{\\'e}dicaux scientifiques et vulgaris{\\'e}s en russe et en japonais,2006,-1,-1,3,0,50652,sonia krivine,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Dans cet article, nous cherchons {\`a} affiner la notion de comparabilit{\'e} des corpus. Nous {\'e}tudions en particulier la distinction entre les documents scientifiques et vulgaris{\'e}s dans le domaine m{\'e}dical. Nous supposons que cette distinction peut apporter des informations importantes, par exemple en recherche d{'}information. Nous supposons par l{\`a} m{\^e}me que les documents, {\'e}tant le reflet de leur contexte de production, fournissent des crit{\`e}res n{\'e}cessaires {\`a} cette distinction. Nous {\'e}tudions plusieurs crit{\`e}res linguistiques, typographiques, lexicaux et autres pour la caract{\'e}risation des documents m{\'e}dicaux scientifiques et vulgaris{\'e}s. Les r{\'e}sultats pr{\'e}sent{\'e}s sont acquis sur les donn{\'e}es en russe et en japonais. Certains des crit{\`e}res {\'e}tudi{\'e}s s{'}av{\`e}rent effectivement pertinents. Nous faisons {\'e}galement quelques r{\'e}flexions et propositions quant {\`a} la distinction des cat{\'e}gories scientifique et vulgaris{\'e}e et aux questionnements th{\'e}oriques."
2006.jeptalnrecital-long.14,Productivit{\\'e} quantitative des suffixations par -it{\\'e} et -Able dans un corpus journalistique moderne,2006,-1,-1,1,1,5649,natalia grabar,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans ce travail, nous {\'e}tudions en corpus la productivit{\'e} quantitative des suffixations par -Able et par -it{\'e} du fran{\c{c}}ais, d{'}abord ind{\'e}pendamment l{'}une de l{'}autre, puis lorsqu{'}elles s{'}encha{\^\i}nent d{\'e}rivationnellement (la suffixation en -it{\'e} s{'}applique {\`a} des bases en -Able dans environ 15 {\%} des cas). Nous estimons la productivit{\'e} de ces suffixations au moyen de mesures statistiques dont nous suivons l{'}{\'e}volution par rapport {\`a} la taille du corpus. Ces deux suffixations sont productives en fran{\c{c}}ais moderne : elles forment de nouveaux lex{\`e}mes tout au long des corpus {\'e}tudi{\'e}s sans qu{'}on n{'}observe de saturation, leurs indices de productivit{\'e} montrent une {\'e}volution stable bien qu{'}{\'e}tant d{\'e}pendante des calculs qui leur sont appliqu{\'e}s. On note cependant que, de fa{\c{c}}on g{\'e}n{\'e}rale, de ces deux suffixations, c{'}est la suffixation par -it{\'e} qui est la plus fr{\'e}quente en corpus journalistique, sauf pr{\'e}cis{\'e}ment quand -it{\'e} s{'}applique {\`a} un adjectif en -Able. {\'E}tant entendu qu{'}un adjectif en -Able et le nom en -it{\'e} correspondant expriment la m{\^e}me propri{\'e}t{\'e}, ce r{\'e}sultat indique que la complexit{\'e} de la base est un param{\`e}tre {\`a} prendre en consid{\'e}ration dans la formation du lexique possible."
2005.jeptalnrecital-long.9,Utilisation de corpus de sp{\\'e}cialit{\\'e} pour le filtrage de synonymes de la langue g{\\'e}n{\\'e}rale,2005,-1,-1,1,1,5649,natalia grabar,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Les ressources linguistiques les plus facilement disponibles en TAL ressortissent g{\'e}n{\'e}ralement au registre g{\'e}n{\'e}ral d{'}une langue. Lorsqu{'}elles doivent {\^e}tre utilis{\'e}es sur des textes de sp{\'e}cialit{\'e} il peut {\^e}tre utile de les adapter {\`a} ces textes. Cet article est consacr{\'e} {\`a} l{'}adaptation de ressources synonymiques g{\'e}n{\'e}rales {\`a} la langue m{\'e}dicale. L{'}adaptation est obtenue suite {\`a} une s{\'e}rie de filtrages sur un corpus du domaine. Les synonymes originaux et les synonymes filtr{\'e}s sont ensuite utilis{\'e}s comme une des ressources pour la normalisation de variantes de termes dans une t{\^a}che de structuration de terminologie. Leurs apports respectifs sont {\'e}valu{\'e}s par rapport {\`a} la structure terminologique de r{\'e}f{\'e}rence. Cette {\'e}valuation montre que les r{\'e}sultats sont globalement encourageants apr{\`e}s les filtrages, pour une t{\^a}che comme la structuration de terminologies : une am{\'e}lioration de la pr{\'e}cision contre une l{\'e}g{\`e}re diminution du rappel."
2004.jeptalnrecital-poster.11,Rep{\\'e}rage de relations terminologiques transversales en corpus,2004,-1,-1,1,1,5649,natalia grabar,Actes de la 11{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Les relations transversales encodent des relations sp{\'e}cifiques entre les termes, par exemple localis{\'e}-dans, consomme, etc. Elles sont tr{\`e}s souvent d{\'e}pendantes des domaines, voire des corpus. Les m{\'e}thodes automatiques consacr{\'e}es au rep{\'e}rage de relations terminologiques plus classiques (hyperonymie, synonymie), peuvent g{\'e}n{\'e}rer occasionnellement les relations transversales. Mais leur rep{\'e}rage et typage restent sujets {\`a} une conceptualisation : ces relations ne sont pas attendues et souvent pas connues {\`a} l{'}avance pour un nouveau domaine {\`a} explorer. Nous nous attachons ici {\`a} leur rep{\'e}rage mais surtout {\`a} leur typage. En supposant que les relations sont souvent exprim{\'e}es par des verbes, nous misons sur l{'}{\'e}tude des verbes du corpus et de leurs divers d{\'e}riv{\'e}s afin d{'}aborder plus directement la d{\'e}couverte des relations du domaine. Les exp{\'e}riences montrent que ce point d{'}attaque peut {\^e}tre int{\'e}ressant, mais reste pourtant d{\'e}pendant de la polys{\'e}mie verbale et de la synonymie."
2003.jeptalnrecital-long.26,Application d{'}algorithmes de classification automatique pour la d{\\'e}tection des contenus racistes sur l{'}{I}nternet,2003,9,7,2,0,53049,romain vinot,Actes de la 10{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Le filtrage de contenus illicites sur Internet est une probl{\'e}matique difficile qui est actuellement r{\'e}solue par des approches {\`a} base de listes noires et de mots-cl{\'e}s. Les syst{\`e}mes de classification textuelle par apprentissage automatique n{\'e}cessitant peu d{'}interventions humaines, elles peuvent avantageusement remplacer ou compl{\'e}ter les m{\'e}thodes pr{\'e}c{\'e}dentes pour faciliter les mises {\`a} jour. Ces techniques, traditionnellement utilis{\'e}es avec des cat{\'e}gories d{\'e}finies par leur sujet ({\'e}conomie ou sport par exemple), sont fond{\'e}es sur la pr{\'e}sence ou l{'}absence de mots. Nous pr{\'e}sentons une {\'e}valuation de ces techniques pour le filtrage de contenus racistes. Contrairement aux cas traditionnels, les documents ne doivent pas {\^e}tre cat{\'e}goris{\'e}s suivant leur sujet mais suivant le point de vue {\'e}nonc{\'e} (raciste ou antiraciste). Nos r{\'e}sultats montrent que les classifieurs, essentiellement lexicaux, sont n{\'e}anmoins bien adapt{\'e}es : plus de 90{\%} des documents sont correctement class{\'e}s, voir m{\^e}me 99{\%} si l{'}on accepte une classe de rejet (avec 20{\%} d{'}exemples non class{\'e}s)."
2003.jeptalnrecital-long.27,Apprentissage de relations morphologiques en corpus,2003,-1,-1,3,0.333333,8591,pierre zweigenbaum,Actes de la 10{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous proposons une m{\'e}thode pour apprendre des relations morphologiques d{\'e}rivationnelles en corpus. Elle se fonde sur la cooccurrence en corpus de mots formellement proches et un filtrage compl{\'e}mentaire sur la forme des mots d{\'e}riv{\'e}s. Elle est mise en oeuvre et exp{\'e}riment{\'e}e sur un corpus m{\'e}dical. Les relations obtenues avant filtrage ont une pr{\'e}cision moyenne de 75,6 {\%} au 5000{\`e} rang (fen{\^e}tre de 150 mots). L{'}examen d{\'e}taill{\'e} des d{\'e}riv{\'e}s adjectivaux d{'}un {\'e}chantillon de 633 noms du champ de l{'}anatomie montre une bonne pr{\'e}cision de 85{--}91 {\%} et un rappel mod{\'e}r{\'e} de 32{--}34 {\%}. Nous discutons ces r{\'e}sultats et proposons des pistes pour les compl{\'e}ter."
W02-1403,Lexically-Based Terminology Structuring: Some Inherent Limits,2002,9,15,1,1,5649,natalia grabar,{COLING}-02: {COMPUTERM} 2002: Second International Workshop on Computational Terminology,0,"Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms. The present paper focusses on terminology structuring by lexical methods, which match terms on the basis on their content words, taking morphological variants into account. Experiments are done on a 'flat' list of terms obtained from an originally hierarchically-structured terminology: the French version of the US National Library of Medicine MeSH thesaurus. We compare the lexically-induced relations with the original MeSH relations: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis of the 'new' relations not present in the MeSH. This analysis shows, on the one hand, the limits of the lexical structuring method. On the other hand, it also reveals some specific structuring choices and naming conventions made by the MeSH designers, and emphasizes ontological commitments that cannot be left to automatic structuring."
W02-0304,Accenting unknown words in a specialized language,2002,12,7,2,0,8591,pierre zweigenbaum,Proceedings of the {ACL}-02 Workshop on Natural Language Processing in the Biomedical Domain,0,"We propose two internal methods for accenting unknown words, which both learn on a reference set of accented words the contexts of occurrence of the various accented forms of a given letter. One method is adapted from POS tagging, the other is based on finite state transducers.We show experimental results for letter e on the French version of the Medical Subject Headings thesaurus. With the best training set, the tagging method obtains a precision-recall breakeven point of 84.2xc2xb14.4% and the transducer method 83.8xc2xb14.5% (with a baseline at 64%) for the unknown words that contain this letter. A consensus combination of both increases precision to 92.0xc2xb13.7% with a recall of 75%. We perform an error analysis and discuss further steps that might help improve over the current performance."
2002.jeptalnrecital-long.3,Accentuation de mots inconnus : application au thesaurus biom{\\'e}dical {M}e{SH},2002,10,1,2,0,8591,pierre zweigenbaum,Actes de la 9{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Certaines ressources textuelles ou terminologiques sont {\'e}crites sans signes diacritiques, ce qui freine leur utilisation pour le traitement automatique des langues. Dans un domaine sp{\'e}cialis{\'e} comme la m{\'e}decine, il est fr{\'e}quent que les mots rencontr{\'e}s ne se trouvent pas dans les lexiques {\'e}lectroniques disponibles. Se pose alors la question de l{'}accentuation de mots inconnus : c{'}est le sujet de ce travail. Nous proposons deux m{\'e}thodes d{'}accentuation de mots inconnus fond{\'e}es sur un apprentissage par observation des contextes d{'}occurrence des lettres {\`a} accentuer dans un ensemble de mots d{'}entra{\^\i}nement, l{'}une adapt{\'e}e de l{'}{\'e}tiquetage morphosyntaxique, l{'}autre adapt{\'e}e d{'}une m{\'e}thode d{'}apprentissage de r{\`e}gles morphologiques. Nous pr{\'e}sentons des r{\'e}sultats exp{\'e}rimentaux pour la lettre e sur un thesaurus biom{\'e}dical en fran{\c{c}}ais : le MeSH. Ces m{\'e}thodes obtiennent une pr{\'e}cision de 86 {\`a} 96 {\%} (+-4 {\%}) pour un rappel allant de 72 {\`a} 86 {\%}."
2001.jeptalnrecital-poster.13,L{'}apport de connaissances morphologiques pour la projection de requ{\\^e}tes sur une terminologie normalis{\\'e}e,2001,8,0,2,0,8591,pierre zweigenbaum,Actes de la 8{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"L{'}apport de connaissances linguistiques {\`a} la recherche d{'}information reste un sujet de d{\'e}bat. Nous examinons ici l{'}influence de connaissances morphologiques (flexion, d{\'e}rivation) sur les r{\'e}sultats d{'}une t{\^a}che sp{\'e}cifique de recherche d{'}information dans un domaine sp{\'e}cialis{\'e}. Cette influence est {\'e}tudi{\'e}e {\`a} l{'}aide d{'}une liste de requ{\^e}tes r{\'e}elles recueillies sur un serveur op{\'e}rationnel ne disposant pas de connaissances linguistiques. Nous observons que pour cette t{\^a}che, flexion et d{\'e}rivation apportent un gain mod{\'e}r{\'e} mais r{\'e}el."
