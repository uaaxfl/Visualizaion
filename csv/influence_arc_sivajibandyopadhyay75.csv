2002.eamt-1.2,2001.mtsummit-teach.1,0,0.040388,"are machine translation systems, either commercial or research, available in those languages. Motivation for the Students The fourth factor is the motivation for the students in terms of higher studies and the development of the software industry in the multilingual setting where the students can possibly be absorbed after their course. This potential must exist not only within the country but also in the global frame. This necessitates that the students must learn a foreign language beside English. This can be made optional during the course and the students can learn a language afterwards. Balkan (2001) has given an overview of what the World Wide Web currently offers in the way of resources for teachers and learners of machine translation and discusses how the potential can be further exploited in the future. Language Pattern in the Education System The third important factor that guides teaching machine translation is the language pattern that students usually follow in the education system of the country. Let us take the example for India. The students are generally taught in their mother tongue up to the school level but Abaitua (1999) examines why it is worth learning translation techno"
2002.eamt-1.2,2001.mtsummit-teach.6,0,0.0268796,"ly multidisciplinary in nature, it will be taught to a wide variety of students. The level of research in turn determines whether on-line research MT systems in the native languages are available or not which can serve as a teaching aid. Availability of commercial MT systems also depends on the level of research in the country. Continuous academic and research interactions with various Universities and Institutions world wide increases the level of research work in the country. It also works as a motivating factor for students since it increases the possibility of their future higher studies. Kenny and Way (2001) describes the teaching of MT courses to two sets of students with different backgrounds: machine translation course to undergraduate students in Applied Computational Linguistics and ComputerAssisted Translation (CAT) course in two translator-training programs, one undergraduate and another postgraduate. The students are made aware that all systems can be expected to show an overall improvement in quality if notions of sublanguage and controlled language are taken into account. Students of Computational linguistics are viewed primarily as developers of MT systems. Translation students are mor"
2002.eamt-1.2,2001.mtsummit-teach.2,0,0.0253955,"ount. Students of Computational linguistics are viewed primarily as developers of MT systems. Translation students are more interested in CAT, and specially TM tools. The most important tools as far as MT teaching is concerned are the MT/MAT tools themselves. MAT tools include the dictionaries, terminologies, thesauri, bilingual concordances and translation memory systems. Blanc (2001) describes an interactive hypertextual environment for MT training. The main tool used for MT research at GETA (Groupe d&apos;Etude pour la Traduction Automatique), France is the ARIANE-G5 MT shell, a generator of MT Belam (2001) addresses the transferability of skills during an MT course mainly designed for students of modern languages. Transferable skills refer to the acquisition of abilities that are required for the mastery of the subject to be taught but also have a wider application in other areas. The transferable skills that a student of Computer Science and Engineering or 15 systems. The ARIANE-G5 shell and the methodology developed at GETA have been utilized in the 15-language Universal Networking Language project initiated by the Institute of Advanced Studies of the United Nations University (http://www.ias"
2002.eamt-1.2,2001.mtsummit-teach.7,0,0.0273217,"Missing"
2002.eamt-1.2,2001.mtsummit-teach.3,0,0.0373239,"rograms, one undergraduate and another postgraduate. The students are made aware that all systems can be expected to show an overall improvement in quality if notions of sublanguage and controlled language are taken into account. Students of Computational linguistics are viewed primarily as developers of MT systems. Translation students are more interested in CAT, and specially TM tools. The most important tools as far as MT teaching is concerned are the MT/MAT tools themselves. MAT tools include the dictionaries, terminologies, thesauri, bilingual concordances and translation memory systems. Blanc (2001) describes an interactive hypertextual environment for MT training. The main tool used for MT research at GETA (Groupe d&apos;Etude pour la Traduction Automatique), France is the ARIANE-G5 MT shell, a generator of MT Belam (2001) addresses the transferability of skills during an MT course mainly designed for students of modern languages. Transferable skills refer to the acquisition of abilities that are required for the mastery of the subject to be taught but also have a wider application in other areas. The transferable skills that a student of Computer Science and Engineering or 15 systems. The A"
2002.eamt-1.2,2001.mtsummit-teach.8,0,0.0343813,"on MT among (future) translators, translation instruction should be primarily practical and realistic, as well as learner-centred. Students with different backgrounds and interests have different needs with respect to MT teaching. This requires the development of separate courseware and the laboratory work for the different classes of students. The background of the students identifies their expectations from the course, their motivations, the preconceptions they have about the subject, the transferability of skills during an MT course and also what the instructors expect the students to do. Somers (2001) has identified three different but related perspectives of MT teaching depending on three distinct types of students: students of Computational Linguistics (CL), trainee translators and foreign language learners. Students of Computer Science & Engineering and Information Technology courses pose a different perspective than the students of linguistics. MT offers some interesting computational problems for computer scientists. Translation software offers some good examples in human-computer interaction and interfaces, and specially in software documentation. There is an ever increasing range of"
2002.eamt-1.2,2001.mtsummit-teach.4,0,0.0329906,"ng of MT and most of it is very recent. Again, that literature is based on the experiences of teaching the subject as a formal courseware. But there are students who usually do various developmental and research projects as part of their course curriculum and they need to be taught the basics of natural language processing in general and machine translation in particular. interested in knowing about MT technology. For the students and teachers of CL, then, MT systems can be used to illustrate problems (and solutions) in language analysis at various levels both monolingually and contrastively. Clavier and Poudat (2001) reports the educational experience in teaching machine translation in non-computer science subjects. In French universities, machine translation is more often taught in Faculties of Arts Linguistics and Foreign Language Departments - rather than in Faculties of Science - Computer Science Departments. Background of the students Translators need some insight into how MT works, why it is difficult, what kinds of translation tasks MT is appropriate for, what alternative computational tools are available and how to integrate them into the workflow. The aim with these students is rather to emphasiz"
2002.eamt-1.2,2001.mtsummit-teach.9,0,0.0608895,"Missing"
2002.eamt-1.2,2000.bcs-1.7,0,0.0337265,"ere are students who learn in English medium schools but they are also exposed to their mother tongue while in their family or in other gatherings. The exposure of two languages since schools days prepares the students at least as amateur translators. Moreover, the students usually learn Hindi, the national language of India, through televisions, films and other media. Languages are taught to the school students with strong grammar content. The students are thus able to understand the intricacies of Rule based machine translation systems. Higher education in India is taught in English medium. Forcada (2000) has proposed a laboratory assignment for non-computer science majors in a translation course using commercial machine translation systems to discover the word reordering patterns used by the commercial MT systems. The assignment is suitable for computer science majors also. A laboratory assignment has been proposed in (PerezOrtiz and Forcada, 2001) to study the way in which some commercial machine translation programs translate whole sentences and how the translation differs from a word-for-word translation. The students need not learn a foreign language during the course on MT if they know a"
2005.mtsummit-ebmt.16,2000.bcs-1.10,1,0.723892,"is most suitable for translation of news headlines. The EBMT strategy also allows the 2 Related Works In India, a heuristic approach for translating news headings from English to Hindi is found in (Sinha, 2002). A human-aided MT system for translating English news texts to Hindi is being developed at the Centre for Development of Advanced Computing, Mumbai (Rao et al, 2000). The system is now being enhanced and adopted for web translation service to the news agencies. A hybrid system for translating news items from English to Bengali (Naskar & Bandyopadhyay, 2005; Bandyopadhyay & Saha, 2002; Bandyopadhyay, 2000a, 2000b) is being developed at the Jadavpur University, India. The NHK System of Japan which translates English newspaper articles to Japanese is described in (Hutchins, 1999). The improvement of translation quality of English newspaper headlines by automatic pre-editing in the English to Japanese machine translation system being developed at the Sharp Corporation of Japan is discussed in (Yoshimi, 2001). The work focuses on the absence of the verb be and formulates a set of rewriting rules for putting the verb properly into headlines, based on information obtained by morpholexical and rough"
2005.mtsummit-ebmt.16,2005.mtsummit-posters.8,1,0.414,") Phrasal Example Base. The Direct Example Base is like the Translation Memory that stores the headlines in the source language and their translation in the target language. The Generalized Tagged Example Base stores the tagged examples with proper alignment. The Phrasal Example Base, stores the various phrase patterns in terms of the part of speech of the constituent words in the source language and their corresponding translation in the target language. The system uses the phrasal example base of English and Bengali phrase patterns used in a phrasal example based machine translation system (Naskar and Bandyopadhyay, 2005). The phrasal EBMT system is being developed for translating English news items to Bengali. The Phrasal Example base consists of translation examples ( phrasal templates ) that store the part of speech of the constituent words of the phrases along with necessary syntactic information. Some examples of noun phrasal examples are: <art $ a / an> <noun & singular, human, nominative> ÅÆ<AEõLX [ekjan]> <noun> (ii) <art $ the> <noun & singular, human, objective> ÅÆ <noun> <-×»OôãEõ [-tike]> (iii) <art $ a / an> <adj> <noun & singular, inanimate, objective> ÅÆ <AEõ×»Oô [ekti]> <adj> <noun> Dictionary"
2005.mtsummit-posters.21,2003.mtsummit-systems.15,0,0.227061,"Missing"
2005.mtsummit-posters.21,2003.mtsummit-papers.19,0,0.0490042,"Missing"
2005.mtsummit-posters.8,2003.mtsummit-systems.15,0,0.0440617,"amese. An account of the structural and categorical divergence problem for English and Hindi have been given in (Gupta and Chatterjee, 2003). They have proposed an algorithm for identification of this divergence for an English-Hindi EBMT system. This identification helps in partitioning the example database into divergence / non-divergence categories, which in turn should facilitate efficient retrieval and adaptation in an EBMT system. The development of an English to Hindi MachineAided Translation System named AnglaHindi based on the ANGLABHARTI translation methodology has been described in (Sinha and Jain, 2003). AnglaBharti is a pattern directed rule based system with context free grammar like structure for analysis of English as source language. It analyses English sentences and creates an intermediate structure that has the word and word-group order as per the structure of the group of target languages. The intermediate structure is then converted to final translation in the target language through a process of text-generation. The next three sections 2, 3 and 4 describe the English NPs, VPs and PPs and how these phrases are translated in Bengali. Section 5 discusses about the phrase ordering rule"
2005.mtsummit-posters.8,2003.mtsummit-papers.19,0,0.0258981,"phrase may refer to a plural entity, it does not have a plural ending, e.g., anek-jon sadasso ‘many members’, du-ti boi ‘two books’. An NP in English also includes pronouns and adjectives and in the next two subsections (2.1 and 2.2) we discuss how such words are handled in English and Bengali. Subsection 2.3 includes a snapshot of the phrasal example base for NPs that encode the NP transfer rules. Vietnamese bilingual corpus in order to transfer word orders from English into Vietnamese. An account of the structural and categorical divergence problem for English and Hindi have been given in (Gupta and Chatterjee, 2003). They have proposed an algorithm for identification of this divergence for an English-Hindi EBMT system. This identification helps in partitioning the example database into divergence / non-divergence categories, which in turn should facilitate efficient retrieval and adaptation in an EBMT system. The development of an English to Hindi MachineAided Translation System named AnglaHindi based on the ANGLABHARTI translation methodology has been described in (Sinha and Jain, 2003). AnglaBharti is a pattern directed rule based system with context free grammar like structure for analysis of English"
2005.mtsummit-posters.8,P02-1040,0,0.0774275,"Missing"
2005.mtsummit-posters.8,1992.tmi-1.12,0,0.498967,"n, number information of the subject and tense and aspect information of the verb. But for any particular root verb, there are only a few verb forms in English, whereas in Bengali it shows a lot of variations. The structural divergence between English and Bengali suggests that a Phrasal EBMT system would suit best for translating English to Bengali. In the present system, grammar examples expressed as context-sensitive rewrite rules (using semantic features) are stored in a phrasal example base. Some references on Example Based Machine Translation (EBMT) strategy can be found in (Nagao, 1984; Furuse and Iida, 1992; Somers, 2000; Carl and Andy, 2003). A hybrid approach based on fixed rules (rulebased) and stochastic methods (corpus-based) which extracts word order transfer rules between two languages has been presented in (Dien et. al., 2003). They have used this approach for translating from English to Vietnamese. They have used the Transfer-based Learning (TBL) machine learning approach for learning transfer rules from EnglishAbstract The present work describes a Phrasal Example Based Machine Translation system from English to Bengali that identifies the phrases in the input through a shallow analysis"
2011.mtsummit-papers.23,W05-0909,0,0.167478,"Missing"
2011.mtsummit-papers.23,J93-2003,0,0.0408229,"Predicates (CPs) is absent, such complex predicates are considered as Multi Word Expressions (MWEs)(Baldwin and Kim, 2010, Sinha, 2009). The other types of predicates such as ĒĂĘĠĺñĊ niye gelo ‘take-go’ (took and went), ĒĀĘĠ ĺñĊ diye gelo ‘give-go’ (gave and went) follow the lexical pattern FV+LV, similar to Complex Predicates (CPs) but the Full Verb and Light Verb behave as independent syntactic entities. These verb patterns are nonComplex Predicates (non-CPs) and are also termed as Serial Verb (SV) (Mukherjee et al., 2006). Traditional approaches to word alignment follow the IBM Models (Brown et al., 1993). These approaches are unable to handle many-tomany alignments and hence do not work well with multi-word expressions, especially with NEs, reduplications and complex predicates. The alignment probabilities in the well-known Hidden Markov Model (HMM: Vogel et al., 1996) depend on the alignment position of the previous word. The HMM model does not explicitly consider many-to-many alignments. In this experiment, we address this many-tomany alignment problem indirectly. Our objective is to see how the identification of MWEs enhances the performance of the SMT system. In this work, several types o"
2011.mtsummit-papers.23,N10-1029,0,0.0315736,"ported a discriminative approach to use the compositionality information of verb-based multi-word expressions in order to improve the word alignment quality. Ren et al. (2009) presented a log likelihood ratio based hierarchical reducing algorithm to automatically extract bilingual MWEs. They investigated the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al., 2007). They observed the highest improvement with an additional feature that identifies whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010) who replaced the binary feature by a count feature representing the number of MWEs in the source language phrase. Intuitively, MWEs on the source and the target sides should be both aligned in the parallel corpus and translated as a whole. However, in the state-of-the-art PB-SMT systems, the constituents of an MWE are marked and aligned as parts of consecutive phrases, since PB-SMT (or any other approaches to SMT) does not generally treat MWEs as special tokens. Another problem with SMT systems is the wrong translation of verb phrases. Sometimes verb phrases are deleted in the output sentence"
2011.mtsummit-papers.23,C08-2007,0,0.0657892,"Missing"
2011.mtsummit-papers.23,W10-3710,1,0.919301,"fied on both sides of the parallel corpus. Das et al. (2010) analyzed and identified a category of compound verbs (Verb + Verb) and conjunct verbs (Noun /Adjective/Adverb + Verb) for Bengali. We adapted their strategy for identification of compound verbs as well as serial verbs (Verb + Verb + Verb) in Bengali. For the identification of Named-Entities and their alignment, we have adopted a similar technique as reported in Pal.et.al (2010). Reduplicated phrases do not occur very frequently in the English corpus; some of them (like correlatives, semantic reduplications) are not found in English (Chakraborty and Bandyopadhyay, 2010). But reduplication plays a crucial role on the target Bengali side as they occur with high frequency. These reduplicated phrases are considered as a single-token so that they may map to a single word on the source side. Phrasal prepositions and verb object combinations are also treated as single tokens. Once the compound verbs and the NEs are identified on both sides of the parallel corpus, they are assembled into single tokens. When converting these MWEs into single tokens, we replace the spaces with underscores (‘_’). Since there are already some hyphenated words in the corpus, we do not us"
2011.mtsummit-papers.23,C04-1114,0,0.022419,"ex predicates are identified on both sides of the parallel corpus. In the target side, identification of the Noun-noun MWEs and reduplicated phrases are carried out. We use simple rule-based and statistical approaches to identify these MWEs. Source and target language NEs are aligned using a statistical transliteration technique. We rely on these automatically aligned NEs and treat them as translation examples (Pal.et.al, 2010). Adding bilingual dictionaries, which in effect are instances of atomic translation pairs, to the parallel corpus is a well-known practice in domain adaptation in SMT (Eck et al., 2004; Wu et al., 2008). We modify the parallel corpus by converting the MWEs into single tokens and adding the aligned NEs and complex predicates in the parallel cor216 pus to improve the word alignment and hence the phrase alignment quality. The preprocessing of the parallel corpus results in improved MT quality in terms of automatic MT evaluation metrics. The remainder of the paper is organized as follows. Next section briefly elaborates the related work. The English-Bengali PBSMT system is described in Section 3. Section 4 states the tools and resources used for the various experiments. Section"
2011.mtsummit-papers.23,W09-3539,1,0.886048,"Missing"
2011.mtsummit-papers.23,W04-3248,0,0.021351,"tes the related work. The English-Bengali PBSMT system is described in Section 3. Section 4 states the tools and resources used for the various experiments. Section 5 includes the results obtained, together with some analysis. Section 6 concludes and provides avenues for further work. 2 Related Work Moore (2003) used capitalization cues for identifying NEs on the English side and then applied statistical techniques to decide which portion of the target language corresponds to the specified English NE. A Maximum Entropy model based approach for English—Chinese NE alignment has been proposed in Feng et al. (2004) which significantly outperforms IBM Model 4 and HMM. A method for automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization has been proposed in Huang et al. (2003). Venkatapathy and Joshi (2006) reported a discriminative approach to use the compositionality information of verb-based multi-word expressions in order to improve the word alignment quality. Ren et al. (2009) presented a log likelihood ratio based hierarchical reducing algorithm to automatically extract bilingual MWEs. They investigated the usefulness of these biling"
2011.mtsummit-papers.23,W03-1502,0,0.0222628,"n the parallel cor216 pus to improve the word alignment and hence the phrase alignment quality. The preprocessing of the parallel corpus results in improved MT quality in terms of automatic MT evaluation metrics. The remainder of the paper is organized as follows. Next section briefly elaborates the related work. The English-Bengali PBSMT system is described in Section 3. Section 4 states the tools and resources used for the various experiments. Section 5 includes the results obtained, together with some analysis. Section 6 concludes and provides avenues for further work. 2 Related Work Moore (2003) used capitalization cues for identifying NEs on the English side and then applied statistical techniques to decide which portion of the target language corresponds to the specified English NE. A Maximum Entropy model based approach for English—Chinese NE alignment has been proposed in Feng et al. (2004) which significantly outperforms IBM Model 4 and HMM. A method for automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization has been proposed in Huang et al. (2003). Venkatapathy and Joshi (2006) reported a discriminative approac"
2011.mtsummit-papers.23,N03-1017,0,0.055513,"rallel corpus. The sentences on the target side (Bengali) are POS-tagged by using the tools obtained from the consortium mode project “Development of Indian Language to Indian Language Machine Translation (IL-ILMT) System2”. NEs in Bengali are identified using the NER system of Ekbal and Bandyopadhyay (2008). We have used the Stanford Parser and the Bengali NER. The effectiveness of the MWE-aligned parallel corpus is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phrase-extraction heuristics described in (Koehn et al., 2003), minimumerror-rate training (Och, 2003) on a held-out development set, target language model trained using SRILM toolkit (Stolcke, 2002) with Knes2 The EILMT and ILILMT projects are funded by the Department of Information Technology (DIT), Ministry of Communications and Information Technology (MCIT), Government of India. 3 http://nlp.stanford.edu/software/lex-parser.shtml 4 http://crfchunker.sourceforge.net/ 5 http://wordnet.princeton.edu/ 220 er-Ney smoothing (Kneser and Ney, 1995) and the Moses decoder (Koehn et al., 2007). 5 Experiments and Results We have randomly identified 500 sentences"
2011.mtsummit-papers.23,P07-2045,0,0.0205942,"omatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization has been proposed in Huang et al. (2003). Venkatapathy and Joshi (2006) reported a discriminative approach to use the compositionality information of verb-based multi-word expressions in order to improve the word alignment quality. Ren et al. (2009) presented a log likelihood ratio based hierarchical reducing algorithm to automatically extract bilingual MWEs. They investigated the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al., 2007). They observed the highest improvement with an additional feature that identifies whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010) who replaced the binary feature by a count feature representing the number of MWEs in the source language phrase. Intuitively, MWEs on the source and the target sides should be both aligned in the parallel corpus and translated as a whole. However, in the state-of-the-art PB-SMT systems, the constituents of an MWE are marked and aligned as parts of consecutive phrases, since PB-SMT (or any other ap"
2011.mtsummit-papers.23,W04-3250,0,0.162084,"Missing"
2011.mtsummit-papers.23,W09-2906,0,0.047178,"Missing"
2011.mtsummit-papers.23,E03-1035,0,0.0223954,"ates in the parallel cor216 pus to improve the word alignment and hence the phrase alignment quality. The preprocessing of the parallel corpus results in improved MT quality in terms of automatic MT evaluation metrics. The remainder of the paper is organized as follows. Next section briefly elaborates the related work. The English-Bengali PBSMT system is described in Section 3. Section 4 states the tools and resources used for the various experiments. Section 5 includes the results obtained, together with some analysis. Section 6 concludes and provides avenues for further work. 2 Related Work Moore (2003) used capitalization cues for identifying NEs on the English side and then applied statistical techniques to decide which portion of the target language corresponds to the specified English NE. A Maximum Entropy model based approach for English—Chinese NE alignment has been proposed in Feng et al. (2004) which significantly outperforms IBM Model 4 and HMM. A method for automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization has been proposed in Huang et al. (2003). Venkatapathy and Joshi (2006) reported a discriminative approac"
2011.mtsummit-papers.23,P03-1021,0,0.0278586,"Bengali) are POS-tagged by using the tools obtained from the consortium mode project “Development of Indian Language to Indian Language Machine Translation (IL-ILMT) System2”. NEs in Bengali are identified using the NER system of Ekbal and Bandyopadhyay (2008). We have used the Stanford Parser and the Bengali NER. The effectiveness of the MWE-aligned parallel corpus is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phrase-extraction heuristics described in (Koehn et al., 2003), minimumerror-rate training (Och, 2003) on a held-out development set, target language model trained using SRILM toolkit (Stolcke, 2002) with Knes2 The EILMT and ILILMT projects are funded by the Department of Information Technology (DIT), Ministry of Communications and Information Technology (MCIT), Government of India. 3 http://nlp.stanford.edu/software/lex-parser.shtml 4 http://crfchunker.sourceforge.net/ 5 http://wordnet.princeton.edu/ 220 er-Ney smoothing (Kneser and Ney, 1995) and the Moses decoder (Koehn et al., 2007). 5 Experiments and Results We have randomly identified 500 sentences each for the development set and the te"
2011.mtsummit-papers.23,W10-3707,1,0.463916,"Missing"
2011.mtsummit-papers.23,P02-1040,0,0.092291,"Missing"
2011.mtsummit-papers.23,W09-2907,0,0.0191371,"on of the target language corresponds to the specified English NE. A Maximum Entropy model based approach for English—Chinese NE alignment has been proposed in Feng et al. (2004) which significantly outperforms IBM Model 4 and HMM. A method for automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization has been proposed in Huang et al. (2003). Venkatapathy and Joshi (2006) reported a discriminative approach to use the compositionality information of verb-based multi-word expressions in order to improve the word alignment quality. Ren et al. (2009) presented a log likelihood ratio based hierarchical reducing algorithm to automatically extract bilingual MWEs. They investigated the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al., 2007). They observed the highest improvement with an additional feature that identifies whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010) who replaced the binary feature by a count feature representing the number of MWEs in the source language phrase. Intuitively, MWEs on the source and t"
2011.mtsummit-papers.23,W06-1204,0,0.0606765,"Missing"
2011.mtsummit-papers.23,C96-2141,0,0.381653,"ollow the lexical pattern FV+LV, similar to Complex Predicates (CPs) but the Full Verb and Light Verb behave as independent syntactic entities. These verb patterns are nonComplex Predicates (non-CPs) and are also termed as Serial Verb (SV) (Mukherjee et al., 2006). Traditional approaches to word alignment follow the IBM Models (Brown et al., 1993). These approaches are unable to handle many-tomany alignments and hence do not work well with multi-word expressions, especially with NEs, reduplications and complex predicates. The alignment probabilities in the well-known Hidden Markov Model (HMM: Vogel et al., 1996) depend on the alignment position of the previous word. The HMM model does not explicitly consider many-to-many alignments. In this experiment, we address this many-tomany alignment problem indirectly. Our objective is to see how the identification of MWEs enhances the performance of the SMT system. In this work, several types of MWEs like phrasal prepositions and Verb-object combinations are automatically identified on the source side while named-entities and complex predicates are identified on both sides of the parallel corpus. In the target side, identification of the Noun-noun MWEs and re"
2011.mtsummit-papers.23,C08-1125,0,0.039656,"Missing"
2013.mtsummit-papers.8,W05-0909,0,0.502789,"Missing"
2013.mtsummit-papers.8,W12-5108,0,0.0173497,"y extract bilingual MWEs has been described in Ren et al. (2009). They examined the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al., 2007). They observed the highest improvement with an additional feature that identifies whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010) who replaced the binary feature by a count feature representing the number of MWEs in the source language phrase. A hybrid approach to identify MWEs from the English-French parallel corpus proposed by (Bouamor et al., 2012a), they aligned only many to many correspondences and deals with highly correlated MWE in a sentence pair, those are then integrated into the MOSES SMT System (Bouamor et al., 2012b). MWEs in SMT for Verbmobil corpus has also been proposed by (Lambert et al., 2005), the performance of the system evaluated in terms of alignment and translation quality. Instinctively, MWEs on the source and the target sides should be both aligned in the parallel corpus and translated as a whole. However, the constituents of an MWE are identified and aligned as parts of consecutive phrases in the state-of-the-ar"
2013.mtsummit-papers.8,bouamor-etal-2012-identifying,0,0.039193,"Missing"
2013.mtsummit-papers.8,J93-2003,0,0.0662809,"Missing"
2013.mtsummit-papers.8,N10-1029,0,0.0691116,"discriminative approach to use the compositionality information of verb-based multi-word expressions in order to improve the word alignment quality. A log likelihood ratio based hierarchical reducing algorithm to automatically extract bilingual MWEs has been described in Ren et al. (2009). They examined the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al., 2007). They observed the highest improvement with an additional feature that identifies whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010) who replaced the binary feature by a count feature representing the number of MWEs in the source language phrase. A hybrid approach to identify MWEs from the English-French parallel corpus proposed by (Bouamor et al., 2012a), they aligned only many to many correspondences and deals with highly correlated MWE in a sentence pair, those are then integrated into the MOSES SMT System (Bouamor et al., 2012b). MWEs in SMT for Verbmobil corpus has also been proposed by (Lambert et al., 2005), the performance of the system evaluated in terms of alignment and translation quality. Instinctively, MWEs on"
2013.mtsummit-papers.8,W10-3710,1,0.92874,"ailable. Furthermore, this is the first time when the identification of MWEs in Bengali language is used to enhance the performance of an EnglishBengali Machine Translation System. 3 3.1 System Description Preprocessing of the parallel corpus We considered several types of multi-word expressions: noun-noun MWEs, reduplicated phrases, complex predicates, phrasal prepositions, and verb-object combination. For the identification of complex predicates, we adopted a similar technique as reported in (Das et al., 2010). There are no frequent occurrences of reduplicated phrases in the English Corpus (Chakraborty and Bandyopadhyay, 2010) in comparison with the Bengali corpus, so this plays very crucial role in machine translation as they occur with high frequently in the Bengali corpus. Once the MWEs are identified, they are converted into single-tokens by replacing the spaces with underscores (‗_‘) so that we can establish 1to-1 alignments between the source and target MWEs. 63 3.2 MWE Identification Noun-Noun MWE Identification: When two or more nouns are united together to form a solo phrase such as ‗bed room‘ or ‗dining table‘ (Baldwin and Kim, 2010), these are termed as compound nouns or nominal compounds. Compound noun"
2013.mtsummit-papers.8,J90-1003,0,0.487629,"Missing"
2013.mtsummit-papers.8,J93-1003,0,0.47067,"Missing"
2013.mtsummit-papers.8,C04-1114,0,0.0325266,"unds are automatically extracted on the source side while noun-noun compounds, reduplicated phrases and complex predicates are identified on the target side of the parallel corpus. We use simple rule-based and statistical approaches to identify these MWEs. We have also extracted MWEs from comparable corpora which enhance not only MWE identification quality but also provide out-of-vocabulary words to SMT system. This also helps to accrue some knowledge of out of domain data. Source and target language MWEs are aligned using a Hybrid technique. A well-known practice in domain adaptation in SMT (Eck et al., 2004; Wu et al., 2008) is to incorporate bilingual dictionaries to the training corpus; which affects on the instances of atomic translation pairs. The work has been carried out into three direction (i) The parallel corpus has been modified by single tokenization of MWEs, (ii) The alignment of MWEs are added in the parallel corpus as additional data to improve the word alignment as well as the phrase alignment quality and (iii) The alignment of MWE has been directly incorporated into the word alignment model. The preprocessing of the parallel corpus results in improved MT quality in terms of autom"
2013.mtsummit-papers.8,N03-1017,0,0.0246219,"eline system: GIZA++ implementation of IBM word alignment 2 The EILMT project is funded by the Department of Electronics and Information Technology (DEITY), Ministry of Communications and Information Technology (MCIT), Government of India. 3 http://nlp.stanford.edu/software/lex-parser.shtml 4 http://crfchunker.sourceforge.net/ 5 http://wordnet.princeton.edu/ 6 The IL-ILMT project is funded by the Department of Electronics and Information Technology (DEITY), Ministry of Communications and Information Technology (MCIT), Government of India. 65 model 4, phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model trained using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) and the Moses decoder (Koehn et al., 2007) have been used in the present study. 5 Experiments and Evaluations We have randomly identified 500 sentences each for the development set and the test set from the initial parallel corpus. The rest are considered as the training corpus. The training corpus was filtered with the maximum allowable sentence length of 100 words and sentence length ratio of 1:2 (either wa"
2013.mtsummit-papers.8,P07-2045,0,0.016243,"eriments. Section 5 includes the results obtained, together with some analysis. Section 6 concludes and provides avenues for further work. 2 Related Work Venkatapathy and Joshi (2006) reported a discriminative approach to use the compositionality information of verb-based multi-word expressions in order to improve the word alignment quality. A log likelihood ratio based hierarchical reducing algorithm to automatically extract bilingual MWEs has been described in Ren et al. (2009). They examined the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al., 2007). They observed the highest improvement with an additional feature that identifies whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010) who replaced the binary feature by a count feature representing the number of MWEs in the source language phrase. A hybrid approach to identify MWEs from the English-French parallel corpus proposed by (Bouamor et al., 2012a), they aligned only many to many correspondences and deals with highly correlated MWE in a sentence pair, those are then integrated into the MOSES SMT System (Bouamor et al., 20"
2013.mtsummit-papers.8,W04-3250,0,0.388494,"Missing"
2013.mtsummit-papers.8,2005.mtsummit-posters.11,0,0.059502,"Missing"
2013.mtsummit-papers.8,P03-1021,0,0.0185684,"ignment 2 The EILMT project is funded by the Department of Electronics and Information Technology (DEITY), Ministry of Communications and Information Technology (MCIT), Government of India. 3 http://nlp.stanford.edu/software/lex-parser.shtml 4 http://crfchunker.sourceforge.net/ 5 http://wordnet.princeton.edu/ 6 The IL-ILMT project is funded by the Department of Electronics and Information Technology (DEITY), Ministry of Communications and Information Technology (MCIT), Government of India. 65 model 4, phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model trained using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) and the Moses decoder (Koehn et al., 2007) have been used in the present study. 5 Experiments and Evaluations We have randomly identified 500 sentences each for the development set and the test set from the initial parallel corpus. The rest are considered as the training corpus. The training corpus was filtered with the maximum allowable sentence length of 100 words and sentence length ratio of 1:2 (either way). Finally the training corpus contained"
2013.mtsummit-papers.8,W10-3707,1,0.735788,"Missing"
2013.mtsummit-papers.8,P02-1040,0,0.0918365,"s and 1,223 of noun-noun compounds. The experiments have been carried out in various experimental settings: (i) single tokenization of MWEs on both sides in the parallel corpus, (ii) single tokenized MWEs added with the parallel training data, (iii) single tokenized MWEs directly integrated into the word alignment model, and finally, (iv) bootstrapping with single iteration using the experimental setup (ii) and (iii) to examine how the parallel MWE alignment set can be increased. Extrinsic evaluation was carried out on the MT quality using the well-known automatic MT evaluation metrics: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) and the evaluation results are reported in Table 2. By considering single tokenization (experiment 2), the system achieves performance improvement to some extent. Use of comparable corpora (experiment 3) improves the MWE identification performance which in turn improves the translation quality. Training set CPs English T U 8142 388 9 55 15 Bengali T U 2017 7154 4 185 150 reduplicated word Noun-noun 892 711 489 300 compound Noun-noun 1792 981 889 700 compound with Comparable corpora Phrasal prep- 1782 137 osition 9 Verb-object 231 145 combination Phrasal verb 549 53"
2013.mtsummit-papers.8,W09-2907,0,0.0230463,"work. The English-Bengali PB-SMT system is described in Section 3. Section 4 states the tools and resources used for the various experiments. Section 5 includes the results obtained, together with some analysis. Section 6 concludes and provides avenues for further work. 2 Related Work Venkatapathy and Joshi (2006) reported a discriminative approach to use the compositionality information of verb-based multi-word expressions in order to improve the word alignment quality. A log likelihood ratio based hierarchical reducing algorithm to automatically extract bilingual MWEs has been described in Ren et al. (2009). They examined the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al., 2007). They observed the highest improvement with an additional feature that identifies whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010) who replaced the binary feature by a count feature representing the number of MWEs in the source language phrase. A hybrid approach to identify MWEs from the English-French parallel corpus proposed by (Bouamor et al., 2012a), they aligned only many to many correspon"
2013.mtsummit-papers.8,W09-2906,0,0.0684193,"Missing"
2013.mtsummit-papers.8,W03-1803,0,0.101085,"Missing"
2013.mtsummit-papers.8,W06-1204,0,0.0428091,"Missing"
2013.mtsummit-papers.8,C08-1125,0,0.412897,"Missing"
2016.gwc-1.35,C04-1054,0,0.652071,"Missing"
2018.gwc-1.2,W98-1118,0,0.201476,"2.0 and the development 2 http://alexabe.pbworks.com/f/Dictionary+of+Medical+ Terms+4th+Ed.-+(Malestrom).pdf steps of WME 3.0; Section 5 discusses the validation process of the proposed lexicon; finally, Section 6 illustrates the concluding remarks and future scopes of the research. 2 Background Biomedical information extraction is treated as one of the challenging research tasks as it deals with available medical corpora that are either unstructured or semi-structured. Hence, a domainspecific lexicon becomes an essential component to convert a structured corpus from the unstructured corpus (Borthwick et al., 1998). Also, it helps in extracting the subjective and conceptual information related to medical concepts from the corpus. Various researchers have tried to build various ontologies and lexicons such as UMLS, SNOMED-CT (Systematized Nomenclature of Medicine-Clinical Terms), MWN (Medical WordNet), SentiHealth, and WordNet of Medical Events (WME 1.0 and WME 2.0) etc. in the domain of healthcare (Miller and Fellbaum, 1998; Smith and Fellbaum, 2004; Asghar et al., 2016; Asghar et al., 2014). UMLS helps to enhance the access to biomedical literature by facilitating the development of computer systems th"
2018.gwc-1.2,C16-1251,1,0.921398,"al., 2010a; Cambria et al., 2010b). However, medical text is in general unstructured since doctors do not like to fill forms and prefer free-form notes of their observations. Hence, a lexical design is difficult due to lack of any prior knowledge of medical terms and contexts. Therefore, we are motivated to enhance a medical lexicon namely WordNet of Medical Events (WME 2.0) which helps to identify medical concepts and their features. In order to enrich this lexicon, we have employed various well-known resources like conventional WordNet, SentiWordNet (Esuli and Sebastiani, 2006), SenticNet (Cambria et al., 2016), Bing Liu (Liu, 2012), and Taboada’s Adjective list (Taboada et al., 2011) and a preprocessed English medical dictionary1 on top of WME 1.0 and WME 2.0 lexicons (Mondal et al., 2015; Mondal et al., 2016). WME 1.0 contains 6415 number of medical concepts and their glosses, POS, polarity scores, and sentiment. Thereafter, Mondal et. al., (2016) enhanced WME 1.0 by adding few more features as affinity score, gravity score, and SSW to the medical concepts and presented as WME 2.0. The affinity and gravity scores present the hidden link between the pair of medical concepts and the concept with the"
2018.gwc-1.2,esuli-sebastiani-2006-sentiwordnet,0,0.45784,"non-experts such as patients (Cambria et al., 2010a; Cambria et al., 2010b). However, medical text is in general unstructured since doctors do not like to fill forms and prefer free-form notes of their observations. Hence, a lexical design is difficult due to lack of any prior knowledge of medical terms and contexts. Therefore, we are motivated to enhance a medical lexicon namely WordNet of Medical Events (WME 2.0) which helps to identify medical concepts and their features. In order to enrich this lexicon, we have employed various well-known resources like conventional WordNet, SentiWordNet (Esuli and Sebastiani, 2006), SenticNet (Cambria et al., 2016), Bing Liu (Liu, 2012), and Taboada’s Adjective list (Taboada et al., 2011) and a preprocessed English medical dictionary1 on top of WME 1.0 and WME 2.0 lexicons (Mondal et al., 2015; Mondal et al., 2016). WME 1.0 contains 6415 number of medical concepts and their glosses, POS, polarity scores, and sentiment. Thereafter, Mondal et. al., (2016) enhanced WME 1.0 by adding few more features as affinity score, gravity score, and SSW to the medical concepts and presented as WME 2.0. The affinity and gravity scores present the hidden link between the pair of medical"
2018.gwc-1.2,2016.gwc-1.35,1,0.918354,"fficult due to lack of any prior knowledge of medical terms and contexts. Therefore, we are motivated to enhance a medical lexicon namely WordNet of Medical Events (WME 2.0) which helps to identify medical concepts and their features. In order to enrich this lexicon, we have employed various well-known resources like conventional WordNet, SentiWordNet (Esuli and Sebastiani, 2006), SenticNet (Cambria et al., 2016), Bing Liu (Liu, 2012), and Taboada’s Adjective list (Taboada et al., 2011) and a preprocessed English medical dictionary1 on top of WME 1.0 and WME 2.0 lexicons (Mondal et al., 2015; Mondal et al., 2016). WME 1.0 contains 6415 number of medical concepts and their glosses, POS, polarity scores, and sentiment. Thereafter, Mondal et. al., (2016) enhanced WME 1.0 by adding few more features as affinity score, gravity score, and SSW to the medical concepts and presented as WME 2.0. The affinity and gravity scores present the hidden link between the pair of medical concepts and the concept with the various source of glosses respectively. SSW of a medical concept refers the similar sentiment words (SSW) which follow the common sentiment property. In the current research, we have focused on enriching"
2018.gwc-1.2,C04-1054,0,0.0643487,"unstructured or semi-structured. Hence, a domainspecific lexicon becomes an essential component to convert a structured corpus from the unstructured corpus (Borthwick et al., 1998). Also, it helps in extracting the subjective and conceptual information related to medical concepts from the corpus. Various researchers have tried to build various ontologies and lexicons such as UMLS, SNOMED-CT (Systematized Nomenclature of Medicine-Clinical Terms), MWN (Medical WordNet), SentiHealth, and WordNet of Medical Events (WME 1.0 and WME 2.0) etc. in the domain of healthcare (Miller and Fellbaum, 1998; Smith and Fellbaum, 2004; Asghar et al., 2016; Asghar et al., 2014). UMLS helps to enhance the access to biomedical literature by facilitating the development of computer systems that understand biomedical language (Bodenreider, 2004). SNOMED-CT is a standardized, multilingual vocabulary that contains clinical terminologies and assists in exchanging the electronic healthcare information among physicians (Donnelly, 2006). Furthermore, Fellbaum and Smith (2004) proposed Medical WordNet (MWN) with two subnetworks e.g., Medical FactNet (MFN) and Medical BeliefNet (MBN) for justifying the consumer health. The MWN follows"
2018.gwc-1.2,J11-2001,0,0.35707,"structured since doctors do not like to fill forms and prefer free-form notes of their observations. Hence, a lexical design is difficult due to lack of any prior knowledge of medical terms and contexts. Therefore, we are motivated to enhance a medical lexicon namely WordNet of Medical Events (WME 2.0) which helps to identify medical concepts and their features. In order to enrich this lexicon, we have employed various well-known resources like conventional WordNet, SentiWordNet (Esuli and Sebastiani, 2006), SenticNet (Cambria et al., 2016), Bing Liu (Liu, 2012), and Taboada’s Adjective list (Taboada et al., 2011) and a preprocessed English medical dictionary1 on top of WME 1.0 and WME 2.0 lexicons (Mondal et al., 2015; Mondal et al., 2016). WME 1.0 contains 6415 number of medical concepts and their glosses, POS, polarity scores, and sentiment. Thereafter, Mondal et. al., (2016) enhanced WME 1.0 by adding few more features as affinity score, gravity score, and SSW to the medical concepts and presented as WME 2.0. The affinity and gravity scores present the hidden link between the pair of medical concepts and the concept with the various source of glosses respectively. SSW of a medical concept refers th"
2019.icon-1.17,A92-1018,0,0.808522,"Missing"
2019.icon-1.17,P07-2056,0,0.0995736,"Missing"
2019.icon-1.17,J95-2001,0,0.515605,"Missing"
2019.icon-1.17,J88-1003,0,0.601832,"Missing"
2019.icon-1.17,J94-2001,0,0.783527,"Missing"
2019.icon-1.17,C90-3038,0,0.78336,"Missing"
2019.icon-1.17,W16-5804,0,0.0637624,"Missing"
2019.icon-1.17,H92-1022,0,0.736268,"Missing"
2019.icon-1.17,W16-5811,1,0.837745,"Missing"
2019.icon-1.17,N03-1028,0,0.481393,"Missing"
2020.icon-adapmt.1,W17-3204,0,0.0642154,"Missing"
2020.icon-adapmt.1,Q17-1026,0,0.0179153,"ed on the transformer architecture, and worked on the principle of APE. The description of the all the three systems and the training process for the same is given in Sections 3.1, 3.2 and 3.3 respectively. 3.1 Statistical Machine Translation For designing the model we followed some standard preprocessing steps on 2,50,480 sentence pairs, which are discussed below. 3.1.1 Moses 3.2 Neural Machine Translation In order to develop the NMT framework, we decided to employ a character-level neural machine translation system. The Character based NMT (CNMT) is based on the architecture as described in Lee et al. (2017) and it relies on the sequence-to-sequence (Sutskever et al., 2014) model. We opted for character embedding based NMT for this task because of the benefits it provides over word embedding based NMT. The benefits, as stated in Chung et al. (2016), are Preprocessing The following steps were applied to preprocess and clean the data before using it for training our Statistical machine translation model. We used the NLTK toolkit7 for performing the steps. • Tokenization: Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens. In"
2020.icon-adapmt.1,P02-1040,0,0.108574,"he second parallel corpus was used to tune the model. For this, we fed the SMT system with the English part of the second parallel corpus. In turn, the SMT model gave us the translation of these sentences as output. These outputs were then considered as source sentences to an NMT model and the Hindi part of the second parallel corpus was considered as the target. The architecture of the hybrid model is shown in Figure 3. 4 Evaluation For evaluation purposes, the organizers provided us with a test data of 507 sentences. Upon evaluation, the performance of our systems was calculated using BLEU (Papineni et al., 2002) metric and they are shown in Table 1. 5 6 Conclusion The present paper describes the systems submitted to the translation shared task organized by ICON 2020: 17th International Conference on Natural Language Processing. We participated in the English-Hindi translation task and the training data belonged to the general domain. Three systems, Discussion From Table 1, we can see that SMT performs very well when participating languages belong to a lowresourced setting (Banerjee et al., 2018; Koehn and 4 SMT, NMT, and a hybrid model was trained using these data. The models were pretty straightforw"
2020.icon-adapmt.1,Y18-3013,0,0.0226102,"Missing"
2020.icon-adapmt.1,P06-4018,0,0.0110992,"parallel corpora were provided by the organizers for training the translation systems. Among these, we decided on using the parallel corpus from CVIT-PIB2 and CVIT-MKB3 . Another high-quality corpus from TDIL4 was also used to train our developed systems. The number of parallel sentences in the CVIT-MKB dataset was 5,272, in the CVIT-PIB dataset were 1,95,208, and in the TDIL dataset were 50,000. In total, we were able to arrange for parallel English-Hindi corpora of 2,50,480 sentences. The data was then tokenized to be used for our further experiments. For tokenizing the English data, NLTK5 (Bird, 2006) was used and for tokenizing the Hindi data, Indic NLP Library6 (Kunchukuttan, 2020) was used. 3 • Cleaning: Long sentences (No. of tokens > 80) were removed. 3.1.2 Moses is a statistical machine translation system that allows you to automatically train translation models for any language pair when trained with a large collection of translated texts (parallel corpus). Once the model has been trained, an efficient search algorithm quickly finds the highest probability translation among the exponential number of choices. We trained Moses using 2,50,480 sentence pairs provided by the organizers,"
2020.icon-adapmt.1,P16-1160,0,0.0320307,"For designing the model we followed some standard preprocessing steps on 2,50,480 sentence pairs, which are discussed below. 3.1.1 Moses 3.2 Neural Machine Translation In order to develop the NMT framework, we decided to employ a character-level neural machine translation system. The Character based NMT (CNMT) is based on the architecture as described in Lee et al. (2017) and it relies on the sequence-to-sequence (Sutskever et al., 2014) model. We opted for character embedding based NMT for this task because of the benefits it provides over word embedding based NMT. The benefits, as stated in Chung et al. (2016), are Preprocessing The following steps were applied to preprocess and clean the data before using it for training our Statistical machine translation model. We used the NLTK toolkit7 for performing the steps. • Tokenization: Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens. In our case, these tokens were words, 2 http://preon.iiit.ac.in/ jerin/resources/datasets/pib v0.2.tar http://preon.iiit.ac.in/ jerin/resources/datasets/mkb-v0.tar 4 https://tdil.meity.gov.in/ 5 https://www.nltk.org/ 6 https://github.com/anoopkunc"
2020.icon-adapmt.1,W11-2123,0,0.0265724,"ences (No. of tokens > 80) were removed. 3.1.2 Moses is a statistical machine translation system that allows you to automatically train translation models for any language pair when trained with a large collection of translated texts (parallel corpus). Once the model has been trained, an efficient search algorithm quickly finds the highest probability translation among the exponential number of choices. We trained Moses using 2,50,480 sentence pairs provided by the organizers, with English as the source language and Hindi as the target language. For building the Language Model we used KenLM8 (Heafield, 2011) with 3-grams from the target corpus. Training the Moses statistical MT system resulted in the generation of the Phrase Model and Translation Model that helps in translating between source-target language pairs. Moses scores the phrase in the phrase table with respect to a given source sentence and produces the best-scored phrases as output. Machine Translation After the English-Hindi parallel corpora were compiled, we proceeded to develop our MT systems. As discussed earlier, the first two MT systems were based on SMT and NMT. The third MT system was a hybrid system, using both SMT and NMT, b"
2020.icon-adapmt.1,P07-2045,0,0.0203198,"the fourth spot in the competition leaderboard. 1 • SubTask 1 : To show sentence level Machine translation capability for on General domain. • SubTask 2 : To show sentence level Machine translation capability for on specified domains. We took part in the first sub-task and proceeded with developing translation systems with the help of the provided English-Hindi parallel corpus. Using the provided parallel corpus, we developed three systems. The first two systems was based on Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). For training the SMT system, Moses Toolkit (Koehn et al., 2007) was used. The NMT system was a character based seq-to-seq model, that was trained using Bi-Directional Long Short-Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997). The third system was a hybrid system, that works on the principles of Automated Post Editing (APE). In this model, a transformer (Vaswani et al., 2017) based NMT model was used to post edit the outputs, generated by an SMT based translation system. The rest of the paper is organized as follows. Section 2 describes the parallel corpus that was used to train the above-mentioned translation systems. Section 3 contains the d"
2020.icon-main.7,N03-1017,0,0.0424458,"diting Effort for each sentence. We measure: Manipuri and Mizo are the lingua francas of Manipur and Mizoram, two neighbouring north-eastern states of India. Both Manipuri and Mizo are low resource languages. Limited availability of data in Manipuri and Mizo is one of the main reasons that hamper the development of NLP systems for the language. The training datasets used for training the MT system for the languages are shown in Table 3. On the same EnglishManipuri training dataset, we first examine the performance of MT systems trained with Phrase Based Statistical Machine Translation, PBSMT (Koehn et al., 2003) and the RNNbased NMT with attention mechanism (Bahdanau et al., 2014). The trained MT systems are evaluated on a held-out test dataset of 900 sentences. The result shows a BLEU score of 6.45, (34.7/9.6/3.5/1.5) on the PBSMT system while the NMT system achieved a BLEU score of 0.00, (11.8/0.3/0.0/0.0). For this reason, we use PBSMT systems for both English-Manipuri and English-Mizo MT systems as our parallel NMT results are substantially worse in these low-resource scenarios. To build language models for the target languages, we used the dataset in (Singh and Bandyopadhyay, 2010b) and (Meetei"
2020.icon-main.7,2012.amta-wptp.2,0,0.0297502,"ready annotated, the posteditors could skip the effort of identifying the errors and concentrate only on the highlighted error text segment in the PE process. Focusing on how PE effort changes with the different types of MT errors, the authors reported a weak correlation between PE time and PE effort. The authors also report that no direct dependency was found between the temporal and technical PE effort. Investigating the various types of PE operations for French to English and English to Spanish translation outputs, Popovic et al. (2014) reported lexical edits as the main factor in PE time. Koponen et al. (2012), study the cognitive effort of post-editing MT output based on measuring PE time and HTER (Snover et al., 2006). HTER (Human-targeted Translation Edit Rate), is an automatic metric that computes the minimum number of edits required to change MT output into the post-edited version. The authors reported that the absolute PE time increases with the number of printable keystrokes and sentence length while seconds per word remain relatively constant. Despite the fact that HTER captures the difference between the final translation and raw Related Work Early studies on the correlation between PE eff"
2020.icon-main.7,D19-5224,1,0.874746,"f PE include O’Brien (2005). O’Brien studied the temporal, technical and cognitive effort involved in PE by analyzing keyboard-data using Translog 51 MT, it does not disclose much of the time and keystroke effort required to produce the final result. A similar study is also reported by Moorkens et al. (2015) where the human (or H-) variants of the reference based similarity measure such as BLEU (Papineni et al., 2002) is used to analyze PE effort. Singh and Bandyopadhyay (2010a); Singh (2013) focus on MT for English to Manipuri, Pathak et al. (2019) on English to Mizo and Singh et al. (2017); Meetei et al. (2019b) on English to Hindi, using different MT approaches. But, to date there is no report on PE effort required to turn raw MT output for these target languages into useful translations. To address this gap in the literature, our paper investigates different aspects that impact PE effort and time spend to generate a reasonable target text from MT into Manipuri, Mizo and Hindi. 3 Total Dataset, D Sample Dataset, DP E Tokens 1688440 5500 Table 2: Statistics of our collected dataset and data partitioning. from the CAT tool to analyze PE effort and the time required to generate a reasonable target te"
2020.icon-main.7,P02-1040,0,0.108324,"emain relatively constant. Despite the fact that HTER captures the difference between the final translation and raw Related Work Early studies on the correlation between PE effort and various aspects of PE include O’Brien (2005). O’Brien studied the temporal, technical and cognitive effort involved in PE by analyzing keyboard-data using Translog 51 MT, it does not disclose much of the time and keystroke effort required to produce the final result. A similar study is also reported by Moorkens et al. (2015) where the human (or H-) variants of the reference based similarity measure such as BLEU (Papineni et al., 2002) is used to analyze PE effort. Singh and Bandyopadhyay (2010a); Singh (2013) focus on MT for English to Manipuri, Pathak et al. (2019) on English to Mizo and Singh et al. (2017); Meetei et al. (2019b) on English to Hindi, using different MT approaches. But, to date there is no report on PE effort required to turn raw MT output for these target languages into useful translations. To address this gap in the literature, our paper investigates different aspects that impact PE effort and time spend to generate a reasonable target text from MT into Manipuri, Mizo and Hindi. 3 Total Dataset, D Sample"
2020.icon-main.7,P07-2045,0,0.0165078,"ess3 . The news articles are in English. The complete dataset consists of 3770 news articles from the period July 2011 to October 2019 comprising 64976 sentences. We randomly select 200 sentences (DP E ) for our PE experiment. The statistics of the dataset and data partitioning are shown in Table 2. The dataset is collected using a web-scrapper built in-house. 4.2 Pre-processing Data collected from the web is not free from noise. The pre-processing step includes removal of non-ascii special characters. Each of the news articles in our dataset is split into sentences using the Moses tokenizer (Koehn et al., 2007). Methodology and Experimental Design We use an English language corpus collected from a local daily newspaper as the source text. We normalize the data in a pre-processing step. The normalized text is then machine translated into three different target languages using different MT systems. After post-editing a sample dataset of the machine translated text using a CAT tool, we study the data collected 4.3 Building Machine Translated Target Text We build a machine translated dataset using MT systems resulting in three language pairs, 3 52 https://ifp.co.in/ en-mn monomn en-mz monomz Sentences T"
2020.icon-main.7,W17-5717,0,0.0156915,"and various aspects of PE include O’Brien (2005). O’Brien studied the temporal, technical and cognitive effort involved in PE by analyzing keyboard-data using Translog 51 MT, it does not disclose much of the time and keystroke effort required to produce the final result. A similar study is also reported by Moorkens et al. (2015) where the human (or H-) variants of the reference based similarity measure such as BLEU (Papineni et al., 2002) is used to analyze PE effort. Singh and Bandyopadhyay (2010a); Singh (2013) focus on MT for English to Manipuri, Pathak et al. (2019) on English to Mizo and Singh et al. (2017); Meetei et al. (2019b) on English to Hindi, using different MT approaches. But, to date there is no report on PE effort required to turn raw MT output for these target languages into useful translations. To address this gap in the literature, our paper investigates different aspects that impact PE effort and time spend to generate a reasonable target text from MT into Manipuri, Mizo and Hindi. 3 Total Dataset, D Sample Dataset, DP E Tokens 1688440 5500 Table 2: Statistics of our collected dataset and data partitioning. from the CAT tool to analyze PE effort and the time required to generate a"
2020.icon-main.7,W13-0802,1,0.813607,"the final translation and raw Related Work Early studies on the correlation between PE effort and various aspects of PE include O’Brien (2005). O’Brien studied the temporal, technical and cognitive effort involved in PE by analyzing keyboard-data using Translog 51 MT, it does not disclose much of the time and keystroke effort required to produce the final result. A similar study is also reported by Moorkens et al. (2015) where the human (or H-) variants of the reference based similarity measure such as BLEU (Papineni et al., 2002) is used to analyze PE effort. Singh and Bandyopadhyay (2010a); Singh (2013) focus on MT for English to Manipuri, Pathak et al. (2019) on English to Mizo and Singh et al. (2017); Meetei et al. (2019b) on English to Hindi, using different MT approaches. But, to date there is no report on PE effort required to turn raw MT output for these target languages into useful translations. To address this gap in the literature, our paper investigates different aspects that impact PE effort and time spend to generate a reasonable target text from MT into Manipuri, Mizo and Hindi. 3 Total Dataset, D Sample Dataset, DP E Tokens 1688440 5500 Table 2: Statistics of our collected data"
2020.icon-main.7,W10-3811,1,0.885174,"captures the difference between the final translation and raw Related Work Early studies on the correlation between PE effort and various aspects of PE include O’Brien (2005). O’Brien studied the temporal, technical and cognitive effort involved in PE by analyzing keyboard-data using Translog 51 MT, it does not disclose much of the time and keystroke effort required to produce the final result. A similar study is also reported by Moorkens et al. (2015) where the human (or H-) variants of the reference based similarity measure such as BLEU (Papineni et al., 2002) is used to analyze PE effort. Singh and Bandyopadhyay (2010a); Singh (2013) focus on MT for English to Manipuri, Pathak et al. (2019) on English to Mizo and Singh et al. (2017); Meetei et al. (2019b) on English to Hindi, using different MT approaches. But, to date there is no report on PE effort required to turn raw MT output for these target languages into useful translations. To address this gap in the literature, our paper investigates different aspects that impact PE effort and time spend to generate a reasonable target text from MT into Manipuri, Mizo and Hindi. 3 Total Dataset, D Sample Dataset, DP E Tokens 1688440 5500 Table 2: Statistics of ou"
2020.icon-main.7,W10-3605,1,0.818486,"Missing"
2020.icon-main.7,2006.amta-papers.25,0,0.0603019,"lighted error text segment in the PE process. Focusing on how PE effort changes with the different types of MT errors, the authors reported a weak correlation between PE time and PE effort. The authors also report that no direct dependency was found between the temporal and technical PE effort. Investigating the various types of PE operations for French to English and English to Spanish translation outputs, Popovic et al. (2014) reported lexical edits as the main factor in PE time. Koponen et al. (2012), study the cognitive effort of post-editing MT output based on measuring PE time and HTER (Snover et al., 2006). HTER (Human-targeted Translation Edit Rate), is an automatic metric that computes the minimum number of edits required to change MT output into the post-edited version. The authors reported that the absolute PE time increases with the number of printable keystrokes and sentence length while seconds per word remain relatively constant. Despite the fact that HTER captures the difference between the final translation and raw Related Work Early studies on the correlation between PE effort and various aspects of PE include O’Brien (2005). O’Brien studied the temporal, technical and cognitive effo"
2020.icon-main.7,W14-0314,1,0.836424,"n may not always be sufficient to meet the demand. MT output may sometimes be erroneous and needs to be checked and corrected. The use of translation technology such as MT systems, transla1 2 https://ifp.co.in/ http://censusindia.gov.in 50 Proceedings of the 17th International Conference on Natural Language Processing, pages 50–59 Patna, India, December 18 - 21, 2020. ©2020 NLP Association of India (NLPAI) English SVO Roman Manipuri SOV Bengali Mizo OSV Roman Hindi SOV Devanagari and Choice Network Analysis (CNA). Several studies investigated bi-lingual PE and monolingual PE. In bilingual PE (Zampieri and Vela, 2014) post editors have access to the source text, while in monolingual PE (Nitzke, 2016) MT output is edited without the source text. Zampieri and Vela (2014) studied the use of TMs generated by MT output and their effect on human translation. The authors reported a significant increase in translation speed while using the TM as compared to translating from the scratch. Table 1: Typological Word Order and Script of Languages in the Study. Acronyms: O = Object, S = Subject, V = Verb. target languages Manipuri, Mizo and Hindi resulting in three parallel datasets. The basic word order of the language"
2020.loresmt-1.5,D19-1080,0,0.0219264,"et al., 2015) which pays attention globally and locally to all source words. The RNN based NMT approach is not able to process all the input words parallelly, to solve parallelization transformer-based NMT (Vaswani et al., 2017) is proposed by using a self-attention mechanism. Despite modifying NMT architecture, it needs reasonable parallel training data which is a challenge for low resource language pair translation. Generally, language pairs can be considered as low-resource when training data is less than a million (Kocmi, 2020). For low resource language pair translation, pivot-based NMT (Kim et al., 2019) is an effective approach where an intermediate language is considered as a pivot language (source to pivot and pivot to target). (Johnson et al., 2017) introduced a zero-shot approach to language pair translation without considering the parallel data using multilingual-based NMT. In this paper, we have participated in the LoResMT 2020 shared task of zero-shot NMT approach on RussianHindi pair using the only monolingual corpus and the same has been implemented using MASS-based unsupervised NMT (Song et al., 2019). The reason behind choosing MASS-based unsupervised NMT is that it achieves state"
2020.loresmt-1.5,L18-1548,0,0.0284853,"4.1 Pre-training For the pre-training step, following (Song et al., 2019) we have undertaken the log likelihood objective function (LF ) as shown in Equation 1. Here, s belongs to the source sentence corpus S. And in a particular sentence s, the region from u to v is masked, such that the sentence length remains constant. Dataset Description The LoResMT 2020 shared task organizer (Ojha et al., 2020) provided the Russian-Hindi monolingual dataset of train, valid, and test sets, which is summarized in Table 1. Additionally, we have used external monolingual data set of Hindi (9 GB) from IITB1 (Kunchukuttan et al., 2018; Bojar et al., 2014) and Russian (9GB) from WMT162 . 1 Σs∈S log P (su:v |su:v ; θ) |S| v Y 1 u:v u:v = Σs∈S log P (su:v ; θ). t |s<t , s |S| t=u (1) Here, the seq2seq model learns the parameter θ to compute the conditional probability. t denotes the word position. 4 4.2 LF (θ; S) = System Description Fine Tuning Since, the parallel data is not made available by the LoResMT 2020 organizers for this specific task, we have undertaken the unsupervised approach as followed by (Song et al., 2019). Only the monolingual data is used here. Here simply backtranslation is employed to generate pseudo b"
2020.loresmt-1.5,W14-4012,0,0.129917,"Missing"
2020.loresmt-1.5,J82-2005,0,0.622909,"Missing"
2020.loresmt-1.5,D14-1179,0,0.0425653,"Missing"
2020.loresmt-1.5,D15-1166,0,0.285441,"Missing"
2020.loresmt-1.5,D10-1092,0,0.401661,"Missing"
2020.loresmt-1.5,2020.loresmt-1.4,0,0.0448069,"(Sennrich et al., 2016) and vocabulary creation, we have used the cross-language model (XLM) (Lample and Conneau, 2019) codebase as given in their repository3 . Moses is used for tokenization (Koehn and Hoang, 2010). The MASS (Song et al., 2019) based model leverages encode-decoder framework to develop complete sentences from given fractured pieces of sentences as shown in Figure 1. The model details are further described in Section 4.1 and 4.2, where we have shown the pre-training and fine tuning step respectively. Table 1: Data Statistics provided by the LoResMT 2020 shared task organizer (Ojha et al., 2020) ing bilingual corpus and only utilizing monolingual data achieves BLEU score 37.50, 34.90 on English to French and French to English translation and for English to German and German to English, it attains BLEU score 28.30, 35.20 respectively. Transformer architecture (Vaswani et al., 2017) based MASS implemented in two steps: pre-training on the monolingual data and then fine-tuning with the self-generated back translation data which acts as a pseudo bilingual corpus during the training process. 3 4.1 Pre-training For the pre-training step, following (Song et al., 2019) we have undertaken the"
2020.loresmt-1.5,P02-1040,0,0.110037,"Missing"
2020.loresmt-1.5,P16-1162,0,0.0353931,"on in MT because it deals with many 38 Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages, pages 38–42 c Decmber 04, 2020. 2020 Association for Computational Linguistics X5 Encoder X1 X2 X3 X4 _ Decoder Attention X6 X7 X8 _ _ _ _ _ _ _ Figure 1: The encoder-decoder framework of the MASS model used (as adopted from (Song et al., 2019)) Type Train Valid Test Language Hindi Russian Hindi Russian Hindi Russian Sentences 473,605 154,589 500 500 500 500 Tokens 7,092,870 1,007,029 2,538 2,000 3,150 9,057 step which are discussed in the sub-sections 4.1 and 4.2. For BPE (Sennrich et al., 2016) and vocabulary creation, we have used the cross-language model (XLM) (Lample and Conneau, 2019) codebase as given in their repository3 . Moses is used for tokenization (Koehn and Hoang, 2010). The MASS (Song et al., 2019) based model leverages encode-decoder framework to develop complete sentences from given fractured pieces of sentences as shown in Figure 1. The model details are further described in Section 4.1 and 4.2, where we have shown the pre-training and fine tuning step respectively. Table 1: Data Statistics provided by the LoResMT 2020 shared task organizer (Ojha et al., 2020) ing b"
2020.loresmt-1.9,W14-0135,0,0.119609,". Therefore, to facilitate greater access among the local community, it is necessary to translate the official documents like orders, notices, messages released by the government of India into the regional language of Assam. Moreover, English is a high resource and widely accepted language all over the globe. To establish a better communication in the native languages at the national or international level, MT system for the English-Assamese pair is very much essential. But due to the non availability of a suitable corpus, the MT system for this language pair still remains in beginning stage (Barman et al., 2014; Baruah et al., 2014). In MT, based on the availability of corpus resource, there are two categories of natural languages: high and low resource languages. High resource languages are those languages which are resource-rich languages like English, German, French, and Hindi. On the other hand, low resource languages are resource-poor like many Indian languages especially found in the north-eastern region of India like Assamese, Boro, Khasi, Manipuri, Kokborok, and Mizo. Data scarcity is one of the major problems in MT for low resource language scenarios. For EnglishAssamese, there is a lack of"
2020.loresmt-1.9,P14-1129,0,0.110823,"Missing"
2020.loresmt-1.9,P17-4012,0,0.0792528,"Missing"
2020.loresmt-1.9,P07-2045,0,0.0170968,"of Google OCR helped us to extract the data from a wide range of sources without facing any font related issues. The 4 5 For the baseline systems, we have trained two models: phrase-based SMT (Koehn et al., 2003) and recurrent neural network (RNN) based NMT with attention (Bahdanau et al., 2015) to estimate benchmark translation accuracy for English to Assamese and Assamese to English translation respectively. We have used the developed dataset EnAsCorp1.0 and English monolingual data about 3 million sentences from WMT166 . 3.1 SMT Setup We have trained the phrase-based SMT using the Moses7 (Koehn et al., 2007) toolkit. GIZA++ and IRSTLM (Federico et al., 2008) are used to generate phrase pairs and language model following default settings. 6 http://www.statmt.org/wmt16/ translation-task.html 7 http://www.statmt.org/moses/ https://scrapy.org/ https://cloud.google.com/vision/ 66 Translation English to Assamese Assamese to English System SMT NMT SMT NMT BLEU 3.43 5.55 4.54 7.72 Acknowledgement We would like to thank Center for Natural Language Processing (CNLP) and Department of Computer Science and Engineering at National Institute of Technology, Silchar, India for providing the requisite support and"
2020.loresmt-1.9,N03-1017,0,0.0293447,"Missing"
2020.loresmt-1.9,P02-1040,0,0.110405,"Missing"
2020.loresmt-1.9,D14-1162,0,0.0823657,"Missing"
2020.loresmt-1.9,P16-1009,0,0.0523745,"like parallel corpus, monolingual corpus for English and Assamese are available in various online sources, but since Assamese is a low resource language the digitized monolingual corpus is difficult to find. The standard English monolingual data are available from the shared task of MT2 , but there is lack of standard Assamese monolingual data. This is an another challenge to prepare an Assamese monolingual corpora from available online sources. The monolingual corpus plays a vital role in improving the translation quality for both SMT (Koehn, 2010) and NMT in low resource language scenarios (Sennrich et al., 2016). গীজৰাঘৰৰ গীজৰাঘৰ Stem ৰ Singular Sufﬁx Marker Figure 2: Example of an Assamese Morpheme phological rich language unlike English. Figure 2 presents an example of Assamese morpheme. There are total of 52 letters in the Assamese alphabet comprising of 41 consonants and 11 vowels. Assamese script and numerals are very different from English and identical to the Bengali language except for the two letters, ৰ (ro) and ৱ (vo) (Mahanta, 2012) In this paper, our focus is to create an English-Assamese corpus that we hope that it will be of benefit to the research community. 2 2.1 Corpus Role of Corp"
2020.wat-1.11,D17-1105,0,0.077803,"edings of the 7th Workshop on Asian Translation, pages 109–113 c December 4, 2020. 2020 Association for Computational Linguistics Type Train Test (Evaluation Set) Test (Challenge Set) Validation Name Text Data (English - Hindi) Image Data Text Data (English - Hindi) Image Data Text Data (English - Hindi) Image Data Text Data (English - Hindi) Image Data Instances/Items 28,927 28,927 1,595 1,595 1,400 1,400 998 998 Tokens (English / Hindi) 143,164 / 145,448 7,853 / 7,852 8,186 / 8,639 4,922 / 4,978 Table 1: Parallel Data Statistics (Nakazawa et al., 2020; Parida et al., 2019). NMT settings of (Calixto and Liu, 2017), achieves BLEU score 24.2 for Hindi to English translation. Moreover, in the WAT2019 multi-modal translation task of English to Hindi, (Sanayai Meetei et al., 2019) based on recurrent neural network (RNN) (Calixto and Liu, 2017) achieves BLEU score of 12.58, 28.45 for the challenge and evaluation test respectively. And, on the same task of WAT2019, we have achieved the highest BLEU score of 20.37, 40.55 for the challenge and evaluation test respectively (Laskar et al., 2019c). We have used RNN encoder and doubly-attentive RNN decoder based model (Calixto and Liu, 2017; Calixto et al., 2017)."
2020.wat-1.11,P17-1175,0,0.103304,"Missing"
2020.wat-1.11,W18-3405,0,0.266555,"ation Understudy (BLEU) score of 33.57, Rankbased Intuitive Bilingual Evaluation Score (RIBES) 0.754141, Adequacy-Fluency Metrics (AMFM) score 0.787320 and for evaluation test data, BLEU, RIBES, and, AMFM score of 40.51, 0.803208, and 0.820980 for English to Hindi translation respectively. 1 2 Introduction Multi-modal NMT aims to draw information from the input data from different modalities like text, image, and audio. By combining information from Related Works The literature survey finds out very limited existing works on English-Hindi language pair translation using multi-modal NMT (Dutta Chowdhury et al., 2018; Sanayai Meetei et al., 2019; Laskar et al., 2019c). The work by (Dutta Chowdhury et al., 2018) uses synthetic data, following multi-modal 109 Proceedings of the 7th Workshop on Asian Translation, pages 109–113 c December 4, 2020. 2020 Association for Computational Linguistics Type Train Test (Evaluation Set) Test (Challenge Set) Validation Name Text Data (English - Hindi) Image Data Text Data (English - Hindi) Image Data Text Data (English - Hindi) Image Data Text Data (English - Hindi) Image Data Instances/Items 28,927 28,927 1,595 1,595 1,400 1,400 998 998 Tokens (English / Hindi) 143,164"
2020.wat-1.11,D10-1092,0,0.118977,"lti-modal translation 3 http://lotus.kuee.kyoto-u.ac.jp/WAT/ evaluation/index.html 111 task for English to Hindi and our system’s results are presented in Table 3. Our team name is CNLPNITS and participated in text-only and multi-modal submission track of the same task. In text-only translation submission track, a total of four teams participated for both challenges and evaluation test data and for multi-modal translation submission track, only our team participated. The submitted predicted translations are evaluated via standard evaluation metrics namely, BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). From the Table 2, it is observed that our multi-modal NMT system obtained higher scores on the ground of BLEU, RIBES, AMFM than our text-only NMT system. This reasons about combination of visual and textual features in multi-modal NMT shows better performance than only textual features based NMT. Moreover, our systems used pretrained word embedding of monolingual data and adopted BRNN encoder that reasons about outperform previous work (Laskar et al., 2019c) at WAT2019. Figure 1 and 2 present best and worst performance our systems outputs, where included Google"
2020.wat-1.11,P17-4012,0,0.0570098,"moved one image (id:2326837) from the image dataset since it is not available in train parallel text data. Therefore, the total number of parallel sentences and image become 28,927 in Hindi Visual Genome 1.1 dataset as summarized in Table 1. Additionally, we have used English-Hindi parallel corpus and monolingual data of Hindi from Tokens 107,597,494 44,949,045 1,832,008,594 743,723,731 Table 2: Monolingual Data Statistics collected from IITB and WMT16. IITB1 (Kunchukuttan et al., 2018) and English monolingual data from WMT162 as shown in Table 2. 4 System Description We have used OpenNMT-py (Klein et al., 2017) to setup our multi-modal NMT and text-only NMT systems. The key process of the operations include data preprocessing, system training to generate an optimum trained model, and then obtained trained model is used in the testing/translation process to predict translation on the given unseen data. 4.1 3 Sentences Data Preprocessing For multi-modal translation, pre-trained CNN with VGG19 is used for the extraction of global and local features from the provided image dataset. The pre-trained CNN with VGG19 is publicly available in OpenNMT-py. In the text-only and multi-modal task, we have used Glo"
2020.wat-1.11,L18-1548,0,0.321206,"Missing"
2020.wat-1.11,R19-1052,0,0.17876,"s issues like variable-length phrases using sequence to sequence learning, the problem of long term dependency using Long Short Term Memory (LSTM) (Sutskever et al., 2014). However, in the case of very long sentences, the basic encoderdecoder architecture is unable to encode all the information. To resolve this issue, the attention mechanism is proposed which pays attention to all source words locally as well as globally (Bahdanau et al., 2015; Luong et al., 2015). For Indian language translation, attention-based NMT yields remarkable performance (Pathak and Pakray, 2018; Pathak et al., 2018; Laskar et al., 2019b,a). Besides, without modifying the system architecture, NMT performance can be improved using monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), which is very effective in the case of low resource language translation. This paper investigates English to Hindi translation using the multimodal concept with monolingual data to improve the translation quality at the WAT2020 translation task. Machine translation (MT) focuses on the automatic translation of text from one natural language to another natural language. Neural machine translation (NMT) achieves state-of-theart results in"
2020.wat-1.11,W19-5427,1,0.862414,"s issues like variable-length phrases using sequence to sequence learning, the problem of long term dependency using Long Short Term Memory (LSTM) (Sutskever et al., 2014). However, in the case of very long sentences, the basic encoderdecoder architecture is unable to encode all the information. To resolve this issue, the attention mechanism is proposed which pays attention to all source words locally as well as globally (Bahdanau et al., 2015; Luong et al., 2015). For Indian language translation, attention-based NMT yields remarkable performance (Pathak and Pakray, 2018; Pathak et al., 2018; Laskar et al., 2019b,a). Besides, without modifying the system architecture, NMT performance can be improved using monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), which is very effective in the case of low resource language translation. This paper investigates English to Hindi translation using the multimodal concept with monolingual data to improve the translation quality at the WAT2020 translation task. Machine translation (MT) focuses on the automatic translation of text from one natural language to another natural language. Neural machine translation (NMT) achieves state-of-theart results in"
2020.wat-1.11,D19-5205,1,0.431686,"Missing"
2020.wat-1.11,D15-1166,0,0.112129,"text-only translation. For text-only based NMT, encoder-decoder architecture is a widely used technique in the MT community. Because it handles various issues like variable-length phrases using sequence to sequence learning, the problem of long term dependency using Long Short Term Memory (LSTM) (Sutskever et al., 2014). However, in the case of very long sentences, the basic encoderdecoder architecture is unable to encode all the information. To resolve this issue, the attention mechanism is proposed which pays attention to all source words locally as well as globally (Bahdanau et al., 2015; Luong et al., 2015). For Indian language translation, attention-based NMT yields remarkable performance (Pathak and Pakray, 2018; Pathak et al., 2018; Laskar et al., 2019b,a). Besides, without modifying the system architecture, NMT performance can be improved using monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), which is very effective in the case of low resource language translation. This paper investigates English to Hindi translation using the multimodal concept with monolingual data to improve the translation quality at the WAT2020 translation task. Machine translation (MT) focuses on the aut"
2020.wat-1.11,P02-1040,0,0.107016,"ed the evaluation result3 of multi-modal translation 3 http://lotus.kuee.kyoto-u.ac.jp/WAT/ evaluation/index.html 111 task for English to Hindi and our system’s results are presented in Table 3. Our team name is CNLPNITS and participated in text-only and multi-modal submission track of the same task. In text-only translation submission track, a total of four teams participated for both challenges and evaluation test data and for multi-modal translation submission track, only our team participated. The submitted predicted translations are evaluated via standard evaluation metrics namely, BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). From the Table 2, it is observed that our multi-modal NMT system obtained higher scores on the ground of BLEU, RIBES, AMFM than our text-only NMT system. This reasons about combination of visual and textual features in multi-modal NMT shows better performance than only textual features based NMT. Moreover, our systems used pretrained word embedding of monolingual data and adopted BRNN encoder that reasons about outperform previous work (Laskar et al., 2019c) at WAT2019. Figure 1 and 2 present best and worst performance our systems"
2020.wat-1.11,D14-1162,0,0.0891389,"setup our multi-modal NMT and text-only NMT systems. The key process of the operations include data preprocessing, system training to generate an optimum trained model, and then obtained trained model is used in the testing/translation process to predict translation on the given unseen data. 4.1 3 Sentences Data Preprocessing For multi-modal translation, pre-trained CNN with VGG19 is used for the extraction of global and local features from the provided image dataset. The pre-trained CNN with VGG19 is publicly available in OpenNMT-py. In the text-only and multi-modal task, we have used GloVe (Pennington et al., 2014) to pretrain on monolingual data of English-Hindi and generated global vectors of word embedding. The OpenNMT-py tool is used to create a vocabulary size of 5004 for both source and target sentences. We have not used any word-segmentation technique. 110 1 http://www.cfilt.iitb.ac.in/iitb_ parallel/ 2 http://www.statmt.org/wmt16/ translation-task.html Our System Text-only NMT Multi-modal NMT Test Set Challenge Evaluation Challenge Evaluation BLEU 27.75 38.84 33.57 40.51 RIBES 0.714980 0.793416 0.754141 0.803208 AMFM 0.750320 0.804250 0.787320 0.820980 Table 3: Our system’s results on English to"
2020.wat-1.11,D19-5224,1,0.75883,"33.57, Rankbased Intuitive Bilingual Evaluation Score (RIBES) 0.754141, Adequacy-Fluency Metrics (AMFM) score 0.787320 and for evaluation test data, BLEU, RIBES, and, AMFM score of 40.51, 0.803208, and 0.820980 for English to Hindi translation respectively. 1 2 Introduction Multi-modal NMT aims to draw information from the input data from different modalities like text, image, and audio. By combining information from Related Works The literature survey finds out very limited existing works on English-Hindi language pair translation using multi-modal NMT (Dutta Chowdhury et al., 2018; Sanayai Meetei et al., 2019; Laskar et al., 2019c). The work by (Dutta Chowdhury et al., 2018) uses synthetic data, following multi-modal 109 Proceedings of the 7th Workshop on Asian Translation, pages 109–113 c December 4, 2020. 2020 Association for Computational Linguistics Type Train Test (Evaluation Set) Test (Challenge Set) Validation Name Text Data (English - Hindi) Image Data Text Data (English - Hindi) Image Data Text Data (English - Hindi) Image Data Text Data (English - Hindi) Image Data Instances/Items 28,927 28,927 1,595 1,595 1,400 1,400 998 998 Tokens (English / Hindi) 143,164 / 145,448 7,853 / 7,852 8,186"
2020.wat-1.11,P16-1009,0,0.391753,"Memory (LSTM) (Sutskever et al., 2014). However, in the case of very long sentences, the basic encoderdecoder architecture is unable to encode all the information. To resolve this issue, the attention mechanism is proposed which pays attention to all source words locally as well as globally (Bahdanau et al., 2015; Luong et al., 2015). For Indian language translation, attention-based NMT yields remarkable performance (Pathak and Pakray, 2018; Pathak et al., 2018; Laskar et al., 2019b,a). Besides, without modifying the system architecture, NMT performance can be improved using monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), which is very effective in the case of low resource language translation. This paper investigates English to Hindi translation using the multimodal concept with monolingual data to improve the translation quality at the WAT2020 translation task. Machine translation (MT) focuses on the automatic translation of text from one natural language to another natural language. Neural machine translation (NMT) achieves state-of-theart results in the task of machine translation because of utilizing advanced deep learning techniques and handles issues like long-term dependency, an"
2020.wat-1.11,W16-2363,0,0.272158,"Missing"
2020.wat-1.11,D16-1160,0,0.0670709,"r et al., 2014). However, in the case of very long sentences, the basic encoderdecoder architecture is unable to encode all the information. To resolve this issue, the attention mechanism is proposed which pays attention to all source words locally as well as globally (Bahdanau et al., 2015; Luong et al., 2015). For Indian language translation, attention-based NMT yields remarkable performance (Pathak and Pakray, 2018; Pathak et al., 2018; Laskar et al., 2019b,a). Besides, without modifying the system architecture, NMT performance can be improved using monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), which is very effective in the case of low resource language translation. This paper investigates English to Hindi translation using the multimodal concept with monolingual data to improve the translation quality at the WAT2020 translation task. Machine translation (MT) focuses on the automatic translation of text from one natural language to another natural language. Neural machine translation (NMT) achieves state-of-theart results in the task of machine translation because of utilizing advanced deep learning techniques and handles issues like long-term dependency, and context-analysis. Nev"
2020.wmt-1.135,D19-1632,0,0.0219779,"Missing"
2020.wmt-1.135,P17-1042,0,0.0249771,"et al., 2014; Bahdanau et al., 2014) has become the de-facto MT system in recent times achieving near human level translation quality for many language pair however at the cost of millions of bi-text data. Unfortunately, bi-text data for many languages is scarce or non-existent. Unsupervised MT (Lample et al., 2018a; Artetxe et al., 2018b) is one of the techniques to handle the bi-text unavailability by exploiting monolingual data (Sennrich et al., 2016a). Primitive unsupervised MT first maps the monolingual data into a common cross-lingual shared vector embedding space (Conneau et al., 2017; Artetxe et al., 2017) and infer a bilingual dictionary from this shared space using adversarial training (Lample et al., 2018a) or through self learning (Artetxe et al., 2018b) and further improve the model through a combination of de-noising auto-encoder and iterative or on-the-fly back-translation. Subsequently, this principle has been applied in SMT (Lample et al., 2018b; Artetxe et al., 2018a) or a combination of NMT and SMT (Marie and Fujita, 2018; Ren et al., 2019) to further improve the unsupervised MT. However, in this work, we follow a newer approach of cross-lingual language model pretraining (Lample and"
2020.wmt-1.135,D18-1399,0,0.113851,"ata (Mhsb )and then train a pseudo-supervised 0 model using {Mde ,Mhsb } from scratch. The remaining of the paper is arranged in following manner: Section 2 gives a brief background of an unsupervised MT. Section 3 describes the Background NMT (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014) has become the de-facto MT system in recent times achieving near human level translation quality for many language pair however at the cost of millions of bi-text data. Unfortunately, bi-text data for many languages is scarce or non-existent. Unsupervised MT (Lample et al., 2018a; Artetxe et al., 2018b) is one of the techniques to handle the bi-text unavailability by exploiting monolingual data (Sennrich et al., 2016a). Primitive unsupervised MT first maps the monolingual data into a common cross-lingual shared vector embedding space (Conneau et al., 2017; Artetxe et al., 2017) and infer a bilingual dictionary from this shared space using adversarial training (Lample et al., 2018a) or through self learning (Artetxe et al., 2018b) and further improve the model through a combination of de-noising auto-encoder and iterative or on-the-fly back-translation. Subsequently, this principle has been"
2020.wmt-1.135,D13-1176,0,0.0173871,"ransformer (Vaswani et al., 2017) based encoder and decoder model using masked sequence to sequence (MASS) pre-training (Song et al., 2019) and fine-tune using the back-translation (Sennrich et al., 2016a) loss. The final model trained using MASS objective is then used to translate the source side (Mde ) monolingual data into a synthetic target 0 side data (Mhsb )and then train a pseudo-supervised 0 model using {Mde ,Mhsb } from scratch. The remaining of the paper is arranged in following manner: Section 2 gives a brief background of an unsupervised MT. Section 3 describes the Background NMT (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014) has become the de-facto MT system in recent times achieving near human level translation quality for many language pair however at the cost of millions of bi-text data. Unfortunately, bi-text data for many languages is scarce or non-existent. Unsupervised MT (Lample et al., 2018a; Artetxe et al., 2018b) is one of the techniques to handle the bi-text unavailability by exploiting monolingual data (Sennrich et al., 2016a). Primitive unsupervised MT first maps the monolingual data into a common cross-lingual shared vector embedding space (Conneau et al.,"
2020.wmt-1.135,P07-2045,0,0.0100465,"qrt, beta1=0.9,beta2=0.98,lr=0.0001 --word mass 0.5 --min len 5 5M 756.3 K 2K 2K Table 1: Statistics of the monolingual and the dev/test set. 3.1 Data Description We use a randomly sampled 5M monolingual corpus for German side from News Crawl1 dataset, while we use all the available monolingual data2 and the parallel side3 of Upper Sorbian4 as the combined monolingual data for the same and summing up 756,271 number of sentences. For tuning and evaluation5 , we use the provided devtest6 data with 2000 sentences for both the dev and test files as shown in Table 1. 3.2 Preprocessing We use Moses(Koehn et al., 2007) toolkit for preprocessing the data. The corpus underwent removal of non-printing characters and tokenization. For the Upper Sorbian, we used Czech(cs) language code for tokenization as Upper Sorbian(hsb) language code is unavailable in Moses toolkit7 and considering the relatedness of these languages8 . The above preprocessing is used by MASS pretrain and MASS finetune models while the pseudosupervised model uses the raw data and learns a Sentencepiece BPE. The details are described in Section 4.2. 4 UNMT System Our UNMT system is a pipeline of encoderdecoder pretraining and fine-tuning using"
2020.wmt-1.135,D18-2012,0,0.0119089,"betas ’(0.9, 0.98)’ --share-all-embeddings Table 3: MASS finetuning parameters model to train a forward pseudo-supervised model. In our case, we first generate a synthetic data 0 (Mhsb ) from the source monolingual data (Mde ) using beam search decoding with a beam size of 10 from the MASS fine tuned model. Unlike Marie et al. (2019) where back translation was applied, we use forward translation from the source side monolingual (He et al., 2020) data to generate synthetic data. The synthetic data is detokenized, and we learn a joint subword BPE from the raw Mde 0 and Mhsb using Sentencepiece (Kudo and Richardson, 2018) and limit the shared vocabulary to 10 K units. Noisy Pseudo-Supervised NMT: We add perturbations or noise, specifically we apply word dropout, word shuffle and word blank to our synthetic data. This kind of perturbation is found to be effective for overcoming the local minima by enforcing local smoothness (He et al., 2020; Shen et al., 2019). We train our pseudo-supervised NMT in a pseudo self-training approach by leveraging the source side monolingual data. This self-training is partial in the sense that we only use the pseudoparallel data which lacks any sort of real labelled data for a sin"
2020.wmt-1.135,J82-2005,0,0.710817,"Missing"
2020.wmt-1.135,W19-5330,0,0.0252896,"ta is used to pretrain the encoder and decoder jointly by the cross lingual MASS objective and the training is done for 100 epochs. The parameters for the MASS pretraining is shown in Table 2. MASS Fine-tuning: The pretrained model is capable to generate translations but it is merely a copy task. So, in order to make the model more robust, it is further fine-tuned using the loss objective of back-translation. The fine-tuning is halted after the 10th epoch before being converged due to resource limitation. The parameters for fine-tuning is listed in Table 3. 4.2 Pseudo-Supervised NMT We follow Marie et al. (2019) style of using the pseudo-parallel data generated from a previous 1140 9 https://github.com/microsoft/MASS --bt steps ’de-hsb-de,hsb-de-hsb’ --encoder only false --emb dim 1024 --n layers 6 --n heads 8 --dropout 0.1 --attention dropout 0.1 --gelu activation true --tokens per batch 2000 --optimizer adam inverse sqrt, beta1=0.9,beta2=0.98,lr=0.0001 --eval bleu true --encoder-normalize-before --decoder-normalize-before --dropout 0.3 --relu-dropout 0.3 --attention-dropout 0.3 --label-smoothing 0.2 --criterion label smoothed cross entropy --weight-decay 0.0001 --lr-scheduler inverse sqrt --min-lr"
2020.wmt-1.135,N19-4009,0,0.0132777,"add perturbations or noise, specifically we apply word dropout, word shuffle and word blank to our synthetic data. This kind of perturbation is found to be effective for overcoming the local minima by enforcing local smoothness (He et al., 2020; Shen et al., 2019). We train our pseudo-supervised NMT in a pseudo self-training approach by leveraging the source side monolingual data. This self-training is partial in the sense that we only use the pseudoparallel data which lacks any sort of real labelled data for a single iteration. The pseudo-supervised NMT is trained from scratch using Fairseq (Ott et al., 2019) toolkit10 i.e, we do not use the previous models weights rather we apply random weight initialization for our new model. The model is trained for 300 K update steps. We follow Guzm´an et al. (2019) style transformer architecture of 5 encoder and decoder layers, 512 embedding dimension, the feed-forward hidden dimension is 2048 with 4 multi-head attentions11 . The rest of the parameters are listed in Table 4. We 10 https://github.com/pytorch/fairseq We have used 4 attention heads instead of 8 as in Guzm´an et al. (2019) 11 Table 4: Pseudo-supervised NMT training parameters make our primary sub"
2020.wmt-1.135,P16-1009,0,0.225646,"chnology, Silchar, India (NITS-CNLP) in the WMT 2020 shared task for Unsupervised and Very Low Resource machine translation for German and Upper-Sorbian language pair. Specifically, we made our primary submission for the unsupervised task in de → hsb direction. We use the data provided by the organisers only i.e, in a constrained manner. Our unsupervised neural machine translation (UNMT) system first pre-trains a transformer (Vaswani et al., 2017) based encoder and decoder model using masked sequence to sequence (MASS) pre-training (Song et al., 2019) and fine-tune using the back-translation (Sennrich et al., 2016a) loss. The final model trained using MASS objective is then used to translate the source side (Mde ) monolingual data into a synthetic target 0 side data (Mhsb )and then train a pseudo-supervised 0 model using {Mde ,Mhsb } from scratch. The remaining of the paper is arranged in following manner: Section 2 gives a brief background of an unsupervised MT. Section 3 describes the Background NMT (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014) has become the de-facto MT system in recent times achieving near human level translation quality for many language pair however at"
2020.wmt-1.135,P16-1162,0,0.104497,"chnology, Silchar, India (NITS-CNLP) in the WMT 2020 shared task for Unsupervised and Very Low Resource machine translation for German and Upper-Sorbian language pair. Specifically, we made our primary submission for the unsupervised task in de → hsb direction. We use the data provided by the organisers only i.e, in a constrained manner. Our unsupervised neural machine translation (UNMT) system first pre-trains a transformer (Vaswani et al., 2017) based encoder and decoder model using masked sequence to sequence (MASS) pre-training (Song et al., 2019) and fine-tune using the back-translation (Sennrich et al., 2016a) loss. The final model trained using MASS objective is then used to translate the source side (Mde ) monolingual data into a synthetic target 0 side data (Mhsb )and then train a pseudo-supervised 0 model using {Mde ,Mhsb } from scratch. The remaining of the paper is arranged in following manner: Section 2 gives a brief background of an unsupervised MT. Section 3 describes the Background NMT (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014) has become the de-facto MT system in recent times achieving near human level translation quality for many language pair however at"
2020.wmt-1.135,2006.amta-papers.25,0,0.0703228,"ture of 5 encoder and decoder layers, 512 embedding dimension, the feed-forward hidden dimension is 2048 with 4 multi-head attentions11 . The rest of the parameters are listed in Table 4. We 10 https://github.com/pytorch/fairseq We have used 4 attention heads instead of 8 as in Guzm´an et al. (2019) 11 Table 4: Pseudo-supervised NMT training parameters make our primary submission of the test source generated using a beam search decoding with beam size of 5 and a length penalty of 1.2. 5 Result The official automatic evaluation uses the the following metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), BEER (Stanojevi´c and Sima’an, 2014), and CharactTER (Wang et al., 2016). Our primary submission (NITS-CNLP), the pseudo-supervised NMT achieves a cased BLEU of 15.4 and 15.8 as the uncased BLEU score on the newstest2020 blind-test data. The scores are reported in Table 5. We also present the sample input-output of our primary system (NITS-CNLP) from two randomly selected test sentences from the matrix12 in Table 6. We also report the Sacrebleu score of our various settings with the released test set (non blind test) in Table 7. 6 Conclusion We report here the system description for our subm"
2020.wmt-1.135,W14-3354,0,0.0631555,"Missing"
2020.wmt-1.135,W16-2342,0,0.0135889,"ard hidden dimension is 2048 with 4 multi-head attentions11 . The rest of the parameters are listed in Table 4. We 10 https://github.com/pytorch/fairseq We have used 4 attention heads instead of 8 as in Guzm´an et al. (2019) 11 Table 4: Pseudo-supervised NMT training parameters make our primary submission of the test source generated using a beam search decoding with beam size of 5 and a length penalty of 1.2. 5 Result The official automatic evaluation uses the the following metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), BEER (Stanojevi´c and Sima’an, 2014), and CharactTER (Wang et al., 2016). Our primary submission (NITS-CNLP), the pseudo-supervised NMT achieves a cased BLEU of 15.4 and 15.8 as the uncased BLEU score on the newstest2020 blind-test data. The scores are reported in Table 5. We also present the sample input-output of our primary system (NITS-CNLP) from two randomly selected test sentences from the matrix12 in Table 6. We also report the Sacrebleu score of our various settings with the released test set (non blind test) in Table 7. 6 Conclusion We report here the system description for our submission to the WMT 2020 shared task of Unsupervised MT for German-Upper Sor"
2020.wmt-1.135,P02-1040,0,0.12041,"9) style transformer architecture of 5 encoder and decoder layers, 512 embedding dimension, the feed-forward hidden dimension is 2048 with 4 multi-head attentions11 . The rest of the parameters are listed in Table 4. We 10 https://github.com/pytorch/fairseq We have used 4 attention heads instead of 8 as in Guzm´an et al. (2019) 11 Table 4: Pseudo-supervised NMT training parameters make our primary submission of the test source generated using a beam search decoding with beam size of 5 and a length penalty of 1.2. 5 Result The official automatic evaluation uses the the following metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), BEER (Stanojevi´c and Sima’an, 2014), and CharactTER (Wang et al., 2016). Our primary submission (NITS-CNLP), the pseudo-supervised NMT achieves a cased BLEU of 15.4 and 15.8 as the uncased BLEU score on the newstest2020 blind-test data. The scores are reported in Table 5. We also present the sample input-output of our primary system (NITS-CNLP) from two randomly selected test sentences from the matrix12 in Table 6. We also report the Sacrebleu score of our various settings with the released test set (non blind test) in Table 7. 6 Conclusion We report here the syst"
2020.wmt-1.45,J90-2002,0,0.898966,"ed and corpus-based, where rule-based is based on a pre-defined rules on the concerned languages and corpus-based finds a generalized approach after being trained on a large corpus. MT switches from rule-based approach to the corpus-based which blots out the need for linguistic expertise. In the corpus-based approach, example-based machine translation (EBMT), statistical machine translation (SMT) and NMT techniques are available. The disadvantage of EBMT is that even though the corpus is large, all examples are not covered. To mitigate the issues of the contemporary approach SMT is introduced Brown et al. (1990); Koehn (2010). The SMT based system makes an assumption based on probability scores of the translated text. And hence, the ranking is done. SMT also faces many issues like system complexity, long term dependency problem, context-analyzing inability, word-alignment and the rare word problem. The inefficiency of SMT leads to the development of the NMT Devlin et al. (2014). But like SMT, the NMT based model also suffers the requirement of sufficient training parallel corpus, which is a challenge in the case of low resource languages. For this reason, there is a demand for direct translation amon"
2020.wmt-1.45,W18-6315,0,0.11068,"d model also suffers the requirement of sufficient training parallel corpus, which is a challenge in the case of low resource languages. For this reason, there is a demand for direct translation among similar language pairs by utilizing similarity features and monolingual data, so that less availability of the parallel data does not pose a challenge. However, the NMT technique achieves state-of-the-art approach in MT because of its transformer model Vaswani et al. (2017). For low resource language pair translation, NMT models have been improved with monolingual corpus Sennrich et al. (2016b); Burlot and Yvon (2018); Wu et al. (2019). In this work, we have adopted cross-lingual language model (XLM) Conneau and Lample (2019) to implement an NMT model for Hindi-Marathi similar language translation task because XLM shows significant improvements for low-resource languages by utilizing the monolingual corpora. 2 Related Work Hindi-Marathi translation lacks background work. However, similar work is found on Hindi-Nepali pair at WMT19 shared task of similar language translation Laskar et al. (2019). The literature survey mainly focuses on NMT for low resource language pairs since NMT outperforms conventional S"
2020.wmt-1.45,P14-1129,0,0.079543,"Missing"
2020.wmt-1.45,R19-1052,0,0.180629,"ource language pair translation, NMT models have been improved with monolingual corpus Sennrich et al. (2016b); Burlot and Yvon (2018); Wu et al. (2019). In this work, we have adopted cross-lingual language model (XLM) Conneau and Lample (2019) to implement an NMT model for Hindi-Marathi similar language translation task because XLM shows significant improvements for low-resource languages by utilizing the monolingual corpora. 2 Related Work Hindi-Marathi translation lacks background work. However, similar work is found on Hindi-Nepali pair at WMT19 shared task of similar language translation Laskar et al. (2019). The literature survey mainly focuses on NMT for low resource language pairs since NMT outperforms conventional SMT on low resource pairs like English to Mizo, English to Hindi, English to Punjabi, and English to Tamil Pathak et al. (2018); Pathak and Pakray (2018); Laskar et al. (2019). It is noticed that train396 Proceedings of the 5th Conference on Machine Translation (WMT), pages 396–401 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Masked Language Modeling (MLM) मुझे अचछा हुई [/s] Transformer Token embeddings [/s] [MASK]  से [MASK] बहुत खुशी [MASK] [/"
2020.wmt-1.45,W19-5427,1,0.529729,"ource language pair translation, NMT models have been improved with monolingual corpus Sennrich et al. (2016b); Burlot and Yvon (2018); Wu et al. (2019). In this work, we have adopted cross-lingual language model (XLM) Conneau and Lample (2019) to implement an NMT model for Hindi-Marathi similar language translation task because XLM shows significant improvements for low-resource languages by utilizing the monolingual corpora. 2 Related Work Hindi-Marathi translation lacks background work. However, similar work is found on Hindi-Nepali pair at WMT19 shared task of similar language translation Laskar et al. (2019). The literature survey mainly focuses on NMT for low resource language pairs since NMT outperforms conventional SMT on low resource pairs like English to Mizo, English to Hindi, English to Punjabi, and English to Tamil Pathak et al. (2018); Pathak and Pakray (2018); Laskar et al. (2019). It is noticed that train396 Proceedings of the 5th Conference on Machine Translation (WMT), pages 396–401 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Masked Language Modeling (MLM) मुझे अचछा हुई [/s] Transformer Token embeddings [/s] [MASK]  से [MASK] बहुत खुशी [MASK] [/"
2020.wmt-1.45,P02-1040,0,0.106729,"wing settings of Conneau and Lample (2019), attention dropout was set to 0.1, gelu activation was used. Also, adam was used as an optimizer with an initial learning rate of 0.0001. Rest of the parameters are same as used by Conneau and Lample (2019) in their experiments and as given in their GitHub repository3 . 6 Result and Analysis The WMT20 organizer declared result for the shared task of similar language translation on Hindi to Marathi4 and Marathi to Hindi5 and the results of our system’s is reported in Table 3. Our team’s name is NITS-CNLP. The participated systems are evaluated by BLEU Papineni et al. (2002), RIBES Isozaki et al. (2010) and TER Snover et al. (2006) and the tracks are ranked by BLEU score. A total of 21 teams participated in Hindi to Marathi translation track and 23 teams for Marathi to Hindi translation track including both primary and contrastive system types. Our system’s rank is 10 with BLEU score 11.59 for Hindi to Marathi translation https://github.com/glample/fastBPE 398 2 The model was trained on a Quadro P200 GPU having 5GB of GPU RAM 3 https://github.com/facebookresearch/XLM 4 http://mzampieri.com/workshops/wmt/HI-MR. pdf 5 http://mzampieri.com/workshops/wmt/MR-HI. pdf T"
2020.wmt-1.45,D17-1039,0,0.0309314,"Missing"
2020.wmt-1.45,W16-2323,0,0.365918,"ut like SMT, the NMT based model also suffers the requirement of sufficient training parallel corpus, which is a challenge in the case of low resource languages. For this reason, there is a demand for direct translation among similar language pairs by utilizing similarity features and monolingual data, so that less availability of the parallel data does not pose a challenge. However, the NMT technique achieves state-of-the-art approach in MT because of its transformer model Vaswani et al. (2017). For low resource language pair translation, NMT models have been improved with monolingual corpus Sennrich et al. (2016b); Burlot and Yvon (2018); Wu et al. (2019). In this work, we have adopted cross-lingual language model (XLM) Conneau and Lample (2019) to implement an NMT model for Hindi-Marathi similar language translation task because XLM shows significant improvements for low-resource languages by utilizing the monolingual corpora. 2 Related Work Hindi-Marathi translation lacks background work. However, similar work is found on Hindi-Nepali pair at WMT19 shared task of similar language translation Laskar et al. (2019). The literature survey mainly focuses on NMT for low resource language pairs since NMT"
2020.wmt-1.45,P16-1009,0,0.622893,"ut like SMT, the NMT based model also suffers the requirement of sufficient training parallel corpus, which is a challenge in the case of low resource languages. For this reason, there is a demand for direct translation among similar language pairs by utilizing similarity features and monolingual data, so that less availability of the parallel data does not pose a challenge. However, the NMT technique achieves state-of-the-art approach in MT because of its transformer model Vaswani et al. (2017). For low resource language pair translation, NMT models have been improved with monolingual corpus Sennrich et al. (2016b); Burlot and Yvon (2018); Wu et al. (2019). In this work, we have adopted cross-lingual language model (XLM) Conneau and Lample (2019) to implement an NMT model for Hindi-Marathi similar language translation task because XLM shows significant improvements for low-resource languages by utilizing the monolingual corpora. 2 Related Work Hindi-Marathi translation lacks background work. However, similar work is found on Hindi-Nepali pair at WMT19 shared task of similar language translation Laskar et al. (2019). The literature survey mainly focuses on NMT for low resource language pairs since NMT"
2020.wmt-1.45,2006.amta-papers.25,0,0.122407,"t was set to 0.1, gelu activation was used. Also, adam was used as an optimizer with an initial learning rate of 0.0001. Rest of the parameters are same as used by Conneau and Lample (2019) in their experiments and as given in their GitHub repository3 . 6 Result and Analysis The WMT20 organizer declared result for the shared task of similar language translation on Hindi to Marathi4 and Marathi to Hindi5 and the results of our system’s is reported in Table 3. Our team’s name is NITS-CNLP. The participated systems are evaluated by BLEU Papineni et al. (2002), RIBES Isozaki et al. (2010) and TER Snover et al. (2006) and the tracks are ranked by BLEU score. A total of 21 teams participated in Hindi to Marathi translation track and 23 teams for Marathi to Hindi translation track including both primary and contrastive system types. Our system’s rank is 10 with BLEU score 11.59 for Hindi to Marathi translation https://github.com/glample/fastBPE 398 2 The model was trained on a Quadro P200 GPU having 5GB of GPU RAM 3 https://github.com/facebookresearch/XLM 4 http://mzampieri.com/workshops/wmt/HI-MR. pdf 5 http://mzampieri.com/workshops/wmt/MR-HI. pdf Type Short Medium Source: Hindi Target: Marathi Source Test"
2020.wmt-1.45,P19-2017,0,0.0151904,"ple (2019). Diagram adapted from (Conneau and Lample, 2019) after suitable changes. ing performance improves while parallel training data increases. For low resource languages, it is difficult to collect parallel data unlike monolingual data which is easily found through online sources. Hence, monolingual based NMT systems are introduced to enhance the translation quality of low resource language pair translation Sennrich et al. (2016b); Burlot and Yvon (2018); Wu et al. (2019). To get the advantage of monolingual data, unsupervised pre-train methods are introduced Ramachandran et al. (2017); Variš and Bojar (2019). Conneau and Lample (2019) proposed XLM based on bidirectional encoder representations from transformers (BERT) where the contextual language model is built with words based on preceding and succeeding context. No work has been done on Hindi-Marathi low resource language pair with such advanced NMT based approach, from the best of our knowledge. Our work investigates XLM model on Hindi-Marathi low resource language pair translation. 3 3.1 Dataset Description The organizers of WMT20 provided parallel and monolingual corpus for both Hindi and Marathi. The training dataset available for the WMT2"
2020.wmt-1.45,D19-1430,0,0.0931109,"requirement of sufficient training parallel corpus, which is a challenge in the case of low resource languages. For this reason, there is a demand for direct translation among similar language pairs by utilizing similarity features and monolingual data, so that less availability of the parallel data does not pose a challenge. However, the NMT technique achieves state-of-the-art approach in MT because of its transformer model Vaswani et al. (2017). For low resource language pair translation, NMT models have been improved with monolingual corpus Sennrich et al. (2016b); Burlot and Yvon (2018); Wu et al. (2019). In this work, we have adopted cross-lingual language model (XLM) Conneau and Lample (2019) to implement an NMT model for Hindi-Marathi similar language translation task because XLM shows significant improvements for low-resource languages by utilizing the monolingual corpora. 2 Related Work Hindi-Marathi translation lacks background work. However, similar work is found on Hindi-Nepali pair at WMT19 shared task of similar language translation Laskar et al. (2019). The literature survey mainly focuses on NMT for low resource language pairs since NMT outperforms conventional SMT on low resource"
2021.dravidianlangtech-1.4,2020.peoples-1.5,0,0.439296,"Missing"
2021.dravidianlangtech-1.4,2021.dravidianlangtech-1.17,0,0.0791384,"Missing"
2021.dravidianlangtech-1.4,2018.gwc-1.10,0,0.0237566,"Missing"
2021.dravidianlangtech-1.4,2021.ltedi-1.30,0,0.0366455,"Missing"
2021.dravidianlangtech-1.4,2020.peoples-1.6,0,0.0743382,"Missing"
2021.dravidianlangtech-1.4,2020.vardial-1.6,0,0.0540313,"Missing"
2021.dravidianlangtech-1.4,W14-5152,0,0.045565,"Missing"
2021.dravidianlangtech-1.4,2020.semeval-1.171,1,0.706796,"the Indian languages that are spoken by the majority of the population, like Hindi and Bengali, the same cannot be said for under-resourced languages such as, Tamil, Telugu, and Malayalam. For over 2600 years, recorded Tamil literature has been documented. Sangam literature, the earliest period of Tamil literature, is dated from around 600 BC- 300 AD. Among the Dravidian languages, Tamil has the oldest existing literature. Tamil is the oldest living language in India. Moreover, with the advent of social media, sentiment analysis research has become even more wide-spread (Mahata et al., 2020; Garain et al., 2020) as it takes into account conversations of customers around the social space and puts them into context. But, in the context of the Indian subcontinent, social media texts are not in one language and are largely code-mixed in nature. This is because India, has had much foreign acquaintance historically, and this has led the diaspora to adopt English as one of their official languages. Due to this, much of the Indian population are familiar with English as well as one or more regional languages (Mahata et al., 2019). This leads to communication in sentences, which contain more than one language"
2021.dravidianlangtech-1.4,2021.dravidianlangtech-1.30,0,0.0353618,"Missing"
2021.dravidianlangtech-1.4,voss-etal-2014-finding,0,0.0278001,"re developed using traditional ML algorithms.Finally, section 5 and 6 deals with the evaluation of our model and the concluding remarks. 2 these kinds of data are more or less always written in the Roman script, analyzing these kinds of data, with help of NLP tools, becomes even more difficult. Over the years, many experiments have been performed on code-mixed data. These include language identification, sentiment analysis, etc., to name a few. Language identification tasks have been earlier performed on various language pairs, such as Spanish-English (Negr´on Goldbarg, 2009), French-English (Voss et al., 2014), Hindi-English (Vyas et al., 2014; Das and Gamb¨ack, 2014) and Bengali-English (Das and Gamb¨ack, 2014). While these experiments were conducted with the help of dictionary word matching and ML-based algorithms such as Support Vector Machines (SVM), word-based logistic regression classifiers, and Latent Direchlet Allocation (LDA) (Blei et al., 2003), we use more state-of-the-art deep learning approaches to achieve the same. Also, sentiment analysis or opinion mining from code-mixed data is a trivial task because • Generally, code-mixed data is noisy in nature and requires cleaning and normaliz"
2021.dravidianlangtech-1.4,D14-1105,0,0.0327356,"lgorithms.Finally, section 5 and 6 deals with the evaluation of our model and the concluding remarks. 2 these kinds of data are more or less always written in the Roman script, analyzing these kinds of data, with help of NLP tools, becomes even more difficult. Over the years, many experiments have been performed on code-mixed data. These include language identification, sentiment analysis, etc., to name a few. Language identification tasks have been earlier performed on various language pairs, such as Spanish-English (Negr´on Goldbarg, 2009), French-English (Voss et al., 2014), Hindi-English (Vyas et al., 2014; Das and Gamb¨ack, 2014) and Bengali-English (Das and Gamb¨ack, 2014). While these experiments were conducted with the help of dictionary word matching and ML-based algorithms such as Support Vector Machines (SVM), word-based logistic regression classifiers, and Latent Direchlet Allocation (LDA) (Blei et al., 2003), we use more state-of-the-art deep learning approaches to achieve the same. Also, sentiment analysis or opinion mining from code-mixed data is a trivial task because • Generally, code-mixed data is noisy in nature and requires cleaning and normalization. • It needs several steps su"
2021.dravidianlangtech-1.4,2021.dravidianlangtech-1.16,0,0.0661037,"Missing"
2021.dravidianlangtech-1.4,2020.trac-1.6,0,0.0954703,"Missing"
2021.mtsummit-loresmt.9,N18-1032,0,0.0257654,", native speakers, and 3 http://lisindia.ciil.org/Khasi/Khasi_ script.html Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 89 computational resources. The Natural languages are categorized into three broad categories: high, medium, and lowresource. A language falls un der the lowresource category when it has lim ited online resources Megerdoomian and Parvaz (2008); Probst et al. (2001). This categorization can also be made on the quantity of data required to train an NMT model Gu et al. (2018). Accord ing to Kocmi (2020), a language is considered a lowresource language if the number of training in stances present in the corpus is below one million. Along with the corpus’s size, the diversity of both language and structure is of importance too. Struc turally, it must consist of all types of sentences, including short, medium, and long sentences. Dif ferent dialects of the same language might give rise to some inconsistency in translations. Therefore, the designation of a language as “lowresource” is not precise and requires consideration of many factors. Most world languages a"
2021.mtsummit-loresmt.9,D10-1092,0,0.0507385,"short term memory (LSTM), having 500 units in each layer with at tention (Bahdanau et al., 2015). The default learn ing rate of 0.001 with Adam optimizer and 0.3 dropouts are used. Moreover, GloVe10 Penning ton et al. (2014) pretrained word vectors are used by utilizing the monolingual data. For English monolingual data, we have used 3 million sen tences collected from WMT16. 3.2 Results To evaluate baseline systems, we used the auto matic evaluation metrics, namely, bilingual eval uation understudy (BLEU) Papineni et al. (2002), rankbased intuitive bilingual evaluation score (RIBES) Isozaki et al. (2010), translation edit rate (TER) Snover et al. (2006), word error rate (WER) Morris et al. (2004), metric for evaluation of translation with explicit ordering (METEOR) 9 10 https://github.com/OpenNMT/OpenNMT-py https://github.com/stanfordnlp/GloVe Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 91 Corpus Parallel Monolingual English After Jesus died, God re stored him to life as a spirit person. We’ll go to any length to send our child to a good university. How are you?   Khasi Had"
2021.mtsummit-loresmt.9,L18-1548,0,0.0829448,"age, and common sentences. 3 Baseline System In the area of MT, the NMT achieves a stateof theart approach for both high and lowresource language pairs Bahdanau et al. (2015); Pathak et al. (2018); Pathak and Pakray (2018); Laskar et al. (2019); Laskar et al. (2019, 2020b, 2021a). There fore, we have chosen NMT to build the base line system to estimate benchmark translation ac curacy for both EntoKha and KhatoEn trans lation.A sequencetosequence (seq2seq) model based encoderdecoder architecture is adopted for this work following the NMT baseline system of Laskar et al. (2020a); Kunchukuttan et al. (2018). 3.1 Experimental Setup The OpenNMTpy9 toolkit is employed to build two seq2seq models, namely RNN and BRNN. We have used twolayer long short term memory (LSTM), having 500 units in each layer with at tention (Bahdanau et al., 2015). The default learn ing rate of 0.001 with Adam optimizer and 0.3 dropouts are used. Moreover, GloVe10 Penning ton et al. (2014) pretrained word vectors are used by utilizing the monolingual data. For English monolingual data, we have used 3 million sen tences collected from WMT16. 3.2 Results To evaluate baseline systems, we used the auto matic evaluation"
2021.mtsummit-loresmt.9,R19-1052,0,0.0270328,"ata is aligned by separating them into source and target files. The process of alignment and verifica tion took substantial human effort. Additionally, we collected parallel sentences manually from the : LearnKhasi website. 2.3 Domain Coverage The proposed corpus, EnKhCorp1.0 encompasses different domains, including religious material (the Bible), literature, daily usage, and common sentences. 3 Baseline System In the area of MT, the NMT achieves a stateof theart approach for both high and lowresource language pairs Bahdanau et al. (2015); Pathak et al. (2018); Pathak and Pakray (2018); Laskar et al. (2019); Laskar et al. (2019, 2020b, 2021a). There fore, we have chosen NMT to build the base line system to estimate benchmark translation ac curacy for both EntoKha and KhatoEn trans lation.A sequencetosequence (seq2seq) model based encoderdecoder architecture is adopted for this work following the NMT baseline system of Laskar et al. (2020a); Kunchukuttan et al. (2018). 3.1 Experimental Setup The OpenNMTpy9 toolkit is employed to build two seq2seq models, namely RNN and BRNN. We have used twolayer long short term memory (LSTM), having 500 units in each layer with at tention (Bahdanau"
2021.mtsummit-loresmt.9,2020.wmt-1.45,1,0.837744,"Missing"
2021.mtsummit-loresmt.9,W19-5427,1,0.84326,"ata is aligned by separating them into source and target files. The process of alignment and verifica tion took substantial human effort. Additionally, we collected parallel sentences manually from the : LearnKhasi website. 2.3 Domain Coverage The proposed corpus, EnKhCorp1.0 encompasses different domains, including religious material (the Bible), literature, daily usage, and common sentences. 3 Baseline System In the area of MT, the NMT achieves a stateof theart approach for both high and lowresource language pairs Bahdanau et al. (2015); Pathak et al. (2018); Pathak and Pakray (2018); Laskar et al. (2019); Laskar et al. (2019, 2020b, 2021a). There fore, we have chosen NMT to build the base line system to estimate benchmark translation ac curacy for both EntoKha and KhatoEn trans lation.A sequencetosequence (seq2seq) model based encoderdecoder architecture is adopted for this work following the NMT baseline system of Laskar et al. (2020a); Kunchukuttan et al. (2018). 3.1 Experimental Setup The OpenNMTpy9 toolkit is employed to build two seq2seq models, namely RNN and BRNN. We have used twolayer long short term memory (LSTM), having 500 units in each layer with at tention (Bahdanau"
2021.mtsummit-loresmt.9,megerdoomian-parvaz-2008-low,0,0.134316,"ficial neural networks in MT systems. The data or resources for train ing the translation systems may include corpora from various online sources, native speakers, and 3 http://lisindia.ciil.org/Khasi/Khasi_ script.html Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 89 computational resources. The Natural languages are categorized into three broad categories: high, medium, and lowresource. A language falls un der the lowresource category when it has lim ited online resources Megerdoomian and Parvaz (2008); Probst et al. (2001). This categorization can also be made on the quantity of data required to train an NMT model Gu et al. (2018). Accord ing to Kocmi (2020), a language is considered a lowresource language if the number of training in stances present in the corpus is below one million. Along with the corpus’s size, the diversity of both language and structure is of importance too. Struc turally, it must consist of all types of sentences, including short, medium, and long sentences. Dif ferent dialects of the same language might give rise to some inconsistency in translations. Therefor"
2021.mtsummit-loresmt.9,P02-1040,0,0.117516,"d to build two seq2seq models, namely RNN and BRNN. We have used twolayer long short term memory (LSTM), having 500 units in each layer with at tention (Bahdanau et al., 2015). The default learn ing rate of 0.001 with Adam optimizer and 0.3 dropouts are used. Moreover, GloVe10 Penning ton et al. (2014) pretrained word vectors are used by utilizing the monolingual data. For English monolingual data, we have used 3 million sen tences collected from WMT16. 3.2 Results To evaluate baseline systems, we used the auto matic evaluation metrics, namely, bilingual eval uation understudy (BLEU) Papineni et al. (2002), rankbased intuitive bilingual evaluation score (RIBES) Isozaki et al. (2010), translation edit rate (TER) Snover et al. (2006), word error rate (WER) Morris et al. (2004), metric for evaluation of translation with explicit ordering (METEOR) 9 10 https://github.com/OpenNMT/OpenNMT-py https://github.com/stanfordnlp/GloVe Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 91 Corpus Parallel Monolingual English After Jesus died, God re stored him to life as a spirit person. We’ll go t"
2021.mtsummit-loresmt.9,D14-1162,0,0.0844738,"Missing"
2021.mtsummit-loresmt.9,2001.mtsummit-road.7,0,0.342512,"tems. The data or resources for train ing the translation systems may include corpora from various online sources, native speakers, and 3 http://lisindia.ciil.org/Khasi/Khasi_ script.html Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 89 computational resources. The Natural languages are categorized into three broad categories: high, medium, and lowresource. A language falls un der the lowresource category when it has lim ited online resources Megerdoomian and Parvaz (2008); Probst et al. (2001). This categorization can also be made on the quantity of data required to train an NMT model Gu et al. (2018). Accord ing to Kocmi (2020), a language is considered a lowresource language if the number of training in stances present in the corpus is below one million. Along with the corpus’s size, the diversity of both language and structure is of importance too. Struc turally, it must consist of all types of sentences, including short, medium, and long sentences. Dif ferent dialects of the same language might give rise to some inconsistency in translations. Therefore, the designation of"
2021.mtsummit-loresmt.9,2020.loresmt-1.10,0,0.0372495,"tance too. Struc turally, it must consist of all types of sentences, including short, medium, and long sentences. Dif ferent dialects of the same language might give rise to some inconsistency in translations. Therefore, the designation of a language as “lowresource” is not precise and requires consideration of many factors. Most world languages are categorized as lowresource on account of resource availability. In India, the limited MT works are performed on the northeastern region’s lowresource languages, including Mizo Lalrempuii et al. (2021), Assamese Laskar et al. (2021b), Manipuri Singh and Singh (2020), and Khasi Thabah and Purkayastha (2021). We can consider the English–Khasi pair as a low resource pair based on limited resources. In this work, we have developed an English– Khasi corpus: EnKhCorp1.0 and built baseline systems based on NMT. There is no standard cor pus available for the lowresource English–Khasi pair to the best of our knowledge. It is the hope of the authors that this resource fills that gap and leads to the development of more and better resources for the Khasi community. The structure of the rest of the paper is as follows: Section 2 presents the overview of corpus pr"
2021.mtsummit-loresmt.9,2006.amta-papers.25,0,0.0100699,"layer with at tention (Bahdanau et al., 2015). The default learn ing rate of 0.001 with Adam optimizer and 0.3 dropouts are used. Moreover, GloVe10 Penning ton et al. (2014) pretrained word vectors are used by utilizing the monolingual data. For English monolingual data, we have used 3 million sen tences collected from WMT16. 3.2 Results To evaluate baseline systems, we used the auto matic evaluation metrics, namely, bilingual eval uation understudy (BLEU) Papineni et al. (2002), rankbased intuitive bilingual evaluation score (RIBES) Isozaki et al. (2010), translation edit rate (TER) Snover et al. (2006), word error rate (WER) Morris et al. (2004), metric for evaluation of translation with explicit ordering (METEOR) 9 10 https://github.com/OpenNMT/OpenNMT-py https://github.com/stanfordnlp/GloVe Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 91 Corpus Parallel Monolingual English After Jesus died, God re stored him to life as a spirit person. We’ll go to any length to send our child to a good university. How are you?   Khasi Hadien ba u Jisu u la ïap, U Blei u la ai biang ha u"
2021.wat-1.17,P17-1175,0,0.0361454,"Missing"
2021.wat-1.17,W18-3405,0,0.0233975,"nt modalities like text, image, and audio. Combining information from more than one modality attempts to amend the quality of low resource language translation. (Shah et al., 2016) show, combining the visual features of images with corresponding textual features of the input bitext to translate sentences outperform text-only translation. Encoder-decoder architecture is a widely used technique in the MT community for text-only-based NMT as it handles Related Works For the English-Hindi language pair, the literature survey revealed minor existing works on translation using multimodal NMT (Dutta Chowdhury et al., 2018; Sanayai Meetei et al., 2019; Laskar et al., 2019c). (Dutta Chowdhury et al., 2018) uses synthetic data, following multimodal NMT settings (Calixto and Liu, 2017), and attains a BLEU score of 24.2 for Hindi to English translation. However, in the WAT 2019 multimodal translation task of English to Hindi, we achieved the highest BLEU score of 20.37 for the challenge test set (Laskar et al., 2019c). This score was improved later in the task of WAT2020 (Laskar et al., 2020c) to obtain the BLEU score of 33.57 on the challenge 155 Proceedings of the 8th Workshop on Asian Translation, pages 155–160"
2021.wat-1.17,D10-1092,0,0.0379486,"image test data. 5 Result and Analysis The WAT2021 shared task organizer published the evaluation result6 of multimodal translation task for English to Hindi and our team stood second position in multimodal submission for challenge test set. Our team name is CNLP-NITS-PP, and we have participated in the multimodal and text-only submission tracks of the same task. In both multimodal and text-only translation submission tracks, a total of three teams participated in both evaluation and challenges test data. The results are evaluated using automatic metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). The results of our system is reported in Table 4, and it is noticed that the multimodal NMT obtains higher than text-only NMT. It is because the combination of textual and visual features outperforms text-only NMT. Furthermore, our system’s results are improved as compared to our previous work on the same task (Laskar et al., 2020c). It shows the BLEU, RIBES, AMFM scores of present work show (+5.71, +9.41), (+0.037956, +0.055641), (+0.04291, +0.04835) increments on the challenge test set for multimodal and text-only NMT, where it is realised that phrase pairs a"
2021.wat-1.17,P17-4012,0,0.0344595,"tences having ID numbers 2391240, 2385507, 2328549 from parallel data and one image having ID number 2326837 (since corresponding text not present in parallel data), the parallel and image train data reduced to 28,927. Moreover, EnglishHindi (En-Hi) parallel and Hindi monolingual corpus1 (Kunchukuttan et al., 2018) and also, English monolingual data available at WMT162 are used. Table 1 and 2 depict the data statistics. 1 http://www.cfilt.iitb.ac.in/iitb_ parallel/ 2 http://www.statmt.org/wmt16/ translation-task.html System Description To build multimodal and text-only NMT models, OpenNMT-py (Klein et al., 2017) tool is used. There are four operations which include data augmentation, preprocessing, training and testing. Our multi-model NMT gets advantages from both image and textual features with phrase pairs and word embeddings. 4.1 Data Augmentation In (Sen et al., 2020), authors used SMT-based phrase pairs to augment with the original parallel data to improve low-resource language pairs translation. In SMT3 , Giza++ word alignment tool is used to extract phrase pair. Inspired by the work (Sen et al., 2020), we have extracted phrase 3 156 http://www.statmt.org/moses/ Figure 2: Examples of our best"
2021.wat-1.17,L18-1548,0,0.0231104,"augmentation for English-to-Hindi multimodal NMT. Dataset Description 4 We have used the Hindi Visual Genome 1.1 dataset provided by WAT2021 organizers (Nakazawa et al., 2021; Parida et al., 2019). The train data contains English-Hindi 28,930 parallel sentences and 28,928 images. After removing duplicate sentences having ID numbers 2391240, 2385507, 2328549 from parallel data and one image having ID number 2326837 (since corresponding text not present in parallel data), the parallel and image train data reduced to 28,927. Moreover, EnglishHindi (En-Hi) parallel and Hindi monolingual corpus1 (Kunchukuttan et al., 2018) and also, English monolingual data available at WMT162 are used. Table 1 and 2 depict the data statistics. 1 http://www.cfilt.iitb.ac.in/iitb_ parallel/ 2 http://www.statmt.org/wmt16/ translation-task.html System Description To build multimodal and text-only NMT models, OpenNMT-py (Klein et al., 2017) tool is used. There are four operations which include data augmentation, preprocessing, training and testing. Our multi-model NMT gets advantages from both image and textual features with phrase pairs and word embeddings. 4.1 Data Augmentation In (Sen et al., 2020), authors used SMT-based phrase"
2021.wat-1.17,R19-1052,0,0.0322109,"ious issues like variable-length phrases using sequence to sequence learning, the problem of longterm dependency using Long Short Term Memory (LSTM) (Sutskever et al., 2014). Nevertheless, the basic encoder-decoder architecture cannot encode all the information when it comes to very long sentences. The attention mechanism is proposed to handle such issues, which pays attention to all source words locally and globally (Bahdanau et al., 2015; Luong et al., 2015). The attention-based NMT yields substantial performance for Indian language translation (Pathak and Pakray, 2018; Pathak et al., 2018; Laskar et al., 2019a,b, 2020a, 2021b,a). Moreover, NMT performance can be enhanced by utilizing monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016; Laskar et al., 2020b) and phrase pair injection (Sen et al., 2020), effective in low resource language pair translation. This paper aims English to Hindi translation using the multimodal concept by taking advantage of monolingual data and phrase pair injections to improve the translation quality at the WAT2021 translation task. Machine translation performs automatic translation from one natural language to another. Neural machine translation attains a stat"
2021.wat-1.17,2020.loresmt-1.9,1,0.819616,"Missing"
2021.wat-1.17,2020.wmt-1.45,1,0.864032,"Missing"
2021.wat-1.17,2020.wat-1.11,1,0.0912639,"Missing"
2021.wat-1.17,W19-5427,1,0.851696,"ious issues like variable-length phrases using sequence to sequence learning, the problem of longterm dependency using Long Short Term Memory (LSTM) (Sutskever et al., 2014). Nevertheless, the basic encoder-decoder architecture cannot encode all the information when it comes to very long sentences. The attention mechanism is proposed to handle such issues, which pays attention to all source words locally and globally (Bahdanau et al., 2015; Luong et al., 2015). The attention-based NMT yields substantial performance for Indian language translation (Pathak and Pakray, 2018; Pathak et al., 2018; Laskar et al., 2019a,b, 2020a, 2021b,a). Moreover, NMT performance can be enhanced by utilizing monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016; Laskar et al., 2020b) and phrase pair injection (Sen et al., 2020), effective in low resource language pair translation. This paper aims English to Hindi translation using the multimodal concept by taking advantage of monolingual data and phrase pair injections to improve the translation quality at the WAT2021 translation task. Machine translation performs automatic translation from one natural language to another. Neural machine translation attains a stat"
2021.wat-1.17,D19-5205,1,0.763491,"Missing"
2021.wat-1.17,D15-1166,0,0.0675704,"ring National Institute of Technology Silchar Assam, India {sahinur rs, abdullah ug, darsh ug, partha}@cse.nits.ac.in, sivaji.cse.ju@gmail.com Abstract various issues like variable-length phrases using sequence to sequence learning, the problem of longterm dependency using Long Short Term Memory (LSTM) (Sutskever et al., 2014). Nevertheless, the basic encoder-decoder architecture cannot encode all the information when it comes to very long sentences. The attention mechanism is proposed to handle such issues, which pays attention to all source words locally and globally (Bahdanau et al., 2015; Luong et al., 2015). The attention-based NMT yields substantial performance for Indian language translation (Pathak and Pakray, 2018; Pathak et al., 2018; Laskar et al., 2019a,b, 2020a, 2021b,a). Moreover, NMT performance can be enhanced by utilizing monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016; Laskar et al., 2020b) and phrase pair injection (Sen et al., 2020), effective in low resource language pair translation. This paper aims English to Hindi translation using the multimodal concept by taking advantage of monolingual data and phrase pair injections to improve the translation quality at the W"
2021.wat-1.17,P02-1040,0,0.109088,"tion Task. 6 visual features of image test data. 5 Result and Analysis The WAT2021 shared task organizer published the evaluation result6 of multimodal translation task for English to Hindi and our team stood second position in multimodal submission for challenge test set. Our team name is CNLP-NITS-PP, and we have participated in the multimodal and text-only submission tracks of the same task. In both multimodal and text-only translation submission tracks, a total of three teams participated in both evaluation and challenges test data. The results are evaluated using automatic metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). The results of our system is reported in Table 4, and it is noticed that the multimodal NMT obtains higher than text-only NMT. It is because the combination of textual and visual features outperforms text-only NMT. Furthermore, our system’s results are improved as compared to our previous work on the same task (Laskar et al., 2020c). It shows the BLEU, RIBES, AMFM scores of present work show (+5.71, +9.41), (+0.037956, +0.055641), (+0.04291, +0.04835) increments on the challenge test set for multimodal and text-only NMT, where it i"
2021.wat-1.17,D14-1162,0,0.0869051,"he obtained phrase pairs are augmented to the original parallel data. The data statistics of extracted phrase pairs is given in Table 3. Additionally, IITB parallel data is directly augmented with the original parallel to expand the train data. The diagram of data augmentation is presented in Figure 1. 4.2 Data Preprocessing To extract visual features from image data, we have used publicly available5 pre-trained CNN with VGG19. The visual features are extracted independently for train, validation, and test data. To get the advantage of monolingual data on both multimodal and text-only, GloVe (Pennington et al., 2014) is used to generate vectors of word embeddings. For tokenization of text data, the OpenNMT-py tool is utilized and obtained a vocabulary size of 50004 for source-target sentences. We have not used any word-segmentation technique. 4.3 Training The multimodal and text-only based NMT are trained independently. During the multimodal training process, extracted visual features, pre-trained 4 https://github.com/ayushidalmia/ Phrase-Based-Model 5 https://github.com/iacercalixto/ MultimodalNMT word vectors are fine-tuned with the augmented parallel data. The bidirectional RNN (BRNN) at encoder type a"
2021.wat-1.17,D19-5224,1,0.846605,"and audio. Combining information from more than one modality attempts to amend the quality of low resource language translation. (Shah et al., 2016) show, combining the visual features of images with corresponding textual features of the input bitext to translate sentences outperform text-only translation. Encoder-decoder architecture is a widely used technique in the MT community for text-only-based NMT as it handles Related Works For the English-Hindi language pair, the literature survey revealed minor existing works on translation using multimodal NMT (Dutta Chowdhury et al., 2018; Sanayai Meetei et al., 2019; Laskar et al., 2019c). (Dutta Chowdhury et al., 2018) uses synthetic data, following multimodal NMT settings (Calixto and Liu, 2017), and attains a BLEU score of 24.2 for Hindi to English translation. However, in the WAT 2019 multimodal translation task of English to Hindi, we achieved the highest BLEU score of 20.37 for the challenge test set (Laskar et al., 2019c). This score was improved later in the task of WAT2020 (Laskar et al., 2020c) to obtain the BLEU score of 33.57 on the challenge 155 Proceedings of the 8th Workshop on Asian Translation, pages 155–160 Bangkok, Thailand (online), A"
2021.wat-1.17,P16-1009,0,0.0377283,"sing Long Short Term Memory (LSTM) (Sutskever et al., 2014). Nevertheless, the basic encoder-decoder architecture cannot encode all the information when it comes to very long sentences. The attention mechanism is proposed to handle such issues, which pays attention to all source words locally and globally (Bahdanau et al., 2015; Luong et al., 2015). The attention-based NMT yields substantial performance for Indian language translation (Pathak and Pakray, 2018; Pathak et al., 2018; Laskar et al., 2019a,b, 2020a, 2021b,a). Moreover, NMT performance can be enhanced by utilizing monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016; Laskar et al., 2020b) and phrase pair injection (Sen et al., 2020), effective in low resource language pair translation. This paper aims English to Hindi translation using the multimodal concept by taking advantage of monolingual data and phrase pair injections to improve the translation quality at the WAT2021 translation task. Machine translation performs automatic translation from one natural language to another. Neural machine translation attains a state-ofthe-art approach in machine translation, but it requires adequate training data, which is a severe problem for l"
2021.wat-1.17,W16-2363,0,0.0249383,"sk in both text-only and multimodal translation. We have achieved second rank on the challenge test set for English to Hindi multimodal translation where Bilingual Evaluation Understudy (BLEU) score of 39.28, Rank-based Intuitive Bilingual Evaluation Score (RIBES) 0.792097, and AdequacyFluency Metrics (AMFM) score 0.830230 respectively. 1 2 Introduction Multimodal NMT (MNMT) intends to draw insights from the input data through different modalities like text, image, and audio. Combining information from more than one modality attempts to amend the quality of low resource language translation. (Shah et al., 2016) show, combining the visual features of images with corresponding textual features of the input bitext to translate sentences outperform text-only translation. Encoder-decoder architecture is a widely used technique in the MT community for text-only-based NMT as it handles Related Works For the English-Hindi language pair, the literature survey revealed minor existing works on translation using multimodal NMT (Dutta Chowdhury et al., 2018; Sanayai Meetei et al., 2019; Laskar et al., 2019c). (Dutta Chowdhury et al., 2018) uses synthetic data, following multimodal NMT settings (Calixto and Liu,"
2021.wat-1.17,D16-1160,0,0.0237685,"mory (LSTM) (Sutskever et al., 2014). Nevertheless, the basic encoder-decoder architecture cannot encode all the information when it comes to very long sentences. The attention mechanism is proposed to handle such issues, which pays attention to all source words locally and globally (Bahdanau et al., 2015; Luong et al., 2015). The attention-based NMT yields substantial performance for Indian language translation (Pathak and Pakray, 2018; Pathak et al., 2018; Laskar et al., 2019a,b, 2020a, 2021b,a). Moreover, NMT performance can be enhanced by utilizing monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016; Laskar et al., 2020b) and phrase pair injection (Sen et al., 2020), effective in low resource language pair translation. This paper aims English to Hindi translation using the multimodal concept by taking advantage of monolingual data and phrase pair injections to improve the translation quality at the WAT2021 translation task. Machine translation performs automatic translation from one natural language to another. Neural machine translation attains a state-ofthe-art approach in machine translation, but it requires adequate training data, which is a severe problem for low-resource language p"
C10-2027,C00-1044,0,0.117604,"Missing"
C10-2027,E06-1039,0,\N,Missing
C12-2090,H92-1023,0,0.0908623,"Missing"
C12-2090,A00-1031,0,0.223768,"Missing"
C12-2090,H92-1022,0,0.32804,"Missing"
C12-2090,J95-4004,0,0.130528,"Missing"
C12-2090,E09-1015,0,0.0402471,"Missing"
C12-2090,A92-1018,0,0.300951,"Missing"
C12-2090,P07-2056,0,0.0416637,"Missing"
C12-2090,O12-1029,1,0.841103,"ectively. KEYWORDS : Kokborok, POS Tagger, Suffix, Prefix, CRF, SVM, Morph analyser. Proceedings of COLING 2012: Posters, pages 923–932, COLING 2012, Mumbai, December 2012. 923 1 Introduction From the very beginning, POS tagging has been playing its significant roles in several Natural Language Processing (NLP) applications such as chunking, parsing, developing Information Extraction systems, semantic processing, Question Answering (QA), Summarization, Event Tracking etc. To the best of our knowledge, no prior work on POS tagging has been done for Kokborok except the development of a stemmer (Patra et al., 2012). Thus, in this paper, we have basically described the development of a POS tagger in Kokborok, a less privileged native language of the Borok people of Tripura, a state in North Eastern part of India. Kokborok is also spoken by neighboring states such as Assam, Manipur, Mizoram and the countries like Bangladesh, Myanmar etc. The language comprises of more than 2.5 millions of people1 and belongs to Tibeto-Burman (TB) language family. It has several unique features if compared with other South-Asian Tibeto-Burman languages. Kokborok literatures were written in Koloma or Swithaih borok script w"
C12-2090,W96-0213,0,0.378955,"Missing"
C12-2090,P06-2100,0,0.0347638,"Missing"
C12-2090,I08-3015,1,\N,Missing
C12-3003,Y10-1063,1,0.28247,"Missing"
C12-3003,W09-2902,0,0.178785,"Missing"
C16-1186,baccianella-etal-2010-sentiwordnet,0,0.0252202,"Missing"
C16-1186,bakliwal-etal-2012-hindi,0,0.0291119,"ssed in the following subsections. Preprocessing: First we cleaned the lyrics dataset by removing the junk characters and HTML tags. Subsequently we removed the duplicate lines as it was observed that the starting stanza is usually repeated in the song at least a few times. Therefore, we removed these duplicate sentences to remove the biasness in the lyric. 3.2.1 Sentiment Lexicons (SL) The emotion or sentiment words are one of the most important features for mood classification from lyrics. These words in the Hindi lyrics were identified using three lexicons - Hindi Subjective Lexicon (HSL) (Bakliwal et al., 2012), Hindi SentiWordnet (HSW) (Joshi et al., 2010) and Hindi Wordnet Affect (HWA) (Das et al., 2012). Similarly, we used two lexicons - SentiWordNet (Baccianellaet al., 2010) (SWN) and WordNetAffect (Strapparava and Valitutti, 2004) (WA) for identifying the sentiment words from the lyrics of the Western songs. It was observed that the number of sentiment words found in the lyrics of Hindi songs was less than the number of sentiment words found in the lyrics of Western 1983 songs. The main reason was that the the performances of the POS tagger and stemmer/lemmatizer for Hindi language were not up"
C16-1186,W13-4104,1,0.740743,"songs. This paper is organized as follows: Section 2 introduces the mood taxonomy and describes the process of dataset preparation. Section 3 describes the audio and lyrics features. The FFNNs and the developed systems with comparison are described in the section 4. Finally, the conclusion is drawn in Section 5. 2 2.1 Mood Taxonomy and Dataset Mood Taxonomy We chose the Russell’s circumplex model (Russell, 1980) to build our own mood taxonomy. The circumplex model and a subset of this dimensional model have been used earlier for several mood classification studies (Ujlambkar and Attar, 2012; Patra et al., 2013a; Patra et al., 2013b; Patra et al., 2015c; Patra et al., 2016a; Patra et al., 2016b). The circumplex model is based on valence and arousal, which is widely accepted by the research community. Valence indicates the positivity and negativity of emotions whereas arousal indicates the emotional intensity. The mood taxonomy is prepared by clustering the similar affect words of the circumplex model into a single class and each class contains three affect words of the circumplex model as shown in Figure 1. Each of our mood classes has distinct positions in terms of arousal and valence. We considere"
C16-1186,W15-5939,1,0.57059,"nal License. http://creativecommons.org/licenses/by/4.0/ 1 https://www.cia.gov/library/publications/the-world-factbook/fields/2098.html License details: 1980 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1980–1989, Osaka, Japan, December 11-17 2016. Figure 1: Russell’s circumplex model of 28 affect words (Russell, 1980) In order to deal with the above mentioned issues, the contributions of the authors are given below. 1. We employed our earlier proposed mood taxonomy for music mood classification in Hindi and Western songs (Patra et al., 2015c; Patra et al., 2016a; Patra et al., 2016b). 2. We annotated the audio and lyrics of the Hindi and Western songs using the above mood taxonomy. 3. We observed difference in mood while annotating the mood at the time of listening to the music and reading its corresponding lyric in case of the Hindi songs. 4. We identified important features using correlation based feature selection technique. 5. The Feed Forward Neural Networks (FFNNs) is implemented for mood classification purpose. 6. We have developed a multimodal system based on the audio and lyrics of the songs. This paper is organized as"
C16-1186,strapparava-valitutti-2004-wordnet,0,0.0392257,"a is usually repeated in the song at least a few times. Therefore, we removed these duplicate sentences to remove the biasness in the lyric. 3.2.1 Sentiment Lexicons (SL) The emotion or sentiment words are one of the most important features for mood classification from lyrics. These words in the Hindi lyrics were identified using three lexicons - Hindi Subjective Lexicon (HSL) (Bakliwal et al., 2012), Hindi SentiWordnet (HSW) (Joshi et al., 2010) and Hindi Wordnet Affect (HWA) (Das et al., 2012). Similarly, we used two lexicons - SentiWordNet (Baccianellaet al., 2010) (SWN) and WordNetAffect (Strapparava and Valitutti, 2004) (WA) for identifying the sentiment words from the lyrics of the Western songs. It was observed that the number of sentiment words found in the lyrics of Hindi songs was less than the number of sentiment words found in the lyrics of Western 1983 songs. The main reason was that the the performances of the POS tagger and stemmer/lemmatizer for Hindi language were not up to the mark. The CRF based Shallow Parser5 is available for POS tagging and lemmatization, but it also did not perform well on the lyrics data because of the free word order nature of Hindi language. Most of the inflected sentime"
D19-5205,D17-1105,0,0.351025,"ong Kong, China, November 4, 2019. 2019 098 099 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 have used global features of the images. tion (text and image) integrating into the attentionbased encoder-decoder architecture. (Huang et al., 2016), proposed a model using attention based NMT, where regional and global visual features are attached in parallel with multiple encoding threads and each thread is followed by the text sequence. They obtained BLEU score 36.5, which outperformed the text-only baseline model BLEU score 34.5. (Calixto and Liu, 2017) used bidirectional recurrent neural network (RNN) with gated recurrent unit (GRU) in the encoding phase instead of single-layer unidirectional LSTM in (Huang et al., 2016) and also, used image features separately either as a word in the source sentence or directly for encoder or decoder initialization unlike word only in (Huang et al., 2016), achieved BLEU score 38.5, 43.9 in English to German and German to English translation respectively. (Calixto et al., 2017), introduced two independent attention mechanisms over source language words and visual features in a single decoder RNN, which sign"
D19-5205,P17-1175,0,0.158745,"Missing"
D19-5205,W18-3405,0,0.235757,") and also, used image features separately either as a word in the source sentence or directly for encoder or decoder initialization unlike word only in (Huang et al., 2016), achieved BLEU score 38.5, 43.9 in English to German and German to English translation respectively. (Calixto et al., 2017), introduced two independent attention mechanisms over source language words and visual features in a single decoder RNN, which significantly improve over the models used in (Huang et al., 2016), obtained BLEU score 39.0, 43.2 in English to German and German to English translation respectively. (Dutta Chowdhury et al., 2018), investigated multimodel NMT following settings of (Calixto and Liu, 2017) for Hindi to English translation and acquired BLEU score 24.2. 151 3.1 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 3 Data Preprocessing The data preprocessing steps of each track are carried out separately. In the multi-modal translation track, firstly, image features for training, validation and test data are extracted from the image data set as mentioned in Table 1. We have used publicly available pre-trained CNN with VGG19 via batch normalization for extraction of both global and"
D19-5205,P02-1040,0,0.103316,"Missing"
D19-5205,W16-2363,0,0.143842,"Missing"
D19-5205,W16-2360,0,0.065152,"nd Hindi Image Captioning 003 004 Sahinur Rahman Laskar, Rohit Pratap Singh, Partha Pakray and Sivaji Bandyopadhyay Department of Computer Science and Engineering National Institute of Technology Silchar Assam, India {sahinurlaskar.nits,rohitkako,parthapakray,sivaji.cse.ju}@gmail.com 007 008 009 010 011 058 059 060 061 063 013 Abstract 014 015 source-target text to translate sentences (Shah et al., 2016). Interestingly, multi-modal concept improved the translation quality of generating the captions of the images (Dash et al., 2019) as well as significant improvement over text-only NMT system (Huang et al., 2016). In text-only NMT system, the encoder-decoder framework of NMT is a widely accepted technique used in the task of MT. Because it handles sequence to sequence learning problem for variable length source and target sentences and also, handles long term dependency problem using long short term memory (LSTM) (Sutskever et al., 2014). The demerits of basic encoder-decoder model is that it fails to encode all necessary information into the context vector when the sentence is too long. Hence, to handle such problem attention-based encoderdecoder model is introduced, which allows the decoder to focus"
D19-5205,D10-1092,0,0.391536,"f Hindi-only image captioning track, we have used merge-model following settings of (Tanti et al., 2018). The preprocessed image feature vector of 4096 elements are processed by a dense layer to provide 256 elements for representation of the image. Afterward, the input text sequence of 22 words length are fed into a word embedding layer to convert it into vector form which is followed by LSTM based RNN layer contains 256 nodes. Both the fixed-length vectors (Image and text) generated are merged together and processed by a dense layer to build the train models up to 20 epoch. 3.3 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015) are used to measure performance of predicted translations. We have participated in all the track of the multi-modal translation task and our team name is 683. In multimodal translation track, a total of three teams, including our team participated for both challenge and evaluation test data in English to Hindi translation. We have acquired BLEU, RIBES, AMFM score 20.37, 0.642838, 0.668260 for challenge test set and BLEU, RIBES, AMFM score 40.55, 0.760080, 0.770860 for evaluation test set respectively, higher than other teams as shown in Table 2. However, we have"
D19-5205,P17-4012,0,0.0560412,"maximum description length of 22 words, are cleaned to get the vocabulary size of 5605. 128 129 150 System Description 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 The primary steps of the system operations are data preprocessing, system training and system testing and the same have been illustrated in following subsections. The multimodal NMT toolkit (Calixto and Liu, 2017; Calixto et al., 2017) is employed to build the multimodal NMT system for multimodal translation task, which are based on the pytorch port of OpenNMT (Klein et al., 2017). For text-only translation task, OpenNMT is deployed to build the NMT system and in the case of Hindi-only image captioning track, publicly available VGG16 and LSTM in Keras library, are used to build the system (Simonyan and Zisserman, 2015; Tanti et al., 2018). We have used Hindi visual genome dataset in each track of WAT2019 multi-modal translation task provided by the organizer (Nakazawa et al., 2019). We have not used image coordinates (Width, Height) provided in the dataset to indicate the rectangular region in the image described by the caption. Because, we 3.2 System Training After pr"
D19-5205,W19-5427,1,0.67479,"hat it fails to encode all necessary information into the context vector when the sentence is too long. Hence, to handle such problem attention-based encoderdecoder model is introduced, which allows the decoder to focus on different parts of the source sequence at different decoding steps (Bahdanau et al., 2015). (Luong et al., 2015) enhanced the attention model that merges global, accompanying to all source words and local, only pay attention to a part of source words. The attention-based NMT system shows a promising outcome in various languages (Pathak and Pakray, 2018; Pathak et al., 2018; Laskar et al., 2019). Current work has been investigated for English to Hindi translation. There are three different tracks, namely, multimodal translation, Hindi-only image captioning and text-only translation using NMT system and participated in WAT2019 multi-modal translation task. With the widespread use of Machine Translation (MT) techniques, attempt to minimize communication gap among people from diverse linguistic backgrounds. We have participated in Workshop on Asian Translation 2019 (WAT2019) multi-modal translation task. There are three types of submission track namely, multi-modal translation, Hindionl"
D19-5205,D15-1166,0,0.110609,"ed in the task of MT. Because it handles sequence to sequence learning problem for variable length source and target sentences and also, handles long term dependency problem using long short term memory (LSTM) (Sutskever et al., 2014). The demerits of basic encoder-decoder model is that it fails to encode all necessary information into the context vector when the sentence is too long. Hence, to handle such problem attention-based encoderdecoder model is introduced, which allows the decoder to focus on different parts of the source sequence at different decoding steps (Bahdanau et al., 2015). (Luong et al., 2015) enhanced the attention model that merges global, accompanying to all source words and local, only pay attention to a part of source words. The attention-based NMT system shows a promising outcome in various languages (Pathak and Pakray, 2018; Pathak et al., 2018; Laskar et al., 2019). Current work has been investigated for English to Hindi translation. There are three different tracks, namely, multimodal translation, Hindi-only image captioning and text-only translation using NMT system and participated in WAT2019 multi-modal translation task. With the widespread use of Machine Translation (M"
D19-5224,D14-1179,0,0.0582709,"Missing"
D19-5224,W16-3210,0,0.0516446,"Missing"
D19-5224,P02-1040,0,0.105678,"and image dataset from Subsection 4.1 are fed into our model (Subsection 3.5). A pre-trained model, VGG19-CNN, is employed to extract the global features from the image. The system is trained for 30 epoch with a learning rate set to 0.002, dropout rate of 0.3 and using Adam optimizer. 5 (a) Image Caption Generation https://github.com/rsennrich/subword-nmt 185 Set . The experiment for the three tasks is carried out separately on both the test dataset. Evaluation metrics: The evaluation of the translation system is carried out using three different techniques: AFMF (Banchs et al., 2015), BLEU (Papineni et al., 2002) score and RIBES (Isozaki et al., 2010). Task TOT HIC MMT BLEU 20.13 2.59 28.45 RIBES 0.57 0.15 0.63 dom human evaluation. We can conclude that, for the case of image caption generation, there is a need for a different type of evaluation metrics. 6 In this paper, we reported the evaluation of English-Hindi translation with different approaches as a part of WAT2019 shared task. It is observed that the multimodal approach of incorporating the visual features paired with text data gives significant improvement in translation than the other approaches. We also conclude that the same evaluation met"
D19-5224,D10-1092,0,0.209479,"re fed into our model (Subsection 3.5). A pre-trained model, VGG19-CNN, is employed to extract the global features from the image. The system is trained for 30 epoch with a learning rate set to 0.002, dropout rate of 0.3 and using Adam optimizer. 5 (a) Image Caption Generation https://github.com/rsennrich/subword-nmt 185 Set . The experiment for the three tasks is carried out separately on both the test dataset. Evaluation metrics: The evaluation of the translation system is carried out using three different techniques: AFMF (Banchs et al., 2015), BLEU (Papineni et al., 2002) score and RIBES (Isozaki et al., 2010). Task TOT HIC MMT BLEU 20.13 2.59 28.45 RIBES 0.57 0.15 0.63 dom human evaluation. We can conclude that, for the case of image caption generation, there is a need for a different type of evaluation metrics. 6 In this paper, we reported the evaluation of English-Hindi translation with different approaches as a part of WAT2019 shared task. It is observed that the multimodal approach of incorporating the visual features paired with text data gives significant improvement in translation than the other approaches. We also conclude that the same evaluation metrics used for the machine translation i"
D19-5224,D19-5224,1,0.0513221,"es learned from 1 3 System Architecture In our model, the dataset from the Hindi Visual Genome2 are used for three separate tasks: 1) Translation of English-Hindi using only the text dataset, 2) Generate the captions from the image, 3) Multimodal translation of English-Hindi using the image and the parallel text corpus. Figure 1 shows a brief representation of our working model. Following of this section illustrates the details of the dataset, the various methods used in our implementation for the three tasks. 3.1 Dataset Hindi Visual Genome, HVG: The dataset used in our work is from the HVG (Parida et al., 2019) as a part of WAT2019 MultiModal Translation Task3 . The dataset consists of a total of 31525 randomly selected images from Visual Genome (Krishna et al., 2 https://ufal.mff.cuni.cz/ hindi-visual-genome/ 3 https://ufal.mff.cuni.cz/ hindi-visual-genome/wat-2019-multimodal-task https://nlp.amrita.edu/mtil_cen/ 182 Figure 1: System Architecture Dataset distribution Training set Development set Evaluation set Challenge set Items 28932 998 1595 1400 encoder and, an alignment model paired with a LSTM in the decoder model is used. Table 1: Hindi Visual Genome dataset details. 2017) and a parallel ima"
D19-5224,D13-1176,0,0.0517129,"NIT Silchar loisanayai@gmail.com Thoudam Doren Singh Sivaji Bandyopadhyay NIT Silchar NIT Silchar thoudam.doren@gmail.com sivaji.cse.ju@gmail.com Abstract easy. The application of machine translation, can also be applied in our daily healthcare services (Wołk and Marasek, 2015; Yellowlees et al., 2015), government services, disaster management, etc. The methodology of machine translation system where the traditional statistical machine translation (SMT) (Koehn et al., 2007) is replaced by the neural machine translation (NMT) system, a MT system based on artificial neural network proposed by (Kalchbrenner and Blunsom, 2013), results to a better translation. Using deep learning and representation learning, NMT translate a source text to a target text. In the encoderdecoder model of NMT (Cho et al., 2014), the encoder encodes the input text into a fixed length of input vector and the decoder generates a sequence of words as the output text from the input vector. The system is reported to learn the linguistic regularities of both at the phrase level and word level. With the advancement in Computer Vision, the work on generating caption of an image is becoming popular. In an image caption generation model, a deep ne"
D19-5224,P17-4012,0,0.0898259,"spectively, and bf and bb as bias vectors. 184 4 Experimental Setup With the model described in Section 3, the experimental setup for each of the three tasks are explained in the Subsections below. The translation of English to Hindi on the HVG dataset is evaluated in three separate tasks: 4.2 NMT Text only Translation To carry out the experiment, the dataset from the HVG is processed as described in the following Subsection 4.1. Using the processed text data from Subsection 4.1, the translation of English-Hindi is carried out on a neural machine translation open-source tool based on OpenNMT (Klein et al., 2017). We used the attention mechanism of (Bahdanau et al., 2014). Along with other parameters such as learning rate at 0.002, Adam optimizer (Kingma and Ba, 2014), a dropout rate of 0.1, we train the system for 25 epoch. 4.1 4.3 • Using only the text dataset. • Using only the image dataset. • Using both the image and the text dataset. Dataset Preparation Our second task is to generate the caption of an image in Hindi. For this task, we trained our system (Subsection 3.4) with the processed images from Subsection 4.1 paired with its Hindi captions. For extracting the features from the image a 16-la"
D19-5224,P07-2045,0,0.0113003,"Missing"
D19-5224,W17-5717,0,0.213771,"ixed-length input vector, the encoder-decoder model of NMT suffers during the translation of long text. By introducing an attention mechanism (Bahdanau et al., 2014), the source text is no longer encoded into a fixed-length vector. Rather, the decoder attends to different parts of the source text at each step of the output generation. In their experiment (Bahdanau et al., 2014) of English to French translation task, the attention mechanism is observed to improve the translation performance of long input sentences. The NMT translation of English to Hindi is carried out by (Mahata et al., 2019; Singh et al., 2017). Mahata et al. (2019) evaluate the performance of NMT model over the SMT system as a part of MTIL20171 shared task. The author reported that NMT performs better in short sentences while SMT outperforms NMT in translating longer sentences. Sennrich et al. (2015) introduced an effective approach of preprocessing for NMT task where the text is segmented into subword units. The NMT model supports open-vocabulary translation where sequences of subword units encoded from the rare and unknown words are used. The proposed approach is reported to perform better than the back-off to a dictionary look-u"
I08-2077,W03-0430,0,0.0760769,"Missing"
I08-2077,N03-1028,0,0.0307936,"=1 k which as in HMMs, can be obtained efficiently by dynamic programming. To train a CRF, the objective function to be maximized is the penalized log-likelihood of the state sequences given the observation sequences: L∧ = N X log(P∧ (s(i) |o(i) )) − i=1 X λ2 k k 2σ 2 NE tag PER , where {< o(i) , s(i) >} is the labeled training data. The second sum corresponds to a zero-mean, σ 2 -variance Gaussian prior over parameters, which facilitates optimization by making the likelihood surface strictly convex. Here, we set parameters λ to maximize the penalized log-likelihood using Limited-memory BFGS (Sha and Pereira, 2003), a quasi-Newton method that is significantly more efficient, and which results in only minor changes in accuracy due to changes in λ. When applying CRFs to the NER problem, an observation sequence is a token of a sentence or document of text and the state sequence is its corresponding label sequence. While CRFs generally can use real-valued functions, in our experiments maximum of the features are binary valued. A feature function fk (st−1 , st , o, t) has a value of 0 for most cases and is only set to be 1, when st−1 , st are certain states and the observation has certain properties. We have"
I08-2095,E91-1028,0,\N,Missing
I08-2095,J02-1003,0,\N,Missing
I08-3012,A00-1031,0,0.0156988,"lections varies 7. The inflections may suggest mutually contraa lot among the POSs. Hence, if the system is POS aware, it will be able to generate more accurate dictory results. As for example token েখিল [kheli] result. This can be achieved by sending POS can be derived by applying two legitimate inflectagged text to the stemmer system, which will ap- tions িল [li] and ি◌ [i] on two different stems খা ply POS specific rules to discover stems. This [khaa] and েখl [khel_] respectively. Finding out the proposition is quite viable as statistical POS tag- correct stem can be tricky. gers like TnT (Brants, 2000) are available. 8. Because of contradictory rules and morphoThe context of the proposed system is provided logical similarities in different stems there will be below: ambiguities. Example: েসথা [sethaa] &lt; েস [se] (stem) + থা [thaa] (inflection representing location). This inflection is not applicable to nouns. Moreover, unlike noun, a pronoun stem may have one or more post-inflection forms. Example: stem আিম [aami] becomes আমা [aamaa] (আমােক &lt; আমা + েক) or েমা [mo] (েমােদর &lt; েমা + েদর) once inflected. 4.3 Plain Text POS Tagger Stemmed Text Tagged Text Stemmer Figure 1: Context of Proposed Ste"
I08-3015,H92-1023,0,0.633281,"032, India sivaji_cse_ju@yahoo.com of affix helps to implement the tagger for a resource poor language like Manipuri with high performance. There are many POS taggers developed using different techniques for many major languages such as transformation-based error-driven learning (Brill, 1995), decision trees (Black et al., 1992), Markov model (Cutting et al., 1992), maximum entropy methods (Ratnaparkhi, 1996) etc for English. Decision trees are used to estimate marginal probabilities in a maximum entropy model for predicting the parts-of-speech of a word given the context in which it appears (Black et al., 1992). The rules in a rule-based system are usually difficult to construct and typically are not very robust (Brill, 1992). Large tables of statistics are not needed for the rule-based tagger. In a stochastic tagger, tens of thousands of lines of statistical information are needed to capture the contextual information (Brill, 1992). For a tagger to function as a practical component in a language processing system, a tagger must be robust, efficient, accurate, tunable and reusable (Cutting, 1992). Abstract A good POS tagger is a critical component of a machine translation system and other related NL"
I08-3015,H92-1022,0,0.0775961,"gh performance. There are many POS taggers developed using different techniques for many major languages such as transformation-based error-driven learning (Brill, 1995), decision trees (Black et al., 1992), Markov model (Cutting et al., 1992), maximum entropy methods (Ratnaparkhi, 1996) etc for English. Decision trees are used to estimate marginal probabilities in a maximum entropy model for predicting the parts-of-speech of a word given the context in which it appears (Black et al., 1992). The rules in a rule-based system are usually difficult to construct and typically are not very robust (Brill, 1992). Large tables of statistics are not needed for the rule-based tagger. In a stochastic tagger, tens of thousands of lines of statistical information are needed to capture the contextual information (Brill, 1992). For a tagger to function as a practical component in a language processing system, a tagger must be robust, efficient, accurate, tunable and reusable (Cutting, 1992). Abstract A good POS tagger is a critical component of a machine translation system and other related NLP applications where an appropriate POS tag will be assigned to individual words in a collection of texts. There is n"
I08-3015,J95-4004,0,0.623511,"Missing"
I08-3015,A92-1018,0,0.31908,"m entropy model for predicting the parts-of-speech of a word given the context in which it appears (Black et al., 1992). The rules in a rule-based system are usually difficult to construct and typically are not very robust (Brill, 1992). Large tables of statistics are not needed for the rule-based tagger. In a stochastic tagger, tens of thousands of lines of statistical information are needed to capture the contextual information (Brill, 1992). For a tagger to function as a practical component in a language processing system, a tagger must be robust, efficient, accurate, tunable and reusable (Cutting, 1992). Abstract A good POS tagger is a critical component of a machine translation system and other related NLP applications where an appropriate POS tag will be assigned to individual words in a collection of texts. There is not enough POS tagged corpus available in Manipuri language ruling out machine learning approaches for a POS tagger in the language. A morphology driven Manipuri POS tagger that uses three dictionaries containing root words, prefixes and suffixes has been designed and implemented using the affix information irrespective of the context of the words. We have tested the current P"
I08-3015,A94-1024,0,0.0208196,"onosyllabic, influenced and enriched by the Indo-Aryan languages of Sanskrit origin and English. The affixes play the most important role in the structure of the language. A clear -cut demarcation between morphology and syntax is not possible. In Manipuri, words are formed in three processes called affixation, derivation and compounding (Thoudam, 2006). The majority of the roots found in the language are bound and the affixes are the determining factor of the class of the words in the language. Classification of words using the role Morphology based POS tagging of some languages like Turkish (Oflazer and Kuruoz, 1994), Czech (Hajic, et al., 2001) has been tried out using a combination of hand-crafted rules and statistical learning. A Marathi rule based POS tagger used a technique called SRR (suffix replacement rule) (Burange et al., 2006) with considerable accuracy. A POS tagger for Hindi overcomes the handicap of annotated corpora scarcity by exploiting the rich morphology of the language (Singh et al., 2006). To the best of our knowledge, there is no record available of work done on a Manipuri POS tagger. A related work of word class and sentence type identification in a Manipuri Morphological Analyzer 9"
I08-3015,P06-2100,0,0.0735051,"e bound and the affixes are the determining factor of the class of the words in the language. Classification of words using the role Morphology based POS tagging of some languages like Turkish (Oflazer and Kuruoz, 1994), Czech (Hajic, et al., 2001) has been tried out using a combination of hand-crafted rules and statistical learning. A Marathi rule based POS tagger used a technique called SRR (suffix replacement rule) (Burange et al., 2006) with considerable accuracy. A POS tagger for Hindi overcomes the handicap of annotated corpora scarcity by exploiting the rich morphology of the language (Singh et al., 2006). To the best of our knowledge, there is no record available of work done on a Manipuri POS tagger. A related work of word class and sentence type identification in a Manipuri Morphological Analyzer 91 Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 91–98, c Hyderabad, India, January 2008. 2008 Asian Federation of Natural Language Processing is found in (Thoudam and Bandyopadhyay, 2006) where the classification of few word categories and sentence type identification are discussed based on affix rules. determined on the basis of position in the word (category 1"
I08-5006,W03-2201,0,0.133497,"Missing"
I08-5006,W97-0312,0,0.0825691,"Missing"
I08-5006,N03-1028,0,0.349029,"features for named entity recognition in Bengali, Hindi, Telugu, Oriya and Urdu. 2 Conditional Random Fields Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models, a special case of which corresponds to conditionally trained probabilistic finite state automata. Being conditionally trained, these CRFs can easily incorporate a large number of arbitrary, nonindependent features while still having efficient procedures for non-greedy finite-state inference and training. CRFs have shown success in various sequence modeling tasks including noun phrase segmentation (Sha and Pereira, 2003) and table extraction (Pinto et al., 2003). CRFs are used to calculate the conditional probability of values on designated output nodes given values on other designated input nodes. The conditional probability of a state sequence S s1, s 2, , sT given an observation sequence O o1, o 2, ....., oT ) is calculated as: T 1 exp( kfk (st 1, st , o, t )), where Zo t 1 k fk ( st 1, st , o, t ) is a feature function whose weight P ( s |o) k is to be learned via training. The values of the feature functions may range between ..... , but typically they are binary. To make all conditional probabilities su"
I08-5006,W03-0430,0,0.0522082,"is much cheaper than that of a rule-based one. The representative machine-learning approaches used in NER are HMM (BBN’s IdentiFinder in (Bikel, 1999)), Maximum Entropy http://ltrc.iiit.ac.in/ner-ssea-08 33 Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 33–40, c Hyderabad, India, January 2008. 2008 Asian Federation of Natural Language Processing (New York University’s MENE in (Borthwick, 1999)), Decision Tree (New York University’s system in (Sekine 1998), SRA’s system in (Bennet, 1997) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; McCallum and Li, 2003). There is no concept of capitalization in Indian languages (ILs) like English and this fact makes the NER task more difficult and challenging in ILs. There has been very little work in the area of NER in Indian languages. In Indian languages particularly in Bengali, the work in NER can be found in (Ekbal and Bandyopadhyay, 2007a) and (Ekbal and Bandyopadhyay, 2007b). These two systems are based on the pattern directed shallow parsing approach. An HMM-based NER in Bengali can be found in (Ekbal et al., 2007c). Other than Bengali, the work on NER can be found in (Li and McCallum, 2004) for Hind"
I08-5006,M98-1019,0,\N,Missing
I08-5006,M98-1020,0,\N,Missing
I08-5008,N01-1025,0,0.0107121,"t it is trainable and adoptable and the maintenance of a machine-learning system is much cheaper than that of a rule-based one. The representative machine-learning approaches used in NER are Hidden Markov Model (HMM) (BBN’s IdentiFinder in (Bikel, 1999)), Maximum Entropy (New York University’s MEME in (Borthwick, 1999)), Decision Tree (New York University’s system in (Sekine, 1998) and Conditional Random Fields (CRFs) (Lafferty et al., 2001). Support Vector Machines (SVMs) based NER system was proposed by Yamada et al. (2002) for Japanese. His system is an extension of Kudo’s chunking system (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks. The other SVM-based NER systems can be found in (Takeuchi and Collier, 2002) and (Asahara and Matsumoto, 2003). Named entity identification in Indian languages in general and particularly in Bengali is difficult and challenging. In English, the NE always appears with capitalized letter but there is no concept of capitalization in Bengali. There has been a very 51 Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 51–58, c Hyderabad, India, January 2008. 2008 Asian Federation of Natural Languag"
I08-5008,W00-0730,0,0.0137702,"Missing"
I08-5008,N03-1002,0,0.0160218,"roaches used in NER are Hidden Markov Model (HMM) (BBN’s IdentiFinder in (Bikel, 1999)), Maximum Entropy (New York University’s MEME in (Borthwick, 1999)), Decision Tree (New York University’s system in (Sekine, 1998) and Conditional Random Fields (CRFs) (Lafferty et al., 2001). Support Vector Machines (SVMs) based NER system was proposed by Yamada et al. (2002) for Japanese. His system is an extension of Kudo’s chunking system (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks. The other SVM-based NER systems can be found in (Takeuchi and Collier, 2002) and (Asahara and Matsumoto, 2003). Named entity identification in Indian languages in general and particularly in Bengali is difficult and challenging. In English, the NE always appears with capitalized letter but there is no concept of capitalization in Bengali. There has been a very 51 Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 51–58, c Hyderabad, India, January 2008. 2008 Asian Federation of Natural Language Processing little work in the area of NER in Indian languages. In Indian languages, particularly in Bengali, the works in NER can be found in (Ekbal and Bandyopadhyay,"
I08-5008,W03-2201,0,0.00846914,"classification of NEs are very crucial and pose a very big challenge to the NLP researchers. The level of ambiguity in NER makes it difficult to attain human performance NER has drawn more and more attention from the NE tasks (Chinchor 95; Chinchor 98) in Message Understanding Conferences (MUCs) [MUC6; MUC7]. The problem of correct identification of NEs is specifically addressed and benchmarked by the developers of Information Extraction System, such as the GATE system (Cunningham, 2001). NER also finds application in question-answering systems (Maldovan et al., 2002) and machine translation (Babych and Hartley, 2003). The current trend in NER is to use the machinelearning approach, which is more attractive in that it is trainable and adoptable and the maintenance of a machine-learning system is much cheaper than that of a rule-based one. The representative machine-learning approaches used in NER are Hidden Markov Model (HMM) (BBN’s IdentiFinder in (Bikel, 1999)), Maximum Entropy (New York University’s MEME in (Borthwick, 1999)), Decision Tree (New York University’s system in (Sekine, 1998) and Conditional Random Fields (CRFs) (Lafferty et al., 2001). Support Vector Machines (SVMs) based NER system was pro"
I08-5008,W02-2029,0,0.0424697,"presentative machine-learning approaches used in NER are Hidden Markov Model (HMM) (BBN’s IdentiFinder in (Bikel, 1999)), Maximum Entropy (New York University’s MEME in (Borthwick, 1999)), Decision Tree (New York University’s system in (Sekine, 1998) and Conditional Random Fields (CRFs) (Lafferty et al., 2001). Support Vector Machines (SVMs) based NER system was proposed by Yamada et al. (2002) for Japanese. His system is an extension of Kudo’s chunking system (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks. The other SVM-based NER systems can be found in (Takeuchi and Collier, 2002) and (Asahara and Matsumoto, 2003). Named entity identification in Indian languages in general and particularly in Bengali is difficult and challenging. In English, the NE always appears with capitalized letter but there is no concept of capitalization in Bengali. There has been a very 51 Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 51–58, c Hyderabad, India, January 2008. 2008 Asian Federation of Natural Language Processing little work in the area of NER in Indian languages. In Indian languages, particularly in Bengali, the works in NER can be f"
I08-5008,M98-1019,0,\N,Missing
I08-5008,M98-1020,0,\N,Missing
I08-6008,W97-0707,0,0.119306,"Missing"
I08-6008,P02-1058,0,\N,Missing
I08-7001,bertagna-etal-2004-content,0,0.0157974,"use is not in the familiar territory of computational linguistics. The web walked into the ACL meetings started in 1999. The use of the web as a corpus for teaching and research on language technology has been proposed a number of times (Rundel, 2000; Fletcher, 2001; Robb, 2003; Fletcher, 2003). There is a long history of creating a standard for western language resources. The human language technology (HLT) society in Europe has been particularly zealous for the standardization, making a series of attempts such as EAGLES3, PROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Calzolari et al., 2003; Bertagna et al., 2004) and more recently multilingual lexical database generation from parallel texts in 20 European languages (Giguet and Luquet, 2006). On the other hand, in spite of having great linguistic and cultural diversities, Asian language resources have received much less attention than their western counterparts. A new project (Takenobou et al., 2006) has been started to create a common standard for Asian language resources. They have extended an existing description framework, the sourceforge.net/project/nlp-sanchay http://ltrc.iiit.ac.in/ner-ssea-08 3 1 http://www.ilc.cnr.it/Eagles96/home.html The 6th"
I08-7001,P06-2035,0,0.0275213,"of the web as a corpus for teaching and research on language technology has been proposed a number of times (Rundel, 2000; Fletcher, 2001; Robb, 2003; Fletcher, 2003). There is a long history of creating a standard for western language resources. The human language technology (HLT) society in Europe has been particularly zealous for the standardization, making a series of attempts such as EAGLES3, PROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Calzolari et al., 2003; Bertagna et al., 2004) and more recently multilingual lexical database generation from parallel texts in 20 European languages (Giguet and Luquet, 2006). On the other hand, in spite of having great linguistic and cultural diversities, Asian language resources have received much less attention than their western counterparts. A new project (Takenobou et al., 2006) has been started to create a common standard for Asian language resources. They have extended an existing description framework, the sourceforge.net/project/nlp-sanchay http://ltrc.iiit.ac.in/ner-ssea-08 3 1 http://www.ilc.cnr.it/Eagles96/home.html The 6th Workshop on Asian Languae Resources, 2008 tion of the corpus has been manually annotated with the twelve NE tags as part of the I"
I08-7001,bel-etal-2000-simple,0,0.0119807,"ge of research activities. The web is anarchic, and its use is not in the familiar territory of computational linguistics. The web walked into the ACL meetings started in 1999. The use of the web as a corpus for teaching and research on language technology has been proposed a number of times (Rundel, 2000; Fletcher, 2001; Robb, 2003; Fletcher, 2003). There is a long history of creating a standard for western language resources. The human language technology (HLT) society in Europe has been particularly zealous for the standardization, making a series of attempts such as EAGLES3, PROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Calzolari et al., 2003; Bertagna et al., 2004) and more recently multilingual lexical database generation from parallel texts in 20 European languages (Giguet and Luquet, 2006). On the other hand, in spite of having great linguistic and cultural diversities, Asian language resources have received much less attention than their western counterparts. A new project (Takenobou et al., 2006) has been started to create a common standard for Asian language resources. They have extended an existing description framework, the sourceforge.net/project/nlp-sanchay http://ltrc.iiit.ac.in/ner-s"
I08-7001,C02-1025,0,\N,Missing
I08-7001,P06-2106,0,\N,Missing
I11-1146,N10-1029,0,0.0257945,"o the existing phrase-based model. 2 Related Works Koehn and Knight (2003) discussed empirical methods for compound splitting by learning rules from monolingual and parallel corpora. Lambert and Banchs (2005) proposed technique for extracting bilingual MWEs based on grouping as units before performing the statistical alignment. (Ren et al., 2009) presented the Log Likelihood Ratio based hierarchical reducing algorithm to automatically extract bilingual MWE and investigated the performance of three different strategies in applying bilingual MWEs for SMT system using Moses (Koehn et al., 2007). Carpuat and Diab (2010) explored static integration strategy that segments training and test sentences according to the MWE vocabulary, and dynamic integration strategy that adds a new MWE-based feature in SMT translation lexicon. Handling of 1305 named entities and compound verbs in PBSMT (Pal et al., 2010) has been reported in the EnglishBengali task in the Indian language context. They established prior NE alignments in the parallel corpora by transliterating source NEs into the target language using modified joint source channel transliteration technique (Ekbal et al., 2006) which incorporates different contextu"
I11-1146,P06-2025,1,0.875579,"using Moses (Koehn et al., 2007). Carpuat and Diab (2010) explored static integration strategy that segments training and test sentences according to the MWE vocabulary, and dynamic integration strategy that adds a new MWE-based feature in SMT translation lexicon. Handling of 1305 named entities and compound verbs in PBSMT (Pal et al., 2010) has been reported in the EnglishBengali task in the Indian language context. They established prior NE alignments in the parallel corpora by transliterating source NEs into the target language using modified joint source channel transliteration technique (Ekbal et al., 2006) which incorporates different contextual information into the model. In the process, the identified NEs and compound verbs are converted into a single token by replacing spaces between the constituent words by underscores. 3 are the examples of enrichment of Manipuri languages using Manipuri transliteration from English. Manipuri-English Parallel Corpora The Manipuri-English Parallel corpus (Singh and Bandyopadhyay, 2010b) on News Domain is used for training and testing. The Manipuri news corpus is collected from the website http://www.thesangaiexpress.com/. On analyzing the corpus, the statis"
I11-1146,2005.mtsummit-posters.11,0,0.0262559,"ent models used. Integrating MWEs into the Machine Translation (MT) systems in general and phrase based Statistical Machine Translation (PBSMT) system in particular is a critical problem. Thus, there is the need for identifying Manipuri RMWE and integrating into the PBSMT system. In the present work, we integrate Manipuri Named entities (both single word and multiword), RMWEs and non-NE transliterated entities into the existing phrase-based model. 2 Related Works Koehn and Knight (2003) discussed empirical methods for compound splitting by learning rules from monolingual and parallel corpora. Lambert and Banchs (2005) proposed technique for extracting bilingual MWEs based on grouping as units before performing the statistical alignment. (Ren et al., 2009) presented the Log Likelihood Ratio based hierarchical reducing algorithm to automatically extract bilingual MWE and investigated the performance of three different strategies in applying bilingual MWEs for SMT system using Moses (Koehn et al., 2007). Carpuat and Diab (2010) explored static integration strategy that segments training and test sentences according to the MWE vocabulary, and dynamic integration strategy that adds a new MWE-based feature in SM"
I11-1146,W04-3250,0,0.020064,"tic scores of various models are given in the table below: Model BLEU NIST Baseline 13.452 4.31 Baseline + RMWE 13.829 4.43 Baseline +NE 13.901 4.47 Baseline + Transliterated 13.911 4.21 non-NE Baseline + RMWE + NE + 15.023 5.21 Transliterated non-NE Table 5: Automatic Scores of Manipuri-English SMT systems the system reflects a change in the overall translation quality. It is found that the difference between the baseline and the (Baseline + RMWE + NE + Transliterated non-NE) model is significant producing statistically significant improvements as measured by the bootstrap resampling method (Koehn, 2004) on BLEU. Manipuri Fluency Adequacy Sentence length Baseline <=15 words 1.93 2.31 >15 words 1.51 1.76 Baseline+ <=15 words 1.92 2.85 Level Interpretation RMWE >15 words 1.62 2.07 4 Flawless with no grammatical error Baseline + <=15 words 2.06 2.82 3 Good output with minor errors NE >15 words 1.75 2.10 2 Disfluent ungrammatical with correct phrase Baseline + <=15 words 2.10 2.79 1 Incomprehensible Transliterated >15 words 1.67 2.10 <=15 words 2.11 3.11 >15 words 2.78 non-NE Table 6: Fluency scale Level Interpretation 4 Full meaning is conveyed 3 Most of the meaning is conveyed 2 Poor meaning is"
I11-1146,P07-2045,0,0.00998355,"iterated entities into the existing phrase-based model. 2 Related Works Koehn and Knight (2003) discussed empirical methods for compound splitting by learning rules from monolingual and parallel corpora. Lambert and Banchs (2005) proposed technique for extracting bilingual MWEs based on grouping as units before performing the statistical alignment. (Ren et al., 2009) presented the Log Likelihood Ratio based hierarchical reducing algorithm to automatically extract bilingual MWE and investigated the performance of three different strategies in applying bilingual MWEs for SMT system using Moses (Koehn et al., 2007). Carpuat and Diab (2010) explored static integration strategy that segments training and test sentences according to the MWE vocabulary, and dynamic integration strategy that adds a new MWE-based feature in SMT translation lexicon. Handling of 1305 named entities and compound verbs in PBSMT (Pal et al., 2010) has been reported in the EnglishBengali task in the Indian language context. They established prior NE alignments in the parallel corpora by transliterating source NEs into the target language using modified joint source channel transliteration technique (Ekbal et al., 2006) which incorp"
I11-1146,P03-1021,0,0.0211177,"built using the MWEs extracted using the techniques described in section 5 and 6. For simplicity, the probability 1 is assigned to all the four probabilities of the MWE phrase table. During the decoding process, the MWEs are searched in both the phrase tables. One of the possible techniques of integrating MWE in the SMT system is by introducing a new feature in a phrase table that indicates the presence of MWE. 9 where, ℎ? and ?? denote the ith feature function and weight respectively. The feature weight λi in the log linear model is determined by using the minimum error rate training method (Och, 2003). Intuitively, the ?(?|?) depend on language model — P(e) and translation model — P(f|e). 10 Experimental Setup The first Manipuri-English SMT task is reported in (Singh and Bandyopadhyay, 2010c) on news domain using factored translation model demonstrating improvement not only in the BLEU and NIST scores but also improvement in the fluency and adequacy by subjective evaluation method. Earlier, an English-Manipuri SMT system using morpho-syntactic and semantic information is reported by (Singh and Bandyopadhyay, 2010d). In the present experimental setup, Moses decoder (Koehn et al., 2005) is u"
I11-1146,J03-1002,0,0.0056874,"Missing"
I11-1146,W10-3707,1,0.873041,"Missing"
I11-1146,P02-1040,0,0.0833697,"Missing"
I11-1146,W09-2907,0,0.0217994,"em in particular is a critical problem. Thus, there is the need for identifying Manipuri RMWE and integrating into the PBSMT system. In the present work, we integrate Manipuri Named entities (both single word and multiword), RMWEs and non-NE transliterated entities into the existing phrase-based model. 2 Related Works Koehn and Knight (2003) discussed empirical methods for compound splitting by learning rules from monolingual and parallel corpora. Lambert and Banchs (2005) proposed technique for extracting bilingual MWEs based on grouping as units before performing the statistical alignment. (Ren et al., 2009) presented the Log Likelihood Ratio based hierarchical reducing algorithm to automatically extract bilingual MWE and investigated the performance of three different strategies in applying bilingual MWEs for SMT system using Moses (Koehn et al., 2007). Carpuat and Diab (2010) explored static integration strategy that segments training and test sentences according to the MWE vocabulary, and dynamic integration strategy that adds a new MWE-based feature in SMT translation lexicon. Handling of 1305 named entities and compound verbs in PBSMT (Pal et al., 2010) has been reported in the EnglishBengal"
I11-1146,W10-3605,1,0.505942,"e table. During the decoding process, the MWEs are searched in both the phrase tables. One of the possible techniques of integrating MWE in the SMT system is by introducing a new feature in a phrase table that indicates the presence of MWE. 9 where, ℎ? and ?? denote the ith feature function and weight respectively. The feature weight λi in the log linear model is determined by using the minimum error rate training method (Och, 2003). Intuitively, the ?(?|?) depend on language model — P(e) and translation model — P(f|e). 10 Experimental Setup The first Manipuri-English SMT task is reported in (Singh and Bandyopadhyay, 2010c) on news domain using factored translation model demonstrating improvement not only in the BLEU and NIST scores but also improvement in the fluency and adequacy by subjective evaluation method. Earlier, an English-Manipuri SMT system using morpho-syntactic and semantic information is reported by (Singh and Bandyopadhyay, 2010d). In the present experimental setup, Moses decoder (Koehn et al., 2005) is used which can support multiple phrase tables. The target language model is developed using the SRILM (Stolcke, 2002) toolkit. The language model is the 4-gram model using Kneser-Ney smoothing ("
I11-1146,W10-3811,1,0.720621,"e table. During the decoding process, the MWEs are searched in both the phrase tables. One of the possible techniques of integrating MWE in the SMT system is by introducing a new feature in a phrase table that indicates the presence of MWE. 9 where, ℎ? and ?? denote the ith feature function and weight respectively. The feature weight λi in the log linear model is determined by using the minimum error rate training method (Och, 2003). Intuitively, the ?(?|?) depend on language model — P(e) and translation model — P(f|e). 10 Experimental Setup The first Manipuri-English SMT task is reported in (Singh and Bandyopadhyay, 2010c) on news domain using factored translation model demonstrating improvement not only in the BLEU and NIST scores but also improvement in the fluency and adequacy by subjective evaluation method. Earlier, an English-Manipuri SMT system using morpho-syntactic and semantic information is reported by (Singh and Bandyopadhyay, 2010d). In the present experimental setup, Moses decoder (Koehn et al., 2005) is used which can support multiple phrase tables. The target language model is developed using the SRILM (Stolcke, 2002) toolkit. The language model is the 4-gram model using Kneser-Ney smoothing ("
I11-1146,Y09-2045,1,\N,Missing
I11-1146,E03-1076,0,\N,Missing
I13-1078,W10-3208,1,0.846591,"on developing sentiment lexicon with three sentiment classes. For instance, Takamura et al. (2005) have developed a lexicon of emotion words tagged with the classes desirable and undesirable using Spin model. A number of other polarity sentiment lexicons are available in English such as SentiWordNet 3.0 (Esuli et al., 2010), Subjectivity Word List (Wilson et al., 2005), WordNet-Affect list (Strapparava et al., 2004), Taboada‟s adjective list (Taboada et al., 2006). On the other hand, several polarity sentiment lexicons have been developed in different languages like Hindi, Bengali and Telegu (Das and Bandyopadhyay, 2010), Japanese (Torii et al., 2012) etc. Among all these publicly available sentiment lexicons, SentiWordNet is one of the well-known and widely used ones (number of citations is higher than other resources 1 ), having been uti1 http://citeseerx.ist.psu.edu/index 674 International Joint Conference on Natural Language Processing, pages 674–679, Nagoya, Japan, 14-18 October 2013. lized in several applications such as sentiment analysis, opinion mining and emotion analysis. Undoubtedly, manual compilation is the best way to create such an emotion lexicon but is much expensive in terms of time and hum"
I13-1078,esuli-sebastiani-2006-sentiwordnet,0,0.13151,"Missing"
I13-1078,strapparava-valitutti-2004-wordnet,0,0.533599,"Missing"
I13-1078,P07-2034,0,0.0562995,"Missing"
I13-1078,P05-1017,1,0.957128,"ional classes like happy, sad, fear, anger, surprise and disgust. In the previous example, frequent appearance of words from the happy class in a blog document would imply that the writer of the comment is quite happy with the new rule proposed by the Government. Several works have been conducted on building emotional corpora in different languages such as in English (Aman and Szpakowicz, 2007), Chinese (Yang et al., 2007; Quan and Ren, 2010), and Bengali (Das and Bandyopadhyay, 2010) etc. All these works have focused on developing sentiment lexicon with three sentiment classes. For instance, Takamura et al. (2005) have developed a lexicon of emotion words tagged with the classes desirable and undesirable using Spin model. A number of other polarity sentiment lexicons are available in English such as SentiWordNet 3.0 (Esuli et al., 2010), Subjectivity Word List (Wilson et al., 2005), WordNet-Affect list (Strapparava et al., 2004), Taboada‟s adjective list (Taboada et al., 2006). On the other hand, several polarity sentiment lexicons have been developed in different languages like Hindi, Bengali and Telegu (Das and Bandyopadhyay, 2010), Japanese (Torii et al., 2012) etc. Among all these publicly availabl"
I13-1078,N07-1037,1,0.824129,"ives if the adjectives appear in a conjunctive form in the corpus. If the adjectives are connected by “and”, the link belongs to SL. If they are connected by “but”, the link belongs to DL. We call this network the gloss-thesaurus-corpus network (GTC). We have used gloss-thesauruscorpus network in our experiments. ferent values of β. Then the accuracies were computed manually as well as using the WordNet Affect lexicon. We also classified the words into two classes, i.e. positive and negative. The accuracies of two classes were calculated using the SentiWordNet. 4.2 5.1 Classification of Words Takamura et al., (2007) used the Potts model for extracting semantic orientation of phrases (pair of adjective and a noun): positive, negative or neutral. In contrast to that, we have used the Potts model for identifying the emotional class (es) of a word. We have used one seed word from each class to start with the experiment. Each seed word is assigned a class manually. We therefore estimate the state of nodes in the lexical network for each class of emotions. The only drawback is that, it could not assign any emotional class to a word which is not present in the lexical network. These words may be referred to as"
I13-1078,J11-2001,0,0.100786,"Missing"
I13-1078,baccianella-etal-2010-sentiwordnet,0,0.423122,"Missing"
I13-1078,H05-1044,0,0.0227693,"ks have been conducted on building emotional corpora in different languages such as in English (Aman and Szpakowicz, 2007), Chinese (Yang et al., 2007; Quan and Ren, 2010), and Bengali (Das and Bandyopadhyay, 2010) etc. All these works have focused on developing sentiment lexicon with three sentiment classes. For instance, Takamura et al. (2005) have developed a lexicon of emotion words tagged with the classes desirable and undesirable using Spin model. A number of other polarity sentiment lexicons are available in English such as SentiWordNet 3.0 (Esuli et al., 2010), Subjectivity Word List (Wilson et al., 2005), WordNet-Affect list (Strapparava et al., 2004), Taboada‟s adjective list (Taboada et al., 2006). On the other hand, several polarity sentiment lexicons have been developed in different languages like Hindi, Bengali and Telegu (Das and Bandyopadhyay, 2010), Japanese (Torii et al., 2012) etc. Among all these publicly available sentiment lexicons, SentiWordNet is one of the well-known and widely used ones (number of citations is higher than other resources 1 ), having been uti1 http://citeseerx.ist.psu.edu/index 674 International Joint Conference on Natural Language Processing, pages 674–679, N"
I13-1078,P97-1023,0,0.0600689,"ork is shown in Table 1. Next, we assign weights W = (wij) to links as follows: By minimizing F(n) under the condition that ( ) ,∑ , we obtain the following fixed point equation for : ( ) Potts Model for Construction of Emotional Lexicon No. of words 20497 3751 55285 8482 Table 1. Statistics of Lexical network We have also constructed another network, the gloss thesaurus network (GT), by linking syno676 nyms, antonyms and hypernyms, in addition to the above linked words. Only antonym links are in DL. We enhanced the gloss-thesaurus network with co-occurrence information extracted from corpus. Hatzivassiloglou and McKeown (1997) focused on conjunctive expressions such as “simple and well-received” and “simplistic but well-received”, where the former pair of words tend to have the same semantic orientation, and the latter tend to have the opposite orientation. Following their method, we connect two adjectives if the adjectives appear in a conjunctive form in the corpus. If the adjectives are connected by “and”, the link belongs to SL. If they are connected by “but”, the link belongs to DL. We call this network the gloss-thesaurus-corpus network (GTC). We have used gloss-thesauruscorpus network in our experiments. fere"
I13-1078,W11-1710,1,0.822398,"Missing"
I13-1078,taboada-etal-2006-methods,0,\N,Missing
I13-1113,W12-5003,1,0.801642,"s. The features in question classification task can be categorized into three different types: lexical, syntactical and semantic features (Loni, 2011). Loni et al. (2011) also represented a question in the QC task similar to document representation in vector space model, i.e., a question is a vector which is described by the words inside it. Therefore a question Q can be represented as: Q = (W1 , W2 , W3 , ..., WN −1 , WN ) Where, WK = frequency of term K in question Q, and N = total number of Terms We have also used three types of features for QC. We use the same features previously used by (Banerjee and Bandyopadhyay, 2012). Lexical features (fLex ): wh-word, wh-word positions, wh-type, question length, end marker, word shape. Syntactical features (fSyn ): POS tags, head word. Semantic features (fSem ): related words, named entity. 5 Experiments 6.2 Results In total thirteen different experiments have been performed. Four different experiments have been performed for each bagging and boosting. So, altogether eight different experiments have been performed for the ensemble approach. Four different experiments have been performed for stacking. But for voting, a single experiment has been performed. Actually, each"
I13-1113,W12-6002,1,0.849493,"s. The features in question classification task can be categorized into three different types: lexical, syntactical and semantic features (Loni, 2011). Loni et al. (2011) also represented a question in the QC task similar to document representation in vector space model, i.e., a question is a vector which is described by the words inside it. Therefore a question Q can be represented as: Q = (W1 , W2 , W3 , ..., WN −1 , WN ) Where, WK = frequency of term K in question Q, and N = total number of Terms We have also used three types of features for QC. We use the same features previously used by (Banerjee and Bandyopadhyay, 2012). Lexical features (fLex ): wh-word, wh-word positions, wh-type, question length, end marker, word shape. Syntactical features (fSyn ): POS tags, head word. Semantic features (fSem ): related words, named entity. 5 Experiments 6.2 Results In total thirteen different experiments have been performed. Four different experiments have been performed for each bagging and boosting. So, altogether eight different experiments have been performed for the ensemble approach. Four different experiments have been performed for stacking. But for voting, a single experiment has been performed. Actually, each"
I13-1113,D08-1097,0,0.0609337,"Missing"
I13-1113,H01-1069,0,\N,Missing
O10-2008,A00-2004,0,0.226136,"ons among words in a sentence. The current trend in the emotion analysis area is exploring machine learning techniques [28], which consider the problem as text categorization or analogous to topic classification that underscores the difference between machine learning methods and human-produced baseline models [29]. Affective text shared task on news headlines at SemEval 2007 for emotion and valence level identification [20] has drawn the focus to this field. Prior work in identification of opinion holders has sometimes identified only a single opinion per sentence [30], and sometimes several [1]. Identification of opinion holders for Question Answering with supporting annotation task was attempted in the very beginning [17]. Before that, another work on labeling the arguments of the verbs with their semantic roles using a novel frame matching technique was carried out in [31]. Based on the traditional perspectives, another work discussed in [35] uses an emotion knowledge base for extracting emotion holder. The machine learning based classification task for “not holder”, “weak holder”, “medium holder”, or “strong holder” is described in [34]. Kim and Hovy [11] identified opinion holde"
O10-2008,P09-2038,1,0.870621,"said that he was remembering this beautiful comic while reading your poem. Emotional Expression: সুnর েকৗতুক (beautiful comic), Holder: Topic: কিবতা (poem) < writer, রােশদ (Rashed) &gt;, The information of emotion is useful for the domain of Question Answering (QA), Information Retrieval (IR), product reviews, social media, stock markets, and customer relationship management. Especially, the blog posts contain instant views, updated views or influenced views regarding single or multiple topics. Blogs are the communicative and informative repository of text based emotional contents in the Web 2.0 [2]. Researches on emotion show that blogs play the role of a substrate to analyze the reactions of different emotional enzymes. Many blogs act as online diary of the blogger reporting on the blogger’s daily activities and surroundings. Sometimes, blog posts are annotated by other bloggers. Large blog data set is also suitable for machine learning models. Thus the present task deals with the identification of users’ emotion on different topics from a Bengali blog corpus [7]. Each sentence of the Bengali blog corpus is annotated with the emotional components such as emotional expression (word/phra"
O10-2008,W10-3207,1,0.792524,"e or multiple topics. Blogs are the communicative and informative repository of text based emotional contents in the Web 2.0 [2]. Researches on emotion show that blogs play the role of a substrate to analyze the reactions of different emotional enzymes. Many blogs act as online diary of the blogger reporting on the blogger’s daily activities and surroundings. Sometimes, blog posts are annotated by other bloggers. Large blog data set is also suitable for machine learning models. Thus the present task deals with the identification of users’ emotion on different topics from a Bengali blog corpus [7]. Each sentence of the Bengali blog corpus is annotated with the emotional components such as emotional expression (word/phrase), intensity, associated holder and topic(s). Ekman’s six emotion classes (anger, disgust, fear, happy, sad and surprise) along with three types of intensities (high, general and low) are annotated at sentence level. The 3367 sentences with respect to eight different potential topics along with 22 different blog users are considered for conducting the present task. The emotion topic annotated in each of the sentences is directly linked with the topic of the document th"
O10-2008,esuli-sebastiani-2006-sentiwordnet,0,0.0117151,"e pre-processed to build up a general list that contains all member verbs and their available syntactic frames with holder and topic related thematic information (e.g. Experiencer, Agent, Actor, Beneficiary and Topic, Theme, Event etc.). The pre-processed list is searched to acquire the syntactic frames of each verb. Semantic Features Emotion/Affect Words (EW): The presence of a word in the WordNet Affect lists [6] identifies the emotion/affect words. Intensifiers (INTF): The Bengali SentiWordNet is being developed by replacing each word entry in the synonymous set of the English SentiWordNet [9] by its possible set of Bengali 357 synsets using a synset based English to Bengali bilingual dictionary being developed as part of the EILMT project5. The chunks containing JJ (adjective) and RB (adverb) tagged elements are considered as intensifiers. If the intensifier is found in the SentiWordNet, then the positive and negative scores of the intensifier are retrieved from the SentiWordNet. The intensifier is classified into the list of positive (pos) (INTFpos) or negative (neg) (INTFneg) for which the average retrieved score is higher. Multiword Expressions: Reduplication (সn সn sanda sanda"
O10-2008,W06-0301,0,0.114255,"In psychology and common use, emotion is an aspect of a person&apos;s mental state of being, normally based in or tied to the person’s internal (physical) and external (social) sensory feeling [21]. The source or holder of an emotional expression is the speaker or writer or experiencer [17]. Extraction of emotion holder is important for discriminating between emotions that are viewed from different perspectives [22]. By grouping opinion holders of different stance on diverse social and political issues, we can a have better understanding of the relationships among countries or among organizations [11]. Topic is the real world object, event, or abstract entity that is the primary subject of the emotion or opinion as intended by the holder and the topic depends on the context in which its associated emotional expression occurs [15]. The following Bengali sentence shows the emotional expression, its associated holder and topic. As the sentence is collected from a blog post, the writer is also considered as a default holder [17]. Example 1: 350 Proceedings of the 22nd Conference on Computational Linguistics and Speech Processing (ROCLING 2010), Pages 350-363, Puli, Nantou,Taiwan, September 201"
O10-2008,stoyanov-cardie-2008-annotating,0,0.201407,"xpression is the speaker or writer or experiencer [17]. Extraction of emotion holder is important for discriminating between emotions that are viewed from different perspectives [22]. By grouping opinion holders of different stance on diverse social and political issues, we can a have better understanding of the relationships among countries or among organizations [11]. Topic is the real world object, event, or abstract entity that is the primary subject of the emotion or opinion as intended by the holder and the topic depends on the context in which its associated emotional expression occurs [15]. The following Bengali sentence shows the emotional expression, its associated holder and topic. As the sentence is collected from a blog post, the writer is also considered as a default holder [17]. Example 1: 350 Proceedings of the 22nd Conference on Computational Linguistics and Speech Processing (ROCLING 2010), Pages 350-363, Puli, Nantou,Taiwan, September 2010. রােশদ বেলেছন আপনার কিবতাটা পড়েত িগেয় (Rashed) (bolechen) (apnar) (kobitata) (porte) (giye) তার ei সুnর েকৗতুকটা মেন পড়িছেলা। (tar) (ei) (sundar) (koutukta) (mone) (porchilo). Rashed said that he was remembering this beautiful comi"
O10-2008,C08-1103,0,0.0638536,"hallow chunked portions formed by removing emotional expressions and holders are identified as the responsible spans that contain one or more potential emotion topics. The words of the shallow chunks containing POS tags of NNP or NNC are allowed to include as emotion topics. User-Topic Coreference: The identified holders and topics associated with an emotional expression in each of the sentences are coreferent if they share the same emotional expression. The emotion topic is intended by the emotion holder and the topic depends on the context in which its associated emotional expression occurs [16]. Based on the hypothesis, a rule based unsupervised technique is devised to identify the coreference between user and topic with Fi 1 Th D it respect to a particular type of emotional expression if the chunks responsible for emotion holder or topic are the immediate neighboring chunks of the emotional expression. [4. Supervised Framework] Topic coreference resolution resembles another well-known problem in NLP - noun phrase (NP) coreference resolution that considers machine learning frameworks [26] [27]. Therefore, we adapt a standard machine learning based approach to user-topic coreference"
O10-2008,P07-2034,0,0.068903,"Missing"
O10-2008,passonneau-2004-computing,0,0.0281459,"les and problems in mind, we hypothesize that the notion of user-topic coreference will facilitate both the manual and automatic identification of emotional views: Two components of emotion such as holders and topics are emotion coreferent if they share the same emotional expressions. The baseline system extracts the emotional expressions using Bengali WordNet Affect lists [6]. An unsupervised system is developed for identifying emotion holders and topics with respect to the emotional expressions. The co reference of the emotion topics and emotion holders is measured using Passonneau’s (2004) [24] generalization of Krippendorff’s (1980) [25] α metric. On the other hand, The Support Vector Machine [10] based supervised classifier is employed for coreference classification. Each of the input vectors containing emotional expression, associated holder and topic is prepared from each of the annotated Bengali blog sentences. The feature vector is prepared based on the information present in the sentences containing lexical, syntactic, semantic, rhetoric and overlapping features (word, part-of speech (POS), Named Entity (NE)). Training with 2234 sentences, the feature analysis has been conduc"
O10-2008,P02-1014,0,0.0207741,"holder and the topic depends on the context in which its associated emotional expression occurs [16]. Based on the hypothesis, a rule based unsupervised technique is devised to identify the coreference between user and topic with Fi 1 Th D it respect to a particular type of emotional expression if the chunks responsible for emotion holder or topic are the immediate neighboring chunks of the emotional expression. [4. Supervised Framework] Topic coreference resolution resembles another well-known problem in NLP - noun phrase (NP) coreference resolution that considers machine learning frameworks [26] [27]. Therefore, we adapt a standard machine learning based approach to user-topic coreference resolution. The Support Vector Machine (SVM) [10] based supervised framework has been used to extract the input vectors that contain emotional expressions, users or holders and topics. The system is trained with 2234 sentences. The best feature set is identified using the 630 development sentences. The Information Gain Based Pruning (IGBP), Admissible Tag Sequence (ATS), Class Splitting technique and Emotional Composition features are applied 354 on the development set and it improves the performanc"
O10-2008,J01-4004,0,0.0819284,"r and the topic depends on the context in which its associated emotional expression occurs [16]. Based on the hypothesis, a rule based unsupervised technique is devised to identify the coreference between user and topic with Fi 1 Th D it respect to a particular type of emotional expression if the chunks responsible for emotion holder or topic are the immediate neighboring chunks of the emotional expression. [4. Supervised Framework] Topic coreference resolution resembles another well-known problem in NLP - noun phrase (NP) coreference resolution that considers machine learning frameworks [26] [27]. Therefore, we adapt a standard machine learning based approach to user-topic coreference resolution. The Support Vector Machine (SVM) [10] based supervised framework has been used to extract the input vectors that contain emotional expressions, users or holders and topics. The system is trained with 2234 sentences. The best feature set is identified using the 630 development sentences. The Information Gain Based Pruning (IGBP), Admissible Tag Sequence (ATS), Class Splitting technique and Emotional Composition features are applied 354 on the development set and it improves the performance of"
O10-2008,H05-1073,0,0.0272984,"sms are specified in Section 5. Finally Section 6 concludes the paper. [2. Related Work] In order to estimate affects in text, the model proposed in [19] processes symbolic cues and employs natural language processing techniques for word/phrase/sentence level analysis, considering relations among words in a sentence. The current trend in the emotion analysis area is exploring machine learning techniques [28], which consider the problem as text categorization or analogous to topic classification that underscores the difference between machine learning methods and human-produced baseline models [29]. Affective text shared task on news headlines at SemEval 2007 for emotion and valence level identification [20] has drawn the focus to this field. Prior work in identification of opinion holders has sometimes identified only a single opinion per sentence [30], and sometimes several [1]. Identification of opinion holders for Question Answering with supporting annotation task was attempted in the very beginning [17]. Before that, another work on labeling the arguments of the verbs with their semantic roles using a novel frame matching technique was carried out in [31]. Based on the traditional"
O10-2008,W09-3411,1,0.92493,"corresponding dictionary forms of 374 Bengali verbs containing simple and light verb entries. The dictionary forms of the Bengali compound or conjunct verbs are made by 355 incorporating the dictionary forms of the light verbs with their preceding noun words that are tagged as ‘NN’ present in the retrieved lexical patterns. English Equivalent Synset Identification: The determination of equivalent English verbs of a Bengali verb is carried out using a Bengali to English bilingual dictionary4. The method to extract the English equivalent synsets of the Bengali verbs is based on the work done in [32]. We have identified the equivalent English verb synsets of the Bengali verb entries that are present in the dictionary. For example, the dictionary entries for simple verb ভােলাবাসা bhalobasa ‘love’ and conjunct verb আনn ananda করা kara ‘enjoy’ are as follows. < ভােলাবাসা [bhālōbāsā] v to love, to be amorous to wards; to like; to have attachment or affection, fondness for …&gt; < আনn করা v. to rejoice; to make merry.…&gt; Different synonyms for a Bengali verb having the same sense are separated using “,” and different senses are separated using “;” in the dictionary. The synonyms including similar"
O10-2008,H05-1043,0,0.0129919,"ask for “not holder”, “weak holder”, “medium holder”, or “strong holder” is described in [34]. Kim and Hovy [11] identified opinion holder with topic from media text using semantic role labeling. An anaphor resolution based opinion holder identification method exploiting lexical and syntactic information from online news documents was attempted in [36]. The syntactic models of identifying emotion holder for English emotional verbs are developed in [5]. In the related area of opinion topic extraction, different researchers contributed their efforts. Some of the works are mentioned in [13] [18] [37]. But, all these works are based on lexicon look up and are applied on the domain of product reviews. The topic annotation task on the MPQA corpus is described in [15]. The authors have pointed out that the target spans alone are insufficient for many applications as they neither contain information indicating which opinions are about the same topic, nor provide a concise textual representation of the topics. The method of identifying an opinion with its holder and topic from online news is described in [11]. The model extracts opinion topics for subjective expressions signaled by verbs and ad"
O10-2008,H05-2017,0,\N,Missing
O10-2008,S07-1013,0,\N,Missing
O12-1029,E09-1015,0,0.312967,"Missing"
O12-1029,I08-3012,1,0.899222,"Missing"
O13-2004,W09-3411,1,0.892196,"Missing"
O13-2004,H05-1045,0,0.0811028,"Missing"
O13-2004,P09-2038,1,0.882947,"Missing"
O13-2004,esuli-sebastiani-2006-sentiwordnet,0,0.0958314,"Missing"
O13-2004,W06-0301,0,0.0692385,"Missing"
O13-2004,P02-1014,0,0.227546,"Missing"
O13-2004,H05-1043,0,0.169323,"Missing"
O13-2004,J01-4004,0,0.0884919,"Missing"
O13-2004,stoyanov-cardie-2008-annotating,0,0.0394572,"Missing"
O13-2004,C08-1103,0,0.0464725,"Missing"
P06-2025,C00-1056,0,0.32554,"Missing"
P06-2025,W98-1005,0,0.218658,"large name collections like census data, electoral roll and railway reservation information must be available to multilingual citizens of the country in their vernacular. In the present work, the various proposed models have been evaluated on a training corpus of person names. A hybrid neural network and knowledge-based system to generate multiple English spellings for Arabic personal names is described in (Arbabi et al., 1994). (Knight and Graehl, 1998) developed a phoneme-based statistical model using finite state transducer that implements transformation rules to do back-transliteration. (Stalls and Knight, 1998) adapted this approach for back transliteration from Arabic to English for English names. A spelling-based model is described in (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c) that directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations. The phonetics-based and spelling-based models have been linearly combined into a single transliteration model in (Al-Onaizan and Knight, 2002b) for transliteration of Arabic named entities into English. Several"
P06-2025,W03-1508,0,0.320963,"Missing"
P06-2025,P02-1051,0,0.0843851,"Missing"
P06-2025,2005.mtsummit-papers.37,0,0.0691749,"Missing"
P06-2025,2005.mtsummit-papers.36,0,\N,Missing
P06-2025,P04-1021,0,\N,Missing
P06-2025,W02-0505,0,\N,Missing
P06-2025,W05-0700,0,\N,Missing
P09-2038,esuli-sebastiani-2006-sentiwordnet,0,0.0368184,"Missing"
P09-2038,H05-1073,0,0.40079,"Missing"
P09-2038,S07-1013,0,\N,Missing
P09-2038,P07-2034,0,\N,Missing
P11-4009,S07-1022,0,0.0730324,"Missing"
P11-4009,W02-1011,0,0.013505,"Missing"
P11-4009,P05-2008,0,0.0407236,"Missing"
P11-4009,D08-1103,0,0.0344928,"Missing"
P11-4009,strapparava-valitutti-2004-wordnet,0,0.193977,"Missing"
P11-4009,baccianella-etal-2010-sentiwordnet,0,\N,Missing
P11-4009,P06-1134,0,\N,Missing
pal-etal-2014-word,popovic-ney-2006-pos,0,\N,Missing
pal-etal-2014-word,W11-2127,0,\N,Missing
pal-etal-2014-word,holmqvist-etal-2012-alignment,0,\N,Missing
pal-etal-2014-word,J93-2003,0,\N,Missing
pal-etal-2014-word,D08-1089,0,\N,Missing
pal-etal-2014-word,P02-1040,0,\N,Missing
pal-etal-2014-word,W09-0435,0,\N,Missing
pal-etal-2014-word,N09-1028,0,\N,Missing
pal-etal-2014-word,W10-1735,0,\N,Missing
pal-etal-2014-word,W05-0909,0,\N,Missing
pal-etal-2014-word,P07-2045,0,\N,Missing
pal-etal-2014-word,P10-2033,0,\N,Missing
pal-etal-2014-word,W12-4207,0,\N,Missing
pal-etal-2014-word,C10-1043,0,\N,Missing
pal-etal-2014-word,E09-1011,0,\N,Missing
pal-etal-2014-word,N03-1017,0,\N,Missing
pal-etal-2014-word,J03-1002,0,\N,Missing
pal-etal-2014-word,P05-1066,0,\N,Missing
pal-etal-2014-word,N10-3010,0,\N,Missing
pal-etal-2014-word,2007.mtsummit-papers.28,0,\N,Missing
pal-etal-2014-word,2005.iwslt-1.8,0,\N,Missing
pal-etal-2014-word,P03-1021,0,\N,Missing
R11-1084,S10-1077,1,0.890585,"Missing"
R11-1084,P98-1013,0,0.106407,"A subcategorization frame is a statement of what types of syntactic arguments a verb (or an adjective) takes, such as objects, infinitives, that-clauses, participial clauses, and subcategorized prepositional phrases (Manning et al. 1993). VerbNet (Kipper-Schuler et al, 2005) is the largest online verb lexicon with explicitly stated syntactic and semantic information based on Levin’s verb classification (Levin et al 1993). It is a hierarchical domain-independent, broadcoverage verb lexicon with mappings to other lexical resources such as WordNet (Miller et al, 1990), XTAG (2001) and FrameNet (Baker et al, 1998). We use VerbNet throughout this experiment for identifying the event actors. The existing syntax for each event verb is extracted from VerbNet and a separate rule based argument structure acquisition system is developed in the present task for identifying the event actor. The acquired argument structures are compared against the extracted VerbNet frame syntaxes. If the acquired argument structure matches with any of the extracted frame syntaxes, the event actor corresponding to each event verb is tagged with the actor information in the appropriate slot in the sentence. Syntax Acquisition fro"
R11-1084,de-marneffe-etal-2006-generating,0,\N,Missing
R11-1084,C98-1013,0,\N,Missing
R11-1084,P93-1032,0,\N,Missing
R11-1084,J02-3001,0,\N,Missing
R11-1084,N04-1030,0,\N,Missing
S07-1043,vasilescu-etal-2004-evaluating,0,0.0322373,"ances). The proposed WSD algorithm is POS-sense-tagged gloss (from Related Works Banerjee and Pedersen (2002) reports an adaptation of Lesk’s dictionary-based WSD algorithm which makes use of WordNet glosses and tests on English lexical sample from SENSEVAL-2. They define overlap as the longest sequence of one or more consecutive content words that occurs in both glosses. Each overlap contributes a score equal to the square of the number of words in the overlap. A version of Lesk algorithm in combination with WordNet has been reported for achieving good results in (Ramakrishnan et al., 2004). Vasilescu et al. (2004) carried on a series of experiments on the Lesk algorithm, adapted to WordNet, and on some variants. They studied the effect of varying the number of words in the contexts, centered around the target word. But till now no work has been reported which makes use of Extended WordNet for Lesk-like gloss-oriented approach. 203 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 203–206, c Prague, June 2007. 2007 Association for Computational Linguistics 4 Proposed Sense Disambiguation Algorithm The proposed sense disambiguation algorithm is a major modificati"
S07-1043,W99-0501,0,0.0207961,"algorithm relies on the POS-sense tagged synset glosses provided by the Extended WordNet. The basic unit of disambiguation of our algorithm is the entire sentence under consideration. It takes a global approach where all the words in the target sentence are simultaneously disambiguated. The context includes previous and next sentence. The system assigns the default WordNet first sense to a word when the algorithm fails to predict the sense of the word. The system produces a precision and recall of .402 on the SemEval-2007 English All-Words test data. 1 2 Extended WordNet The eXtended WordNet (Harabagiu et al., 1999) project aims to transform the WordNet glosses into a format that allows the derivation of additional semantic and logic relations. It intends to syntactically parse the glosses, transform glosses into logical forms and tag semantically the nouns, verbs, adjectives and adverbs of the glosses automatically. The last release of the Extended WordNet is based on WordNet 2.0 and has three stages: POS tagging and parsing, logic form transformation, and semantic disambiguation. 3 Introduction In Senseval 1, most of the systems disambiguating English words, were outperformed by a Lesk variant serving"
S10-1045,de-marneffe-etal-2006-generating,0,0.0564326,"Missing"
S10-1045,W09-2415,0,\N,Missing
S10-1077,P07-2044,0,0.0796423,"Missing"
S10-1077,W06-0110,0,0.0606228,"Missing"
S10-1077,N03-1028,0,0.194286,"Missing"
S10-1077,S07-1014,0,0.117204,"Missing"
S10-1077,P06-1095,0,\N,Missing
S12-1083,S12-1051,0,0.086584,"h module are identified using rule based techniques. The Pearson Correlation of our system in the task is 0.3880.  MSR-Paraphrase, Microsoft Research Paraphrase Corpus (750 pairs of sentences.)  MSR-Video, Microsoft Research Video Description Corpus (750 pairs of sentences.)  SMTeuroparl: WMT2008 development dataset (Europarl section) (459 pairs of sentences.)  SMTnews: news conversation sentence pairs from WMT.(399 pairs of sentences.)  OnWN: pairs of sentences where the first comes from Ontonotes and the second from a WordNet definition. (750 pairs of sentences.) 1 Introduction Task-61 [1] of SemEval-2012 deals with semantic similarity of text pairs. The task is to find the similarity between the sentences in the text pair (s1 and s2) and return a similarity score and an optional confidence score. There are five datasets 1 http://www.cs.york.ac.uk/semeval-2012/task6/ Similarity score ranges from 0 to 5 and confidence score from 0 to 100. An s1-s2 pair gets a similarity score of 5 if they are completely equivalent. Similarity score 4 is allocated for mostly equivalent s1-s2 pair. Similarly, score 3 is allocated for roughly equivalent pair. Score 2, 1 and 0 are allocated for non-"
S12-1083,W07-1401,0,\N,Missing
S12-1103,S12-1053,0,0.021537,"guage using the Microsoft Bing translation system. The entailment system considers Named Entity, Noun Chunks, Part of speech, N-Gram and some text similarity measures of the text pair to decide the entailment judgments. Rules have been developed to encounter the multi way entailment issue. Our system decides on the entailment judgment after comparing the entailment scores for the text pairs. Four different rules have been developed 1 Introduction Textual Entailment (TE) (Dagan and Glickman, 2004) is one of the recent challenges of Natural Language Processing (NLP). The Task 8 of SemEval-20121 [1] defines a textual entailment system that specifies two major aspects: the task is based on cross-lingual corpora and the entailment decision must be four ways. Given a pair of topically related text fragments (T1 and T2) in different languages, the CLTE task consists of automatically annotating it with one of the following entailment judgments: i. Bidirectional (T1 -&gt;T2 & T1 &lt;- T2): the two fragments entail each other (semantic equivalence) ii. Forward (T1 -&gt; T2 & T1!&lt;- T2): unidirectional entailment from T1 to T2 . iii. Backward (T1! -&gt; T2 & T1 &lt;- T2): unidirectional entailment from T2 to T1"
S12-1103,W07-1401,0,0.0411315,"task consists of 1,000 CLTE dataset pairs (500 for 1 http://www.cs.york.ac.uk/semeval2012/index.php?id=tasks 689 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 689–695, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics training and 500 for test) available for the following language combinations: - Spanish/English (spa-eng) - German/English (deu-eng). - Italian/English (ita-eng) - French/English (fra-eng) Seven Recognizing Textual Entailment (RTE) evaluation tracks have already been held: RTE-1 in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE task produces a generic framework for entailment task across NLP applications. The RTE challenges have moved from 2 – way entailment task (YES, NO) to 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13"
S12-1103,N10-1045,0,0.102265,"ecognizing Textual Entailment (RTE) evaluation tracks have already been held: RTE-1 in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE task produces a generic framework for entailment task across NLP applications. The RTE challenges have moved from 2 – way entailment task (YES, NO) to 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by SemEval-2. Recognizing Inference in Text (RITE)2 organized by NTCIR-9 in 2011 is the first to expand TE as a 5-way entailment task (forward, backward, bi-directional, contradiction and independent) in a monolingual scenario [14]. We have participated in RTE-5 [15], RTE-6 [16], RTE-7 [17], SemEval-2 Parser Training and Evaluation using Textual Entailment Task and RITE [18]. Section 2 describes our Cross-lingual Textual Entailment system. The various experim"
S12-1103,W10-0734,0,0.127073,"zing Textual Entailment (RTE) evaluation tracks have already been held: RTE-1 in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE task produces a generic framework for entailment task across NLP applications. The RTE challenges have moved from 2 – way entailment task (YES, NO) to 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by SemEval-2. Recognizing Inference in Text (RITE)2 organized by NTCIR-9 in 2011 is the first to expand TE as a 5-way entailment task (forward, backward, bi-directional, contradiction and independent) in a monolingual scenario [14]. We have participated in RTE-5 [15], RTE-6 [16], RTE-7 [17], SemEval-2 Parser Training and Evaluation using Textual Entailment Task and RITE [18]. Section 2 describes our Cross-lingual Textual Entailment system. The various experiments c"
S12-1103,P11-1134,0,0.0509001,"extual Entailment (RTE) evaluation tracks have already been held: RTE-1 in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE task produces a generic framework for entailment task across NLP applications. The RTE challenges have moved from 2 – way entailment task (YES, NO) to 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by SemEval-2. Recognizing Inference in Text (RITE)2 organized by NTCIR-9 in 2011 is the first to expand TE as a 5-way entailment task (forward, backward, bi-directional, contradiction and independent) in a monolingual scenario [14]. We have participated in RTE-5 [15], RTE-6 [16], RTE-7 [17], SemEval-2 Parser Training and Evaluation using Textual Entailment Task and RITE [18]. Section 2 describes our Cross-lingual Textual Entailment system. The various experiments carried"
S12-1103,S10-1009,0,0.0299875,"[4] in 2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE task produces a generic framework for entailment task across NLP applications. The RTE challenges have moved from 2 – way entailment task (YES, NO) to 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by SemEval-2. Recognizing Inference in Text (RITE)2 organized by NTCIR-9 in 2011 is the first to expand TE as a 5-way entailment task (forward, backward, bi-directional, contradiction and independent) in a monolingual scenario [14]. We have participated in RTE-5 [15], RTE-6 [16], RTE-7 [17], SemEval-2 Parser Training and Evaluation using Textual Entailment Task and RITE [18]. Section 2 describes our Cross-lingual Textual Entailment system. The various experiments carried out on the development and test data sets are described in Section 3 along with the results. The conclusions"
S13-2011,setzer-gaizauskas-2000-annotating,0,0.148104,"Missing"
S13-2011,J02-3001,0,0.303093,"Missing"
S13-2011,N04-1030,0,0.130323,"Missing"
S13-2011,P11-2061,0,0.149426,"Missing"
S13-2011,S07-1014,0,\N,Missing
S13-2011,S10-1010,0,\N,Missing
S14-2063,N07-1038,0,0.0348543,"ed on topic modelling, that use Latent Dirichlet Allocation (LDA) (Brody and Elhadad, 2010). But, the standard Latent Dirichlet Allocation (LDA) is not exactly suitable for the task of aspect detection due to their inherent nature of capturing global topics in the data, rather than finding local aspects related to the predefined entity. This approach was further modified in Sentence-LDA (SLDA) and Aspect and Sentiment Unification Model (ASUM) (Jo and Oh, 2011). Similarly, the identification of focussed text spans for opinion topics and targets were identified in (Das and Bandyopadhyay, 2010). Snyder and Barzilay (2007) addressed the problem of identifying categories for multiple related aspect terms appeared in the text. For instance, in a restaurant review, such categories may include food, ambience and service etc. In our task, we call them as aspect or review categories. The authors implemented the Good Grief decoding algorithm on a corpus collected on restaurant review4, which outperforms over the famous PRank algorithm (Crammer and Singer, 2001). Ganu et al., (2009) have classified the restaurant reviews collected from City search New York5 into six categories namely Food, Service, Price, Ambience, Ane"
S14-2063,I13-1078,1,0.851195,"ciated with each category has also been identified and both the experiments were carried out using Support Vector Machine classifiers. Finally, they implemented the regression based model containing MATLAB regression 4 5 function (mvregress) to give rating (1 to 5) to each review. To determine the sentiment or polarity of the aspect term and aspect category, we need a prior sentiment annotated lexicon. Several works have been conducted on building emotional corpora in different English languages such as SentiWordNet (Baccianella et al., 2010), WordNet Affect (Strapparava and Valitutti, 2004) (Patra et al., 2013) etc. Among all these publicly available sentiment lexicons, SentiWordNet is one of the well-known and widely used ones (number of citations is higher than other resources6) that has been utilized in several applications such as sentiment analysis, opinion mining and emotion analysis. Several works have been performed on the automated opinion detection or polarity identification from reviews (Yu and Hatzivassiloglou, 2003; Hu and Liu, 2004). Yu and Hatzivassiloglou (2003) has focused on characterizing opinions and facts in a generic manner, without examining who the opinion holder is or what t"
S14-2063,H05-1043,0,0.0610235,"en observed that most of the previous works on aspect detection were based on information extraction, to find the most frequent noun phrases (Hu and Liu, 2004). This approach is generally useful in finding aspects which are strongly associated with a single noun. But, one principal disadvantage of this approach is that it cannot detect the aspect terms which are of low frequency and noun phrases (e.g., different names of dishes like Biryani, Dosa and Uttapam etc. for the aspect category, “food”). The proposed work of such problem involves semantic hierarchy, rule-based or combination of both (Popescu and Etzioni 2005). More recent approaches of aspect detection are based on topic modelling, that use Latent Dirichlet Allocation (LDA) (Brody and Elhadad, 2010). But, the standard Latent Dirichlet Allocation (LDA) is not exactly suitable for the task of aspect detection due to their inherent nature of capturing global topics in the data, rather than finding local aspects related to the predefined entity. This approach was further modified in Sentence-LDA (SLDA) and Aspect and Sentiment Unification Model (ASUM) (Jo and Oh, 2011). Similarly, the identification of focussed text spans for opinion topics and target"
S14-2063,N10-1122,0,0.365362,"and Liu, 2004). This approach is generally useful in finding aspects which are strongly associated with a single noun. But, one principal disadvantage of this approach is that it cannot detect the aspect terms which are of low frequency and noun phrases (e.g., different names of dishes like Biryani, Dosa and Uttapam etc. for the aspect category, “food”). The proposed work of such problem involves semantic hierarchy, rule-based or combination of both (Popescu and Etzioni 2005). More recent approaches of aspect detection are based on topic modelling, that use Latent Dirichlet Allocation (LDA) (Brody and Elhadad, 2010). But, the standard Latent Dirichlet Allocation (LDA) is not exactly suitable for the task of aspect detection due to their inherent nature of capturing global topics in the data, rather than finding local aspects related to the predefined entity. This approach was further modified in Sentence-LDA (SLDA) and Aspect and Sentiment Unification Model (ASUM) (Jo and Oh, 2011). Similarly, the identification of focussed text spans for opinion topics and targets were identified in (Das and Bandyopadhyay, 2010). Snyder and Barzilay (2007) addressed the problem of identifying categories for multiple rel"
S14-2063,W06-0301,0,0.112082,"Missing"
S14-2063,strapparava-valitutti-2004-wordnet,0,0.264056,"Missing"
S14-2063,baccianella-etal-2010-sentiwordnet,0,0.0535732,"Food, Service, Price, Ambience, Anecdotes, and Miscellaneous. Sentiment associated with each category has also been identified and both the experiments were carried out using Support Vector Machine classifiers. Finally, they implemented the regression based model containing MATLAB regression 4 5 function (mvregress) to give rating (1 to 5) to each review. To determine the sentiment or polarity of the aspect term and aspect category, we need a prior sentiment annotated lexicon. Several works have been conducted on building emotional corpora in different English languages such as SentiWordNet (Baccianella et al., 2010), WordNet Affect (Strapparava and Valitutti, 2004) (Patra et al., 2013) etc. Among all these publicly available sentiment lexicons, SentiWordNet is one of the well-known and widely used ones (number of citations is higher than other resources6) that has been utilized in several applications such as sentiment analysis, opinion mining and emotion analysis. Several works have been performed on the automated opinion detection or polarity identification from reviews (Yu and Hatzivassiloglou, 2003; Hu and Liu, 2004). Yu and Hatzivassiloglou (2003) has focused on characterizing opinions and facts in"
S14-2063,W03-1017,0,0.118729,"orks have been conducted on building emotional corpora in different English languages such as SentiWordNet (Baccianella et al., 2010), WordNet Affect (Strapparava and Valitutti, 2004) (Patra et al., 2013) etc. Among all these publicly available sentiment lexicons, SentiWordNet is one of the well-known and widely used ones (number of citations is higher than other resources6) that has been utilized in several applications such as sentiment analysis, opinion mining and emotion analysis. Several works have been performed on the automated opinion detection or polarity identification from reviews (Yu and Hatzivassiloglou, 2003; Hu and Liu, 2004). Yu and Hatzivassiloglou (2003) has focused on characterizing opinions and facts in a generic manner, without examining who the opinion holder is or what the opinion is about. Then, they have identified the polarity or sentiment of the fact using Naive Bayes classifier. Hu and Liu, (2004) has summarized the customer review and then identified the sentiment of that review. They have achieved promising accuracy in case of identifying polarity of the reviews. 3 Data The sentences collected from the customer reviews of Restaurants and Laptops are used in these tasks. The traini"
S14-2063,S14-2004,0,0.0342398,"011). For example, in case of Laptop reviews, “touchpad” is considered an aspect. Similarly, given a predefined entity, an aspect term describes a specific aspect of that entity (e.g., for the entity “restaurant”, “wine” can be an aspect term). Aspect term can be appeared as a single word (e.g., “menu”) or multiple words (“side dish”). It is observed that for a particular entity, one or more number of aspect terms can be grouped into a single category (e.g., aspect terms “drinks”, “main course” belongs to the same category, “food”). The main goal of the Aspect Based Sentiment Analysis (ABSA) (Pontiki et al., 2014) task is to identify the aspect terms and their categories from the given target entities as well as to identify the sentiments expressed towards each of the aspect terms. The datasets provided by the shared task organizers consist of customer reviews with human-annotations. We have participated in all of the four tasks. A combination of Conditional Random Field (CRF) based machine learning algorithm and rule based techniques has been adopted for identifying the aspect term, aspect category and their sentiments. We have used several features like Part of Speech (POS), Stanford dependency relat"
S14-2063,H05-2017,0,\N,Missing
S16-1071,W11-1701,0,0.292205,"Missing"
S16-1071,baccianella-etal-2010-sentiwordnet,0,0.0568222,"“#HillaryClinton” in the topic bag “Hillary”. Further, we checked the topic bags manually and removed some unrelated words collected using the WordNet. The detailed word level statistics of the topic bags are given in Table 1. At first, we checked that whether a tweet contains any word from the corresponding target related topic bag or not. If no word is present, then we tagged that tweet as ‘NONE’ and no further processing is performed on that tweet. Further, this topic bags are used along with the dependency information. 3.1.2 Sentiment Words We have used three lexicons namely SentiWordNet (Baccianella et al., 2010), NRC Emotion Lexicon (Mohammad and Turney, 2013), and NRC Hashtag Emotion Lexicon (Mohammad, 2012) for our experiments along with manually created lexicons for each of the targets. Moreover, two bags were created for each of the targets namely ‘favor’ and ‘against’ bags. Hashtags are also included in the above lexicons. The word level statistics of the ‘favor’ and ‘against’ bags are given in Table 1. We used the frequency of the sentiment words matched with the above sentiment or emotion lexicons as features. Again, we used these lexicons in the later experiments also. 2 https://rednoise.org/"
S16-1071,I13-1191,0,0.0181398,"outlook towards specific targets and propositions rather than simply considering whether the speaker is angry or happy (Mohammad et al., 2016). Stance detection even becomes more difficult when it is performed on the short texts like tweets. The latter being an important sub-field of sentiment analysis/opinion mining. Automatic stance detection can be used in several applications such as information retrieval, text summarization, and textual entailment. More recent approaches to stance detection have been performed using linguistic rules on online debate dataset (Somasundaran and Wiebe, 2009; Hasan and Ng, 2013; Sridhar et al., 2014) and NPOV Corpus from Wikipedia (Recasens et al., 2013). To the best of our knowledge, not much computational attempts have been performed on tweets for stance detection. It is also observed that both the supervised and unsupervised machine learning algorithms have been implemented for identifying the stance and several features like n-gram, frame semantic features, and dependency were used in stance detection tasks (Somasundaran and Wiebe, 2009; Anand et al., 2011; Sridhar et al., 2014). We participated in the SemEval 2016-Task 6: Detecting Stance in Tweets.1 The main g"
S16-1071,S16-1003,0,0.0429807,"Missing"
S16-1071,S12-1033,0,0.0347167,"related words collected using the WordNet. The detailed word level statistics of the topic bags are given in Table 1. At first, we checked that whether a tweet contains any word from the corresponding target related topic bag or not. If no word is present, then we tagged that tweet as ‘NONE’ and no further processing is performed on that tweet. Further, this topic bags are used along with the dependency information. 3.1.2 Sentiment Words We have used three lexicons namely SentiWordNet (Baccianella et al., 2010), NRC Emotion Lexicon (Mohammad and Turney, 2013), and NRC Hashtag Emotion Lexicon (Mohammad, 2012) for our experiments along with manually created lexicons for each of the targets. Moreover, two bags were created for each of the targets namely ‘favor’ and ‘against’ bags. Hashtags are also included in the above lexicons. The word level statistics of the ‘favor’ and ‘against’ bags are given in Table 1. We used the frequency of the sentiment words matched with the above sentiment or emotion lexicons as features. Again, we used these lexicons in the later experiments also. 2 https://rednoise.org/rita/reference/RiWordNet.html each of the simple sentences, we removed the stopwords (except the ne"
S16-1071,S14-2063,1,0.85197,"a simple sentence, we tagged it as positive. If there are only two sentiment words, one with positive and other as negative, we tagged the sentence as negative. It is observed that the frequency of the negative instances is much higher than the positive instances in the training data. Finally, we counted the number of positive and negative instances in a tweet and used as features. 3.2 Classification Framework Figure 1: Dependency relations from Stanford Parser 3.1.3 Dependency Information It is found in the literature that the dependency relations act as useful feature in sentiment analysis (Patra et al., 2014a; Patra et al., 2014b). Thus, we used the Stanford Parser3 to get the dependency relations. We searched for the word pairs in dependency relations that consist of two component words, one of which is present in either ‘favor’ or ‘against’ bag, whereas the other one is present in SentiWordNet. In the above Figure 1, we found a relation “dobj(support-7, campaign-9)”. The word ‘campaign’ is present in the ‘favor’ bag for the target “Hillary Clinton” and the word ‘support’ is present in SentiWordNet as positive. This relation is considered as favor positive type. Similarly, three other types name"
S16-1071,P13-1162,0,0.0276676,"dering whether the speaker is angry or happy (Mohammad et al., 2016). Stance detection even becomes more difficult when it is performed on the short texts like tweets. The latter being an important sub-field of sentiment analysis/opinion mining. Automatic stance detection can be used in several applications such as information retrieval, text summarization, and textual entailment. More recent approaches to stance detection have been performed using linguistic rules on online debate dataset (Somasundaran and Wiebe, 2009; Hasan and Ng, 2013; Sridhar et al., 2014) and NPOV Corpus from Wikipedia (Recasens et al., 2013). To the best of our knowledge, not much computational attempts have been performed on tweets for stance detection. It is also observed that both the supervised and unsupervised machine learning algorithms have been implemented for identifying the stance and several features like n-gram, frame semantic features, and dependency were used in stance detection tasks (Somasundaran and Wiebe, 2009; Anand et al., 2011; Sridhar et al., 2014). We participated in the SemEval 2016-Task 6: Detecting Stance in Tweets.1 The main goal of this task is to identify stance present in a tweet towards a specific t"
S16-1071,P09-1026,0,0.0355757,"about the author’s evaluative outlook towards specific targets and propositions rather than simply considering whether the speaker is angry or happy (Mohammad et al., 2016). Stance detection even becomes more difficult when it is performed on the short texts like tweets. The latter being an important sub-field of sentiment analysis/opinion mining. Automatic stance detection can be used in several applications such as information retrieval, text summarization, and textual entailment. More recent approaches to stance detection have been performed using linguistic rules on online debate dataset (Somasundaran and Wiebe, 2009; Hasan and Ng, 2013; Sridhar et al., 2014) and NPOV Corpus from Wikipedia (Recasens et al., 2013). To the best of our knowledge, not much computational attempts have been performed on tweets for stance detection. It is also observed that both the supervised and unsupervised machine learning algorithms have been implemented for identifying the stance and several features like n-gram, frame semantic features, and dependency were used in stance detection tasks (Somasundaran and Wiebe, 2009; Anand et al., 2011; Sridhar et al., 2014). We participated in the SemEval 2016-Task 6: Detecting Stance in"
S16-1071,W14-2715,0,0.272571,"ific targets and propositions rather than simply considering whether the speaker is angry or happy (Mohammad et al., 2016). Stance detection even becomes more difficult when it is performed on the short texts like tweets. The latter being an important sub-field of sentiment analysis/opinion mining. Automatic stance detection can be used in several applications such as information retrieval, text summarization, and textual entailment. More recent approaches to stance detection have been performed using linguistic rules on online debate dataset (Somasundaran and Wiebe, 2009; Hasan and Ng, 2013; Sridhar et al., 2014) and NPOV Corpus from Wikipedia (Recasens et al., 2013). To the best of our knowledge, not much computational attempts have been performed on tweets for stance detection. It is also observed that both the supervised and unsupervised machine learning algorithms have been implemented for identifying the stance and several features like n-gram, frame semantic features, and dependency were used in stance detection tasks (Somasundaran and Wiebe, 2009; Anand et al., 2011; Sridhar et al., 2014). We participated in the SemEval 2016-Task 6: Detecting Stance in Tweets.1 The main goal of this task is to"
S16-1152,N15-2002,0,0.0129605,"words decreases the readability of the document. Thus, the complex word identification (CWI) task aims to classify those challenging words in a sentence with respect to a particular target audience. For example, in the following sentence, the words in italics are complex words. These words are related to biology and are rarely used in our daily life. e.g. “The first amniotes, such as Casineria, resembled small lizards and evolved from amphibian reptiliomorphs about 340 million years ago.” Some research has been performed in CWI task in comparison to the lexical simplification (Shardlow, 2013; Paetzold, 2015). The important features which have been used previously in the CWI task are frequency thresholding and lexical matching etc. (Shardlow, 2013). Some motivations of the CWI task are to understand the defining characteristics of the words which are challenging for non-native speakers to interpret. Another is assessing an individual’s vocabulary limitations from the group he is a part of. We have participated in the SemEval 2016-Task 11: Complex Word Identification2 (Paetzold and Specia, 2016). The main goal of this task is to identify the complex words from English sentences. We identified highl"
S16-1152,P13-3015,0,0.429879,"use of complex words decreases the readability of the document. Thus, the complex word identification (CWI) task aims to classify those challenging words in a sentence with respect to a particular target audience. For example, in the following sentence, the words in italics are complex words. These words are related to biology and are rarely used in our daily life. e.g. “The first amniotes, such as Casineria, resembled small lizards and evolved from amphibian reptiliomorphs about 340 million years ago.” Some research has been performed in CWI task in comparison to the lexical simplification (Shardlow, 2013; Paetzold, 2015). The important features which have been used previously in the CWI task are frequency thresholding and lexical matching etc. (Shardlow, 2013). Some motivations of the CWI task are to understand the defining characteristics of the words which are challenging for non-native speakers to interpret. Another is assessing an individual’s vocabulary limitations from the group he is a part of. We have participated in the SemEval 2016-Task 11: Complex Word Identification2 (Paetzold and Specia, 2016). The main goal of this task is to identify the complex words from English sentences. We"
S16-1152,S12-1046,0,0.0675581,"Missing"
S16-1152,S16-1085,0,\N,Missing
W06-1908,W04-0507,0,0.0340171,"Missing"
W06-1908,C02-1042,0,0.0708111,"Missing"
W06-1908,W00-1014,0,\N,Missing
W06-1908,W04-0502,0,\N,Missing
W06-2113,W04-2608,0,0.709386,"Missing"
W06-2113,2005.mtsummit-posters.8,1,0.838936,"Missing"
W06-2113,P98-2201,0,0.48185,"Missing"
W06-2113,C98-2196,0,\N,Missing
W08-1139,J02-1003,0,\N,Missing
W08-1139,I08-2095,1,\N,Missing
W09-3411,C94-1042,0,0.248761,"Missing"
W09-3411,W98-1505,0,0.0189298,"erbs that resulted in extremely low yields for subcategorization frame acquisition is described in (Brent, 1991). A rule based system for automatically acquiring six verb subcategorization frames and their frequencies from a large corpus is mentioned in (Ushioda et al., 1993). An open class vocabulary of 35,000 words was analyzed manually in (Briscoe and Carroll, 1997) for subcategorization frames and predicate associations. The result was compared against associations in ANLT and COMLEX. Variations of subcategorization frequencies across corpus type (written vs. spoken) have been studied in (Carroll and Rooth, 1998). A mechanism for resolving verb class ambiguities using subcategorization frames is reported in (Lapata and Brew, 1999). All these works deal with English. Several works on the term classification of verb diathesis roles or the lexical semantics of predicates in natural language have been reported in ((McCarthy, 2001), 1 2 System Outline 3.2 Identification and Selection of Verbs Our previous work (Das et.al., 2009) on the acquisition of Bengali subcategorization frames from the same Bengali news corpus was carried out for the most frequent verb “ দখা” (dekha) (see) in that corpus. The next hi"
W09-3411,C00-2100,0,0.0356902,"corpus. Linguists have suggested that these frames do appear in Bengali and hence can be acquired. The rest of the paper is organized as follows. Section 2 gives the description of the related works carried out in this area. Section 3 describes the framework for the acquisition of subcategorization frames for ten compound Bengali verbs. Evaluation results of the system are discussed in section 4. Finally section 5 concludes the paper. 2 (Korhonen, 2002), (Stevenson and Merlo, 1999) and (Walde, 1998)). A cross lingual work on learning verbargument structure for Czech language is described in (Sarkar and Zeman, 2000). (Samantaray, 2007) gives a method of acquiring different subcategorization frames for the purpose of machine aided translation system for Indian languages. The work on subcategorization frame acquisition of Japanese verbs using breadth-first algorithm is described in (Muraki et al., 1997). 3 We have developed several modules for the acquisition of verb subcategorization frames from the Bengali newspaper corpus. The modules consist of POS tagging and chunking, Identification and Selection of Verbs, English Verb Determination, Frames Acquisition from VerbNet and Bengali Verb Subcategorization"
W09-3411,1997.tmi-1.20,0,0.206225,"Missing"
W09-3411,W93-0109,0,0.330908,"Missing"
W09-3411,P91-1027,0,0.253749,"Missing"
W09-3411,W99-0632,0,0.0675342,"Missing"
W09-3411,E99-1007,0,0.0340222,"Missing"
W09-3411,P98-1013,0,0.172537,"Missing"
W09-3411,A92-1012,0,0.0167423,"Missing"
W09-3411,A97-1052,0,0.0963679,"Missing"
W09-3411,J87-3002,0,\N,Missing
W09-3411,H91-1067,0,\N,Missing
W09-3411,W02-0907,0,\N,Missing
W09-3411,C98-1013,0,\N,Missing
W09-3411,P93-1032,0,\N,Missing
W09-3517,P06-2025,1,0.893972,"Missing"
W09-3517,2003.mtsummit-papers.17,0,0.504975,"ering Department Jadavpur University, Kolkata-700032, India amitava.research@gmail.com, asif.ekbal@gmail.com, tapabratamondal@gmail.com, sivaji_cse_ju@yahoo.com different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of NEs is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). Abstract This paper reports about our work in the NEWS 2009 Machine Transliteration Shared Task held as part of ACL-IJCNLP 2009. We submitted one standard run and two nonstan"
W09-3517,C00-1056,0,0.396199,"Missing"
W09-3517,P04-1021,0,0.304488,"ji Bandyopadhyay Computer Science and Engineering Department Jadavpur University, Kolkata-700032, India amitava.research@gmail.com, asif.ekbal@gmail.com, tapabratamondal@gmail.com, sivaji_cse_ju@yahoo.com different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of NEs is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). Abstract This paper reports about our work in the NEWS 2009 Machine Transliteration Shared Task held as part of ACL-IJCNLP 2009"
W09-3517,2005.mtsummit-papers.36,0,0.226978,"Missing"
W09-3517,I08-1009,0,0.162852,"Missing"
W09-3517,W03-1508,0,0.0702944,"Computer Science and Engineering Department Jadavpur University, Kolkata-700032, India amitava.research@gmail.com, asif.ekbal@gmail.com, tapabratamondal@gmail.com, sivaji_cse_ju@yahoo.com different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of NEs is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). Abstract This paper reports about our work in the NEWS 2009 Machine Transliteration Shared Task held as part of ACL-IJCNLP 2009. We submitted one standard"
W09-3517,P02-1051,0,0.119126,"Missing"
W09-3517,W09-3501,0,\N,Missing
W09-3517,W09-3502,0,\N,Missing
W09-3517,J98-4003,0,\N,Missing
W09-3539,I08-1045,0,0.0446319,"such lists are not available in Bengali. This necessitates the use of transliteration for creating such lists. A HMM based NER system for Bengali has been reported in Ekbal et al. (2007b), where additional contextual information has been considered during emission probabilities and NE suffixes are used for handling the unknown words. More recently, the works in the area of Bengali NER can be found in Ekbal et al. (2008a), and Ekbal and Bandyopadhyay (2008b) with the CRF, and SVM approach, respectively. Other than Bengali, the works on Hindi can be found in Li and McCallum (2004) with CRF and Saha et al. (2008) with a hybrid feature set based ME approach. Various works of NER involving Indian languages are reported in IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL) 1 using various techniques. 2 Named Entity Recognition in Bengali We have used a Bengali news corpus (Ekbal and Bandyopadhyay, 2008c), developed from the web-archive of a widely read Bengali newspaper for NER. A portion of this corpus containing 200K wordforms has been manually annotated with the four NE tags namely, Person, Location, Organization and Miscellaneous. We have also used the NE annotated data of 1"
W09-3539,I08-2077,1,0.57992,"uired measure. 6. Although Indian languages have a very old and rich literary history, technological developments are of recent origin. 7. Web sources for name lists are available in English, but such lists are not available in Bengali. This necessitates the use of transliteration for creating such lists. A HMM based NER system for Bengali has been reported in Ekbal et al. (2007b), where additional contextual information has been considered during emission probabilities and NE suffixes are used for handling the unknown words. More recently, the works in the area of Bengali NER can be found in Ekbal et al. (2008a), and Ekbal and Bandyopadhyay (2008b) with the CRF, and SVM approach, respectively. Other than Bengali, the works on Hindi can be found in Li and McCallum (2004) with CRF and Saha et al. (2008) with a hybrid feature set based ME approach. Various works of NER involving Indian languages are reported in IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL) 1 using various techniques. 2 Named Entity Recognition in Bengali We have used a Bengali news corpus (Ekbal and Bandyopadhyay, 2008c), developed from the web-archive of a widely read Bengali newspaper for NER. A portio"
W09-3539,I08-5008,1,0.620587,"Indian languages have a very old and rich literary history, technological developments are of recent origin. 7. Web sources for name lists are available in English, but such lists are not available in Bengali. This necessitates the use of transliteration for creating such lists. A HMM based NER system for Bengali has been reported in Ekbal et al. (2007b), where additional contextual information has been considered during emission probabilities and NE suffixes are used for handling the unknown words. More recently, the works in the area of Bengali NER can be found in Ekbal et al. (2008a), and Ekbal and Bandyopadhyay (2008b) with the CRF, and SVM approach, respectively. Other than Bengali, the works on Hindi can be found in Li and McCallum (2004) with CRF and Saha et al. (2008) with a hybrid feature set based ME approach. Various works of NER involving Indian languages are reported in IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL) 1 using various techniques. 2 Named Entity Recognition in Bengali We have used a Bengali news corpus (Ekbal and Bandyopadhyay, 2008c), developed from the web-archive of a widely read Bengali newspaper for NER. A portion of this corpus containing 200K word"
W09-3539,W03-0425,0,0.144415,"Missing"
W10-2411,I08-1009,0,0.0692551,"Missing"
W10-2411,W09-3517,1,0.31321,"ithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Run: HSR), Kannada (Kannada Standard Run: KSR) and Tamil (Tamil Standard Run: TSR) were submitted. Two non-standard runs for English to Hindi (Hindi Non-Standard Run 1 & 2: HNSR1 & HNSR2) and Bengali (Bengali NonStandard Run 1 & 2: BNSR1 & BNSR1) transliteration were submitted. Only one non-standard run were submitted for Kannada (Kannada NonStandard Run-1: KNSR1) and Tamil (Tamil Non-Standard Run-1: TNSR1). Abstract This paper reports about our work in the NEWS 2010 Shared Task on Transliteration Generation held"
W10-2411,W03-1508,0,0.211651,"niversity of Heidelberg Im Neuenheimer Feld 325 69120 Heidelberg, Germany ekbal@cl.uni-heidelberg.de of alphabets is trivial: the word is left as it is. However, for languages those use different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hind"
W10-2411,P02-1051,0,0.0210547,"e alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Run: HSR), Kannada (Kannada Standard Run: KSR) and Tamil (Tamil Standard Run: TSR) were submitted. Two non-standard runs for English to Hindi (Hindi Non-Standard Run 1 & 2: HNSR1 & HNSR2) and Bengali (Bengali NonStandard Run 1 & 2: BNSR1 & BNSR1) transliterati"
W10-2411,P06-2025,1,0.798718,"uage Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Run: HSR), Kannada (Kannada Standard Run: KSR) and Tamil (Tamil Standard Run: TSR) were submitted. Two non-standard runs for English to Hindi (Hindi Non-Standard Run 1 & 2: HNSR1 & HNSR2) and Bengali (Bengali NonStandard Run 1 & 2: BNSR1 & BNSR1) transliteration were submitted. Only one non-standard run were submitted for Kannada (Kannada NonStandard Run-1: KNSR1) and Tamil (Tam"
W10-2411,2003.mtsummit-papers.17,0,0.0288078,"Neuenheimer Feld 325 69120 Heidelberg, Germany ekbal@cl.uni-heidelberg.de of alphabets is trivial: the word is left as it is. However, for languages those use different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Ru"
W10-2411,C00-1056,0,0.0909754,"Missing"
W10-2411,P04-1021,0,0.131127,"al Linguistics4 University of Heidelberg Im Neuenheimer Feld 325 69120 Heidelberg, Germany ekbal@cl.uni-heidelberg.de of alphabets is trivial: the word is left as it is. However, for languages those use different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Benga"
W10-2411,2005.mtsummit-papers.36,0,0.0629612,"Missing"
W10-2411,W10-2402,0,\N,Missing
W10-2411,P97-1017,0,\N,Missing
W10-3207,J96-2004,0,0.180191,"Missing"
W10-3207,H05-1073,0,0.17506,"the paper. 2 Related Work One of the most well known tasks of annotating the private states in texts is carried out by (Wiebe et al., 2005). They manually annotated the private states including emotions, opinions, and sentiment in a 10,000-sentence corpus (the MPQA corpus) of news articles. The opinion holder information is also annotated in the MPQA corpus but the topic annotation task has been initiated later by (Stoyanov and Cardie, 2008a). In contrast, the present annotation strategy includes the fine-grained emotion classes and specially handles the emoticons present in the blog posts. (Alm et al., 2005) have considered eight emotion categories (angry, disgusted, fearful, 48 to the above emotion entities, the present approach also includes the annotation of single or multiple emotion topics in a target span. Recent study shows that non-native English speakers support the growing use of the Internet 2. This raises the demand of linguistic resources for languages other than English. Bengali is the fifth popular language in the World, second in India and the national language in Bangladesh but it is less computerized compared to English. To the best of our knowledge, at present, there is no such"
W10-3207,passonneau-2006-measuring,0,0.015194,"<EW_F> 0 </EW_F> '  1  <NEG> </NEG> </ES_Su></ES_A> <ES_H>1 <top2c>   </top2c> 13e  ei <EW_H> 4  </EW_H>  13+- </ES_H> 3.3 Emotional expressions are words or strings of words that are selected by the annotators. The agreement is carried out between the sets of text spans selected by the two annotators for each of the emotional expressions. As there is no fixed category in this case, we have employed two different strategies instead of kappa () to calculate the agreement between annotators. Firstly, we chose the measure of agreement on set-valued items (MASI) (Passonneau, 2006) that was used for measuring agreement on co reference annotation (Passonneau, 2004) and in the semantic and pragmatic annotation (Passonneau, 2006). MASI is a distance between sets whose value is 1 for identical sets, and 0 for disjoint sets. For sets A and B it is defined as: MASI = J * M, where the Jaccard metric is: “Figure 1. Annotated sample of the corpus” 3.2 Agreement of Sentential Emotion and Intensity Three annotators identified as A1, A2 and A3 have used an open source graphical tool to carry out the annotation 4 . As the Ekman’s emotion classes and intensity types belong to some de"
W10-3207,P09-2038,1,0.873776,"Missing"
W10-3207,D09-1150,0,0.0648,"Missing"
W10-3207,C08-1111,0,0.0416724,"Missing"
W10-3207,passonneau-2004-computing,0,\N,Missing
W10-3207,H05-1045,0,\N,Missing
W10-3207,P07-2034,0,\N,Missing
W10-3207,stoyanov-cardie-2008-annotating,0,\N,Missing
W10-3208,S07-1022,0,0.0273142,"Missing"
W10-3208,W02-1011,0,0.0157695,"Missing"
W10-3208,P05-2008,0,0.141591,"Missing"
W10-3208,esuli-sebastiani-2006-sentiwordnet,0,0.801056,"Missing"
W10-3208,P06-1134,0,0.0246901,"Missing"
W10-3208,C00-1044,0,0.0281407,"Missing"
W10-3208,H05-1044,0,0.0578517,"Missing"
W10-3208,P07-1123,0,0.0326869,"Missing"
W10-3208,D08-1103,0,\N,Missing
W10-3208,P06-1034,0,\N,Missing
W10-3402,kipper-etal-2006-extending,0,0.0658009,"Missing"
W10-3402,J05-1004,0,0.061272,"Missing"
W10-3603,J95-3006,0,0.550791,"Missing"
W10-3603,W01-0708,0,0.0406621,"Missing"
W10-3605,J03-3001,0,0.0303512,"on The NER and MWE identification are important tasks for natural language applications that include machine translation and information retrieval. The present work reports the NER and reduplicated MWE identification of Manipuri on web based news corpus. The use of web as a corpus for teaching and research on languages Sivaji Bandyopadhyay Department of Computer Science and Engineering Jadavpur University sivaji_cse_ju@yahoo.com has been proposed several times (Rundell, 2000; Fletcher, 2001; Robb, 2003; Fletcher 2004). A special issue of the Computational Linguistics journal on web as corpus (Kilgarriff and Grefenstette, 2003) was published. Several studies have used different methods to mine web data. The web walked into the ACL meetings starting in 1999. The special interest group of ACL on web as corpus is promoting interest in the use of the web as a source of linguistic data, and as an object of study in its own right. India is a multilingual country with a lot of cultural diversity. Bharati et al. (2001) reports an effort to create lexical resources such as transfer lexicon and grammar from English to several Indian languages and dependency Treebank of annotated corpora for several Indian languages. In Indian"
W10-3605,Y09-2045,1,\N,Missing
W10-3706,W09-2906,0,0.745863,"Missing"
W10-3706,W06-1205,0,\N,Missing
W10-3706,C08-2007,0,\N,Missing
W10-3707,C04-1114,0,0.441408,"ither. We address this many-to-many alignment problem indirectly. Our objective is to see how to best handle the MWEs in SMT. In this work, two types of MWEs, namely NEs and compound verbs, are automatically identified on both sides of the parallel corpus. Then, source and target language NEs are aligned using a statistical transliteration method. We rely on these automatically aligned NEs and treat them as translation examples. Adding bilingual dictionaries, which in effect are instances of atomic translation pairs, to the parallel corpus is a well-known practice in domain adaptation in SMT (Eck et al., 2004; Wu et al., 2008). We modify the parallel corpus by converting the MWEs into single tokens and adding the aligned NEs in the parallel corpus in a bid to improve the word alignment, and hence the phrase alignment quality. This 46 Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 46–54, Beijing, August 2010 preprocessing results in improved MT quality in terms of automatic MT evaluation metrics. The remainder of the paper is organized as follows. In section 2 we discuss related work. The System is described in Section 3. Section 4 includes the results obtai"
W10-3707,W09-3539,1,0.741895,"Missing"
W10-3707,W05-0909,0,0.209211,"Missing"
W10-3707,W04-3248,0,0.280025,"Missing"
W10-3707,J93-2003,0,0.0150925,"ed MWEs can bring about any further improvement on top of that. We carried out our experiments on an English—Bangla translation task, a relatively hard task with Bangla being a morphologically richer language. 3 System Description 3.1 PB-SMT Translation is modeled in SMT as a decision process, in which the translation e1I = e1 . . . ei . . . eI of a source sentence f1J = f1 . . . fj . . . fJ is chosen to maximize (1): (1) arg max P(e1I |f1J ) = arg max P( f1J |e1I ).P(e1I ) I ,e1I I ,e1I where P ( f1J |e1I ) and P (e1I ) denote respectively the translation model and the target language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( e1I |f1J ) is directly modeled as a log-linear combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): M log P(e1I |f 1J ) = ∑ λ m hm ( f 1J , e1I , s1K ) m =1 + λLM log P(e1I ) (2) where s = s1...sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (eˆ1 ,..., eˆk ) and ( fˆ1 ,..., fˆk ) such that (we set i0 = 0) (3): k 1 ∀1 ≤ k ≤ K , sk = (ik, bk, jk), eˆk = eik −1 +1...eik , fˆ = f ... f . k bk jk (3) and eac"
W10-3707,W03-1502,0,0.128214,"r simultaneous NE identification and translation. He uses capitalization cues for identifying NEs on the English side, and then he applies statistical techniques to decide which portion of the target language corresponds to the specified English NE. Feng et al. (2004) proposed a Maximum Entropy model based approach for English— Chinese NE alignment which significantly outperforms IBM Model4 and HMM. They considered 4 features: translation score, transliteration score, source NE and target NE's co-occurrence score, and the distortion score for distinguishing identical NEs in the same sentence. Huang et al. (2003) proposed a method for automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization. The costs considered are transliteration cost, word-based translation cost, and NE tagging cost. Venkatapathy and Joshi (2006) reported a discriminative approach of using the compositionality information about verb-based multi-word expressions to improve word alignment quality. (Ren et al., 2009) presented log likelihood ratiobased hierarchical reducing algorithm to automatically extract bilingual MWEs, and investigated the usefulness of these bilin"
W10-3707,N10-1029,0,0.108076,"reported a discriminative approach of using the compositionality information about verb-based multi-word expressions to improve word alignment quality. (Ren et al., 2009) presented log likelihood ratiobased hierarchical reducing algorithm to automatically extract bilingual MWEs, and investigated the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into Moses (Koehn et al., 2007) in three ways. They observed the highest improvement when they used an additional feature to represent whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010). In their work, the binary feature was replaced by a count feature representing the number of MWEs in the source language phrase. Intuitively, MWEs should be both aligned in the parallel corpus and translated as a whole. However, in the state-of-the-art PB-SMT, it could well be the case that constituents of an 47 MWE are marked and aligned as parts of consecutive phrases, since PB-SMT (or any other approaches to SMT) does not generally treat MWEs as special tokens. Another problem SMT suffers from is that verb phrases are often wrongly translated, or even sometimes deleted in the output in or"
W10-3707,C08-2007,0,0.104429,"Missing"
W10-3707,P07-2045,0,0.0291762,"gual equivalences between Chinese and English based on multi-feature cost minimization. The costs considered are transliteration cost, word-based translation cost, and NE tagging cost. Venkatapathy and Joshi (2006) reported a discriminative approach of using the compositionality information about verb-based multi-word expressions to improve word alignment quality. (Ren et al., 2009) presented log likelihood ratiobased hierarchical reducing algorithm to automatically extract bilingual MWEs, and investigated the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into Moses (Koehn et al., 2007) in three ways. They observed the highest improvement when they used an additional feature to represent whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010). In their work, the binary feature was replaced by a count feature representing the number of MWEs in the source language phrase. Intuitively, MWEs should be both aligned in the parallel corpus and translated as a whole. However, in the state-of-the-art PB-SMT, it could well be the case that constituents of an 47 MWE are marked and aligned as parts of consecutive phrases, since"
W10-3707,2006.amta-papers.25,0,0.0728121,"Missing"
W10-3707,C96-2141,0,0.388305,"(or spaces)” (Sag et al., 2002). Traditional approaches to word alignment following IBM Models (Brown et al., 1993) do not work well with multi-word expressions, especially with NEs, due to their inability to handle manyto-many alignments. Firstly, they only carry out alignment between words and do not consider the case of complex expressions, such as multiword NEs. Secondly, the IBM Models only allow at most one word in the source language to correspond to a word in the target language (Marcu, 2001, Koehn et al., 2003). In another well-known word alignment approach, Hidden Markov Model (HMM: Vogel et al., 1996), the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. We address this many-to-many alignment problem indirectly. Our objective is to see how to best handle the MWEs in SMT. In this work, two types of MWEs, namely NEs and compound verbs, are automatically identified on both sides of the parallel corpus. Then, source and target language NEs are aligned using a statistical transliteration method. We rely on these automatically aligned NEs and treat them as translation examples. Adding bilingual dictionar"
W10-3707,W04-3250,0,0.31373,"Missing"
W10-3707,W06-1204,0,0.435823,"Missing"
W10-3707,P01-1050,0,0.00961498,"pus. Multi-word expressions (MWE) are defined as “idiosyncratic interpretations that cross word boundaries (or spaces)” (Sag et al., 2002). Traditional approaches to word alignment following IBM Models (Brown et al., 1993) do not work well with multi-word expressions, especially with NEs, due to their inability to handle manyto-many alignments. Firstly, they only carry out alignment between words and do not consider the case of complex expressions, such as multiword NEs. Secondly, the IBM Models only allow at most one word in the source language to correspond to a word in the target language (Marcu, 2001, Koehn et al., 2003). In another well-known word alignment approach, Hidden Markov Model (HMM: Vogel et al., 1996), the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. We address this many-to-many alignment problem indirectly. Our objective is to see how to best handle the MWEs in SMT. In this work, two types of MWEs, namely NEs and compound verbs, are automatically identified on both sides of the parallel corpus. Then, source and target language NEs are aligned using a statistical transliteration me"
W10-3707,C08-1125,0,0.154066,"Missing"
W10-3707,E03-1035,0,0.114084,"igned NEs in the parallel corpus in a bid to improve the word alignment, and hence the phrase alignment quality. This 46 Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 46–54, Beijing, August 2010 preprocessing results in improved MT quality in terms of automatic MT evaluation metrics. The remainder of the paper is organized as follows. In section 2 we discuss related work. The System is described in Section 3. Section 4 includes the results obtained, together with some analysis. Section 5 concludes, and provides avenues for further work. 2 Related Work Moore (2003) presented an approach for simultaneous NE identification and translation. He uses capitalization cues for identifying NEs on the English side, and then he applies statistical techniques to decide which portion of the target language corresponds to the specified English NE. Feng et al. (2004) proposed a Maximum Entropy model based approach for English— Chinese NE alignment which significantly outperforms IBM Model4 and HMM. They considered 4 features: translation score, transliteration score, source NE and target NE's co-occurrence score, and the distortion score for distinguishing identical N"
W10-3707,P03-1021,0,0.0160755,"guages to Indian Languages Machine Translation (ILILMT) System”. NEs in Bangla are identified using the NER system of Ekbal and Bandyopadhyay (2008). We use the Stanford Parser, Stanford NER and the NER for Bangla along with the default model files provided, i.e., with no additional training. The effectiveness of the MWE-aligned parallel corpus developed in the work is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phraseextraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model with Kneser-Ney smoothing (Kneser and 1 The EILMT and ILILMT projects are funded by the Department of Information Technology (DIT), Ministry of Communications and Information Technology (MCIT), Government of India. 2 http://nlp.stanford.edu/software/lex-parser.shtml 3 4 Ney, 1995) trained with SRILM (Stolcke, 2002), and Moses decoder (Koehn et al., 2007). http://crfchunker.sourceforge.net/ http://nlp.stanford.edu/software/CRF-NER.shtml 50 Experiments and Results We randomly extracted 500 sentences each for the development set and testset fr"
W10-3707,P02-1040,0,0.0819899,"Missing"
W10-3707,W09-2907,0,0.376728,"re, transliteration score, source NE and target NE's co-occurrence score, and the distortion score for distinguishing identical NEs in the same sentence. Huang et al. (2003) proposed a method for automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization. The costs considered are transliteration cost, word-based translation cost, and NE tagging cost. Venkatapathy and Joshi (2006) reported a discriminative approach of using the compositionality information about verb-based multi-word expressions to improve word alignment quality. (Ren et al., 2009) presented log likelihood ratiobased hierarchical reducing algorithm to automatically extract bilingual MWEs, and investigated the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into Moses (Koehn et al., 2007) in three ways. They observed the highest improvement when they used an additional feature to represent whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010). In their work, the binary feature was replaced by a count feature representing the number of MWEs in the source language phrase. Intuitively, MWE"
W10-3707,N03-1017,0,\N,Missing
W10-3710,W09-2903,0,0.0281281,"uplications at both levels in Bengali. Reduplication phenomenon is not an exotic feature of Indian Languages. For instance, Yiddish English has duplication of the form X schm-X, as in &quot;duplication schmuplication&quot;. Semantic duplication is also rich in Related Work The works on MWE identification and extraction have been continuing in English (Fillmore, 2003; Sag et. al, 2002). After tokenization, multiword expressions are important in understanding the meaning in applications like Machine Translation, Information Retrieval system etc. Some of the MWE extraction tasks in English can be seen in (Diab and Bhutada, 2009; Enivre and Nilson, 2004). Among Indian languages, Hindi compound noun MWE extraction has been studied in (Kunchukuttan and Damani, 2008). Manipuri reduplicated MWE identification is discussed in (Nongmeikapam and Bandyopadhyay, 2010). There are no published works on reduplicated MWE identification in Bengali. 3 Reduplication of Words in Bengali Identification of MWEs is done during the tokenization phase and is absolutely necessary 73 Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 73–76, Beijing, August 2010 during POS tagging as is outlined in (Thoud"
W10-3710,I08-3015,1,0.782448,"2009; Enivre and Nilson, 2004). Among Indian languages, Hindi compound noun MWE extraction has been studied in (Kunchukuttan and Damani, 2008). Manipuri reduplicated MWE identification is discussed in (Nongmeikapam and Bandyopadhyay, 2010). There are no published works on reduplicated MWE identification in Bengali. 3 Reduplication of Words in Bengali Identification of MWEs is done during the tokenization phase and is absolutely necessary 73 Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 73–76, Beijing, August 2010 during POS tagging as is outlined in (Thoudam and Bandyopadhyay, 2008). POS tagger identifies MWE as unknown word at token level. Bengali Shallow Parser 1 can only identify hyphened reduplication and gives them separate tags like RDP (reduplication) or ECH (echo). Another objective for identifying reduplicated MWEs is to extract correct sense of reduplicated MWEs as discussed in Section 3.2. Sometime, reduplication is used for sentiment marking to identify whether the speaker uses it in positive or negative sense. For example, (i) Eto Bara Bara Asha Kisher?(Why are you thinking so high?) (Positive Sense) (ii) Ki Bara Bara Bari Ekhane! (Here, the buildings are ve"
W10-3811,D07-1091,0,0.0754618,"Missing"
W10-3811,D07-1077,0,0.0181602,"ng perl module Parse::RecDescent. By doing this, the SVO order of English is changed to SOV order for Manipuri, and post modifiers are converted to pre-modifiers. The basic difference of Manipuri phrase order compared to English is handled by reordering the input sentence following the rule (Rao et al., 2000): SS mV VmOOmC m C'mS'mS'O'mO'V'mV' where, S: Subject O: Object V : Verb Cm: Clause modifier X': Corresponding constituent in Manipuri, where X is S, O, or V Xm: modifier of X There are two reasons why the syntactic reordering approach improves over the baseline phrase-based SMT system (Wang et al., 2007). One obvious benefit is that the word order of the transformed source sentence is much closer to the target sentence, which reduces the reliance on the distortion model to perform reordering during decoding. Another potential benefit is that the alignment between the two sides will be of higher quality because of fewer “distortions” between the source and the target, so that the resulting phrase table of the reordered system would be better. However, a counter argument is that the reordering is very error prone, so that the added noise in the reordered data actually hurts the alignments and h"
W10-3811,E06-1032,0,\N,Missing
W10-3811,P02-1040,0,\N,Missing
W10-3811,P07-2045,0,\N,Missing
W10-3811,P08-1087,0,\N,Missing
W10-3811,J04-2003,0,\N,Missing
W10-3811,P09-1090,0,\N,Missing
W10-3811,I08-3015,1,\N,Missing
W11-0803,W03-1812,0,0.298219,"Missing"
W11-0803,W10-3710,1,0.445025,"Missing"
W11-0803,J90-1003,0,0.11651,"ental results and the various observations derived from our research are discussed in Section 5. Finally, Section 6 concludes the paper. 8 Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 8–13, c Portland, Oregon, USA, 23 June 2011. 2011 Association for Computational Linguistics 2 Related Work A number of research activities regarding MWE identification have been carried out in various languages like English, German and many other European languages. The statistical co-occurrence measurements such as Mutual Information (MI) (Church and Hans, 1990), Log-Likelihood (Dunning, 1993) and Salience (Kilgarriff and Rosenzweig, 2000) have been suggested for identification of MWEs. An unsupervised graph-based algorithm to detect the compositionality of MWEs has been proposed in (Korkontzelos and Manandhar 2009). In case of Indian languages, an approach in compound noun MWE extraction (Kunchukuttan and Damani, 2008) and a classification based approach for Noun-Verb collocations (Venkatapathy and Joshi, 2009) have been reported. In Bengali, the works on automated extraction of MWEs are limited in number. One method of automatic extraction of Noun-"
W11-0803,P09-2017,0,0.0219319,", pages 8–13, c Portland, Oregon, USA, 23 June 2011. 2011 Association for Computational Linguistics 2 Related Work A number of research activities regarding MWE identification have been carried out in various languages like English, German and many other European languages. The statistical co-occurrence measurements such as Mutual Information (MI) (Church and Hans, 1990), Log-Likelihood (Dunning, 1993) and Salience (Kilgarriff and Rosenzweig, 2000) have been suggested for identification of MWEs. An unsupervised graph-based algorithm to detect the compositionality of MWEs has been proposed in (Korkontzelos and Manandhar 2009). In case of Indian languages, an approach in compound noun MWE extraction (Kunchukuttan and Damani, 2008) and a classification based approach for Noun-Verb collocations (Venkatapathy and Joshi, 2009) have been reported. In Bengali, the works on automated extraction of MWEs are limited in number. One method of automatic extraction of Noun-Verb MWE in Bengali (Agarwal et al., 2004) has been carried out using significance function. In contrast, we have proposed a clustering technique to identify Bengali MWEs using semantic similarity measurement. It is worth noting that the conducted experiments"
W11-0803,passonneau-2006-measuring,0,0.0269213,"Missing"
W11-0803,H05-1113,0,0.0368216,"Missing"
W11-0904,baccianella-etal-2010-sentiwordnet,0,0.0590986,"Missing"
W11-0904,banea-etal-2008-bootstrapping,0,0.0707488,"Missing"
W11-0904,S07-1094,0,0.0622787,"Missing"
W11-0904,J02-3001,0,0.0596275,"Missing"
W11-0904,S07-1067,0,0.045748,"Missing"
W11-0904,S10-1077,1,0.88507,"Missing"
W11-0904,strapparava-valitutti-2004-wordnet,0,0.169951,"Missing"
W11-0904,stoyanov-cardie-2008-annotating,0,0.18126,"o annotate emotional content. Our motivation is that though events and sentiments are closely coupled with each other from social, psychological and commercial perspectives, very little attention has been given about their detection and analysis. To the best of our knowledge, only a few tasks have been attempted (Fukuhara et al., 2007) (Das et al., 2010). 21 Sometimes, the opinion topics are not necessarily spatially coherent as there may be two opinions in the same sentence on different topics, as well as opinions that are on the same topic separated by opinions that do not share that topic (Stoyanov and Cardie 2008). The authors have established their hypothesis by applying the coreference technique. Similarly, we have adopted the co-reference technique based on basic rhetoric components for identifying the association between event and sentiment expressions. In addition to that, we have also employed the lexical equivalence approach for identifying their association. 3 Event Identification In this work, we propose a hybrid approach for event identification from the text under the TempEval-2010 framework. We use Conditional Random Field (CRF) as the underlying machine learning algorithm. We observe that"
W11-0904,de-marneffe-etal-2006-generating,0,\N,Missing
W11-0904,P00-1010,0,\N,Missing
W11-0904,W00-1308,0,\N,Missing
W11-0904,S07-1013,0,\N,Missing
W11-0904,S10-1063,0,\N,Missing
W11-0904,N04-1030,0,\N,Missing
W11-1710,S07-1022,0,0.050743,"Missing"
W11-1710,baccianella-etal-2010-sentiwordnet,0,0.442447,"Missing"
W11-1710,banea-etal-2008-bootstrapping,0,0.0606121,"Missing"
W11-1710,esuli-sebastiani-2006-sentiwordnet,0,0.559548,"Missing"
W11-1710,P97-1023,0,0.0953214,"e rest of the paper is organized as follows. Different developmental phases of the Japanese WordNet Affect are described in Section 3. Prepa3 http://wordnet.princeton.edu/wordnet/download/ http://sentiwordnet.isti.cnr.it/ 5 http://mecab.sourceforge.net/ 6 http://translate.google.com/# 4 81 ration of the translated Japanese corpus, different experiments and evaluations based on morphology and the annotated emotion scores are elaborated in Section 4. Finally Section 5 concludes the paper. 2 Related Works The extraction and annotation of subjective terms started with machine learning approaches (Hatzivassiloglou and McKeown, 1997). Some well known sentiment lexicons have been developed, such as subjective adjective list (Baroni and Vegnaduzzo, 2004), English SentiWordNet (Esuli et. al., 2006), Taboada’s adjective list (Voll and Taboada, 2007), SubjectivityWord List (Banea et al., 2008) etc. Andreevskaia and Bergler (2006) present a method for extracting positive or negative sentiment bearing adjectives from WordNet using the Sentiment Tag Extraction Program (STEP). The proposed methods in (Wiebe and Riloff, 2006) automatically generate resources for subjectivity analysis for a new target language from the available res"
W11-1710,W10-0204,0,0.0222294,"y in the experiments on English as well as Japanese lexicons. But, it was also aimed for sentiment bearing words. Instead of English WordNet Affect (Strapparava and Valitutti, 2004), there are a few attempts in other languages such as, Russian and Romanian (Bobicev et al., 2010), Bengali (Das and Bandyopadhyay, 2010) etc. Our present approach is similar to some of these approaches but in contrast, we have evaluated our Japanese WordNet Affect on the SemEval 2007 affect sensing corpus translated into Japanese. In recent trends, the application of mechanical turk for generating emotion lexicon (Mohammad and Turney, 2010) shows promising results. In the present task, we have incorporated the open source, available and accessible resources to achieve our goals. 3 3.1 Developmental Phases WordNet Affect The English WordNet Affect, based on Ekman’s six emotion types is a small lexical resource compared to the complete WordNet but its affective annotation helps in emotion analysis. Some collection of WordNet Affect synsets was provided as a resource for the shared task of Affective Text in SemEval2007. The whole data is provided in six files named by the six emotions. Each file contains a list of synsets and one s"
W11-1710,strapparava-valitutti-2004-wordnet,0,0.671974,"Missing"
W11-1710,W09-3401,0,\N,Missing
W11-1710,S07-1013,0,\N,Missing
W11-1710,P05-1017,1,\N,Missing
W11-2839,T75-2001,0,0.665317,"ropean language family, widely spoken on six continents. HOO shared task is organized to help authors with the writing tasks. Identifying grammatical and linguistic errors in a text of a language is an open challenge to the researchers. In recent times, researchers (Heidorn, 2000) have acquired quite a benchmark for spell checker and grammar checkers, which is commonly available. In this task it is aimed to correct errors beyond the scope of these commonly available checkers i.e. detection and correction of jarring errors at part-of-speech (POS) level, syntax level and semantic level. Earlier Heidorn, 1975) developed augmented phrase structure grammar. Tetreault et. 250 Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 250–253, c Nancy, France, September 2011. 2011 Association for Computational Linguistics al., 2008, has dealt with error pattern with preposition by non-native speakers. 3 System Description At the beginning of the work, we found that generation of list of rules to detect and correct the probable linguistic errors is a non-exhaustive set. So we have decided to list out the errors from the training corpus documents. We have listed the errors doc"
W11-2839,W10-4236,0,0.221341,"Missing"
W11-2839,C08-1109,0,0.156742,"Missing"
W11-3105,W09-2704,0,0.0332419,"Missing"
W11-3105,P89-1020,0,0.163086,"expressions mainly belong to the adjective (like good, better, best) or the adverbs followed by the adjective (like more popular, most popular). The expression depicts the degree of comparison (e.g. general or positive/ negative, comparative, superlative). The Comparative or Evaluative expression may (for example, cheapest hotel to stay in Las Vegas where ‘cheapest’ is the comparative expression) or may not be (for example, best hotel to stay in Las Vegas where ‘best’ is comparative expression) directly quantifiable. So, a mechanism is necessary to convert all the Comparative 2 Related works Friedman (1989) presents a general approach to process comparative expressions by syntactically treating them to conform to a standard form containing the comparative operator and the clauses that are involved in the comparison. Another ap29 Proceedings of the KRAQ11 Workshop, pages 29–37, Chiang Mai, Thailand, November 12, 2011. proach would be to automatically extract comparative relations in sentences via machine learning. Olawsky (1989) attempts to study the semantic context by generating a set of candidate interpretations of comparative expressions. Then, the user is prompted to choose among these to sp"
W11-3105,P89-1021,0,0.37881,"est hotel to stay in Las Vegas where ‘best’ is comparative expression) directly quantifiable. So, a mechanism is necessary to convert all the Comparative 2 Related works Friedman (1989) presents a general approach to process comparative expressions by syntactically treating them to conform to a standard form containing the comparative operator and the clauses that are involved in the comparison. Another ap29 Proceedings of the KRAQ11 Workshop, pages 29–37, Chiang Mai, Thailand, November 12, 2011. proach would be to automatically extract comparative relations in sentences via machine learning. Olawsky (1989) attempts to study the semantic context by generating a set of candidate interpretations of comparative expressions. Then, the user is prompted to choose among these to specify his intent. Kennedy (2006) proposed that comparisons may be in relation to properties within the same object, degree of comparisons of the same property between different objects, or different properties of different objects. The properties at stake in the comparison are embedded in the semantics of the words in the question, and possibly in the context that comes with the question. To date, there is obviously no widely"
W11-3709,H05-1073,0,0.0326818,"natural language. Natural language domains such as News (Strapparava and Mihalcea, 2007) and Blogs (Mishne and Rijke, 2006) are also becoming a popular, communicative and informative repository of text based emotional contents in the Web 2.0 for mining and summarizing opinion at word, sentence and document level granularities (Ku et al., 2006). The model proposed in (Neviarouskaya et al., 2007) processes symbolic cues and employs NLP techniques to estimate the affects in text. Machine learning techniques were used either to predict text-based emotions based on the SNoW learning architecture (Alm et al., 2005) or to identify the mood of the authors during reading and writing (Yang et al., 2009). The ISEAR corpus was used in (Boldrini et al., 2010) for the experiments concerning emotional expressions and fine-grained analysis of affect in text. Their aim was to build the subjectivity expression models and they did not explore the intensity or physiological variables in case of identifying emotions. Psychiatric query document retrieval can assist individuals to locate query documents relevant to their depression-related problems efficiently and effectively (Yeh et. al., 2008). A DSM-IV based screenin"
W11-3709,baccianella-etal-2010-sentiwordnet,0,0.0158159,"Missing"
W12-0113,W10-3710,1,0.889358,"Missing"
W12-0113,Y04-1013,0,0.0812868,"Missing"
W12-0113,N03-1017,0,0.0314252,"Missing"
W12-0113,E03-1035,0,0.0882408,"Missing"
W12-0113,W03-1803,0,0.0914701,"Missing"
W12-0113,C04-1114,0,\N,Missing
W12-0113,P02-1040,0,\N,Missing
W12-0113,W05-0909,0,\N,Missing
W12-0113,P07-2045,0,\N,Missing
W12-0113,W09-3539,1,\N,Missing
W12-0113,W09-2907,0,\N,Missing
W12-0113,C08-1125,0,\N,Missing
W12-0113,N10-1029,0,\N,Missing
W12-0113,W04-3250,0,\N,Missing
W12-0113,P03-1021,0,\N,Missing
W12-2023,W11-2844,0,0.0398385,"Missing"
W12-2023,C10-2031,0,0.0205571,"benchmark for spell checker and grammar checkers, which is commonly available. In this task it is aimed to correct errors beyond the scope of these commonly available checkers i.e. detection and correction of jarring errors at part-ofspeech (POS) level, syntax level and semantic level. Earlier Heidorn (1975) developed augmented phrase structure grammar. (Tetreault et. al., 2008) has dealt with error pattern with preposition by non-native speakers. Meurers and Wunsch (2010) showed a surface based state-of-the-art machine learning technique, which deals with some frequently used prepositions. (Elghafari et al., 2010) worked on Data-Driven Prediction of Prepositions in English. Boyd et al. (2011) used an n-gram based machine-learning approach. Last year we have also participated in this shared task; our system report was reported in (Bhaskar et. al., 2011). 3 Corpus Statistics There are two sets of data, training set and test set provided by the organizer. The training set has 1000 documents, which are collected from the FCE dataset. The publicly available dataset was in the native FCE format. So, the organizer first converted it to the HOO data format. Then CUP annotators found the errors and marked them"
W12-2023,T75-2001,0,0.610787,", c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics shared task is organized to help authors with writing tasks. Identifying grammatical and linguistic errors in text is an open challenge to researchers. In recent times, researchers (Heidorn, 2000) have provided quite a benchmark for spell checker and grammar checkers, which is commonly available. In this task it is aimed to correct errors beyond the scope of these commonly available checkers i.e. detection and correction of jarring errors at part-ofspeech (POS) level, syntax level and semantic level. Earlier Heidorn (1975) developed augmented phrase structure grammar. (Tetreault et. al., 2008) has dealt with error pattern with preposition by non-native speakers. Meurers and Wunsch (2010) showed a surface based state-of-the-art machine learning technique, which deals with some frequently used prepositions. (Elghafari et al., 2010) worked on Data-Driven Prediction of Prepositions in English. Boyd et al. (2011) used an n-gram based machine-learning approach. Last year we have also participated in this shared task; our system report was reported in (Bhaskar et. al., 2011). 3 Corpus Statistics There are two sets of"
W12-2023,C08-1109,0,0.063479,"Missing"
W12-2023,W11-2839,1,0.834918,"Missing"
W12-2023,W10-4236,0,0.0126575,"writing, which makes the papers harder for the reviewer to understand the intentions of author. This kind of problem will be faced in any field where someone has to provide material in a language other than his/her first language. The mentoring service of Association for Computational Linguistics (ACL) is one part of a response. This service can address a wider range of problems than those related purely to writing. The aim of this service is that a research paper should be judged only on its research content. The organizer of “Help Our Own” (HOO) proposed and initiated a shared task in 2011 (Dale and Kilgarriff, 2010), which attempts to tackle the problem by developing tools or techniques for the non-native speaker of English, which will automatically correct the English prose of the papers so that they can be accepted. This tools and techniques may also help native English speakers. This task is simply expressed as text-to-text generation or Natural language Generation (NLG). In the 2011 shared task, all possible errors were covered which made the task enormously huge. In 2012, the task is more specific and only deals with determiners and prepositions as described in (Dale and Kilgarriff, 2011). For this"
W12-2023,W11-2838,0,0.0133864,"k in 2011 (Dale and Kilgarriff, 2010), which attempts to tackle the problem by developing tools or techniques for the non-native speaker of English, which will automatically correct the English prose of the papers so that they can be accepted. This tools and techniques may also help native English speakers. This task is simply expressed as text-to-text generation or Natural language Generation (NLG). In the 2011 shared task, all possible errors were covered which made the task enormously huge. In 2012, the task is more specific and only deals with determiners and prepositions as described in (Dale and Kilgarriff, 2011). For this shared task, HOO, we have developed two models, one is rule-based model and the other is the statistical model for both determiners and prepositions. Then we have combined both these models and developed our system for HOO 2012. 2 Related Work The English language belongs to the Germanic languages branch of the Indo-European language family, widely spoken on six continents. The HOO 201 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 201–207, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics shared task is o"
W12-2023,dale-narroway-2012-framework,0,0.0113787,"em elevates the accuracy. 5 6 Evaluation The system was evaluated for its performance in detecting, recognizing and correcting preposition and determiner errors in English documents. Separate scores were calculated for detection, recognition and correction for both the errors of preposition and determiner separately and then combined scores were also calculated. For all results, the organizer has provided three measures: Precision, Recall and F-Score. The precise definitions of these measures as implemented in the evaluation tool, and further details on the evaluation process are provided in (Dale and Narroway, 2012) and elaborated on at the HOO website.4 Each team was allowed to submit up to 10 separate runs over the test data, thus allowing them to have different configurations of their systems evalElement Preposition Determiner Combined uated. Teams were asked to indicate whether they had used only publicly available data to train their systems, or whether they had made use of privately held data. We have submitted only one run (JU_run1) which has demonstrated F-scores of 7.1, 6.46 and 2.58 for detection, recognition and correction respectively before revision. And after revision it has demonstrated F-"
W12-2023,W12-2006,0,0.027615,"Missing"
W12-5003,H01-1069,0,0.637102,"Missing"
W12-5003,D08-1097,0,0.35643,"Missing"
W12-5003,P07-1098,0,0.088974,"Missing"
W12-5003,C02-1150,0,0.140849,"Missing"
W12-5003,D09-1057,0,0.0415872,"Missing"
W12-5004,O12-1029,1,0.824052,"from the Bible and the story books and the stemmed data are checked manually. Then we have assigned the POS and the English meaning manually. 46 The stemming algorithm used for the development of root dictionary is given below and Table 9 shows the root dictionary entries. Kokborok Achuk POS Verb English Sit TABLE 9 – Root Dictionary Entries. 3.2 Stemming algorithm for generating Root Words The algorithm is designed to remove both multiple suffixes as well as prefixes from the inflected words. It has been observed that the boundary of root words in Kokborok change after addition of suffixes (Patra et al., 2012). Thus we have added some rules in the algorithm as boundary changes after addition of suffixes. The algorithm is given below. 3.2.1 Prefix Stripping Algorithm 1. 2. repeat the step 2 until all the prefixes are removed read the prefix, if matched then store it in array and decrease the length of string else read another prefix. 3. If length of string &gt;2 then go for suffix stripping, else exit 3.2.2 1. 2. 3. Suffix Stripping Algorithm repeat the step 2 until all the suffixes are removed read the largest suffix, if matched then check for rules. then store it in array and decrease the length of s"
W12-6002,P07-1098,0,0.0370943,"ted in third section and fourth section respectively; Corpus for procedural text and evaluation are described in fifth section and sixth section respectively; and finally seventh section describes the conclusions of our study and outlines directions of our future work. 2 Related Work Question classification in TREC QA has been intensively studied during the past decade. Many researchers have employed machine learning methods (e.g., maximum entropy and support vector machine) by using different features, such as syntactic features (Zhang et al., 2003; Nguyen et al, 2008) and semantic features (Moschitti et al, 2007). However, these methods mainly focused on factoid questions and confined themselves to classify a question into two or a few predefined categories (e.g., ""what"", ""how"", ""why"", ""when"", ""where"" and so on). However, question classification in procedural text is dramatically different from factoid question classification. Therefore, traditional methods may fail to achieve the satisfactory results. Research on procedural texts was initiated by works in psychology, cognitive ergonomics, and didactics, (Mortara et al., 1988), (Greimas, 1983), (Kosseim, 2000) to cite just a few. The issues of title i"
W12-6002,delpech-saint-dizier-2008-investigating,0,0.0245214,"damage the connector”. So, performer of a procedure pay much attention about this type of instructions and forms query: “What are the warnings for <X>?” or “What are the instructions must follow for <X>?”; where X= “procedure name” . The pattern could be- R10: <WH><WARN/PREV VERB><X> and R11: <WH><PREV VERB><X> Where, WARN VERB=Warning; PREV VERB= Prevention verb, e.g., “risk”, “avoid”, “damage” etc., X= “Procedure name” Simple Instruction (SI) Associated Questions Identification: More often an instruction has no support and considered as simple instruction or instruction with empty support (Delpech and Saint-Dizier, 2008). For example, “Add the chili powder, salt and tomatoes.”, “Heat oil in pan, fry the onions and green chilies.” So, queries on these action instructions are aimed to extract the timing of action. For example, in cooking recipe, the query could be “When to add chili powder in cooking chilly chicken? Most of the cases, the answer may be after completion of the preceding action instruction or before completion of the following action instruction. So, the query of this type often is in the form: R12: <WH><ACTION VERB><ITEM><X>? Where, ACTION VERB= Action verb, e.g. “do”, “perform”, “add”, “start”"
W12-6002,P05-1045,0,0.0148787,"Missing"
W12-6002,N03-1033,0,0.00633067,"Missing"
W12-6002,W08-1606,0,0.242989,"llenges of answering procedural questions from procedural text have been investigated (Saint-Dizier P, 2008) and procedural title identification and tagging, instructions and instruction arguments have also been investigated and processed. Parsing and analyzing argumentative structures in procedural texts have been addressed successfully (Fontan et al., 2008). A conceptual categorization of procedural questions based on verb categories has also been addressed for French (Aouladomar et al., 2005).Also, identification of advice and warning structures from procedural texts has been investigated (Fontan and Saint-Dizier, 2008). A quite large corpus (about 1700 texts) from several domains (basic: cooking, do it yourself, gardening, and complex: social relations, health) and a large number of web sites have been constructed for experiment and it has been found that warnings are basically organized around an ‘avoid expression’ combined with a proposition. During the last decade, a number of researches have been done on addressing procedural text structure for various domains. But, those investigations were only carried out for French language and unfortunately, so far fewer researches have been carried out for classif"
W12-6002,E06-3006,0,0.0211436,"ow-questions’, are questions whose induced response is typically a fragment, more or less large, of a procedure, i.e., a set of coherent instructions designed to reach a goal. Answering procedural questions thus requires being able to extract well-formed text structure unlike factoid question and analyzing a procedural text requires a dedicated discourse analysis, e.g. by means of a grammar (Delpech et al., 2008). Though less research has been conducted so far on other types of non-factoid QA, such as why-questions (Verberne et al.,2007; Pechsiri et al,2008) and procedural (how-to) questions (Yin, 2006; Delpech et al., 2008), during the last decade challenges of procedural text and argument extraction have been addressed (Fontan et al., 2008; Adam et al., 2008). In this work, we have focused on question classification and answering from the procedural text in English and building a generic domain independent procedural question answering system. The remainder of the paper is organized as follows: in the next section, we review the related works. Corpus preparation and system description are elaborated in third section and fourth section respectively; Corpus for procedural text and evaluatio"
W12-6002,Y08-1006,0,0.287382,"answering procedural questions from procedural text have been investigated (Saint-Dizier P, 2008) and procedural title identification and tagging, instructions and instruction arguments have also been investigated and processed. Parsing and analyzing argumentative structures in procedural texts have been addressed successfully (Fontan et al., 2008). A conceptual categorization of procedural questions based on verb categories has also been addressed for French (Aouladomar et al., 2005).Also, identification of advice and warning structures from procedural texts has been investigated (Fontan and Saint-Dizier, 2008). A quite large corpus (about 1700 texts) from several domains (basic: cooking, do it yourself, gardening, and complex: social relations, health) and a large number of web sites have been constructed for experiment and it has been found that warnings are basically organized around an ‘avoid expression’ combined with a proposition. During the last decade, a number of researches have been done on addressing procedural text structure for various domains. But, those investigations were only carried out for French language and unfortunately, so far fewer researches have been carried out for classif"
W12-6002,D08-1097,0,0.400785,"Missing"
W12-6002,W08-1603,0,\N,Missing
W13-2509,P07-2045,0,0.0161102,"ing such corpus is that it can be prepared easily unlike the one that is domain specific. The effectiveness of the parallel fragments of text developed from the comparable corpora in the present work is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phrase extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002), and Moses decoder (Koehn et al., 2007). Related Work There has been a growing interest in approaches focused on extracting word translations from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Dejean et al., 2002; Kaji, 2005; Gamallo, 2007; Saralegui et al., 2008). Most of the strategies follow a standard method based on context similarity. The idea behind this method is as follows: A target word t is the translation of a source word s if the words with which t co-occurs are translations of words with which s co-occurs. The basis of the method is to find the target words th"
W13-2509,N03-1017,0,0.00632178,"the other details are discarded. It is evident that the corpus is not confined to any particular domain. The challenge is to exploit this kind of corpus to help machine translation systems improve. The advantage of using such corpus is that it can be prepared easily unlike the one that is domain specific. The effectiveness of the parallel fragments of text developed from the comparable corpora in the present work is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phrase extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002), and Moses decoder (Koehn et al., 2007). Related Work There has been a growing interest in approaches focused on extracting word translations from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Dejean et al., 2002; Kaji, 2005; Gamallo, 2007; Saralegui et al., 2008). Most of the strategies follow a standard method based on context similarity. The idea behind thi"
W13-2509,P06-1011,0,0.288737,"tion of each other. And as a result, parallel fragments of text are rarely found in these document pairs. The bigger the size of the fragment the less probable it is to find its parallel version in the target side. Nevertheless, there is always chance of getting parallel phrase, tokens or even sentences in comparable documents. The challenge is to find those parallel texts which can be useful in increasing machine translation performance. In our present work, we have concentrated on finding small fragments of parallel text instead of rigidly looking for parallelism at entire sentential level. Munteanu and Marcu (2006) believed that comparable corpora tend to have parallel data at sub-sentential level. This approach is particularly useful for this type of corpus under consideration, because there is a very little chance of getting exact translation of bigger fragments of text in the target side. Instead, searching for parallel chunks would be more logical. If a sentence in the source side has a parallel sentence in the target side, then all of its chunks need to have their parallel translations in the target side as well. It is to be noted that, although we have document level alignment in our corpus, it is"
W13-2509,C02-2020,0,0.680158,"Missing"
W13-2509,P03-1021,0,0.023157,"at the corpus is not confined to any particular domain. The challenge is to exploit this kind of corpus to help machine translation systems improve. The advantage of using such corpus is that it can be prepared easily unlike the one that is domain specific. The effectiveness of the parallel fragments of text developed from the comparable corpora in the present work is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phrase extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002), and Moses decoder (Koehn et al., 2007). Related Work There has been a growing interest in approaches focused on extracting word translations from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Dejean et al., 2002; Kaji, 2005; Gamallo, 2007; Saralegui et al., 2008). Most of the strategies follow a standard method based on context similarity. The idea behind this method is as follows: A target word t i"
W13-2509,2007.mtsummit-papers.26,0,0.920458,"Missing"
W13-2509,P02-1040,0,0.0925493,"Missing"
W13-2509,P99-1067,0,0.163851,"ent work is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phrase extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002), and Moses decoder (Koehn et al., 2007). Related Work There has been a growing interest in approaches focused on extracting word translations from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Dejean et al., 2002; Kaji, 2005; Gamallo, 2007; Saralegui et al., 2008). Most of the strategies follow a standard method based on context similarity. The idea behind this method is as follows: A target word t is the translation of a source word s if the words with which t co-occurs are translations of words with which s co-occurs. The basis of the method is to find the target words that have the most similar distributions with a given source word. The starting point of this method is a list of bilingual expressions that are used to build the context vectors of al"
W13-2509,N10-1063,0,0.0933741,"Missing"
W13-2509,W97-0119,0,\N,Missing
W13-2509,P98-1069,0,\N,Missing
W13-2509,C98-1066,0,\N,Missing
W13-2814,P06-1097,0,0.019944,"re capitalization cues have been used for identifying NEs on the English side. Statistical techniques are applied to decide which portion of the target language corresponds to the specified English NE, for simultaneous NE identification and translation. To improve the learning process of unlabeled data using labeled data (Chapelle et al., 2006), the semi-supervised learning method is the most useful learning technique. Semi-supervised learning is a broader area of Machine Learning. Researchers have begun to explore semisupervised word alignment models that use both labeled and unlabeled data. Fraser and Marcu (2006) proposed a semi-supervised training algo95 guage resources in Bengali are not widely available. 3 ods. Our approach deals with the latter case. The supervised technique of Berkeley aligner helps us to align those words which could not be aligned by rule based word aligner. Hybrid Word Alignment Model The hybrid word alignment model is described as the combination of three word alignment models as follows: 3.1 3.3 The proposed Rule based aligner aligns Named Entities (NEs) and chunks. For NE alignment, we first identify NEs from the source side (i.e. English) using Stanford NER. The NEs on the"
W13-2814,J93-2003,0,0.05003,"Model for Phrase-Based Statistical Machine Translation Santanu Pal*, Sudip Kumar Naskar† and Sivaji Bandyopadhyay* * Department of Computer Science & Engineering Jadavpur University, Kolkata, India santanu.pal.ju@gmail.com, sivaji_cse_ju@yahoo.com † Department of Computer & System Sciences Visva-Bharati University, Santiniketan, India sudip.naskar@gmail.com tion of each other. Statistical machine translation usually suffers from many-to-many word links which existing statistical word alignment algorithms can not handle well. The unsupervised word alignment models are based on IBM models 1–5 (Brown et al., 1993) and the HMM model (Ney and Vogel, 1996; Och and Ney, 2003). Models 3, 4 and 5 are based on fertility based models which are asymmetric. To improve alignment quality, the Berkeley Aligner is based on the symmetric property by intersecting alignments induced in each translation direction. In the present work, we propose improvement of word alignment quality by combining three word alignment tables (i) GIZA++ alignment (ii) Berkeley Alignment and (iii) rule based alignment. Our objective is to perceive the effectiveness of the Hybrid model in word alignment by improving the quality of translatio"
W13-2814,P04-1023,0,0.0291512,"scribed in Section 3. Section 4 presents the tools and resources used for the various experiments. Section 5 includes the results obtained, together with some analysis. Section 6 concludes and provides avenues for further work. 2 rithm. The weighting parameters are learned from discriminative error training on labeled data, and the parameters are estimated by maximum-likelihood EM training on unlabeled data. They have also used a log-linear model which is trained on the available labeled data to improve performance. Interpolating human alignments with automatic alignments has been proposed by Callison-Burch et al. (2004), where the alignments of higher quality have gained much higher weight than the lower-quality alignments. Wu et al. (2006) have developed two separate models of standard EM algorithm which learn separately from both labeled and unlabeled data. Two models are then interpolated as a learner in the semisupervised Ada-Boost algorithm to improve word alignment. Ambati et al. (2010) proposed active learning query strategies to identify highly uncertain or most informative alignment links under an unsupervised word alignment model. Intuitively, multiword NEs on the source and the target sides should"
W13-2814,C04-1114,0,0.0277213,"ork, we have implemented a rule based alignment model by considering several types of chunks which are automatically extracted on the source side. Each individual source chunk is translated using a baseline PBSMT system and validated with the target chunks on the target side. The validated source-target chunks are added in the rule based alignment table. Work has been carried out into three directions: (i) three alignment tables are combined together by taking their union; (ii) extra alignment pairs are added into the alignment table. This is a well-known practice in domain adaptation in SMT (Eck et al., 2004; Wu et al., 2008); (iii) the alignment table is updated through semisupervised alignment technique. Abstract This paper proposes a hybrid word alignment model for Phrase-Based Statistical Machine translation (PB-SMT). The proposed hybrid alignment model provides most informative alignment links which are offered by both unsupervised and semi-supervised word alignment models. Two unsupervised word alignment models (GIZA++ and Berkeley aligner) and a rule based aligner are combined together. The rule based aligner only aligns named entities (NEs) and chunks. The NEs are aligned through translit"
W13-2814,W09-3539,1,0.819859,"re not widely available. 3 ods. Our approach deals with the latter case. The supervised technique of Berkeley aligner helps us to align those words which could not be aligned by rule based word aligner. Hybrid Word Alignment Model The hybrid word alignment model is described as the combination of three word alignment models as follows: 3.1 3.3 The proposed Rule based aligner aligns Named Entities (NEs) and chunks. For NE alignment, we first identify NEs from the source side (i.e. English) using Stanford NER. The NEs on the target side (i.e. Bengali) are identified using a method described in (Ekbal and Bandyopadhyay, 2009). The accuracy of the Bengali Named Entity recognizers (NER) is much poorer compared to that of English NER due to several reasons: (i) there is no capitalization cue for NEs in Bengali; (ii) most of the common nouns in Bengali are frequently used as proper nouns; (iii) suffixes (case markers, plural markers, emphasizers, specifiers) get attached to proper names as well in Bengali. Bengali shallow parser 1 has been used to improve the performance of NE identification by considering proper names as NE. Therefore, NER and shallow parser are jointly employed to detect NEs from the Bengali sentenc"
W13-2814,W04-3248,0,0.105861,"ou et. al., 2004). Pal et, al. (2012) proposed a bootstrapping method for chunk alignment; they used an SMT based model for chunk translation and then aligned the sourcetarget chunk pairs after validating the translated chunk. Ma et. al. (2007) simplified the task of automatic word alignment as several consecutive words together correspond to a single word in the opposite language by using the word aligner itself, i.e., by bootstrapping on its output. A Maximum Entropy model based approach for English—Chinese NE alignment which significantly outperforms IBM Model4 and HMM has been proposed by Feng et al. (2004). They considered 4 features: translation score, transliteration score, source NE and target NE&apos;s co-occurrence score and the distortion score for distinguishing identical NEs in the same sentence. Moore (2003) presented an approach where capitalization cues have been used for identifying NEs on the English side. Statistical techniques are applied to decide which portion of the target language corresponds to the specified English NE, for simultaneous NE identification and translation. To improve the learning process of unlabeled data using labeled data (Chapelle et al., 2006), the semi-supervi"
W13-2814,J03-1002,0,0.189531,"anu Pal*, Sudip Kumar Naskar† and Sivaji Bandyopadhyay* * Department of Computer Science & Engineering Jadavpur University, Kolkata, India santanu.pal.ju@gmail.com, sivaji_cse_ju@yahoo.com † Department of Computer & System Sciences Visva-Bharati University, Santiniketan, India sudip.naskar@gmail.com tion of each other. Statistical machine translation usually suffers from many-to-many word links which existing statistical word alignment algorithms can not handle well. The unsupervised word alignment models are based on IBM models 1–5 (Brown et al., 1993) and the HMM model (Ney and Vogel, 1996; Och and Ney, 2003). Models 3, 4 and 5 are based on fertility based models which are asymmetric. To improve alignment quality, the Berkeley Aligner is based on the symmetric property by intersecting alignments induced in each translation direction. In the present work, we propose improvement of word alignment quality by combining three word alignment tables (i) GIZA++ alignment (ii) Berkeley Alignment and (iii) rule based alignment. Our objective is to perceive the effectiveness of the Hybrid model in word alignment by improving the quality of translation in the SMT system. In the present work, we have implement"
W13-2814,W03-1502,0,0.0165294,"unk. Ma et. al. (2007) simplified the task of automatic word alignment as several consecutive words together correspond to a single word in the opposite language by using the word aligner itself, i.e., by bootstrapping on its output. A Maximum Entropy model based approach for English—Chinese NE alignment which significantly outperforms IBM Model4 and HMM has been proposed by Feng et al. (2004). They considered 4 features: translation score, transliteration score, source NE and target NE&apos;s co-occurrence score and the distortion score for distinguishing identical NEs in the same sentence. Moore (2003) presented an approach where capitalization cues have been used for identifying NEs on the English side. Statistical techniques are applied to decide which portion of the target language corresponds to the specified English NE, for simultaneous NE identification and translation. To improve the learning process of unlabeled data using labeled data (Chapelle et al., 2006), the semi-supervised learning method is the most useful learning technique. Semi-supervised learning is a broader area of Machine Learning. Researchers have begun to explore semisupervised word alignment models that use both la"
W13-2814,P06-2117,0,0.0375047,"Missing"
W13-2814,N03-1017,0,0.0334238,"e source chunks into the target language using a baseline PB-SMT system and subsequently validating the target chunks using a fuzzy matching technique against the target corpus. All the experiments are carried out after single-tokenizing the multi-word NEs. Our best system provided significant improvements over the baseline as measured by BLEU. 1 Introduction Word alignment is the backbone of PB-SMT system or any data driven approaches to Machine Translation (MT) and it has received a lot of attention in the area of statistical machine translation (SMT) (Brown et al., 1993; Och and Ney, 2003; Koehn et al., 2003). Word alignment is not an end task in itself and is usually used as an intermediate step in SMT. Word alignment is defined as the detection of corresponding alignment of words from parallel sentences that are transla94 Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 94–101, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics The remainder of the paper is organized as follows. Section 2 discusses related work. The proposed hybrid word alignment model is described in Section 3. Section 4 presents the tools and resources used for the va"
W13-2814,P07-2045,0,0.0140433,"alignment from A2 and A3 (using A2∩A3). Step 4: Add additional entries with SA. Figure 1: Establishing alignments through Rule based methods. The extracted chunks on the source side may not have a one to one correspondence with the target side chunks. The alignment validation process is focused on the proper identification of the head words and not between the translated source chunk and target chunk. The matching process has been carried out using a fuzzy 97 3.5 language model trained using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) and the Moses decoder (Koehn et al., 2007) have been used in the present study. Berkeley Semi-supervised Alignment The correctness of the alignments is verified by manually checking the performance of the various alignment system. We start with the combined alignment table which is produced by Algorithm 1. Iinitially, we take a subset of the alignments by manually inspecting from the combined alignment table. Then we train the Barkley supervised aligner with this labeled data. A subset of the unlabeled data from the combined alignment table is tested with the supervised model. The output is then added as additional labeled training da"
W13-2814,W04-3250,0,0.170255,"Missing"
W13-2814,P03-1021,0,0.0597904,"Missing"
W13-2814,W12-0113,1,0.855575,"Missing"
W13-2814,W10-3707,1,0.62711,"engali; (ii) most of the common nouns in Bengali are frequently used as proper nouns; (iii) suffixes (case markers, plural markers, emphasizers, specifiers) get attached to proper names as well in Bengali. Bengali shallow parser 1 has been used to improve the performance of NE identification by considering proper names as NE. Therefore, NER and shallow parser are jointly employed to detect NEs from the Bengali sentences. The source NEs are then transliterated using a modified joint source-channel model (Ekbal et al., 2006) and aligned to their target side equivalents following the approach of Pal et al. (2010). The target side equivalents NEs are transformed into canonical form after omitting their ‗matras‘. Similarly Bengali NEs are also transformed into canonical forms as Bengali NEs may differ in their choice of matras (vowel modifiers). The transliterated NEs are then matched with the corresponding parallel target NEs and finally we align the NEs if match is found. After identification of multiword NEs on both sides, we pre-processed the corpus by replacing space with the underscore character (‗_‘). We have used underscore (‗_‘) instead of hyphen (‗‘) since there already exists some hyphenated"
W13-2814,P02-1040,0,0.0883936,"us respectively, of which 22,273 NEs are unique in English and 22,010 NEs in Bengali. A total of 14,023 NEs have been aligned through transliteration. The experiments have been carried out with various experimental settings: (i) single tokenization of NEs on both sides of the parallel corpus, (ii) using Berkeley Aligner with unsupervised training, (iii) union of the three alignment models: rule based, GIZA++ with GDFA and Berkeley Alignment, (iv) hybridization of the three alignment models and (v) supervised Berkeley Aligner. Eextrinsic evaluation was carried out on the MT quality using BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 3 The EILMT project is funded by the Department of Electronics and Information Technology (DEITY), Ministry of Communications and Information Technology (MCIT), Government of India. 4 http://nlp.stanford.edu/software/lex-parser.shtml 5 http://crfchunker.sourceforge.net/ 6 The IL-ILMT project is funded by the Department of Electronics and Information Technology (DEITY), Ministry of Communications and Information Technology (MCIT), Government of India. 98 Experiment BLEU NIST Baseline system using GIZA++ with GDFA Exp no. 1 10.92 4.13 PB-SMT system using Berkeley Al"
W13-2814,C96-2141,0,0.621907,"Missing"
W13-2814,C08-1125,0,0.0411533,"Missing"
W13-4104,W12-5310,0,0.169885,"osed for the image emotion classification as we cannot say a piece of music is disgust. In music psychology, our traditional approach is to describe mood using the adjective like gloomy, pathetic and hopeful etc. However, there is no standard taxonomy available which is acceptable to the researchers. Russel (1980) proposed the circumplex model of affect based on the two dimensional model. These two dimensions are denoted as “pleasant3 Figure 1. Russell’s circumplex model of 28 affects words To the best of our knowledge, no work has been carried out on Hindi music mood classification. However, Velankar and Sahasrabuddhe (2012) had worked on the preparation of data for Hindustani classical music mood classification. They have performed several sessions for classifying the three Indian Ragas into 13 mood classes. 3 Mood Taxonomy and Data Set In the present task, a standard data set has been used for the mood classification task. This data has been collected manually and prepared by five human annotators. The songs used in the experiments are collected from Indian Hindi music website 4 . This collected data set includes five moods clusters of MIREX. MIREX mood cluster follows the Theyer’s model (Thayer, 1989). 4 http:"
W13-4305,P91-1022,0,0.475773,"Missing"
W13-4305,P93-1002,0,0.0841543,"correctness of the alignment of the rule based system. The remainder of the paper is organized as follows. Next section briefly elaborates the related work. The proposed system is described in Section 3. Section 4 states the tools and resources used for the various experiments. Section 5 includes the results obtained, together with some analysis. Section 6 concludes and provides avenues for further work. 2 Related Works The works related to alignment are mostly developed for machine translation task. Some works in sentence alignment can be found in (Brown, 1991) and (Gale and Church, 1993). (Chen, 1993) developed a method which was slower but more accurate than the sentencelength based Brown and Gale algorithm. (Wu, 1994) used an approach which was adopted from Gale and Church‘s method for Chinese. They used a small corpus-specific bilingual lexicon to improve alignment accuracy in texts containing multiple sentences of similar length. (Melamed 1996, 1997) also proposed a method based on word correspondences. (Plamondon, 1998) developed a two-pass approach, in which a method similar to the one proposed by Melamed identifies points of correspondence in the text that constrain a second-pass se"
W13-4305,P07-2045,0,0.00635855,"Missing"
W13-4305,R11-1084,1,0.828413,"Missing"
W13-4305,W04-3248,0,0.0288507,"words and sentences. In the hybrid model, they used the sentence pairs that are assigned the highest probability of alignment to train a modified version of IBM Translation Model 1 (Brown, 1993). (Fung, 1994) presented K-vec, an alternative alignment strategy, that starts by estimating the lexicon. Moore (2003) used capitalization cues for identifying NEs on the English side and then applied statistical techniques to decide which portion of the target language corresponds to the specified English NE. A Maximum Entropy model based approach for English—Chinese NE alignment has been proposed in Feng et al. (2004) which significantly outperforms IBM Model 4 and HMM. A method for 1 http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallow _parser.php 37 automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization has been proposed in Huang et al. (2003). 3 3.2 It has been observed from the detailed text analysis that almost all events are associated with some actors (―anything having existence (living or nonliving)”), either active or passive. More generally, event actions are associated with persons or organizations and sometimes with l"
W13-4305,W96-0201,0,0.202806,"Missing"
W13-4305,C94-2178,0,0.220366,"Missing"
W13-4305,P97-1039,0,0.121292,"Missing"
W13-4305,moore-2002-fast,0,0.042022,"rate than the sentencelength based Brown and Gale algorithm. (Wu, 1994) used an approach which was adopted from Gale and Church‘s method for Chinese. They used a small corpus-specific bilingual lexicon to improve alignment accuracy in texts containing multiple sentences of similar length. (Melamed 1996, 1997) also proposed a method based on word correspondences. (Plamondon, 1998) developed a two-pass approach, in which a method similar to the one proposed by Melamed identifies points of correspondence in the text that constrain a second-pass search based on the statistical translation model. (Moore, 2002) developed a hybrid sentence-alignment method using sentence length-based and word-correspondencebased models. This model is fast, very accurate, and requires that the corpus be separated into words and sentences. In the hybrid model, they used the sentence pairs that are assigned the highest probability of alignment to train a modified version of IBM Translation Model 1 (Brown, 1993). (Fung, 1994) presented K-vec, an alternative alignment strategy, that starts by estimating the lexicon. Moore (2003) used capitalization cues for identifying NEs on the English side and then applied statistical"
W13-4305,E03-1035,0,0.0393444,"e in the text that constrain a second-pass search based on the statistical translation model. (Moore, 2002) developed a hybrid sentence-alignment method using sentence length-based and word-correspondencebased models. This model is fast, very accurate, and requires that the corpus be separated into words and sentences. In the hybrid model, they used the sentence pairs that are assigned the highest probability of alignment to train a modified version of IBM Translation Model 1 (Brown, 1993). (Fung, 1994) presented K-vec, an alternative alignment strategy, that starts by estimating the lexicon. Moore (2003) used capitalization cues for identifying NEs on the English side and then applied statistical techniques to decide which portion of the target language corresponds to the specified English NE. A Maximum Entropy model based approach for English—Chinese NE alignment has been proposed in Feng et al. (2004) which significantly outperforms IBM Model 4 and HMM. A method for 1 http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallow _parser.php 37 automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization has been proposed in Huan"
W13-4305,J02-3001,0,0.248147,"Missing"
W13-4305,P03-1021,0,0.0329635,"Missing"
W13-4305,W03-1502,0,0.0389762,"he text that constrain a second-pass search based on the statistical translation model. (Moore, 2002) developed a hybrid sentence-alignment method using sentence length-based and word-correspondencebased models. This model is fast, very accurate, and requires that the corpus be separated into words and sentences. In the hybrid model, they used the sentence pairs that are assigned the highest probability of alignment to train a modified version of IBM Translation Model 1 (Brown, 1993). (Fung, 1994) presented K-vec, an alternative alignment strategy, that starts by estimating the lexicon. Moore (2003) used capitalization cues for identifying NEs on the English side and then applied statistical techniques to decide which portion of the target language corresponds to the specified English NE. A Maximum Entropy model based approach for English—Chinese NE alignment has been proposed in Feng et al. (2004) which significantly outperforms IBM Model 4 and HMM. A method for 1 http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallow _parser.php 37 automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization has been proposed in Huan"
W13-4305,N03-1017,0,0.010592,"Missing"
W13-4305,J93-1004,0,\N,Missing
W13-4305,de-marneffe-etal-2006-generating,0,\N,Missing
W13-4305,W09-1104,0,\N,Missing
W13-4305,J93-2003,0,\N,Missing
W13-4305,W09-3539,1,\N,Missing
W13-4305,W04-3250,0,\N,Missing
W13-4705,P79-1022,0,0.102828,"Missing"
W13-4705,W10-3607,0,\N,Missing
W13-4705,I05-3027,0,\N,Missing
W14-5114,W10-3208,1,0.839958,"al. (2004), which significantly outperforms IBM Model 4 and HMM. Fung (1994) presented K-vec, an alternative alignment strategy that starts by estimating the lexicon. Sentiment detection is the task of determining positive or negative sentiment of words, phrases, sentences and documents. The computational approach to sentiment analysis in textual data requires annotated lexicons with polarity tags (Patra et al., 2013). Research has been carried out on building sentiment or emotional corpora in English (Strapparava and Valitutti, 2004; Baccianella et al., 2010; Patra et al., 2013) and Bengali (Das and Bandyopadhyay, 2010; Das and Bandyopadhyay, 2010a). Identifying the sentiment holder is another task closely related to subjectivity detection (Kim and Hovy, 2004). Several methods have been implemented to identify the sentiment holders such as rule based methods (using dependency information) (Kolya et al., 2012) and supervised machine learning methods (Kim and Hovy, 2004; Kolya et al., 2012). To the best of our knowledge, no prior work on improving SMT systems using aligned sentiment expressions, holders and their corresponding objects have been developed yet. There is research on creating sentiment lexica and"
W14-5114,strapparava-valitutti-2004-wordnet,0,0.263172,"Missing"
W14-5114,D08-1014,0,0.0219678,"cy information) (Kolya et al., 2012) and supervised machine learning methods (Kim and Hovy, 2004; Kolya et al., 2012). To the best of our knowledge, no prior work on improving SMT systems using aligned sentiment expressions, holders and their corresponding objects have been developed yet. There is research on creating sentiment lexica and cross-lingual sentiment identification. Automatic translation is a viable alternative for the construction of resources and tools for subjectivity or sentiment analysis in a new resource-constrained language using a resourcerich language as a starting point (Banea et al., 2008). Banea et al., (2008) generated resources for subjectivity annotation in Spanish and Romanian using English corpora. In context of Indian languages, Das et al., 2010 have developed a sentiment lexicon for Bengali Languages using an English to Bengali MT system. Similarly, a Hindi sentiment corpus has been developed using English to Hindi MT system (Balamurali et al., 2010). Hiroshi et al., (2004) developed a high-precision sentiment analysis system with low development cost, by making use of an existing transfer-based MT engine. 3 Dataset In our experiment, an English-Bengali parallel corpus"
W14-5114,P05-1033,0,0.267679,"Missing"
W14-5114,W04-3248,0,0.0173424,"nment template approach for PB-SMT (Och et al., 2004) allows many-tomany relations between words. A model that uses hierarchical phrases based on synchronous grammars is presented in (Chiang et al., 2005). To date there is little research on English-Bengali SMT: PB-SMT systems can be improved (Pal et al., 2011; 2013) by single tokenizing Multiword Expressions (MWEs) on both sides of the parallel corpus. Researches on alignment were mostly developed for MT tasks (Brown, 1991; Gale and 90 Church, 1993). A Maximum Entropy model based approach for English-Chinese NE alignment has been proposed in Feng et al. (2004), which significantly outperforms IBM Model 4 and HMM. Fung (1994) presented K-vec, an alternative alignment strategy that starts by estimating the lexicon. Sentiment detection is the task of determining positive or negative sentiment of words, phrases, sentences and documents. The computational approach to sentiment analysis in textual data requires annotated lexicons with polarity tags (Patra et al., 2013). Research has been carried out on building sentiment or emotional corpora in English (Strapparava and Valitutti, 2004; Baccianella et al., 2010; Patra et al., 2013) and Bengali (Das and Ba"
W14-5114,J04-4002,0,0.224891,"Missing"
W14-5114,C08-1125,0,0.0655339,"Missing"
W14-5114,C04-1071,0,0.0377596,"ation is a viable alternative for the construction of resources and tools for subjectivity or sentiment analysis in a new resource-constrained language using a resourcerich language as a starting point (Banea et al., 2008). Banea et al., (2008) generated resources for subjectivity annotation in Spanish and Romanian using English corpora. In context of Indian languages, Das et al., 2010 have developed a sentiment lexicon for Bengali Languages using an English to Bengali MT system. Similarly, a Hindi sentiment corpus has been developed using English to Hindi MT system (Balamurali et al., 2010). Hiroshi et al., (2004) developed a high-precision sentiment analysis system with low development cost, by making use of an existing transfer-based MT engine. 3 Dataset In our experiment, an English-Bengali parallel corpus containing 23,492 parallel sentences comprising of 488,026 word tokens from the travel and tourism domain has been used. We randomly selected 500 sentences each for the development set and the test set from the initial parallel corpus. The rest of the sentences were used as the training corpus. The training corpus was filtered with the maximum allowable sentence length of 100 words and sentence le"
W14-5114,P02-1040,0,0.107491,"rie Actions) (Grant No. 317471) and the “Development of English to Indian Languages Machine Translation (EILMT) System - Phase II” project funded by Department of Information Technology, Government of India. Our experiments have been carried out in two directions. First we improved the baseline model using the aligned sentiment phrases. Then, we automatically post-edited the translation output by using the sentiment knowledge of the source input test sentence. The evaluation results are reported in Table 1. The evaluation was carried out using well-known automatic MT evaluation metrics: BLEU (Papineni et al., 2002, NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006). In experiment 2, the extracted parallel sentiment phrase alignments are incorporated with the existing baseline phrase table and the resulting model performs better than the baseline system. Experiment 3 shows how post-editing the output of experiment 2 brings about further improvements. 7 Conclusions and Future Research In this paper, we successfully illustrated how sentiment analysis can improve the translation of an English-Bengali PB-SMT system. We have also shown how sentiment knowledge is useful"
W14-5114,N10-1029,0,0.0575798,"Missing"
W14-5114,C94-2178,0,0.372118,"Missing"
W14-5114,J93-2003,0,0.107473,"Missing"
W14-5114,N03-1017,0,0.107016,"carried out using the positional information of sentiment components. The rest of the paper is organized in the following manner. Section 2 briefly elaborates the related work. Section 3 provides an overview of the dataset used in our experiments. The proposed system is described in Section 4 while Section 5 provides the system setup for the various experiments. Section 6 includes the experiments and results obtained. Finally, Section 7 concludes and provides avenues for further work. 2 Related Work SMT systems have undergone considerable improvements over the years. Moreover, PB-SMT models (Koehn et al., 2003) outperform wordbased models. The alignment template approach for PB-SMT (Och et al., 2004) allows many-tomany relations between words. A model that uses hierarchical phrases based on synchronous grammars is presented in (Chiang et al., 2005). To date there is little research on English-Bengali SMT: PB-SMT systems can be improved (Pal et al., 2011; 2013) by single tokenizing Multiword Expressions (MWEs) on both sides of the parallel corpus. Researches on alignment were mostly developed for MT tasks (Brown, 1991; Gale and 90 Church, 1993). A Maximum Entropy model based approach for English-Chin"
W14-5114,W04-3250,0,0.269403,"Missing"
W14-5114,2013.mtsummit-papers.8,1,0.870543,"Missing"
W14-5114,W05-0909,0,0.0587776,"English to Indian Languages Machine Translation (EILMT) System - Phase II” project funded by Department of Information Technology, Government of India. Our experiments have been carried out in two directions. First we improved the baseline model using the aligned sentiment phrases. Then, we automatically post-edited the translation output by using the sentiment knowledge of the source input test sentence. The evaluation results are reported in Table 1. The evaluation was carried out using well-known automatic MT evaluation metrics: BLEU (Papineni et al., 2002, NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006). In experiment 2, the extracted parallel sentiment phrase alignments are incorporated with the existing baseline phrase table and the resulting model performs better than the baseline system. Experiment 3 shows how post-editing the output of experiment 2 brings about further improvements. 7 Conclusions and Future Research In this paper, we successfully illustrated how sentiment analysis can improve the translation of an English-Bengali PB-SMT system. We have also shown how sentiment knowledge is useful for automatic post-editing the MT output. In either case, we"
W14-5114,C04-1200,0,0.0836786,"g the lexicon. Sentiment detection is the task of determining positive or negative sentiment of words, phrases, sentences and documents. The computational approach to sentiment analysis in textual data requires annotated lexicons with polarity tags (Patra et al., 2013). Research has been carried out on building sentiment or emotional corpora in English (Strapparava and Valitutti, 2004; Baccianella et al., 2010; Patra et al., 2013) and Bengali (Das and Bandyopadhyay, 2010; Das and Bandyopadhyay, 2010a). Identifying the sentiment holder is another task closely related to subjectivity detection (Kim and Hovy, 2004). Several methods have been implemented to identify the sentiment holders such as rule based methods (using dependency information) (Kolya et al., 2012) and supervised machine learning methods (Kim and Hovy, 2004; Kolya et al., 2012). To the best of our knowledge, no prior work on improving SMT systems using aligned sentiment expressions, holders and their corresponding objects have been developed yet. There is research on creating sentiment lexica and cross-lingual sentiment identification. Automatic translation is a viable alternative for the construction of resources and tools for subjectiv"
W14-5114,W06-0301,0,0.104449,"Missing"
W14-5114,baccianella-etal-2010-sentiwordnet,0,0.071767,"English-Chinese NE alignment has been proposed in Feng et al. (2004), which significantly outperforms IBM Model 4 and HMM. Fung (1994) presented K-vec, an alternative alignment strategy that starts by estimating the lexicon. Sentiment detection is the task of determining positive or negative sentiment of words, phrases, sentences and documents. The computational approach to sentiment analysis in textual data requires annotated lexicons with polarity tags (Patra et al., 2013). Research has been carried out on building sentiment or emotional corpora in English (Strapparava and Valitutti, 2004; Baccianella et al., 2010; Patra et al., 2013) and Bengali (Das and Bandyopadhyay, 2010; Das and Bandyopadhyay, 2010a). Identifying the sentiment holder is another task closely related to subjectivity detection (Kim and Hovy, 2004). Several methods have been implemented to identify the sentiment holders such as rule based methods (using dependency information) (Kolya et al., 2012) and supervised machine learning methods (Kim and Hovy, 2004; Kolya et al., 2012). To the best of our knowledge, no prior work on improving SMT systems using aligned sentiment expressions, holders and their corresponding objects have been dev"
W14-5114,J93-1003,0,0.359133,"Missing"
W14-5114,J93-1004,0,0.625572,"Missing"
W14-5140,W00-0729,0,0.120093,"Missing"
W14-5140,W97-0301,0,0.269273,"Missing"
W15-5939,bakliwal-etal-2012-hindi,0,0.117611,"ss_An 1 49 95 Class_Ca 83 12 91 Class_Ex 85 6 100 Class_Ha 96 4 125 Class_Sa 7 117 461 Total Songs Table 2. Confusion matrix of two annotation schemes and statistics of total songs. 5 Classification Framework We adopted a wide range of textual features such as sentiment Lexicons, stylistic features and ngrams in order to develop the music mood classification framework. We have illustrated all the features below. 5.1 Features based on Sentiment Lexicons: We used three Hindi sentiment lexicons to classify the sentiment words present in the lyrics texts, which are Hindi Subjective Lexicon (HSL) (Bakliwal et al., 2012), Hindi SentiWordnet (HSW) (Joshi et al., 2010) and Hindi Wordnet Affect (HWA) (Das et al., 2012). HSL contains two lists, one is for adjectives (3909 positive, 2974 negative and 1225 neutral) and another is for adverbs (193 positive, 178 negative and 518 neutral). HSW consists of 2168 positive, 1391 negative and 6426 neutral words along with their parts-of-speech (POS) and synset id extracted from the Hindi WordNet. HWA contains 2986, 357, 500, 3185, 801 and 431 words with their parts-of-speech from angry, disgust, fear, happy, sad and surprise classes, respectively. The statistics of the sen"
W15-5939,W12-5310,0,0.225135,"Taxonomy: Preparation of an annotated dataset requires the selection of proper mood classes to be used. With respect to Indian music, limited work on mood detection by considering audio features has been reported till today. Koduri and Indurkhya (2010) worked on the mood classification of South Indian Classical music, i.e. Carnatic music. The main goal of their experiment was to verify the raagas that really evoke a particular rasa(s) (emotion) specific to each user. They considered the taxonomy consisting of ten rasas e.g., Srungaram (Romance, Love), Hasyam (Laughter, Comedy) etc. Similarly, Velankar and Sahasrabuddhe (2012) prepared data for mood classification of Hindustani classical music consisting of 13 mood taxonomies (Happy, Exciting, Satisfaction, Peaceful, Graceful, Gentle, Huge, Surrender, Love, Request, Emotional, Pure, Meditative). Patra et al. (2013a) used the standard MIREX taxonomy for their experiments whereas Ujlambkar and Attar, (2012) experimented based on audio features for five mood classes, namely Happy, Sad, Silent, Excited and Romantic along with three or more subclasses based on two dimensional “Energy and Stress” model. Mood Classification using Audio Features: Automatic music mood class"
W15-5939,W13-4104,1,0.863559,"Missing"
W15-5939,D12-1054,0,0.0298218,"Missing"
W16-2333,D11-1033,0,0.150111,", large amount of additional out-domain data may bias the resultant distribution towards the out-domain. In practice, 442 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 442–448, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2 3 Related Work System Description 3.1 Data selection Approach Among the different approaches proposed for data selection, the two most popular and successful methodologies are based on monolingual crossentropy difference (Moore and Lewis, 2010) and bilingual cross-entropy difference (Axelrod et al., 2011). The data selection approach taken in the present work is also motivated by the bilingual cross-entropy difference (Axelrod et al., 2011) based data selection. However, instead of using bilingual cross-entropy difference, we applied bilingual cross-perplexity difference to model our data selection process. The difference in crossentropy is computed on two language models (LM); the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm"
W16-2333,W08-0321,0,0.0258029,"sj − tj ] is calculated based on Equation 2. H(Plm ) = − Koehn et al. (2007) used multiple decoding paths for combining multiple domain-specific translation tables in the state-of-the-art PB-SMT decoder MOSES. Banerjee et al. (2013) combined an in-domain model (translation and reordering model) with an out-of-domain model into MOSES and they derived log-linear features to distinguish between phrases of multiple domains by applying the data-source indicator features and showed modest improvement in translation quality. score = |P Pinsl (sj ) − P Posl (sj )| + |P Pintl (tj ) − P Potl (tj ) |(2) Bach et al. (2008) suggested that sentences may be weighted by how much it matches with the target domain. A comparison among different domain adaptation methods for different subject matters in patent translation was carried out by (Ceaus¸fu et al., 2011) which led to a small gain over the baseline. Subsequently, sentence pairs [s − t] from the out-domain corpus (o) are ranked based on this score. 3.2 Interpolation Approach To combine multiple translation and language models, a common approach is to linearly interpolate them. The language model interpolation weights are automatically learnt by minimizing the p"
W16-2333,2011.mtsummit-papers.32,1,0.872589,"Missing"
W16-2333,N03-1017,0,0.00959327,"performance of the in-domain MT system. The following subsections describe the datasets used for the experiments, detailed experimental settings and systematic evaluation on both the development set and test set. 4.1 4.2 Experimental Settings We used the standard log-linear PB-SMT model for our experiments. All the experiments were carried out using a maximum phrase length of 7 for the translation model and 5-gram language models. The other experimental settings involved word alignment model between EN–DE trained with Berkeley Aligner (Liang et al., 2006). The phraseextraction heuristics of (Koehn et al., 2003) were used to build the phrase-based SMT systems. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) (Galley and Manning, 2008) method and conditioned on both the source and target languages. The 5-gram language models were built using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e., 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning w"
W16-2333,2013.mtsummit-papers.13,1,0.874847,"Missing"
W16-2333,E12-1045,0,0.0171699,"the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different directions, ranging from adapating language models and translation model"
W16-2333,W04-3250,0,0.0396345,"otivated by the bilingual cross-entropy difference (Axelrod et al., 2011) based data selection. However, instead of using bilingual cross-entropy difference, we applied bilingual cross-perplexity difference to model our data selection process. The difference in crossentropy is computed on two language models (LM); the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in"
W16-2333,2005.mtsummit-papers.11,0,0.0178143,"he bilingual cross-entropy difference (Axelrod et al., 2011) based data selection. However, instead of using bilingual cross-entropy difference, we applied bilingual cross-perplexity difference to model our data selection process. The difference in crossentropy is computed on two language models (LM); the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingu"
W16-2333,2011.eamt-1.5,0,0.0592153,"Missing"
W16-2333,W07-0734,0,0.0408698,"e performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the parameters were tuned, decoding was carried out on the held out development test set (Batch2 in Table 3) as well as test set released by the shared task organizers. We evaluated the systems using three well known automatic MT evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The evaluation results of our baseline systems trained on in-domain and out-domain data are reported in Table 3. Datasets In-domain Data: The detailed statistics of indomain data is reported in Table 1. We considered all the data provided by the WMT-2016 organizers for the IT translation task. We combined all data and performed cleaning in two steps: (i) Cleaning1: following the cleaning process described in (Pal et al., 2015), and (ii) Cleaning 2: using the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and maximum number of tokens set to 1 and"
W16-2333,N06-1014,0,0.0556417,"an them. We also use out of domain data to accelerate the performance of the in-domain MT system. The following subsections describe the datasets used for the experiments, detailed experimental settings and systematic evaluation on both the development set and test set. 4.1 4.2 Experimental Settings We used the standard log-linear PB-SMT model for our experiments. All the experiments were carried out using a maximum phrase length of 7 for the translation model and 5-gram language models. The other experimental settings involved word alignment model between EN–DE trained with Berkeley Aligner (Liang et al., 2006). The phraseextraction heuristics of (Koehn et al., 2003) were used to build the phrase-based SMT systems. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) (Galley and Manning, 2008) method and conditioned on both the source and target languages. The 5-gram language models were built using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e., 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing s"
W16-2333,2010.iwslt-papers.5,0,0.0226244,"is computed on two language models (LM); the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different directions, ranging f"
W16-2333,W07-0717,0,0.0383431,"erence in crossentropy is computed on two language models (LM); the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different dir"
W16-2333,D07-1036,0,0.0628601,"Missing"
W16-2333,W06-1607,0,0.20109,"esearch Center for Artificial Intelligence (DFKI), Germany {pahari.koushik,alapan.cse}@gmail.com, sudip.naskar@jdvu.ac.in, sivaji cse ju@yahoo.com {santanu.pal, josef.vangenabith}@uni-saarland.de Abstract it is often difficult to obtain sufficient amount of in-domain parallel data to train a system which can provide good performance in a specific domain. The performance of an in-domain model can be improved by selecting a subset from the out-domain data which is very similar to the indomain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distributions (Foster et al., 2006; Sennrich et al., 2013) in favor of the in-domain data. This paper presents the JU-USAAR English–German domain adaptive machine translation (MT) system submitted to the IT domain translation task organized in WMT-2016 . Our system brings improvements over the in-domain baseline system by incorporating out-domain knowledge. We applied two methodologies to accelerate the performance of our in-domain MT system: (i) additional training material extraction from out-domain data using data selection method, and (ii) language model and translation model adaptation through interpolation. Our primary s"
W16-2333,D09-1074,0,0.060014,"Missing"
W16-2333,D08-1089,0,0.0115611,"set and test set. 4.1 4.2 Experimental Settings We used the standard log-linear PB-SMT model for our experiments. All the experiments were carried out using a maximum phrase length of 7 for the translation model and 5-gram language models. The other experimental settings involved word alignment model between EN–DE trained with Berkeley Aligner (Liang et al., 2006). The phraseextraction heuristics of (Koehn et al., 2003) were used to build the phrase-based SMT systems. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) (Galley and Manning, 2008) method and conditioned on both the source and target languages. The 5-gram language models were built using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e., 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the paramete"
W16-2333,P10-2041,0,0.539223,"ata, India 3 Universit¨at des Saarlandes, Saarbr¨ucken, Germany 4 German Research Center for Artificial Intelligence (DFKI), Germany {pahari.koushik,alapan.cse}@gmail.com, sudip.naskar@jdvu.ac.in, sivaji cse ju@yahoo.com {santanu.pal, josef.vangenabith}@uni-saarland.de Abstract it is often difficult to obtain sufficient amount of in-domain parallel data to train a system which can provide good performance in a specific domain. The performance of an in-domain model can be improved by selecting a subset from the out-domain data which is very similar to the indomain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distributions (Foster et al., 2006; Sennrich et al., 2013) in favor of the in-domain data. This paper presents the JU-USAAR English–German domain adaptive machine translation (MT) system submitted to the IT domain translation task organized in WMT-2016 . Our system brings improvements over the in-domain baseline system by incorporating out-domain knowledge. We applied two methodologies to accelerate the performance of our in-domain MT system: (i) additional training material extraction from out-domain data using data selection method, and (ii) language mode"
W16-2333,W12-3154,0,0.0151824,"s (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different directions, ranging from adapating language models and translation models to alignment adaptation approach to improve domainspecific wor"
W16-2333,P03-1021,0,0.00947555,"rdering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) (Galley and Manning, 2008) method and conditioned on both the source and target languages. The 5-gram language models were built using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e., 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the parameters were tuned, decoding was carried out on the held out development test set (Batch2 in Table 3) as well as test set released by the shared task organizers. We evaluated the systems using three well known automatic MT evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The evaluation results of our baseline systems trained on in-domain and out-domain data are reported in Table 3. Datasets In-domain D"
W16-2333,W11-2123,0,0.0263772,"rried out using a maximum phrase length of 7 for the translation model and 5-gram language models. The other experimental settings involved word alignment model between EN–DE trained with Berkeley Aligner (Liang et al., 2006). The phraseextraction heuristics of (Koehn et al., 2003) were used to build the phrase-based SMT systems. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) (Galley and Manning, 2008) method and conditioned on both the source and target languages. The 5-gram language models were built using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e., 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the parameters were tuned, decoding was carried out on the held out development test set (Batch2 in Table 3) as well as test set released by th"
W16-2333,P02-1040,0,0.099321,"To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the parameters were tuned, decoding was carried out on the held out development test set (Batch2 in Table 3) as well as test set released by the shared task organizers. We evaluated the systems using three well known automatic MT evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The evaluation results of our baseline systems trained on in-domain and out-domain data are reported in Table 3. Datasets In-domain Data: The detailed statistics of indomain data is reported in Table 1. We considered all the data provided by the WMT-2016 organizers for the IT translation task. We combined all data and performed cleaning in two steps: (i) Cleaning1: following the cleaning process described in (Pal et al., 2015), and (ii) Cleaning 2: using the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and max"
W16-2333,P13-1082,0,0.0831355,"tificial Intelligence (DFKI), Germany {pahari.koushik,alapan.cse}@gmail.com, sudip.naskar@jdvu.ac.in, sivaji cse ju@yahoo.com {santanu.pal, josef.vangenabith}@uni-saarland.de Abstract it is often difficult to obtain sufficient amount of in-domain parallel data to train a system which can provide good performance in a specific domain. The performance of an in-domain model can be improved by selecting a subset from the out-domain data which is very similar to the indomain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distributions (Foster et al., 2006; Sennrich et al., 2013) in favor of the in-domain data. This paper presents the JU-USAAR English–German domain adaptive machine translation (MT) system submitted to the IT domain translation task organized in WMT-2016 . Our system brings improvements over the in-domain baseline system by incorporating out-domain knowledge. We applied two methodologies to accelerate the performance of our in-domain MT system: (i) additional training material extraction from out-domain data using data selection method, and (ii) language model and translation model adaptation through interpolation. Our primary submission obtained a BLE"
W16-2333,E12-1055,0,0.0841094,"timated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different directions, ranging from adapating language models and translation models to alignment a"
W16-2333,N06-2037,0,0.0324717,"age model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different directions, ranging from adapating language models and translation models to alignment adaptation approach to improve domainspecific word alignment. N 1 X log Plm (wi |wi−k+1 ...wi−1 ) N i=1 (1) We calculated perplexity (P P = 2H ) of individual sentences of out-domain with respect to indomain LM and out-domain LM for both source (sl) a"
W16-2333,2006.amta-papers.25,0,0.0417351,"e table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the parameters were tuned, decoding was carried out on the held out development test set (Batch2 in Table 3) as well as test set released by the shared task organizers. We evaluated the systems using three well known automatic MT evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The evaluation results of our baseline systems trained on in-domain and out-domain data are reported in Table 3. Datasets In-domain Data: The detailed statistics of indomain data is reported in Table 1. We considered all the data provided by the WMT-2016 organizers for the IT translation task. We combined all data and performed cleaning in two steps: (i) Cleaning1: following the cleaning process described in (Pal et al., 2015), and (ii) Cleaning 2: using the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and maximum number of tokens set to 1 and 80 respectively. Additionally"
W16-2333,W14-3323,1,0.898642,"Missing"
W16-2333,P07-2045,0,\N,Missing
W16-2333,W15-3017,1,\N,Missing
W16-6333,H92-1022,0,0.430802,"l the POS tagging identifies the types of noun, pronoun, verb, adjective, determiner, etc. The Manipuri language is one type of TibetoBurman language. Tibeto-Burman languages are generally agglutinative in nature. The Manipuri language is not simply agglutinative but one can observe that it is highly agglutinative like Turkish. Manipuri language or otherwise known as the POS tagging forms one of the basic Natural Language Processing (NLP) work. Several works are reported for different languages. To list some among them, the POS tagger for English is reported with a Simple Rule-based based in (Brill, 1992). Also a transformation-based error-driven learning based POS tagger in (Brill, 1995), maximum entropy methods based POS tagger in (Ratnaparakhi, 1996) and Hidden Markov Model (HMM) based POS tagger in (Kupiec, 1992). The works of Chinese language are also reported which ranges from rule based (Lin et al., 1992), HMM (Chang et al., 1993)to Genetic Algorithms (Lua, 1996). For Indian languages like Bengali works are reported in (Ekbal et al., 2007a), (Ekbal et al., 2007b) (Ekbal et al., 2008c), (Anthony et al., 2010) for Malayalam and for Hindi in (Smriti et al., 2006). CRF based Manipuri POS ta"
W16-6333,J95-4004,0,0.515911,"etc. The Manipuri language is one type of TibetoBurman language. Tibeto-Burman languages are generally agglutinative in nature. The Manipuri language is not simply agglutinative but one can observe that it is highly agglutinative like Turkish. Manipuri language or otherwise known as the POS tagging forms one of the basic Natural Language Processing (NLP) work. Several works are reported for different languages. To list some among them, the POS tagger for English is reported with a Simple Rule-based based in (Brill, 1992). Also a transformation-based error-driven learning based POS tagger in (Brill, 1995), maximum entropy methods based POS tagger in (Ratnaparakhi, 1996) and Hidden Markov Model (HMM) based POS tagger in (Kupiec, 1992). The works of Chinese language are also reported which ranges from rule based (Lin et al., 1992), HMM (Chang et al., 1993)to Genetic Algorithms (Lua, 1996). For Indian languages like Bengali works are reported in (Ekbal et al., 2007a), (Ekbal et al., 2007b) (Ekbal et al., 2008c), (Anthony et al., 2010) for Malayalam and for Hindi in (Smriti et al., 2006). CRF based Manipuri POS tagging is reported in (Kishorjit et al., 2012a) and also reported that an improvement"
W16-6333,W93-0305,0,0.578597,"Missing"
W16-6333,W96-0213,0,0.727949,"Missing"
W16-6333,P99-1023,0,0.154265,"Missing"
W16-6333,O92-1001,0,0.465354,"Missing"
W16-6624,W05-0909,0,0.0347111,"ms whose outputs on the Prodigy-METEO testset are also available in the Prodigy-METEO corpus. These ten NLG systems are PCFG-Greedy, PSCFG-Semantic, PSCFG-Unstructured, PCFG-Viterbii, PCFG2gram, PCFG-Roulette, PBSMT-Unstructured, Figure 4: An sample of input and outputs of different NLG system SumTime-Hybrid, PBSMT-Structured and PCFGRandom (Belz and Kow, 2009). Figure 4 shows a sample input and outputs of all the above mentioned systems including our system. 4.2.1 Automatic Evaluation For automatic evaluation, we used two automatic evaluation metrics; BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). Both BLEU and METEOR were originally proposed for evaluation of machine translation (MT) systems However, due to the similarity between the two tasks (i.e., MT and NLG) from the point of view of their working principles, most of the NLG systems are also evaluated using these two automatic MT evaluation metrics. Because of the relatively small size of the dataset, we took a five-fold cross validation policy which was predefined in the Prodigy-METEO corpus. Table 1 presents the evaluation results obtained with BLEU and METEOR on our system along with the ten other NLG systems. 149 System Corpu"
W16-6624,W09-0603,0,0.726589,"language model. Since its inception, statistical machine translation (Brown et al., 1993; Koehn, 2010) has gained immense popularity and it is the most prominent approach and represents the state-of-the-art in automatic machine translation. The task of NLG can be thought as a machine translation task because of the similarity between their end objectives - converting from one language to another. Langner and Black (2009) proposed an NLG system, Mountain, which modelled the task of NLG as statistical machine translation (SMT). They used the MOSES1 toolkit (Koehn et al., 2007) for this purpose. Belz and Kow (2009) proposed another SMT based NLG system which made use of the phrase-based SMT (PB-SMT) model (Koehn et al., 2003). The MOSES toolkit offers an efficient implementation of the PBSMT model. However, the linguistic quality and readability of PB-SMT based NLG systems were not as good as compared to other statistical NLG systems like Nitrogen, Oxygen, etc. (Belz and Kow, 1 http://www.statmt.org/moses 144 2009). Some semi-automatic NLG systems had also been proposed. The Probabilistic synchronous contextfree grammar (PSCFG) generator (Belz, 2008) represents this category of NLG systems which can be"
W16-6624,J93-2003,0,0.0709952,"stems have been proposed and some of them achieved quite good results in the generation task. Two such successful statistical NLG systems are Nitrogen (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998) and Oxygen (Habash, 2000). These two NLG systems are based on statistical sentence realizer. Similarly Halogen (Langkilde and Knight, 1998) represents another statistical language generator which is based on statistical n-gram language model. Oh and Rudnicky (2000) also proposed an NLG system based on statistical language model. Since its inception, statistical machine translation (Brown et al., 1993; Koehn, 2010) has gained immense popularity and it is the most prominent approach and represents the state-of-the-art in automatic machine translation. The task of NLG can be thought as a machine translation task because of the similarity between their end objectives - converting from one language to another. Langner and Black (2009) proposed an NLG system, Mountain, which modelled the task of NLG as statistical machine translation (SMT). They used the MOSES1 toolkit (Koehn et al., 2007) for this purpose. Belz and Kow (2009) proposed another SMT based NLG system which made use of the phrase-b"
W16-6624,P95-1034,0,0.346082,"and resources, our system is able to generate intelligible and readable text output. The remainder of the paper is organized as follows. Section 2 briefly presents relevant related work. The proposed NLG system is described in Section 3. Section 4 elaborates the experimental settings, dataset and the corresponding results. Section 5 concludes the paper. 2 Related works Till date, a number of knowledge-light approach based language generator systems have been proposed and some of them achieved quite good results in the generation task. Two such successful statistical NLG systems are Nitrogen (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998) and Oxygen (Habash, 2000). These two NLG systems are based on statistical sentence realizer. Similarly Halogen (Langkilde and Knight, 1998) represents another statistical language generator which is based on statistical n-gram language model. Oh and Rudnicky (2000) also proposed an NLG system based on statistical language model. Since its inception, statistical machine translation (Brown et al., 1993; Koehn, 2010) has gained immense popularity and it is the most prominent approach and represents the state-of-the-art in automatic machine translation. The task of NL"
W16-6624,N03-1017,0,0.0108914,"immense popularity and it is the most prominent approach and represents the state-of-the-art in automatic machine translation. The task of NLG can be thought as a machine translation task because of the similarity between their end objectives - converting from one language to another. Langner and Black (2009) proposed an NLG system, Mountain, which modelled the task of NLG as statistical machine translation (SMT). They used the MOSES1 toolkit (Koehn et al., 2007) for this purpose. Belz and Kow (2009) proposed another SMT based NLG system which made use of the phrase-based SMT (PB-SMT) model (Koehn et al., 2003). The MOSES toolkit offers an efficient implementation of the PBSMT model. However, the linguistic quality and readability of PB-SMT based NLG systems were not as good as compared to other statistical NLG systems like Nitrogen, Oxygen, etc. (Belz and Kow, 1 http://www.statmt.org/moses 144 2009). Some semi-automatic NLG systems had also been proposed. The Probabilistic synchronous contextfree grammar (PSCFG) generator (Belz, 2008) represents this category of NLG systems which can be created mostly automatically but requires manual help to certain extent. In synchronous contextfree grammar (SCFG"
W16-6624,P07-2045,0,0.00852566,"ed an NLG system based on statistical language model. Since its inception, statistical machine translation (Brown et al., 1993; Koehn, 2010) has gained immense popularity and it is the most prominent approach and represents the state-of-the-art in automatic machine translation. The task of NLG can be thought as a machine translation task because of the similarity between their end objectives - converting from one language to another. Langner and Black (2009) proposed an NLG system, Mountain, which modelled the task of NLG as statistical machine translation (SMT). They used the MOSES1 toolkit (Koehn et al., 2007) for this purpose. Belz and Kow (2009) proposed another SMT based NLG system which made use of the phrase-based SMT (PB-SMT) model (Koehn et al., 2003). The MOSES toolkit offers an efficient implementation of the PBSMT model. However, the linguistic quality and readability of PB-SMT based NLG systems were not as good as compared to other statistical NLG systems like Nitrogen, Oxygen, etc. (Belz and Kow, 1 http://www.statmt.org/moses 144 2009). Some semi-automatic NLG systems had also been proposed. The Probabilistic synchronous contextfree grammar (PSCFG) generator (Belz, 2008) represents this"
W16-6624,J10-4005,0,0.025254,"osed and some of them achieved quite good results in the generation task. Two such successful statistical NLG systems are Nitrogen (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998) and Oxygen (Habash, 2000). These two NLG systems are based on statistical sentence realizer. Similarly Halogen (Langkilde and Knight, 1998) represents another statistical language generator which is based on statistical n-gram language model. Oh and Rudnicky (2000) also proposed an NLG system based on statistical language model. Since its inception, statistical machine translation (Brown et al., 1993; Koehn, 2010) has gained immense popularity and it is the most prominent approach and represents the state-of-the-art in automatic machine translation. The task of NLG can be thought as a machine translation task because of the similarity between their end objectives - converting from one language to another. Langner and Black (2009) proposed an NLG system, Mountain, which modelled the task of NLG as statistical machine translation (SMT). They used the MOSES1 toolkit (Koehn et al., 2007) for this purpose. Belz and Kow (2009) proposed another SMT based NLG system which made use of the phrase-based SMT (PB-S"
W16-6624,P98-1116,0,0.440094,"c.in Abstract Knowledge-intensive generation approaches take significant human effort or expert advise for building an NLG system. Some examples of this type of NLG systems are SumTime system (Reiter et al., 2005), FoG system (Goldberg et al., 1994), PLANDOC system (McKeown et al., 1994), etc. On the other hand, knowledge-light NLG systems mostly use statistical methods to generate output text and take less human effort. Being automatic systems, knowledge-light systems mostly employ machine learning and data mining techniques. There are many types of knowledge-light systems; n-gram based NLG (Langkilde and Knight, 1998), neural network based NLG (Sutskever et al., 2011), case based NLG (Pan and Shaw, 2004), etc. However, it has been observed that knowledge-intensive systems typically perform better than knowledge-light systems as per human evaluation (Adeyanju, 2012). Most of the existing natural language generation (NLG) techniques employing statistical methods are typically resource and time intensive. On the other hand, handcrafted rulebased and template-based NLG systems typically require significant human/designer efforts. In this paper, we proposed a statistical NLG technique which does not require any"
W16-6624,A94-1002,0,0.33437,"Missing"
W16-6624,W00-0306,0,0.122681,"responding results. Section 5 concludes the paper. 2 Related works Till date, a number of knowledge-light approach based language generator systems have been proposed and some of them achieved quite good results in the generation task. Two such successful statistical NLG systems are Nitrogen (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998) and Oxygen (Habash, 2000). These two NLG systems are based on statistical sentence realizer. Similarly Halogen (Langkilde and Knight, 1998) represents another statistical language generator which is based on statistical n-gram language model. Oh and Rudnicky (2000) also proposed an NLG system based on statistical language model. Since its inception, statistical machine translation (Brown et al., 1993; Koehn, 2010) has gained immense popularity and it is the most prominent approach and represents the state-of-the-art in automatic machine translation. The task of NLG can be thought as a machine translation task because of the similarity between their end objectives - converting from one language to another. Langner and Black (2009) proposed an NLG system, Mountain, which modelled the task of NLG as statistical machine translation (SMT). They used the MOSE"
W16-6624,P02-1040,0,0.0986526,"system with ten existing NLG systems whose outputs on the Prodigy-METEO testset are also available in the Prodigy-METEO corpus. These ten NLG systems are PCFG-Greedy, PSCFG-Semantic, PSCFG-Unstructured, PCFG-Viterbii, PCFG2gram, PCFG-Roulette, PBSMT-Unstructured, Figure 4: An sample of input and outputs of different NLG system SumTime-Hybrid, PBSMT-Structured and PCFGRandom (Belz and Kow, 2009). Figure 4 shows a sample input and outputs of all the above mentioned systems including our system. 4.2.1 Automatic Evaluation For automatic evaluation, we used two automatic evaluation metrics; BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). Both BLEU and METEOR were originally proposed for evaluation of machine translation (MT) systems However, due to the similarity between the two tasks (i.e., MT and NLG) from the point of view of their working principles, most of the NLG systems are also evaluated using these two automatic MT evaluation metrics. Because of the relatively small size of the dataset, we took a five-fold cross validation policy which was predefined in the Prodigy-METEO corpus. Table 1 presents the evaluation results obtained with BLEU and METEOR on our system along with the t"
W16-6624,W15-4639,0,0.0122505,"es very little manual help and if the given prior dataset covers almost all types of input instances then CBR based systems perform better. Recently, some neural network based NLG systems have been proposed. With the advent of recurrent neural network (RNN) based language models (RNNLM) (Mikolov et al., 2010), some RNN based NLG systems have been proposed. An idea of generating text through recurrent neural network based approach with Hessian-free optimization was proposed by (Sutskever et al., 2011). However, this method takes a long training time. An RNN based NLG technique was proposed by (Wen et al., 2015) based on a joint recurrent and convolutional neural network structure. This system was able to train on dialogue act-utterance pairs without any semantic alignments or predefined grammar trees. Although rule based knowledge-intensive NLG systems take long time and expert knowledge and feedback to be developed, this type of systems most of the times are able to generate high quality natural language text output. For example, SumTime (Reiter et al., 2005) weather forecasting system is essentially a rule based NLG system, however, its output text quality was found to be quite better compared to"
W16-6624,C98-1112,0,\N,Missing
W17-2511,P91-1022,0,0.838247,"Missing"
W17-2511,1992.tmi-1.7,0,0.765608,"Missing"
W17-2511,P93-1001,0,0.447518,"Missing"
W17-2511,W14-3323,0,0.0143851,"tion process from corpora of translated texts. SMT has shown good results for many language pairs and is responsible for the recent surge in terms of popularity of Machine Translation among the research communities. But, for a SMT system to work efficiently, it has to be fed with large parallel corpus, for producing high quality phrase table and translation models (Brown et. al., 1991; Church et. al., 1993; Dagan et. al., 1999). Since availability of large parallel corpus is an issue for low resourced languages, building one from scratch involves high manual labor and cost (Pal et. al., 2014; Tan and Pal, 2014; Mahata et. al., 2016). This is the reason why lot of research has gone into the concept of building parallel corpus, from comparable corpus (JagarlaThe algorithm of the proposed work has been constructed primarily using Moses (Koehn, 2015) toolkit that has been fed with parallel corpus from Europarl2, with French as the source language and English as the target language. Also, the similarity based on sentence length has been used for the preliminary alignment because equivalent sentences in comparable corpus may roughly correspond with respect to length. Cosine Similarity algorithm was used"
W17-2511,E09-1096,0,0.0856275,"Missing"
W17-2511,P91-1023,0,0.698144,"y Algorithm Cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0, 1]. The formula used in our approach is as fo follows.  cos  Sentence similarity based on sentence length .  ‖‖‖‖ ∑   ∑  ∑  (1) Where &quot;A&quot; and &quot;B&quot; are the translated English sentence and one of the English sentences from the test data found out using the preliminary alig alignment system, respectively. One sentence from the translated English corpus is taken and is matched with the selected sentences in English corpus from heir paper paper, proGale and Church (1991) in their posed a system for aligning corresponding sense tences in a parallel corpora, based on the principle that equivalent sentences should roughly correscorre pond in length—that that is, longer sentences in one language should correspond rrespond to longer sentences iin 57 ACL 2017 Submission sion ***. Confidential review C Copy. DO NOT DISTRIBUTE. the test data, using the Cosine Similari Similarity algorithm. The sentence pair with the highest Cosine SimiSim larity value ue is considered as the final alignment. Sentence_id&apos;s of the selected sentence pair aare extracted and given as outp"
W17-2511,P11-2026,0,0.0507666,"Missing"
W17-2511,J93-1006,0,0.678294,"Missing"
W17-2511,W16-2373,1,0.406854,"Missing"
W17-2511,P93-1003,0,0.522563,"Missing"
W17-2511,2012.eamt-1.62,0,0.0728448,"Missing"
W17-7527,W98-1118,0,0.0430349,"s the dataset preparation and brat representation technique. Section 4 and Section 5 present the proposed relation extraction system and its evaluation approach. Finally, Section 6 presents the concluding remarks related to our study. 2 2.1 Related Work Medical Ontologies and Lexicons Biomedical information extraction research is challenging due to the availability of a large number of daily produced unstructured and semistructured medical corpus. To represent the structured corpus and extracting the subjective and conceptual information from the corpus, a domainspecific lexicon is essential (Borthwick et al., 1998). To this end, the standard vocabularies and ontologies such as UMLS (Unified Medical Language System) and SNOMED-CT (Systematized Nomenclature of Medicine-Clinical Terms), and lexicons like MEN (Medical WordNet) and WME (WordNet of Medical Event) have used by the researchers (Smith and Fellbaum, 2004; Kilgarriff and Fellbaum, 2000; Mondal et al., 2016a). 2.2 Medical Category and Relation Extraction These ontologies and lexicons assist in extracting the relevant information from the corpus such as medical concept categories and relations between medical concepts. Relation Dr-SyDi Explanation a"
W17-7527,embarek-ferret-2008-learning,0,0.0633269,"Missing"
W17-7527,W10-1912,0,0.0809788,"Missing"
W17-7527,C92-2082,0,0.497768,"eparation by a group of medical practitioners. The dataset has been labeled with medical concepts and their categories and proposed eight types of category-based relations in a context. We have acquired the dataset from SemEval-2015 Task-6 and MedicineNet resources which contain around 2000 number of medical contexts. The dataset helps to design and validate the relationship extraction system. II. Relationship extraction plays a key role in identifying the semantic information from the corpus. To extract these relations, we have proposed a linguistic rule-based (Abacha and Zweigenbaum, 2011a; Hearst, 1992) and a feature-oriented ma213 chine learning (Rink et al., 2011; Zhu et al., 2009) approach. The rule-based patterns help to identify the specific relations from the dataset, whereas machine learning approach assists in extracting generalize relations with promising accuracy. For an example, the following medical context is able to extract disease - symptom (illustration, inflammation symptom for the adnexitis disease) and symptom - human anatomy (illustration, inflammation affect the uterus) relations. ”The adnexitis disease characterizes inflammation symptom of attachments of the uterus huma"
W17-7527,P00-1043,0,0.293276,"Missing"
W17-7527,2016.gwc-1.35,1,0.689967,"types of different semantic relations viz. drug-drug, disease-drug, and human anatomy-symptom from the medical context. Thereafter, we have validated both rules and features-oriented approaches and offers an average FMeasures of 0.79 and 0.86 individually. 1 Introduction The availability of medical documents such as reports, discharge summaries, and prescriptions and their related information are growing quickly. In order to extract critical and crucial information, the researchers have applied various statistical and ontology-based approaches with well-known ma212 chine learning classifiers (Mondal et al., 2016b; Uzuner et al., 2011). The extracted informations are medical concepts (terms), categories (classes), and their relations, which assist the experts such as doctors and other medical practitioners as well as the non-experts as patients in understanding the problems (e.g. diseases, symptoms) and their related remedies (e.g. drugs). The medical concepts are presented by the key terms like words or phrases of the corpus whereas the category refers to the fundamental classes of medical concepts such as diseases and symptoms. The assigned categories of medical concepts and their in-between relatio"
W17-7527,H05-1092,0,0.121217,"Missing"
W17-7527,C04-1054,0,0.0422605,"rmation extraction research is challenging due to the availability of a large number of daily produced unstructured and semistructured medical corpus. To represent the structured corpus and extracting the subjective and conceptual information from the corpus, a domainspecific lexicon is essential (Borthwick et al., 1998). To this end, the standard vocabularies and ontologies such as UMLS (Unified Medical Language System) and SNOMED-CT (Systematized Nomenclature of Medicine-Clinical Terms), and lexicons like MEN (Medical WordNet) and WME (WordNet of Medical Event) have used by the researchers (Smith and Fellbaum, 2004; Kilgarriff and Fellbaum, 2000; Mondal et al., 2016a). 2.2 Medical Category and Relation Extraction These ontologies and lexicons assist in extracting the relevant information from the corpus such as medical concept categories and relations between medical concepts. Relation Dr-SyDi Explanation and Example A drug how helps to improve or cure or side effects the diseases or symptoms. Warfarin is also used to reduce the risk of clots causing strokes or heart attacks. Ha-SyDi A disease or symptom which effects a part of the body. A painful inflammation of the big toe and foot. Di-Sy The symptoms"
W17-7536,C16-1186,1,0.914323,"mantic information than audio for some of Hindi songs, i.e., the annotators perceived differ1 https://play.google.com/music/listen https://www.apple.com/music 3 https://www.last.fm 4 https://www.pandora.com 5 https://www.spotify.com 6 https://gaana.com 7 http://www.hungama.com 8 https://www.saavn.com 9 https://www.wynk.in/music S Bandyopadhyay, D S Sharma and R Sangal. Proc. of the 14th Intl. Conference on Natural Language Processing, pages 290–297, c Kolkata, India. December 2017. 2016 NLP Association of India (NLPAI) 2 ent moods while reading lyrics and listening to the corresponding songs (Patra et al., 2016b). People are interested in listening to songs specific to situation and mood (Duncan and Fox, 2005). There is a need for recommendation system based on information within the music as well as the metadata of music such as mood, genre, artist name, and so on. Music similarity measures can help to understand why two music pieces are perceived alike by the listener and to guide the user in efficiently retrieving desired piece of music (Schedl et al., 2011). Query by hamming helps to find an exact song with respect to a query humming. Again, a lyrics based retrieval system could be helpful for s"
W17-7536,W15-5939,1,0.681636,"RADAR was developed by Sasaki et al. (2014) and they visualized the topics of Japaneses lyrics by using a Latent Dirichlet Allocation (LDA). Several experiments were performed on retrieving similar lyrics for Western songs by (Mahedero et al., 2005; Knees et al., 2007; Schedl et al., 2011), Mandarin lyrics by (Wang et al., 2010), and Chinese lyrics by (Han et al., 2015). 2.1 Experiments on Indian Songs MIR in Indian songs is at early stage. Recently, mood classification of Hindi songs have been performed using audio (Ujlambkar and Attar, 2012; Patra et al., 2013; Patra et al., 2016a), lyrics (Patra et al., 2015), and combination of both (Patra et al., 2016b; Patra et al., 2016c). The datasets used in above experiments are small and not adequate for development of recommendation system. Some other tasks like raga identification of south Indian Carnatic music (Sridhar et al., 2011), multimodal sentiment analysis of Telugu songs (Abburi et al., 2016), melody identification of Carnatic music (Koduri et al., 2011), rhythm analysis of Indian art music (Srinivasamurthy et al., 2014) etc. have been performed till date. To the best of author’s knowledge, almost no work exists for retrieving similar lyrics for"
W18-6418,W16-2373,1,0.414314,"complish the given task. The current paper documents the preprocessing steps, the description of the submitted system and the results produced using the same. Our system garnered a BLEU score of 12.9. 1 Introduction Machine Translation (MT) is automated translation of one natural language to another using computer software. Translation is a tough task, not only for computers, but humans as well as it incorporates a thorough understanding of the syntax and semantics of both languages. For any MT system to return good translations, it needs good quality and sufficient amount of parallel corpus (Mahata et al., 2016, 2017). In the modern context, MT systems can be categorized into Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). SMT has had its share in making MT very popular among the masses. It includes creating statistical models, whose input parameters are derived from the analysis of bilingual text corpora, created by professional translators (Weaver, 1955). The state-of-art for SMT is Moses Toolkit1 , created by Koehn et al. (2007), incorporates subcomponents like Language Model generation, Word Alignment and Phrase Table generation. Various works have been done in SMT (L"
W18-6418,W17-2511,1,0.336269,"Missing"
W18-6418,D13-1140,0,0.0651378,"u et al., 2014), though relatively new, has shown considerable improvements in the translation results when compared to SMT (Mahata et al., 2018). This includes better fluency of the output and better handling of the Out-of-Vocabulary problem. Unlike SMT, it doesn’t depend on alignment and phrasal unit translations (Kalchbrenner and Blunsom, 2013). On the contrary, it uses an EncoderDecoder approach incorporating Recurrent Neural Cells (Cho et al., 2014). As a result, when given sufficient amount of training data, it gives much more accurate results when compared to SMT (Doherty et al., 2010; Vaswani et al., 2013; Liu et al., 2014). Further, NMT can be of two types, namely Word Level NMT and Character Level NMT. Word Level NMT, though very successful, suffers from a few disadvantages. It are unable to model rare words (Lee et al., 2016). Also, since it does not learn the morphological structure of a language it suffers when accommodating morphologically rich languages (Ling et al., 2015). We can address this issue, by training the models with huge parallel corpus, but, this in turn, produces very complex and resource consuming models that aren’t feasible enough. To combat this, we plan to use Characte"
W18-6418,W14-4012,0,0.359469,"Missing"
W18-6418,D13-1176,0,0.116149,"ine Translation of Finnish to English Sainik Kumar Mahata, Dipankar Das, Sivaji Bandyopadhyay Computer Science and Engineering Jadavpur University, Kolkata, India sainik.mahata@gmail.com, dipankar.dipnil2005@gmail.com, sivaji cse ju@yahoo.com Abstract On the other hand NMT (Bahdanau et al., 2014), though relatively new, has shown considerable improvements in the translation results when compared to SMT (Mahata et al., 2018). This includes better fluency of the output and better handling of the Out-of-Vocabulary problem. Unlike SMT, it doesn’t depend on alignment and phrasal unit translations (Kalchbrenner and Blunsom, 2013). On the contrary, it uses an EncoderDecoder approach incorporating Recurrent Neural Cells (Cho et al., 2014). As a result, when given sufficient amount of training data, it gives much more accurate results when compared to SMT (Doherty et al., 2010; Vaswani et al., 2013; Liu et al., 2014). Further, NMT can be of two types, namely Word Level NMT and Character Level NMT. Word Level NMT, though very successful, suffers from a few disadvantages. It are unable to model rare words (Lee et al., 2016). Also, since it does not learn the morphological structure of a language it suffers when accommodati"
W18-6418,P09-5002,0,0.183812,"the modern context, MT systems can be categorized into Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). SMT has had its share in making MT very popular among the masses. It includes creating statistical models, whose input parameters are derived from the analysis of bilingual text corpora, created by professional translators (Weaver, 1955). The state-of-art for SMT is Moses Toolkit1 , created by Koehn et al. (2007), incorporates subcomponents like Language Model generation, Word Alignment and Phrase Table generation. Various works have been done in SMT (Lopez, 2008; Koehn, 2009) and it has shown good results for many language pairs. 1 2 http://www.statmt.org/moses/ 445 http://www.statmt.org/wmt18/translation-task.html Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 445–448 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64045 nizers provided the required parallel corpora, consisting of 3,255,303 sentence pairs, for training the translation model. The statistics of the parallel corpus is depicted in Table 1 Our model was trained on"
W18-6418,P07-2045,0,0.00827476,"nd semantics of both languages. For any MT system to return good translations, it needs good quality and sufficient amount of parallel corpus (Mahata et al., 2016, 2017). In the modern context, MT systems can be categorized into Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). SMT has had its share in making MT very popular among the masses. It includes creating statistical models, whose input parameters are derived from the analysis of bilingual text corpora, created by professional translators (Weaver, 1955). The state-of-art for SMT is Moses Toolkit1 , created by Koehn et al. (2007), incorporates subcomponents like Language Model generation, Word Alignment and Phrase Table generation. Various works have been done in SMT (Lopez, 2008; Koehn, 2009) and it has shown good results for many language pairs. 1 2 http://www.statmt.org/moses/ 445 http://www.statmt.org/wmt18/translation-task.html Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 445–448 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64045 nizers provided the required parallel corp"
W18-6418,P16-1160,0,0.0543514,"ary problem. Unlike SMT, it doesn’t depend on alignment and phrasal unit translations (Kalchbrenner and Blunsom, 2013). On the contrary, it uses an EncoderDecoder approach incorporating Recurrent Neural Cells (Cho et al., 2014). As a result, when given sufficient amount of training data, it gives much more accurate results when compared to SMT (Doherty et al., 2010; Vaswani et al., 2013; Liu et al., 2014). Further, NMT can be of two types, namely Word Level NMT and Character Level NMT. Word Level NMT, though very successful, suffers from a few disadvantages. It are unable to model rare words (Lee et al., 2016). Also, since it does not learn the morphological structure of a language it suffers when accommodating morphologically rich languages (Ling et al., 2015). We can address this issue, by training the models with huge parallel corpus, but, this in turn, produces very complex and resource consuming models that aren’t feasible enough. To combat this, we plan to use Character level NMT, so that it can learn the morphological aspects of a language and construct a word, character by character, and hence tackle the rare word occurrence problem to some extent. In the current work, we participated in th"
W19-5328,W16-2373,1,0.836578,"n model. The current paper documents the architecture of our model, descriptions of the various modules and the results produced using the same. Our system garnered a BLEU score of 17.6. 1 Introduction Machine Translation (MT) is automated translation of one natural language to another using a computer. Translation, itself, is a very tough task for both humans as well as a computer. It requires a thorough understanding of the syntax and semantics of both the languages under consideration. For producing good translations, a MT system needs good quality and sufficient amount of parallel corpus (Mahata et al., 2016, 2017). In the modern context, MT systems can be categorized into Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). SMT has had its share in making MT very popular among the masses. It includes creating statistical models, whose input parameters are derived from the analysis of bilingual text corpora, created by professional translators (Weaver, 1955). The state-of-art for SMT is Moses Toolkit1 , created by Koehn et al. (2007), incorporates subcomponents like Language Model generation, Word Alignment and Phrase Table generation. Various works have been done in SMT (L"
W19-5328,W17-2511,1,0.890953,"Missing"
W19-5328,W18-6418,1,0.42814,"Missing"
W19-5328,W14-4012,0,0.165722,"Missing"
W19-5328,W11-2123,0,0.0171126,"c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics # sentences in Lt corpus # sentences in En corpus # words in Lt corpus # words in En corpus # word vocab size for Lt corpus # word vocab size for En corpus 9,62,022 9,62,022 1,16,65,937 1,56,22,488 4,88,593 2,27,131 search algorithm quickly finds the highest probability translation among the exponential number of choices. We trained Moses using 7,62,022 sentence pairs provided by WMT2019, with Lithuanian as the source language and English as the target language. For building the Language Model we used KenLM4 (Heafield, 2011) with 7-grams from the target corpus. The English monolingual corpus from WMT2019 was used to build the language model Training the Moses statistical MT system resulted in generation of Phrase Model and Translation Model that helps in translating between source-target language pairs. Moses scores the phrase in the phrase table with respect to a given source sentence and produces best scored phrases as output. Table 1: Statistics of the Lithuanian-English parallel corpus provided by the organizers. ”#” depicts No. of. ”Lt” and ”En” depict Lithuanian and English, respectively. ”vocab” means voca"
W19-5328,P02-1040,0,0.105969,"parametric function outk re2,00,000 translated English sentences and the returns the conditional probability using the next tarspective gold standard 2,00,000 sentences, from get symbol k. the Lithuanian-English sentence pair, were given as input to a word embedding based NMT model. 1 (y t = k |y < t, X) = exp(outk (E y (y t −1), st , ct )) As a result, this constituted our Hybrid model. Z 2.3.2 Testing Z is the normalizing constant, For the testing purpose, 10k Lithuanian Sentences X were fed to the Hybrid model, and the output, j exp(outj (E y (y t − 1), st , ct )) when checked using BLEU (Papineni et al., 2002), resulted in an accuracy of 21.6. The training and The entire model can be trained end-to-end by testing architecture is shown in Figure 1 minimizing the log likelihood which is defined as 3 n N Ty 1 XX L=− logp(y t = y t n , y ¡t n , X n ) N WMT2019 provided us with a test set of Lithuanian sentences in .SGM format. This file was parsed and fed to our hybrid system. The output file was again converted to .SGM format and submitted to the organizers. Our system garnered a BLEU Score of 17.6, when it was scored using automated accuracy metrics. Other accuracy scores are mentioned in Table 2. n="
W19-5328,D13-1176,0,0.0495964,"Rayala, Dipankar Das, Sivaji Bandyopadhyay Computer Science and Engineering Jadavpur University, Kolkata, India sainik.mahata@gmail.com, avishekgarain@gmail.com, mailsofadityar@gmail.com, dipankar.dipnil2005@gmail.com, sivaji cse ju@yahoo.com Abstract On the other hand NMT (Bahdanau et al., 2014), though relatively new, has shown considerable improvements in the translation results when compared to SMT (Mahata et al., 2018b). This includes better fluency of the output and better handling of the Out-of-Vocabulary problem. Unlike SMT, it doesnt depend on alignment and phrasal unit translations (Kalchbrenner and Blunsom, 2013). On the contrary, it uses an EncoderDecoder approach incorporating Recurrent Neural Cells (Cho et al., 2014). As a result, when given sufficient amount of training data, it gives much more accurate results when compared to SMT (Doherty et al., 2010; Vaswani et al., 2013; Liu et al., 2014). For the given task2 , we attempted to create a MT system that can translate sentences from Lithuanian to English. Since, using only SMT or NMT models leads to some or the other disadvantages, we tried to use both in a pipeline. This leads to an improvement of the results over the individual usage of either"
W19-5328,P09-5002,0,0.0209299,"the modern context, MT systems can be categorized into Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). SMT has had its share in making MT very popular among the masses. It includes creating statistical models, whose input parameters are derived from the analysis of bilingual text corpora, created by professional translators (Weaver, 1955). The state-of-art for SMT is Moses Toolkit1 , created by Koehn et al. (2007), incorporates subcomponents like Language Model generation, Word Alignment and Phrase Table generation. Various works have been done in SMT (Lopez, 2008; Koehn, 2009) and it has shown good results for many language pairs. 1 2 http://www.statmt.org/moses/ http://www.statmt.org/wmt19/translation-task.html 283 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 283–286 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics # sentences in Lt corpus # sentences in En corpus # words in Lt corpus # words in En corpus # word vocab size for Lt corpus # word vocab size for En corpus 9,62,022 9,62,022 1,16,65,937 1,56,22,488 4,88,593 2,27,131 search algorithm quickly finds the high"
W19-5328,D13-1140,0,0.0353085,"u et al., 2014), though relatively new, has shown considerable improvements in the translation results when compared to SMT (Mahata et al., 2018b). This includes better fluency of the output and better handling of the Out-of-Vocabulary problem. Unlike SMT, it doesnt depend on alignment and phrasal unit translations (Kalchbrenner and Blunsom, 2013). On the contrary, it uses an EncoderDecoder approach incorporating Recurrent Neural Cells (Cho et al., 2014). As a result, when given sufficient amount of training data, it gives much more accurate results when compared to SMT (Doherty et al., 2010; Vaswani et al., 2013; Liu et al., 2014). For the given task2 , we attempted to create a MT system that can translate sentences from Lithuanian to English. Since, using only SMT or NMT models leads to some or the other disadvantages, we tried to use both in a pipeline. This leads to an improvement of the results over the individual usage of either SMT or NMT. The main idea was to train a SMT model for translating Lithuanian language to English. Thereafter, a test set was translated using this model. Then, a word embedding based NMT model was trained to learn the mappings between the SMT output (in English) and the"
W19-5328,P07-2045,0,0.00753716,"h the languages under consideration. For producing good translations, a MT system needs good quality and sufficient amount of parallel corpus (Mahata et al., 2016, 2017). In the modern context, MT systems can be categorized into Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). SMT has had its share in making MT very popular among the masses. It includes creating statistical models, whose input parameters are derived from the analysis of bilingual text corpora, created by professional translators (Weaver, 1955). The state-of-art for SMT is Moses Toolkit1 , created by Koehn et al. (2007), incorporates subcomponents like Language Model generation, Word Alignment and Phrase Table generation. Various works have been done in SMT (Lopez, 2008; Koehn, 2009) and it has shown good results for many language pairs. 1 2 http://www.statmt.org/moses/ http://www.statmt.org/wmt19/translation-task.html 283 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 283–286 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics # sentences in Lt corpus # sentences in En corpus # words in Lt corpus # words in En cor"
W19-5328,P16-1160,0,0.0625872,"Missing"
W19-5328,P14-1140,0,\N,Missing
W19-5328,Q17-1026,0,\N,Missing
W19-5328,P03-1020,0,\N,Missing
W19-5328,W18-6401,0,\N,Missing
W19-5427,D14-1179,0,0.0280784,"Missing"
W19-5427,P18-4020,0,0.0652298,"Missing"
W19-5427,D15-1166,0,0.476972,"kar.nits, parthapakray, sivaji.cse.ju}@gmail.com 054 053 055 056 057 058 009 059 010 060 011 Abstract fixed-length vector and same is decoded to generate the target sentence (Cho et al., 2014). The simple RNN adopted Long Short Term Memory (LSTM), which is a gated RNN used to improve the translation quality of longer sentences. The importance of LSTM component is to learn long term features for encoding and decoding. Besides, LSTM, other aspects that improve the performance of the NMT system like the requirement of test-time decoding using beam search, input feeding using attention mechanism (Luong et al., 2015). The reason behind the massive unfolding of the NMT system over SMT is the ability of context analysis and fluent translation (Mahata et al., 2018; Pathak and Pakray, 2018; Pathak et al., 2018). Motivated by the merits of the NMT over other MT systems and the importance of direct translation in between pairs of similar languages, current work has investigated similar language pair namely, Hindi-Nepali, for translation from Hindi to Nepali and vice-versa using the NMT system. Due to lack of background work of similar language pair translation, the specific translation work for Hindi ⇔ Nepali i"
W19-5427,P02-1040,0,0.11221,"thak and Pakray, 2018; Pathak et al., 2018). Motivated by the merits of the NMT over other MT systems and the importance of direct translation in between pairs of similar languages, current work has investigated similar language pair namely, Hindi-Nepali, for translation from Hindi to Nepali and vice-versa using the NMT system. Due to lack of background work of similar language pair translation, the specific translation work for Hindi ⇔ Nepali is still in its infancy. To examine the efficiency of our NMT systems, the predicted translations exposed to automatic evaluation using the BLEU score (Papineni et al., 2002). The rest of the paper is structured as follows: Section 2, details of the system description is presented. Section 3, result and analysis are discussed and lastly, Section 4, concludes the paper with future scope. 012 With the extensive use of Machine Translation (MT) technology, there is progressively interest in directly translating between pairs of similar languages. Because the main challenge is to overcome the limitation of available parallel data to produce a precise MT output. Current work relies on the Neural Machine Translation (NMT) with attention mechanism for the similar language"
Y08-1016,W02-2007,0,0.064774,"Missing"
Y08-1016,I08-2077,1,0.673545,"iterary history, technological developments are of recent origin.  Web sources for name lists are available in English, but such lists are not available in Bengali forcing the use of transliteration. A pattern directed shallow parsing approach for NER in Bengali is reported in Ekbal and Bandyopadhyay (2007a). A HMM based NER system for Bengali has been reported in Ekbal et al. (2007b), where additional contextual information has been considered during emission probabilities and NE suffixes are kept for handling the unknown words. More recently, the related works in this area can be found in Ekbal et al. (2008a), Ekbal and Bandyopadhyay (2008b) with the CRF, and SVM approach, respectively. Other than Bengali, the works on Hindi can be found in Li and McCallum (2004) with CRF and Cucerzan and Yarowsky (1999) with a language independent method. As part of the IJCNLP-08 NER shared task, various works of NER in Indian languages using various approaches can be found in IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL)2. 2. Named Entity Recognition in Bengali Bengali is the seventh popular language in the world, second in India and the national language of Bangladesh. We have u"
Y08-1016,I08-5008,1,0.912233,"ological developments are of recent origin.  Web sources for name lists are available in English, but such lists are not available in Bengali forcing the use of transliteration. A pattern directed shallow parsing approach for NER in Bengali is reported in Ekbal and Bandyopadhyay (2007a). A HMM based NER system for Bengali has been reported in Ekbal et al. (2007b), where additional contextual information has been considered during emission probabilities and NE suffixes are kept for handling the unknown words. More recently, the related works in this area can be found in Ekbal et al. (2008a), Ekbal and Bandyopadhyay (2008b) with the CRF, and SVM approach, respectively. Other than Bengali, the works on Hindi can be found in Li and McCallum (2004) with CRF and Cucerzan and Yarowsky (1999) with a language independent method. As part of the IJCNLP-08 NER shared task, various works of NER in Indian languages using various approaches can be found in IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL)2. 2. Named Entity Recognition in Bengali Bengali is the seventh popular language in the world, second in India and the national language of Bangladesh. We have used a Bengali news corpus (Ekbal"
Y08-1016,W03-0425,0,0.0387153,"The current trend in NER is to use the machine-learning (ML) approach, which is more attractive in that it is trainable and adoptable and the maintenance of a ML based system is much cheaper than that of a rule-based one. The representative ML approaches used in NER are Hidden Markov Model (HMM) (BBN’s IdentiFinder in (Bikel, 1999)), ME (New York University’s MENE in (Borthwick, 1999)), CRFs (Lafferty et al., 2001) and SVM (Yamada et al., 2002). The process of stacking and voting method for combining strong classifiers like boosting, SVM and TBL, on NER task can be found in (Wu et al., 2003). Florian et al. (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best. The work reported in this paper differs from the existing works in the sense that here, we have conducted a number of experiments to improve the performance of the classifiers with the lexical context patterns, which are generated in a semi-automatic way from an unlabeled corpus of 3 million wordforms, and used several post-processing techniques to improve the performance of each classifier before applying weighted voting. Named Entity (NE) identification in Indian languages"
Y08-1016,W03-0431,0,0.0609327,"Missing"
Y08-1016,W03-0433,0,0.0325944,"ummarization etc. The current trend in NER is to use the machine-learning (ML) approach, which is more attractive in that it is trainable and adoptable and the maintenance of a ML based system is much cheaper than that of a rule-based one. The representative ML approaches used in NER are Hidden Markov Model (HMM) (BBN’s IdentiFinder in (Bikel, 1999)), ME (New York University’s MENE in (Borthwick, 1999)), CRFs (Lafferty et al., 2001) and SVM (Yamada et al., 2002). The process of stacking and voting method for combining strong classifiers like boosting, SVM and TBL, on NER task can be found in (Wu et al., 2003). Florian et al. (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best. The work reported in this paper differs from the existing works in the sense that here, we have conducted a number of experiments to improve the performance of the classifiers with the lexical context patterns, which are generated in a semi-automatic way from an unlabeled corpus of 3 million wordforms, and used several post-processing techniques to improve the performance of each classifier before applying weighted voting. Named Entity (NE) identificat"
Y09-1014,A00-1031,0,0.0849849,"Missing"
Y09-1014,A92-1018,0,0.533676,"Missing"
Y09-1014,I08-5008,1,0.912091,"ed with the assumption that the words belonging to the same POS classes contain some common suffix/prefix. This feature works effectively for the highly inflective Indian languages like Bengali. 3. POS Information: POS information of the previous word(s) can play a crucial role in deciding the POS tag of the current word. This is the only dynamic feature in the experiment. 4. Named Entity (NE) Information: The NE information of the current and/or the surrounding word(s) does have an important role in the overall accuracy of the POS tagger. In order to use this feature, a SVM based NER system (Ekbal and Bandyopadhyay, 2008b) has been used. The NE tag(s) of the current and/or the surrounding word(s) have been used as the features in the ME/CRF/SVM based POS tagging models. The NE information has been included into the system in order to reduce the rate of errors that we faced in our earlier experiments for HMM based POS tagging (Ekbal et al., 2007). The confusion matrix of the HMM based POS tagger showed that most of the errors were concerned with NNP (Proper noun) vs. NN (Common noun). 5. Lexicon Feature: A lexicon (Ekbal and Bandyopadhyay, 2008c) in Bengali has been used to improve the performance of the POS t"
Y09-1014,N01-1025,0,0.148917,"Missing"
Y09-1014,W02-2018,0,0.023462,"arly in Bengali, has started to appear very recently as there was neither any standard POS tagset nor any available tagged corpus just one/two years ago. In this work, we have developed POS taggers for Bengali using ME, CRF and SVM frameworks. These POS taggers have been combined together into a final system with the help of weighted voting techniques. We have used the C++ based ME package3 for building the ME based POS tagger. A number of POS tagging models have been built that are differentiated from each other by the features, which are included in the model. The system uses L-BFGS method (Malouf, 2002) to build the ME model, which is guaranteed to converge to a solution in this kind of problem. The sequential classification approach like ME can handle many correlated features but it suffers from the label bias problem. Careful feature selection is very essential in the ME framework. In contrast, CRF (Lafferty et al., 2001) is a sequential modeling framework that has all the advantages of ME and also solves the problem of label bias in a principled way. Moreover, CRFs bring together the best of generative and classification models. We have used the OpenNLP C++ based CRF++ package (http://crf"
Y09-1014,J94-2001,0,0.309132,"Missing"
Y09-1014,N03-1028,0,0.0524283,"kind of problem. The sequential classification approach like ME can handle many correlated features but it suffers from the label bias problem. Careful feature selection is very essential in the ME framework. In contrast, CRF (Lafferty et al., 2001) is a sequential modeling framework that has all the advantages of ME and also solves the problem of label bias in a principled way. Moreover, CRFs bring together the best of generative and classification models. We have used the OpenNLP C++ based CRF++ package (http://crfpp.sourceforge.net). For parameter estimation, the system uses L-BFGS method (Sha and Pereira, 2003) to build the CRF model, which is guaranteed to converge to a solution in this kind of problem. SVM (Vapnik, 1995) achieves high generalization even with training data of a very high dimension. Further, by introducing the Kernel function, SVMs handle non-linear feature spaces, and carry out training considering combinations of more than one feature. We have used the YamCha toolkit (http://chasen-org/~taku/software/yamcha) for training and TinySVM-0.07 (http://cl.aist-nara.ac.jp/~taku-ku/software/TinySVM) classifier for classification. A number of experiments have been carried out with the diff"
Y09-1014,W96-0213,0,\N,Missing
Y09-2045,I08-2077,1,0.534144,"relatively free word order language. Thus NEs can appear in subject and object positions making the NER task more difficult compared to others. 5 Manipuri is a resource-constrained language. Annotated corpus, name dictionaries, sophisticated morphological analyzers, POS taggers etc. are not yet available. In Indian language context, a HMM based NER system for Bengali has been reported in Ekbal et al. (2007), where additional contextual information has been considered for emission probabilities and NE suffixes are used for handling the unknown words. Other works in Bengali NER can be found in Ekbal et al. (2008), and Ekbal and Bandyopadhyay (2008) with the CRF, and SVM approaches, respectively. Other than Bengali, the works on Hindi can be found in Li and McCallum (2004) with CRF. Various works on NER involving Indian languages are reported in IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL) 1 using various techniques. In this paper, Manipuri NER systems have been developed using an active learning technique as well as SVM. We collected the data from http://www.thesangaiexpress.com/ , a popular Manipuri newspaper. Initially, a baseline system has been developed based on an"
Y09-2045,I08-5008,1,0.60592,"er language. Thus NEs can appear in subject and object positions making the NER task more difficult compared to others. 5 Manipuri is a resource-constrained language. Annotated corpus, name dictionaries, sophisticated morphological analyzers, POS taggers etc. are not yet available. In Indian language context, a HMM based NER system for Bengali has been reported in Ekbal et al. (2007), where additional contextual information has been considered for emission probabilities and NE suffixes are used for handling the unknown words. Other works in Bengali NER can be found in Ekbal et al. (2008), and Ekbal and Bandyopadhyay (2008) with the CRF, and SVM approaches, respectively. Other than Bengali, the works on Hindi can be found in Li and McCallum (2004) with CRF. Various works on NER involving Indian languages are reported in IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL) 1 using various techniques. In this paper, Manipuri NER systems have been developed using an active learning technique as well as SVM. We collected the data from http://www.thesangaiexpress.com/ , a popular Manipuri newspaper. Initially, a baseline system has been developed based on an active learning technique that gene"
Y09-2045,D08-1017,0,0.0639195,"Missing"
Y09-2045,M98-1018,0,\N,Missing
Y09-2045,A97-1029,0,\N,Missing
Y10-1013,H05-1073,0,0.429018,"Missing"
Y10-1013,P09-2038,1,0.886754,"Missing"
Y10-1013,esuli-sebastiani-2006-sentiwordnet,0,0.0650139,"Missing"
Y10-1013,C04-1200,0,0.0720567,"s compared to the accuracy values of the word level system (Das and Bandyopadhyay, 2009a). The rest of the paper is organized as follows. Section 2 surveys the related work. Preprocessing is described in Section 3. The baseline system is discussed in Section 4. Section 5 describes a supervised framework for identifying the emotional components. Experimental results are discussed in Section 6. Finally Section 7 concludes the paper. 2 Related Work The earlier works on polarity shifters and prior or contextual polarity approaches for sentiment analysis are described in (Polanyi and Zaenen, 2004; Kim and Hovy, 2004). Another related work described in (Wilson et al., 2005) gives the ground to analyze sentiment at phrase levels. The tasks described in (Meena and Prabhakar, 2007) determine the sentential sentiment based on the phrase level information considering the impact of conjuncts or intensifiers. Majority of the above studies are carried out for sentiment expressions whereas our present approach aims to identify emotional expressions and assigns the emotion tags and intensities to the sentences. In order to estimate affects in text, the model proposed in (Neviarouskaya et al., 2007) processes symboli"
Y10-1013,strapparava-valitutti-2004-wordnet,0,0.169324,"Missing"
Y10-1013,P02-1053,0,0.00575441,"Missing"
Y10-1013,H05-1044,0,0.10479,"Missing"
Y10-1013,de-marneffe-etal-2006-generating,0,\N,Missing
Y10-1013,P07-1055,0,\N,Missing
Y10-1051,S07-1014,0,0.0629916,"Missing"
Y10-1051,P07-2044,0,0.0443036,"Missing"
Y10-1051,N03-1028,0,0.0550466,", as in HMMs, can be obtained efficiently by dynamic programming. To train a CRF, the objective function to be maximized is the penalized log-likelihood of the state sequences given the observation sequence: N K i =1 k =1 L ∧ = ∑ log( P ∧ ( s (i ) |o (i ) )) −∑ λ 2σ 2 k 2 , where, { &lt; o ( i ) , s (i ) &gt; } is the labeled training data. The second sum corresponds to a zero-mean, σ 2 -variance Gaussian prior over parameters, which facilitates optimization by making the likelihood surface strictly convex. Here, we set parameters λ to maximize the penalized loglikelihood using Limited-memory BFGS (Sha and Pereira, 2003), a quasi-Newton method that is significantly more efficient, and results in only minor changes in accuracy due to changes in σ. CRFs generally can use real-valued functions but it is often required to incorporate the binary valued features. A feature function fk ( st − 1, st , o, t ) has a value of 0 for most cases and is only set to 1, when st − 1, st are certain states and the observation has certain properties. We have used the C++ based CRF++ package 1 , a simple, customizable, and open source implementation of CRF for segmenting /labeling sequential data. 3.2 Temporal Features Used for C"
Y10-1063,I08-6008,1,\N,Missing
Y10-1063,P02-1058,0,\N,Missing
Y10-1071,H05-1045,0,0.18741,"model is discussed in Section 5. Evaluation mechanism along with the associated results is mentioned in Section 6. Finally Section 7 concludes the paper. 2 Related Work The work on labeling the arguments of the verbs with their semantic roles using a novel frame matching technique is mentioned in (Swier and Stevenson, 2004). Identification of the opinion propositions and their holders is described in (Bethard et al., 2004) mainly for verbs. Identification of opinion holders for Question Answering with supporting annotation task has been attempted from the very beginning (Wiebe et al., 2005). (Choi et al., 2005) used the named entities (NEs) to identify the opinion holders with the help of machine learning and pattern-based techniques. Based on the traditional perspectives, another work discussed in (Hu et al., 2006) uses an emotion knowledge base for extracting emotion holder. The machine learning based classification task for “not holder”, “weak holder”, “medium holder”, or “strong holder” is carried out in (Evans, 2007). Kim and Hovy (2006) identified opinion holder with topic from media text using semantic role labeling. An anaphor resolution based opinion holder identification method exploiting"
Y10-1071,P09-2038,1,0.780124,"sts are annotated by other bloggers. The utilization of blog medium containing users’ emotional contents is therefore considered as an affective substrate to analyze the reaction of emotion catalyst like emotion holder. In the present task, identification of emotion holder is attempted for Bengali; a less privileged, less computerized and morphologically rich language. There is no existing emotion holder annotated corpus in Bengali. Manual annotation of emotion holder and the successive interannotator agreements have been carried out on a small set of 500 sentences of the Bengali blog corpus (Das and Bandyopadhyay, 2009). The corpus is tagged with Ekman’s (1993) six emotion types at sentence level. The phrase based similarity clues containing different part-of-speech (POS) combinations of the blog sentences are considered as the probable candidates of emotion holder for the baseline model. * The work reported in this paper was supported by a grant from the India-Japan Cooperative Programme (DSTJST) 2009 Research project entitled “Sentiment Analysis where AI meets Psychology” funded by Department of Science and Technology (DST), Government of India. Copyright 2010 by Dipankar Das and Sivaji Bandyopadhyay 621 6"
Y10-1071,W06-0301,0,0.0307612,"erbs. Identification of opinion holders for Question Answering with supporting annotation task has been attempted from the very beginning (Wiebe et al., 2005). (Choi et al., 2005) used the named entities (NEs) to identify the opinion holders with the help of machine learning and pattern-based techniques. Based on the traditional perspectives, another work discussed in (Hu et al., 2006) uses an emotion knowledge base for extracting emotion holder. The machine learning based classification task for “not holder”, “weak holder”, “medium holder”, or “strong holder” is carried out in (Evans, 2007). Kim and Hovy (2006) identified opinion holder with topic from media text using semantic role labeling. An anaphor resolution based opinion holder identification method exploiting lexical and syntactic information from online news documents is carried out in (Kim et al., 2007). The syntactic models of identifying emotion holder for English emotional verbs are discussed in (Das and Bandyopadhyay, 2010). The above works are closely related to the present one. But the present approach aims to acquire all probable emotion holders from a sentence if there multiple occurrences exist. Apart from utilizing traditional hi"
Y10-1071,W09-3411,1,0.824068,"Missing"
Y10-1071,ruppenhofer-etal-2008-finding,0,\N,Missing
Y10-1071,P93-1032,0,\N,Missing
Y10-1092,S07-1022,0,0.0669519,"Missing"
Y10-1092,W10-3208,1,0.154305,"Missing"
Y10-1092,esuli-sebastiani-2006-sentiwordnet,0,0.249077,"Missing"
Y10-1092,P07-1123,0,0.0472979,"Missing"
Y10-1092,D08-1103,0,0.0250758,"Missing"
Y10-1092,W02-1011,0,0.0114942,"Missing"
Y10-1092,P05-2008,0,0.0868094,"Missing"
Y10-1092,strapparava-valitutti-2004-wordnet,0,0.452938,"Missing"
Y10-1092,P06-1134,0,0.0695823,"Missing"
Y10-1092,H05-1044,0,0.0270228,"Missing"
