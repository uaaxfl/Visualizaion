S19-2057,{S}ymanto{R}esearch at {S}em{E}val-2019 Task 3: Combined Neural Models for Emotion Classification in Human-Chatbot Conversations,2019,0,2,4,0,11971,angelo basile,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"In this paper, we present our participation to the EmoContext shared task on detecting emotions in English textual conversations between a human and a chatbot. We propose four neural systems and combine them to further improve the results. We show that our neural ensemble systems can successfully distinguish three emotions (SAD, HAPPY, and ANGRY) and separate them from the rest (OTHERS) in a highly-imbalanced scenario. Our best system achieved a 0.77 F1-score and was ranked fourth out of 165 submissions."
R19-1131,Automated Text Simplification as a Preprocessing Step for Machine Translation into an Under-resourced Language,2019,0,0,1,1,24998,sanja vstajner,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"In this work, we investigate the possibility of using fully automatic text simplification system on the English source in machine translation (MT) for improving its translation into an under-resourced language. We use the state-of-the-art automatic text simplification (ATS) system for lexically and syntactically simplifying source sentences, which are then translated with two state-of-the-art English-to-Serbian MT systems, the phrase-based MT (PBMT) and the neural MT (NMT). We explore three different scenarios for using the ATS in MT: (1) using the raw output of the ATS; (2) automatically filtering out the sentences with low grammaticality and meaning preservation scores; and (3) performing a minimal manual correction of the ATS output. Our results show improvement in fluency of the translation regardless of the chosen scenario, and difference in success of the three scenarios depending on the MT approach used (PBMT or NMT) with regards to improving translation fluency and post-editing effort."
P19-1377,A Spreading Activation Framework for Tracking Conceptual Complexity of Texts,2019,0,0,2,0,14529,ioana hulputextcommabelows,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We propose an unsupervised approach for assessing conceptual complexity of texts, based on spreading activation. Using DBpedia knowledge graph as a proxy to long-term memory, mentioned concepts become activated and trigger further activation as the text is sequentially traversed. Drawing inspiration from psycholinguistic theories of reading comprehension, we model memory processes such as semantic priming, sentence wrap-up, and forgetting. We show that our models capture various aspects of conceptual text complexity and significantly outperform current state of the art."
W18-7006,Improving Machine Translation of {E}nglish Relative Clauses with Automatic Text Simplification,2018,0,2,1,1,24998,sanja vstajner,Proceedings of the 1st Workshop on Automatic Text Adaptation ({ATA}),0,None
W18-3104,Word Embeddings-Based Uncertainty Detection in Financial Disclosures,2018,-1,-1,2,0,28364,christoph theil,Proceedings of the First Workshop on Economics and Natural Language Processing,0,"In this paper, we use NLP techniques to detect linguistic uncertainty in financial disclosures. Leveraging general-domain and domain-specific word embedding models, we automatically expand an existing dictionary of uncertainty triggers. We furthermore examine how an expert filtering affects the quality of such an expansion. We show that the dictionary expansions significantly improve regressions on stock return volatility. Lastly, we prove that the expansions significantly boost the automatic detection of uncertain sentences."
W18-0507,A Report on the Complex Word Identification Shared Task 2018,2018,16,0,6,0.731707,282,seid yimam,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We report the findings of the second Complex Word Identification (CWI) shared task organized as part of the BEA workshop co-located with NAACL-HLT{'}2018. The second CWI shared task featured multilingual and multi-genre datasets divided into four tracks: English monolingual, German monolingual, Spanish monolingual, and a multilingual track with a French test set, and two tasks: binary classification and probabilistic classification. A total of 12 teams submitted their results in different task/track combinations and 11 of them wrote system description papers that are referred to in this report and appear in the BEA workshop proceedings."
L18-1479,A Detailed Evaluation of Neural Sequence-to-Sequence Models for In-domain and Cross-domain Text Simplification,2018,0,4,1,1,24998,sanja vstajner,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1615,{CATS}: A Tool for Customized Alignment of Text Simplification Corpora,2018,0,7,1,1,24998,sanja vstajner,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-3005,Data-Driven Text Simplification,2018,0,2,1,1,24998,sanja vstajner,Proceedings of the 27th International Conference on Computational Linguistics: Tutorial Abstracts,0,None
C18-1027,Automatic Assessment of Conceptual Text Complexity Using Knowledge Graphs,2018,0,0,1,1,24998,sanja vstajner,Proceedings of the 27th International Conference on Computational Linguistics,0,"Complexity of texts is usually assessed only at the lexical and syntactic levels. Although it is known that conceptual complexity plays a significant role in text understanding, no attempts have been made at assessing it automatically. We propose to automatically estimate the conceptual complexity of texts by exploiting a number of graph-based measures on a large knowledge base. By using a high-quality language learners corpus for English, we show that graph-based measures of individual text concepts, as well as the way they relate to each other in the knowledge graph, have a high discriminative power when distinguishing between two versions of the same text. Furthermore, when used as features in a binary classification task aiming to choose the simpler of two versions of the same text, our measures achieve high performance even in a default setup."
W17-5030,Effects of Lexical Properties on Viewing Time per Word in Autistic and Neurotypical Readers,2017,-1,-1,1,1,24998,sanja vstajner,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Eye tracking studies from the past few decades have shaped the way we think of word complexity and cognitive load: words that are long, rare and ambiguous are more difficult to read. However, online processing techniques have been scarcely applied to investigating the reading difficulties of people with autism and what vocabulary is challenging for them. We present parallel gaze data obtained from adult readers with autism and a control group of neurotypical readers and show that the former required higher cognitive effort to comprehend the texts as evidenced by three gaze-based measures. We divide all words into four classes based on their viewing times for both groups and investigate the relationship between longer viewing times and word length, word frequency, and four cognitively-based measures (word concreteness, familiarity, age of acquisition and imagability)."
yimam-etal-2017-multilingual,Multilingual and Cross-Lingual Complex Word Identification,2017,6,3,2,0.731707,282,seid yimam,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"Complex Word Identification (CWI) is an important task in lexical simplification and text accessibility. Due to the lack of CWI datasets, previous works largely depend on Simple English Wikipedia and edit histories for obtaining {`}gold standard{'} annotations, which are of doubtable quality, and limited only to English. We collect complex words/phrases (CP) for English, German and Spanish, annotated by both native and non-native speakers, and propose language independent features that can be used to train multilingual and cross-lingual CWI models. We show that the performance of cross-lingual CWI systems (using a model trained on one language and applying it on the other languages) is comparable to the performance of monolingual CWI systems."
P17-2014,Exploring Neural Text Simplification Models,2017,10,26,2,0,18363,sergiu nisioi,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We present the first attempt at using sequence to sequence neural networks to model text simplification (TS). Unlike the previously proposed automated TS systems, our neural text simplification (NTS) systems are able to simultaneously perform lexical simplification and content reduction. An extensive human evaluation of the output has shown that NTS systems achieve almost perfect grammaticality and meaning preservation of output sentences and higher level of simplification than the state-of-the-art automated TS systems"
P17-2016,Sentence Alignment Methods for Improving Text Simplification Systems,2017,7,4,1,1,24998,sanja vstajner,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We provide several methods for sentence-alignment of texts with different complexity levels. Using the best of them, we sentence-align the Newsela corpora, thus providing large training materials for automatic text simplification (ATS) systems. We show that using this dataset, even the standard phrase-based statistical machine translation models for ATS can outperform the state-of-the-art ATS systems."
I17-2068,{CWIG}3{G}2 - Complex Word Identification Task across Three Text Genres and Two User Groups,2017,0,8,2,0.731707,282,seid yimam,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Complex word identification (CWI) is an important task in text accessibility. However, due to the scarcity of CWI datasets, previous studies have only addressed this problem on Wikipedia sentences and have solely taken into account the needs of non-native English speakers. We collect a new CWI dataset (CWIG3G2) covering three text genres News, WikiNews, and Wikipedia) annotated by both native and non-native English speakers. Unlike previous datasets, we cover single words, as well as complex phrases, and present them for judgment in a paragraph context. We present the first study on cross-genre and cross-group CWI, showing measurable influences in native language and genre types."
W16-3411,Can Text Simplification Help Machine Translation?,2016,0,9,1,1,24998,sanja vstajner,Proceedings of the 19th Annual Conference of the {E}uropean Association for Machine Translation,0,"This article explores the use of text simplification as a pre-processing step for statistical machine translation of grammatically complex under-resourced languages. Our experiments on English-to-Serbian translation show that this approach can improve grammaticality (fluency) of the translation output and reduce technical post-editing effort (number of post-edit operations). Furthermore, the use of more aggressive text simplification methods (which do not only simplify the given sentence but also discard irrelevant information thus producing syntactically very simple sentences) also improves meaning preservation (adequacy) of the translation output."
L16-1094,Use of Domain-Specific Language Resources in Machine Translation,2016,0,3,1,1,24998,sanja vstajner,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper, we address the problem of Machine Translation (MT) for a specialised domain in a language pair for which only a very small domain-specific parallel corpus is available. We conduct a series of experiments using a purely phrase-based SMT (PBSMT) system and a hybrid MT system (TectoMT), testing three different strategies to overcome the problem of the small amount of in-domain training data. Our results show that adding a small size in-domain bilingual terminology to the small in-domain training corpus leads to the best improvements of a hybrid MT system, while the PBSMT system achieves the best results by adding a combination of in-domain bilingual terminology and a larger out-of-domain corpus. We focus on qualitative human evaluation of the output of two best systems (one for each approach) and perform a systematic in-depth error analysis which revealed advantages of the hybrid MT system over the pure PBSMT system for this specific task."
L16-1438,Bootstrapping a Hybrid {MT} System to a New Language Pair,2016,8,3,4,0,8864,joao rodrigues,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The usual concern when opting for a rule-based or a hybrid machine translation (MT) system is how much effort is required to adapt the system to a different language pair or a new domain. In this paper, we describe a way of adapting an existing hybrid MT system to a new language pair, and show that such a system can outperform a standard phrase-based statistical machine translation system with an average of 10 persons/month of work. This is specifically important in the case of domain-specific MT for which there is not enough parallel data for training a statistical machine translation system."
W15-5713,Machine Translation for Multilingual Troubleshooting in the {IT} Domain: A Comparison of Different Strategies,2015,23,1,1,1,24998,sanja vstajner,Proceedings of the 1st Deep Machine Translation Workshop,0,None
R15-1079,Translating from Original to Simplified Sentences using {M}oses: When does it Actually Work?,2015,16,0,1,1,24998,sanja vstajner,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"In recent years, several studies have approached the Text Simplification (TS) task as a machine translation (MT) problem. They report promising results in learning how to translate from xe2x80x98originalxe2x80x99 to xe2x80x98simplifiedxe2x80x99 language using the standard phrasebased translation model. However, our results indicate that this approach works well only when the training dataset consists mostly of those sentence pairs in which the simplified sentence is already very similar to its original. Our findings suggest that the standard phrase-based approach might not be appropriate to learn strong simplifications which are needed for certain target populations."
R15-1080,Automatic Text Simplification for {S}panish: Comparative Evaluation of Various Simplification Strategies,2015,21,4,1,1,24998,sanja vstajner,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"In this paper, we explore statistical machine translation (SMT) approaches to automatic text simplification (ATS) for Spanish. First, we compare the performances of the standard phrase-based (PB) and hierarchical (HIERO) SMT models in this specific task. In both cases, we build two models, one using the TS corpus with xe2x80x9clightxe2x80x9d simplifications and the other using the TS corpus with xe2x80x9cheavyxe2x80x9d simplifications. Next, we compare the two best systems with the state-of-the-art text simplification system for Spanish (Simplext). Our results, based on an extensive human evaluation, show that the SMT-based systems perform equally as well as, or better than, Simplext, despite the very small datasets used for training and tuning."
P15-2011,Simplifying Lexical Simplification: Do We Need Simplified Corpora?,2015,21,24,2,0.350877,7439,goran glavavs,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Simplification of lexically complex texts, by replacing complex words with their simpler synonyms, helps non-native speakers, children, and language-impaired people understand text better. Recent lexical simplification methods rely on manually simplified corpora, which are expensive and time-consuming to build. We present an unsupervised approach to lexical simplification that makes use of the most recent word vector representations and requires only regular corpora. Results of both automated and human evaluation show that our simple method is as effective as systems that rely on simplified corpora."
P15-2135,A Deeper Exploration of the Standard {PB}-{SMT} Approach to Text Simplification and its Evaluation,2015,14,9,1,1,24998,sanja vstajner,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Comunicacio presentada a: the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing del 26 al 31 de juliol de 2015 a Beijing, Xina."
W14-5604,"The Fewer, the Better? A Contrastive Study about Ways to Simplify",2014,7,5,2,0,5503,ruslan mitkov,Proceedings of the Workshop on Automatic Text Simplification - Methods and Applications in the Multilingual Society ({ATS}-{MA} 2014),0,"Simplified texts play an important role in providing accessible and easy-to-understand information for a whole range of users who, due to linguistic, developmental or social barriers, would have difficulty in understanding materials which are not adapted and/or simplified. However, the production of simplified texts can be a time-consuming and labour-intensive task. In this paper we show that the employment of a short list of simple simplification rules could result in texts of comparable readability to those written as a result of applying a long list of more fine-grained rules. We also prove that the simplification process based on the short list of simple rules is more time efficient and consistent. 1 Rationale Simplified texts play an important role in providing accessible and easy-to-understand information for a whole range of users who, due to linguistic, developmental or social barriers, would have difficulty in understanding materials which are not adapted and/or simplified. Such users include but are not limited to people with insufficient knowledge of the language in which the document is written, people with specific language disorders and people with low literacy levels. However, while the production of simplified texts is certainly an indispensable activity, it often proves to be a time-consuming and labour-intensive task. Various methodologies and simplification strategies have been developed which are often employed by authors to simplify original texts. Most methods involve a high number of rules which could result not only in the simplification task being time-consuming but also in the authors getting confused as to which rules to apply. We hypothesise that it is possible to achieve a comparable simplification effect by using a small set of simple rules similar to the ones used in Controlled Languages which, in addition, enhances the productivity and reliability of the simplification process. In order to test our hypothesis we conduct the following experiments. First, we propose six Controlled Language-inspired rules which we believe are simple and easy enough for writers of simplified texts to understand and apply. We then ask two writers to apply these rules to a selection of newswire texts and also to produce simplified versions of these texts using the 28 rules used in the Simplext project (Saggion et al., 2011). Both sets of texts are compared in terms of readability. In both simplification tasks the time efficiency is assessed and the inter-annotator agreement is evaluated. In an additional experiment, we seek to investigate the possible effect of familiarisation in simplification. In this experiment a third writer simplifies a sample of the texts used in the previous experiments by applying each set of rules in a mixed sequence pattern which does not offer any familiarisation nor the advantage of one set of rules over the other. Using these samples, three-way inter-annotator agreement is reported. The rest of the paper is structured as follows. Section 2 outlines related work on simplification rules. Section 3 introduces our proposal for a small set of easy-to-understand and easy-to-apply rules and contrasts them with the longer and more elaborate rules employed in the Simplext proposal. Section 4 details the experiments conducted in order to validate or refute our hypothesis, and outlines the data used for the experiments. Section 5 presents and discusses the results, while the last section of the paper summarises the main conclusions of this study."
W14-5606,Assessing Conformance of Manually Simplified Corpora with User Requirements: the Case of Autistic Readers,2014,35,1,1,1,24998,sanja vstajner,Proceedings of the Workshop on Automatic Text Simplification - Methods and Applications in the Multilingual Society ({ATS}-{MA} 2014),0,"In the state of the art, there are scarce resources available to support development and evaluation of automatic text simplification (TS) systems for specific target populations. These comprise parallel corpora consisting of texts in their original form and in a form that is more accessible for different categories of target reader, including neurotypical second language learners and young readers. In this paper, we investigate the potential to exploit resources developed for such readers to support the development of a text simplification system for use by people with autistic spectrum disorders (ASD). We analysed four corpora in terms of nineteen linguistic features which pose obstacles to reading comprehension for people with ASD. The results indicate that the Britannica TS parallel corpus (aimed at young readers) and the Weekly Reader TS parallel corpus (aimed at second language learners) may be suitable for training a TS system to assist people with ASD. Two sets of classification experiments intended to discriminate between original and simplified texts according to the nineteen features lent further support for those findings."
W14-1201,One Step Closer to Automatic Evaluation of Text Simplification Systems,2014,36,10,1,1,24998,sanja vstajner,Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations ({PITR}),0,"This study explores the possibility of replacing the costly and time-consuming human evaluation of the grammaticality and meaning preservation of the output of text simplification (TS) systems with some automatic measures. The focus is on six widely used machine translation (MT) evaluation metrics and their correlation with human judgements of grammaticality and meaning preservation in text snippets. As the results show a significant correlation between them, we go further and try to classify simplified sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising."
R13-2011,Event-Centered Simplification of News Stories,2013,34,4,2,0.350877,7439,goran glavavs,Proceedings of the Student Research Workshop associated with {RANLP} 2013,0,"Newswire text is often linguistically complex and stylistically decorated, hence very difficult to comprehend for people with reading disabilities. Acknowledging that events represent the most important information in news, we propose an eventcentered approach to news simplification. Our method relies on robust extraction of factual events and elimination of surplus information which is not part of event mentions. Experimental results obtained by combining automated readability measures with human evaluation of correctness justify the proposed event-centered approach to text simplification."
I13-1043,Readability Indices for Automatic Evaluation of Text Simplification Systems: A Feasibility Study for {S}panish,2013,25,8,1,1,24998,sanja vstajner,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"This paper addresses the problem of automatic evaluation of text simplification systems for Spanish. We test whether already-existing readability formulae would be suitable for this task. We adapt three existing readability indices (two measuring lexical complexity and one measuring syntactic complexity) to be computed automatically, which are then applied to a corpus of original news texts and their manual simplifications aimed at people with cognitive disabilities. We show that there is a significant correlation between each of the three readability indices and several linguistically motivated features which might be seen as reading obstacles for various target populations. Furthermore, we show that there is a significant correlation between the two readability indices which measure lexical complexity."
stajner-mitkov-2012-diachronic,Diachronic Changes in Text Complexity in 20th Century {E}nglish Language: An {NLP} Approach,2012,14,3,1,1,24998,sanja vstajner,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"A syntactically complex text may represent a problem for both comprehension by humans and various NLP tasks. A large number of studies in text simplification are concerned with this problem and their aim is to transform the given text into a simplified form in order to make it accessible to the wider audience. In this study, we were investigating what the natural tendency of texts is in 20th century English language. Are they becoming syntactically more complex over the years, requiring a higher literacy level and greater effort from the readers, or are they becoming simpler and easier to read? We examined several factors of text complexity (average sentence length, Automated Readability Index, sentence complexity and passive voice) in the 20th century for two main English language varieties - British and American, using the `Brown family' of corpora. In British English, we compared the complexity of texts published in 1931, 1961 and 1991, while in American English we compared the complexity of texts published in 1961 and 1992. Furthermore, we demonstrated how the state-of-the-art NLP tools can be used for automatic extraction of some complex features from the raw text version of the corpora."
W11-4112,Diachronic Stylistic Changes in {B}ritish and {A}merican Varieties of 20th Century Written {E}nglish Language,2011,28,5,1,1,24998,sanja vstajner,Proceedings of the Workshop on Language Technologies for Digital Humanities and Cultural Heritage,0,"In this paper we present the results of a study investigating the diachronic changes of four stylistic features: average sentence length, Automated Readability Index, lexical density and lexical richness in 20th century written English language. All experiments were conducted on the largest existing diachronic corpora of British and American English xe2x80x90 the Brown xe2x80x98familyxe2x80x99 corpora, employing NLP techniques for automatic extraction of the features. Additionally, we compare the trends of changes between the two English varieties and make suggestions for future studies of diachronic language change."
R11-2003,Towards a Better Exploitation of the Brown {`}Family{'} Corpora in Diachronic Studies of {B}ritish and {A}merican {E}nglish Language Varieties,2011,-1,-1,1,1,24998,sanja vstajner,Proceedings of the Second Student Research Workshop associated with {RANLP} 2011,0,None
