2021.naacl-main.16,Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation,2021,-1,-1,3,1,3263,alexandra chronopoulou,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Successful methods for unsupervised neural machine translation (UNMT) employ cross-lingual pretraining via self-supervision, often in the form of a masked language modeling or a sequence generation task, which requires the model to align the lexical- and high-level representations of the two languages. While cross-lingual pretraining works for similar languages with abundant corpora, it performs poorly in low-resource and distant languages. Previous research has shown that this is because the representations are not sufficiently aligned. In this paper, we enhance the bilingual masked language model pretraining with lexical-level information by using type-level cross-lingual subword embeddings. Empirical results demonstrate improved performance both on UNMT (up to 4.5 BLEU) and bilingual lexicon induction using our method compared to a UNMT baseline."
2021.mrl-1.4,Do not neglect related languages: The case of low-resource {O}ccitan cross-lingual word embeddings,2021,-1,-1,3,0,5204,lisa woller,Proceedings of the 1st Workshop on Multilingual Representation Learning,0,"Cross-lingual word embeddings (CLWEs) have proven indispensable for various natural language processing tasks, e.g., bilingual lexicon induction (BLI). However, the lack of data often impairs the quality of representations. Various approaches requiring only weak cross-lingual supervision were proposed, but current methods still fail to learn good CLWEs for languages with only a small monolingual corpus. We therefore claim that it is necessary to explore further datasets to improve CLWEs in low-resource setups. In this paper we propose to incorporate data of related high-resource languages. In contrast to previous approaches which leverage independently pre-trained embeddings of languages, we (i) train CLWEs for the low-resource and a related language jointly and (ii) map them to the target language to build the final multilingual space. In our experiments we focus on Occitan, a low-resource Romance language which is often neglected due to lack of resources. We leverage data from French, Spanish and Catalan for training and evaluate on the Occitan-English BLI task. By incorporating supporting languages our method outperforms previous approaches by a large margin. Furthermore, our analysis shows that the degree of relatedness between an incorporated language and the low-resource language is critically important."
2021.ltedi-1.3,Cross-Lingual Transfer Learning for Hate Speech Detection,2021,-1,-1,3,0,5328,irina bigoulaeva,"Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion",0,"We address the task of automatic hate speech detection for low-resource languages. Rather than collecting and annotating new hate speech data, we show how to use cross-lingual transfer learning to leverage already existing data from higher-resource languages. Using bilingual word embeddings based classifiers we achieve good performance on the target language by training only on the source dataset. Using our transferred system we bootstrap on unlabeled target language data, improving the performance of standard cross-lingual transfer approaches. We use English as a high resource language and German as the target language for which only a small amount of annotated corpora are available. Our results indicate that cross-lingual transfer learning together with our approach to leverage additional unlabeled data is an effective way of achieving good performance on low-resource target languages without the need for any target-language annotations."
2021.findings-emnlp.315,Adapting Entities across Languages and Cultures,2021,-1,-1,4,0.952381,7194,denis peskov,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"How would you explain Bill Gates to a German? He is associated with founding a company in the United States, so perhaps the German founder Carl Benz could stand in for Gates in those contexts. This type of translation is called adaptation in the translation community. Until now, this task has not been done computationally. Automatic adaptation could be used in natural language processing for machine translation and indirectly for generating new question answering datasets and education. We propose two automatic methods and compare them to human results for this novel NLP task. First, a structured knowledge base adapts named entities using their shared properties. Second, vector-arithmetic and orthogonal embedding mappings methods identify better candidates, but at the expense of interpretable features. We evaluate our methods through a new dataset of human adaptations."
2021.adaptnlp-1.9,Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation,2021,-1,-1,2,1,3264,dario stojanovski,Proceedings of the Second Workshop on Domain Adaptation for NLP,0,"Achieving satisfying performance in machine translation on domains for which there is no training data is challenging. Traditional supervised domain adaptation is not suitable for addressing such zero-resource domains because it relies on in-domain parallel data. We show that when in-domain parallel data is not available, access to document-level context enables better capturing of domain generalities compared to only having access to a single sentence. Having access to more information provides a more reliable domain estimation. We present two document-level Transformer models which are capable of using large context sizes and we compare these models against strong Transformer baselines. We obtain improvements for the two zero-resource domains we study. We additionally provide an analysis where we vary the amount of context and look at the case where in-domain data is available."
2021.acl-short.30,Anchor-based Bilingual Word Embeddings for Low-Resource Languages,2021,-1,-1,3,0,12523,tobias eder,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Good quality monolingual word embeddings (MWEs) can be built for languages which have large amounts of unlabeled text. MWEs can be aligned to bilingual spaces using only a few thousand word translation pairs. For low resource languages training MWEs monolingually results in MWEs of poor quality, and thus poor bilingual word embeddings (BWEs) as well. This paper proposes a new approach for building BWEs in which the vector space of the high resource source language is used as a starting point for training an embedding space for the low resource target language. By using the source vectors as anchors the vector spaces are automatically aligned during training. We experiment on English-German, English-Hiligaynon and English-Macedonian. We show that our approach results not only in improved BWEs and bilingual lexicon induction performance, but also in improved target language MWE quality as measured using monolingual word similarity."
2020.wmt-1.80,Findings of the {WMT} 2020 Shared Tasks in Unsupervised {MT} and Very Low Resource Supervised {MT},2020,-1,-1,1,1,3265,alexander fraser,Proceedings of the Fifth Conference on Machine Translation,0,"We describe the WMT 2020 Shared Tasks in Unsupervised MT and Very Low Resource Supervised MT. In both tasks, the community studied German to Upper Sorbian and Upper Sorbian to German MT, which is a very realistic machine translation scenario (unlike the simulated scenarios used in particular in much of the unsupervised MT work in the past). We were able to obtain most of the digital data available for Upper Sorbian, a minority language of Germany, which was the original motivation for the Unsupervised MT shared task. As we were defining the task, we also obtained a small amount of parallel data (about 60000 parallel sentences), allowing us to offer a Very Low Resource Supervised MT task as well. Six primary systems participated in the unsupervised shared task, two of these systems used additional data beyond the data released by the organizers. Ten primary systems participated in the very low resource supervised task. The paper discusses the background, presents the tasks and results, and discusses best practices for the future."
2020.wmt-1.128,{T}he {LMU} {M}unich {S}ystem for the {WMT} 2020 {U}nsupervised {M}achine {T}ranslation {S}hared {T}ask,2020,-1,-1,4,1,3263,alexandra chronopoulou,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes the submission of LMU Munich to the WMT 2020 unsupervised shared task, in two language directions, GermanâUpper Sorbian. Our core unsupervised neural machine translation (UNMT) system follows the strategy of Chronopoulou et al. (2020), using a monolingual pretrained language generation model (on German) and fine-tuning it on both German and Upper Sorbian, before initializing a UNMT model, which is trained with online backtranslation. Pseudo-parallel data obtained from an unsupervised statistical machine translation (USMT) system is used to fine-tune the UNMT model. We also apply BPE-Dropout to the low resource (Upper Sorbian) data to obtain a more robust system. We additionally experiment with residual adapters and find them useful in the Upper SorbianâGerman direction. We explore sampling during backtranslation and curriculum learning to use SMT translations in a more principled way. Finally, we ensemble our best-performing systems and reach a BLEU score of 32.4 on GermanâUpper Sorbian and 35.2 on Upper SorbianâGerman."
2020.wmt-1.131,The {LMU} {M}unich System for the {WMT}20 Very Low Resource Supervised {MT} Task,2020,-1,-1,4,0,13977,jindvrich libovicky,Proceedings of the Fifth Conference on Machine Translation,0,"We present our systems for the WMT20 Very Low Resource MT Task for translation between German and Upper Sorbian. For training our systems, we generate synthetic data by both back- and forward-translation. Additionally, we enrich the training data with German-Czech translated from Czech to Upper Sorbian by an unsupervised statistical MT system incorporating orthographically similar word pairs and transliterations of OOV words. Our best translation system between German and Sorbian is based on transfer learning from a Czech-German system and scores 12 to 13 BLEU higher than a baseline system built using the available parallel data only."
2020.lrec-1.313,"Exploring Bilingual Word Embeddings for {H}iligaynon, a Low-Resource Language",2020,-1,-1,3,0,17284,leah michel,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper investigates the use of bilingual word embeddings for mining Hiligaynon translations of English words. There is very little research on Hiligaynon, an extremely low-resource language of Malayo-Polynesian origin with over 9 million speakers in the Philippines (we found just one paper). We use a publicly available Hiligaynon corpus with only 300K words, and match it with a comparable corpus in English. As there are no bilingual resources available, we manually develop a English-Hiligaynon lexicon and use this to train bilingual word embeddings. But we fail to mine accurate translations due to the small amount of data. To find out if the same holds true for a related language pair, we simulate the same low-resource setup on English to German and arrive at similar results. We then vary the size of the comparable English and German corpora to determine the minimum corpus size necessary to achieve competitive results. Further, we investigate the role of the seed lexicon. We show that with the same corpus size but with a smaller seed lexicon, performance can surpass results of previous studies. We release the lexicon of 1,200 English-Hiligaynon word pairs we created to encourage further investigation."
2020.findings-emnlp.150,On the Language Neutrality of Pre-trained Multilingual Representations,2020,37,0,3,0,13977,jindvrich libovicky,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Multilingual contextual embeddings, such as multilingual BERT and XLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work probed the cross-linguality of the representations indirectly using zero-shot transfer learning on morphological and syntactic tasks. We instead investigate the language-neutrality of multilingual contextual embeddings directly and with respect to lexical semantics. Our results show that contextual embeddings are more language-neutral and, in general, more informative than aligned static word-type embeddings, which are explicitly trained for language neutrality. Contextual embeddings are still only moderately language-neutral by default, so we propose two simple methods for achieving stronger language neutrality: first, by unsupervised centering of the representation for each language and second, by fitting an explicit projection on small parallel data. Besides, we show how to reach state-of-the-art accuracy on language identification and match the performance of statistical methods for word alignment of parallel sentences without using parallel data."
2020.emnlp-main.203,Towards Reasonably-Sized Character-Level Transformer {NMT} by Finetuning Subword Systems,2020,22,0,2,0,13977,jindvrich libovicky,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train. These problems can be partially overcome by incorporating a segmentation into tokens in the model. We show that by initially training a subword model and then finetuning it on characters, we can obtain a neural machine translation model that works at the character level without requiring token segmentation. We use only the vanilla 6-layer Transformer Base architecture. Our character-level models better capture morphological phenomena and show more robustness to noise at the expense of somewhat worse overall translation quality. Our study is a significant step towards high-performance and easy to train character-based models that are not extremely large."
2020.emnlp-main.214,{R}eusing a {P}retrained {L}anguage {M}odel on {L}anguages with {L}imited {C}orpora for {U}nsupervised {NMT},2020,-1,-1,3,1,3263,alexandra chronopoulou,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Using a language model (LM) pretrained on two languages with large monolingual data in order to initialize an unsupervised neural machine translation (UNMT) system yields state-of-the-art results. When limited data is available for one language, however, this method leads to poor translations. We present an effective approach that reuses an LM that is pretrained only on the high-resource language. The monolingual LM is fine-tuned on both languages and is then used to initialize a UNMT model. To reuse the pretrained LM, we have to modify its predefined vocabulary, to account for the new language. We therefore propose a novel vocabulary extension method. Our approach, RE-LM, outperforms a competitive cross-lingual pretraining model (XLM) in English-Macedonian (En-Mk) and English-Albanian (En-Sq), yielding more than +8.3 BLEU points for all four translation directions."
2020.coling-main.417,{C}ontra{CAT}: Contrastive Coreference Analytical Templates for Machine Translation,2020,-1,-1,4,1,3264,dario stojanovski,Proceedings of the 28th International Conference on Computational Linguistics,0,"Recent high scores on pronoun translation using context-aware neural machine translation have suggested that current approaches work well. ContraPro is a notable example of a contrastive challenge set for EnglishâGerman pronoun translation. The high scores achieved by transformer models may suggest that they are able to effectively model the complicated set of inferences required to carry out pronoun translation. This entails the ability to determine which entities could be referred to, identify which entity a source-language pronoun refers to (if any), and access the target-language grammatical gender for that entity. We first show through a series of targeted adversarial attacks that in fact current approaches are not able to model all of this information well. Inserting small amounts of distracting information is enough to strongly reduce scores, which should not be the case. We then create a new template test set ContraCAT, designed to individually assess the ability to handle the specific steps necessary for successful pronoun translation. Our analyses show that current approaches to context-aware NMT rely on a set of surface heuristics, which break down when translations require real reasoning. We also propose an approach for augmenting the training data, with some improvements."
2020.coling-main.531,Combining Word Embeddings with Bilingual Orthography Embeddings for Bilingual Dictionary Induction,2020,-1,-1,3,0,21646,silvia severini,Proceedings of the 28th International Conference on Computational Linguistics,0,"Bilingual dictionary induction (BDI) is the task of accurately translating words to the target language. It is of great importance in many low-resource scenarios where cross-lingual training data is not available. To perform BDI, bilingual word embeddings (BWEs) are often used due to their low bilingual training signal requirements. They achieve high performance, but problematic cases still remain, such as the translation of rare words or named entities, which often need to be transliterated. In this paper, we enrich BWE-based BDI with transliteration information by using Bilingual Orthography Embeddings (BOEs). BOEs represent source and target language transliteration word pairs with similar vectors. A key problem in our BDI setup is to decide which information source {--} BWEs (or semantics) vs. BOEs (or orthography) {--} is more reliable for a particular word pair. We propose a novel classification-based BDI system that uses BWEs, BOEs and a number of other features to make this decision. We test our system on English-Russian BDI and show improved performance. In addition, we show the effectiveness of our BOEs by successfully using them for transliteration mining based on cosine similarity."
2020.bucc-1.8,{LMU} Bilingual Dictionary Induction System with Word Surface Similarity Scores for {BUCC} 2020,2020,-1,-1,3,0,21646,silvia severini,Proceedings of the 13th Workshop on Building and Using Comparable Corpora,0,"The task of Bilingual Dictionary Induction (BDI) consists of generating translations for source language words which is important in the framework of machine translation (MT). The aim of the BUCC 2020 shared task is to perform BDI on various language pairs using comparable corpora. In this paper, we present our approach to the task of English-German and English-Russian language pairs. Our system relies on Bilingual Word Embeddings (BWEs) which are often used for BDI when only a small seed lexicon is available making them particularly effective in a low-resource setting. On the other hand, they perform well on high frequency words only. In order to improve the performance on rare words as well, we combine BWE based word similarity with word surface similarity methods, such as orthography In addition to the often used top-n translation method, we experiment with a margin based approach aiming for dynamic number of translations for each source word. We participate in both the open and closed tracks of the shared task and we show improved results of our method compared to simple vector similarity based approaches. Our system was ranked in the top-3 teams and achieved the best results for English-Russian."
2020.amta-research.8,Towards Handling Compositionality in Low-Resource Bilingual Word Induction,2020,-1,-1,2,1,5205,viktor hangya,Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track),0,None
2020.acl-main.389,Modeling Word Formation in {E}nglish{--}{G}erman Neural Machine Translation,2020,-1,-1,2,1,22882,marion marco,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"This paper studies strategies to model word formation in NMT using rich linguistic information, namely a word segmentation approach that goes beyond splitting into substrings by considering fusional morphology. Our linguistically sound segmentation is combined with a method for target-side inflection to accommodate modeling word formation. The best system variants employ source-side morphological analysis and model complex target-side words, improving over a standard system."
W19-6614,Improving Anaphora Resolution in Neural Machine Translation Using Curriculum Learning,2019,0,3,2,1,3264,dario stojanovski,Proceedings of Machine Translation Summit XVII: Research Track,0,None
W19-5344,The {LMU} {M}unich Unsupervised Machine Translation System for {WMT}19,2019,0,1,4,1,3264,dario stojanovski,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,We describe LMU Munich{'}s machine translation system for GermanâCzech translation which was used to participate in the WMT19 shared task on unsupervised news translation. We train our model using monolingual data only from both languages. The final model is an unsupervised neural model using established techniques for unsupervised translation such as denoising autoencoding and online back-translation. We bootstrap the model with masked language model pretraining and enhance it with back-translations from an unsupervised phrase-based system which is itself bootstrapped using unsupervised bilingual word embeddings.
W19-5345,Combining Local and Document-Level Context: The {LMU} {M}unich Neural Machine Translation System at {WMT}19,2019,0,2,2,1,3264,dario stojanovski,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"We describe LMU Munich{'}s machine translation system for EnglishâGerman translation which was used to participate in the WMT19 shared task on supervised news translation. We specifically participated in the document-level MT track. The system used as a primary submission is a context-aware Transformer capable of both rich modeling of limited contextual information and integration of large-scale document-level context with a less rich representation. We train this model by fine-tuning a big Transformer baseline. Our experimental results show that document-level context provides for large improvements in translation quality, and adding a rich representation of the previous sentence provides a small additional gain."
W19-1425,Cross-lingual Annotation Projection Is Effective for Neural Part-of-Speech Tagging,2019,0,0,3,0.259002,5061,matthias huck,"Proceedings of the Sixth Workshop on {NLP} for Similar Languages, Varieties and Dialects",0,"We tackle the important task of part-of-speech tagging using a neural model in the zero-resource scenario, where we have no access to gold-standard POS training data. We compare this scenario with the low-resource scenario, where we have access to a small amount of gold-standard POS training data. Our experiments focus on Ukrainian as a representative of under-resourced languages. Russian is highly related to Ukrainian, so we exploit gold-standard Russian POS tags. We consider four techniques to perform Ukrainian POS tagging: zero-shot tagging and cross-lingual annotation projection (for the zero-resource scenario), and compare these with self-training and multilingual learning (for the low-resource scenario). We find that cross-lingual annotation projection works particularly well in the zero-resource scenario."
P19-1118,Unsupervised Parallel Sentence Extraction with Parallel Segment Detection Helps Machine Translation,2019,0,1,2,1,5205,viktor hangya,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Mining parallel sentences from comparable corpora is important. Most previous work relies on supervised systems, which are trained on parallel data, thus their applicability is problematic in low-resource scenarios. Recent developments in building unsupervised bilingual word embeddings made it possible to mine parallel sentences based on cosine similarities of source and target language words. We show that relying only on this information is not enough, since sentences often have similar words but different meanings. We detect continuous parallel segments in sentence pair candidates and rely on them when mining parallel sentences. We show better mining accuracy on three language pairs in a standard shared task on artificial data. We also provide the first experiments showing that parallel sentences mined from real life sources improve unsupervised MT. Our code is available, we hope it will be used to support low-resource MT research."
P19-1581,Better {OOV} Translation with Bilingual Terminology Mining,2019,0,0,3,0.259002,5061,matthias huck,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Unseen words, also called out-of-vocabulary words (OOVs), are difficult for machine translation. In neural machine translation, byte-pair encoding can be used to represent OOVs, but they are still often incorrectly translated. We improve the translation of OOVs in NMT using easy-to-obtain monolingual data. We look for OOVs in the text to be translated and translate them using simple-to-construct bilingual word embeddings (BWEs). In our MT experiments we take the 5-best candidates, which is motivated by intrinsic mining experiments. Using all five of the proposed target language words as queries we mine target-language sentences. We then back-translate, forcing the back-translation of each of the five proposed target-language OOV-translation-candidates to be the original source-language OOV. We show that by using this synthetic data to fine-tune our system the translation of OOVs can be dramatically improved. In our experiments we use a system trained on Europarl and mine sentences containing medical terms from monolingual data."
W18-6428,The {LMU} {M}unich Unsupervised Machine Translation Systems,2018,0,0,4,1,3264,dario stojanovski,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"We describe LMU Munich{'}s unsupervised machine translation systems for EnglishâGerman translation. These systems were used to participate in the WMT18 news translation shared task and more specifically, for the unsupervised learning sub-track. The systems are trained on English and German monolingual data only and exploit and combine previously proposed techniques such as using word-by-word translated data based on bilingual word embeddings, denoising and on-the-fly backtranslation."
W18-6446,{LMU} {M}unich{'}s Neural Machine Translation Systems at {WMT} 2018,2018,0,0,4,0.290057,5061,matthias huck,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"We present the LMU Munich machine translation systems for the English{--}German language pair. We have built neural machine translation systems for both translation directions (EnglishâGerman and GermanâEnglish) and for two different domains (the biomedical domain and the news domain). The systems were used for our participation in the WMT18 biomedical translation task and in the shared task on machine translation of news. The main focus of our recent system development efforts has been on achieving improvements in the biomedical domain over last year{'}s strong biomedical translation engine for EnglishâGerman (Huck et al., 2017a). Considerable progress has been made in the latter task, which we report on in this paper."
W18-6477,An Unsupervised System for Parallel Corpus Filtering,2018,0,1,2,1,5205,viktor hangya,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"In this paper we describe LMU Munich{'}s submission for the \textit{WMT 2018 Parallel Corpus Filtering} shared task which addresses the problem of cleaning noisy parallel corpora. The task of mining and cleaning parallel sentences is important for improving the quality of machine translation systems, especially for low-resource languages. We tackle this problem in a fully unsupervised fashion relying on bilingual word embeddings created without any bilingual signal. After pre-filtering noisy data we rank sentence pairs by calculating bilingual sentence-level similarities and then remove redundant data by employing monolingual similarity as well. Our unsupervised system achieved good performance during the official evaluation of the shared task, scoring only a few BLEU points behind the best systems, while not requiring any parallel training data."
W18-6306,Coreference and Coherence in Neural Machine Translation: A Study Using Oracle Experiments,2018,0,5,2,1,3264,dario stojanovski,Proceedings of the Third Conference on Machine Translation: Research Papers,0,"Cross-sentence context can provide valuable information in Machine Translation and is critical for translation of anaphoric pronouns and for providing consistent translations. In this paper, we devise simple oracle experiments targeting coreference and coherence. Oracles are an easy way to evaluate the effect of different discourse-level phenomena in NMT using BLEU and eliminate the necessity to manually define challenge sets for this purpose. We propose two context-aware NMT models and compare them against models working on a concatenation of consecutive sentences. Concatenation models perform better, but are computationally expensive. We show that NMT models taking advantage of context oracle signals can achieve considerable gains in BLEU, of up to 7.02 BLEU for coreference and 1.89 BLEU for coherence on subtitles translation. Access to strong signals allows us to make clear comparisons between context-aware models."
W18-1805,Neural Morphological Tagging of Lemma Sequences for Machine Translation,2018,0,0,3,0,215,costanza conforti,Proceedings of the 13th Conference of the Association for Machine Translation in the {A}mericas (Volume 1: Research Track),0,None
P18-1075,Two Methods for Domain Adaptation of Bilingual Tasks: Delightfully Simple and Broadly Applicable,2018,0,7,3,1,5205,viktor hangya,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Bilingual tasks, such as bilingual lexicon induction and cross-lingual classification, are crucial for overcoming data sparsity in the target language. Resources required for such tasks are often out-of-domain, thus domain adaptation is an important problem here. We make two contributions. First, we test a delightfully simple method for domain adaptation of bilingual word embeddings. We evaluate these embeddings on two bilingual tasks involving different domains: cross-lingual twitter sentiment classification and medical bilingual lexicon induction. Second, we tailor a broadly applicable semi-supervised classification method from computer vision to these tasks. We show that this method also helps in low-resource setups. Using both methods together we achieve large improvements over our baselines, by using only additional unlabeled data."
P18-1141,Embedding Learning Through Multilingual Concept Induction,2018,0,3,4,0,705,philipp dufter,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,We present a new method for estimating vector space representations of words: embedding learning by concept induction. We test this method on a highly parallel corpus and learn semantic representations of words in 1259 different languages in a single common space. An extensive experimental evaluation on crosslingual word similarity and sentiment analysis indicates that concept-based multilingual embedding learning performs better than previous approaches.
N18-2030,Evaluating bilingual word embeddings on the long tail,2018,0,1,4,1,10142,fabienne braune,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"Bilingual word embeddings are useful for bilingual lexicon induction, the task of mining translations of given words. Many studies have shown that bilingual word embeddings perform well for bilingual lexicon induction but they focused on frequent words in general domains. For many applications, bilingual lexicon induction of rare and domain-specific words is of critical importance. Therefore, we design a new task to evaluate bilingual word embeddings on rare words in different domains. We show that state-of-the-art approaches fail on this task and present simple new techniques to improve bilingual word embeddings for mining rare words. We release new gold standard datasets and code to stimulate research on this task."
W17-4704,Modeling Target-Side Inflection in Neural Machine Translation,2017,11,0,3,0.966834,4973,alevs tamchyna,Proceedings of the Second Conference on Machine Translation,0,"NMT systems have problems with large vocabulary sizes. Byte-pair encoding (BPE) is a popular approach to solving this problem, but while BPE allows the system to generate any target-side word, it does not enable effective generalization over the rich vocabulary in morphologically rich languages with strong inflectional phenomena. We introduce a simple approach to overcome this problem by training a system to produce the lemma of a word and its morphologically rich POS tag, which is then followed by a deterministic generation step. We apply this strategy for English-Czech and English-German translation scenarios, obtaining improvements in both settings. We furthermore show that the improvement is not due to only adding explicit morphological information."
W17-4706,Target-side Word Segmentation Strategies for Neural Machine Translation,2017,28,12,3,0.327513,5061,matthias huck,Proceedings of the Second Conference on Machine Translation,0,None
W17-4730,{LMU} {M}unich{'}s Neural Machine Translation Systems for News Articles and Health Information Texts,2017,21,3,3,0.327513,5061,matthias huck,Proceedings of the Second Conference on Machine Translation,0,None
P17-4001,"Annotating tense, mood and voice for {E}nglish, {F}rench and {G}erman",2017,8,3,4,1,32509,anita ramm,"Proceedings of {ACL} 2017, System Demonstrations",0,"We present the first open-source tool for annotating morphosyntactic tense, mood and voice for English, French and German verbal complexes. The annotation is based on a set of language-specific rul ..."
J17-2003,"Statistical Models for Unsupervised, Semi-Supervised Supervised Transliteration Mining",2017,66,7,3,0.488641,3156,hassan sajjad,Computational Linguistics,0,"We present a generative model that efficiently mines transliteration pairs in a consistent fashion in three different settings: unsupervised, semi-supervised, and supervised transliteration mining. The model interpolates two sub-models, one for the generation of transliteration pairs and one for the generation of non-transliteration pairs (i.e., noise). The model is trained on noisy unlabeled data using the EM algorithm. During training the transliteration sub-model learns to generate transliteration pairs and the fixed non-transliteration model generates the noise pairs. After training, the unlabeled data is disambiguated based on the posterior probabilities of the two sub-models. We evaluate our transliteration mining system on data from a transliteration mining shared task and on parallel corpora. For three out of four language pairs, our system outperforms all semi-supervised and supervised systems that participated in the NEWS 2010 shared task. On word pairs extracted from parallel corpora with fewer than 2{\%} transliteration pairs, our system achieves up to 86.7{\%} F-measure with 77.9{\%} precision and 97.8{\%} recall."
E17-2059,Producing Unseen Morphological Variants in Statistical Machine Translation,2017,0,6,4,0.327513,5061,matthias huck,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Translating into morphologically rich languages is difficult. Although the coverage of lemmas may be reasonable, many morphological variants cannot be learned from the training data. We present a statistical translation system that is able to produce these inflected word forms. Different from most previous work, we do not separate morphological prediction from lexical choice into two consecutive steps. Our approach is novel in that it is integrated in decoding and takes advantage of context information from both the source language and the target language sides."
E17-2099,"Addressing Problems across Linguistic Levels in {SMT}: Combining Approaches to Model Morphology, Syntax and Lexical Choice",2017,9,0,2,1,22882,marion marco,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Many errors in phrase-based SMT can be attributed to problems on three linguistic levels: morphological complexity in the target language, structural differences and lexical choice. We explore combinations of linguistically motivated approaches to address these problems in English-to-German SMT and show that they are complementary to one another, but also that the popular verbal pre-ordering can cause problems on the morphological and lexical level. A discriminative classifier can overcome these problems, in particular when enriching standard lexical features with features geared towards verbal inflection."
W16-2315,The {E}dinburgh/{LMU} Hierarchical Machine Translation System for {WMT} 2016,2016,39,2,2,0.327513,5061,matthias huck,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the hierarchical phrase-based machine translation system built jointly by the University of Edinburgh and the University of Munich (LMU) for the shared translation task at the ACL 2016 First Conference on Machine Translation (WMT16). The WMT16 Edinburgh/LMU system was trained for translation of news domain texts from English into Romanian. We participated in the shared task for machine translation of news under xe2x80x9cconstrainedxe2x80x9d conditions, i.e. using the provided training data only."
W16-2320,The {QT}21/{H}im{L} Combined Machine Translation System,2016,5,6,6,0,30412,janthorsten peter,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the joint submission of the QT21 and HimL projects for the Englishxe2x86x92Romanian translation task of the ACL 2016 First Conference on Machine Translation (WMT 2016). The submission is a system combination which combines twelve different statistical machine translation systems provided by the different groups (RWTH Aachen University, LMU Munich, Charles University in Prague, University of Edinburgh, University of Sheffield, Karlsruhe Institute of Technology, LIMSI, University of Amsterdam, Tilde). The systems are combined using RWTHxe2x80x99s system combination approach. The final submission shows an improvement of 1.0 BLEU compared to the best single system on newstest2016."
W16-2325,{CUNI}-{LMU} Submissions in {WMT}2016: Chimera Constrained and Beaten,2016,14,4,4,0.966834,4973,alevs tamchyna,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the phrase-based systems jointly submitted by CUNI and LMU to English-Czech and English-Romanian News translation tasks of WMT16. In contrast to previous years, we strictly limited our training data to the constraint datasets, to allow for a reliable comparison with other research systems. We experiment with using several additional models in our system, including a feature-rich discriminative model of phrasal translation."
W16-2203,Modeling verbal inflection for {E}nglish to {G}erman {SMT},2016,9,0,2,1,32509,anita ramm,"Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers",0,None
W16-2205,Modeling Complement Types in Phrase-Based {SMT},2016,18,0,2,1,22882,marion marco,"Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers",0,"We explore two approaches to model complement types (NPs and PPs) in an Englishto-German SMT system: A simple abstract representation inserts pseudo-prepositions that mark the beginning of noun phrases, to improve the symmetry of source and target complement types, and to provide a flat structural information on phrase boundaries. An extension of this representation generates context-aware synthetic phrasetable entries conditioned on the source side, to model complement types in terms of grammatical case and preposition choice. Both the simple preposition-informed system and the context-aware system significantly improve over the baseline; and the context-aware system is slightly better than the system without context information."
W16-2210,A Framework for Discriminative Rule Selection in Hierarchical {M}oses,2016,0,0,2,1,10142,fabienne braune,"Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers",0,None
P16-1161,Target-Side Context for Discriminative Models in Statistical Machine Translation,2016,18,4,2,0.966834,4973,alevs tamchyna,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target-context model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses."
W15-4923,Target-Side Generation of Prepositions for {SMT},2015,16,3,2,1,36553,marion weller,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,"We present a translation system that models the selection of prepositions in a targetside generation component. This novel approach allows the modeling of all subcategorized elements of a verb as either NPs or PPs according to target-side requirements relying on source and target side features. The BLEU scores are encouraging, but fail to surpass the baseline. We additionally evaluate the preposition accuracy for a carefully selected subset and discuss how typical problems of translating prepositions can be modeled with our method."
W15-3007,{C}im{S} - The {CIS} and {IMS} Joint Submission to {WMT} 2015 addressing morphological and syntactic differences in {E}nglish to {G}erman {SMT},2015,23,2,4,0.972222,30805,fabienne cap,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"We present the CimS submissions to the WMT 2015 Shared Task for the translation direction English to German. Similar to our previous submissions, all of our systems are aware of the complex nominal morphology of German. In this paper, we combine source-side reordering and target-side compound processing with basic morphological processing in order to obtain improved translation results. We also report on morphological processing for English to French."
W15-1008,Predicting Prepositions for {SMT},2015,4,0,2,1,36553,marion weller,"Proceedings of the Ninth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"Representation and Prediction Features Initial experiments showed that replacing prepositions by simple place-holders decreases the translation quality. As an extension to the basic approach with plain place-holders, we thus experiment with enriching the place-holders such that they contain more relevant information and represent the content of a preposition while still being in an abstract form. For example, the representation can be enriched by annotating the place-holder with the grammatical case of the preposition it represents: for overt prepositions, case is often an indicator of the content (e.g. direction/location), whereas for NPs, case indicates"
K15-1017,Labeled Morphological Segmentation with Semi-{M}arkov Models,2015,35,9,3,0,1281,ryan cotterell,Proceedings of the Nineteenth Conference on Computational Natural Language Learning,0,"We present labeled morphological segmentationxe2x80x94an alternative view of morphological processing that unifies several tasks. We introduce a new hierarchy of morphotactic tagsets and CHIPMUNK, a discriminative morphological segmentation system that, contrary to previous work, explicitly models morphotactics. We show improved performance on three tasks for all six languages: (i) morphological segmentation, (ii) stemming and (iii) morphological tag classification. For morphological segmentation our method shows absolute improvements of 2-6 points F1 over a strong baseline."
J15-2001,The Operation Sequence {M}odel{---}{C}ombining N-Gram-Based and Phrase-Based Statistical Machine Translation,2015,48,18,3,0.814466,3159,nadir durrani,Computational Linguistics,0,"In this article, we present a novel machine translation model, the Operation Sequence Model OSM, which combines the benefits of phrase-based and N-gram-based statistical machine translation SMT and remedies their drawbacks. The model represents the translation process as a linear sequence of operations. The sequence includes not only translation operations but also reordering operations. As in N-gram-based SMT, the model is: i based on minimal translation units, ii takes both source and target information into account, iii does not make a phrasal independence assumption, and iv avoids the spurious phrasal segmentation problem. As in phrase-based SMT, the model i has the ability to memorize lexical reordering triggers, ii builds the search graph dynamically, and iii decodes with large translation units during search. The unique properties of the model are i its strong coupling of reordering and translation where translation and reordering decisions are conditioned on n previous translation and reordering decisions, and ii the ability to model local and long-range reorderings consistently. Using BLEU as a metric of translation accuracy, we found that our system performs significantly better than state-of-the-art phrase-based systems Moses and Phrasal and N-gram-based systems Ncode on standard translation tasks. We compare the reordering component of the OSM to the Moses lexical reordering model by integrating it into Moses. Our results show that OSM outperforms lexicalized reordering on all translation tasks. The translation quality is shown to be improved further by learning generalized representations with a POS-based OSM."
D15-1129,Rule Selection with Soft Syntactic Features for String-to-Tree Statistical Machine Translation,2015,30,2,3,1,10142,fabienne braune,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"In syntax-based machine translation, rule selection is the task of choosing the correct target side of a translation rule among rules with the same source side. We define a discriminative rule selection model for systems that have syntactic annotation on the target language side (stringto-tree). This is a new and clean way to integrate soft source syntactic constraints into string-to-tree systems as features of the rule selection model. We release our implementation as part of Moses."
D15-1272,Joint Lemmatization and Morphological Tagging with Lemming,2015,26,44,3,0,1772,thomas muller,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We present LEMMING, a modular loglinear model that jointly models lemmatization and tagging and supports the integration of arbitrary global features. It is trainable on corpora annotated with gold standard tags and lemmata and does not rely on morphological dictionaries or analyzers. LEMMING sets the new state of the art in token-based statistical lemmatization on six languages; e.g., for Czech lemmatization, we reduce the error by 60%, from 4.05 to 1.58. We also give empirical evidence that jointly modeling morphological tags and lemmata is mutually beneficial."
W14-5709,Distinguishing Degrees of Compositionality in Compound Splitting for Statistical Machine Translation,2014,28,12,5,1,36553,marion weller,Proceedings of the First Workshop on Computational Approaches to Compound Analysis ({C}om{AC}om{A} 2014),0,"The paper presents an approach to morphological compound splitting that takes the degree of compositionality into account. We apply our approach to German noun compounds and particle verbs within a Germanxe2x80x90English SMT system, and study the effect of only splitting compositional compounds as opposed to an aggressive splitting. A qualitative study explores the translational behaviour of non-compositional compounds."
W14-3305,{C}im{S} {--} The {CIS} and {IMS} joint submission to {WMT} 2014 translating from {E}nglish into {G}erman,2014,31,4,4,0.972222,30805,fabienne cap,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"We present the CimS submissions to the 2014 Shared Task for the language pair EN!DE. We address the major problems that arise when translating into German: complex nominal and verbal morphology, productive compounding and flexible word ordering. Our morphologyaware translation systems handle word formation issues on different levels of morpho-syntactic modeling."
E14-1061,How to Produce Unseen Teddy Bears: Improved Morphological Processing of Compounds in {SMT},2014,16,14,2,0.972222,30805,fabienne cap,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Compounding in morphologically rich languages is a highly productive process which often causes SMT approaches to fail because of unseen words. We present an approach for translation into a compounding language that splits compounds into simple words for training and, due to an underspecified representation, allows for free merging of simple words into compounds after translation. In contrast to previous approaches, we use features projected from the source language to predict compound mergings. We integrate our approach into end-to-end SMT and show that many compounds matching the reference translation are produced which did not appear in the training data. Additional manual evaluations support the usefulness of generalizing compound formation in SMT."
C14-1041,Investigating the Usefulness of Generalized Word Representations in {SMT},2014,48,26,4,0.911458,3159,nadir durrani,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"We investigate the use of generalized representations (POS, morphological analysis and word clusters) in phrase-based models and the N-gram-based Operation Sequence Model (OSM). Our integration enables these models to learn richer lexical and reordering patterns, consider wider contextual information and generalize better in sparse data conditions. When interpolating generalized OSM models on the standard IWSLT and WMT tasks we observed improvements of up to 1.35 on the English-to-German task and 0.63 for the German-to-English task. Using automatically generated word classes in standard phrase-based models and the OSM models yields an average improvement of 0.80 across 8 language pairs on the IWSLT shared task."
2014.eamt-1.3,Combining bilingual terminology mining and morphological modeling for domain adaptation in {SMT},2014,-1,-1,2,1,36553,marion weller,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,None
2014.amta-researchers.21,Using noun class information to model selectional preferences for translating prepositions in {SMT},2014,-1,-1,3,1,36553,marion weller,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"Translating prepositions is a difficult and under-studied problem in SMT. We present a novel method to improve the translation of prepositions by using noun classes to model their selectional preferences. We compare three variants of noun class information: (i) classes induced from the lexical resource GermaNet or obtained from clusterings based on either (ii) window information or (iii) syntactic features. Furthermore, we experiment with PP rule generalization. While we do not significantly improve over the baseline, our results demonstrate that (i) integrating selectional preferences as rigid class annotation in the parse tree is sub-optimal, and that (ii) clusterings based on window co-occurrence are more robust than syntax-based clusters or GermaNet classes for the task of modeling selectional preferences."
W13-2213,{M}unich-{E}dinburgh-{S}tuttgart Submissions of {OSM} Systems at {WMT}13,2013,26,12,2,1,3159,nadir durrani,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"This paper describes Munich-EdinburghStuttgartxe2x80x99s submissions to the Eighth Workshop on Statistical Machine Translation. We report results of the translation tasks from German, Spanish, Czech and Russian into English and from English to German, Spanish, Czech, French and Russian. The systems described in this paper use OSM (Operation Sequence Model). We explain different pre-/post-processing steps that we carried out for different language pairs. For German-English we used constituent parsing for reordering and compound splitting as preprocessing steps. For Russian-English we transliterated the unknown words. The transliteration system is learned with the help of an unsupervised transliteration mining algorithm."
W13-2228,{QCRI}-{MES} Submission at {WMT}13: Using Transliteration Mining to Improve Statistical Machine Translation,2013,18,9,4,0.9375,3156,hassan sajjad,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"This paper describes QCRI-MESxe2x80x99s submission on the English-Russian dataset to the Eighth Workshop on Statistical Machine Translation. We generate improved word alignment of the training data by incorporating an unsupervised transliteration mining module to GIZA and build a phrase-based machine translation system. For tuning, we use a variation of PRO which provides better weights by optimizing BLEU1 at corpus-level. We transliterate out-of-vocabulary words in a postprocessing step by using a transliteration system built on the transliteration pairs extracted using an unsupervised transliteration mining system. For the Russian to English translation direction, we apply linguistically motivated pre-processing on the Russian side of the data."
W13-2230,{M}unich-{E}dinburgh-{S}tuttgart Submissions at {WMT}13: Morphological and Syntactic Processing for {SMT},2013,24,7,4,1,36553,marion weller,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We present 5 systems of the MunichEdinburgh-Stuttgart 1 joint submissions to the 2013 SMT Shared Task: FR-EN, ENFR, RU-EN, DE-EN and EN-DE. The first three systems employ inflectional generalization, while the latter two employ parser-based reordering, and DE-EN performs compound splitting. For our experiments, we use standard phrase-based Moses systems and operation sequence models (OSM)."
P13-2071,Can {M}arkov Models Over Minimal Translation Units Help Phrase-Based {SMT}?,2013,29,43,2,1,3159,nadir durrani,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"The phrase-based and N-gram-based SMT frameworks complement each other. While the former is better able to memorize, the latter provides a more principled model that captures dependencies across phrasal boundaries. Some work has been done to combine insights from these two frameworks. A recent successful attempt showed the advantage of using phrasebased search on top of an N-gram-based model. We probe this question in the reverse direction by investigating whether integrating N-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption. A large scale evaluation over 8 language pairs shows that performance does significantly improve."
P13-1058,Using subcategorization knowledge to improve case prediction for translation to {G}erman,2013,34,14,2,1,36553,marion weller,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper demonstrates the need and impact of subcategorization information for SMT. We combine (i) features on sourceside syntactic subcategorization and (ii) an external knowledge base with quantitative, dependency-based information about target-side subcategorization frames. A manual evaluation of an English-toGerman translation task shows that the subcategorization information has a positive impact on translation quality through better prediction of case."
N13-1001,"Model With Minimal Translation Units, But Decode With Phrases",2013,23,32,2,1,3159,nadir durrani,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"N-gram-based models co-exist with their phrase-based counterparts as an alternative SMT framework. Both techniques have pros and cons. While the N-gram-based framework provides a better model that captures both source and target contexts and avoids spurious phrasal segmentation, the ability to memorize and produce larger translation units gives an edge to the phrase-based systems during decoding, in terms of better search performance and superior selection of translation units. In this paper we combine N-grambased modeling with phrase-based decoding, and obtain the benefits of both approaches. Our experiments show that using this combination not only improves the search accuracy of the N-gram model but that it also improves the BLEU scores. Our system outperforms state-of-the-art phrase-based systems (Moses and Phrasal) and N-gram-based systems by a significant margin on German, French and Spanish to English translation tasks."
J13-1005,"Knowledge Sources for Constituent Parsing of {G}erman, a Morphologically Rich and Less-Configurational Language",2013,58,14,1,1,3265,alexander fraser,Computational Linguistics,0,"We study constituent parsing of German, a morphologically rich and less-configurational language. We use a probabilistic context-free grammar treebank grammar that has been adapted to the morphologically rich properties of German by markovization and special features added to its productions. We evaluate the impact of adding lexical knowledge. Then we examine both monolingual and bilingual approaches to parse reranking. Our reranking parser is the new state of the art in constituency parsing of the TIGER Treebank. We perform an analysis, concluding with lessons learned, which apply to parsing other morphologically rich and less-configurational languages."
F13-4001,Improving Translation to Morphologically Rich Languages (Am{\\'e}liorer la traduction des langages morphologiquement riches) [in {F}rench],2013,-1,-1,1,1,3265,alexander fraser,Proceedings of TALN 2013 (Volume 4: Invited Conferences),0,None
P12-1049,A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining,2012,17,27,2,1,3156,hassan sajjad,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose a novel model to automatically extract transliteration pairs from parallel corpora. Our model is efficient, language pair independent and mines transliteration pairs in a consistent fashion in both unsupervised and semi-supervised settings. We model transliteration mining as an interpolation of transliteration and non-transliteration sub-models. We evaluate on NEWS 2010 shared task data and on parallel corpora with competitive results."
E12-1068,Modeling Inflection and Word-Formation in {SMT},2012,22,22,1,1,3265,alexander fraser,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,The current state-of-the-art in statistical machine translation (SMT) suffers from issues of sparsity and inadequate modeling power when translating into morphologically rich languages. We model both inflection and word-formation for the task of translating into German. We translate from English words to an underspecified German representation and then use linear-chain CRFs to predict the fully specified German representation. We show that improved modeling of inflection and wordformation leads to improved SMT.
E12-1074,Determining the placement of {G}erman verbs in {E}nglish{--}to{--}{G}erman {SMT},2012,12,18,2,0,43176,anita gojun,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"When translating English to German, existing reordering models often cannot model the long-range reorderings needed to generate German translations with verbs in the correct position. We reorder English as a preprocessing step for English-to-German SMT. We use a sequence of hand-crafted reordering rules applied to English parse trees. The reordering rules place English verbal elements in the positions within the clause they will have in the German translation. This is a difficult problem, as German verbal elements can appear in different positions within a clause (in contrast with English verbal elements, whose positions do not vary as much). We obtain a significant improvement in translation performance."
2012.eamt-1.42,Long-distance reordering during search for hierarchical phrase-based {SMT},2012,18,9,3,1,10142,fabienne braune,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,"Long-distance reordering of syntactically divergent language pairs is a critical problem. SMT has had limited success in handling these reorderings during inference, and thus deterministic preprocessing based on reordering parse trees is used. We consider German-to-English translation using Hiero. We show how to effectively model long-distance reorderings during search. Our work is novel in that we look at reordering distances of up to 50 words, and conduct a detailed manual analysis based on a new gold standard."
P11-1044,An Algorithm for Unsupervised Transliteration Mining with an Application to Word Alignment,2011,20,14,2,1,3156,hassan sajjad,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We propose a language-independent method for the automatic extraction of transliteration pairs from parallel corpora. In contrast to previous work, our method uses no form of supervision, and does not require linguistically informed preprocessing. We conduct experiments on data sets from the NEWS 2010 shared task on transliteration mining and achieve an F-measure of up to 92%, outperforming most of the semi-supervised systems that were submitted. We also apply our method to English/Hindi and English/Arabic parallel corpora and compare the results with manually built gold standards which mark transliterated word pairs. Finally, we integrate the transliteration module into the GIZA word aligner and evaluate it on two word alignment tasks achieving improvements in both precision and recall measured against gold standard word alignments."
P11-1105,A Joint Sequence Translation Model with Integrated Reordering,2011,21,75,3,1,3159,nadir durrani,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We present a novel machine translation model which models translation by a linear sequence of operations. In contrast to the N-gram model, this sequence includes not only translation but also reordering operations. Key ideas of our model are (i) a new reordering approach which better restricts the position to which a word or phrase can be moved, and is able to handle short and long distance re-orderings in a unified way, and (ii) a joint sequence model for the translation and reordering probabilities which is more flexible than standard phrase-based MT. We observe statistically significant improvements in BLEU over Moses for German-to-English and Spanish-to-English tasks, and comparable results for a French-to-English task."
I11-1015,Comparing Two Techniques for Learning Transliteration Models Using a Parallel Corpus,2011,17,10,4,1,3156,hassan sajjad,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We compare the use of an unsupervised transliteration mining method and a rulebased method to automatically extract lists of transliteration word pairs from a parallel corpus of Hindi/Urdu. We build joint source channel models on the automatically aligned orthographic transliteration units of the automatically extracted lists of transliteration pairs resulting in two transliteration systems. We compare our systems with three transliteration systems available on the web, and show that our systems have better performance. We perform an extensive analysis of the results of using both methods and show evidence that the unsupervised transliteration mining method is superior for applications requiring high recall transliteration lists, while the rule-based method is useful for obtaining high precision lists."
W10-1734,How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for {G}erman Compound Processing,2010,13,33,2,0,45417,fabienne fritzinger,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"Compound splitting is an important problem in many Nlp applications which must be solved in order to address issues of data sparsity. Previous work has shown that linguistic approaches for German compound splitting produce a correct splitting more often, but corpus-driven approaches work best for phrase-based statistical machine translation from German to English, a worrisome contradiction. We address this situation by combining linguistic analysis with corpus-driven statistics and obtaining better results in terms of both producing splittings according to a gold standard and statistical machine translation performance."
P10-1048,{H}indi-to-{U}rdu Machine Translation through Transliteration,2010,20,40,3,1,3159,nadir durrani,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We present a novel approach to integrate transliteration into Hindi-to-Urdu statistical machine translation. We propose two probabilistic models, based on conditional and joint probability formulations, that are novel solutions to the problem. Our models consider both transliteration and translation when translating a particular Hindi word given the context whereas in previous work transliteration is only used for translating OOV (out-of-vocabulary) words. We use transliteration as a tool for disambiguation of Hindi homonyms which can be both translated or transliterated or transliterated differently based on different contexts. We obtain final BLEU scores of 19.35 (conditional probability model) and 19.00 (joint probability model) as compared to 14.30 for a baseline phrase-based system and 16.25 for a system which transliterates OOV words in the baseline system. This indicates that transliteration is useful for more than only translating OOV words for language pairs like Hindi-Urdu."
N10-1113,Bitext-Based Resolution of {G}erman Subject-Object Ambiguities,2010,14,5,2,0,45818,florian schwarck,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We present a method for disambiguating syntactic subjects from syntactic objects (a frequent ambiguity) in German sentences taken from an English-German bitext. We exploit the fact that subject and object are usually easily determined in English. We show that a simple method disambiguates some subject-object ambiguities in German, while making few errors. We view this procedure as the first step in automatically acquiring (mostly) correct labeled data. We also evaluate using it to improve a state of the art statistical parser."
C10-2010,Improved Unsupervised Sentence Alignment for Symmetrical and Asymmetrical Parallel Corpora,2010,15,50,2,1,10142,fabienne braune,Coling 2010: Posters,0,We address the problem of unsupervised and language-pair independent alignment of symmetrical and asymmetrical parallel corpora. Asymmetrical parallel corpora contain a large proportion of 1-to-0/0-to-1 and 1-to-many/many-to-1 sentence correspondences. We have developed a novel approach which is fast and allows us to achieve high accuracy in terms of F1 for the alignment of both asymmetrical and symmetrical parallel corpora. The source code of our aligner and the test sets are freely available.
W09-0420,Experiments in Morphosyntactic Processing for Translating to and from {G}erman,2009,22,21,1,1,3265,alexander fraser,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,We describe two shared task systems and associated experiments. The German to English system used reordering rules applied to parses and morphological splitting and stemming. The English to German system used an additional translation step which recreated compound words and generated morphological inflection.
E09-1033,Rich Bitext Projection Features for Parse Reranking,2009,26,8,1,1,3265,alexander fraser,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,Many different types of features have been shown to improve accuracy in parse reranking. A class of features that thus far has not been considered is based on a projection of the syntactic structure of a translation of the text to be parsed. The intuition for using this type of bitext projection feature is that ambiguous structures in one language often correspond to unambiguous structures in another. We show that reranking based on bitext projection features increases parsing accuracy significantly.
2009.mtsummit-posters.11,Word Alignment by Thresholded Two-Dimensional Normalization,2009,-1,-1,2,0,44269,hamidreza kobdani,Proceedings of Machine Translation Summit XII: Posters,0,None
J07-3002,Squibs and Discussions: Measuring Word Alignment Quality for Statistical Machine Translation,2007,23,172,1,1,3265,alexander fraser,Computational Linguistics,0,"Automatic word alignment plays a critical role in statistical machine translation. Unfortunately, the relationship between alignment quality and statistical machine translation performance has not been well understood. In the recent literature, the alignment task has frequently been decoupled from the translation task and assumptions have been made about measuring alignment quality for machine translation which, it turns out, are not justified. In particular, none of the tens of papers published over the last five years has shown that significant decreases in alignment error rate (AER) result in significant increases in translation performance. This paper explains this state of affairs and presents steps towards measuring alignment quality in a way which is predictive of statistical machine translation performance."
D07-1006,Getting the Structure Right for Word Alignment: {LEAF},2007,30,56,1,1,3265,alexander fraser,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"Word alignment is the problem of annotating parallel text with translational correspondence. Previous generative word alignment models have made structural assumptions such as the 1-to-1, 1-to-N, or phrase-based consecutive word assumptions, while previous discriminative models have either made such an assumption directly or used features derived from a generative model making one of these assumptions. We present a new generative alignment model which avoids these structural limitations, and show that it is effective when trained using both unsupervised and semi-supervised training methods."
P06-1097,Semi-Supervised Training for Statistical Word Alignment,2006,443,114,1,1,3265,alexander fraser,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus. We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality."
W05-0814,{ISI}{`}s Participation in the {R}omanian-{E}nglish Alignment Task,2005,8,13,1,1,3265,alexander fraser,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,We discuss results on the shared task of Romanian-English word alignment. The baseline technique is that of symmetrizing two word alignments automatically generated using IBM Model 4. A simple vocabulary reduction technique results in an improvement in performance. We also report on a new alignment model and a new training algorithm based on alternating maximization of likelihood with minimization of error rate.
N04-1034,Improved Machine Translation Performance via Parallel Sentence Extraction from Comparable Corpora,2004,0,77,2,0,28507,dragos munteanu,Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004,0,None
