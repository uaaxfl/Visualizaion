2010.jeptalnrecital-long.31,1997.mtsummit-papers.6,0,0.0949857,"Missing"
2011.jeptalnrecital-long.1,C96-2167,1,0.681086,"Missing"
2011.jeptalnrecital-long.20,2003.mtsummit-papers.26,0,0.0783052,"Missing"
2011.jeptalnrecital-long.20,1995.mtsummit-1.1,0,0.0237433,"Missing"
2011.jeptalnrecital-long.20,W02-1118,1,0.843749,"Missing"
C86-1133,C82-1024,0,\N,Missing
C92-2073,J88-2003,0,\N,Missing
C92-2073,J88-2004,0,\N,Missing
C92-2073,P87-1003,0,\N,Missing
C92-2073,P87-1002,0,\N,Missing
C92-2073,J88-2006,0,\N,Missing
C92-2073,P87-1001,0,\N,Missing
C92-2073,E89-1017,0,\N,Missing
C92-2073,J88-2005,0,\N,Missing
C92-2073,P87-1021,0,\N,Missing
C96-2167,W96-0404,0,0.0642797,"Missing"
enguix-etal-2014-graph,J90-1003,0,\N,Missing
enguix-etal-2014-graph,W09-2905,0,\N,Missing
enguix-etal-2014-graph,D09-1066,0,\N,Missing
enguix-etal-2014-graph,W14-0509,1,\N,Missing
enguix-etal-2014-graph,P06-2084,0,\N,Missing
F13-2012,joubert-lafourcade-2012-new,0,0.0219972,"Missing"
F13-2012,W03-2408,0,0.0664104,"Missing"
F13-2012,W06-3909,0,0.135448,"Missing"
F13-2012,J06-3003,0,\N,Missing
gala-etal-2010-tool,laureys-etal-2004-evaluation,0,\N,Missing
gala-etal-2010-tool,W08-1902,1,\N,Missing
N16-2005,W04-3205,0,0.0777785,"e constituent words extracted from Wikipedia hyperlinkhierarchy is used to further refine. Precision is improved for several reasons: relations encoding LSPs which link hyperlinks and WPT are more reliable than word pairs connected via arbitrary sentences. The features learned from the Wikipedia hyperlink-hierarchy further cleaned the word pairs extracted by LSPs. Recall is also improved since word pairs indirectly linked via their respective higher/lower order hierarchy were also extracted. 34 4 4.1 Related Works Syntactic approaches The work of (Turney, 2005, 2006; Turney and Littman, 2005; Chklovski and Pantel, 2004) is closely related to our work (CoSP-Fre) as it also relies on the use of the distribution of syntactic patterns. However, their goals, algorithms and tasks are different. The work of (Turney, 2005, 2006; and Turney and Littma, 2005) is aimed at measuring relational similarity and is applied to the classification of word pairs (ex. quart: volume vs mile: distance) while we are aimed at extracting SRs. 4.2 Hybrid approaches The work of Girju et.al (2005) is more related to our WHH-FRe in that they combined LSPs with the semantic analysis of the constituent words to disambiguate the LSPs. They"
N16-2005,S07-1028,0,0.0205044,"n the use of the distribution of syntactic patterns. However, their goals, algorithms and tasks are different. The work of (Turney, 2005, 2006; and Turney and Littma, 2005) is aimed at measuring relational similarity and is applied to the classification of word pairs (ex. quart: volume vs mile: distance) while we are aimed at extracting SRs. 4.2 Hybrid approaches The work of Girju et.al (2005) is more related to our WHH-FRe in that they combined LSPs with the semantic analysis of the constituent words to disambiguate the LSPs. They used WN to get the semantics of the constituent words. Alicia (2007) converts word pairs of the positive examples into a semantic graph mapping the pairs to the WN hypernym hierarchy. Claudio (2007) combines information from syntactic processing and semantic information of the constituent words from WN. Wikipedia-based approaches mainly focused on the identification of similarity (Nakayama et. al, 2007; Yulan et, al , 2007). Also, there is hardly any recent work concerning the extraction of meronyms. Many researchers are working on the identification of semantic similarity achieving excellent result by using standard datasets (Camacho-Collados, Taher and Navig"
N16-2005,C92-2082,0,0.501894,"umber of applications needing this kind of information. Question Answering, Information Retrieval and Text Summarization being examples in case (Turney and Littman, 2005; Girju et al., 2005). SRs extraction approaches can be categorized on the basis of the kind of information used. For example, one can rely on syntactic patterns or semantic features of the constituent words. One may as well combine these two approaches. The method using only syntactic information relies on the extraction of word-level, phrase-level, or sentence-level syntactic information. This approach has been introduced by Hearst (1992) who showed that by using a small set of lexicosyntactic patterns (LSP) one could extract with high precision hypernym noun pairs. Similar methods have been used since then by (Auger and Barriere, 2008; Marshman and L‟Homme, 2006). These authors reported results of high precision for some relations, for example hyponymy, noting poor recall which was low. Furthermore, the performance of this approach varies considerably depending on the type of relation considered (Ravichandran and Hovy, 2002, Girju et al., 2005. An alternative to the syntactic approach is a method relying on the semantics feat"
N16-2005,de-marneffe-etal-2006-generating,0,0.175419,"Missing"
N16-2005,W04-2609,0,0.0884871,"Missing"
N16-2005,P06-1040,0,0.0920979,"Missing"
N16-2005,S07-1039,0,\N,Missing
P06-1036,ferret-2006-building,1,0.672118,"t that they rely too heavily on WordNet and some of its more sophisticated features (such as the definitions associated with the synsets). While often being exploited by acquisition methods, these features are generally lacking in similar lexico-semantic networks. Moreover, these methods attempt to learn topical knowledge from a lexical network rather than topical relations. Since our goal is different, we have chosen not to rely on any significant resource, all the more as we would like our method to be applicable to a wide array of languages. In consequence, we took an incremental approach (Ferret, 2006): starting from a network of lexical co-occurrences7 collected from a large corpus, we used these latter to select potential topical relations by using a topical analyzer. 4.2 From a network of co-occurrences to a set of Topical Units We start by extracting lexical co-occurrences from a corpus to build a network. To this end we follow the method introduced by (Church and Hanks, 1990), i.e. by sliding a window of a given size over some texts. The parameters of this extraction were set in such a way as to catch the most obvious topical relations: the window was fairly large (20-words wide), and"
P06-1036,C02-1033,1,0.933224,"must contain many kinds of relations on the syntagmatic and paradigmatic axis to allow for natural and flexible access of words. Synonymy, hypernymy or meronymy fall clearly in this latter category, and well known resources like WordNet (Miller, 1995), EuroWordNet (Vossen, 1998) or MindNet (Richardson et al., 1998) contain them. However, as various researchers have pointed out (Harabagiu et al., 1999), these networks lack information, in particular with regard to syntagmatic associations, which are generally unsystematic. These latter, called TIORA (Zock and Bilac, 2004) or topical relations (Ferret, 2002) account for the fact that two words refer to the same topic, or take part in the same situation or scenario. Word-pairs like doctor–hospital, burglar–policeman or plane–airport, are examples in case. The lack of such topical relations in resources like WordNet has been dubbed as the tennis problem (Roger Chaffin, cited in Fellbaum, 1998). Some of these links have been introduced more recently in WordNet via the domain relation. Yet their number remains still very small. For instance, WordNet 2.1 does not contain any of the three associations mentioned here above, despite their high frequency."
P06-1036,W99-0501,0,0.0573433,"Missing"
P06-1036,magnini-cavaglia-2000-integrating,0,0.0305485,"eir high frequency. The lack of systematicity of these topical relations makes their extraction and typing very difficult on a large scale. This is why some researchers have proposed to use automatic learning techniques to extend lexical networks like WordNet. In (Harabagiu & Moldovan, 1998), this was done by extracting topical relations from the glosses associated to the synsets. Other researchers used external sources: Mandala et al. (1999) integrated co-occurrences and a thesaurus to WordNet for query expansion; Agirre et al. (2001) built topic signatures from texts in relation to synsets; Magnini and Cavagliá (2000) annotated the synsets with Subject Field Codes. This last idea has been taken up and extended by (Avancini et al., 2003) who expanded the domains built from this annotation. Despite the improvements, all these approaches are limited by the fact that they rely too heavily on WordNet and some of its more sophisticated features (such as the definitions associated with the synsets). While often being exploited by acquisition methods, these features are generally lacking in similar lexico-semantic networks. Moreover, these methods attempt to learn topical knowledge from a lexical network rather th"
P06-1036,P98-2180,0,0.0907252,"e some associations are too complex to be extracted automatically by machine, others are clearly within reach. We will illustrate in the next section how this can be achieved. 4 Automatic extraction of topical relations 4.1 Definition of the problem We have argued in the previous sections that dictionaries must contain many kinds of relations on the syntagmatic and paradigmatic axis to allow for natural and flexible access of words. Synonymy, hypernymy or meronymy fall clearly in this latter category, and well known resources like WordNet (Miller, 1995), EuroWordNet (Vossen, 1998) or MindNet (Richardson et al., 1998) contain them. However, as various researchers have pointed out (Harabagiu et al., 1999), these networks lack information, in particular with regard to syntagmatic associations, which are generally unsystematic. These latter, called TIORA (Zock and Bilac, 2004) or topical relations (Ferret, 2002) account for the fact that two words refer to the same topic, or take part in the same situation or scenario. Word-pairs like doctor–hospital, burglar–policeman or plane–airport, are examples in case. The lack of such topical relations in resources like WordNet has been dubbed as the tennis problem (Ro"
P06-1036,W02-1118,1,0.806365,"et al., 1999) evaluates the probability that a randomly chosen pair of words, separated by k words, is wrongly classified, i.e. they are found in the same segment by TOPICOLL, while they are actually in different ones (miss of a document break), or they are found in different segments, while they are actually in the same one (false alarm). method is an effective way of selecting topical relations by preference. 5 Discussion and conclusion We have raised and partially answered the question of how a dictionary should be indexed in order to support word access, a question initially addressed in (Zock, 2002) and (Zock and Bilac, 2004). We were particularly concerned with the language producer, as his needs (and knowledge at the onset) are quite different from the ones of the language receiver (listener/reader). It seems that, in order to achieve our goal, we need to do two things: add to an existing electronic dictionary information that people tend to associate with a word, that is, build and enrich a semantic network, and provide a tool to navigate in it. To this end we have suggested to label the links, as this would reduce the graph complexity and allow for type-based navigation. Actually our"
P06-1036,W04-2105,1,0.893887,"in the previous sections that dictionaries must contain many kinds of relations on the syntagmatic and paradigmatic axis to allow for natural and flexible access of words. Synonymy, hypernymy or meronymy fall clearly in this latter category, and well known resources like WordNet (Miller, 1995), EuroWordNet (Vossen, 1998) or MindNet (Richardson et al., 1998) contain them. However, as various researchers have pointed out (Harabagiu et al., 1999), these networks lack information, in particular with regard to syntagmatic associations, which are generally unsystematic. These latter, called TIORA (Zock and Bilac, 2004) or topical relations (Ferret, 2002) account for the fact that two words refer to the same topic, or take part in the same situation or scenario. Word-pairs like doctor–hospital, burglar–policeman or plane–airport, are examples in case. The lack of such topical relations in resources like WordNet has been dubbed as the tennis problem (Roger Chaffin, cited in Fellbaum, 1998). Some of these links have been introduced more recently in WordNet via the domain relation. Yet their number remains still very small. For instance, WordNet 2.1 does not contain any of the three associations mentioned here"
P06-1036,J90-1003,0,\N,Missing
P06-1036,E99-1013,0,\N,Missing
P06-1036,C98-2175,0,\N,Missing
W02-1118,1993.eamt-1.1,0,0.15578,"ic actor isa treat ako assistant ako actor 6 The idea according to which the mental dictionary (or encyclopedia) is basically an associative network, composed of nodes (words or concepts) and links (associations) is not new. Actually the very notion of association goes back at least to Aristotle (350 before our time), but it is also inherent in work done by philosophers (Locke, Hume) physiologists (James & Stuart Mills), psychologists (Galton, 1880 ; Freud, 1901 ; Jung & Riklin, 1906) and psycholinguists (Deese, 1965 ; Jenkins, 1970, Schvaneveldt, 1989 ). For surveys in psycholinguistics see (Hörmann, 1972 ; chapters 6-10), or more recent work (Spitzer, 1999). The notion of association is also implicit in work on semantic networks (Sowa, 1992), hypertext (Bush, 1945), the web (Nelson, 1967), connectionism (Stemberger, 1985 ; Dell, 1986) and of course WordNet (Miller, 1990, Fellbaum, 1998). health institution isa isa isa ako doctor isa ako gynecologist ako isa military hospital sanatorium asylum actor hospital synonym physician psychiatric hospital Figure 2a : Search based on propagation in a network (internal representation) 7 Of course, in case of ambiguity the user would have to signal the sp"
W02-1118,C88-2149,0,\N,Missing
W04-2105,J90-1003,0,0.155366,"to account the evolution of a society. Hence, the goal is to automatically extract associations from large corpora. This problem was addressed by a large number of researchers, but in most cases it was reduced to extraction of collocations which are a proper subset of the set of associated words. While hard to define, collocations appear often enough in corpora to be extractable by statistical and information-theory based methods. There are several basic methods for evaluating associations between words: based on frequency counts (Choueka, 1988; Wettler and Rapp, 1993), information theoretic (Church and Hanks, 1990) and statistical significance (Smadja, 1993). The statistical significance often evaluate whether two words are independant using hypothesis tests such as t-score (Church et al., 1991), the X 2 , the log-likelihood (Dunning, 1993) and Fisher’s exact test (Pedersen, 1996). Extracted sets for associated words are further pruned using numerical methods, or linguistic knowledge to obtain a subset of collocations. The various extraction measures have been discussed in great detail in the literature (Manning and Sch¨ utze, 1999; McKeown and Radev, 2000), their performance has been compared (Dunning,"
W04-2105,J93-1003,0,0.00913207,"which are a proper subset of the set of associated words. While hard to define, collocations appear often enough in corpora to be extractable by statistical and information-theory based methods. There are several basic methods for evaluating associations between words: based on frequency counts (Choueka, 1988; Wettler and Rapp, 1993), information theoretic (Church and Hanks, 1990) and statistical significance (Smadja, 1993). The statistical significance often evaluate whether two words are independant using hypothesis tests such as t-score (Church et al., 1991), the X 2 , the log-likelihood (Dunning, 1993) and Fisher’s exact test (Pedersen, 1996). Extracted sets for associated words are further pruned using numerical methods, or linguistic knowledge to obtain a subset of collocations. The various extraction measures have been discussed in great detail in the literature (Manning and Sch¨ utze, 1999; McKeown and Radev, 2000), their performance has been compared (Dunning, 1993; Pedersen, 1996; Evert and Krenn, 2001), and the methods have been combined to improve overall performance (Inkpen and Hirst, 2002). Most of these methods were originally applied in large text corpora, but more recently the"
W04-2105,P01-1025,0,0.0135456,"icance (Smadja, 1993). The statistical significance often evaluate whether two words are independant using hypothesis tests such as t-score (Church et al., 1991), the X 2 , the log-likelihood (Dunning, 1993) and Fisher’s exact test (Pedersen, 1996). Extracted sets for associated words are further pruned using numerical methods, or linguistic knowledge to obtain a subset of collocations. The various extraction measures have been discussed in great detail in the literature (Manning and Sch¨ utze, 1999; McKeown and Radev, 2000), their performance has been compared (Dunning, 1993; Pedersen, 1996; Evert and Krenn, 2001), and the methods have been combined to improve overall performance (Inkpen and Hirst, 2002). Most of these methods were originally applied in large text corpora, but more recently the web has been used as a corpus (Pearce, 2001; Inkpen and Hirst, 2002). Collocation extraction methods have been used not only for English, but for many other languages: French (Ferret, 2002), German (Evert and Krenn, 2001) and Japanese (Nagao and Mori, 1994), to cite but those. The most obvious question in this context is to clarify to what extent available collocation extraction techniques fulfill our needs of e"
W04-2105,C02-1033,0,0.0328636,"tions. The various extraction measures have been discussed in great detail in the literature (Manning and Sch¨ utze, 1999; McKeown and Radev, 2000), their performance has been compared (Dunning, 1993; Pedersen, 1996; Evert and Krenn, 2001), and the methods have been combined to improve overall performance (Inkpen and Hirst, 2002). Most of these methods were originally applied in large text corpora, but more recently the web has been used as a corpus (Pearce, 2001; Inkpen and Hirst, 2002). Collocation extraction methods have been used not only for English, but for many other languages: French (Ferret, 2002), German (Evert and Krenn, 2001) and Japanese (Nagao and Mori, 1994), to cite but those. The most obvious question in this context is to clarify to what extent available collocation extraction techniques fulfill our needs of extracting and labeling word associations. Since collocations are a subset of association, it is possible to apply collocation extraction techniques to obtain related words, ordered in terms of the relative strength of association. The result of this kind of numerical extraction would be a large set of numerically weighted word pairs. The problem with this approach is that"
W04-2105,1993.eamt-1.1,0,0.0701276,"Missing"
W04-2105,W02-0909,0,0.0226387,"dependant using hypothesis tests such as t-score (Church et al., 1991), the X 2 , the log-likelihood (Dunning, 1993) and Fisher’s exact test (Pedersen, 1996). Extracted sets for associated words are further pruned using numerical methods, or linguistic knowledge to obtain a subset of collocations. The various extraction measures have been discussed in great detail in the literature (Manning and Sch¨ utze, 1999; McKeown and Radev, 2000), their performance has been compared (Dunning, 1993; Pedersen, 1996; Evert and Krenn, 2001), and the methods have been combined to improve overall performance (Inkpen and Hirst, 2002). Most of these methods were originally applied in large text corpora, but more recently the web has been used as a corpus (Pearce, 2001; Inkpen and Hirst, 2002). Collocation extraction methods have been used not only for English, but for many other languages: French (Ferret, 2002), German (Evert and Krenn, 2001) and Japanese (Nagao and Mori, 1994), to cite but those. The most obvious question in this context is to clarify to what extent available collocation extraction techniques fulfill our needs of extracting and labeling word associations. Since collocations are a subset of association, it"
W04-2105,C94-1101,0,0.0274639,"n great detail in the literature (Manning and Sch¨ utze, 1999; McKeown and Radev, 2000), their performance has been compared (Dunning, 1993; Pedersen, 1996; Evert and Krenn, 2001), and the methods have been combined to improve overall performance (Inkpen and Hirst, 2002). Most of these methods were originally applied in large text corpora, but more recently the web has been used as a corpus (Pearce, 2001; Inkpen and Hirst, 2002). Collocation extraction methods have been used not only for English, but for many other languages: French (Ferret, 2002), German (Evert and Krenn, 2001) and Japanese (Nagao and Mori, 1994), to cite but those. The most obvious question in this context is to clarify to what extent available collocation extraction techniques fulfill our needs of extracting and labeling word associations. Since collocations are a subset of association, it is possible to apply collocation extraction techniques to obtain related words, ordered in terms of the relative strength of association. The result of this kind of numerical extraction would be a large set of numerically weighted word pairs. The problem with this approach is that the links are only labeled in terms of their relative associative s"
W04-2105,J93-1007,0,0.0265396,"is to automatically extract associations from large corpora. This problem was addressed by a large number of researchers, but in most cases it was reduced to extraction of collocations which are a proper subset of the set of associated words. While hard to define, collocations appear often enough in corpora to be extractable by statistical and information-theory based methods. There are several basic methods for evaluating associations between words: based on frequency counts (Choueka, 1988; Wettler and Rapp, 1993), information theoretic (Church and Hanks, 1990) and statistical significance (Smadja, 1993). The statistical significance often evaluate whether two words are independant using hypothesis tests such as t-score (Church et al., 1991), the X 2 , the log-likelihood (Dunning, 1993) and Fisher’s exact test (Pedersen, 1996). Extracted sets for associated words are further pruned using numerical methods, or linguistic knowledge to obtain a subset of collocations. The various extraction measures have been discussed in great detail in the literature (Manning and Sch¨ utze, 1999; McKeown and Radev, 2000), their performance has been compared (Dunning, 1993; Pedersen, 1996; Evert and Krenn, 2001"
W04-2105,C02-1038,0,0.0220182,"the notion of association strength are inadequate for the kind of navigation described here above. Hence another step is necessary: qualification of the links according to their types. Only once this is done, a human being could use it to navigate through a large conceptual-lexical network (the dictionary) as described above. Unfortunately, research on automatic link identification has been rather sparse. Most attempts have been devoted to the extraction of certain types of links (usually syntactic type (Lin, 1998) or on extensions of WordNet with topical information contained in a thesaurus (Stevenson, 2002) or on the WWW (Agirre et al., 2000). Additional methods need to be considered in order to reveal (automatically) the kind of associations holding between words and/or concepts. Earlier in this paper we have suggested the use of an encyclopedia as a source of general world knowledge. It should be noted, though, that there are important differences between large corpora and encyclopedias. Large corpora usually contain a lot of repetitive texts on a limited number of topics (e.g. newspaper articles) which makes them very suitable for statistical methods. On the other hand, while being maximally"
W04-2105,W93-0310,0,0.0390714,"mpossible to repeat these experiments to take into account the evolution of a society. Hence, the goal is to automatically extract associations from large corpora. This problem was addressed by a large number of researchers, but in most cases it was reduced to extraction of collocations which are a proper subset of the set of associated words. While hard to define, collocations appear often enough in corpora to be extractable by statistical and information-theory based methods. There are several basic methods for evaluating associations between words: based on frequency counts (Choueka, 1988; Wettler and Rapp, 1993), information theoretic (Church and Hanks, 1990) and statistical significance (Smadja, 1993). The statistical significance often evaluate whether two words are independant using hypothesis tests such as t-score (Church et al., 1991), the X 2 , the log-likelihood (Dunning, 1993) and Fisher’s exact test (Pedersen, 1996). Extracted sets for associated words are further pruned using numerical methods, or linguistic knowledge to obtain a subset of collocations. The various extraction measures have been discussed in great detail in the literature (Manning and Sch¨ utze, 1999; McKeown and Radev, 2000"
W04-2105,W02-1118,1,\N,Missing
W08-1902,dutoit-nugues-2002-algorithm,0,0.643129,"onary (WordNet) and an encyclopedia (Wikipedia) (http://www.onelook.com/reverse-dictionary.shtml). 11 idea or concept,9 and the system will display all connected words. If the user can find the item he is looking for in this list, search stops, otherwise it will continue, the user giving other words of the list, or words evoked by them. Of course, remains the question of how to build this resource, in particular, how to populate the axis devoted to the trigger words, i.e. accesskeys. At present we consider three approaches: one, where we use the words occurring in word definitions (see also, (Dutoit and Nugues, 2002; Bilac et al., 2004)), the other is to mine a wellbalanced corpus, to find co-occurrences within a given window (Ferret and Zock, 2006), the size depending a bit on the text type (encyclopedia) or type of corpus. Still another solution would be to draw on the association lists produced by psychologists, see for example http://www.usf.edu/, or http://www.eat.rl.ac.uk. Of course, the idea of using matrices in linguistics is not new. There are at least two authors who have proposed its use: M. Gross (Gross, 1984) used it for coding the syntactic behavior of lexical items, hence the term lexicon-"
W08-1902,P06-1036,1,0.883456,"ll display all connected words. If the user can find the item he is looking for in this list, search stops, otherwise it will continue, the user giving other words of the list, or words evoked by them. Of course, remains the question of how to build this resource, in particular, how to populate the axis devoted to the trigger words, i.e. accesskeys. At present we consider three approaches: one, where we use the words occurring in word definitions (see also, (Dutoit and Nugues, 2002; Bilac et al., 2004)), the other is to mine a wellbalanced corpus, to find co-occurrences within a given window (Ferret and Zock, 2006), the size depending a bit on the text type (encyclopedia) or type of corpus. Still another solution would be to draw on the association lists produced by psychologists, see for example http://www.usf.edu/, or http://www.eat.rl.ac.uk. Of course, the idea of using matrices in linguistics is not new. There are at least two authors who have proposed its use: M. Gross (Gross, 1984) used it for coding the syntactic behavior of lexical items, hence the term lexicon-grammar, and G. Miller, the father of WN (Miller et al., 1990) suggested it to support lexical access. While the former work is not rele"
W08-1902,P98-2180,0,0.117161,"Missing"
W08-1902,P84-1058,0,0.290431,"s: one, where we use the words occurring in word definitions (see also, (Dutoit and Nugues, 2002; Bilac et al., 2004)), the other is to mine a wellbalanced corpus, to find co-occurrences within a given window (Ferret and Zock, 2006), the size depending a bit on the text type (encyclopedia) or type of corpus. Still another solution would be to draw on the association lists produced by psychologists, see for example http://www.usf.edu/, or http://www.eat.rl.ac.uk. Of course, the idea of using matrices in linguistics is not new. There are at least two authors who have proposed its use: M. Gross (Gross, 1984) used it for coding the syntactic behavior of lexical items, hence the term lexicon-grammar, and G. Miller, the father of WN (Miller et al., 1990) suggested it to support lexical access. While the former work is not relevant for us here, Miller’s proposal is. What are the differences between his proposal and ours? There are basically four main differences: chologists have gathered in their association experiments (Jung and Riklin, 1906; Deese, 1965; Schvaneveldt, 1989). Note, that instead of putting a boolean value at the intersection of the tw and the aw , we will put weights and the type of"
W08-1902,C96-1002,0,0.0501127,"different for the speaker and listener, even if both of them draw on the same resource. When speaking or writing we encounter basically either of the following two situations: one where everything works automatically, somehow like magic, words popping up one after another c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 9 Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 9–17 Manchester, August 2008 cording to Levelt (Levelt, 1996) the generation of words (synthesis) involves the following stages: conceptual preparation, lexical selection, phonological- and phonetic encoding, articulation. Bear in mind that having performed ’lexical selection’ does not imply access to the phonetic form (see the experiments on the tip-of-the-tongue phenomenon). Neither do we know how to represent them. Yet, there are ways around this problem as we will show. Whether concepts and words are organized and accessed differently is a question we cannot answer here. We can agree though on the fact that getting information concerning words is fa"
W08-1902,C00-1072,0,0.0776676,"s on underspecified, i.e. imperfect input. To achieve this goal we’ve started building an AM composed of form elements (the words and expressions of a given language) and aws . The role of the latter being to lead to or to evoke the tw . In the last part we’ve described briefly the results obtained by comparing two resources (WN and Wikipedia) and various inputs. Given the fact that the project is still quite young, only preliminary results can be shown at this point. Our next steps will be to take a closer look at the following work: clustering of similar words (Lin, 1998), topic signatures (Lin and Hovy, 2000) and Kilgariff’s sketch engine (Kilgarriff et al., 2004). We plan also to add other lexical functions to enrich our database with aws . We plan to experiment with corpora, trying to find out which ones are best for our purpose15 and we will certainly experiment with the window size 16 to see which size is best for which text type. Finally, we plan to insert in our AM the relations holding between the aw and the tw . As these links are contained in our corpus, we should be able to identify and type them. The question is, to what extent this can be done automatically. Obviously, the success of o"
W08-1902,P98-2127,0,0.0700144,"ical access based 15 References on underspecified, i.e. imperfect input. To achieve this goal we’ve started building an AM composed of form elements (the words and expressions of a given language) and aws . The role of the latter being to lead to or to evoke the tw . In the last part we’ve described briefly the results obtained by comparing two resources (WN and Wikipedia) and various inputs. Given the fact that the project is still quite young, only preliminary results can be shown at this point. Our next steps will be to take a closer look at the following work: clustering of similar words (Lin, 1998), topic signatures (Lin and Hovy, 2000) and Kilgariff’s sketch engine (Kilgarriff et al., 2004). We plan also to add other lexical functions to enrich our database with aws . We plan to experiment with corpora, trying to find out which ones are best for our purpose15 and we will certainly experiment with the window size 16 to see which size is best for which text type. Finally, we plan to insert in our AM the relations holding between the aw and the tw . As these links are contained in our corpus, we should be able to identify and type them. The question is, to what extent this can be done aut"
W08-1902,W04-2105,1,0.746284,"less for this kind of corpus: (a) size: though, all words are tagged, the corpus remains small as it contains only 63.941 different words; (b) in consequence, the corpus lacks many syntagmatic associations encoding encyclopedic knowledge. 12 http://www.wikipedia.org The optimal window-size depends probably on the text type (encyclopedia vs. unformatted text). Yet, in the absence of clear criteria, we consider the optimal window-size as an open, empirical question. 14 This latter aspect is not implemented yet, but will be added in the future, as it is a necessary component for easy navigation (Zock and Bilac, 2004; Zock, 2006; Zock, 2007). 13 14 5.3.3 Corpus Building Input We start arbitrarily from some page (for our experiment, we have chosen ”wine” as input), apply the algorithm outlined here above and pick then randomly a noun within this page to fetch with this input a new page on Wikipedia. This process is repeated until a given sample size is obtained (in our case 1000 pages). Of course, instead of picking randomly a noun, we could have decided to process all the nouns of a given page, and to add then incrementally the nouns of the next pages. Yet, doing this would have led us to privilege a spec"
W08-1902,C98-2175,0,\N,Missing
W08-1902,W06-1007,0,\N,Missing
W08-1902,C98-2122,0,\N,Missing
W08-1911,P07-1044,0,0.194511,"is more important to her. For example, (Ferret and Zock, 2006) have proposed to present results from a dictionary enriched with topical associations in chunks to allow for categorial search. There will be cases where the user may find acceptable only grammatical results, while in other cases the user might accept agrammatical results provided they contain interesting suggestions. Moreover, it seems extremely important that result ranking can take into account the phrase substitution into the original context. s(p2 , p1 , C) = X λm hm (p1 , p2 , C) (2) m∈M 3.1 Control over fluency As noted by (Mutton et al., 2007), the notion of sentence-level fluency is not uniformely agreed upon, and its evaluation by human judges is sometimes found subjective, but in practice judges can obtain high levels of agreement about what can be considered fluent or not. Like (Callison-Burch, 2007), we can use a language model (L M) to assess the local fluency of a sentence after a phrase has been substituted with a rephrasing. A degradation in score (with a fluent original sentence) can indicate that the rephrasing segment should be adapted to the sentence, and/or that the sentence itself should be modified in order to integ"
W08-1911,P05-1074,0,0.579537,"ne raison → je suis d’accord avec vous), in specific grammatical contexts (e.g. pouvoir accueillir dans de bonnes conditions les pays → comme il se doit) and/or pragmatic contexts (e.g. c’est un bon d´ebut → nous partons du bon pied). tents of phrases, and the fact that their translations are not conditioned on their context. If phrase extraction is performed in two opposite directions, then it is possible to find the possible translations of a given phrase (and their conditional probabilities), and then to translate back those phrases into the original language. In this approach proposed by (Bannard and Callison-Burch, 2005), the second language acts as a pivot, as illustrated on figure 1. Because of the nature of the possible alignments, this pivot can represent various senses, which in context can be equivalent or comparable to that of the original phrase. In turn, the same phenomena can take place when translating back from the pivot phrases to the original language, and the resulting rephrasings can be equivalent or comparable in meaning to that of the original phrase in some context, may also be incomplete and/or require other changes in the rephrased sentence. Bannard and Callison-Burch have defined a parap"
W08-1911,J03-1002,0,0.0110014,"arcu, 2006)). In order to extract a comprehensive phrase lexicon, a very large number of sentences should be compared to extract potential rephrasings, which furthermore may often correspond to phrases that are too remotely connected. Parallel corpora provide the interesting advantage that it is reasonable to assume that elements from one side of the corpus should be aligned to elements on the other side, and that associations of elements can be reinforced by the number of times they occur in the corpus. Various approaches for word alignment from parallel corpora have been proposed (see e.g. (Och and Ney, 2003)), and the phrase-based approach to Statistical Machine Translation (Koehn et al., 2003) has led to the development of heuristics for obtaining alignments between phrases of any number of words. • The replacing element may require syntactic changes of the matrix, i.e. the text in which it is embedded. This occurs if the source word and the target word have different syntactic requirements, and this can be seen as a good reason to replace entire sentences, or at least sentence fragments. This assumes a pattern dictionary, where patterns achieving the same conceptual goal are grouped together. I"
W08-1911,N03-1003,0,0.231212,"d the relevant lexicon entries. In the latter case, the similarity used to select examples could take the context of the phrases into account in terms of dependency relationships. ing of take his place, due to the possessive determiner. The actual meaning of words depends on the context in which they are used. The work done by the team of Gross on lexicon-grammar (e.g. (Gross, 1984)) showed that a relatively small set of clause patterns and syntactic constraints suffices to cover most of common French. Comparable monolingual corpora have been used for automatic paraphrasing. Barzilay and Lee (Barzilay and Lee, 2003) learned paraphrasing patterns as pairs of word lattices, which are then used to produce sentence level paraphrases. Their corpus contained news agency articles on the same events, which allows precise sentence paraphrasing, but on a small sets of phenomena and for a limited domain. As sentential paraphrasing is more likely to alter meaning, Quirk et al. (Quirk et al., 2004) approached paraphrasing as a monotonous decoding by a phrase-based SMT system. Their corpus consisted of monolingual sentences extracted from a comparable corpus that were automatically aligned so as to allow aligned phras"
W08-1911,N03-1024,0,0.136956,"rns as pairs of word lattices, which are then used to produce sentence level paraphrases. Their corpus contained news agency articles on the same events, which allows precise sentence paraphrasing, but on a small sets of phenomena and for a limited domain. As sentential paraphrasing is more likely to alter meaning, Quirk et al. (Quirk et al., 2004) approached paraphrasing as a monotonous decoding by a phrase-based SMT system. Their corpus consisted of monolingual sentences extracted from a comparable corpus that were automatically aligned so as to allow aligned phrase extraction. Pang et al. (Pang et al., 2003) used parallel monolingual corpora built from news stories that had been independantly translated several times to learn lattices from a syntax-based alignment process. Bannard and Callison-Burch (Bannard and Callison-Burch, 2005) proposed to use pivot translation for paraphrasing phrases. Fujita (Fujita, 2005) proposed a transfer-and-revision framework using linguistic knowledge for generating paraphrases in Japanese and a model for error detection. At the lexical level, a recent evaluation on English lexical substitution was held (McCarthy and Navigli, 2007) in which systems had to find lexi"
W08-1911,W04-3219,0,0.164899,"984)) showed that a relatively small set of clause patterns and syntactic constraints suffices to cover most of common French. Comparable monolingual corpora have been used for automatic paraphrasing. Barzilay and Lee (Barzilay and Lee, 2003) learned paraphrasing patterns as pairs of word lattices, which are then used to produce sentence level paraphrases. Their corpus contained news agency articles on the same events, which allows precise sentence paraphrasing, but on a small sets of phenomena and for a limited domain. As sentential paraphrasing is more likely to alter meaning, Quirk et al. (Quirk et al., 2004) approached paraphrasing as a monotonous decoding by a phrase-based SMT system. Their corpus consisted of monolingual sentences extracted from a comparable corpus that were automatically aligned so as to allow aligned phrase extraction. Pang et al. (Pang et al., 2003) used parallel monolingual corpora built from news stories that had been independantly translated several times to learn lattices from a syntax-based alignment process. Bannard and Callison-Burch (Bannard and Callison-Burch, 2005) proposed to use pivot translation for paraphrasing phrases. Fujita (Fujita, 2005) proposed a transfer"
W08-1911,2007.tmi-papers.28,0,0.0292629,"Missing"
W08-1911,W04-2105,1,0.828682,"ries have called for, candidate, the, etc.) Different target phrases associated with a given source phrase can either represent paraphrases or phrases with different meanings. Among the limitations of this type of phrasal alignments are their inability to model non-consecutive words and to generalize the conLexicon of phrase rephrasings Dictionaries and semantic resources such as thesauri can be used to find words by following links of different kinds from a given entry point. WordNet (Fellbaum, 1998) is one such resource. For a proposal of other kinds of links and navigational aids see also (Zock and Bilac, 2004; Zock, 2006; Zock, 2007). Words are the traditional units that people expect to find in dictionaries. Whereas some types of dictionaries can contain multiword expressions, such as compound nouns and terms, those correspond to linguistically-motivated units. In order to rephrase phrases of any type with a dictionary, a writer may have to look up several words, combine various information and validate the result using her experience of the language or throught the use of a concordancer. Moreover, dictionary lookups 78 in all contexts (e.g. je vous donne raison → je suis d’accord avec vous), in"
W08-1911,P06-1036,1,0.837029,"sing English as pivot. puted, where M is the set of models used, hm is the logarithm of the normalized score of a model P and λm its weight (with m∈M λm = 1), and C is the original sentence and the placeholder for the rephrased phrase. to discard some of them. The proposed ranking should reflect as best as possible the preferences of the user for the task at hand in order to minimize reading time and maintain the user’s interest in using the phrase lexicon. It is essential to give the user some control over how the results are returned depending on what is more important to her. For example, (Ferret and Zock, 2006) have proposed to present results from a dictionary enriched with topical associations in chunks to allow for categorial search. There will be cases where the user may find acceptable only grammatical results, while in other cases the user might accept agrammatical results provided they contain interesting suggestions. Moreover, it seems extremely important that result ranking can take into account the phrase substitution into the original context. s(p2 , p1 , C) = X λm hm (p1 , p2 , C) (2) m∈M 3.1 Control over fluency As noted by (Mutton et al., 2007), the notion of sentence-level fluency is"
W08-1911,P84-1058,0,0.487594,"s proposed bilingual phrase lexicon could include rephrasing memory features to learn from interaction with the user, and concordancing features to display the context of use in the bilingual corpus of the segments used to build the relevant lexicon entries. In the latter case, the similarity used to select examples could take the context of the phrases into account in terms of dependency relationships. ing of take his place, due to the possessive determiner. The actual meaning of words depends on the context in which they are used. The work done by the team of Gross on lexicon-grammar (e.g. (Gross, 1984)) showed that a relatively small set of clause patterns and syntactic constraints suffices to cover most of common French. Comparable monolingual corpora have been used for automatic paraphrasing. Barzilay and Lee (Barzilay and Lee, 2003) learned paraphrasing patterns as pairs of word lattices, which are then used to produce sentence level paraphrases. Their corpus contained news agency articles on the same events, which allows precise sentence paraphrasing, but on a small sets of phenomena and for a limited domain. As sentential paraphrasing is more likely to alter meaning, Quirk et al. (Quir"
W08-1911,N03-1017,0,0.0204928,"Missing"
W08-1911,2005.mtsummit-papers.11,0,0.0140819,"e and its rephrasing, we use a model (L EM) that returns a proportion of lemmas for full words that only belong to a rephrasing over all such lemmas for an initial phrase and its rephrasing (see (Max, 2008)). 4 Experiments and evaluation We carried out an evaluation on the local rephrasing of French sentences, using English as the pivot language.2 We extracted phrase alignments of up to 7 word forms using the Giza++ alignment tool (Och and Ney, 2003) and the grow-diag-final-and heuristics described in (Koehn et al., 2003) on 948,507 sentences of the French-English part of the Europarl corpus (Koehn, 2005) and obtained some 42 million phrase pairs for which probabilities were estimated using maximum likelihood estimation. Statistics for the extracted lexicons are reported on figure 2. Entries of the monolingual phrase lexicon are built dynamically from the entries of the monolingual lexicons. For the L M model, we used a 5-gram language model trained on the French part of the corpus using Kneser-Ney smoothing. The robust parser for French S YNTEX (Bourigault et al., 2005) was used to obtain lemmas for word and labeled dependency relationships between words, used respectively for the L EM and D"
W08-1911,S07-1009,0,0.0349439,"ow aligned phrase extraction. Pang et al. (Pang et al., 2003) used parallel monolingual corpora built from news stories that had been independantly translated several times to learn lattices from a syntax-based alignment process. Bannard and Callison-Burch (Bannard and Callison-Burch, 2005) proposed to use pivot translation for paraphrasing phrases. Fujita (Fujita, 2005) proposed a transfer-and-revision framework using linguistic knowledge for generating paraphrases in Japanese and a model for error detection. At the lexical level, a recent evaluation on English lexical substitution was held (McCarthy and Navigli, 2007) in which systems had to find lexical synonyms and disambiguate the context. 6 There are several open issues to the presented work. Important issues are where the phrases can come from and the bias introduced by the resource used. Using a bilingual corpora such as the Europarl corpus with this pivot approach yields both generic and domain/genre-specific rephrasings, and it is important to be able to determine their appropriate context of use. It would also be interesting to investigate enriching this framework with phrases learnt from monolingual corpora from a given domain or genre, and to us"
W08-1911,P06-1011,0,0.0249205,"that a choice is appropriate for a given context, which can be quite difficult, for example when writing in a second language. by another does not have any consequences overall. This is often the case when a word is replaced by its synonym or a similar word. • An entire expression or sentence is replaced by its equivalent. In this case the problem is generally to obtain a good fit with regard to the surrounding text, the replacing unit being well-formed by definition. One way of obtaining phrase rephrasings is by looking at phrases that occur in similar contexts in a monolingual corpus (e.g. (Munteanu and Marcu, 2006)). In order to extract a comprehensive phrase lexicon, a very large number of sentences should be compared to extract potential rephrasings, which furthermore may often correspond to phrases that are too remotely connected. Parallel corpora provide the interesting advantage that it is reasonable to assume that elements from one side of the corpus should be aligned to elements on the other side, and that associations of elements can be reinforced by the number of times they occur in the corpus. Various approaches for word alignment from parallel corpora have been proposed (see e.g. (Och and Ney"
W10-3411,C00-1072,0,0.0107701,"tained in the material we use, since our resource will be based on this data. Hence, taking as corpus only the newspapers read by an elite (say, Le Monde, in France), will surely not sufﬁce to capture the information we need, as it will not relate information ordinary citizens, say sport fans, are familiar with or interested in. In sum, we need to take a wide variety of sources to extract then the needed information. While there is shortage of some document types needed, there are nevertheless quite a few sources one may consider to begin with: Wikipedia, domain taxonomies, topic signatures, (Lin and Hovy, 2000), a database like (http://openrdf.org), etc. 2.2 Building the resource There are two elements we need to get a clearer picture of: the nature of the resource (semantic map), and the search method i.e. the way to explore it. Concerning the resource, there are many possible sources (dictionary, thesaurus, corpora, or a mix of all this) and many ways of building it. Since our main goal is the building of an index based on the notion of word relations (triples composed of two terms and a link), the two prime candidates are of course corpora and association lists like the ones collected by psycholo"
W10-3411,P06-1036,1,0.802579,"m may be due to the distance between the source and the target word: the link may be mediated. • Topic sensitivity Weights are important, but they tend to change dynamically with time and the topic. Think of the word ’piano’ uttered in the contexts of a ’concert’ or ’household moving’. It is only in this latter case that this term evokes ideas like size or weight. The dynamic recompution of weights as a function of topic changes requires that the system be able to recognize the topic changes, as otherwise it might mislead the user by providing of inadequate weights. For some initial work see (Ferret and Zock, 2006). Association lists: Psychologists have built such lists already decades ago (Deese, 1965; Schvaneveldt, 1989). Similar lists are nowadays freely available on the web. For example, for English there is the Edinburgh Associative Thesaurus 6 and the compilation done by Nelson and his colleagues in Florida 7 . There are also some re6 7 77 http://www.eat.rl.ac.uk/ http://cyber.acomp.usf.edu/FreeAssociation/ sources for German (see 8 or 9 ), for Japanese,10 and probably many other languages. While association lists are generally built manually, one can also try to do so automatically or with the he"
W10-3411,W04-2105,1,0.525696,"ts have built such lists already decades ago (Deese, 1965; Schvaneveldt, 1989). Similar lists are nowadays freely available on the web. For example, for English there is the Edinburgh Associative Thesaurus 6 and the compilation done by Nelson and his colleagues in Florida 7 . There are also some re6 7 77 http://www.eat.rl.ac.uk/ http://cyber.acomp.usf.edu/FreeAssociation/ sources for German (see 8 or 9 ), for Japanese,10 and probably many other languages. While association lists are generally built manually, one can also try to do so automatically or with the help of people (see section 5 in (Zock and Bilac, 2004)). JeuxdeMot (JdM), a collectively built resource focusing on French being an example in case.11 2.3 Searching The goal of searching is more complex than one might think. Of course, ultimately one should ﬁnd the object one is looking for,12 but the very process should also be carried out quickly and naturally. In addition we want to allow for recovery in case of having taken the wrong turn, and we want to avoid looping, that is, walking in circles, without ever getting closer to the goal. Last, but not least we want to make sure that stored information can also be accessed. That this is less o"
W10-3411,W08-1902,1,0.848952,"on French being an example in case.11 2.3 Searching The goal of searching is more complex than one might think. Of course, ultimately one should ﬁnd the object one is looking for,12 but the very process should also be carried out quickly and naturally. In addition we want to allow for recovery in case of having taken the wrong turn, and we want to avoid looping, that is, walking in circles, without ever getting closer to the goal. Last, but not least we want to make sure that stored information can also be accessed. That this is less obvious than it might seem at ﬁrst sight has been shown by (Zock and Schwab, 2008). Taking two resources (WN and Wikipedia) that contain both a given target word, we wanted to see whether we could access it or not. The target word was vintage. In order to ﬁnd it we provided two access keys, i.e. trigger words: wine and harvest. Combining the two produced a list of 6 items in the case of WN and 45 in the case of Wikipedia, yet, while the latter displayed the target word, it was absent from the list produced by WN. This example illustrates the fact that our claim concerning storage and acess is well founded. Having stored something does by no means guarantee its access."
W10-3908,P07-1018,0,0.0170797,"ranslated equiThere is also the approach of identifying orthographically similar words (Koehn & Knight, 2002) which does not even require a corpus as simple word lists will suffice. However, this approach is promising only for closely related languages but appears to have limited scope otherwise. For this reason we will not further discuss it here. 2 51 valents should also co-occur more frequently than expected in a corpus of language B. A great number of variants of this approach has been proposed, e.g. emphasizing aspects of corpus selection or expanding it to collocations or short phrases (Babych et al., 2007). What is common to these studies is that they consider the source and the target language as two distinct semantic spaces, without any links at the beginning. Therefore, in order to connect the two, a base dictionary is required, and the purpose of the system is to expand this base dictionary. Building a dictionary from scratch is not possible this way or at least computationally unfeasible (see Rapp, 1995). Whether the assumption of two completely distinct semantic spaces is realistic remains an open issue. Are separate lexical networks really a reasonable model for the processing of differe"
W10-3908,J93-1003,0,0.36481,"items. The stop words had been manually selected from a corpusderived list of high frequency words. In the resulting corpus associations between words need to be identified, something that is usually done on the basis of co-occurrences. In Note that the results of both directions may be combined. This is something we leave for future work. 3 order to count the co-occurrences between pairs of words, a text window comprising the ten words preceding and following a given foreign word is considered. On the resulting co-occurrence counts a standard association metric like the log-likelihood ratio (Dunning, 1993) is applied. Note that the above mentioned window size of ±10 words from the given word relates to the preprocessed corpus from which function words have already been removed. Since in English roughly every second word tends to be a function word, the effective window size is about ±20 words. This window size is somewhat larger than what we typically find in other studies. However, the reason for this is quite obvious: As citations of foreign words are rare, we have a severe problem of data sparseness, and by looking at a relatively large window we try to somewhat compensate for this.4 Despite"
W10-3908,W97-0119,0,0.381381,"he sentences translated by a machine tend to be garbled and of lower quality. However, the big problem with this approach is to ensure that the retrieved sentence pairs are indeed translations of each other. While there is no perfect solution to this problem, several studies have shown that such data can be useful for building or supplementing translation models in SMT (see e. g. Munteanu & Marcu, 2005; Wu & Fung, 2005). Another approach for exploiting comparable corpora in dictionary generation is based on the observation that word co-occurrence patterns between languages tend to be similar (Fung & McKeown, 1997; Rapp, 1995; Chiao et al., 2004). If, for example, two words X and Y cooccur more often than expected by chance in a corpus of language A, then their translated equiThere is also the approach of identifying orthographically similar words (Koehn & Knight, 2002) which does not even require a corpus as simple word lists will suffice. However, this approach is promising only for closely related languages but appears to have limited scope otherwise. For this reason we will not further discuss it here. 2 51 valents should also co-occur more frequently than expected in a corpus of language B. A grea"
W10-3908,P98-1069,0,0.592771,"Missing"
W10-3908,2005.mtsummit-papers.11,0,0.041683,"can happens to also belong to English, meaning something completely different. Moreover, can is a high frequency word, occurring millions of times in a large corpus. Of course, if we had a perfect word sense disambiguator, we could separate the Catalan and the English occurrences of can, thereby solving the problem. 7 Unfortunately, existing tools are not powerful enough to do the job. What is worse, such collisions are not Which, for example, by using open source tools such as Moses and Giza++ (see www.statmt.org) can be easily generated from parallel corpora, e.g. from the Europarl corpus (Koehn, 2005) or the JRC Acquis corpus (Steinberger et al., 2006). 7 If we assume that foreign words typically occur in clusters, we could also use language identification software. 6 uncommon between languages using the same script. So what can we do? Our suggestion is exactly the same as above for the problem of data sparseness, i.e. to look at several source languages in parallel. But it is clear that collapsing all source words into a single item does not work. If only one of them happens to be also a common word in the target language, it is very likely that its co-occurrences will override the co-occ"
W10-3908,J05-4003,0,0.108929,"large corpus of the target language for sentences similar to the translations. The advantage of this procedure is that the sentences retrieved this way are correct sentences as they were produced by humans, whereas the sentences translated by a machine tend to be garbled and of lower quality. However, the big problem with this approach is to ensure that the retrieved sentence pairs are indeed translations of each other. While there is no perfect solution to this problem, several studies have shown that such data can be useful for building or supplementing translation models in SMT (see e. g. Munteanu & Marcu, 2005; Wu & Fung, 2005). Another approach for exploiting comparable corpora in dictionary generation is based on the observation that word co-occurrence patterns between languages tend to be similar (Fung & McKeown, 1997; Rapp, 1995; Chiao et al., 2004). If, for example, two words X and Y cooccur more often than expected by chance in a corpus of language A, then their translated equiThere is also the approach of identifying orthographically similar words (Koehn & Knight, 2002) which does not even require a corpus as simple word lists will suffice. However, this approach is promising only for closel"
W10-3908,P95-1050,1,0.552456,"d by a machine tend to be garbled and of lower quality. However, the big problem with this approach is to ensure that the retrieved sentence pairs are indeed translations of each other. While there is no perfect solution to this problem, several studies have shown that such data can be useful for building or supplementing translation models in SMT (see e. g. Munteanu & Marcu, 2005; Wu & Fung, 2005). Another approach for exploiting comparable corpora in dictionary generation is based on the observation that word co-occurrence patterns between languages tend to be similar (Fung & McKeown, 1997; Rapp, 1995; Chiao et al., 2004). If, for example, two words X and Y cooccur more often than expected by chance in a corpus of language A, then their translated equiThere is also the approach of identifying orthographically similar words (Koehn & Knight, 2002) which does not even require a corpus as simple word lists will suffice. However, this approach is promising only for closely related languages but appears to have limited scope otherwise. For this reason we will not further discuss it here. 2 51 valents should also co-occur more frequently than expected in a corpus of language B. A great number of"
W10-3908,P99-1067,1,0.813579,"usible model, assuming a person lived for some years in one country, and then for some more years in another country, assuming further that this person never looked at a dictionary or another multilingual document and never communicated with a person mixing both languages. It is known that this can work. The reason is probably the following: Many words of the basic dictionary assumed above correspond to items of the physical world. These items generally have names in natural languages which can serve as mediators. That the extrapolation to more abstract notions is possible has been claimed by Rapp (1999). Still, although persons proceeding this way can easily understand and, after some years, even think in each of the two languages, experience shows that they tend to have some difficulties when making translations, especially literal translations. So, although the above scenario is possible, we do not think that it is a typical one for our modern times. There are certainly good reasons why there are so many language courses, and why there is such an abundance of dictionaries. It is a matter of commonsense that the person trying to acquire a new language will look at a multilingual dictionary."
W10-3908,steinberger-etal-2006-jrc,0,0.0667499,"meaning something completely different. Moreover, can is a high frequency word, occurring millions of times in a large corpus. Of course, if we had a perfect word sense disambiguator, we could separate the Catalan and the English occurrences of can, thereby solving the problem. 7 Unfortunately, existing tools are not powerful enough to do the job. What is worse, such collisions are not Which, for example, by using open source tools such as Moses and Giza++ (see www.statmt.org) can be easily generated from parallel corpora, e.g. from the Europarl corpus (Koehn, 2005) or the JRC Acquis corpus (Steinberger et al., 2006). 7 If we assume that foreign words typically occur in clusters, we could also use language identification software. 6 uncommon between languages using the same script. So what can we do? Our suggestion is exactly the same as above for the problem of data sparseness, i.e. to look at several source languages in parallel. But it is clear that collapsing all source words into a single item does not work. If only one of them happens to be also a common word in the target language, it is very likely that its co-occurrences will override the co-occurrences of the foreign words we are interested in."
W10-3908,I05-1023,0,0.280422,"get language for sentences similar to the translations. The advantage of this procedure is that the sentences retrieved this way are correct sentences as they were produced by humans, whereas the sentences translated by a machine tend to be garbled and of lower quality. However, the big problem with this approach is to ensure that the retrieved sentence pairs are indeed translations of each other. While there is no perfect solution to this problem, several studies have shown that such data can be useful for building or supplementing translation models in SMT (see e. g. Munteanu & Marcu, 2005; Wu & Fung, 2005). Another approach for exploiting comparable corpora in dictionary generation is based on the observation that word co-occurrence patterns between languages tend to be similar (Fung & McKeown, 1997; Rapp, 1995; Chiao et al., 2004). If, for example, two words X and Y cooccur more often than expected by chance in a corpus of language A, then their translated equiThere is also the approach of identifying orthographically similar words (Koehn & Knight, 2002) which does not even require a corpus as simple word lists will suffice. However, this approach is promising only for closely related language"
W10-3908,C98-1066,0,\N,Missing
W10-3908,W02-0902,0,\N,Missing
W10-4005,J90-2002,0,0.735891,"ing at the contexts of a foreign word and by computing its strongest associations from these. In this work we focus on the question what results can be expected for 20 language pairs involving five major European languages. We also compare the results for two different types of corpora, namely newsticker texts and web corpora. Our findings show that results are best if English is the source language, and that noisy web corpora are better suited for this task than well edited newsticker texts. 1 Introduction Established methods for the identification of word translations are based on parallel (Brown et al., 1990) or comparable corpora (Fung & McKeown, 1997; Fung & Yee, 1998; Rapp, 1995; Rapp 1999; Chiao et al., 2004). The work using parallel corpora such as Europarl (Koehn, 2005; Armstrong et al., 1998) or JRC Acquis (Steinberger et al., 2006) typically performs a length-based sentence alignment of the translated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into practice using a number of freely available open source tools such as"
W10-4005,J93-1003,0,0.0269198,"assume that the strongest association to a foreign word is likely to be its translation. This can be justified by typical usage patterns of foreign words often involving, for example, an explanation right after their first occurrence in a text. Associations between words can be computed in a straightforward manner by counting word co-occurrences followed by the application of an association measure on the cooccurrence counts. Co-occurrence counts are based on a text window comprising the 20 words on either side of a given foreign word. On the resulting counts we apply the loglikelihood ratio (Dunning, 1993). As explained by Dunning, this measure has the advantage to be applicable also on low counts, which is an important characteristic in our setting where the problem of data sparseness is particularly severe. This is also the reason why we chose a window size somewhat larger than the ones used in most other studies. Despite its simplicity this procedure of computing associations to foreign words is well suited for identifying word translations. As mentioned above, we assume that the strongest association to a foreign word is its best translation. We did this for words from five languages (Engli"
W10-4005,W97-0119,0,0.0277961,"y computing its strongest associations from these. In this work we focus on the question what results can be expected for 20 language pairs involving five major European languages. We also compare the results for two different types of corpora, namely newsticker texts and web corpora. Our findings show that results are best if English is the source language, and that noisy web corpora are better suited for this task than well edited newsticker texts. 1 Introduction Established methods for the identification of word translations are based on parallel (Brown et al., 1990) or comparable corpora (Fung & McKeown, 1997; Fung & Yee, 1998; Rapp, 1995; Rapp 1999; Chiao et al., 2004). The work using parallel corpora such as Europarl (Koehn, 2005; Armstrong et al., 1998) or JRC Acquis (Steinberger et al., 2006) typically performs a length-based sentence alignment of the translated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into practice using a number of freely available open source tools such as Moses (Koehn et al., 2007) and Giza++ (Och &"
W10-4005,P98-1069,0,0.0392083,"est associations from these. In this work we focus on the question what results can be expected for 20 language pairs involving five major European languages. We also compare the results for two different types of corpora, namely newsticker texts and web corpora. Our findings show that results are best if English is the source language, and that noisy web corpora are better suited for this task than well edited newsticker texts. 1 Introduction Established methods for the identification of word translations are based on parallel (Brown et al., 1990) or comparable corpora (Fung & McKeown, 1997; Fung & Yee, 1998; Rapp, 1995; Rapp 1999; Chiao et al., 2004). The work using parallel corpora such as Europarl (Koehn, 2005; Armstrong et al., 1998) or JRC Acquis (Steinberger et al., 2006) typically performs a length-based sentence alignment of the translated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into practice using a number of freely available open source tools such as Moses (Koehn et al., 2007) and Giza++ (Och & Ney, 2003). Howev"
W10-4005,2005.mtsummit-papers.11,0,0.0245523,"pairs involving five major European languages. We also compare the results for two different types of corpora, namely newsticker texts and web corpora. Our findings show that results are best if English is the source language, and that noisy web corpora are better suited for this task than well edited newsticker texts. 1 Introduction Established methods for the identification of word translations are based on parallel (Brown et al., 1990) or comparable corpora (Fung & McKeown, 1997; Fung & Yee, 1998; Rapp, 1995; Rapp 1999; Chiao et al., 2004). The work using parallel corpora such as Europarl (Koehn, 2005; Armstrong et al., 1998) or JRC Acquis (Steinberger et al., 2006) typically performs a length-based sentence alignment of the translated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into practice using a number of freely available open source tools such as Moses (Koehn et al., 2007) and Giza++ (Och & Ney, 2003). However, parallel texts are a scarce resource for many language pairs (Rapp & Martín Vide, 2007), which is why m"
W10-4005,P07-2045,0,0.0132767,"parable corpora (Fung & McKeown, 1997; Fung & Yee, 1998; Rapp, 1995; Rapp 1999; Chiao et al., 2004). The work using parallel corpora such as Europarl (Koehn, 2005; Armstrong et al., 1998) or JRC Acquis (Steinberger et al., 2006) typically performs a length-based sentence alignment of the translated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into practice using a number of freely available open source tools such as Moses (Koehn et al., 2007) and Giza++ (Och & Ney, 2003). However, parallel texts are a scarce resource for many language pairs (Rapp & Martín Vide, 2007), which is why methods based on comparable corpora have come into focus. One approach is to extract parallel sentences from comparable corpora (Munteanu & Marcu, 2005; Wu & Fung, 2005). Another approach relates co-occurrence patterns between languages. Hereby the underlying assumption is that across languages there is a correlation between the cooccurrences of words which are translations of each other. If, for example, in a text of one language two words A and B co-oc"
W10-4005,J05-4003,0,0.0254999,"ranslated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into practice using a number of freely available open source tools such as Moses (Koehn et al., 2007) and Giza++ (Och & Ney, 2003). However, parallel texts are a scarce resource for many language pairs (Rapp & Martín Vide, 2007), which is why methods based on comparable corpora have come into focus. One approach is to extract parallel sentences from comparable corpora (Munteanu & Marcu, 2005; Wu & Fung, 2005). Another approach relates co-occurrence patterns between languages. Hereby the underlying assumption is that across languages there is a correlation between the cooccurrences of words which are translations of each other. If, for example, in a text of one language two words A and B co-occur more often than expected by chance, then in a text of another language those words which are the translations of A and B should also co-occur more frequently than expected. However, to exploit this observation some bridge needs to be built between the two languages. This can be done via a"
W10-4005,J03-1002,0,0.00232831,"1997; Fung & Yee, 1998; Rapp, 1995; Rapp 1999; Chiao et al., 2004). The work using parallel corpora such as Europarl (Koehn, 2005; Armstrong et al., 1998) or JRC Acquis (Steinberger et al., 2006) typically performs a length-based sentence alignment of the translated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into practice using a number of freely available open source tools such as Moses (Koehn et al., 2007) and Giza++ (Och & Ney, 2003). However, parallel texts are a scarce resource for many language pairs (Rapp & Martín Vide, 2007), which is why methods based on comparable corpora have come into focus. One approach is to extract parallel sentences from comparable corpora (Munteanu & Marcu, 2005; Wu & Fung, 2005). Another approach relates co-occurrence patterns between languages. Hereby the underlying assumption is that across languages there is a correlation between the cooccurrences of words which are translations of each other. If, for example, in a text of one language two words A and B co-occur more often than expected"
W10-4005,P99-1067,1,0.926968,"n this work we focus on the question what results can be expected for 20 language pairs involving five major European languages. We also compare the results for two different types of corpora, namely newsticker texts and web corpora. Our findings show that results are best if English is the source language, and that noisy web corpora are better suited for this task than well edited newsticker texts. 1 Introduction Established methods for the identification of word translations are based on parallel (Brown et al., 1990) or comparable corpora (Fung & McKeown, 1997; Fung & Yee, 1998; Rapp, 1995; Rapp 1999; Chiao et al., 2004). The work using parallel corpora such as Europarl (Koehn, 2005; Armstrong et al., 1998) or JRC Acquis (Steinberger et al., 2006) typically performs a length-based sentence alignment of the translated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into practice using a number of freely available open source tools such as Moses (Koehn et al., 2007) and Giza++ (Och & Ney, 2003). However, parallel texts are"
W10-4005,W10-3908,1,0.253732,"losely related to newspaper text. It is usually carefully edited, and the vocabulary is geared towards easy understanding for the intended readership. This implies that foreign word citations are kept to a minimum. In contrast, the WaCky Corpora have been downloaded from the web and represent a great variety of text types and styles. Hence, not all texts can be expected to have been carefully edited, and mixes between languages are probably more frequent than with newsticker text. As in this work English is the main source language, and as we have dealt with it as a target language already in Rapp & Zock (2010), we do not use the respective English versions of these corpora here. We also do not use the Wikipedia XML Corpora (Denoyer et al., 2006) as these greatly vary in size for different languages which makes comparisons across languages somewhat problematic. In contrast, the sizes of the above corpora are within the same order of magnitude (1 billion words each), which is why we do not control for corpus size here. Concerning the number of foreign words within these corpora, we might expect that, given the status of English as the world’s premiere language, English foreign words should be the mos"
W10-4005,steinberger-etal-2006-jrc,0,0.0638993,"Missing"
W10-4005,I05-1023,0,\N,Missing
W10-4005,C98-1066,0,\N,Missing
W10-4005,W02-0902,0,\N,Missing
W12-5104,dutoit-nugues-2002-algorithm,0,0.0919194,"Missing"
W12-5104,P99-1008,0,0.202636,"Missing"
W12-5104,2003.mtsummit-papers.42,0,0.135157,"Missing"
W12-5104,P04-3026,0,0.024406,"e sense. In our current version we have only one type of relation i.e. meronym and the senses are not labelled semantically. The senses are learned from the number of clusters built on the basis of the parts of the concepts. Example, 'table has parts: column, row, leg, tabletop and tableware'. The cosine value of each part is compared with all other parts to identify the clusters. To this end we used the k-means clustering technique20. In our 'table' example, 'column and row' and 'leg, tabletop and tableware' are grouped together given their respective vectors. To identify senses we use like (Rapp, 2004; Diab & Resnik, 2002; Kaji, 2003; Pinto et al., 2007) a clustering method. However, our task is narrower in that the clusters are formed only from a small set of words associated with a given word at a time. Also we have considered meronymic word senses only i.e. senses that affect PT-WHRs. The extracted wholes and their parts are organized into a network. Concepts are organized hierarchically i.e. going from the whole to its parts. For example 'tooth' is part of 'gear' which is part of an 'engine' which is part of a 'car'. In this case, 'car' is the root. Concepts which are parts of 19 http:"
W12-5104,N03-1015,0,\N,Missing
W12-5104,P02-1033,0,\N,Missing
W12-5104,J06-1005,0,\N,Missing
W12-5104,P06-1036,1,\N,Missing
W14-0509,P06-2084,0,0.123461,"o-occurrence network is able to mimic human associative behavior. Such a network consists of nodes, which in our case correspond to words (or lemmas), and of weights connecting the nodes. The strengths of these weights are computed on the basis of word co-occurrence data, and by optionally applying an association measure. But there are many association measures. Given their number and diversity some researchers (Evert & Krenn, 2001) felt that there was a need to define some criteria and methods in order to allow for quantitative comparisons via task-based evaluations. Pursuing a similar goal, Pecina & Schlesinger (2006) compared 82 different association measures for collocation extraction, while Hoang et al. (2009) classified them. Michelbacher et al. (2011) investigated the potential of asymmetric association measures, i.e. ""associations whose associational strength is significantly greater in one direction (e.g., from Pyrrhic to victory) than in the other (e.g., from victory to Pyrrhic)"". Washtell & Markert (2009) tried to determine whether word associations should be computed via window-based co-occurrence counts or rather via a windowless approach measuring the distances between words. Our work is relate"
W14-0509,J90-1003,0,0.705155,"y (Schwartz & Reisberg, 1991) explains how these strengths (or weights) are acquired. The strength between two perceived events increases by a constant fraction of a maximally possible increment at each co-occurrence, and decreases in the opposite case. Wettler et al. (2005) have shown that this mechanism can be replicated by looking at word co-occurrence frequencies in large text collections. But there had been earlier corpus-linguistic work: For example, Wettler & Rapp (1989) compared several association measures in order to find search terms to be used for queries in information retrieval. Church & Hanks (1990) suggested to use mutual information, an information theoretic measure, for computing association strength. Prior to this, a lot of work had been done without reliance of corpora. For example, Collins & Loftus (1975) used associative semantic networks to show the distance between words. Others (Rosenzweig, 1961:358; Ekpo-Ufot, 1978) tried to show the universal status of a large subset of associations. While all these findings are important, we will not consider them further Free word associations are the words people spontaneously come up with in response to a stimulus word. Such information h"
W14-0509,P01-1025,0,0.0299926,"(CogACLL) @ EACL 2014, pages 43–48, c Gothenburg, Sweden, April 26 2014. 2014 Association for Computational Linguistics here. Rather we will focus on the claim that a corpus-derived co-occurrence network is able to mimic human associative behavior. Such a network consists of nodes, which in our case correspond to words (or lemmas), and of weights connecting the nodes. The strengths of these weights are computed on the basis of word co-occurrence data, and by optionally applying an association measure. But there are many association measures. Given their number and diversity some researchers (Evert & Krenn, 2001) felt that there was a need to define some criteria and methods in order to allow for quantitative comparisons via task-based evaluations. Pursuing a similar goal, Pecina & Schlesinger (2006) compared 82 different association measures for collocation extraction, while Hoang et al. (2009) classified them. Michelbacher et al. (2011) investigated the potential of asymmetric association measures, i.e. ""associations whose associational strength is significantly greater in one direction (e.g., from Pyrrhic to victory) than in the other (e.g., from victory to Pyrrhic)"". Washtell & Markert (2009) trie"
W14-0509,D09-1066,0,0.247164,"earchers (Evert & Krenn, 2001) felt that there was a need to define some criteria and methods in order to allow for quantitative comparisons via task-based evaluations. Pursuing a similar goal, Pecina & Schlesinger (2006) compared 82 different association measures for collocation extraction, while Hoang et al. (2009) classified them. Michelbacher et al. (2011) investigated the potential of asymmetric association measures, i.e. ""associations whose associational strength is significantly greater in one direction (e.g., from Pyrrhic to victory) than in the other (e.g., from victory to Pyrrhic)"". Washtell & Markert (2009) tried to determine whether word associations should be computed via window-based co-occurrence counts or rather via a windowless approach measuring the distances between words. Our work is related to previous studies comparing human word associations with those derived from corpus statistics (e.g. Wettler et al., 2005; Tamir, 2005, Seidensticker, 2006). The main differences are that we categorize our stimulus words and present results for each class, and that we have a stronger focus on the graph aspect of our network. 2 noise and data sparsity while improving speed and accuracy during evalua"
W14-4701,more-climent-2014-machine,0,0.0296862,"one stimulus increases significantly the response to another. Meyer and Schvaneveldt (1971) showed in their seminal experiments that people were faster in deciding that a string of letters is a word when it was followed by an associatively or semantically related word. For example, nurse is recognized more quickly following doctor than following bread. These findings supported also the idea of activation spreading as a method of access or search (Collins & Loftus, 1975). Associative networks can be considered as a special type of semantic network which were introduced by Richens (1956) and by Ceccato (1956) for quite a different purpose. They were meant to serve as an interlingua for machine translation. These knowledge representation structures were then further developed in the sixties by Simmons (1963) and Quillian (1963, 1966, 1967, 1968, 1969). They finally became famous due to the work done by Quillian and two psychologistst (Collins & Quillian, 1969 & 1970 and Collins & Loftus, 1975). Note that semantic networks can represent language at various levels of granularity: word, sentence (Sowa, 1984) or discourse (Mann & Thomson, 1988). Also, and very relevant for us here is the fact that at t"
W14-4701,E99-1013,0,0.0538156,"blem have systematically shown (Aitchison, 2003; Brown, 1991; Brown & McNeill, 1996) that users being in this state always know ‘something’ concerning the target word: fragments of the meaning, origin, number of syllables, etc. This being so, any of this could be used to guide the search. Suppose we focused only on the semantic aspects. In such a case it is reasonable to assume that the target form can be found on the basis of its defining elements (bag of the words contained in the definition). While not being perfect, this works quite well (Dutoit & Nugues, 2002; El-Kahlout & Oflazer, 2004; Mandala et al., 1999; Michiels, 1982). Actually, even Google - although not designed for this is able to recover in many cases the elusive word. Just try the following example, spring, typically found in Iceland or in the Yellowstone National Park, discharging hot water and steam, and chances are that you will find the target word geyser. Although not perfect, this is nevertheless quite useful. However, this represents only one kind of cognitive state (knowledge of the definition), and this is certainly neither the only one nor the most frequent one. Indeed, there are many situations where it is hard to come up w"
W14-4701,P99-1067,1,0.237424,"toll, officer, or country can be expected to come up in both cases. That is, their meaning vectors should be similar, and this similarity can be quantified e.g. by computing the cosine similarity between them. We thus have a method which allows us to measure the similarity between sentences in a way that to some extend takes their meanings into account. Finally, we can try to cross language barriers and make the step to association-based machine translation (ABMT). To translate a source language phrase, we compute its meaning vector. Presupposing that we have a basic dictionary, in analogy to Rapp (1999) we can translate this meaning vector into the target language.10 Further assuming that we already know the meaning vectors of a very large number of target language phrases, we next select the target language meaning vector which is most similar to the source language meaning vector. The respective target language phrase can be considered to be the translation of the source language phrase. Optionally, to improve translation quality, the target language phrase can be modified by adding, removing, substituting, or reordering words with the aim of improving the similarity between the meaning ve"
W14-4701,rapp-2014-corpus,1,0.87648,"e stimulus (prime), we have reversed this situation. Given a set of associations, the system was supposed to predict its trigger. More concretely speaking, participants were given 2000 sets of words, each set containing five words. The task was to determine automatically the sixth element, i.e. the prime (or stimulus), evoking the five words. One could object that this task does not really address the word access problem or its solution, but this is not quite so as we will try to show. In particular, it seems quite reasonable to claim that an association network with bi-directional links (see Rapp, 2014) is a suitable resource to support word ‘finding’. Since words are connected via bidirectional links either of the connected items can be the source or the target during the search (or during navigation). Although systems designed for the shared task can have many applications (see Section 6), a prototypical one is the tip-of-the-tongue problem, which is a special case (yet a quite frequent one) of word access. So let us briefly describe this problem and the steps needed to overcome it. One of the most vexing problems in speaking or writing is that one knows a given word, yet fails to access i"
W14-4701,1983.tc-1.13,0,0.644953,"place if exposure to one stimulus increases significantly the response to another. Meyer and Schvaneveldt (1971) showed in their seminal experiments that people were faster in deciding that a string of letters is a word when it was followed by an associatively or semantically related word. For example, nurse is recognized more quickly following doctor than following bread. These findings supported also the idea of activation spreading as a method of access or search (Collins & Loftus, 1975). Associative networks can be considered as a special type of semantic network which were introduced by Richens (1956) and by Ceccato (1956) for quite a different purpose. They were meant to serve as an interlingua for machine translation. These knowledge representation structures were then further developed in the sixties by Simmons (1963) and Quillian (1963, 1966, 1967, 1968, 1969). They finally became famous due to the work done by Quillian and two psychologistst (Collins & Quillian, 1969 & 1970 and Collins & Loftus, 1975). Note that semantic networks can represent language at various levels of granularity: word, sentence (Sowa, 1984) or discourse (Mann & Thomson, 1988). Also, and very relevant for us here"
W14-4701,P98-2123,1,\N,Missing
W14-4701,C98-2118,1,\N,Missing
W14-4726,1993.eamt-1.1,0,0.414874,"Missing"
W14-4726,P98-2180,0,0.0406768,"of dictionaries or lexical resources, very few of them can be said to meet truly the authors’ needs. To be fair though, one must admit that great efforts have been made to improve the situation both with respect to lexical resources and electronic dictionaries. In fact, there are quite a few onomasiological dictionaries (van Sterkenburg, 2003). For example, Roget’s Thesaurus (Roget, 1852), analogical dictionaries (Boissi`ere, 1862; Robert et al., 1993), Longman’s Language Activator 224 (Summers, 1993), various network-based dictionaries: WordNet (Fellbaum, 1998; Miller et al., 1990), MindNet (Richardson et al., 1998), HowNet (Dong and Dong, 2006), Pathfinder (Schvaneveldt, 1989), ’The active vocabulary for French’ (Mel’ˇcuk and Polgu`ere, 2007) and Fontenelle (Fontenelle, 1997). Other proposals have been made by Sierra (Sierra, 2000) and Moerdijk (2008). There are also various collocation dictionaries (Benson et al., 2010), reverse dictionaries (Bernstein, 1975; Kahn, 1989; Edmonds, 1999) and OneLook,3 which combines a dictionary (WordNet) and an encyclopedia (Wikipedia). Finally, there is MEDAL (Rundell and Fox, 2002), a thesaurus produced with the help of Kilgariff’s Sketch Engine (Kilgarriff et al., 20"
W14-4726,C96-2167,1,0.592185,"t differently, rather than starting from a full fledged definition or complete meaning representation, authors may well start from an underspecified input (’small bird’ rather than ’sparrow’). Note that the specific requirements of a culture may help us to clarify our thoughts, as well as induce biases or imprecisions because of lexical gaps. Hence we end up using an existing words (eventhough it does not express excatly what we had in mind) rather than coining a new one fitting better our purpose (expressibility problem). For a psycholinguistic explanation concerning gradual refinement, see (Zock, 1996). Let me briefly illustrate this here via an example, and comment then on the way how specific knowledge states may ask for different kind of information from the lexicon. Suppose you wanted to talk about a given reptile having certain features (dangerous, size, living space, ...). If you cannot come up immediately with the intended word, any of the following could be candidates: alligator, crocodile, cayman. At some point you need to make up your mind though, as the form synthesizer needs to know what items to activate so that it can produce the corresponding form (graphemes, sounds). scene s"
W14-4726,C98-2175,0,\N,Missing
W14-6706,P10-1023,0,0.0808708,"Missing"
W16-5308,C12-1017,1,0.852364,"omic relations, such as co-hyponyms. The second-best resource in this evaluation is the word co-occurrence network, which outperforms WN on all metrics except the P@100 of MinRank scores. We also analyzed the differences qualitatively and looked at cue-target-pairs where the three networks perform very differently. As our findings show, different networks have different potentials with respect to the retrieval of ToT targets based on a given cue: • WordNet good, Co-occurrence poor: Synonyms or near-synonyms, like javelin – spear, cadaver – corpse. These do not co-occur in sentences, also cf. (Biemann et al., 2012). • WordNet poor, Co-occurrence good: associations, like hospital–doctor or hospital–sick. They are not encoded in WordNet, its associative relations are very spotty. Note that placing them first in the order of relations did not increase performance. • WordNet good, Similarity poor: meronyms/holonyms, such as door–knob, road–asphalt. These are not similar at all from a distributional point of view. • WordNet poor, Similarity good: relations that should be in WN, but for some reason are missing, e.g. torpedo–missile, calligraphy–art, gazebo–pavilion. • Co-occurrence good, Similarity poor: asso"
W16-5308,J93-1003,0,0.0594457,"tistically significant sentence-based word co-occurrences using the same corpus as here above, and following the methodology of (Quasthoff et al., 2006)10. We expect this resource to be suited for free associations, i.e. cue words whose link to the target cannot be specified. This resource has by far the highest rate of relations across different word classes, as they may occur in patterns like “With Xs, especially with Y ones, you can Z and W” (e.g. “with mochas, especially with iced ones, you can chill and have cookies”). Co-occurrences are ranked by the log-likelihood significance measure (Dunning, 1993). 6.2 Network Access Given the structural differences of our resources, our networks are accessed with different query strategies. The general setup is to query the resource via a cue and to insert then the retrieved terms into a ranking. As long as the system has not found all the desired words, it will keep going by querying with words according to their rank, inserting previously un-retrieved terms below the ranking. • WordNet: Having noticed that people tend to use hypernyms (flower) as cues to find the hyponym (rose, the target), we defined a heuristic supporting queries using this relati"
W16-5308,dutoit-nugues-2002-algorithm,0,0.0619055,"and various network-based lexical resources: WordNet, henceforth WN (Miller,1990), Framenet (Fillmore et al. 2003); MindNet (Richardson et al., 1998), and HowNet (Dong & Dong, 2006;). Finally, there are collocation dictionaries (Benson et al., 2010), and web-based tools like Lexical FreeNet3 or Onelook (Beeferman, 2003), which, like BabelNet (Navigli & Ponzetto, 2012) combines a dictionary (WN) and an encyclopedia (Wikipedia), though putting the emphasis on onomasiological search, access by meaning. Reverse dictionaries have been built by hand (Bernstein, 1975) and with the help of machines (Dutoit and Nugues, 2002). In both cases, one draws on the words occurring in the definition. Thorat and Choudhari (2016) try to extend this idea by introducing a distance-based approach to compute word similarity. Given a small set of words they compare their approach with Onelook and with dense-vector similarity. While we adopt part of their methodology in our evaluation scheme, we are more reserved with respect to their architecture. Since it requires a fully computed similarity matrix for the entire vocabulary, their work cannot scale up: it is unreasonable to assume that the lexicon is stored in a fully connected"
W16-5308,quasthoff-etal-2006-corpus,0,0.0344205,"newswire corpus of 100 million sentences in English. We expect this resource to be suitable for most associative queries, that is to help us find words occurring in contexts like “X is somehow like a Y or a Z” (e.g. “a panda is somehow like a koala or a grizzly”). This example illustrates ‘co-hyponymy’, a relation not directly encoded in WordNet. Similarities (for example, panda/koala vs. panda/dog) are ranked by context overlap. • Word Co-occurrence: We compute statistically significant sentence-based word co-occurrences using the same corpus as here above, and following the methodology of (Quasthoff et al., 2006)10. We expect this resource to be suited for free associations, i.e. cue words whose link to the target cannot be specified. This resource has by far the highest rate of relations across different word classes, as they may occur in patterns like “With Xs, especially with Y ones, you can Z and W” (e.g. “with mochas, especially with iced ones, you can chill and have cookies”). Co-occurrences are ranked by the log-likelihood significance measure (Dunning, 1993). 6.2 Network Access Given the structural differences of our resources, our networks are accessed with different query strategies. The gen"
W16-5308,P98-2180,0,0.201234,"eir task (our goal). Lexicographers bridge this gap. Unfortunately, until recently most of their tools have been built for the language receiver. Nevertheless, nowadays there are also some tools for the language producer. For example, Roget’s thesaurus (Roget, 1852) or its modern incarnation built with the help of corpus linguistics (Dornseiff, 2003). There are also the Language Activator (Summers, 1993), the Oxford Learner’s Wordfinder Dictionary (Trappes-Lomax, 1997), and various network-based lexical resources: WordNet, henceforth WN (Miller,1990), Framenet (Fillmore et al. 2003); MindNet (Richardson et al., 1998), and HowNet (Dong & Dong, 2006;). Finally, there are collocation dictionaries (Benson et al., 2010), and web-based tools like Lexical FreeNet3 or Onelook (Beeferman, 2003), which, like BabelNet (Navigli & Ponzetto, 2012) combines a dictionary (WN) and an encyclopedia (Wikipedia), though putting the emphasis on onomasiological search, access by meaning. Reverse dictionaries have been built by hand (Bernstein, 1975) and with the help of machines (Dutoit and Nugues, 2002). In both cases, one draws on the words occurring in the definition. Thorat and Choudhari (2016) try to extend this idea by in"
W16-5308,C16-1263,0,0.0131414,"(Fillmore et al. 2003); MindNet (Richardson et al., 1998), and HowNet (Dong & Dong, 2006;). Finally, there are collocation dictionaries (Benson et al., 2010), and web-based tools like Lexical FreeNet3 or Onelook (Beeferman, 2003), which, like BabelNet (Navigli & Ponzetto, 2012) combines a dictionary (WN) and an encyclopedia (Wikipedia), though putting the emphasis on onomasiological search, access by meaning. Reverse dictionaries have been built by hand (Bernstein, 1975) and with the help of machines (Dutoit and Nugues, 2002). In both cases, one draws on the words occurring in the definition. Thorat and Choudhari (2016) try to extend this idea by introducing a distance-based approach to compute word similarity. Given a small set of words they compare their approach with Onelook and with dense-vector similarity. While we adopt part of their methodology in our evaluation scheme, we are more reserved with respect to their architecture. Since it requires a fully computed similarity matrix for the entire vocabulary, their work cannot scale up: it is unreasonable to assume that the lexicon is stored in a fully connected similarity matrix, which grows quadratically in the size of the vocabulary. Note that while den"
W16-5308,W93-0310,0,0.840815,"n particular, association thesauri, they are too small to allow us to solve the ToT-problem. Projected resource would still have to be built, and while one could imagine the use of combined resources, like Babelnet (Navigli and Ponzetto, 2012), or the combination of WN with other resources like topic maps (Agirre et al. 2001), Roget’s Thesaurus (Mandala, 1999) or ConceptNet (Liu and Sing, 2004), it is not easy to tell which combination is best, all the more as besides encyclopedic knowledge, we also need episodic knowledge (Tulving, 1983). One straightforward solution might be co-occurrences (Wettler & Rapp, 1993; Lemaire & Denhière, 2004; Schulte im Walde & Melinger, 2008). While co-occurring words contain many appropriate clue – target pairs, they also contain many unrelated terms that hamper access – even after application of appropriate significance measures. More severely, there are no structural elements that generalize across queries. Another solution could be lexical functions (Mel'čuk, 1996) or semagrams (Moerdijk, 2008) which are reminiscent of the lexical-semantic networks produced by Fontenelle (1997) on the basis of the Collins-Robert dictionary enriched with Melcuk's lexical functions. S"
W16-5308,C98-2175,0,\N,Missing
W98-1407,1993.eamt-1.1,0,0.0690738,"simple binary Value (important vs. unimportant) 1. • All this reflects, of course, in the content and form of the final text. Relative importance is signaled by different means at the text level (headers, paragraphs, etc.) and at the sentence level (word choice, •syntactic structure: main clause versus subordinate clause, topic-comment •structures). •Concerning the prominence status (i.e. relative importance of a piece of information), semioticians and text linguists have reached a similar conclusion by distinguishing between the &apos;Yoreground/background&quot; or &quot;primary/secondary level&quot; of a text [Bar66, vD77, AP89, Com92]. According to Combettes [Com92], the &quot;primary level&quot; deals with the core meaning, i.e. events and facts that make the text progress, while th e &quot;secondary level&quot; deaIs •with descriptions, evaluations, comments, and:reformulati0ns. &quot; . - - ~: &quot; i ~. : : : The distinction of levels, with information Of varying shades (salience gradation), implies t h a t it should be possible to identify corresponding linguistic &quot;markers&quot; for each one of them. Yet, as Combettes has pointed out [Com92], the means used for marking the relative importance of information may vary •from one type of text to another."
