2021.findings-acl.339,Leveraging {A}bstract {M}eaning {R}epresentation for Knowledge Base Question Answering,2021,-1,-1,4,0,3561,pavan kapanipathi,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.507,Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based {AMR} Parsing,2021,-1,-1,6,0,4550,jiawei zhou,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Predicting linearized Abstract Meaning Representation (AMR) graphs using pre-trained sequence-to-sequence Transformer models has recently led to large improvements on AMR parsing benchmarks. These parsers are simple and avoid explicit modeling of structure but lack desirable properties such as graph well-formedness guarantees or built-in graph-sentence alignments. In this work we explore the integration of general pre-trained sequence-to-sequence language models and a structure-aware transition-based approach. We depart from a pointer-based transition system and propose a simplified transition set, designed to better exploit pre-trained language models for structured fine-tuning. We also explore modeling the parser state within the pre-trained encoder-decoder architecture and different vocabulary strategies for the same purpose. We provide a detailed comparison with recent progress in AMR parsing and show that the proposed parser retains the desirable properties of previous transition-based approaches, while being simpler and reaching the new parsing state of the art for AMR 2.0, without the need for graph re-categorization."
2021.eacl-main.30,Bootstrapping Multilingual {AMR} with Contextual Word Alignments,2021,-1,-1,6,0,10550,janaki sheth,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"We develop high performance multilingual Abstract Meaning Representation (AMR) systems by projecting English AMR annotations to other languages with weak supervision. We achieve this goal by bootstrapping transformer-based multilingual word embeddings, in particular those from cross-lingual RoBERTa (XLM-R large). We develop a novel technique for foreign-text-to-English AMR alignment, using the contextual word alignment between English and foreign language tokens. This word alignment is weakly supervised and relies on the contextualized XLM-R word embeddings. We achieve a highly competitive performance that surpasses the best published results for German, Italian, Spanish and Chinese."
2021.acl-short.34,A Semantics-aware Transformer Model of Relation Linking for Knowledge Base Question Answering,2021,-1,-1,7,0.727287,4551,tahira naseem,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Relation linking is a crucial component of Knowledge Base Question Answering systems. Existing systems use a wide variety of heuristics, or ensembles of multiple systems, heavily relying on the surface question text. However, the explicit semantic parse of the question is a rich source of relation information that is not taken advantage of. We propose a simple transformer-based neural model for relation linking that leverages the AMR semantic parse of a sentence. Our system significantly outperforms the state-of-the-art on 4 popular benchmark datasets. These are based on either DBpedia or Wikidata, demonstrating that our approach is effective across KGs."
2020.findings-emnlp.288,Pushing the Limits of {AMR} Parsing with Self-Learning,2020,-1,-1,6,0,8302,youngsuk lee,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Abstract Meaning Representation (AMR) parsing has experienced a notable growth in performance in the last two years, due both to the impact of transfer learning and the development of novel architectures specific to AMR. At the same time, self-learning techniques have helped push the performance boundaries of other natural language processing applications, such as machine translation or question answering. In this paper, we explore different ways in which trained models can be applied to improve AMR parsing performance, including generation of synthetic text and AMR annotations as well as refinement of actions oracle. We show that, without any additional human annotations, these techniques improve an already performant parser and achieve state-of-the-art results on AMR 1.0 and AMR 2.0."
2020.emnlp-main.440,Multi-Stage Pre-training for Low-Resource Domain Adaptation,2020,-1,-1,8,0,7451,rong zhang,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Transfer learning techniques are particularly useful for NLP tasks where a sizable amount of high-quality annotated data is difficult to obtain. Current approaches directly adapt a pretrained language model (LM) on in-domain text before fine-tuning to downstream tasks. We show that extending the vocabulary of the LM with domain-specific terms leads to further gains. To a bigger effect, we utilize structure in the unlabeled data to create auxiliary synthetic tasks, which helps the LM transfer to downstream tasks. We apply these approaches incrementally on a pretrained Roberta-large LM and show considerable performance gain on three tasks in the IT domain: Extractive Reading Comprehension, Document Ranking and Duplicate Question Detection."
2020.emnlp-demos.5,{ARES}: A Reading Comprehension Ensembling Service,2020,-1,-1,4,0,12663,anthony ferritto,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"We introduce ARES (A Reading Comprehension Ensembling Service): a novel Machine Reading Comprehension (MRC) demonstration system which utilizes an ensemble of models to increase F1 by 2.3 points. While many of the top leaderboard submissions in popular MRC benchmarks such as the Stanford Question Answering Dataset (SQuAD) and Natural Questions (NQ) use model ensembles, the accompanying papers do not publish their ensembling strategies. In this work, we detail and evaluate various ensembling strategies using the NQ dataset. ARES leverages the CFO (Chakravarti et al., 2019) and ReactJS distributed frameworks to provide a scalable interactive Question Answering experience that capitalizes on the agreement (or lack thereof) between models to improve the answer visualization experience."
2020.coling-industry.9,Towards building a Robust Industry-scale Question Answering System,2020,-1,-1,6,1,3535,rishav chakravarti,Proceedings of the 28th International Conference on Computational Linguistics: Industry Track,0,"Industry-scale NLP systems necessitate two features. 1. Robustness: {``}zero-shot transfer learning{''} (ZSTL) performance has to be commendable and 2. Efficiency: systems have to train efficiently and respond instantaneously. In this paper, we introduce the development of a production model called GAAMA (Go Ahead Ask Me Anything) which possess the above two characteristics. For robustness, it trains on the recently introduced Natural Questions (NQ) dataset. NQ poses additional challenges over older datasets like SQuAD: (a) QA systems need to read and comprehend an entire Wikipedia article rather than a small passage, and (b) NQ does not suffer from observation bias during construction, resulting in less lexical overlap between the question and the article. GAAMA consists of Attention-over-Attention, diversity among attention heads, hierarchical transfer learning, and synthetic data augmentation while being computationally inexpensive. Building on top of the powerful BERTQA model, GAAMA provides a â¼2.0{\%} absolute boost in F1 over the industry-scale state-of-the-art (SOTA) system on NQ. Further, we show that GAAMA transfers zero-shot to unseen real life and important domains as it yields respectable performance on two benchmarks: the BioASQ and the newly introduced CovidQA datasets."
2020.coling-demos.8,A Multilingual Reading Comprehension System for more than 100 Languages,2020,-1,-1,6,0,12663,anthony ferritto,Proceedings of the 28th International Conference on Computational Linguistics: System Demonstrations,0,"This paper presents M-GAAMA, a Multilingual Question Answering architecture and demo system. This is the first multilingual machine reading comprehension (MRC) demo which is able to answer questions in over 100 languages. M-GAAMA answers questions from a given passage in the same or different language. It incorporates several existing multilingual models that can be used interchangeably in the demo such as M-BERT and XLM-R. The M-GAAMA demo also improves language accessibility by incorporating the IBM Watson machine translation widget to provide additional capabilities to the user to see an answer in their desired language. We also show how M-GAAMA can be used in downstream tasks by incorporating it into an END-TO-END-QA system using CFO (Chakravarti et al., 2019). We experiment with our system architecture on the Multi-Lingual Question Answering (MLQA) and the COVID-19 CORD (Wang et al., 2020; Tang et al., 2020) datasets to provide insights into the performance of the system."
2020.acl-main.117,The {T}ech{QA} Dataset,2020,-1,-1,16,0,19709,vittorio castelli,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We introduce TECHQA, a domain-adaptation question answering dataset for the technical support domain. The TECHQA corpus highlights two real-world issues from the automated customer support domain. First, it contains actual questions posed by users on a technical forum, rather than questions generated specifically for a competition or a task. Second, it has a real-world size {--} 600 training, 310 dev, and 490 evaluation question/answer pairs {--} thus reflecting the cost of creating large labeled datasets with actual data. Hence, TECHQA is meant to stimulate research in domain adaptation rather than as a resource to build QA systems from scratch. TECHQA was obtained by crawling the IBMDeveloper and DeveloperWorks forums for questions with accepted answers provided in an IBM Technote{---}a technical document that addresses a specific technical issue. We also release a collection of the 801,998 Technotes available on the web as of April 4, 2019 as a companion resource that can be used to learn representations of the IT domain language."
2020.acl-main.167,{GPT}-too: A Language-Model-First Approach for {AMR}-to-Text Generation,2020,31,1,7,0,5787,manuel mager,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Abstract Meaning Representations (AMRs) are broad-coverage sentence-level semantic graphs. Existing approaches to generating text from AMR have focused on training sequence-to-sequence or graph-to-sequence models on AMR annotated data only. In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring. Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures. In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach."
P19-1451,Rewarding {S}match: Transition-Based {AMR} Parsing with Reinforcement Learning,2019,41,2,5,0.809116,4551,tahira naseem,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Our work involves enriching the Stack-LSTM transition-based AMR parser (Ballesteros and Al-Onaizan, 2017) by augmenting training with Policy Learning and rewarding the Smatch score of sampled graphs. In addition, we also combined several AMR-to-text alignments with an attention mechanism and we supplemented the parser with pre-processed concept identification, named entities and contextualized embeddings. We achieve a highly competitive performance that is comparable to the best published results. We show an in-depth study ablating each of the new components of the parser."
D19-3006,{CFO}: A Framework for Building Production {NLP} Systems,2019,16,0,10,1,3535,rishav chakravarti,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations,0,"This paper introduces a novel orchestration framework, called CFO (Computation Flow Orchestrator), for building, experimenting with, and deploying interactive NLP (Natural Language Processing) and IR (Information Retrieval) systems to production environments. We then demonstrate a question answering system built using this framework which incorporates state-of-the-art BERT based MRC (Machine Reading Com- prehension) with IR components to enable end-to-end answer retrieval. Results from the demo system are shown to be high quality in both academic and industry domain specific settings. Finally, we discuss best practices when (pre-)training BERT based MRC models for production systems. Screencast links: - Short video ({\textless} 3 min): http: //ibm.biz/gaama{\_}demo - Supplementary long video ({\textless} 13 min): http://ibm.biz/gaama{\_}cfo{\_}demo"
P14-1081,Adaptive {HTER} Estimation for Document-Specific {MT} Post-Editing,2014,22,2,4,0,3692,fei huang,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We present an adaptive translation quality estimation (QE) method to predict the human-targeted translation error rate (HTER) for a document-specific machine translation model. We first introduce features derived internal to the translation decoding process as well as externally from the source sentence analysis. We show the effectiveness of such features in both classification and regression of MT quality. By dynamically training the QE model for the document-specific MT model, we are able to achieve consistency and prediction quality across multiple documents, demonstrated by the higher correlation coefficient and F-scores in finding Good sentences. Additionally, the proposed method is applied to IBM English-to-Japanese MT post editing field study and we observe strong correlation with human preference, with a 10% increase in human translatorsxe2x80x99 productivity."
D14-1001,Invited Talk: {IBM} Cognitive Computing - An {NLP} Renaissance!,2014,0,0,1,1,8292,salim roukos,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,None
2014.amta-users.15,A novel use of {MT} in the development of a text level analytic for language learning,2014,-1,-1,2,0,40417,carol essdykema,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Users Track,0,None
D11-1082,A Correction Model for Word Alignments,2011,25,8,3,0,22637,scott mccarley,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"Models of word alignment built as sequences of links have limited expressive power, but are easy to decode. Word aligners that model the alignment matrix can express arbitrary alignments, but are difficult to decode. We propose an alignment matrix model as a correction algorithm to an underlying sequence-based aligner. Then a greedy decoding algorithm enables the full expressive power of the alignment matrix formulation. Improved alignment performance is shown for all nine language pairs tested. The improved alignments also improved translation quality from Chinese to English and English to Italian."
D10-1033,Improving Mention Detection Robustness to Noisy Input,2010,23,19,3,0.27166,4553,radu florian,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"Information-extraction (IE) research typically focuses on clean-text inputs. However, an IE engine serving real applications yields many false alarms due to less-well-formed input. For example, IE in a multilingual broadcast processing system has to deal with inaccurate automatic transcription and translation. The resulting presence of non-target-language text in this case, and non-language material interspersed in data from other applications, raise the research problem of making IE robust to such noisy input text. We address one such IE task: entity-mention detection. We describe augmenting a statistical mention-detection system in order to reduce false alarms from spurious passages. The diverse nature of input noise leads us to pursue a multi-faceted approach to robustness. For our English-language system, at various miss rates we eliminate 97% of false alarms on inputs from other Latin-alphabet languages. In another experiment, representing scenarios in which genre-specific training is infeasible, we process real financial-transactions text containing mixed languages and data-set codes. On these data, because we do not train on data like it, we achieve a smaller but significant improvement. These gains come with virtually no loss in accuracy on clean English text."
C10-1062,Learning to Predict Readability using Diverse Linguistic Features,2010,25,63,7,0,37230,rohit kate,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,In this paper we consider the problem of building a system to predict readability of natural-language documents. Our system is trained using diverse features based on syntax and language models which are generally indicative of readability. The experimental results on a dataset of documents from a mix of genres show that the predictions of the learned system are more accurate than the predictions of naive human judges when compared against the predictions of linguistically-trained expert human judges. The experiments also compare the performances of different learning algorithms and different types of feature sets when used for predicting readability.
2009.mtsummit-commercial.7,Real Time Translation Services at {IBM},2009,-1,-1,2,0,46681,david lubensky,Proceedings of Machine Translation Summit XII: Commercial MT User Program,0,None
P07-1131,Extracting Social Networks and Biographical Facts From Conversational Speech Transcripts,2007,7,18,3,0.368058,49187,hongyan jing,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We present a general framework for automatically extracting social networks and biographical facts from conversational speech. Our approach relies on fusing the output produced by multiple information extraction modules, including entity recognition and detection, relation detection, and event detection modules. We describe the specific features and algorithmic refinements effective for conversational speech. These cumulatively increase the performance of social network extraction from 0.06 to 0.30 for the development set, and from 0.06 to 0.28 for the test set, as measured by f-measure on the ties within a network. The same framework can be applied to other genres of text xe2x80x94 we have built an automatic biography generation system for general domain text using the same approach."
N07-1008,Direct Translation Model 2,2007,17,53,2,1,35485,abraham ittycheriah,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"This paper presents a maximum entropy machine translation system using a minimal set of translation blocks (phrase-pairs). While recent phrase-based statistical machine translation (SMT) systems achieve significant improvement over the original source-channel statistical translation models, they 1) use a large inventory of blocks which have significant overlap and 2) limit the use of training to just a few parameters (on the order of ten). In contrast, we show that our proposed minimalist system (DTM2) achieves equal or better performance by 1) recasting the translation problem in the traditional statistical modeling approach using blocks with no overlap and 2) relying on training most system parameters (on the order of millions or larger). The new model is a direct translation model (DTM) formulation which allows easy integration of additional/alternative views of both source and target sentences such as segmentation for a source language such as Arabic, part-of-speech of both source and target, etc. We show improvements over a state-of-the-art phrase-based decoder in Arabic-English translation."
2006.iwslt-plenaries.2,Rosetta: an analyst{'}s co-pilot,2006,-1,-1,1,1,8292,salim roukos,Proceedings of the Third International Workshop on Spoken Language Translation: Plenaries,0,None
H05-1012,A Maximum Entropy Word Aligner for {A}rabic-{E}nglish Machine Translation,2005,16,109,2,1,35485,abraham ittycheriah,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,This paper presents a maximum entropy word alignment algorithm for Arabic-English based on supervised training data. We demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of supervised and unsupervised methods yields superior performance. The probabilistic model used in the alignment directly models the link decisions. Significant improvement over traditional word alignment techniques is shown as well as improvement on several machine translation tests. Performance of the algorithm is contrasted with human annotation performance.
P04-1018,A Mention-Synchronous Coreference Resolution Algorithm Based On the Bell Tree,2004,15,196,5,1,39124,xiaoqiang luo,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.
2004.iwslt-evaluation.6,{IBM} spoken language translation system evaluation,2004,19,9,2,1,8302,youngsuk lee,Proceedings of the First International Workshop on Spoken Language Translation: Evaluation Campaign,0,We discuss phrase-based statistical machine translation performance enhancing techniques which have proven effective for Japanese-to-English and Chinese-toEnglish translation of BTEC corpus. We also address some issues that arise in conversational speech translation quality evaluations.
P03-1020,t{R}u{E}cas{I}ng,2003,10,99,3,0,48643,lucian lita,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,None
P03-1051,Language Model Based {A}rabic Word Segmentation,2003,10,119,3,1,8302,youngsuk lee,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"We approximate Arabic's rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix* (* denotes zero or more occurrences of a morpheme). Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus. The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input. The language model is initially estimated from a small manually segmented corpus of about 110,000 words. To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus. The resulting Arabic word segmentation system achieves around 97% exact match accuracy on a test corpus containing 28,449 word tokens. We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest."
N03-4001,{TIPS}: A Translingual Information Processing System,2003,5,3,8,0,8332,yaser alonaizan,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Demonstrations,0,"Searching online information is increasingly a daily activity for many people. The multilinguality of online content is also increasing (e.g. the proportion of English web users, which has been decreasing as a fraction the increasing population of web users, dipped below 50% in the summer of 2001). To improve the ability of an English speaker to search mutlilingual content, we built a system that supports cross-lingual search of an Arabic newswire collection and provides on demand translation of Arabic web pages into English. The cross-lingual search engine supports a fast search capability (sub-second response for typical queries) and achieves state-of-the-art performance in the high precision region of the result list. The on demand statistical machine translation uses the Direct Translation model along with a novel statistical Arabic Morphological Analyzer to yield state-of-the-art translation quality. The on demand SMT uses an efficient dynamic programming decoder that achieves reasonable speed for translating web documents."
N03-2014,Identifying and Tracking Entity Mentions in a Maximum Entropy Framework,2003,8,15,5,1,35485,abraham ittycheriah,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,"We present a system for identifying and tracking named, nominal, and pronominal mentions of entities within a text document. Our maximum entropy model for mention detection combines two pre-existing named entity taggers (built to extract different entity categories) and other syntactic and morphological feature streams to achieve competitive performance. We developed a novel maximum entropy model for tracking all mentions of an entity within a document. We participated in the Automatic Content Extraction (ACE) evaluation and performed well. We describe our system and present results of the ACE evaluation."
N03-2029,Automatic Derivation of Surface Text Patterns for a Maximum Entropy Based Question Answering System,2003,8,31,3,0,47397,deepak ravichandran,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,In this paper we investigate the use of surface text patterns for a Maximum Entropy based Question Answering (QA) system. These text patterns are collected automatically in an unsupervised fashion using a collection of trivia question and answer pairs as seeds. These patterns are used to generate features for a statistical question answering system. We report our results on the TREC-10 question set.
W02-0209,A Flexible Framework for Developing Mixed-Initiative Dialog Systems,2002,12,24,3,0,53264,judith hochberg,Proceedings of the Third {SIG}dial Workshop on Discourse and Dialogue,0,"We present a new framework for rapid development of mixed-initiative dialog systems. Using this framework, a developer can author sophisticated dialog systems for multiple channels of interaction by specifying an interaction modality, a rich task hierarchy and task parameters, and domain-specific modules. The framework includes a dialog history that tracks input, output, and results. We present the framework and preliminary results in two application domains."
P02-1016,Active Learning for Statistical Natural Language Parsing,2002,18,144,3,0,53280,min tang,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"It is necessary to have a (large) annotated corpus to build a statistical parser. Acquisition of such a corpus is costly and time-consuming. This paper presents a method to reduce this demand using active learning, which selects what samples to annotate, instead of annotating blindly the whole training corpus.Sample selection for annotation is based upon representativeness and usefulness. A model-based distance is proposed to measure the difference of two sentences and their most likely parse trees. Based on this distance, the active learning process analyzes the sample distribution by clustering and calculates the density of each sample to quantify its representativeness. Further more, a sentence is deemed as useful if the existing model is highly uncertain about its parses, where uncertainty is measured by various entropy-based scores.Experiments are carried out in the shallow semantic parser of an air travel dialog system. Our result shows that for about the same parsing accuracy, we only need to annotate a third of the samples as compared to the usual random selection method."
P02-1040,{B}leu: a Method for Automatic Evaluation of Machine Translation,2002,2,8530,2,1,20420,kishore papineni,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations."
mccarley-roukos-1998-fast,Fast document translation for cross-language information retrieval,1998,11,12,2,0,55466,jscott mccarley,Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"We describe a statistical algorithm for machine translation intended to provide translations of large document collections at speeds far in excess of traditional machine translation systems, and of sufficiently high quality to perform information retrieval on the translated document collections. The model is trained from a parallel corpus and is capable of disambiguating senses of words. Information retrieval (IR) experiments on a French language dataset from a recent cross-language information retrieval evaluation yields results superior to those obtained by participants in the evaluation, and confirm the importance of word sense disambiugation in cross-language information retrieval."
P97-1022,Fertility Models for Statistical Natural Language Understanding,1997,12,22,3,0,55620,stephen pietra,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"Several recent efforts in statistical natural language understanding (NLU) have focused on generating clumps of English words from semantic meaning concepts (Miller et al., 1995; Levin and Pieracini, 1995; Epstein et al., 1996; Epstein, 1996). This paper extends the IBM Machine Translation Group's concept of fertility (Brown et al., 1993) to the generation of clumps for natural language understanding. The basic underlying intuition is that a single concept may be expressed in English as many disjoint clump of words. We present two fertility models which attempt to capture this phenomenon. The first is a Poisson model which leads to appealing computational simplicity. The second is a general nonparametric fertility model. The general model's parameters are boot-strapped from the Poisson model and updated by the EM algorithm. These fertility models can be used to impose clump fertility structure on top of preexisting clump generation models. Here, we present results for adding fertility structure to unigram, bigram, and headword clump generation models on ARPA's Air Travel Information Service (ATIS) domain."
P96-1019,An Iterative Algorithm to Build {C}hinese Language Models,1996,6,12,2,1,39124,xiaoqiang luo,34th Annual Meeting of the Association for Computational Linguistics,1,"We present an iterative procedure to build a Chinese language model (LM). We segment Chinese text into words based on a word-based Chinese language model. However, the construction of a Chinese LM itself requires word boundaries. To get out of the chicken-and-egg problem, we propose an iterative procedure that alternates two operations: segmenting text into words and building an LM. Starting with an initial segmented corpus and an LM based upon it, we use a Viterbi-liek algorithm to segment another set of data. Then, we build an LM based on the second set and use the resulting LM to segment again the first corpus. The alternating procedure provides a self-organized way for the segmenter to detect automatically unseen words and correct segmentation errors. Our preliminary experiment shows that the alternating procedure not only improves the accuracy of our segmentation, but discovers unseen words suprisingly well. The resulting word-based LM has a perplexity of 188 for a general Chinese corpus."
H94-1048,A Maximum Entropy Model for Prepositional Phrase Attachment,1994,12,195,3,0,10323,adwait ratnaparkhi,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,A parser for natural language must often choose between two or more equally grammatical parses for the same sentence. Often the correct parse can be determined from the lexical properties of certain key words or from the context in which the sentence occurs. For example in the sentence.
H94-1099,Automatic Extraction of Grammars From Annotated Text,1994,-1,-1,1,1,8292,salim roukos,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,None
P93-1005,Towards History-based Grammars: Using Richer Models for Probabilistic Parsing,1993,12,145,6,1,49916,ezra black,31st Annual Meeting of the Association for Computational Linguistics,1,"We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."
H93-1021,Adaptive Language Modeling Using the Maximum Entropy Principle,1993,9,48,3,0,35794,raymond lau,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"We describe our ongoing efforts at adaptive statistical language modeling. Central to our approach is the Maximum Entropy (ME) Principle, allowing us to combine evidence from multiple sources, such as long-distance triggers and conventional short-distance trigrams. Given consistent statistical evidence, a unique ME solution is guaranteed to exist, and an iterative algorithm exists which is guaranteed to converge to it. Among the advantages of this approach are its simplicity, its generality, and its incremental nature. Among its disadvantages are its computational requirements. We describe a succession of ME models, culminating in our current Maximum Likelihood/Maximum Entropy (ML/ME) model. Preliminary results with the latter show a 27% perplexity reduction as compared to a conventional trigram model."
H93-1092,Automatic Extraction of Grammars From Annotated Text,1993,1,0,1,1,8292,salim roukos,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"The primary objective of this project is to develop a robust, high-performance parser for English by automatically extracting a grammar from an annotated corpus of bracketed sentences, called the Treebank. The project is a collaboration between the IBM Continuous Speech Recognition Group and the University of Pennsylvania Department of Computer Sciences. Our initial focus is the domain of computer manuals with a vocabulary of 3000 words. We use a Treebank that was developed jointly by IBM and the University of Lancaster, England, during the past three years."
P92-1024,Development and Evaluation of a Broad-Coverage Probabilistic Grammar of {E}nglish-Language Computer Manuals,1992,6,47,3,1,49916,ezra black,30th Annual Meeting of the Association for Computational Linguistics,1,"We present an approach to grammar development where the task is decomposed into two separate subtasks. The first tasks linguistic, with the goal of producing a set of rules that have a large coverage (in the sense that the correct parse is among the proposed parses) on a blind test set of sentences. The second task is statistical, with the goal of developing a model of the grammar which assigns maximum probability for the correct parse. We give parsing results on text from computer manuals."
H92-1023,Decision Tree Models Applied to the Labeling of Text with Parts-of-Speech,1992,6,48,5,1,49916,ezra black,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,We describe work which uses decision trees to estimate marginal probabilities in a maximum entropy model for predicting the part-of-speech of a word given the context in which it appears. Two experiments are presented which exhibit improvements over the usual hidden Markov model approach.
H92-1026,Towards History-based Grammars: Using Richer Models for Probabilistic Parsing,1992,12,91,6,1,49916,ezra black,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Tree-bank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."
H91-1041,Session 7: Natural Language {II},1991,-1,-1,1,1,8292,salim roukos,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,None
H89-1007,Integrating Speech and Natural Language,1989,0,3,1,1,8292,salim roukos,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,"The overall goal of this project is to integrate speech and natural language knowledge sources to build a speech understanding system for human-machine communication using spoken English. The speech knowledge sources use acoustic models based on hidden Markov modeling techniques. The natural language knowledge sources use a Unification grammar formalism for describing the syntax of English, a higher-order intensional logic language for representing the meaning of an utterance, and a 'Montague Grammar style' framework for interfacing syntax and semantics."
H89-1012,The {BBN} Spoken Language System,1989,11,8,5,0,54538,sean boisen,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,"We describe HARC, a system for speech understanding that integrates speech recognition techniques with natural language processing. The integrated system uses statistical pattern recognition to build a lattice of potential words in the input speech. This word lattice is passed to a unification parser to derive all possible associated syntactic structures for these words. The resulting parse structures are passed to a multi-level semantics component for interpretation."
