2020.coling-main.442,W99-0606,0,0.134568,"consistency identification is possible, but there is scope to further improve the tradeoff between effort and data quality. This work provides the basis for a new direction in research for addressing inconsistencies in crowdsourcing by defining a typology of inconsistency types, collecting new benchmark datasets, and exploring directions for automatic identification of inconsistencies. 2 2.1 Related Work Annotation Consistency Inconsistencies have been studied across a wide range of tasks. Part-of-speech tagging has received particular attention, with a range of methods based on model scores (Abney et al., 1999; Eskin, 2000; Matsumoto and Yamashita, 2000; van Halteren, 2000; Ma et al., 2001; Nakagawa and Matsumoto, 2002). One particular method based on variation in POS tags of n-grams (Dickinson and Meurers, 2003) has been extended to predicate-argument relations (Dickinson and Lee, 2008), and dependency parses (Dickinson, 2010; Dickinson and Smith, 2011). More recent work has explored automatic identification methods for word sense and multi-word-expression annotation (Dligach and Palmer, 2011; Hollenstein et al., 2016). 2.2 Crowdsourcing Quality A range of options have been developed for improving"
2020.coling-main.442,dickinson-lee-2008-detecting,0,0.0525592,"s, collecting new benchmark datasets, and exploring directions for automatic identification of inconsistencies. 2 2.1 Related Work Annotation Consistency Inconsistencies have been studied across a wide range of tasks. Part-of-speech tagging has received particular attention, with a range of methods based on model scores (Abney et al., 1999; Eskin, 2000; Matsumoto and Yamashita, 2000; van Halteren, 2000; Ma et al., 2001; Nakagawa and Matsumoto, 2002). One particular method based on variation in POS tags of n-grams (Dickinson and Meurers, 2003) has been extended to predicate-argument relations (Dickinson and Lee, 2008), and dependency parses (Dickinson, 2010; Dickinson and Smith, 2011). More recent work has explored automatic identification methods for word sense and multi-word-expression annotation (Dligach and Palmer, 2011; Hollenstein et al., 2016). 2.2 Crowdsourcing Quality A range of options have been developed for improving crowdsourcing quality. The most common approach is to collect multiple annotations and then aggregate them (Hovy et al., 2013; Passonneau and Carpenter, 2014; Parde and Nielsen, 2017; Dumitrache et al., 2018). This can identify inconsistencies, but at significant cost as each examp"
2020.coling-main.442,E03-1068,0,0.418111,"ng inconsistencies in crowdsourcing by defining a typology of inconsistency types, collecting new benchmark datasets, and exploring directions for automatic identification of inconsistencies. 2 2.1 Related Work Annotation Consistency Inconsistencies have been studied across a wide range of tasks. Part-of-speech tagging has received particular attention, with a range of methods based on model scores (Abney et al., 1999; Eskin, 2000; Matsumoto and Yamashita, 2000; van Halteren, 2000; Ma et al., 2001; Nakagawa and Matsumoto, 2002). One particular method based on variation in POS tags of n-grams (Dickinson and Meurers, 2003) has been extended to predicate-argument relations (Dickinson and Lee, 2008), and dependency parses (Dickinson, 2010; Dickinson and Smith, 2011). More recent work has explored automatic identification methods for word sense and multi-word-expression annotation (Dligach and Palmer, 2011; Hollenstein et al., 2016). 2.2 Crowdsourcing Quality A range of options have been developed for improving crowdsourcing quality. The most common approach is to collect multiple annotations and then aggregate them (Hovy et al., 2013; Passonneau and Carpenter, 2014; Parde and Nielsen, 2017; Dumitrache et al., 201"
2020.coling-main.442,W11-2929,0,0.0200311,"automatic identification of inconsistencies. 2 2.1 Related Work Annotation Consistency Inconsistencies have been studied across a wide range of tasks. Part-of-speech tagging has received particular attention, with a range of methods based on model scores (Abney et al., 1999; Eskin, 2000; Matsumoto and Yamashita, 2000; van Halteren, 2000; Ma et al., 2001; Nakagawa and Matsumoto, 2002). One particular method based on variation in POS tags of n-grams (Dickinson and Meurers, 2003) has been extended to predicate-argument relations (Dickinson and Lee, 2008), and dependency parses (Dickinson, 2010; Dickinson and Smith, 2011). More recent work has explored automatic identification methods for word sense and multi-word-expression annotation (Dligach and Palmer, 2011; Hollenstein et al., 2016). 2.2 Crowdsourcing Quality A range of options have been developed for improving crowdsourcing quality. The most common approach is to collect multiple annotations and then aggregate them (Hovy et al., 2013; Passonneau and Carpenter, 2014; Parde and Nielsen, 2017; Dumitrache et al., 2018). This can identify inconsistencies, but at significant cost as each example must be annotated multiple times. Less expensive options include"
2020.coling-main.442,P10-1075,0,0.0414187,"ng directions for automatic identification of inconsistencies. 2 2.1 Related Work Annotation Consistency Inconsistencies have been studied across a wide range of tasks. Part-of-speech tagging has received particular attention, with a range of methods based on model scores (Abney et al., 1999; Eskin, 2000; Matsumoto and Yamashita, 2000; van Halteren, 2000; Ma et al., 2001; Nakagawa and Matsumoto, 2002). One particular method based on variation in POS tags of n-grams (Dickinson and Meurers, 2003) has been extended to predicate-argument relations (Dickinson and Lee, 2008), and dependency parses (Dickinson, 2010; Dickinson and Smith, 2011). More recent work has explored automatic identification methods for word sense and multi-word-expression annotation (Dligach and Palmer, 2011; Hollenstein et al., 2016). 2.2 Crowdsourcing Quality A range of options have been developed for improving crowdsourcing quality. The most common approach is to collect multiple annotations and then aggregate them (Hovy et al., 2013; Passonneau and Carpenter, 2014; Parde and Nielsen, 2017; Dumitrache et al., 2018). This can identify inconsistencies, but at significant cost as each example must be annotated multiple times. Les"
2020.coling-main.442,W11-0408,0,0.024081,"tasks. Part-of-speech tagging has received particular attention, with a range of methods based on model scores (Abney et al., 1999; Eskin, 2000; Matsumoto and Yamashita, 2000; van Halteren, 2000; Ma et al., 2001; Nakagawa and Matsumoto, 2002). One particular method based on variation in POS tags of n-grams (Dickinson and Meurers, 2003) has been extended to predicate-argument relations (Dickinson and Lee, 2008), and dependency parses (Dickinson, 2010; Dickinson and Smith, 2011). More recent work has explored automatic identification methods for word sense and multi-word-expression annotation (Dligach and Palmer, 2011; Hollenstein et al., 2016). 2.2 Crowdsourcing Quality A range of options have been developed for improving crowdsourcing quality. The most common approach is to collect multiple annotations and then aggregate them (Hovy et al., 2013; Passonneau and Carpenter, 2014; Parde and Nielsen, 2017; Dumitrache et al., 2018). This can identify inconsistencies, but at significant cost as each example must be annotated multiple times. Less expensive options include using examples with known answers to test worker attention (Oppenheimer et al., 2009), or filtering workers based on qualification tasks or pr"
2020.coling-main.442,A00-2020,0,0.227912,"cation is possible, but there is scope to further improve the tradeoff between effort and data quality. This work provides the basis for a new direction in research for addressing inconsistencies in crowdsourcing by defining a typology of inconsistency types, collecting new benchmark datasets, and exploring directions for automatic identification of inconsistencies. 2 2.1 Related Work Annotation Consistency Inconsistencies have been studied across a wide range of tasks. Part-of-speech tagging has received particular attention, with a range of methods based on model scores (Abney et al., 1999; Eskin, 2000; Matsumoto and Yamashita, 2000; van Halteren, 2000; Ma et al., 2001; Nakagawa and Matsumoto, 2002). One particular method based on variation in POS tags of n-grams (Dickinson and Meurers, 2003) has been extended to predicate-argument relations (Dickinson and Lee, 2008), and dependency parses (Dickinson, 2010; Dickinson and Smith, 2011). More recent work has explored automatic identification methods for word sense and multi-word-expression annotation (Dligach and Palmer, 2011; Hollenstein et al., 2016). 2.2 Crowdsourcing Quality A range of options have been developed for improving crowdsourcin"
2020.coling-main.442,P18-1033,1,0.828023,"at annotated slot-filling data is used to train slot-filling models, we study the impact that the noisy datasets have on slot-filling performance. We trained slot-filling models on three versions of each dataset: the raw crowd-annotated dataset (Crowd), a version in which we fixed Slot Format inconsistencies (Formatted), and a version in which we fixed all inconsistencies (Consistent). We include the Formatted version in our analysis since Slot Format inconsistencies are the most common. Each model was tested on a test set with consistent annotations. We used a Bi-LSTM architecture similar to Finegan-Dollak et al. (2018)’s template-based approach with randomly-initialized word embeddings for the slot-filling model. We computed the slot F1 score using 10-fold cross-validation. Results Table 4 shows slot-filling F1 scores for each training set. The difference in model performance when trained on noisy versus consistently annotated training samples is substantial, with the Companies dataset seeing an absolute difference of roughly 40 percent. The differences for the Forex and Flights datasets are also quite profound. We also note that fixing Slot Format inconsistencies (Formatted) yields model improvements, but"
2020.coling-main.442,P17-1044,0,0.0121091,"ring workers based on qualification tasks or preliminary annotation (Li and Liu, 2015; Roit et al., 2020). However, these primarily address worker attentiveness, which does not cover all the inconsistency types we consider. In particular, we also cover inconsistencies related to subtle cases without an obvious correct answer. 2.3 Error Type Categorization Another line of work has explored automatic identification of error types for tasks such as constituency parsing (Kummerfeld et al., 2012; Kummerfeld et al., 2013), coreference resolution (Kummerfeld and Klein, 2013), semantic role labeling (He et al., 2017), and slot-filling (B´echet and Raymond, 2018). However, they focus on evaluating system outputs in comparison to a gold standard reference in order to understand shortcomings of the systems. One notable exception (Niu and Penn, 2019) establishes a taxonomy of annotation errors, although this is for a single corpus (ATIS (Hemphill et al., 1990)). 5036 Type Example Explanation 1. change twenty us dollars to the british pound AMOUNT SOURCE These are cases where the boundaries of a slot are inconsistent. In these examples, “british” is part of TARGET in 1 but not 3, and “the” is part of TARGET in"
2020.coling-main.442,H90-1021,0,0.82941,"pe Categorization Another line of work has explored automatic identification of error types for tasks such as constituency parsing (Kummerfeld et al., 2012; Kummerfeld et al., 2013), coreference resolution (Kummerfeld and Klein, 2013), semantic role labeling (He et al., 2017), and slot-filling (B´echet and Raymond, 2018). However, they focus on evaluating system outputs in comparison to a gold standard reference in order to understand shortcomings of the systems. One notable exception (Niu and Penn, 2019) establishes a taxonomy of annotation errors, although this is for a single corpus (ATIS (Hemphill et al., 1990)). 5036 Type Example Explanation 1. change twenty us dollars to the british pound AMOUNT SOURCE These are cases where the boundaries of a slot are inconsistent. In these examples, “british” is part of TARGET in 1 but not 3, and “the” is part of TARGET in 2 but not 1, and “dollars” is part of SOURCE in 1 but not 2. TARGET 2. convert 1200 of new zealand dollars to the cad Slot Format AMOUNT SOURCE TARGET 3. what is the exchange rate between pesos and british pounds SOURCE TARGET 1. if i have pounds then how many japanese yen would that be Omission When an unlabeled span should be labeled as a sl"
2020.coling-main.442,L16-1629,0,0.017581,"ging has received particular attention, with a range of methods based on model scores (Abney et al., 1999; Eskin, 2000; Matsumoto and Yamashita, 2000; van Halteren, 2000; Ma et al., 2001; Nakagawa and Matsumoto, 2002). One particular method based on variation in POS tags of n-grams (Dickinson and Meurers, 2003) has been extended to predicate-argument relations (Dickinson and Lee, 2008), and dependency parses (Dickinson, 2010; Dickinson and Smith, 2011). More recent work has explored automatic identification methods for word sense and multi-word-expression annotation (Dligach and Palmer, 2011; Hollenstein et al., 2016). 2.2 Crowdsourcing Quality A range of options have been developed for improving crowdsourcing quality. The most common approach is to collect multiple annotations and then aggregate them (Hovy et al., 2013; Passonneau and Carpenter, 2014; Parde and Nielsen, 2017; Dumitrache et al., 2018). This can identify inconsistencies, but at significant cost as each example must be annotated multiple times. Less expensive options include using examples with known answers to test worker attention (Oppenheimer et al., 2009), or filtering workers based on qualification tasks or preliminary annotation (Li an"
2020.coling-main.442,N13-1132,0,0.0339744,"02). One particular method based on variation in POS tags of n-grams (Dickinson and Meurers, 2003) has been extended to predicate-argument relations (Dickinson and Lee, 2008), and dependency parses (Dickinson, 2010; Dickinson and Smith, 2011). More recent work has explored automatic identification methods for word sense and multi-word-expression annotation (Dligach and Palmer, 2011; Hollenstein et al., 2016). 2.2 Crowdsourcing Quality A range of options have been developed for improving crowdsourcing quality. The most common approach is to collect multiple annotations and then aggregate them (Hovy et al., 2013; Passonneau and Carpenter, 2014; Parde and Nielsen, 2017; Dumitrache et al., 2018). This can identify inconsistencies, but at significant cost as each example must be annotated multiple times. Less expensive options include using examples with known answers to test worker attention (Oppenheimer et al., 2009), or filtering workers based on qualification tasks or preliminary annotation (Li and Liu, 2015; Roit et al., 2020). However, these primarily address worker attentiveness, which does not cover all the inconsistency types we consider. In particular, we also cover inconsistencies related to"
2020.coling-main.442,D13-1027,1,0.805027,"worker attention (Oppenheimer et al., 2009), or filtering workers based on qualification tasks or preliminary annotation (Li and Liu, 2015; Roit et al., 2020). However, these primarily address worker attentiveness, which does not cover all the inconsistency types we consider. In particular, we also cover inconsistencies related to subtle cases without an obvious correct answer. 2.3 Error Type Categorization Another line of work has explored automatic identification of error types for tasks such as constituency parsing (Kummerfeld et al., 2012; Kummerfeld et al., 2013), coreference resolution (Kummerfeld and Klein, 2013), semantic role labeling (He et al., 2017), and slot-filling (B´echet and Raymond, 2018). However, they focus on evaluating system outputs in comparison to a gold standard reference in order to understand shortcomings of the systems. One notable exception (Niu and Penn, 2019) establishes a taxonomy of annotation errors, although this is for a single corpus (ATIS (Hemphill et al., 1990)). 5036 Type Example Explanation 1. change twenty us dollars to the british pound AMOUNT SOURCE These are cases where the boundaries of a slot are inconsistent. In these examples, “british” is part of TARGET in 1"
2020.coling-main.442,D12-1096,1,0.781815,". Less expensive options include using examples with known answers to test worker attention (Oppenheimer et al., 2009), or filtering workers based on qualification tasks or preliminary annotation (Li and Liu, 2015; Roit et al., 2020). However, these primarily address worker attentiveness, which does not cover all the inconsistency types we consider. In particular, we also cover inconsistencies related to subtle cases without an obvious correct answer. 2.3 Error Type Categorization Another line of work has explored automatic identification of error types for tasks such as constituency parsing (Kummerfeld et al., 2012; Kummerfeld et al., 2013), coreference resolution (Kummerfeld and Klein, 2013), semantic role labeling (He et al., 2017), and slot-filling (B´echet and Raymond, 2018). However, they focus on evaluating system outputs in comparison to a gold standard reference in order to understand shortcomings of the systems. One notable exception (Niu and Penn, 2019) establishes a taxonomy of annotation errors, although this is for a single corpus (ATIS (Hemphill et al., 1990)). 5036 Type Example Explanation 1. change twenty us dollars to the british pound AMOUNT SOURCE These are cases where the boundaries"
2020.coling-main.442,P13-2018,1,0.814773,"include using examples with known answers to test worker attention (Oppenheimer et al., 2009), or filtering workers based on qualification tasks or preliminary annotation (Li and Liu, 2015; Roit et al., 2020). However, these primarily address worker attentiveness, which does not cover all the inconsistency types we consider. In particular, we also cover inconsistencies related to subtle cases without an obvious correct answer. 2.3 Error Type Categorization Another line of work has explored automatic identification of error types for tasks such as constituency parsing (Kummerfeld et al., 2012; Kummerfeld et al., 2013), coreference resolution (Kummerfeld and Klein, 2013), semantic role labeling (He et al., 2017), and slot-filling (B´echet and Raymond, 2018). However, they focus on evaluating system outputs in comparison to a gold standard reference in order to understand shortcomings of the systems. One notable exception (Niu and Penn, 2019) establishes a taxonomy of annotation errors, although this is for a single corpus (ATIS (Hemphill et al., 1990)). 5036 Type Example Explanation 1. change twenty us dollars to the british pound AMOUNT SOURCE These are cases where the boundaries of a slot are inconsistent"
2020.coling-main.442,D19-1131,1,0.816204,"abel 9.4 35.2 25.4 4.8 12.4 4.2 2.9 5.0 1.8 Swapped Label 6.3 0.0 0.6 ChopJoin Other Any 0.0 0.0 0.6 1.3 12.0 0.6 27.5 67.6 44.6 Table 3: The percentage of samples in each dataset with each inconsistency type. 3.2 Datasets We crowdsourced slot annotations for datasets in three domains to analyze the frequency of each inconsistency type and the effect inconsistencies have on slot-filling model performance. Table 2 shows examples from each dataset and the number of different slot types. The sentences used for two (Forex and Companies) were collected using the crowdsourcing approach described by Larson et al. (2019) and sentences for the third (Flights) were sampled from prior work (Jaech et al., 2016). Forex This dataset consists of short queries about currency exchange rates. Crowd workers were asked to annotate spans with three slots: SOURCE, TARGET, and AMOUNT. Companies This contains queries about metrics measuring properties of companies. There are seven slot types: SECTOR NAME, METRIC, NUMERIC, LOCATION OUTSIDE NAME, LOCATION INSIDE NAME, RANK, and DATE METRIC. Annotating this dataset is challenging because there are more slot types and the setting is probably unfamiliar to most crowd workers due"
2020.coling-main.442,2020.lrec-1.873,1,0.709558,"le inconsistencies. 3.3 Manually Labeling Inconsistencies Once annotated by the crowd, we manually checked each annotated sample for inconsistencies. Inconsistencies in the data were labelled independently by two of the authors of this paper, who then discussed and reconciled any disagreements. We did not predefine an annotation policy for every slot, particularly the Slot Format inconsistencies. Instead, we looked at the data and followed the dominant behavior of workers over all similar annotations in a dataset, labeling annotations in the minority as inconsistencies. We used a search tool (Larson et al., 2020) to help find all relevant samples to determine the majority annotation scheme. For the Flights dataset we also used the original annotations to help identify inconsistencies (but again, followed the crowd majority conventions even when they deviated from the original data). 5039 Dataset Train Raw Crowd Annotations (Crowd) Slot Format Inconsistencies Resolved (Formatted) Consistent Annotations (Consistent) Forex Companies Flights 0.83 0.86 0.95 0.51 0.57 0.90 0.77 0.87 0.94 Table 4: Slot-filling F1 scores when training on data with different levels of inconsistency. The evaluation sets are com"
2020.coling-main.442,matsumoto-yamashita-2000-using,0,0.296399,"sible, but there is scope to further improve the tradeoff between effort and data quality. This work provides the basis for a new direction in research for addressing inconsistencies in crowdsourcing by defining a typology of inconsistency types, collecting new benchmark datasets, and exploring directions for automatic identification of inconsistencies. 2 2.1 Related Work Annotation Consistency Inconsistencies have been studied across a wide range of tasks. Part-of-speech tagging has received particular attention, with a range of methods based on model scores (Abney et al., 1999; Eskin, 2000; Matsumoto and Yamashita, 2000; van Halteren, 2000; Ma et al., 2001; Nakagawa and Matsumoto, 2002). One particular method based on variation in POS tags of n-grams (Dickinson and Meurers, 2003) has been extended to predicate-argument relations (Dickinson and Lee, 2008), and dependency parses (Dickinson, 2010; Dickinson and Smith, 2011). More recent work has explored automatic identification methods for word sense and multi-word-expression annotation (Dligach and Palmer, 2011; Hollenstein et al., 2016). 2.2 Crowdsourcing Quality A range of options have been developed for improving crowdsourcing quality. The most common appr"
2020.coling-main.442,C02-1101,0,0.204853,"fort and data quality. This work provides the basis for a new direction in research for addressing inconsistencies in crowdsourcing by defining a typology of inconsistency types, collecting new benchmark datasets, and exploring directions for automatic identification of inconsistencies. 2 2.1 Related Work Annotation Consistency Inconsistencies have been studied across a wide range of tasks. Part-of-speech tagging has received particular attention, with a range of methods based on model scores (Abney et al., 1999; Eskin, 2000; Matsumoto and Yamashita, 2000; van Halteren, 2000; Ma et al., 2001; Nakagawa and Matsumoto, 2002). One particular method based on variation in POS tags of n-grams (Dickinson and Meurers, 2003) has been extended to predicate-argument relations (Dickinson and Lee, 2008), and dependency parses (Dickinson, 2010; Dickinson and Smith, 2011). More recent work has explored automatic identification methods for word sense and multi-word-expression annotation (Dligach and Palmer, 2011; Hollenstein et al., 2016). 2.2 Crowdsourcing Quality A range of options have been developed for improving crowdsourcing quality. The most common approach is to collect multiple annotations and then aggregate them (Hov"
2020.coling-main.442,P19-1550,0,0.014165,"icular, we also cover inconsistencies related to subtle cases without an obvious correct answer. 2.3 Error Type Categorization Another line of work has explored automatic identification of error types for tasks such as constituency parsing (Kummerfeld et al., 2012; Kummerfeld et al., 2013), coreference resolution (Kummerfeld and Klein, 2013), semantic role labeling (He et al., 2017), and slot-filling (B´echet and Raymond, 2018). However, they focus on evaluating system outputs in comparison to a gold standard reference in order to understand shortcomings of the systems. One notable exception (Niu and Penn, 2019) establishes a taxonomy of annotation errors, although this is for a single corpus (ATIS (Hemphill et al., 1990)). 5036 Type Example Explanation 1. change twenty us dollars to the british pound AMOUNT SOURCE These are cases where the boundaries of a slot are inconsistent. In these examples, “british” is part of TARGET in 1 but not 3, and “the” is part of TARGET in 2 but not 1, and “dollars” is part of SOURCE in 1 but not 2. TARGET 2. convert 1200 of new zealand dollars to the cad Slot Format AMOUNT SOURCE TARGET 3. what is the exchange rate between pesos and british pounds SOURCE TARGET 1. if"
2020.coling-main.442,D17-1204,0,0.136084,"data annotation is completed via small tasks with brief instructions completed by non-expert crowd workers. Collecting high quality data in this setting is challenging, involving multiple rounds of pilot annotations and analysis to develop suitable instructions (Alonso et al., 2015). Figure 1 shows examples of inconsistencies in annotations performed by crowd workers. Training on data with these issues will lead to lower quality models, which in turn decrease the effectiveness of the overall dialog system. Most research on improving data quality has focused on mechanisms such as aggregation (Parde and Nielsen, 2017), worker filtering (Li and Liu, 2015), and attention checks (Oppenheimer et al., 2009). These all raise costs and primarily address clear inconsistencies (such as in examples 1, 4, 5, and 6) but not more subtle cases like the inclusion of “dollar” in examples 2 and 3. Additionally, annotation-asa-service (e.g., scale.ai) is being widely used by developers but often cannot be customized by the developer to add these kinds of mechanisms. If we could identify all of these inconsistencies then we could fix them without expending effort on examples that were correctly annotated and we could improve"
2020.coling-main.442,Q14-1025,0,0.0258598,"method based on variation in POS tags of n-grams (Dickinson and Meurers, 2003) has been extended to predicate-argument relations (Dickinson and Lee, 2008), and dependency parses (Dickinson, 2010; Dickinson and Smith, 2011). More recent work has explored automatic identification methods for word sense and multi-word-expression annotation (Dligach and Palmer, 2011; Hollenstein et al., 2016). 2.2 Crowdsourcing Quality A range of options have been developed for improving crowdsourcing quality. The most common approach is to collect multiple annotations and then aggregate them (Hovy et al., 2013; Passonneau and Carpenter, 2014; Parde and Nielsen, 2017; Dumitrache et al., 2018). This can identify inconsistencies, but at significant cost as each example must be annotated multiple times. Less expensive options include using examples with known answers to test worker attention (Oppenheimer et al., 2009), or filtering workers based on qualification tasks or preliminary annotation (Li and Liu, 2015; Roit et al., 2020). However, these primarily address worker attentiveness, which does not cover all the inconsistency types we consider. In particular, we also cover inconsistencies related to subtle cases without an obvious"
2020.coling-main.442,2020.acl-main.626,0,0.0192322,"urcing Quality A range of options have been developed for improving crowdsourcing quality. The most common approach is to collect multiple annotations and then aggregate them (Hovy et al., 2013; Passonneau and Carpenter, 2014; Parde and Nielsen, 2017; Dumitrache et al., 2018). This can identify inconsistencies, but at significant cost as each example must be annotated multiple times. Less expensive options include using examples with known answers to test worker attention (Oppenheimer et al., 2009), or filtering workers based on qualification tasks or preliminary annotation (Li and Liu, 2015; Roit et al., 2020). However, these primarily address worker attentiveness, which does not cover all the inconsistency types we consider. In particular, we also cover inconsistencies related to subtle cases without an obvious correct answer. 2.3 Error Type Categorization Another line of work has explored automatic identification of error types for tasks such as constituency parsing (Kummerfeld et al., 2012; Kummerfeld et al., 2013), coreference resolution (Kummerfeld and Klein, 2013), semantic role labeling (He et al., 2017), and slot-filling (B´echet and Raymond, 2018). However, they focus on evaluating system"
2020.coling-main.442,W00-1907,0,0.481003,"Missing"
2020.coling-main.442,N19-1026,0,0.0612706,"Missing"
2020.coling-main.604,P14-2134,0,0.257714,"Missing"
2020.coling-main.604,P19-1285,0,0.0630832,"Missing"
2020.coling-main.604,C04-1088,0,0.139666,"Missing"
2020.coling-main.604,O16-2003,0,0.141751,"Missing"
2020.coling-main.604,P18-2111,0,0.0859153,"d up text entry. Another application is dialog systems that follow the speaking style of certain professionals (e.g., counselors, advisors). Finally, personalized word representations could particularly help users with atypical writing styles that are not currently well served by models trained to suit the majority. 2 Related Work Prior work has considered user embeddings, where one vector is learned for each user in the data (we learn a set of vectors per user, one for each word in the vocabulary). User embeddings have been used for dialog generation (Li et al., 2016), query auto-completion (Jaech and Ostendorf, 2018), authorship attribution (Ebrahimi and Dou, 2016), and sarcasm detection (Kolchinski and Potts, 2018). Amer et al. (2016) learn a set of embeddings from the books that a user adds to their profile. Some approaches also use network information (Zeng et al., 2017; Huang et al., 2016). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6856 Proceedings of the 28th International Conference on Computational Linguistics, pages 6856–6862 Barcelona, Spain (Online), December 8-13, 2020 User Example Use Nea"
2020.coling-main.604,D18-1140,0,0.293882,"fessionals (e.g., counselors, advisors). Finally, personalized word representations could particularly help users with atypical writing styles that are not currently well served by models trained to suit the majority. 2 Related Work Prior work has considered user embeddings, where one vector is learned for each user in the data (we learn a set of vectors per user, one for each word in the vocabulary). User embeddings have been used for dialog generation (Li et al., 2016), query auto-completion (Jaech and Ostendorf, 2018), authorship attribution (Ebrahimi and Dou, 2016), and sarcasm detection (Kolchinski and Potts, 2018). Amer et al. (2016) learn a set of embeddings from the books that a user adds to their profile. Some approaches also use network information (Zeng et al., 2017; Huang et al., 2016). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6856 Proceedings of the 28th International Conference on Computational Linguistics, pages 6856–6862 Barcelona, Spain (Online), December 8-13, 2020 User Example Use Nearest Neighbors A B All doctors think this is bad for her health ... it is usually bad for your health"
2020.coling-main.604,P16-1094,0,0.295155,"xt generation for auto-completion to speed up text entry. Another application is dialog systems that follow the speaking style of certain professionals (e.g., counselors, advisors). Finally, personalized word representations could particularly help users with atypical writing styles that are not currently well served by models trained to suit the majority. 2 Related Work Prior work has considered user embeddings, where one vector is learned for each user in the data (we learn a set of vectors per user, one for each word in the vocabulary). User embeddings have been used for dialog generation (Li et al., 2016), query auto-completion (Jaech and Ostendorf, 2018), authorship attribution (Ebrahimi and Dou, 2016), and sarcasm detection (Kolchinski and Potts, 2018). Amer et al. (2016) learn a set of embeddings from the books that a user adds to their profile. Some approaches also use network information (Zeng et al., 2017; Huang et al., 2016). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6856 Proceedings of the 28th International Conference on Computational Linguistics, pages 6856–6862 Barcelona, Spain"
2020.coling-main.604,P19-1542,0,0.0463705,"onduct, experiences, online medical, preventative, insurance, safety, healthcare Table 1: Nearest neighbors of “health” for two personalized embedding spaces and the generic space. Personalization has been studied for marketing, webpage layout, recommendations, query completion, and dialog (Eirinaki and Vazirgiannis, 2003; Das et al., 2007). Our prior work (Welch et al., 2019a; Welch et al., 2019b) explored predicting response time, common messages, and author relationships from personal conversation data. Zhang et al. (2018) conditioned dialog systems on artificially constructed personas and Madotto et al. (2019) used meta-learning to improve this process. Goal-oriented dialog has used demographics (i.e., age, gender) to condition system response generation, showing that this relatively coarse grained personalization improves system performance (Joshi et al., 2017). 3 Personalized Word Embeddings Definition. Personalized word embeddings are vector representations of words derived from the text produced by a single author. We use the text produced by a Reddit user s in their posts Cs to create their word embeddings. We apply the method described below to this set and produce an embedding matrix, |V |×k"
2020.coling-main.604,P14-5010,0,0.00861709,"Missing"
2020.coling-main.604,D14-1162,0,0.09167,"Missing"
2020.coling-main.604,N03-1033,0,0.263913,"Missing"
2020.coling-main.604,2020.emnlp-main.334,1,0.7814,"Missing"
2020.coling-main.604,P18-1205,0,0.0269034,"alth ... N/A preventative, insurance, reform, medical, education professional, mental, conduct, experiences, online medical, preventative, insurance, safety, healthcare Table 1: Nearest neighbors of “health” for two personalized embedding spaces and the generic space. Personalization has been studied for marketing, webpage layout, recommendations, query completion, and dialog (Eirinaki and Vazirgiannis, 2003; Das et al., 2007). Our prior work (Welch et al., 2019a; Welch et al., 2019b) explored predicting response time, common messages, and author relationships from personal conversation data. Zhang et al. (2018) conditioned dialog systems on artificially constructed personas and Madotto et al. (2019) used meta-learning to improve this process. Goal-oriented dialog has used demographics (i.e., age, gender) to condition system response generation, showing that this relatively coarse grained personalization improves system performance (Joshi et al., 2017). 3 Personalized Word Embeddings Definition. Personalized word embeddings are vector representations of words derived from the text produced by a single author. We use the text produced by a Reddit user s in their posts Cs to create their word embedding"
2020.emnlp-main.334,2020.acl-main.236,0,0.0388493,"Missing"
2020.emnlp-main.334,P14-2134,0,0.382498,"Missing"
2020.emnlp-main.334,W18-1112,0,0.0515852,"Missing"
2020.emnlp-main.334,P15-1073,0,0.0515631,"underrepresented groups and these contextualized models require billions of tokens for training. Recent work has also shown that static embeddings are competitive with contextualized ones in some settings (Arora et al., 2020). Personalization. The closest work is Garimella et al. (2017)’s exploration of demographic-specific word embedding spaces. They trained word embeddings for male and female speakers who live in the USA and India using skip-gram architectures that learn a separate word matrix for each demographic group (e.g., male speakers from the USA). Another line of work used discrete (Hovy, 2015) or continuous values (Lynn et al., 2017) to learn speaker embeddings: a single vector for each user. The speaker embedding is appended to the input of the recurrent or output layer, and trained simultaneously with the rest of the model. This idea applies to any contextual information type and was introduced as a way to condition language models on topics learned by topic modeling (Mikolov and Zweig, 2012). It has since been used as a way of representing users in tasks such as task-oriented and open-domain dialog (Wen et al., 2013; Li et al., 2016), information retrieval based on book preferen"
2020.emnlp-main.334,P15-2079,0,0.0172264,"n both cases, we show that our proposed embeddings improve performance over generic word representations. 2 Related Work Embedding Bias. Recent work on embeddings has revealed and attempted to remove racial, gender, religious, and other biases (Manzini et al., 2019; Bolukbasi et al., 2016). The bias in our corpora and embeddings have a societal impact and risks exclusion and demographic misrepresentation (Hovy and Spruit, 2016). This means that users of certain regions, ages, or genders may find NLP technologies more difficult to use. For instance, when using standard corpora for POS tagging, Hovy and Søgaard (2015) found that models perform significantly lower on younger people and ethnic minorities. Similarly, results on textbased geotagging show best results for men over 40 (Pavalanathan and Eisenstein, 2015). Similar results are starting to be found in embeddings produced by contextual embedding methods (May et al., 2019; Kurita et al., 2019). We focus on non-contextual embedding methods because of their computational efficiency, which is crucial if many separate representations are being learned. Additionally, there may not be a large amount of available data for underrepresented groups and these co"
2020.emnlp-main.334,P16-2096,0,0.0225756,"ations of using or drawing conclusions from this method. We explore the value of compositional demographic word embeddings on two English NLP tasks: language modeling and word associations. In both cases, we show that our proposed embeddings improve performance over generic word representations. 2 Related Work Embedding Bias. Recent work on embeddings has revealed and attempted to remove racial, gender, religious, and other biases (Manzini et al., 2019; Bolukbasi et al., 2016). The bias in our corpora and embeddings have a societal impact and risks exclusion and demographic misrepresentation (Hovy and Spruit, 2016). This means that users of certain regions, ages, or genders may find NLP technologies more difficult to use. For instance, when using standard corpora for POS tagging, Hovy and Søgaard (2015) found that models perform significantly lower on younger people and ethnic minorities. Similarly, results on textbased geotagging show best results for men over 40 (Pavalanathan and Eisenstein, 2015). Similar results are starting to be found in embeddings produced by contextual embedding methods (May et al., 2019; Kurita et al., 2019). We focus on non-contextual embedding methods because of their computa"
2020.emnlp-main.334,P19-1285,0,0.0705302,"Missing"
2020.emnlp-main.334,O16-2003,0,0.205684,"a applies to any contextual information type and was introduced as a way to condition language models on topics learned by topic modeling (Mikolov and Zweig, 2012). It has since been used as a way of representing users in tasks such as task-oriented and open-domain dialog (Wen et al., 2013; Li et al., 2016), information retrieval based on book preferences (Amer et al., 2016), query auto-completion (Jaech and Ostendorf, 2018), authorship attribution (Ebrahimi and Dou, 2016), sarcasm detection (Kolchinski and Potts, 2018), sentiment analysis (Zeng et al., 2017), and cold-start language modeling Huang et al. (2016). Finally, a recent study by King and Cook (2020) compared how to improve a language model with user-specific data using priming and interpolation, depending on the amount of data available, learning a new model for each user. More generally, personalization has been extensively applied to marketing, webpage layout, product and news recommendation, query completion, and dialog (Eirinaki and Vazirgiannis, 2003; Das et al., 2007). Welch et al. (2019a,b) explored predicting response time, common messages, and speaker relationships from personal conversation data. Zhang et al. (2018) conditioned d"
2020.emnlp-main.334,P18-2111,0,0.0755574,"rn speaker embeddings: a single vector for each user. The speaker embedding is appended to the input of the recurrent or output layer, and trained simultaneously with the rest of the model. This idea applies to any contextual information type and was introduced as a way to condition language models on topics learned by topic modeling (Mikolov and Zweig, 2012). It has since been used as a way of representing users in tasks such as task-oriented and open-domain dialog (Wen et al., 2013; Li et al., 2016), information retrieval based on book preferences (Amer et al., 2016), query auto-completion (Jaech and Ostendorf, 2018), authorship attribution (Ebrahimi and Dou, 2016), sarcasm detection (Kolchinski and Potts, 2018), sentiment analysis (Zeng et al., 2017), and cold-start language modeling Huang et al. (2016). Finally, a recent study by King and Cook (2020) compared how to improve a language model with user-specific data using priming and interpolation, depending on the amount of data available, learning a new model for each user. More generally, personalization has been extensively applied to marketing, webpage layout, product and news recommendation, query completion, and dialog (Eirinaki and Vazirgiannis, 2"
2020.emnlp-main.334,2020.lrec-1.299,0,0.0291453,"was introduced as a way to condition language models on topics learned by topic modeling (Mikolov and Zweig, 2012). It has since been used as a way of representing users in tasks such as task-oriented and open-domain dialog (Wen et al., 2013; Li et al., 2016), information retrieval based on book preferences (Amer et al., 2016), query auto-completion (Jaech and Ostendorf, 2018), authorship attribution (Ebrahimi and Dou, 2016), sarcasm detection (Kolchinski and Potts, 2018), sentiment analysis (Zeng et al., 2017), and cold-start language modeling Huang et al. (2016). Finally, a recent study by King and Cook (2020) compared how to improve a language model with user-specific data using priming and interpolation, depending on the amount of data available, learning a new model for each user. More generally, personalization has been extensively applied to marketing, webpage layout, product and news recommendation, query completion, and dialog (Eirinaki and Vazirgiannis, 2003; Das et al., 2007). Welch et al. (2019a,b) explored predicting response time, common messages, and speaker relationships from personal conversation data. Zhang et al. (2018) conditioned dialog systems on artificially constructed persona"
2020.emnlp-main.334,D18-1140,0,0.206186,"nput of the recurrent or output layer, and trained simultaneously with the rest of the model. This idea applies to any contextual information type and was introduced as a way to condition language models on topics learned by topic modeling (Mikolov and Zweig, 2012). It has since been used as a way of representing users in tasks such as task-oriented and open-domain dialog (Wen et al., 2013; Li et al., 2016), information retrieval based on book preferences (Amer et al., 2016), query auto-completion (Jaech and Ostendorf, 2018), authorship attribution (Ebrahimi and Dou, 2016), sarcasm detection (Kolchinski and Potts, 2018), sentiment analysis (Zeng et al., 2017), and cold-start language modeling Huang et al. (2016). Finally, a recent study by King and Cook (2020) compared how to improve a language model with user-specific data using priming and interpolation, depending on the amount of data available, learning a new model for each user. More generally, personalization has been extensively applied to marketing, webpage layout, product and news recommendation, query completion, and dialog (Eirinaki and Vazirgiannis, 2003; Das et al., 2007). Welch et al. (2019a,b) explored predicting response time, common messages"
2020.emnlp-main.334,W17-1601,0,0.0125312,"phic variables and only covers a subset of the potential values of each demographic. For instance, we do not use the same granularity across locations, include all locations, religions, or gender identities. We simplify age into ranges. The groups ‘secular’, ‘agnostic’, and ‘atheist’ are grouped into one broader group. Our sample is further biased by the choice of platform as each platform contains text from different populations. Users in our sample are predominately young, male, atheist, and live in the United States. When using gender as a study variable, we followed the recommendations of Larson (2017). Our “gender” extraction method does not refer to biological sex. After running gender extraction patterns, users are assigned to either the ‘male’, ‘female’, or ‘unknown’ label, meaning that on the basis of these phrases one’s gender identity is assumed to be binary or to be a gender identity unknown to our model, which may include those who are transgender, non-binary, or those who do not wish to disclose their gender. However, we are aware that the use of regular expressions for the extraction of demographic attributes can lead to false positives and false negatives (error rates are provid"
2020.emnlp-main.334,P16-1094,0,0.285363,"from the USA). Another line of work used discrete (Hovy, 2015) or continuous values (Lynn et al., 2017) to learn speaker embeddings: a single vector for each user. The speaker embedding is appended to the input of the recurrent or output layer, and trained simultaneously with the rest of the model. This idea applies to any contextual information type and was introduced as a way to condition language models on topics learned by topic modeling (Mikolov and Zweig, 2012). It has since been used as a way of representing users in tasks such as task-oriented and open-domain dialog (Wen et al., 2013; Li et al., 2016), information retrieval based on book preferences (Amer et al., 2016), query auto-completion (Jaech and Ostendorf, 2018), authorship attribution (Ebrahimi and Dou, 2016), sarcasm detection (Kolchinski and Potts, 2018), sentiment analysis (Zeng et al., 2017), and cold-start language modeling Huang et al. (2016). Finally, a recent study by King and Cook (2020) compared how to improve a language model with user-specific data using priming and interpolation, depending on the amount of data available, learning a new model for each user. More generally, personalization has been extensively applied t"
2020.emnlp-main.334,2020.acl-main.592,0,0.0409865,"Missing"
2020.emnlp-main.334,D17-1119,0,0.0481594,"contextualized models require billions of tokens for training. Recent work has also shown that static embeddings are competitive with contextualized ones in some settings (Arora et al., 2020). Personalization. The closest work is Garimella et al. (2017)’s exploration of demographic-specific word embedding spaces. They trained word embeddings for male and female speakers who live in the USA and India using skip-gram architectures that learn a separate word matrix for each demographic group (e.g., male speakers from the USA). Another line of work used discrete (Hovy, 2015) or continuous values (Lynn et al., 2017) to learn speaker embeddings: a single vector for each user. The speaker embedding is appended to the input of the recurrent or output layer, and trained simultaneously with the rest of the model. This idea applies to any contextual information type and was introduced as a way to condition language models on topics learned by topic modeling (Mikolov and Zweig, 2012). It has since been used as a way of representing users in tasks such as task-oriented and open-domain dialog (Wen et al., 2013; Li et al., 2016), information retrieval based on book preferences (Amer et al., 2016), query auto-compl"
2020.emnlp-main.334,P19-1542,0,0.0330596,"red how to improve a language model with user-specific data using priming and interpolation, depending on the amount of data available, learning a new model for each user. More generally, personalization has been extensively applied to marketing, webpage layout, product and news recommendation, query completion, and dialog (Eirinaki and Vazirgiannis, 2003; Das et al., 2007). Welch et al. (2019a,b) explored predicting response time, common messages, and speaker relationships from personal conversation data. Zhang et al. (2018) conditioned dialog systems on artificially constructed personas and Madotto et al. (2019) used meta-learning to improve this process. Goal-oriented dialog has used demographics (i.e. age, gender) to condition system response generation, showing that this relatively coarse grained personalization improves system performance (Joshi et al., 2017). Social Media. We use social media data with demographic attributes inferred from user posts. Prior work has explored extraction or prediction of attributes such as age, gender, region, and political orientation (Rao et al., 2010; Rangel et al., 2013). Work on analyzing the demographics of social media users also includes race/ethnicity, inc"
2020.emnlp-main.334,P14-5010,0,0.00540837,"Missing"
2020.emnlp-main.334,N19-1062,0,0.0285017,"ender, and religion. We examine differences in word usage and association captured by the demographics we extracted and discuss the limitations and ethical considerations of using or drawing conclusions from this method. We explore the value of compositional demographic word embeddings on two English NLP tasks: language modeling and word associations. In both cases, we show that our proposed embeddings improve performance over generic word representations. 2 Related Work Embedding Bias. Recent work on embeddings has revealed and attempted to remove racial, gender, religious, and other biases (Manzini et al., 2019; Bolukbasi et al., 2016). The bias in our corpora and embeddings have a societal impact and risks exclusion and demographic misrepresentation (Hovy and Spruit, 2016). This means that users of certain regions, ages, or genders may find NLP technologies more difficult to use. For instance, when using standard corpora for POS tagging, Hovy and Søgaard (2015) found that models perform significantly lower on younger people and ethnic minorities. Similarly, results on textbased geotagging show best results for men over 40 (Pavalanathan and Eisenstein, 2015). Similar results are starting to be found"
2020.emnlp-main.334,N19-1063,0,0.135391,"ual embeddings may give similar representations, but it has different salient meanings in the personal space of each user. User A tends to talk more about preventative care and insurance, while user B tends to talk about people’s experiences affecting their mental health. The typical approach in natural language processing (NLP) is to use one-size-fits-all language representations, which do not account for variation between people. This may not matter for people whose language style is well represented in the data, but could lead to worse support for others (Pavalanathan and Eisenstein, 2015; May et al., 2019; Kurita et al., 2019). While the way we produce language is not a direct consequence of our demographics or any other grouping, it is possible that by tailoring word embeddings to a group we can more effectively model and support the way they use language. Additionally, personalized embeddings can be useful for applications such as predictive typing systems that auto-complete sentences by providing suggestions to users, or dialog systems that follow the style of certain individuals or professionals (e.g., counselors, advisors). They can also be used to match the communication style of a user,"
2020.emnlp-main.334,D18-1298,0,0.0436508,"Missing"
2020.emnlp-main.334,D15-1256,0,0.0386318,"Missing"
2020.emnlp-main.334,D14-1162,0,0.0983371,"A doctors think this is bad for her health ... preventative, insurance, reform, medical, education B it is usually bad for your health ... professional, mental, conduct, experiences, online All N/A medical, preventative, insurance, safety, healthcare Table 1: Nearest neighbors of the word “health” for two different users in personalized and a generic embedding space. Introduction Word embeddings are used in many natural language processing tasks as a way of representing language. Embeddings can be efficiently trained on large corpora using methods like word2vec or GloVe (Mikolov et al., 2013; Pennington et al., 2014), which learn one vector per word. These embeddings capture syntactic and semantic properties of the language of all individuals who contributed to the corpus. However, they are unable to account for user-specific word preferences (e.g., using the same word in different ways across different contexts), particularly for individuals whose usage deviates from the majority. These individual preferences are reflected in the word’s nearest neighbors. As an example, Table 1 shows the way two users use the word “health” and the word’s five nearest neighbors in their respective personalized embedding s"
2020.emnlp-main.334,2020.coling-main.604,1,0.7814,"Missing"
2020.emnlp-main.334,2020.emnlp-main.696,1,0.887735,"hics improves the most, but we also see improvements when only one demographic value is known. 4Dem Table 4: Perplexity on the demographic data. Our demographic-based approach improves performance. The difference between the last row and generic words is significant (p &lt; 0.00001 with a permutation test). We explored various hyperparameter configurations on our validation set and found the best results using dropout with the same mask for generic and demographic-specific embeddings, untied weights, and fixed input embeddings. Untying and fixing input embeddings is supported by concurrent work (Welch et al., 2020b). Each model is trained for 50 epochs. We use the version from the epoch that had the best validation set perplexity, a standard metric in language modeling that measures the accuracy of the predicted probability distribution. 5.1 2+Dem Gender In both experiments, we use the language model developed by Merity et al. (2018b,a). As discussed in § 2, this model was recently state-of-the-art and has been the basis of many variations. We modify it to initialize the word embeddings with the ones we provide and to concatenate multiple embedding vectors as input to the recurrent layers. The rest of"
2020.emnlp-main.334,P18-1205,0,0.0178778,"uage modeling Huang et al. (2016). Finally, a recent study by King and Cook (2020) compared how to improve a language model with user-specific data using priming and interpolation, depending on the amount of data available, learning a new model for each user. More generally, personalization has been extensively applied to marketing, webpage layout, product and news recommendation, query completion, and dialog (Eirinaki and Vazirgiannis, 2003; Das et al., 2007). Welch et al. (2019a,b) explored predicting response time, common messages, and speaker relationships from personal conversation data. Zhang et al. (2018) conditioned dialog systems on artificially constructed personas and Madotto et al. (2019) used meta-learning to improve this process. Goal-oriented dialog has used demographics (i.e. age, gender) to condition system response generation, showing that this relatively coarse grained personalization improves system performance (Joshi et al., 2017). Social Media. We use social media data with demographic attributes inferred from user posts. Prior work has explored extraction or prediction of attributes such as age, gender, region, and political orientation (Rao et al., 2010; Rangel et al., 2013)."
2020.emnlp-main.650,N19-1423,0,0.0111602,"“uncle sam” is a more creative though still reasonable phrase that we would want our systems to handle; and “shoes” instead of “tires”, which seems unlikely to occur naturally. These stranger cases are relatively rare, but may be worth filtering out with a checking process in future work. In general, these results indicate that current intent classification and slot-filling evaluation datasets are less than ideal insofar as they do not supply the diversity needed to train robust models. We posit that such datasets are also too easy 4.3 Models In both experiments, we use standard models: BERT (Devlin et al., 2019) for intent classification, and a Bi-LSTM for slot-filling. The Appendices show results using an SVM and FastText for intent classification, which showed the same trends as BERT, though more severe. More model details can be found in Appendix D. 5 5.1 Results Challenge Versions of Current Test Sets Tables 3a and 3b show the impact of collecting more diverse test cases using our approach. Performance consistently decreases as the number of taboo words increases from 0 to 4. Even with just two taboo words, the median performance drop for 8100 Round 2 Intent Taboo Words routing routing number hel"
2020.emnlp-main.650,D18-1300,0,0.0456082,"ion of expressions, only slowly filling in the long tail (if at all). This work provides a method to encourage crowd workers to cover the long tail by using constraints to promote diversity. Our results show that by collecting more diverse data, we can produce more robust and therefore useful models. 8097 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8097–8106, c November 16–20, 2020. 2020 Association for Computational Linguistics 2 Related Work 3 Crowdsourcing Dialog Data: Data for most recent task-oriented dialog datasets (Coucke et al., 2018; Gupta et al., 2018; Liu et al., 2019; Larson et al., 2019b), and custom dialog agents (Han et al., 2013; Iyer et al., 2017; Campagna et al., 2017; Ravichander et al., 2017; Shah et al., 2018) has been written by crowd workers via paraphrasing. Recent work has shown that diverse training data is important for robust dialog systems (Kang et al., 2018) and that a range of factors impact the diversity of utterances (Wang et al., 2012; Jiang et al., 2017). There has been some work on improving diversity using outlier detection (Larson et al., 2019a), and our idea is orthogonal to this approach. Taboo Lists: von Ahn"
2020.emnlp-main.650,H90-1021,0,0.682238,"Missing"
2020.emnlp-main.650,P17-1089,0,0.0142148,"urage crowd workers to cover the long tail by using constraints to promote diversity. Our results show that by collecting more diverse data, we can produce more robust and therefore useful models. 8097 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8097–8106, c November 16–20, 2020. 2020 Association for Computational Linguistics 2 Related Work 3 Crowdsourcing Dialog Data: Data for most recent task-oriented dialog datasets (Coucke et al., 2018; Gupta et al., 2018; Liu et al., 2019; Larson et al., 2019b), and custom dialog agents (Han et al., 2013; Iyer et al., 2017; Campagna et al., 2017; Ravichander et al., 2017; Shah et al., 2018) has been written by crowd workers via paraphrasing. Recent work has shown that diverse training data is important for robust dialog systems (Kang et al., 2018) and that a range of factors impact the diversity of utterances (Wang et al., 2012; Jiang et al., 2017). There has been some work on improving diversity using outlier detection (Larson et al., 2019a), and our idea is orthogonal to this approach. Taboo Lists: von Ahn and Dabbish (2004)’s ESP game introduced the taboo list idea that we extend. In their game, a pair of pl"
2020.emnlp-main.650,D19-1131,1,0.846174,"Missing"
2020.emnlp-main.650,P19-1550,0,0.0127118,"y and language modifications. Task Training Test Data Data same unique taboo mixed Intent Classification same unique taboo mixed 99.3 98.7 99.0 99.0 83.2 98.4 89.7 98.8 83.6 80.9 97.6 97.6 88.6 92.7 95.4 98.5 Slot Extraction same unique taboo mixed 90.9 90.1 90.1 95.8 75.8 80.4 77.2 90.9 77.0 75.0 84.9 90.6 81.0 81.7 84.0 92.3 Table 6: Model performance for various combinations of training and test data collection methods. due to this lack of diversity, and do not sufficiently test a model’s ability to generalize. These observations are complementary to recent work (B´echet and Raymond, 2018; Niu and Penn, 2019; Larson et al., 2020) that found the ATIS dataset in particular to lack sufficient diversity to evaluate modern slot-filling models. 5.2 Robust Data Collection Table 6 presents accuracy (top) and F1 (bottom) for models trained and tested with different data collection methods. As expected, mixed is consistently the best approach. Ignoring mixed, the highest scores are on the diagonals: classifiers trained and tested on data collected using the same method perform the strongest. Looking at the off-diagonals, it seems that taboo and unique are introducing different types of diversity. Both meth"
2020.emnlp-main.650,W10-0209,0,0.118506,"lementary to recent work on improving data diversity, and training on data collected with our approach leads to more robust models. 1 Introduction Crowdsourcing is widely used to collect data, including cases where workers are writing new text, such as questions (Rajpurkar et al., 2016), dialog (Budzianowski et al., 2018), and captions (Russakovsky et al., 2015). To avoid repetition of short labels for images, von Ahn and Dabbish (2004) proposed using a taboo list, preventing workers from writing labels that previous workers had written. This idea has since been applied to emotion annotation (Pearl and Steyvers, 2010) and word association (Vickrey et al., 2008; Lafourcade and Joubert, 2012). However, in all of these cases the constraint is that there cannot be an exact match with another label. This limits the approach to tasks where workers write a single word or a short phrase. Meanwhile, recent work on dialog has found that crowdsourced data can have limited diversity (Jiang et al., 2017; Kang et al., 2018; Larson et al., 2019a). This limited diversity has dramatic consequences, as models trained on such data may not generalize well to unseen or uncommon inputs. ∗ † Corresponding email: stefan.dataset@g"
2020.emnlp-main.696,2020.acl-main.236,0,0.0667457,"Missing"
2020.emnlp-main.696,D18-1549,0,0.0222574,"n Data Pretraining and Freezing. Word vectors are frequently used in downstream tasks and recent work has shown that their effectiveness depends on domain similarity (Peters et al., 2019; Arora et al., 2020) For language modeling, Kocmi and Bojar (2017) explored random and pretrained embeddings and found improvements, but did not consider tying and freezing. In-domain data is also useful for continuing to train contextual embedding models before fine-tuning (Gu et al., 2020; Gururangan et al., 2020), and for monolingual pretraining in machine translation (Neishi et al., 2017; Qi et al., 2018; Artetxe et al., 2018). This matches our observations, but does not cover the interactions between freezing and tying we consider. Handling Rare Words. These remain challenging even for large transformer models (Schick and Sch¨utze, 2020). Recent work has explored copying mechanisms and character based generation (Kawakami et al., 2017), with some success. These ideas are complementary to the results of our work, extending coverage to the open vocabulary case. Due to space and computational constraints we only consider English. For other languages, inflectional morphology and other factors may impact the effectiven"
2020.findings-emnlp.38,P17-2082,0,0.0418849,"Missing"
2020.findings-emnlp.38,C16-1058,1,0.911728,"by requiring majority agreement and no uncertainty. These difficult cases are then decided by experts with the necessary knowledge. Second, consider the challenge that there can be an overwhelming number of options. The Filter phase reduces the complexity of the task, focusing attention on likely options. This assumes that our filtering process removes unlikely options without removing the correct ones, which we verify experimentally in Section 5.1. Comparison Approaches In our experiments, we compare with three other data annotation methods. Automatic uses the output of a statistical model (Akbik and Li, 2016), with no human input. Review-Select uses a two phase process. First, five workers review the system prediction. If any worker marks the prediction as incorrect, another set of workers choose an answer and we assign the most common choice. Review-Expert uses the same review process as the previous approach, but an expert chooses the answer rather than the crowd. 3 For argument tasks, there is one more option “none of the above”, to cover situations where the automatic system assigns an argument to an incorrect predicate. 4 Experimental Setup We consider experiments on two sets of data, both fr"
2020.findings-emnlp.38,W15-1601,0,0.0340743,"Missing"
2020.findings-emnlp.38,N19-1224,0,0.0216722,"Missing"
2020.findings-emnlp.38,E14-4044,0,0.056425,"Missing"
2020.findings-emnlp.38,P18-1191,0,0.0426626,"Missing"
2020.findings-emnlp.38,P13-2130,0,0.0482145,"Missing"
2020.findings-emnlp.38,2020.lrec-1.30,0,0.0659117,"Missing"
2020.findings-emnlp.38,P17-1044,0,0.0355788,"Missing"
2020.findings-emnlp.38,D15-1076,0,0.063773,"Missing"
2020.findings-emnlp.38,W11-0404,0,0.055554,"Missing"
2020.findings-emnlp.38,N13-1062,0,0.0671724,"Missing"
2020.findings-emnlp.38,C12-2053,0,0.0434751,"Missing"
2020.findings-emnlp.38,N16-1104,0,0.0354739,"Missing"
2020.findings-emnlp.38,J05-1004,0,0.0652782,"Missing"
2020.findings-emnlp.38,P15-2067,0,0.0508123,"Missing"
2020.findings-emnlp.38,D16-1264,0,0.0459107,"Missing"
2020.findings-emnlp.38,2020.acl-main.626,0,0.0349925,"Missing"
2020.findings-emnlp.38,W11-0409,0,0.0507009,"Missing"
2020.findings-emnlp.38,D13-1170,0,0.00661981,"Missing"
2020.findings-emnlp.38,W13-0215,0,0.0779562,"Missing"
2020.findings-emnlp.38,D17-1205,1,0.881211,"Missing"
2020.findings-emnlp.38,P12-1087,0,0.0411489,"Missing"
2021.acl-short.44,N06-2015,0,0.130014,"Value All of the options above have tradeoffs that will be task dependent and in practise some combination is most likely to be the best approach. The first three have been studied in prior work, but the impact of lowering the threshold has not. In this section, we consider the quality of work completed by workers grouped by how many tasks they have previously completed and what percentage were accepted.3 4.1 Tasks Coreference Resolution This is an unusual task for crowdsourcing, with a novel user interface, shown in Figure 1. Workers were shown a 244 word document from the Ontonotes dataset (Hovy et al., 2006). We identified noun phrases using the Allen NLP parser (Gardner et al., 2018) and asked workers to identify when one of two spe2 One potential drawback of this approach is that the filtering step may produce a biased sample of workers. That may be problematic for more subjective tasks, though with a large enough sample, responses could be weighted to make the results more representative. 3 This was completed as part of a larger study approved by the Michigan IRB under study ID HUM00155689. Sentiment Analysis This task is very intuitive and has been crowdsourced extensively in the past. We clo"
2021.acl-short.44,2021.naacl-main.295,0,0.0364598,"hed to this paper 343 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 343–349 August 1–6, 2021. ©2021 Association for Computational Linguistics 2020), and developing tools to help workers address the power imbalance in the online workplace (Irani and Silberman, 2013, 2016). Concurrent with this work, another study showed that crowdsourcing is being used more each year in NLP research, and there is limited awareness of the ethical issues in this type of work (Shmueli et al., 2021). Prior work has considered hidden labour in the day-to-day work of the crowd (Hara et al., 2018). By observing a large set of workers, they measured time involved in searching for tasks, returned tasks, and breaks. Some of these issues have received additional attention, such as the wasted effort on tasks that are returned rather than completed (Han et al., 2019). While informative, those studies do not account for the hidden labour identified in this paper, which spans a long period and relates to worker qualifications. Part of this work uses online discussion between workers to understand t"
2021.acl-short.44,D13-1170,0,0.00294294,"18) and asked workers to identify when one of two spe2 One potential drawback of this approach is that the filtering step may produce a biased sample of workers. That may be problematic for more subjective tasks, though with a large enough sample, responses could be weighted to make the results more representative. 3 This was completed as part of a larger study approved by the Michigan IRB under study ID HUM00155689. Sentiment Analysis This task is very intuitive and has been crowdsourced extensively in the past. We closely followed the set up used to annotate the Stanford Sentiment Treebank (Socher et al., 2013), with the same task instructions. Workers were shown ten examples whose true scores were evenly spread across 0 to 1. We estimated that the task would take 4 minutes and paid workers 80 cents ($12 / hour). Three reviews of the task on TurkerView indicated that workers hourly earnings were $22.15, $48.00, and $50.53, suggesting that workers were faster than anticipated. 345 Figure 2: Results for all fifteen combinations of qualifications. Left (coreference): The percentage of workers scoring above 80 in each group. Right (sentiment): The percentage of workers whose average error was below 0.15"
2021.acl-short.44,2020.acl-main.626,0,0.0276274,"sks are poorly paid. We conducted a study of two tasks to understand how work quality correlates with these qualifications. We found that trends are task dependent, but lower thresholds can often be used. We recommend either not using the ”HITs accepted” qualification, or running preliminary tests to identify the lowest suitable threshold for your task. This calibration is necessary because worker performance depends on many factors, including the task type, data (including which language), user interface, and instructions. One particularly promising method is to use controlled crowdsourcing (Roit et al., 2020) with a low threshold: run a short task with low or no qualifications to identify workers, then for the full task only allow those workers to participate. This reduces the burden on workers while maintaining high quality work. Acknowledgements We would like to thank Judy Kay, Ellen Stuart, Greg Durrett, attendees at the Conference on Human Computation and Crowdsourcing, and the ACL reviewers for helpful feedback. This aterial is based in part on work supported by DARPA (grant #D19AP00079), Bloomberg (Data Science Research Grant), and the Allen Institute for AI (Key Scientific Challenges Progra"
2021.acl-short.44,sabou-etal-2014-corpus,0,0.0708134,"Missing"
2021.emnlp-main.476,W13-3520,0,0.0128216,"Missing"
2021.emnlp-main.476,2020.acl-main.236,0,0.0446804,"Missing"
2021.emnlp-main.476,W16-2501,0,0.0351993,"Missing"
2021.emnlp-main.476,N19-1423,0,0.0161792,"n downstream tasks, and work that uses embeddings to study specific properties of language. However, research to date on word embedding stability has been exclusively done on English and so is not representative of all languages. In this work, we explore the stability of word embeddings in a wide range of languages. Better understanding the differences caused by diverse languages will provide a foundation for building embeddings and NLP tools in all languages.1 In English and other very high resource languages, it has become common practice to use contextualized word embeddings, such as BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019). These algorithms require huge amounts of computational resources and data. For example, it takes 2.5 days to train XLNet with 512 TPU v3 chips. In addition to requiring heavy computational resources, most contextualized embedding algorithms need large amounts of data. BERT uses 3.3 billion words of training data. In contrast to these large corpora, many datasets from low-resource languages are fairly small (Maxwell and Hughes, 2006). To support scenarios where using huge amounts of data and computational resources is not feasible, it is important to continue dev"
2021.emnlp-main.476,W19-2501,0,0.0151433,"s need large amounts of data. BERT uses 3.3 billion words of training data. In contrast to these large corpora, many datasets from low-resource languages are fairly small (Maxwell and Hughes, 2006). To support scenarios where using huge amounts of data and computational resources is not feasible, it is important to continue developing our understanding of context-independent word embeddings, such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). These algorithms continue to be used in a wide variety of situations, including the computational humanities (Abdulrahim, 2019; Hellrich et al., 2019) and languages where only small corpora are available (Joshi et al., 2019). In this work, we consider how stability varies for different languages, and how linguistic properties are related to stability—a previously understudied relationship. Using regression modeling, we capture relationships between linguistic properties and average stability of a language, and we draw out insights about how linguistic features relate to stability. For instance, we find that embeddings in languages with more affixing tend to be less stable. Our findings provide crucial context for research that uses word emb"
2021.emnlp-main.476,2020.lrec-1.352,0,0.0426831,"te that work on The first part of our work is a comparison of stabilembedding evaluation should take into considera- ity across languages. Before presenting our meation stability, using multiple training runs to con- surements, we define stability and analyze some firm results. Similarly, stability should be con- important methodological decisions. sidered when studying the impact of embeddings on downstream tasks. Leszczynski et al. (2020) 2 Available online at https://sites.google.com/ specifically looked at the downstream instability site/rmyeid/projects/polyglot. 3 Available by contacting McCarthy et al. (2020). of word embeddings, and found that there is a 4 To work with a maximum number of languages, we only stability-memory tradeoff, and higher stability can consider the complete Protestant Bible (i.e., all of the verses be achieved by increasing the embedding dimen- that appear in the English King James Version of the Bible). 5 sion. Available online at https://wals.info. 5892 4.1.1 Model 1: indie, punk, progressive, pop, roll, band, blues, brass, class, alternative Model 2: punk, indie, alternative, progressive, band, sedimentary, bands, psychedelic, climbing, pop Model 3: punk, pop, indie, alt"
2021.emnlp-main.476,D14-1162,0,0.0917236,"e, it takes 2.5 days to train XLNet with 512 TPU v3 chips. In addition to requiring heavy computational resources, most contextualized embedding algorithms need large amounts of data. BERT uses 3.3 billion words of training data. In contrast to these large corpora, many datasets from low-resource languages are fairly small (Maxwell and Hughes, 2006). To support scenarios where using huge amounts of data and computational resources is not feasible, it is important to continue developing our understanding of context-independent word embeddings, such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). These algorithms continue to be used in a wide variety of situations, including the computational humanities (Abdulrahim, 2019; Hellrich et al., 2019) and languages where only small corpora are available (Joshi et al., 2019). In this work, we consider how stability varies for different languages, and how linguistic properties are related to stability—a previously understudied relationship. Using regression modeling, we capture relationships between linguistic properties and average stability of a language, and we draw out insights about how linguistic features relate to stability. For instan"
2021.emnlp-main.476,N18-4005,0,0.0770362,"We consider two widely used algorithms: word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). Our work analyzes embeddings in multiple languages, which is important because embeddings are commonly used across many languages. In particular, there has been interest in embeddings for low-resource languages (Chimalamarri et al., 2020; Stringham and Izbicki, 2020). In this work, we use stability to measure the quality of word embeddings. Similar to the work we present here on stability, other research looks at how nearest neighbors vary as properties of the embedding spaces change. Pierrejean and Tanguy (2018) found that the lowest frequency and the highest frequency words have the highest variation among nearest neighbors. Additional research has explored how semantic and syntactic properties of words change with different embedding algorithm and parameter choices (Artetxe et al., 2018; Yaghoobzadeh and Schütze, 2016). Unlike our work, previous studies only considered English. 3 Data In order to explore the stability of word embeddings in different languages, we work with two datasets, Wikipedia and the Bible. While Wikipedia has more data, the Bible covers more languages. Wikipedia is a comparabl"
2021.emnlp-main.476,C18-1228,0,0.0317626,"Missing"
2021.emnlp-main.476,2020.emnlp-main.285,0,0.039239,"), which often rely on raw embeddings created by GloVe or word2vec. If these embeddings are unstable, then research using them 1 Code is available at https://lit.eecs.umich. needs to take this into account in terms of methodedu/downloads.html. ologies and error analysis. 5891 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5891–5901 c November 7–11, 2021. 2021 Association for Computational Linguistics 2 Related Work Word embeddings are low-dimensional vectors used to represent words, normally in downstream tasks, such as word sense disambiguation (Scarlini et al., 2020) and text summarization (Moradi et al., 2020). They have been shown to capture both syntactic and semantic properties of words, making them useful in a wide range of NLP tasks (Wang et al., 2020b). In this work, we explore word embeddings that generate one embedding per word, regardless of the word’s context. We consider two widely used algorithms: word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). Our work analyzes embeddings in multiple languages, which is important because embeddings are commonly used across many languages. In particular, there has been interest in embeddi"
2021.emnlp-main.476,2020.eval4nlp-1.17,0,0.030617,"en shown to capture both syntactic and semantic properties of words, making them useful in a wide range of NLP tasks (Wang et al., 2020b). In this work, we explore word embeddings that generate one embedding per word, regardless of the word’s context. We consider two widely used algorithms: word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). Our work analyzes embeddings in multiple languages, which is important because embeddings are commonly used across many languages. In particular, there has been interest in embeddings for low-resource languages (Chimalamarri et al., 2020; Stringham and Izbicki, 2020). In this work, we use stability to measure the quality of word embeddings. Similar to the work we present here on stability, other research looks at how nearest neighbors vary as properties of the embedding spaces change. Pierrejean and Tanguy (2018) found that the lowest frequency and the highest frequency words have the highest variation among nearest neighbors. Additional research has explored how semantic and syntactic properties of words change with different embedding algorithm and parameter choices (Artetxe et al., 2018; Yaghoobzadeh and Schütze, 2016). Unlike our work, previous studie"
2021.emnlp-main.476,P16-1013,0,0.0694145,"Missing"
2021.emnlp-main.476,N18-1190,1,0.733922,"insights about correlations with affixing, language gender systems, and other features. This has implications for embedding use, particularly in research that uses them to study language trends. 1 Introduction Word embeddings have become an established part of natural language processing (NLP) (Collobert et al., 2011; Wang et al., 2020a). Stability, defined as the overlap between the nearest neighbors of a word in different embedding spaces, was introduced to measure variations in local embedding neighborhoods across changes in data, algorithms, and word properties (Antoniak and Mimno, 2018; Wendlandt et al., 2018). These studies found that many common English embedding spaces are surprisingly unstable, which has implications for work that uses embeddings as features in downstream tasks, and work that uses embeddings to study specific properties of language. However, research to date on word embedding stability has been exclusively done on English and so is not representative of all languages. In this work, we explore the stability of word embeddings in a wide range of languages. Better understanding the differences caused by diverse languages will provide a foundation for building embeddings and NLP to"
2021.emnlp-main.476,P16-1023,0,0.0208676,"anguages (Chimalamarri et al., 2020; Stringham and Izbicki, 2020). In this work, we use stability to measure the quality of word embeddings. Similar to the work we present here on stability, other research looks at how nearest neighbors vary as properties of the embedding spaces change. Pierrejean and Tanguy (2018) found that the lowest frequency and the highest frequency words have the highest variation among nearest neighbors. Additional research has explored how semantic and syntactic properties of words change with different embedding algorithm and parameter choices (Artetxe et al., 2018; Yaghoobzadeh and Schütze, 2016). Unlike our work, previous studies only considered English. 3 Data In order to explore the stability of word embeddings in different languages, we work with two datasets, Wikipedia and the Bible. While Wikipedia has more data, the Bible covers more languages. Wikipedia is a comparable corpus, whereas the Bible is a parallel corpus. Wikipedia Corpus. We use pre-processed Wikipedia dumps in 40 languages taken from AlRfou’ et al. (2013).2 The size of these Wikipedia corpora varies from 329,136 sentences (Tagalog) to 75,241,648 sentences (English), with an average of 9,292,394 sentences. For all"
2021.emnlp-main.476,W16-4123,0,0.0190838,"ling to identify patterns in the results. Based on the observations above, we use results from GloVe across five downsampled corpora for Wikipedia, and results across five random seeds for the Bible. 5 Regression Modeling We now explore linguistic factors that correlate with stability. To draw conclusions about specific linguistic features, we use a ridge regression model (Hoerl and Kennard, 1970)10 to predict the average stability of all words in a language given features reflecting language properties. Regression models have previously been used to measure the impact of individual features (Singh et al., 2016). Ridge regression regularizes the magnitude of the model weights, producing a more interpretable model than non-regularized linear regression. We experiment with different regularization strengths and use the best-performing value (α = 10).11 We choose to use a linear model here because of its interpretability. While more complicated models might yield additional insight, we show that there are interesting connections to be drawn from a linear model. 5.1 Model Input and Output Our model takes linguistic features of a language as input and predicts stability as output. Since WALS properties ar"
2021.findings-acl.392,L18-1591,1,0.901026,"Missing"
2021.findings-acl.392,P19-1088,1,0.892588,"Missing"
2021.findings-acl.392,strapparava-valitutti-2004-wordnet,0,0.066773,"Missing"
2021.findings-acl.392,D18-1004,0,0.0271365,"urne et al., 2009; Nambisan, 2011). Computational approaches have aided studies in mental health forums, helping reveal positive relationships between linguistic accommodation and social support across subreddits (Sharma and De Choudhury, 2018). One example of insights from this work is that topicfocused communities like subreddits may enable more peer-engagement than non-community based platforms (Sharma et al., 2020). Other studies have revealed certain trade-offs of online support platforms, such as disparities in the level of support offered toward support-seekers of various demographics (Wang and Jurgens, 2018; Nobles et al., 2020) and in condolences extended across different topics of distress (Zhou and Jurgens, 2020). Studying MHP behaviors in such scenarios might help develop approaches that balance these trade-offs. Computational approaches applied in these forums have also shed light on population-level health trends and health information needs, with examinations into how depression and post-traumatic stress disorder (PTSD) affect different demographic strata (Amir et al., 2019). Data mining has also been applied to understand adverse drug reactions (Wang et al., 2014) and public reactions to"
2021.findings-emnlp.360,W19-3018,0,0.0232077,"ur approach are what data should be leveraged and what models should be built to understand and describe the training data. Another way to view this nuance is that feature engineering extracts task-level features that suit the data for a given task. Micromodels, on the other hand, build task-agnostic, domain-level features that can be applied on multiple tasks. Lastly, features from prior work are typically syntactic, statistical, or derivative features, such as lexical term frequencies (Coppersmith et al., 2014), extractions from metadata (Guntuku et al., 2017), or sentiment analyses scores (Chen et al., 2019). In addition to these features, we are able to build contextual features using contextualized language models, which are able to capture more nuanced concepts reflecting domain expertise. While word embeddings have been used as features before (Mohammadi et al., 2019), they are often difficult to interpret. On the other hand, because the researcher defines the behavior of each aggregator, our resulting feature vector is easy to interpret. Because a micromodel architecture orchestrates multiple models, it may appear similar to ensemble learning. The key difference is that every model in an ens"
2021.findings-emnlp.360,2020.acl-main.493,0,0.0124833,"assification, and suicidal risk assessment. 2 Background and Related Work We find inspiration in previous work that addressed explainability, reusability, efficiency under lowresource scenarios, and integration of domain expertise. We focus primarily on research that was carried out in the domain of mental health. Explainability. Neural networks are black-box models that lack transparency and explainability. Structural analyses of neural networks (Vig et al., 2020), such as probing, has become a popular approach to investigate linguistic properties learned by language models (Wu et al., 2021; Chi et al., 2020; Belinkov et al., 2018; Hewitt and Manning, 2019; Tenney et al., 2018). However, these analyses do not explain how the models use their latent information for their tasks and how they reach their decisions. These drawbacks are especially problematic in the mental health domain (Carr, 2020). Linear models implemented with feature engineering can be analyzed via global feature importance scores, but they do not necessarily provide explanations at a query-level. Model-agnostic explanation frameworks such as SHAP or LIME values (Lundberg and Lee, 2017; Ribeiro et al., 2016) can provide query-leve"
2021.findings-emnlp.360,W15-1204,0,0.0582819,"Missing"
2021.findings-emnlp.360,N19-1419,0,0.012873,"t. 2 Background and Related Work We find inspiration in previous work that addressed explainability, reusability, efficiency under lowresource scenarios, and integration of domain expertise. We focus primarily on research that was carried out in the domain of mental health. Explainability. Neural networks are black-box models that lack transparency and explainability. Structural analyses of neural networks (Vig et al., 2020), such as probing, has become a popular approach to investigate linguistic properties learned by language models (Wu et al., 2021; Chi et al., 2020; Belinkov et al., 2018; Hewitt and Manning, 2019; Tenney et al., 2018). However, these analyses do not explain how the models use their latent information for their tasks and how they reach their decisions. These drawbacks are especially problematic in the mental health domain (Carr, 2020). Linear models implemented with feature engineering can be analyzed via global feature importance scores, but they do not necessarily provide explanations at a query-level. Model-agnostic explanation frameworks such as SHAP or LIME values (Lundberg and Lee, 2017; Ribeiro et al., 2016) can provide query-level, or local, feature importance scores, but they"
2021.findings-emnlp.360,W19-3025,0,0.0327049,"Missing"
2021.findings-emnlp.360,N19-1423,0,0.0398785,"Missing"
2021.findings-emnlp.360,2020.emnlp-main.650,1,0.741344,"data can be time consuming This may improve accuracy, but at the cost of in- and expensive. We use BERT and Universal Senterpretability. Given the sensitive and high-risk do- tence Encoders (Cer et al., 2018) to rapidly colmain of healthcare, where even the most accurate lect representative samples for each micromodel. models become impractical without explainability Our approach is inspired by work on collecting (Caruana et al., 2015), we use EBMs in this work. data for dialogue systems. Specifically, Kang et al. Third, researchers can give their own definition (2018), Larson et al. (2019), Larson et al. (2020), of &quot;Text Utterances&quot; (i). In the CLPsych 2015 and Stasaski et al. (2020) proposed ways to build a Shard Task (Section 4.1), we define each &quot;Text Ut- diverse dataset by iteratively collecting data, startterance&quot; to be a single tweet from a user. However, ing from a seed set and crowdsourcing paraphrases. 4260 Figure 2 depicts our pipeline for building our micromodel datasets. For each micromodel, we build an example corpus and gather paraphrases. While crowdsourcing can be thought of a generative approach for paraphrasing, we take a retrieval approach by using a BERT model to search for seman"
2021.findings-emnlp.360,W18-0609,0,0.215534,"h domain (Carr, 2020). Linear models implemented with feature engineering can be analyzed via global feature importance scores, but they do not necessarily provide explanations at a query-level. Model-agnostic explanation frameworks such as SHAP or LIME values (Lundberg and Lee, 2017; Ribeiro et al., 2016) can provide query-level, or local, feature importance scores, but they are approximate explanations of the underlying model. Our approach provides (1) global and local feature importance scores, and (2) evidence from input text data that led to its output. either fine-tune their embeddings (Orabi et al., 2018) or have task-specific layers (Matero et al., 2019). While task-specific designs can boost accuracy, they are difficult to extend to multiple applications. Furthermore, Harrigian et al. (2020) show that models trained for a task in the mental health domain do not generalize across test sets that originate from different sources. Because our micromodels are built on task-agnostic data, they are reusable for multiple applications within a domain. Efficiency in Low-Resource Scenarios. Obtaining data in the mental health domain is difficult because of the sensitive nature of data and the need for"
2021.findings-emnlp.360,W15-1205,0,0.078348,"(2015) Abraham and Fava (1999); Levy and Deykin (1989) Swearer et al. (2001) Cohan et al. (2018) Cohan et al. (2018) Cohan et al. (2018) Cohan et al. (2018) Cohan et al. (2018) Table 1: The micromodels we developed for this work. Model LR CNN UMD WWBP MM Expl? Reuse? D vs C n = 654 P vs C n = 492 D vs P n=573 0.8 0.79 0.86 0.904 0.821 0.817 0.85 0.893 0.916 0.936 0.785 0.87 0.841 0.81 0.892 Table 2: AUC scores for various approaches, where LR is a logistic regression model, CNN is a convolutional neural network, and MM is our micromodel approach. UMD is from Resnik et al. (2015), WWBP is from Preotiuc-Pietro et al. (2015) – these two systems were the only ones that reported AUC scores and are directly comparable to ours. We also indicate whether each approach is explainable and reusable. micromodel has its own example corpus built using our data collection pipeline (Section 3.3), and uses a similarity score threshold value of 0.85. We use two aggregators. One is as described in Section 3.2, which returns the ratio of hits in a binary vector. The other aggregator looks for &quot;windows&quot;: segments within each binary vector where many hits occur close to one another. These windows may represent temporal &quot;episodes&quot; –"
2021.findings-emnlp.360,D19-1410,0,0.0124656,"build an example corpus and gather paraphrases. While crowdsourcing can be thought of a generative approach for paraphrasing, we take a retrieval approach by using a BERT model to search for semantically similar sentences in a separate corpus of unstructured text data. In particular, we use anonymized posts from the r/depression subreddit3 , a peer support forum for anyone struggling with a depressive disorder. While any corpus can be used to retrieve paraphrases, it is important that the linguistic phenomena that is of interest will be prevalent in the corpus. We used Sentence Transformers (Reimers and Gurevych, 2019)4 and the &quot;paraphrase-xlm-r-multilingual-v1&quot; pre-trained model for our semantic similarity searches. There are multiple ways to initialize the example corpus. One can build lexical queries by specifying patterns based on parsers or lexicons and apply them on a text corpus. For instance, to find examples of the labeling cognitive distortion (attaching a negative label to oneself), a lexical query might look for sentences that contain a first person pronoun with a nominal subject relation with a negative token according to the LIWC lexicon (Pennebaker et al., 2001). While this may seem like an o"
2021.findings-emnlp.360,W19-3005,0,0.182983,"odels such as BERT (Devlin availability and the need for explanations. Raw data is often limited and annotating it requires spe- et al., 2019). This training occurs once and then the micromodels can be reused across multiple tasks cialized knowledge (Aguirre et al., 2021). When within a single domain. Second, the task-specific a dataset is available for a task, research on modmodel is trained on the dataset of interest. During els will often overfit, developing optimizations that cannot be reused for other datasets or tasks (Gun- this phase the micromodels are not modified. tuku et al., 2017; Matero et al., 2019; Chen et al., We demonstrate the benefits of micromodels in 2019). Attempts to reduce data needs by integrating the important domain of mental health. Recent domain knowledge often result in inefficient and ex- studies have shown a rapid increase in the prevapensive models (Yang et al., 2019; Liu et al., 2020; lence of depression symptoms in various demoXie et al., 2020). Integrating knowledge graphs is graphics (Ettman et al., 2020), along with elevated another alternative (Zhang et al., 2019), but poses levels of suicidal ideation (Czeisler et al., 2020). challenges in domains in which doma"
2021.findings-emnlp.360,W19-3004,0,0.0358055,"n the other hand, build task-agnostic, domain-level features that can be applied on multiple tasks. Lastly, features from prior work are typically syntactic, statistical, or derivative features, such as lexical term frequencies (Coppersmith et al., 2014), extractions from metadata (Guntuku et al., 2017), or sentiment analyses scores (Chen et al., 2019). In addition to these features, we are able to build contextual features using contextualized language models, which are able to capture more nuanced concepts reflecting domain expertise. While word embeddings have been used as features before (Mohammadi et al., 2019), they are often difficult to interpret. On the other hand, because the researcher defines the behavior of each aggregator, our resulting feature vector is easy to interpret. Because a micromodel architecture orchestrates multiple models, it may appear similar to ensemble learning. The key difference is that every model in an ensemble learns the same task, while the micromodels each have a different aim. Micromodels are also intended to be used across tasks, whereas the models in an ensemble are task specific. 4 Evaluation We evaluate our micromodel architecture in terms of accuracy, reusabili"
2021.findings-emnlp.360,W19-3023,0,0.0561118,"Missing"
2021.findings-emnlp.360,W15-1207,0,0.0200577,"Spitzer et al. (2006) Zahn et al. (2015) Abraham and Fava (1999); Levy and Deykin (1989) Swearer et al. (2001) Cohan et al. (2018) Cohan et al. (2018) Cohan et al. (2018) Cohan et al. (2018) Cohan et al. (2018) Table 1: The micromodels we developed for this work. Model LR CNN UMD WWBP MM Expl? Reuse? D vs C n = 654 P vs C n = 492 D vs P n=573 0.8 0.79 0.86 0.904 0.821 0.817 0.85 0.893 0.916 0.936 0.785 0.87 0.841 0.81 0.892 Table 2: AUC scores for various approaches, where LR is a logistic regression model, CNN is a convolutional neural network, and MM is our micromodel approach. UMD is from Resnik et al. (2015), WWBP is from Preotiuc-Pietro et al. (2015) – these two systems were the only ones that reported AUC scores and are directly comparable to ours. We also indicate whether each approach is explainable and reusable. micromodel has its own example corpus built using our data collection pipeline (Section 3.3), and uses a similarity score threshold value of 0.85. We use two aggregators. One is as described in Section 3.2, which returns the ratio of hits in a binary vector. The other aggregator looks for &quot;windows&quot;: segments within each binary vector where many hits occur close to one another. These"
2021.findings-emnlp.360,W19-3021,0,0.087588,"Missing"
2021.findings-emnlp.360,W19-3020,0,0.0820343,"Missing"
2021.findings-emnlp.360,W18-0603,0,0.0316259,"each have a different aim. Micromodels are also intended to be used across tasks, whereas the models in an ensemble are task specific. 4 Evaluation We evaluate our micromodel architecture in terms of accuracy, reusability, and efficiency under lowresource scenarios. We also address the explainability properties of our model in Section 5. 4.1 Data were collected. The tasks include (1) classifying depression users versus control users (D vs. C), (2) classifying PTSD users versus control users (P vs. C), and (3) classifying depression users versus PTSD users (D vs. P). CLPsych 2019 Shared Task (Shing et al., 2018; Zirikly et al., 2019). This data is from Reddit users who have posted in the r/SuicideWatch 6 subreddit, a peer support forum for anyone struggling with suicidal thoughts, and were annotated with 4 levels of suicidal risk (no risk, low, moderate, severe). A group of users who have never posted on r/SuicideWatch was used as a control group. The shared task includes 3 tasks: Task A is risk assessment looking only at the users’ posts in r/SuicideWatch. Task B is also risk assessment, but also provides posts across other subreddits. Task C is about screening, with only posts that are not in r/Su"
2021.findings-emnlp.360,D17-1322,0,0.0174154,"multiple applications. Furthermore, Harrigian et al. (2020) show that models trained for a task in the mental health domain do not generalize across test sets that originate from different sources. Because our micromodels are built on task-agnostic data, they are reusable for multiple applications within a domain. Efficiency in Low-Resource Scenarios. Obtaining data in the mental health domain is difficult because of the sensitive nature of data and the need for expert annotators. While researchers have turned to proxy-based annotations, in which data is annotated using automated mechanisms (Yates et al., 2017; Winata et al., 2018), these datasets have caveats and biases (Aguirre et al., 2021; Coppersmith et al., 2015). These data limitations make it difficult to apply standard neural methods. Integrating Domain Expertise. Psychologists have long studied effective methods for assessing patients for various mental health illnesses. Assessment modules such as the Patient Health Questionnaire-9 (PHQ-9) (Kroenke et al., 2001) or PTSD Checklist (PCL) (Ruggiero et al., 2003) allow physicians to reliably screen for the presence or severity of various mental statuses. Similarly, cognitive distortions are i"
2021.findings-emnlp.360,2020.acl-main.446,0,0.0722803,"Missing"
2021.findings-emnlp.360,P19-1139,0,0.0202828,"d for other datasets or tasks (Gun- this phase the micromodels are not modified. tuku et al., 2017; Matero et al., 2019; Chen et al., We demonstrate the benefits of micromodels in 2019). Attempts to reduce data needs by integrating the important domain of mental health. Recent domain knowledge often result in inefficient and ex- studies have shown a rapid increase in the prevapensive models (Yang et al., 2019; Liu et al., 2020; lence of depression symptoms in various demoXie et al., 2020). Integrating knowledge graphs is graphics (Ettman et al., 2020), along with elevated another alternative (Zhang et al., 2019), but poses levels of suicidal ideation (Czeisler et al., 2020). challenges in domains in which domain knowledge Because our micromodels represent domain-level 4257 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4257–4272 November 7–11, 2021. ©2021 Association for Computational Linguistics linguistic patterns, they can be reused for multiple tasks within the same domain, while requiring only half or sometimes just a quarter of the taskspecific annotation data, and also having the benefit of explainability across the entire pipeline. The primary contributions of th"
2021.nlp4convai-1.16,2020.acl-main.57,0,0.0423489,"ts of the transfer process using meta-learning. Xu et al. (2020) look at the problem of learning a joint dialog policy using Reinforcement Learning (RL) in a multi-domain setting which can then be transferred to a new domain. They decomposed the state and action representation into features that correspond to low level components that are shared across domains, facilitating cross-domain transfer. They also proposed a Model Agnostic Meta Learning (MAML Finn et al., 2017) based extension that learns to adapt faster to a new domain. Madotto et al. (2019), Mi et al. (2019), Qian and Yu (2019) and Dai et al. (2020) also look at multi-domain settings. They use MAML based meta-learning methods to learn an initialization that adapts fast with few dialog samples from a new task. All of the papers above consider settings where there is access to a large set of training tasks. The meta-learning systems learn to transfer knowledge 3 Proposed Method to a new test task by learning how to do transfer 3.1 Intuition on different training tasks. While each task only has a limited amount of dialog data, they need a Consider a scenario where we are building a restaulot of tasks during training. In contrast, we look ra"
2021.nlp4convai-1.16,W17-5506,0,0.0303287,"Missing"
2021.nlp4convai-1.16,P07-1034,0,0.170274,"the agent in the task of interest in a different domain. While we use a meta-learning based method for learning the weights for the related task data points in this work, there are other techniques in the machine learning literature, especially in the computer vision literature, that can potentially be used to learn the weights. A large section of these recent techniques are based on learning an adversarially trained discriminator for estimating the weights of related image classification task data points (Zhao et al., 2018; Cao et al., 2018; Sankaranarayanan et al., 2018; Wang et al., 2019). Jiang and Zhai (2007) use a combination of several domain adaptation heuristics to assign weights and evaluate on NLP tasks. Moon and Carbonell (2017) cluster the related task data points and learn attention weights for the clusters. An interesting future direction would be to study which weighting methods are best suited for end-to-end learning of neural goaloriented dialog systems using related tasks and under what conditions. The large cost of collecting data for every new dialog task has been widely acknowledged, motivating a range of efforts. One approach is to transfer knowledge from other data to cope with"
2021.nlp4convai-1.16,2020.emnlp-main.273,0,0.0324985,"training dialog data for the new task of interest. For example Zhao et al. (2020) split the dialog model such that most of the model can be learned using ungrounded dialogs and plain text. Only a small part of the dialog model with a small number of parameters is trained with the dialog data available for the task of interest. In contrast, we explore how to learn from related grounded dialogs, and also without any specific constraints on the structure of the end-to-end dialog system architecture. Wen et al. (2016) pre-train the model with data automatically generated from different tasks and Lin et al. (2020) use pre-trained language models as initialization and then fine-tune the dialog model with data from the task of interest. These ideas are complementary to our approach as we make no assumptions about how the model was pre-trained. Recently, there has been work that explored ways to automatically learn certain aspects of the transfer process using meta-learning. Xu et al. (2020) look at the problem of learning a joint dialog policy using Reinforcement Learning (RL) in a multi-domain setting which can then be transferred to a new domain. They decomposed the state and action representation into"
2021.nlp4convai-1.16,P19-1542,0,0.0275183,"een work that explored ways to automatically learn certain aspects of the transfer process using meta-learning. Xu et al. (2020) look at the problem of learning a joint dialog policy using Reinforcement Learning (RL) in a multi-domain setting which can then be transferred to a new domain. They decomposed the state and action representation into features that correspond to low level components that are shared across domains, facilitating cross-domain transfer. They also proposed a Model Agnostic Meta Learning (MAML Finn et al., 2017) based extension that learns to adapt faster to a new domain. Madotto et al. (2019), Mi et al. (2019), Qian and Yu (2019) and Dai et al. (2020) also look at multi-domain settings. They use MAML based meta-learning methods to learn an initialization that adapts fast with few dialog samples from a new task. All of the papers above consider settings where there is access to a large set of training tasks. The meta-learning systems learn to transfer knowledge 3 Proposed Method to a new test task by learning how to do transfer 3.1 Intuition on different training tasks. While each task only has a limited amount of dialog data, they need a Consider a scenario where we are building a"
2021.nlp4convai-1.16,P19-1253,0,0.0197849,"lly learn certain aspects of the transfer process using meta-learning. Xu et al. (2020) look at the problem of learning a joint dialog policy using Reinforcement Learning (RL) in a multi-domain setting which can then be transferred to a new domain. They decomposed the state and action representation into features that correspond to low level components that are shared across domains, facilitating cross-domain transfer. They also proposed a Model Agnostic Meta Learning (MAML Finn et al., 2017) based extension that learns to adapt faster to a new domain. Madotto et al. (2019), Mi et al. (2019), Qian and Yu (2019) and Dai et al. (2020) also look at multi-domain settings. They use MAML based meta-learning methods to learn an initialization that adapts fast with few dialog samples from a new task. All of the papers above consider settings where there is access to a large set of training tasks. The meta-learning systems learn to transfer knowledge 3 Proposed Method to a new test task by learning how to do transfer 3.1 Intuition on different training tasks. While each task only has a limited amount of dialog data, they need a Consider a scenario where we are building a restaulot of tasks during training. I"
2021.nlp4convai-1.16,N16-1015,0,0.0476824,"Missing"
2021.nlp4convai-1.16,E17-1042,0,0.054965,"Missing"
C10-2051,J99-2004,0,0.0568758,"intransitive verb like sleeping receives the same argument structure as the base form sleep, but with the appropriate inflectional feature. This scheme works well for rule-based parsers, but it is less well suited for statistical parsers, as the rules propose categories but do not help the model estimate their likelihood or assign them feature weights. Statistical parsers for lexicalised formalisms such as CCG are very sensitive to the number of categories in the lexicon and the complexity of the mapping between words and categories. The sub-task of assigning lexical categories, supertagging (Bangalore and Joshi, 1999), is most of the parsing task. Supertaggers mitigate sparse data problems by using a label frequency threshold to prune rare categories from the search space. Clark and Curran (2007) employ a tag dictionary that restricts the model to assigning word/category pairs seen in the training data for frequent words. The tag dictionary causes some level of undergeneration, because not all valid word/category pairs will occur in the limited training data available. The morphological tokens we introduce help to mitigate this, by bringing together what were distinct verbs and argument structures, using l"
C10-2051,J02-2002,0,0.0146751,"categories and a few inflectional types. 3 The features are necessary for satisfactory analyses. Without inflectional features, there is no Inflectional Categories We implement the morphemic categories that have been discussed in the CCG literature 446 be −ing good and −ing do good (S [b]NP )/ADJ (S [ng ]NP)(S [b]NP) ADJ conj (S [b]NP )/NP (S [ng ]NP)(S [b]NP) NP (S [ng]NP )/ADJ S [ng]NP <B× (S [ng]NP )/NP &gt; S [ng]NP (S [ng]NP )(S [ng]NP ) <B× &gt; <Φ&gt; < S [ng]NP Figure 1: A single inflection category (in bold) can serve many different argument structures. Freq. Category Example (Bozsahin, 2002; Cha et al., 2002). The inflected 32.964 (S [dcl ]NP )(S [b]NP ) He ran form is broken into two morphemes, and each is 11,431 (S [pss]NP )(S [b]NP ) He was run down assigned a category. The category for the inflec11,324 (S [ng]NP )(S [b]NP ) He was running 4,343 (S [pt]NP )(S [b]NP ) He has run tional suffix is a function from a category with the 3,457 (N /N )(S [b]NP ) the running man bare-form feature [b] to a category that has an in2,011 S [dcl ]S “..”, he says flectional feature. This prevents verbal categories 1,604 (S [dcl ]S )(S [b]S ) “..”, said the boy 169 (S [dcl ]"
C10-2051,J08-1002,0,0.0211112,"Missing"
C10-2051,C04-1041,1,0.886299,"Missing"
C10-2051,J07-4004,1,0.926601,", but it is less well suited for statistical parsers, as the rules propose categories but do not help the model estimate their likelihood or assign them feature weights. Statistical parsers for lexicalised formalisms such as CCG are very sensitive to the number of categories in the lexicon and the complexity of the mapping between words and categories. The sub-task of assigning lexical categories, supertagging (Bangalore and Joshi, 1999), is most of the parsing task. Supertaggers mitigate sparse data problems by using a label frequency threshold to prune rare categories from the search space. Clark and Curran (2007) employ a tag dictionary that restricts the model to assigning word/category pairs seen in the training data for frequent words. The tag dictionary causes some level of undergeneration, because not all valid word/category pairs will occur in the limited training data available. The morphological tokens we introduce help to mitigate this, by bringing together what were distinct verbs and argument structures, using lemmatisation and factoring inflection away from argument structures. The tag dictionaries for the inflectional morphemes will have very high coverage, because there are only a few in"
C10-2051,P06-1088,1,0.885699,"Missing"
C10-2051,J07-3004,0,0.155251,"ntence is associated with a category that specifies its argument structure and the type and features of the constituent that it heads. For instance, in might head a PP -typed constituent with one NP -typed argument, written as PP /NP . The / operator denotes an argument to the right;  denotes an argument to the left. For example, a transitive verb is a function from a rightward NP to and a leftward NP to a sentence, (S NP )/NP . The grammar consists of a few schematic rules to combine the categories: X /Y Y X /Y Y X Y Y /Z Y  X Y Y /Z X Y ⇒&gt; ⇒< ⇒&gt;B X X X /Z ⇒<B ⇒<B × X  X /Z CCGbank (Hockenmaier and Steedman, 2007) extends this grammar with a set of type-changing rules, designed to strike a better balance between sparsity in the category set and ambiguity in the grammar. We mark such productions TC. In wide-coverage descriptions, categories are generally modelled as typed feature structures (Shieber, 1986), rather than atomic symbols. This allows the grammar to include head indices, and to unify under-specified features. In our notation features are annotated in square-brackets, e.g. S [dcl ]. Head-finding indices are annotated on categories as subscripts, e.g. (NPy NPy )/NPz . We occasionally abbrevia"
C10-2051,D09-1126,1,0.856433,"Missing"
D12-1096,H91-1060,0,0.579673,"Missing"
D12-1096,P11-1048,0,0.0156051,"hat can be em1055 ployed. One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000). While re-ranking has led to gains in performance (Charniak and Johnson, 2005), there has been limited analysis of how effectively rerankers are using the set of available options. Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008; Ng et al., 2010; Auli and Lopez, 2011; Ng and Curran, 2012). In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker. The oracle results use the parse in each K-best list with the highest F-score. While this may not give the true oracle result, as F-score does not factor over sentences, it gives a close approximation. The table has the same columns as Table 2, but the ranges on the bars now reflect the min and max for these sets. While there is improvement on all errors when using the reranker, there is very little additional gain beyond the first 5-10 parses. Even for"
D12-1096,D11-1037,0,0.0179858,"ement of the oracle is considerably higher than that of the reranker, particularly compared to the differences for other errors, suggesting that the reranker lacks the features necessary to make the decision better than the parser. The other interesting outlier is NP internal structure, which continues to make improvements for longer lists, unlike the other error types. 5.2 Out-of-Domain Parsing performance drops considerably when shifting outside of the domain a parser was trained on (Gildea, 2001). Clegg and Shepherd (2005) evaluated parsers qualitatively on node types and rule productions. Bender et al. (2011) designed a Wikipedia test set to evaluate parsers on dependencies representing ten specific linguistic phenomena. To provide a deeper understanding of the errors arising when parsing outside of the newswire domain, we analyse performance of the Charniak parser with reranker and self-trained model on the eight parts of the Brown corpus (Marcus et al., 1056 Corpus WSJ 23 Brown F Brown G Brown K Brown L Brown M Brown N Brown P Brown R G-Web Blogs G-Web Email Description Sentences Av. Length Newswire 2416 23.5 Popular 3164 23.4 Biographies 3279 25.5 General 3881 17.2 Mystery 3714 15.7 Science 881"
D12-1096,J04-4004,0,0.0152738,"from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes i"
D12-1096,P11-1045,0,0.014823,"into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner"
D12-1096,P06-2006,0,0.0118261,"o account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are stro"
D12-1096,cer-etal-2010-parsing,0,0.0400421,"Missing"
D12-1096,P05-1022,0,0.0927601,"For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. System F R Exact Speed"
D12-1096,A00-2018,0,0.0745506,"ation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the"
D12-1096,W05-1102,0,0.0192051,"using on the rows for K = 2 we can also see two interesting outliers. The PP attachment improvement of the oracle is considerably higher than that of the reranker, particularly compared to the differences for other errors, suggesting that the reranker lacks the features necessary to make the decision better than the parser. The other interesting outlier is NP internal structure, which continues to make improvements for longer lists, unlike the other error types. 5.2 Out-of-Domain Parsing performance drops considerably when shifting outside of the domain a parser was trained on (Gildea, 2001). Clegg and Shepherd (2005) evaluated parsers qualitatively on node types and rule productions. Bender et al. (2011) designed a Wikipedia test set to evaluate parsers on dependencies representing ten specific linguistic phenomena. To provide a deeper understanding of the errors arising when parsing outside of the newswire domain, we analyse performance of the Charniak parser with reranker and self-trained model on the eight parts of the Brown corpus (Marcus et al., 1056 Corpus WSJ 23 Brown F Brown G Brown K Brown L Brown M Brown N Brown P Brown R G-Web Blogs G-Web Email Description Sentences Av. Length Newswire 2416 23."
D12-1096,P97-1003,0,0.0512335,"rectly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderso"
D12-1096,J03-4003,0,0.0837746,"2 Background Most attempts to understand the behaviour of constituency parsers have focused on overall evaluation metrics. The three main methods are intrinsic evaluation with PARSEVAL, evaluation on dependencies extracted from the constituency parse, and evaluation on downstream tasks that rely on parsing. Intrinsic evaluation with PARSEVAL, which calculates precision and recall over labeled tree nodes, is a useful indicator of overall performance, but does not pinpoint which structures the parser has most difficulty with. Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves ex"
D12-1096,W11-2927,0,0.0149009,"r, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk 1049 and Corston-Oliver, 2006), information extraction (Miyao et al., 2008), textual entailment (Yuret et al., 2010), or semantic dependencies (Dridan and Oepen, 2011). While some of these approaches give a better sense of the impact of parse errors, they require integration into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers w"
D12-1096,W11-2920,0,0.0121497,"ly classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004)."
D12-1096,foster-van-genabith-2008-parser,0,0.0378682,"Missing"
D12-1096,W01-0521,0,0.0158019,"ngle error. Focusing on the rows for K = 2 we can also see two interesting outliers. The PP attachment improvement of the oracle is considerably higher than that of the reranker, particularly compared to the differences for other errors, suggesting that the reranker lacks the features necessary to make the decision better than the parser. The other interesting outlier is NP internal structure, which continues to make improvements for longer lists, unlike the other error types. 5.2 Out-of-Domain Parsing performance drops considerably when shifting outside of the domain a parser was trained on (Gildea, 2001). Clegg and Shepherd (2005) evaluated parsers qualitatively on node types and rule productions. Bender et al. (2011) designed a Wikipedia test set to evaluate parsers on dependencies representing ten specific linguistic phenomena. To provide a deeper understanding of the errors arising when parsing outside of the newswire domain, we analyse performance of the Charniak parser with reranker and self-trained model on the eight parts of the Brown corpus (Marcus et al., 1056 Corpus WSJ 23 Brown F Brown G Brown K Brown L Brown M Brown N Brown P Brown R G-Web Blogs G-Web Email Description Sentences A"
D12-1096,W07-2202,0,0.0204252,"e nodes, is a useful indicator of overall performance, but does not pinpoint which structures the parser has most difficulty with. Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and va"
D12-1096,D09-1121,0,0.019996,"arsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk 1049 and Corston-Oliver, 2006), information extraction (Miyao"
D12-1096,N03-1014,0,0.0513104,"s (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. System F R Exact Speed ENHANCED TRAINING / SYSTEMS Charniak-SR 92.07 92.44 91.70 44.87 1.8 Charniak-R 91.41 91.78 91.04 44.04 1.8 Charniak-S 91.02 91.16 90.89 40.77 1.8 S P NP VP PRP VBD He was STANDARD PARSERS Berkeley Charniak SSN BUBS Bikel Collins-3 Collins-2 Collins-1 Stanford"
D12-1096,P04-1013,0,0.0234916,"unlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. System F R Exact Speed ENHANCED TRAINING / SYSTEMS Charniak-SR 92.07 92.44 91.70 44.87 1.8 Charniak-R 91.41 91.78 91.04 44.04 1.8 Charniak-S 91.02 91.16 90.89 40.77 1.8 S P NP VP PRP VBD He was STANDARD PARSERS Berkeley Charniak SSN BUBS Bikel Collins-3 Collins-2 Collins-1 Stanford-L Stanford-U 90.0"
D12-1096,P08-1067,0,0.0174942,"limits the range of features that can be em1055 ployed. One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000). While re-ranking has led to gains in performance (Charniak and Johnson, 2005), there has been limited analysis of how effectively rerankers are using the set of available options. Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008; Ng et al., 2010; Auli and Lopez, 2011; Ng and Curran, 2012). In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker. The oracle results use the parse in each K-best list with the highest F-score. While this may not give the true oracle result, as F-score does not factor over sentences, it gives a close approximation. The table has the same columns as Table 2, but the ranges on the bars now reflect the min and max for these sets. While there is improvement on all errors when using the reranker, there is very little additional gain"
D12-1096,W03-2401,0,0.0081588,"rs is not taken into account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for c"
D12-1096,P03-1054,1,0.0285971,"er grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. System F R Exact Speed ENHANCED TRAINING / SYSTEMS Charniak-SR 92.07 92.44 91.70 44.87 1.8 Charniak-R 91.41 91.78 91.04 44.04 1.8 Charniak-S 91.02 91.16 90.89 40.77 1.8 S P NP VP PRP VBD He was STANDARD PARSERS Berkeley Charniak SSN BUBS Bikel Collins-3 Collins-2 Collins-1 Stanford-L Stanford-U 90.06 89.71 89.42 88.50 88.16 87.66 87.62 87.09 86.42 85.78 90.30 89.88 89.96 88.57 88.23 87.82 87.77 87.29 86.35 86.48 89."
D12-1096,J93-2004,0,0.04189,"Missing"
D12-1096,N06-1020,0,0.214552,"with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. System F R Exact Speed ENHANCED TRAINING / SYSTEMS Charniak-SR 92.07 92.44"
D12-1096,D07-1013,0,0.0423525,"method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk 1049 and Corston-Oliver, 2006), information extraction (Miyao et al., 2008), textual entailment (Yuret et al., 2010), or semantic depend"
D12-1096,P08-1006,0,0.0174097,"2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk 1049 and Corston-Oliver, 2006), information extraction (Miyao et al., 2008), textual entailment (Yuret et al., 2010), or semantic dependencies (Dridan and Oepen, 2011). While some of these approaches give a better sense of the impact of parse errors, they require integration into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range o"
D12-1096,U10-1014,1,0.836424,"nge of features that can be em1055 ployed. One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000). While re-ranking has led to gains in performance (Charniak and Johnson, 2005), there has been limited analysis of how effectively rerankers are using the set of available options. Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008; Ng et al., 2010; Auli and Lopez, 2011; Ng and Curran, 2012). In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker. The oracle results use the parse in each K-best list with the highest F-score. While this may not give the true oracle result, as F-score does not factor over sentences, it gives a close approximation. The table has the same columns as Table 2, but the ranges on the bars now reflect the min and max for these sets. While there is improvement on all errors when using the reranker, there is very little additional gain beyond the first"
D12-1096,C10-1094,0,0.0190793,"Missing"
D12-1096,N07-1051,1,0.581759,"into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models,"
D12-1096,P06-1055,1,0.700645,"y require integration into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised pa"
D12-1096,W06-1608,0,0.0132702,"Missing"
D12-1096,W11-2907,0,0.01367,"l indicator of overall performance, but does not pinpoint which structures the parser has most difficulty with. Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008;"
D12-1096,S10-1009,0,0.0191066,"Missing"
D13-1027,D08-1031,0,0.183745,"y We analyzed all of the 2011 CoNLL task systems, as well as several publicly available systems. For the shared task systems we used the output data from the task itself, provided by the organizers. For the publicly available systems we used the default configurations. Finally, we included another run of the Stanford system, with their OntoNotes-tuned parameters (S TANFORD - T). The publicly available systems we used are: B ERKELEY (Durrett and Klein, 2013), IMS (Bj¨orkelund and Farkas, 2012), S TANFORD (Lee et al., 2013), R ECONCILE (Stoyanov et al., 2010), BART (Versley et al., 2008), UIUC (Bengtson and Roth, 2008), and C HERRY P ICKER (Rahman and Ng, 2009). The systems from the shared task are listed in Table 1 and in the references. The most frequent error across all systems is Divided Entity. Unlike parsing errors (Kummerfeld et al., 2012), improvements are not monotonic, with better systems often making more errors of one type when decreasing the frequency of another type. One outlier is the Irwin et al. (2011) system, which makes very few mistakes in five categories, but many in the last two. This reflects a high precision, low recall approach, where clusters are only formed when there is high conf"
D13-1027,W12-4503,0,0.0171127,"Missing"
D13-1027,W11-1905,0,0.0431993,"Missing"
D13-1027,W11-1907,0,0.0387752,"Missing"
D13-1027,W11-1904,0,0.0217879,"Missing"
D13-1027,W11-1915,0,0.0207774,"Missing"
D13-1027,W12-4504,0,0.0212077,"sion (Ng and Cardie, 2002; Haghighi and Klein, 2009). More fine consideration of some subtasks does occur, for example, anaphoricity detection, which has been recognized as a key challenge in coreference resolution for decades and regularly has separate results reported (Paice and Husk, 1987; Sobha et al., 2011; Yuan et al., 2012; Bj¨orkelund and Farkas, 2012; Zhekova et al., 2012). Some work has also included anecdotal discussion of specific error types or manual classification of a small set of errors, but these approaches do not effectively quantify the relative impact of different errors (Chen and Ng, 2012; Martschat et al., 2012; Haghighi and Klein, 2009). In a recent paper, Holen (2013) presented a detailed manual analysis that considered a more comprehensive set of error types, but their focus was on exploring the shortcomings of current metrics, rather than understanding the behavior of current systems. The detailed investigation presented by Stoyanov et al. (2009) is the closest to the work we present here. First, they measured accuracy improvements when their system was given gold annotations for three subtasks of coreference resolution: mention detection, named entity recognition, and an"
D13-1027,W11-1921,0,0.0387128,"Missing"
D13-1027,D13-1203,1,0.304001,"nflated Entities. Each remaining Split operation is mapped to a Conflated Entity error, e.g. Figure 3(vii), and the blue and red entities in Figure 2. 4 Methodology We analyzed all of the 2011 CoNLL task systems, as well as several publicly available systems. For the shared task systems we used the output data from the task itself, provided by the organizers. For the publicly available systems we used the default configurations. Finally, we included another run of the Stanford system, with their OntoNotes-tuned parameters (S TANFORD - T). The publicly available systems we used are: B ERKELEY (Durrett and Klein, 2013), IMS (Bj¨orkelund and Farkas, 2012), S TANFORD (Lee et al., 2013), R ECONCILE (Stoyanov et al., 2010), BART (Versley et al., 2008), UIUC (Bengtson and Roth, 2008), and C HERRY P ICKER (Rahman and Ng, 2009). The systems from the shared task are listed in Table 1 and in the references. The most frequent error across all systems is Divided Entity. Unlike parsing errors (Kummerfeld et al., 2012), improvements are not monotonic, with better systems often making more errors of one type when decreasing the frequency of another type. One outlier is the Irwin et al. (2011) system, which makes very few"
D13-1027,D09-1120,1,0.782568,"., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The only common forms of further analysis are results for anaphoricity detection and scores for each mention type (nominal, pronoun, proper). Two exceptions are: the detailed analysis of the Reconcile system by Stoyanov et al. (2009), and the multi-system comparisons in the CoNLL shared task reports (Pradhan et al., 2011, 2012). A common approach to performance analysis is to calculate scores for nominals, pronouns and proper names separately, but this is a very coarse division (Ng and Cardie, 2002; Haghighi and Klein, 2009). More fine consideration of some subtasks does occur, for example, anaphoricity detection, which has been recognized as a key challenge in coreference resolution for decades and regularly has separate results reported (Paice and Husk, 1987; Sobha et al., 2011; Yuan et al., 2012; Bj¨orkelund and Farkas, 2012; Zhekova et al., 2012). Some work has also included anecdotal discussion of specific error types or manual classification of a small set of errors, but these approaches do not effectively quantify the relative impact of different errors (Chen and Ng, 2012; Martschat et al., 2012; Haghighi"
D13-1027,N13-2001,0,0.0612545,"y improvements when partial gold annotations are provided (Stoyanov et al., 2009; Pradhan et al., 2011; Pradhan et al., 2012). For coreference resolution the drawback of this approach is that decisions are often interdependent, and so even partial gold information is extremely informative. Also, previous work only considered errors by counting links, which does not capture certain errors in a natural way, e.g. when a system incorrectly divides a large entity into two parts, each with multiple mentions. Recent work has considered some of these issues, but only with small scale manual analysis (Holen, 2013). Using our tool we perform two studies to understand similarities and differences between systems. First, we compare the error distributions on coreference resolution of all of the systems from the CoNLL 2011 shared task plus several publicly available systems. This comparison adds to the analysis from the shared task by illustrating the substantial variation in the types of errors different systems make. Second, we investigate the aggregate behavior of ten state-of-the-art systems, providing a detailed characterization of each error type. This investigation identifies key outstanding challen"
D13-1027,N06-2015,0,0.0153185,"ill Missing Mentions and Missing Entities because systems do not always choose an antecedent, leaving a mention as a singleton, which is then ignored. While this broad comparison gives a complete view of the range of errors present, it is still a coarse representation. In the next section, we characterize the common errors on a finer level by breaking down each error type by a range of properties. 5 6 Our tool processes the CoNLL task output, with no other information required. During development, and when choosing examples for this paper, we used the development set of the CoNLL shared task (Hovy et al., 2006; Pradhan et al., 2007; Pradhan et al., 2011). The results we present in the rest of the paper are all for the test set. Using the development set would have been misleading, as the entrants in the shared task used it to tune their systems. 4.1 Systems Broad System Comparison Table 1 presents the frequency of errors for each system and F-Scores for standard metrics1 on the test set of the 2011 CoNLL shared task. Each bar is filled in proportion to the number of errors the system made, with a full bar corresponding to the number of errors listed in the bottom row. The metrics provide an effecti"
D13-1027,W11-1913,0,0.0168803,"we used are: B ERKELEY (Durrett and Klein, 2013), IMS (Bj¨orkelund and Farkas, 2012), S TANFORD (Lee et al., 2013), R ECONCILE (Stoyanov et al., 2010), BART (Versley et al., 2008), UIUC (Bengtson and Roth, 2008), and C HERRY P ICKER (Rahman and Ng, 2009). The systems from the shared task are listed in Table 1 and in the references. The most frequent error across all systems is Divided Entity. Unlike parsing errors (Kummerfeld et al., 2012), improvements are not monotonic, with better systems often making more errors of one type when decreasing the frequency of another type. One outlier is the Irwin et al. (2011) system, which makes very few mistakes in five categories, but many in the last two. This reflects a high precision, low recall approach, where clusters are only formed when there is high confidence. The third section of Table 1 shows results for systems that were run with gold noun phrase span information. This reduces all errors slightly, though most noticeably Extra Mention, Missing Mention, and Span Error. On inspection of the remaining Span Errors we found that many are due to inconsistencies regarding the inclusion of the possessive. The final section of the table shows results for syste"
D13-1027,W11-1912,0,0.0387162,"Missing"
D13-1027,W11-1910,0,0.034988,"Missing"
D13-1027,W11-1916,1,0.897324,"Missing"
D13-1027,D12-1096,1,0.724273,"s we used the default configurations. Finally, we included another run of the Stanford system, with their OntoNotes-tuned parameters (S TANFORD - T). The publicly available systems we used are: B ERKELEY (Durrett and Klein, 2013), IMS (Bj¨orkelund and Farkas, 2012), S TANFORD (Lee et al., 2013), R ECONCILE (Stoyanov et al., 2010), BART (Versley et al., 2008), UIUC (Bengtson and Roth, 2008), and C HERRY P ICKER (Rahman and Ng, 2009). The systems from the shared task are listed in Table 1 and in the references. The most frequent error across all systems is Divided Entity. Unlike parsing errors (Kummerfeld et al., 2012), improvements are not monotonic, with better systems often making more errors of one type when decreasing the frequency of another type. One outlier is the Irwin et al. (2011) system, which makes very few mistakes in five categories, but many in the last two. This reflects a high precision, low recall approach, where clusters are only formed when there is high confidence. The third section of Table 1 shows results for systems that were run with gold noun phrase span information. This reduces all errors slightly, though most noticeably Extra Mention, Missing Mention, and Span Error. On inspect"
D13-1027,W11-1914,0,0.0182854,"Missing"
D13-1027,W11-1902,0,0.200383,"Missing"
D13-1027,J13-4004,0,0.0131888,"Entity error, e.g. Figure 3(vii), and the blue and red entities in Figure 2. 4 Methodology We analyzed all of the 2011 CoNLL task systems, as well as several publicly available systems. For the shared task systems we used the output data from the task itself, provided by the organizers. For the publicly available systems we used the default configurations. Finally, we included another run of the Stanford system, with their OntoNotes-tuned parameters (S TANFORD - T). The publicly available systems we used are: B ERKELEY (Durrett and Klein, 2013), IMS (Bj¨orkelund and Farkas, 2012), S TANFORD (Lee et al., 2013), R ECONCILE (Stoyanov et al., 2010), BART (Versley et al., 2008), UIUC (Bengtson and Roth, 2008), and C HERRY P ICKER (Rahman and Ng, 2009). The systems from the shared task are listed in Table 1 and in the references. The most frequent error across all systems is Divided Entity. Unlike parsing errors (Kummerfeld et al., 2012), improvements are not monotonic, with better systems often making more errors of one type when decreasing the frequency of another type. One outlier is the Irwin et al. (2011) system, which makes very few mistakes in five categories, but many in the last two. This refle"
D13-1027,W11-1917,0,0.0423735,"Missing"
D13-1027,H05-1004,0,0.12809,"t analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task. 1 Introduction Metrics produce measurements that concisely summarize performance on the full range of error types, and for coreference resolution there has been extensive work on developing effective metrics (Luo, 2005; Recasens and Hovy, 2011). However, it is also valuable to tease apart the errors to understand their relative importance. Previous investigations of coreference errors have focused on quantifying the importance of subtasks such as named entity recognition and anaphoricity detection, typically by measuring accuracy improvements when partial gold annotations are provided (Stoyanov et al., 2009; Pradhan et al., 2011; Pradhan et al., 2012). For coreference resolution the drawback of this approach is that decisions are often interdependent, and so even partial gold information is extremely inform"
D13-1027,W12-4511,0,0.0409602,"Missing"
D13-1027,P02-1014,0,0.362954,"as MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The only common forms of further analysis are results for anaphoricity detection and scores for each mention type (nominal, pronoun, proper). Two exceptions are: the detailed analysis of the Reconcile system by Stoyanov et al. (2009), and the multi-system comparisons in the CoNLL shared task reports (Pradhan et al., 2011, 2012). A common approach to performance analysis is to calculate scores for nominals, pronouns and proper names separately, but this is a very coarse division (Ng and Cardie, 2002; Haghighi and Klein, 2009). More fine consideration of some subtasks does occur, for example, anaphoricity detection, which has been recognized as a key challenge in coreference resolution for decades and regularly has separate results reported (Paice and Husk, 1987; Sobha et al., 2011; Yuan et al., 2012; Bj¨orkelund and Farkas, 2012; Zhekova et al., 2012). Some work has also included anecdotal discussion of specific error types or manual classification of a small set of errors, but these approaches do not effectively quantify the relative impact of different errors (Chen and Ng, 2012; Martsc"
D13-1027,W11-1906,0,0.0454902,"Missing"
D13-1027,W11-1901,0,0.423362,"etrics produce measurements that concisely summarize performance on the full range of error types, and for coreference resolution there has been extensive work on developing effective metrics (Luo, 2005; Recasens and Hovy, 2011). However, it is also valuable to tease apart the errors to understand their relative importance. Previous investigations of coreference errors have focused on quantifying the importance of subtasks such as named entity recognition and anaphoricity detection, typically by measuring accuracy improvements when partial gold annotations are provided (Stoyanov et al., 2009; Pradhan et al., 2011; Pradhan et al., 2012). For coreference resolution the drawback of this approach is that decisions are often interdependent, and so even partial gold information is extremely informative. Also, previous work only considered errors by counting links, which does not capture certain errors in a natural way, e.g. when a system incorrectly divides a large entity into two parts, each with multiple mentions. Recent work has considered some of these issues, but only with small scale manual analysis (Holen, 2013). Using our tool we perform two studies to understand similarities and differences between"
D13-1027,W12-4501,0,0.176046,"ments that concisely summarize performance on the full range of error types, and for coreference resolution there has been extensive work on developing effective metrics (Luo, 2005; Recasens and Hovy, 2011). However, it is also valuable to tease apart the errors to understand their relative importance. Previous investigations of coreference errors have focused on quantifying the importance of subtasks such as named entity recognition and anaphoricity detection, typically by measuring accuracy improvements when partial gold annotations are provided (Stoyanov et al., 2009; Pradhan et al., 2011; Pradhan et al., 2012). For coreference resolution the drawback of this approach is that decisions are often interdependent, and so even partial gold information is extremely informative. Also, previous work only considered errors by counting links, which does not capture certain errors in a natural way, e.g. when a system incorrectly divides a large entity into two parts, each with multiple mentions. Recent work has considered some of these issues, but only with small scale manual analysis (Holen, 2013). Using our tool we perform two studies to understand similarities and differences between systems. First, we com"
D13-1027,D09-1101,0,0.148211,"s, as well as several publicly available systems. For the shared task systems we used the output data from the task itself, provided by the organizers. For the publicly available systems we used the default configurations. Finally, we included another run of the Stanford system, with their OntoNotes-tuned parameters (S TANFORD - T). The publicly available systems we used are: B ERKELEY (Durrett and Klein, 2013), IMS (Bj¨orkelund and Farkas, 2012), S TANFORD (Lee et al., 2013), R ECONCILE (Stoyanov et al., 2010), BART (Versley et al., 2008), UIUC (Bengtson and Roth, 2008), and C HERRY P ICKER (Rahman and Ng, 2009). The systems from the shared task are listed in Table 1 and in the references. The most frequent error across all systems is Divided Entity. Unlike parsing errors (Kummerfeld et al., 2012), improvements are not monotonic, with better systems often making more errors of one type when decreasing the frequency of another type. One outlier is the Irwin et al. (2011) system, which makes very few mistakes in five categories, but many in the last two. This reflects a high precision, low recall approach, where clusters are only formed when there is high confidence. The third section of Table 1 shows"
D13-1027,W11-1903,0,0.0425142,"Missing"
D13-1027,W11-1922,0,0.0230353,"Missing"
D13-1027,P09-1074,0,0.473254,"task. 1 Introduction Metrics produce measurements that concisely summarize performance on the full range of error types, and for coreference resolution there has been extensive work on developing effective metrics (Luo, 2005; Recasens and Hovy, 2011). However, it is also valuable to tease apart the errors to understand their relative importance. Previous investigations of coreference errors have focused on quantifying the importance of subtasks such as named entity recognition and anaphoricity detection, typically by measuring accuracy improvements when partial gold annotations are provided (Stoyanov et al., 2009; Pradhan et al., 2011; Pradhan et al., 2012). For coreference resolution the drawback of this approach is that decisions are often interdependent, and so even partial gold information is extremely informative. Also, previous work only considered errors by counting links, which does not capture certain errors in a natural way, e.g. when a system incorrectly divides a large entity into two parts, each with multiple mentions. Recent work has considered some of these issues, but only with small scale manual analysis (Holen, 2013). Using our tool we perform two studies to understand similarities a"
D13-1027,P10-2029,0,0.0632463,"i), and the blue and red entities in Figure 2. 4 Methodology We analyzed all of the 2011 CoNLL task systems, as well as several publicly available systems. For the shared task systems we used the output data from the task itself, provided by the organizers. For the publicly available systems we used the default configurations. Finally, we included another run of the Stanford system, with their OntoNotes-tuned parameters (S TANFORD - T). The publicly available systems we used are: B ERKELEY (Durrett and Klein, 2013), IMS (Bj¨orkelund and Farkas, 2012), S TANFORD (Lee et al., 2013), R ECONCILE (Stoyanov et al., 2010), BART (Versley et al., 2008), UIUC (Bengtson and Roth, 2008), and C HERRY P ICKER (Rahman and Ng, 2009). The systems from the shared task are listed in Table 1 and in the references. The most frequent error across all systems is Divided Entity. Unlike parsing errors (Kummerfeld et al., 2012), improvements are not monotonic, with better systems often making more errors of one type when decreasing the frequency of another type. One outlier is the Irwin et al. (2011) system, which makes very few mistakes in five categories, but many in the last two. This reflects a high precision, low recall app"
D13-1027,W11-1920,0,0.0411596,"Missing"
D13-1027,W11-1908,0,0.031567,"Missing"
D13-1027,P08-4003,0,0.0255457,"ies in Figure 2. 4 Methodology We analyzed all of the 2011 CoNLL task systems, as well as several publicly available systems. For the shared task systems we used the output data from the task itself, provided by the organizers. For the publicly available systems we used the default configurations. Finally, we included another run of the Stanford system, with their OntoNotes-tuned parameters (S TANFORD - T). The publicly available systems we used are: B ERKELEY (Durrett and Klein, 2013), IMS (Bj¨orkelund and Farkas, 2012), S TANFORD (Lee et al., 2013), R ECONCILE (Stoyanov et al., 2010), BART (Versley et al., 2008), UIUC (Bengtson and Roth, 2008), and C HERRY P ICKER (Rahman and Ng, 2009). The systems from the shared task are listed in Table 1 and in the references. The most frequent error across all systems is Divided Entity. Unlike parsing errors (Kummerfeld et al., 2012), improvements are not monotonic, with better systems often making more errors of one type when decreasing the frequency of another type. One outlier is the Irwin et al. (2011) system, which makes very few mistakes in five categories, but many in the last two. This reflects a high precision, low recall approach, where clusters are onl"
D13-1027,M95-1005,0,0.631901,"tities that contain a small number of mentions. This work presents a comprehensive investigation of common errors in coreference resolution, identifying particular issues worth focusing on in future research. Our analysis tool is available at code.google.com/p/berkeley-coreference-analyser/. 265 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 265–277, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Background Most coreference work focuses on accuracy improvements, as measured by metrics such as MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The only common forms of further analysis are results for anaphoricity detection and scores for each mention type (nominal, pronoun, proper). Two exceptions are: the detailed analysis of the Reconcile system by Stoyanov et al. (2009), and the multi-system comparisons in the CoNLL shared task reports (Pradhan et al., 2011, 2012). A common approach to performance analysis is to calculate scores for nominals, pronouns and proper names separately, but this is a very coarse division (Ng and Cardie, 2002; Haghighi"
D13-1027,W11-1911,0,0.0503829,"Missing"
D13-1027,W11-1919,0,0.0292982,"Missing"
D13-1027,W12-4507,0,0.0108366,"econcile system by Stoyanov et al. (2009), and the multi-system comparisons in the CoNLL shared task reports (Pradhan et al., 2011, 2012). A common approach to performance analysis is to calculate scores for nominals, pronouns and proper names separately, but this is a very coarse division (Ng and Cardie, 2002; Haghighi and Klein, 2009). More fine consideration of some subtasks does occur, for example, anaphoricity detection, which has been recognized as a key challenge in coreference resolution for decades and regularly has separate results reported (Paice and Husk, 1987; Sobha et al., 2011; Yuan et al., 2012; Bj¨orkelund and Farkas, 2012; Zhekova et al., 2012). Some work has also included anecdotal discussion of specific error types or manual classification of a small set of errors, but these approaches do not effectively quantify the relative impact of different errors (Chen and Ng, 2012; Martschat et al., 2012; Haghighi and Klein, 2009). In a recent paper, Holen (2013) presented a detailed manual analysis that considered a more comprehensive set of error types, but their focus was on exploring the shortcomings of current metrics, rather than understanding the behavior of current systems. The de"
D13-1027,W11-1918,0,0.0243807,"Missing"
D13-1027,W12-4509,0,0.0282044,"Missing"
D13-1027,W11-1909,0,0.127598,"Missing"
D13-1027,D08-1067,0,\N,Missing
D15-1032,D13-1203,1,0.83022,"ng dev set performance over five training iterations, except for constituency parsing, where we took five measurements, 4k instances apart. For the cutting plane methods we cached constraints in memory to save time, but the memory cost was too great to run batch cutting plane on constituency parsing (over 60 Gb), and so is not included in the results. Coreference Resolution This gives an example of training when there are multiple gold outputs for each instance. The system we consider uses latent links between mentions in the same cluster, marginalizing over the possibilities during learning (Durrett and Klein, 2013). Since the model decomposes across mentions, we train by treating them as independent predictions with multiple gold outputs, comparing the inferred link with the gold link that is scored highest under the current model. We use the system’s weighted loss function, and the same data as for NER. 4 Observations From the results in Figure 1 and during tuning, we can make several observations about these optimization methods’ performance on these tasks. Constituency Parsing We considered two different systems. The first uses only sparse indicator features (Hall et al., 2014), while the second is p"
D15-1032,Q14-1037,1,0.270541,"Inference involves solving an integer linear program, the loss function is bigram recall, and the data is from the TAC shared tasks (Dang and Owczarzak, 2008; Dang and Owczarzak, 2009). Tasks and Systems We considered tasks covering a range of structured output spaces, from sequences to non-projective trees. Most of the corresponding systems use models designed for likelihood-based structured prediction. Some use sparse indicator features, while others use dense continuous-valued features. Named Entity Recognition This task provides a case of sequence prediction. We used the NER component of Durrett and Klein (2014)’s entity stack, training it independently of the other components. We define the loss as the number of incorrectly labelled words, and train on the CoNLL 2012 division of OntoNotes (Pradhan et al., 2007). 3.1 Tuning For each method we tuned hyperparameters by considering a grid of values and measuring dev set performance over five training iterations, except for constituency parsing, where we took five measurements, 4k instances apart. For the cutting plane methods we cached constraints in memory to save time, but the memory cost was too great to run batch cutting plane on constituency parsin"
D15-1032,P11-1049,1,0.722984,"standard division of the English Universal Dependencies (Agi´c et al., 2015). The built-in training method for MST parser is averaged, 1-best MIRA, which we include for comparison purposes. 3 Summarization With this task, we explore a case in which there is relatively little training data and the model uses a small number of dense features. The system uses a linear model with features considering counts of bigrams in the input document collection. The system forms the output summary by selecting a subset of the sentences in the input collection that does not exceed a fixed word-length limit (Berg-Kirkpatrick et al., 2011). Inference involves solving an integer linear program, the loss function is bigram recall, and the data is from the TAC shared tasks (Dang and Owczarzak, 2008; Dang and Owczarzak, 2009). Tasks and Systems We considered tasks covering a range of structured output spaces, from sequences to non-projective trees. Most of the corresponding systems use models designed for likelihood-based structured prediction. Some use sparse indicator features, while others use dense continuous-valued features. Named Entity Recognition This task provides a case of sequence prediction. We used the NER component of"
D15-1032,P15-1030,1,0.0373143,"Missing"
D15-1032,Q13-1017,0,0.0198867,"touch the weights corresponding to the (usually sparse) nonzero directions in the current batch’s subgradient. Algorithm 1 gives pseudocode for our implementation, which was based on Dyer (2013). Margin Cutting Plane (Tsochantaridis et al., 2004) Solves a sequence of quadratic programs (QP), each of which is an approximation to the dual formulation of the margin-based learning problem. At each iteration, the current QP is refined by adding additional active constraints. We solve each approximate QP using Sequential Minimal Optimization (Platt, 1999; Taskar et al., 2004). Online Cutting Plane (Chang and Yih, 2013) A modified form of cutting plane that only partially solves the QP on each iteration, operating in the dual space and optimizing a single dual variable on each iteration. We use a variant of Chang and Yih (2013) for the L1 loss margin objective. 2.2 Likelihood Stochastic Gradient Descent The built-in training method for many of the systems was softmax-margin likelihood optimization (Gimpel and Smith, 2010) via subgradient descent with either AdaGrad or AdaDelta (Duchi et al., 2011; Zeiler, 2012). We include results with each system’s default settings as a point of comparison. Online Primal Su"
D15-1032,P05-1022,0,0.0439671,"perceptron. Second, for max-margin objectives, primal optimization methods are often more robust and progress faster than dual methods. This advantage is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple online primal subgradient descent method that, despite being rarely mentioned in the literature, is surprisingly effective in relation to its alternatives. 1 Introduction Structured discriminative models have proven effective across a range of tasks in NLP including tagging (Lafferty et al., 2001; Collins, 2002), reranking parses (Charniak and Johnson, 2005), and many more (Taskar, 2004; Smith, 2011). Common approaches to training such models include margin methods, likelihood methods, and mistake-driven procedures like the averaged perceptron algorithm. In this paper, we primarily consider the relative empirical behavior of several online optimization methods for margin-based objectives, with secondary attention to other approaches for calibration. It is increasingly common to train structured models using a max-margin objective that incorporates a loss function that decomposes in the 2 Learning Algorithms We implemented a range of optimization"
D15-1032,C96-1058,0,0.0684883,"dified form of the perceptron that uses loss-augmented decoding and makes the smallest update necessary to give a margin at least as large as the loss of each solution. MIRA is generally presented as being related to the perceptron because it does not explicitly optimize a global objective, but it also has connections to margin methods, as explored by Chiang (2012). We consider one-best decoding, where the quadratic program for determining the magnitude of the update has a closed form. Dependency Parsing We used the first-order MST parser in two modes, Eisner’s algorithm for projective trees (Eisner, 1996; McDonald et al., 2005b), and the Chu-Liu-Edmonds algorithm for non-projective trees (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005a). The loss function was the number of arcs with an incorrect parent or label, and we used the standard division of the English Universal Dependencies (Agi´c et al., 2015). The built-in training method for MST parser is averaged, 1-best MIRA, which we include for comparison purposes. 3 Summarization With this task, we explore a case in which there is relatively little training data and the model uses a small number of dense features. The system uses a l"
D15-1032,N10-1112,0,0.0573708,"Missing"
D15-1032,P14-1022,1,0.5352,"Missing"
D15-1032,W02-1001,0,0.6832,"outperform both likelihood and the perceptron. Second, for max-margin objectives, primal optimization methods are often more robust and progress faster than dual methods. This advantage is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple online primal subgradient descent method that, despite being rarely mentioned in the literature, is surprisingly effective in relation to its alternatives. 1 Introduction Structured discriminative models have proven effective across a range of tasks in NLP including tagging (Lafferty et al., 2001; Collins, 2002), reranking parses (Charniak and Johnson, 2005), and many more (Taskar, 2004; Smith, 2011). Common approaches to training such models include margin methods, likelihood methods, and mistake-driven procedures like the averaged perceptron algorithm. In this paper, we primarily consider the relative empirical behavior of several online optimization methods for margin-based objectives, with secondary attention to other approaches for calibration. It is increasingly common to train structured models using a max-margin objective that incorporates a loss function that decomposes in the 2 Learning Alg"
D15-1032,J93-2004,0,0.0552861,"hese instance-wise subgradients to optimize the global objective using AdaGrad (Duchi et al., 2011) with either L1 or L2 regularization. The simplest implementation of AdaGrad touches every weight when doing the update 2.3 Mistake Driven Averaged Perceptron (Freund and Schapire, 1999; Collins, 2002) On a mistake, weights for features on the system output are decremented and weights for features on the gold output are incre274 mented. Weights are averaged over the course of training, and decoding is not loss-augmented. of incorrect rule productions, and use the standard Penn Treebank division (Marcus et al., 1993). Margin Infused Relaxed Algorithm (Crammer and Singer, 2003) A modified form of the perceptron that uses loss-augmented decoding and makes the smallest update necessary to give a margin at least as large as the loss of each solution. MIRA is generally presented as being related to the perceptron because it does not explicitly optimize a global objective, but it also has connections to margin methods, as explored by Chiang (2012). We consider one-best decoding, where the quadratic program for determining the magnitude of the update has a closed form. Dependency Parsing We used the first-order"
D15-1032,P05-1012,0,0.0382023,"the perceptron that uses loss-augmented decoding and makes the smallest update necessary to give a margin at least as large as the loss of each solution. MIRA is generally presented as being related to the perceptron because it does not explicitly optimize a global objective, but it also has connections to margin methods, as explored by Chiang (2012). We consider one-best decoding, where the quadratic program for determining the magnitude of the update has a closed form. Dependency Parsing We used the first-order MST parser in two modes, Eisner’s algorithm for projective trees (Eisner, 1996; McDonald et al., 2005b), and the Chu-Liu-Edmonds algorithm for non-projective trees (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005a). The loss function was the number of arcs with an incorrect parent or label, and we used the standard division of the English Universal Dependencies (Agi´c et al., 2015). The built-in training method for MST parser is averaged, 1-best MIRA, which we include for comparison purposes. 3 Summarization With this task, we explore a case in which there is relatively little training data and the model uses a small number of dense features. The system uses a linear model with featur"
D15-1032,H05-1066,0,0.0798595,"the perceptron that uses loss-augmented decoding and makes the smallest update necessary to give a margin at least as large as the loss of each solution. MIRA is generally presented as being related to the perceptron because it does not explicitly optimize a global objective, but it also has connections to margin methods, as explored by Chiang (2012). We consider one-best decoding, where the quadratic program for determining the magnitude of the update has a closed form. Dependency Parsing We used the first-order MST parser in two modes, Eisner’s algorithm for projective trees (Eisner, 1996; McDonald et al., 2005b), and the Chu-Liu-Edmonds algorithm for non-projective trees (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005a). The loss function was the number of arcs with an incorrect parent or label, and we used the standard division of the English Universal Dependencies (Agi´c et al., 2015). The built-in training method for MST parser is averaged, 1-best MIRA, which we include for comparison purposes. 3 Summarization With this task, we explore a case in which there is relatively little training data and the model uses a small number of dense features. The system uses a linear model with featur"
D15-1032,W04-3201,1,0.893268,"able loss-augmented decoding, and trained these models with six different methods. We have released our learning code as a Java library.1 Our results provide support for the conventional wisdom that margin-based optimization is broadly effective, frequently outperforming likelihood optimization and the perceptron algorithm. We also found that directly optimizing the primal structured margin objective based on subgradients calculated from single training instances is surprisingly effective, performing consistently well across all tasks. Despite the convexity of structured maxmargin objectives (Taskar et al., 2004; Tsochantaridis et al., 2004), the many ways to optimize them are not equally effective in practice. We compare a range of online optimization methods over a variety of structured NLP tasks (coreference, summarization, parsing, etc) and find several broad trends. First, margin methods do tend to outperform both likelihood and the perceptron. Second, for max-margin objectives, primal optimization methods are often more robust and progress faster than dual methods. This advantage is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple o"
D17-1275,P13-1035,0,0.049848,"Missing"
D17-1275,D14-1082,0,0.0202088,"evel Evaluation Another axis of variation in metrics comes from whether we consider token-level or phrase-level outputs. As noted in the previous section, we did not annotate noun phrases, but we may actually be interested in identifying them. In Figure 1, for example, extracting Backconnect bot is more useful than extracting bot in isolation, since bot is a less specific characterization of the product. We can convert our token-level annotations to phrase-level annotations by projecting our annotations to the noun phrase level based on the output of an automatic parser. We used the parser of Chen and Manning (2014) to parse all sentences of each post. For each annotated token that was given a nominal tag (N*), we projected that token to the largest NP containing it of length less than or equal to 7; most product NPs are shorter than this, and when the parser predicts a longer NP, our analysis found that it typically reflects a mistake. In Figure 1, the entire noun phrase Backconnect bot would be labeled as a product. For products realized as verbs (e.g., hack), we leave the annotation as the single token. Throughout the rest of this work, we will evaluate sometimes at the token-level and sometimes at 6"
D17-1275,P07-1033,0,0.259721,"Missing"
D17-1275,P05-1045,0,0.0405631,"ervised methods (Section 5). Baselines One approach takes the most frequent noun or verb in a post and classifies all occurrences of that word type as products. A more sophisticated lexical baseline is based on a product dictionary extracted from our training data: we tag the most frequent noun or verb in a post that also appears in this dictionary. This method fails primarily in that it prefers to extract common words like account and website even when they do not occur as products. The most relevant off-theshelf system is an NER tagging model; we retrain the Stanford NER system on our data (Finkel et al., 2005). Finally, we can tag the first noun phrase of the post as a product, which will often capture the product if it is mentioned in the title of the post.8 We also include human performance results. We averaged the results for annotators compared with the consensus annotations. For the phrase level evaluation, we apply the projection method described in Section 3.1. Binary classifier/CRF One learning-based approach to this task is to employ a binary SVM classifier for each token in isolation. We also experimented with a token-level CRF with a binary tagset, and found identical performance, so we"
D17-1275,N13-1014,0,0.134481,"on extraction perspective. Having annotated a dataset, we examine supervised and semi-supervised learning approaches to the product extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to annotate data for every new forum that might need to be analyzed, we explore several methods for adaptation, mixing type-level annotation (Garrette and Baldridge, 2013; Garrette et al., 2013), token-level annotation (Daume III, 2007), and semi-supervised approaches (Turian et al., 2010; Kshirsagar et al., 2015). We find little improvement from these methods and discuss why they fail to have a larger impact. Overall, our results characterize the challenges of our fine-grained domain adaptation problem in online marketplace data. We believe that this new dataset provides a useful testbed for additional inquiry and investigation into modeling of finegrained domain differences. 2 Dataset and Annotation We consider several forums that vary in the nature of produ"
D17-1275,P13-1057,0,0.123805,"ing annotated a dataset, we examine supervised and semi-supervised learning approaches to the product extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to annotate data for every new forum that might need to be analyzed, we explore several methods for adaptation, mixing type-level annotation (Garrette and Baldridge, 2013; Garrette et al., 2013), token-level annotation (Daume III, 2007), and semi-supervised approaches (Turian et al., 2010; Kshirsagar et al., 2015). We find little improvement from these methods and discuss why they fail to have a larger impact. Overall, our results characterize the challenges of our fine-grained domain adaptation problem in online marketplace data. We believe that this new dataset provides a useful testbed for additional inquiry and investigation into modeling of finegrained domain differences. 2 Dataset and Annotation We consider several forums that vary in the nature of products being traded: • Dark"
D17-1275,D15-1157,0,0.0604442,"Missing"
D17-1275,W10-2923,0,0.0239519,"oth slot-filling information extraction (with provenance information) as well as standard named-entity recognition (NER). Compared to NER, our task features a higher dependence on context: we only care about the specific product being bought or sold in a post, not other products that might be mentioned. Moreover, because we are operating over forums, the data is substantially messier than classical NER corpora like CoNLL (Tjong Kim Sang and De Meulder, 2003). While prior work has dealt with these messy characteristics for syntax (Kaljahi et al., 2015) and for discourse (Lui and Baldwin, 2010; Kim et al., 2010; Wang et al., 2011), our work is the first to tackle forum data (and marketplace forums specifically) from an information extraction perspective. Having annotated a dataset, we examine supervised and semi-supervised learning approaches to the product extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to annotate data for e"
D17-1275,D15-1032,1,0.800204,"es a useful form of prior knowledge, namely that each post has exactly one product in almost all cases. Our post-level system is formulated as an instance of a latent SVM (Yu and Joachims, 2009). The output space is the set of all tokens (or noun phrases, in the NP case) in the post. The latent variable is the choice of token/NP to select, since there may be multiple correct choices of product tokens. The features used on each token/NP are the same as in the token classifier. We trained all of the learned models by subgradient descent on the primal form of the objective (Ratliff et al., 2007; Kummerfeld et al., 2015). We use AdaGrad (Duchi et al., 2011) to speed convergence in the presence of a large weight vector with heterogeneous feature types. All product extractors in this section are trained for 5 iterations with `1 -regularization tuned on the development set. 2602 Freq Dict NER Binary Post Human∗ Freq Dict First NER Binary Post Human∗ P 41.9 57.9 59.7 62.4 82.4 86.9 P 61.8 57.9 73.1 63.6 67.0 87.6 87.6 Token Prediction Tokens Products R F1 P R F1 42.5 42.2 48.4 33.5 39.6 51.1 54.3 65.6 44.0 52.7 62.2 60.9 60.8 62.6 61.7 76.0 68.5 58.1 77.6 66.4 36.1 50.3 83.5 56.6 67.5 80.4 83.5 87.7 77.6 82.2 NP"
D17-1275,U10-1009,0,0.0262503,"k has similarities to both slot-filling information extraction (with provenance information) as well as standard named-entity recognition (NER). Compared to NER, our task features a higher dependence on context: we only care about the specific product being bought or sold in a post, not other products that might be mentioned. Moreover, because we are operating over forums, the data is substantially messier than classical NER corpora like CoNLL (Tjong Kim Sang and De Meulder, 2003). While prior work has dealt with these messy characteristics for syntax (Kaljahi et al., 2015) and for discourse (Lui and Baldwin, 2010; Kim et al., 2010; Wang et al., 2011), our work is the first to tackle forum data (and marketplace forums specifically) from an information extraction perspective. Having annotated a dataset, we examine supervised and semi-supervised learning approaches to the product extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to a"
D17-1275,P14-5010,0,0.00355307,"ed every post in the Darkode training, Hack Forums training, Blackhat test, and Nulled test sets; these annotations were then merged into a final annotation by majority vote. The development and test sets for Darkode and Hack Forums were annotated by additional team members (five for Darkode, one for Hack Forums), and then every disagreement was discussed and resolved to produce a final annotation. The authors, who are researchers in either NLP or computer security, did all of the annotation. We preprocessed the data using the tokenizer and sentence-splitter from the Stanford CoreNLP toolkit (Manning et al., 2014). Note that many sentences in the data are already delimited by line breaks, making the sentence-splitting task much easier. We performed annotation on the tokenized data so that annotations would be consistent with surrounding punctuation and hyphenated words. Our full annotation guide is available with our data release.3 Our basic annotation principle is 2 The table does not include additional posts that were labeled by all annotators in order to check agreement. 3 https://evidencebasedsecurity.org/ forums/annotation-guide.pdf to annotate tokens when they are either the product that will be"
D17-1275,P13-1108,0,0.0727509,"Missing"
D17-1275,P10-1040,0,0.780109,"uct extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to annotate data for every new forum that might need to be analyzed, we explore several methods for adaptation, mixing type-level annotation (Garrette and Baldridge, 2013; Garrette et al., 2013), token-level annotation (Daume III, 2007), and semi-supervised approaches (Turian et al., 2010; Kshirsagar et al., 2015). We find little improvement from these methods and discuss why they fail to have a larger impact. Overall, our results characterize the challenges of our fine-grained domain adaptation problem in online marketplace data. We believe that this new dataset provides a useful testbed for additional inquiry and investigation into modeling of finegrained domain differences. 2 Dataset and Annotation We consider several forums that vary in the nature of products being traded: • Darkode: Cybercriminal wares, including exploit kits, spam services, ransomware programs, and steal"
D17-1275,D11-1002,0,0.025689,"nformation extraction (with provenance information) as well as standard named-entity recognition (NER). Compared to NER, our task features a higher dependence on context: we only care about the specific product being bought or sold in a post, not other products that might be mentioned. Moreover, because we are operating over forums, the data is substantially messier than classical NER corpora like CoNLL (Tjong Kim Sang and De Meulder, 2003). While prior work has dealt with these messy characteristics for syntax (Kaljahi et al., 2015) and for discourse (Lui and Baldwin, 2010; Kim et al., 2010; Wang et al., 2011), our work is the first to tackle forum data (and marketplace forums specifically) from an information extraction perspective. Having annotated a dataset, we examine supervised and semi-supervised learning approaches to the product extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different “finegrained domains.” Since we want to avoid having to annotate data for every new forum that"
D17-1275,Q17-1036,0,0.0298178,"to train models available at https://evidencebasedsecurity.org/forums/ 2013). One domain for which automated analysis is particularly useful is Internet security: researchers obtain large amounts of text data pertinent to active threats or ongoing cybercriminal activity, for which the ability to rapidly characterize that text and draw conclusions can reap major benefits (Krebs, 2013a,b). However, conducting automatic analysis is difficult because this data is outof-domain for conventional NLP models, which harms the performance of both discrete models (McClosky et al., 2010) and deep models (Zhang et al., 2017). Not only that, we show that data from one cybercrime forum is even out of domain with respect to another cybercrime forum, making this data especially challenging. In this work, we present the task of identifying products being bought and sold in the marketplace sections of these online cybercrime forums. 2598 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2598–2607 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Words Products Annotated Annotators Inter-annotator agreement Forum Posts per post per pos"
D17-1275,N10-1004,0,0.0237298,"013; O’Connor et al., 1 Dataset and code to train models available at https://evidencebasedsecurity.org/forums/ 2013). One domain for which automated analysis is particularly useful is Internet security: researchers obtain large amounts of text data pertinent to active threats or ongoing cybercriminal activity, for which the ability to rapidly characterize that text and draw conclusions can reap major benefits (Krebs, 2013a,b). However, conducting automatic analysis is difficult because this data is outof-domain for conventional NLP models, which harms the performance of both discrete models (McClosky et al., 2010) and deep models (Zhang et al., 2017). Not only that, we show that data from one cybercrime forum is even out of domain with respect to another cybercrime forum, making this data especially challenging. In this work, we present the task of identifying products being bought and sold in the marketplace sections of these online cybercrime forums. 2598 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2598–2607 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Words Products Annotated Annotators Inter-annotator a"
D19-1131,W17-5522,0,0.0948474,"ope part of our work, with the same type of conversational agent requests. Like our work, both of these datasets were bootstrapped using crowdsourcing. However, the Snips dataset has only a small number of intents and an enormous number of examples of each. Snips does present a low-data variation, with 70 training queries per intent, in which performance drops slightly. The dataset presented in Liu et al. (2019) has a large number of intent classes, yet also contains a wide range of samples per intent class (ranging from 24 to 5,981 queries per intent, and so is not constrained in all cases). Braun et al. (2017) created datasets with constrained training data, but with very few intents, presenting a very different type of challenge. We also include the TREC query classification datasets (Li and Roth, 2002), which have a large set of labels, but they describe the desired response type (e.g., distance, city, abbreviation) rather than the action intents we consider. Moreover, TREC contains only questions and no commands. Crucially, none of the other datasets summarized in Table 4 offer a feasible way to evaluate out-ofscope performance. The Dialog State Tracking Challenge (DSTC) datasets are another rel"
D19-1131,D18-2029,0,0.0320134,"Missing"
D19-1131,N19-1423,0,0.0356808,"nts, we evaluated the performance of a range of classifier models3 and out-of-scope prediction schemes. 3.1 Classifier Models SVM: A linear support vector machine with bagof-words sentence representations. MLP: A multi-layer perceptron with USE embeddings (Cer et al., 2018) as input. FastText: A shallow neural network that averages embeddings of n-grams (Joulin et al., 2017). CNN: A convolutional neural network with nonstatic word embeddings initialized with GloVe (Pennington et al., 2014). BERT: A neural network that is trained to predict elided words in text and then fine-tuned on our data (Devlin et al., 2019). Platforms: Several platforms exist for the development of task-oriented agents. We consider Google’s DialogFlow4 and Rasa NLU5 with spacy-sklearn. 3 See the supplementary material for model details. https://dialogflow.com 5 https://github.com/RasaHQ/rasa 4 Out-of-Scope Prediction We use three baseline approaches for the task of predicting whether a query is out-of-scope: (1) oos-train, where we train an additional (i.e. 151st ) intent on out-of-scope training data; (2) oosthreshold, where we use a threshold on the classifier’s probability estimate; and (3) oos-binary, a two-stage process whe"
D19-1131,C02-1150,0,0.764686,"xt and Rasa. 4.3 Results with oos-binary Table 3 compares classifier performance using the oos-binary scheme. In-scope accuracy suffers for all models using the undersampling scheme when compared to training on the full dataset using the oos-train and oos-threshold approaches shown in Table 2. However, out-of-scope recall improves compared to oos-train on Full but not OOS+. Augmenting the out-of-scope training set appears to help improve both in-scope and out-of-scope performance compared to undersampling, but outof-scope performance remains weak. 1314 Dataset Our Dataset (This Work) TREC-6, (Li and Roth, 2002) TREC-50, (Li and Roth, 2002) Web Apps, (Braun et al., 2017) Ask Ubuntu, (Braun et al., 2017) Chatbot Corpus, (Braun et al., 2017) Snips, (Coucke et al., 2018) Liu et al. (2019) Num. Intents Num. Utterances 150 6 50 8 5 2 7 54 23,700 5,952 5,952 89 162 206 14,484 25,716 Chatbot Style Many Intents Constrained Training Data Out-of-Scope Utterances 3 7 7 7 7 3 3 3 3 7 3 7 7 7 7 3 3 7 3 3 3 3 7 7 3 7 7 7 7 7 7 7 Table 4: Classification dataset properties. Ours has the broadest range of intents and specially collected out-ofscope queries. We consider “chatbot style” queries to be short, possibly un"
D19-1131,P19-1548,0,0.181563,"asets, the idea of out-of-scope data is not considered, and instead the output classes are intended to cover all possible queries (e.g., TREC (Li and Roth, 2002)). Recent work by Hendrycks and Gimpel (2017) considers a similar problem they call outof-distribution detection. They use other datasets or classes excluded during training to form the outof-distribution samples. This means that the outof-scope samples are from a small set of coherent classes that differ substantially from the indistribution samples. Similar experiments were conducted for evaluating unknown intent discovery models in Lin and Xu (2019). In contrast, our out-of-scope queries cover a broad range of phenomena and are similar in style and often similar in topic to in-scope queries, representing things a user might say given partial knowledge of the capabilities of a system. Table 4 compares our dataset with other shortquery intent classification datasets. The Snips (Coucke et al., 2018) dataset and the dataset presented in Liu et al. (2019) are the most similar to the in-scope part of our work, with the same type of conversational agent requests. Like our work, both of these datasets were bootstrapped using crowdsourcing. Howev"
D19-1131,D14-1162,0,0.08217,"narios where data may be in limited or uneven supply. 3 Benchmark Evaluation To quantify the challenges that our new dataset presents, we evaluated the performance of a range of classifier models3 and out-of-scope prediction schemes. 3.1 Classifier Models SVM: A linear support vector machine with bagof-words sentence representations. MLP: A multi-layer perceptron with USE embeddings (Cer et al., 2018) as input. FastText: A shallow neural network that averages embeddings of n-grams (Joulin et al., 2017). CNN: A convolutional neural network with nonstatic word embeddings initialized with GloVe (Pennington et al., 2014). BERT: A neural network that is trained to predict elided words in text and then fine-tuned on our data (Devlin et al., 2019). Platforms: Several platforms exist for the development of task-oriented agents. We consider Google’s DialogFlow4 and Rasa NLU5 with spacy-sklearn. 3 See the supplementary material for model details. https://dialogflow.com 5 https://github.com/RasaHQ/rasa 4 Out-of-Scope Prediction We use three baseline approaches for the task of predicting whether a query is out-of-scope: (1) oos-train, where we train an additional (i.e. 151st ) intent on out-of-scope training data; (2"
D19-1131,W13-4065,0,0.0197296,"aining data, but with very few intents, presenting a very different type of challenge. We also include the TREC query classification datasets (Li and Roth, 2002), which have a large set of labels, but they describe the desired response type (e.g., distance, city, abbreviation) rather than the action intents we consider. Moreover, TREC contains only questions and no commands. Crucially, none of the other datasets summarized in Table 4 offer a feasible way to evaluate out-ofscope performance. The Dialog State Tracking Challenge (DSTC) datasets are another related resource. Specifically, DSTC 1 (Williams et al., 2013), DSTC 2 (Henderson et al., 2014a), and DSTC 3 (Henderson et al., 2014b) contain “chatbot style” queries, but the datasets are focused on state tracking. Moreover, most if not all queries in these datasets are in-scope. In contrast, the focus of our analysis is on both in- and out-of-scope queries that challenge a virtual assistant to determine whether it can provide an acceptable response. 6 Conclusion This paper analyzed intent classification and outof-scope prediction methods with a new dataset consisting of carefully collected out-of-scope data. Our findings indicate that certain models li"
D19-1131,D19-1107,0,0.060333,"Missing"
D19-1131,W14-4337,0,0.0574864,"Missing"
D19-1131,E17-2068,0,0.0242995,"ich there are 250 out-of-scope training examples, rather than 100. These are intended to represent production scenarios where data may be in limited or uneven supply. 3 Benchmark Evaluation To quantify the challenges that our new dataset presents, we evaluated the performance of a range of classifier models3 and out-of-scope prediction schemes. 3.1 Classifier Models SVM: A linear support vector machine with bagof-words sentence representations. MLP: A multi-layer perceptron with USE embeddings (Cer et al., 2018) as input. FastText: A shallow neural network that averages embeddings of n-grams (Joulin et al., 2017). CNN: A convolutional neural network with nonstatic word embeddings initialized with GloVe (Pennington et al., 2014). BERT: A neural network that is trained to predict elided words in text and then fine-tuned on our data (Devlin et al., 2019). Platforms: Several platforms exist for the development of task-oriented agents. We consider Google’s DialogFlow4 and Rasa NLU5 with spacy-sklearn. 3 See the supplementary material for model details. https://dialogflow.com 5 https://github.com/RasaHQ/rasa 4 Out-of-Scope Prediction We use three baseline approaches for the task of predicting whether a quer"
D19-1131,N18-3005,1,0.905257,"Missing"
L18-1492,S16-1176,0,0.0368828,"Missing"
L18-1492,P13-2131,0,0.154104,"nd relations as edges between those nodes. LDC2017T10). The 2014 dataset contains 13,000 sentences, which increased to almost 20,000 in the 2015 set. The 2016 and 2017 datasets contain around 39,000 sentences. The data consist of sentences from English broadcast conversations, weblogs, discussion forums, and newswire data and are annotated with AMR graphs. The datasets build off of each other and include corrections and extensions of the AMR specification (e.g., 2015 introduces wikification and new PropBank frames). The evaluation of these AMR parsers is typically based on the SMATCH F1 tool (Cai and Knight, 2013) which measures the overlap of concept-relation-concept triples in a generated AMR graph as compared to the gold graph. In addition, Damonte et al. (2016) introduced finer-grained evaluations of the subtasks of AMR parsing, which 3119 Percentage of Total Concept Confusion Errors in Baseline or c o ga ni untr za tio y ci ty n co co u un n try try go vt pe -o rs rg and on re b se et w ar ch ee in n ne s w sp go- titut ap e 02 er or go-0 1 co gan un iza try tio go n go priv vtor vt ac g -o y rg pr i or v ga ate ni ge zat io t-0 n un 1 g et us un -05 r ig iver sta ht sit te e y re se ntitl ar e01"
L18-1492,P05-1045,0,0.017185,"l agent, relation, matter, horror, communication, psychological feature, set, process, attribute}. train CAMR to determine an upperbound on the performance of this parser when using world knowledge. We use max-margin learning with AdaGrad instead of the perceptron method in the original code. Previous work has shown this learning method to be effective for a variety of language processing tasks and we observe the same effect (Kummerfeld et al., 2015). In all these experiments, the original CAMR parser naturally constitutes our baseline. Named Entities. We use the Stanford named entity tagger (Finkel et al., 2005), and retrain it on the training set of LDC2014T12. To retrain the tagger we need spans in the AMR training sentences and their assigned labels. Dataset and Evaluations Metrics. We evaluate our work in two ways: one is overall SMATCH performance (Cai and Knight, 2013), which most of the previous work adopts; the other is the finer-grained evaluation introduced by Damonte et al. (2016), which evaluates the quality of each subtask of AMR parsing. We focus on a few particularly relevant metrics: To generate annotated data, we use the alignments automatically generated using the JAMR aligner for A"
L18-1492,P14-1134,0,0.0412891,"ng Representation (AMR), introduced by Banarescu et al. (2012), aims to capture the semantic meaning of sentences using directed acyclic graphs where nodes are labeled with concepts and edges are labeled with relations. An example is shown in Figure 1. A number of recent studies use AMR graphs for downstream tasks (Pan et al., 2015; Liu et al., 2015; Sachan and Xing, 2016; Burns et al., 2016) and growing amounts of annotated data enable the development of statistical parsing algorithms and standardized evaluations. Several parsers have been created that generate an AMR graph given a sentence (Flanigan et al., 2014; Wang et al., 2015b; Damonte et al., 2016), but even the most recent results suggest that there is still significant room to improve the performance for this challenging task. In this paper, we explore the role of world knowledge for the task of semantic parsing with AMR. The paper makes three main contributions. First, we examine the effect of different types of world knowledge for semantic parsing with AMR for the first time.1 Second, we examine the upper bound on world knowledge using gold annotations, and provide new insights into the potential of world knowledge in computational approach"
L18-1492,S16-1186,0,0.0238197,"Missing"
L18-1492,D15-1032,1,0.80586,"s the number of classes small while still having meaningful abstractions. The resulting set of fifteen classes is {group, thing, measure, change, object, substance, causal agent, relation, matter, horror, communication, psychological feature, set, process, attribute}. train CAMR to determine an upperbound on the performance of this parser when using world knowledge. We use max-margin learning with AdaGrad instead of the perceptron method in the original code. Previous work has shown this learning method to be effective for a variety of language processing tasks and we observe the same effect (Kummerfeld et al., 2015). In all these experiments, the original CAMR parser naturally constitutes our baseline. Named Entities. We use the Stanford named entity tagger (Finkel et al., 2005), and retrain it on the training set of LDC2014T12. To retrain the tagger we need spans in the AMR training sentences and their assigned labels. Dataset and Evaluations Metrics. We evaluate our work in two ways: one is overall SMATCH performance (Cai and Knight, 2013), which most of the previous work adopts; the other is the finer-grained evaluation introduced by Damonte et al. (2016), which evaluates the quality of each subtask o"
L18-1492,N15-1114,0,0.0224069,"Missing"
L18-1492,S16-1166,0,0.0222681,"t names represent their word senses. For instance, get-01 means ‘to come into posession’, whereas get-05 means ‘to get to a state’. measure parser effectiveness in terms of capturing named entities, concepts, negations, word sense disambiguations and semantic roles. At the time these experiments were performed many of the high performing parsers were based on JAMR or CAMR (Flanigan et al., 2014; Wang et al., 2015a). We chose to base our parser on CAMR, which was the highest performing entry on SemEval 2016 Task 8 and the parser on which most of the entries for the 2016 shared task were based (May, 2016; Wang et al., 2016). Comparing to previous extensions of CAMR, our work is the first attempt to integrate various forms of world knowledge as features for AMR parsing. 3. AMR Parsing with World Knowledge We hypothesize that introducing world knowledge could potentially increase the overall performance of AMR parsers. In order to identify useful features and effective approaches to improve an existing AMR parser, we first examine the errors produced by AMR parsers. By looking at errors made by CAMR, we found that a significant number of concepts are either mislabeled or missing. The lighter, s"
L18-1492,D16-1183,0,0.0263086,"Missing"
L18-1492,N15-1119,0,0.025793,"Missing"
L18-1492,P16-2079,0,0.0265753,"Missing"
L18-1492,P15-2141,0,0.362505,", introduced by Banarescu et al. (2012), aims to capture the semantic meaning of sentences using directed acyclic graphs where nodes are labeled with concepts and edges are labeled with relations. An example is shown in Figure 1. A number of recent studies use AMR graphs for downstream tasks (Pan et al., 2015; Liu et al., 2015; Sachan and Xing, 2016; Burns et al., 2016) and growing amounts of annotated data enable the development of statistical parsing algorithms and standardized evaluations. Several parsers have been created that generate an AMR graph given a sentence (Flanigan et al., 2014; Wang et al., 2015b; Damonte et al., 2016), but even the most recent results suggest that there is still significant room to improve the performance for this challenging task. In this paper, we explore the role of world knowledge for the task of semantic parsing with AMR. The paper makes three main contributions. First, we examine the effect of different types of world knowledge for semantic parsing with AMR for the first time.1 Second, we examine the upper bound on world knowledge using gold annotations, and provide new insights into the potential of world knowledge in computational approaches to AMR parsing."
L18-1492,N15-1040,0,0.340422,", introduced by Banarescu et al. (2012), aims to capture the semantic meaning of sentences using directed acyclic graphs where nodes are labeled with concepts and edges are labeled with relations. An example is shown in Figure 1. A number of recent studies use AMR graphs for downstream tasks (Pan et al., 2015; Liu et al., 2015; Sachan and Xing, 2016; Burns et al., 2016) and growing amounts of annotated data enable the development of statistical parsing algorithms and standardized evaluations. Several parsers have been created that generate an AMR graph given a sentence (Flanigan et al., 2014; Wang et al., 2015b; Damonte et al., 2016), but even the most recent results suggest that there is still significant room to improve the performance for this challenging task. In this paper, we explore the role of world knowledge for the task of semantic parsing with AMR. The paper makes three main contributions. First, we examine the effect of different types of world knowledge for semantic parsing with AMR for the first time.1 Second, we examine the upper bound on world knowledge using gold annotations, and provide new insights into the potential of world knowledge in computational approaches to AMR parsing."
L18-1492,S16-1181,0,0.0244936,"Missing"
L18-1492,D16-1065,0,0.0275381,"Missing"
N18-1190,Q18-1008,0,0.0850707,"a large impact on how all three of these algorithms behave (Levy et al., 2015). Further work shows that the parameters of the embedding algorithm word2vec influence the geometry of word vectors and their context vectors (Mimno and Thompson, 2017). These parameters can be optimized; Hellrich and Hahn (2016) posit optimal parameters for negative sampling and the number of epochs to train for. They also demonstrate that in addition to parameter settings, word properties, such as word ambiguity, affect embedding quality. In addition to exploring word and algorithmic parameters, concurrent work by Antoniak and Mimno (2018) evaluates how document properties affect the stability of word embeddings. We also explore the stability of embeddings, but focus on a broader range of factors, and consider the effect of stability on downstream tasks. In contrast, Antoniak and Mimno focus on using word embeddings to analyze language (e.g., Garg et al., 2018), rather than to perform tasks. At a higher level of granularity, Tan et al. (2015) analyze word embedding spaces by comparing two spaces. They do this by linearly transforming one space into another space, and they show that words have different usage properties in diffe"
N18-1190,N15-1184,0,0.077191,"Missing"
N18-1190,D17-1242,1,0.835173,"roughout the rest of the paper. 1.0 1024 9998 2 32 20 Frequency in Training Corpus (approx. log-based scale) 1 0.4 0.2 0.0 Figure 2: Stability of GloVe on the PTB. Stability is measured across ten randomized embedding spaces trained on the training data of the PTB (determined using language modeling splits (Mikolov et al., 2010)). Each word is placed in a frequency bucket (left y-axis) and stability is determined using a varying number of nearest neighbors for each frequency bucket (right yaxis). Each row is normalized, and boxes with more than 0.01 of the row’s mass are outlined. tion (e.g., Garimella et al., 2017), where the top ten results are considered in the final evaluation metric. To give some intuition for how changing the number of nearest neighbors affects our stability metric, consider Figure 2. This graph shows how the stability of GloVe changes with the frequency of the word and the number of neighbors used to calculate stability; please see the figure caption for a more detailed explanation of how this graph is structured. Within each frequency bucket, the stability is consistent across varying numbers of neighbors. Ten nearest neighbors performs approximately as well as a higher number of"
N18-1190,C16-1262,0,0.132868,"–2102 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics word2vec (Mikolov et al., 2013b), and GloVe (Pennington et al., 2014). Various aspects of the embedding spaces produced by these algorithms have been previously studied. Particularly, the effect of parameter choices has a large impact on how all three of these algorithms behave (Levy et al., 2015). Further work shows that the parameters of the embedding algorithm word2vec influence the geometry of word vectors and their context vectors (Mimno and Thompson, 2017). These parameters can be optimized; Hellrich and Hahn (2016) posit optimal parameters for negative sampling and the number of epochs to train for. They also demonstrate that in addition to parameter settings, word properties, such as word ambiguity, affect embedding quality. In addition to exploring word and algorithmic parameters, concurrent work by Antoniak and Mimno (2018) evaluates how document properties affect the stability of word embeddings. We also explore the stability of embeddings, but focus on a broader range of factors, and consider the effect of stability on downstream tasks. In contrast, Antoniak and Mimno focus on using word embeddings"
N18-1190,2005.mtsummit-papers.11,0,0.00718392,", we consider the number of different POS present. For a finer-grained representation, we use the number of different WordNet senses associated with the word (Miller, 1995; Fellbaum, 1998). We also consider the number of syllables in a word, determined using the CMU Pronuncing Dictionary (Weide, 1998). If the word is not present in the dictionary, then this is set to zero. 4.3 Data Properties Data features capture properties of the training data (and the word in relation to the training data). For this model, we gather data from two sources: New York Times (NYT) (Sandhaus, 2008) and Europarl (Koehn, 2005). Overall, we consider seven domains of data: (1) NYT - U.S., (2) NYT - New York and Region, (3) NYT - Business, (4) NYT Arts, (5) NYT - Sports, (6) All of the data from 2095 Dataset NYT US NYT NY NYT Business NYT Arts NYT Sports All NYT Europarl Sentences 13,923 36,792 21,048 28,161 21,610 121,534 2,297,621 Vocab. Size 5,787 11,182 7,212 10,508 5,967 24,144 43,888 Num. Tokens / Vocab. Size 64.37 80.41 75.96 65.29 77.85 117.98 1,394.28 Table 3: Dataset statistics. domains 1-5 (denoted “All NYT”), and (7) All of English Europarl. Table 3 shows statistics about these datasets. The first five dom"
N18-1190,Q15-1016,0,0.0577765,"well as a small, but growing, amount of work analyzing the properties of word embeddings. Here, we explore three different embedding methods: PPMI (Bullinaria and Levy, 2007), 2092 Proceedings of NAACL-HLT 2018, pages 2092–2102 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics word2vec (Mikolov et al., 2013b), and GloVe (Pennington et al., 2014). Various aspects of the embedding spaces produced by these algorithms have been previously studied. Particularly, the effect of parameter choices has a large impact on how all three of these algorithms behave (Levy et al., 2015). Further work shows that the parameters of the embedding algorithm word2vec influence the geometry of word vectors and their context vectors (Mimno and Thompson, 2017). These parameters can be optimized; Hellrich and Hahn (2016) posit optimal parameters for negative sampling and the number of epochs to train for. They also demonstrate that in addition to parameter settings, word properties, such as word ambiguity, affect embedding quality. In addition to exploring word and algorithmic parameters, concurrent work by Antoniak and Mimno (2018) evaluates how document properties affect the stabili"
N18-1190,J93-2004,0,0.0610169,"Missing"
N18-1190,S07-1009,0,0.032384,"Missing"
N18-1190,D17-1308,0,0.0646003,"nd Levy, 2007), 2092 Proceedings of NAACL-HLT 2018, pages 2092–2102 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics word2vec (Mikolov et al., 2013b), and GloVe (Pennington et al., 2014). Various aspects of the embedding spaces produced by these algorithms have been previously studied. Particularly, the effect of parameter choices has a large impact on how all three of these algorithms behave (Levy et al., 2015). Further work shows that the parameters of the embedding algorithm word2vec influence the geometry of word vectors and their context vectors (Mimno and Thompson, 2017). These parameters can be optimized; Hellrich and Hahn (2016) posit optimal parameters for negative sampling and the number of epochs to train for. They also demonstrate that in addition to parameter settings, word properties, such as word ambiguity, affect embedding quality. In addition to exploring word and algorithmic parameters, concurrent work by Antoniak and Mimno (2018) evaluates how document properties affect the stability of word embeddings. We also explore the stability of embeddings, but focus on a broader range of factors, and consider the effect of stability on downstream tasks. I"
N18-1190,P16-1013,0,0.12326,"es) are often unstable. We provide empirical evidence for how various factors contribute to the stability of word embeddings, and we analyze the effects of stability on downstream tasks. 100 75 50 1 1e-01 25 1e-02 10 1e-03 5 2 20 24 28 212 216 Frequency of Word in PTB (log scale) Introduction Word embeddings are low-dimensional, dense vector representations that capture semantic properties of words. Recently, they have gained tremendous popularity in Natural Language Processing (NLP) and have been used in tasks as diverse as text similarity (Kenter and De Rijke, 2015), part-of-speech tagging (Tsvetkov et al., 2016), sentiment analysis (Faruqui et al., 2015), and machine translation (Mikolov et al., 2013a). Although word embeddings are widely used across NLP, their stability has not yet been fully evaluated and understood. In this paper, we explore the factors that play a role in the stability of word embeddings, including properties of the data, properties of the algorithm, and properties of the words. We find that word embeddings exhibit substantial instabilities, which can have implications for downstream tasks. Using the overlap between nearest neighbors in an embedding space as a measure of stabilit"
N18-1190,D14-1162,0,0.10588,"at word embeddings are commonly used for. To our knowledge, this is the first study comprehensively examining the factors behind instability. 2 Related Work There has been much recent interest in the applications of word embeddings, as well as a small, but growing, amount of work analyzing the properties of word embeddings. Here, we explore three different embedding methods: PPMI (Bullinaria and Levy, 2007), 2092 Proceedings of NAACL-HLT 2018, pages 2092–2102 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics word2vec (Mikolov et al., 2013b), and GloVe (Pennington et al., 2014). Various aspects of the embedding spaces produced by these algorithms have been previously studied. Particularly, the effect of parameter choices has a large impact on how all three of these algorithms behave (Levy et al., 2015). Further work shows that the parameters of the embedding algorithm word2vec influence the geometry of word vectors and their context vectors (Mimno and Thompson, 2017). These parameters can be optimized; Hellrich and Hahn (2016) posit optimal parameters for negative sampling and the number of epochs to train for. They also demonstrate that in addition to parameter set"
N18-1190,petrov-etal-2012-universal,0,0.041853,"S) of the word. Both of these are represented as bags-of-words of all possible POS, and are determined by looking at the primary (most frequent) and secondary (second most frequent) POS of the word in the Brown corpus3 (Francis and Kucera, 1979). If the word is not present in the Brown corpus, then all of these POS features are set to zero. To get a coarse-grained representation of the 3 Here, we use the universal tagset, which consists of twelve possible POS: adjective, adposition, adverb, conjunction, determiner / article, noun, numeral, particle, pronoun, verb, punctuation mark, and other (Petrov et al., 2012). Word Properties Primary part-of-speech Secondary part-of-speech # Parts-of-speech # WordNet senses Syllables Data Properties Raw frequency in corpus A Raw frequency in corpus B Diff. in raw frequency Vocab. size of corpus A Vocab. size of corpus B Diff. in vocab. size Overlap in corpora vocab. Domains present Do the domains match? Training position in A Training position in B Diff. in training position Algorithm Properties Algorithms present Do the algorithms match? Embedding dimension of A Embedding dimension of B Diff. in dimension Do the dimensions match? Adjective Noun 2 3 5 106 669 563"
N18-1190,P15-2108,0,0.0226935,"t in addition to parameter settings, word properties, such as word ambiguity, affect embedding quality. In addition to exploring word and algorithmic parameters, concurrent work by Antoniak and Mimno (2018) evaluates how document properties affect the stability of word embeddings. We also explore the stability of embeddings, but focus on a broader range of factors, and consider the effect of stability on downstream tasks. In contrast, Antoniak and Mimno focus on using word embeddings to analyze language (e.g., Garg et al., 2018), rather than to perform tasks. At a higher level of granularity, Tan et al. (2015) analyze word embedding spaces by comparing two spaces. They do this by linearly transforming one space into another space, and they show that words have different usage properties in different domains (in their case, Twitter and Wikipedia). Finally, embeddings can be analyzed using second-order properties of embeddings (e.g., how a word relates to the words around it). NewmanGriffis and Fosler-Lussier (2017) validate the usefulness of second-order properties, by demonstrating that embeddings based on second-order properties perform as well as the typical first-order embeddings. Here, we use s"
N18-2099,D09-1030,0,0.0283319,"time-consuming (Hovy et al., 2006). During the last decade, crowdsourcing has been broadly applied to collect natural language data at large scale with reasonable costs (Snow et al., 2008), including for translation (Zaidan and Callison-Burch, 2011), paraphrasing (Burrows et al., 2013; Jiang et al., 2017), dialog generation (Lasecki et al., 2013b,a), and annotation of corpora in tasks like sentiment classification (Hsueh et al., 2009). Since individual workers’ outputs are usually error-prone, aggregation mechanisms such as majority voting (Raykar et al., 2010) and quality verification tasks (Callison-Burch, 2009) have been developed to improve consistency. However, the results receiving the most votes may still miss information that should be included. To address this issue, crowdsourced iterative methods have been developed to divide a complicated task into 629 Person A: I am a CS major and need to schedule classes for next semester. Person B: It looks like you have most of your prerequisites out of the way and you can start taking some more EECS classes. Person A: Cool, I’m very interested in Software Infrastructure Applications and web app. Rewrite Questions Please read each conversation below and"
N18-2099,W04-1008,0,0.0621862,"arization we are interested in does not fit within either query-based or update summarization. To clarify the relationships between all of these different summarization tasks, we propose a new term, aspect-based summarization, and present a hierarchy of tasks. In data mining, recent work has explored summarizing different aspects of graph data given domain context (Jin and Koutra, 2017). In NLP, previous summarization tasks have explored summarization based on information types in individual domains, such as opinion summarization (Condori and Pardo, 2017) and task-focused email summarization (Corston-Oliver et al., 2004). Performance on these tasks is usually lower than traditional summarization tasks due to the difficulty of identifying relevant information in noisy text. We introduce a new crowdsourcing workflow, PinRefine, that improves the quality of data collection for specialized summarization tasks. 2.2 Figure 1: Proposed hierarchy of summarization tasks. a series of micro-stages, each with a different focus (Little et al., 2010; Merritt et al., 2017). For example, Ouyang et al. (2017) developed a dataset of aligned extractive and abstractive summaries by creating separate tasks for summarization, alig"
N18-2099,N06-2015,0,0.0408713,"er as the task of generating a summary that captures the part of a document relevant to the user’s information request. It includes (1) query-based summarization, where the information request is a query indicating the desired topic, (2) update summarization, where the request is whether information is new, and (3) aspect-based summarization where the request is the desired information type, Crowdsourced Corpus Generation Large corpora are critical for training robust natural language processing systems, but traditional expert-driven data collection methods are both costly and time-consuming (Hovy et al., 2006). During the last decade, crowdsourcing has been broadly applied to collect natural language data at large scale with reasonable costs (Snow et al., 2008), including for translation (Zaidan and Callison-Burch, 2011), paraphrasing (Burrows et al., 2013; Jiang et al., 2017), dialog generation (Lasecki et al., 2013b,a), and annotation of corpora in tasks like sentiment classification (Hsueh et al., 2009). Since individual workers’ outputs are usually error-prone, aggregation mechanisms such as majority voting (Raykar et al., 2010) and quality verification tasks (Callison-Burch, 2009) have been de"
N18-2099,E17-2008,0,0.0855776,"ividual domains, such as opinion summarization (Condori and Pardo, 2017) and task-focused email summarization (Corston-Oliver et al., 2004). Performance on these tasks is usually lower than traditional summarization tasks due to the difficulty of identifying relevant information in noisy text. We introduce a new crowdsourcing workflow, PinRefine, that improves the quality of data collection for specialized summarization tasks. 2.2 Figure 1: Proposed hierarchy of summarization tasks. a series of micro-stages, each with a different focus (Little et al., 2010; Merritt et al., 2017). For example, Ouyang et al. (2017) developed a dataset of aligned extractive and abstractive summaries by creating separate tasks for summarization, alignment, and classification of changes. However, maintaining accuracy when the complexity of the given text increases has remained an open question. In our Pin-Refine workflow, workers first identified all text relevant to the given information type, which was aggregated across workers with a threshold, then wrote the summary using that information. This aggregation and priming helps maintain accuracy as text grows more complex. 3 Targeted and Aspect-Based Summarization Traditio"
N18-2099,W09-1904,0,0.0314742,"rced Corpus Generation Large corpora are critical for training robust natural language processing systems, but traditional expert-driven data collection methods are both costly and time-consuming (Hovy et al., 2006). During the last decade, crowdsourcing has been broadly applied to collect natural language data at large scale with reasonable costs (Snow et al., 2008), including for translation (Zaidan and Callison-Burch, 2011), paraphrasing (Burrows et al., 2013; Jiang et al., 2017), dialog generation (Lasecki et al., 2013b,a), and annotation of corpora in tasks like sentiment classification (Hsueh et al., 2009). Since individual workers’ outputs are usually error-prone, aggregation mechanisms such as majority voting (Raykar et al., 2010) and quality verification tasks (Callison-Burch, 2009) have been developed to improve consistency. However, the results receiving the most votes may still miss information that should be included. To address this issue, crowdsourced iterative methods have been developed to divide a complicated task into 629 Person A: I am a CS major and need to schedule classes for next semester. Person B: It looks like you have most of your prerequisites out of the way and you can s"
N18-2099,P17-2017,1,0.760116,"equest is whether information is new, and (3) aspect-based summarization where the request is the desired information type, Crowdsourced Corpus Generation Large corpora are critical for training robust natural language processing systems, but traditional expert-driven data collection methods are both costly and time-consuming (Hovy et al., 2006). During the last decade, crowdsourcing has been broadly applied to collect natural language data at large scale with reasonable costs (Snow et al., 2008), including for translation (Zaidan and Callison-Burch, 2011), paraphrasing (Burrows et al., 2013; Jiang et al., 2017), dialog generation (Lasecki et al., 2013b,a), and annotation of corpora in tasks like sentiment classification (Hsueh et al., 2009). Since individual workers’ outputs are usually error-prone, aggregation mechanisms such as majority voting (Raykar et al., 2010) and quality verification tasks (Callison-Burch, 2009) have been developed to improve consistency. However, the results receiving the most votes may still miss information that should be included. To address this issue, crowdsourced iterative methods have been developed to divide a complicated task into 629 Person A: I am a CS major and"
N18-2099,N04-4027,0,0.123053,"Missing"
N18-2099,D08-1027,0,0.194557,"Missing"
N18-2099,P11-1122,0,0.0395193,"Missing"
N18-3005,Q17-1010,0,0.039241,"the data, lacking quantitative investigation of the data’s impact on downstream model performance (Jiang et al., 2017). In this paper, we propose two novel modelindepedent metrics to evaluate dataset quality. Specifically, we introduce (1) coverage, quantifying how well a training set covers the expression space of a certain task, and (2) diversity, quantifying the heterogeneity of sentences in the training set. We verify the effectiveness of both metrics by correlating them with the model accuracy of two well-known algorithms, SVM (Cortes and Vapnik, 1995) and FastText (Joulin et al., 2017; Bojanowski et al., 2017). We show that while diversity gives a sense of the variation in the data, coverage closely correlates with the model accuracy and serves as an effective metric for evaluating training data quality. We then describe in detail two crowdsourcing methods we use to collect intent classification data for our deployed dialogue system. The key ideas of these two methods are (1) describing the intent as a scenario or (2) providing an example sentence to be paraphrased. We experiment multiple variants of these methods by varying the number and type of prompts and collect training data using each varian"
N18-3005,P11-1020,0,0.060949,"(1) Correlating Metrics with Model Accuracy In order to evaluate the effectiveness of diversity and coverage at representing the training data quality, we collect training data via different methods and of varying sizes, train actual models, measure their accuracy and investigate the correlation between the metrics and the accuracy. We consider two well-known algorithms that have publicly available implementations: a linear SVM and FastText, a neural network-based algorithm. where N is the maximum n-gram length. We use N = 3 in our experiments. Our pairwise score is similar to the PINC score (Chen and Dolan, 2011), except that we use the n-grams from the union of both sentences instead of just one sentence in the denominator of Equation 1. This is because the PINC score is used in paraphrasing tasks to measures how much a paraphrased sentence differ from the original sentence and specifically rewards n-grams that are unique to the paraphrsed sentence. Our metric measures the semantic distance between two sentences and treat the unique n-grams in both sentences as equal contribution to the distance. We define the diversity of a training set as the average distance between all sentence pairs that share t"
N18-3005,W10-0727,0,0.065564,"Missing"
N18-3005,P17-2017,1,0.751022,"nd at Starbucks recently?”. • During the data collection process, how can we identify the point when additional data would have diminishing returns on the performance of the downstream trained models? • Which crowdsourcing method yields the highest-quality training data for intent classification in a production dialogue system? There is limited work on effective techniques to evaluate a crowdsourcing method and the data collected using that method. Prior work has focused on intrinsic analysis of the data, lacking quantitative investigation of the data’s impact on downstream model performance (Jiang et al., 2017). In this paper, we propose two novel modelindepedent metrics to evaluate dataset quality. Specifically, we introduce (1) coverage, quantifying how well a training set covers the expression space of a certain task, and (2) diversity, quantifying the heterogeneity of sentences in the training set. We verify the effectiveness of both metrics by correlating them with the model accuracy of two well-known algorithms, SVM (Cortes and Vapnik, 1995) and FastText (Joulin et al., 2017; Bojanowski et al., 2017). We show that while diversity gives a sense of the variation in the data, coverage closely cor"
N18-3005,sabou-etal-2014-corpus,0,0.0660488,"Missing"
N18-3005,E17-2068,0,0.301949,"intrinsic analysis of the data, lacking quantitative investigation of the data’s impact on downstream model performance (Jiang et al., 2017). In this paper, we propose two novel modelindepedent metrics to evaluate dataset quality. Specifically, we introduce (1) coverage, quantifying how well a training set covers the expression space of a certain task, and (2) diversity, quantifying the heterogeneity of sentences in the training set. We verify the effectiveness of both metrics by correlating them with the model accuracy of two well-known algorithms, SVM (Cortes and Vapnik, 1995) and FastText (Joulin et al., 2017; Bojanowski et al., 2017). We show that while diversity gives a sense of the variation in the data, coverage closely correlates with the model accuracy and serves as an effective metric for evaluating training data quality. We then describe in detail two crowdsourcing methods we use to collect intent classification data for our deployed dialogue system. The key ideas of these two methods are (1) describing the intent as a scenario or (2) providing an example sentence to be paraphrased. We experiment multiple variants of these methods by varying the number and type of prompts and collect train"
N18-3005,D08-1027,0,0.136873,"Missing"
N18-3005,W10-0721,0,0.044803,"is Rogstadius et al. (2011), who also considered how task framing can impact behavior. Their study made a more drastic change than ours though, attempting to shift workers’ intrinsic motivation by changing the perspective to be about assisting a non-profit organization. While this shift did have a significant impact on worker behavior, it is often not applicable. More generally, starting with the work of Snow et al. (2008) there have been several investigations of crowdsourcing design for natural language processing tasks. Factors that have been considered include quality control mechanisms (Rashtchian et al., 2010), payment rates and task descriptions (Grady and Lease, 2010), task naming (Vliegendhart et al., 2011), and worker qualification requirements (Kazai et al., 2013). Other studies have focused on exploring variations for specific tasks, such as named entity recognition (Feyisetan et al., 2017). Recent work has started to combine and summarize these observations together into consistent guidelines (Sabou et al., 2014), though the range of tasks and design factors makes the scope of such guidelines large. Our work adds to this literature, introducing new metrics and evaluation methods to guide cro"
N19-1051,W99-0606,0,0.116239,"on, the task of finding ex2 2.1 Background and Related Work Outlier Detection Outlier detection (Rousseeuw and Leroy, 1987), also called outlier analysis (Aggarwal, 2015) or anomaly detection (Chandola et al., 2009), is the task of identifying examples in a dataset that dif517 Proceedings of NAACL-HLT 2019, pages 517–527 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics fer substantially from the rest of the data. 2.2 For almost two decades, a body of work in NLP has investigated applying these ideas to data in order to identify annotation errors (Abney et al., 1999). Approaches have included the use of scores from trained models for POS tagging (Abney et al., 1999; Eskin, 2000; van Halteren, 2000; Dligach and Palmer, 2011; Fukumoto and Suzuki, 2004), count-based methods that compare examples from across the corpus (Nakagawa and Matsumoto, 2002; Hollenstein et al., 2016), characterizing data based on feature vectors projected down into a low-dimensional space (Guthrie et al., 2008), and tracking the difficulty of learning each example during training (Amiri et al., 2018). One particularly effective approach has been to find n-grams that match but have dif"
N19-1051,N18-1182,0,0.019198,"has investigated applying these ideas to data in order to identify annotation errors (Abney et al., 1999). Approaches have included the use of scores from trained models for POS tagging (Abney et al., 1999; Eskin, 2000; van Halteren, 2000; Dligach and Palmer, 2011; Fukumoto and Suzuki, 2004), count-based methods that compare examples from across the corpus (Nakagawa and Matsumoto, 2002; Hollenstein et al., 2016), characterizing data based on feature vectors projected down into a low-dimensional space (Guthrie et al., 2008), and tracking the difficulty of learning each example during training (Amiri et al., 2018). One particularly effective approach has been to find n-grams that match but have different labels, as shown for annotations including POS tags (Dickinson and Meurers, 2003), syntactic parses (Dickinson and Meurers, 2005; Dickinson, 2010; Dickinson and Smith, 2011), and predicate-argument relations (Dickinson and Lee, 2008). Our work instead uses continuous representations of text derived from neural networks. We build on prior work employing online crowd workers to create data by paraphrasing. In particular, we refine the idea of iteratively asking for paraphrases, where each round prompts w"
N19-1051,W10-0735,0,0.0330724,"), and predicate-argument relations (Dickinson and Lee, 2008). Our work instead uses continuous representations of text derived from neural networks. We build on prior work employing online crowd workers to create data by paraphrasing. In particular, we refine the idea of iteratively asking for paraphrases, where each round prompts workers with sentences from the previous round, leading to more diverse data (Negri et al., 2012; Jiang et al., 2017; Kang et al., 2018). We also apply the idea of a multi-stage process, in which a second set of workers check paraphrases to ensure they are correct (Buzek et al., 2010; Burrows et al., 2013; Coucke et al., 2018). Most notably, by incorporating our outlier detection method, we are able to automate detecting detrimental data points while also prompting workers in subsequent rounds to paraphrase more unique examples. 3 Data Collection Outlier Detection We propose a new outlier detection approach using continuous representations of sentences. Using that approach, we explored two applications: (1) identifying errors in crowdsourced data, and (2) guiding data collection in an iterative pipeline. 3.1 While finding errors is an extremely useful application of outli"
N19-1051,A00-2020,0,0.260233,"e is effective at finding errors while our data collection pipeline yields highly diverse corpora that in turn produce more robust intent classification and slot-filling models. 1 Introduction High-quality annotated data is one of the fundamental drivers of progress in Natural Language Processing (e.g. Marcus et al., 1993; Koehn, 2005). In order to be effective at producing an accurate and robust model, a dataset needs to be correct while also diverse enough to cover the full range of ways in which the phenomena it targets occur. Substantial research effort has considered dataset correctness (Eskin, 2000; Dickinson and Meurers, 2003; Rehbein and Ruppenhofer, 2017), particularly for crowdsourcing (Snow et al., 2008; Jiang et al., 2017), but addressing diversity in data has received less attention, with the exception of using data from diverse domains (Hovy et al., 2006). Outlier detection, the task of finding ex2 2.1 Background and Related Work Outlier Detection Outlier detection (Rousseeuw and Leroy, 1987), also called outlier analysis (Aggarwal, 2015) or anomaly detection (Chandola et al., 2009), is the task of identifying examples in a dataset that dif517 Proceedings of NAACL-HLT 2019, page"
N19-1051,C04-1125,0,0.0826509,"y detection (Chandola et al., 2009), is the task of identifying examples in a dataset that dif517 Proceedings of NAACL-HLT 2019, pages 517–527 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics fer substantially from the rest of the data. 2.2 For almost two decades, a body of work in NLP has investigated applying these ideas to data in order to identify annotation errors (Abney et al., 1999). Approaches have included the use of scores from trained models for POS tagging (Abney et al., 1999; Eskin, 2000; van Halteren, 2000; Dligach and Palmer, 2011; Fukumoto and Suzuki, 2004), count-based methods that compare examples from across the corpus (Nakagawa and Matsumoto, 2002; Hollenstein et al., 2016), characterizing data based on feature vectors projected down into a low-dimensional space (Guthrie et al., 2008), and tracking the difficulty of learning each example during training (Amiri et al., 2018). One particularly effective approach has been to find n-grams that match but have different labels, as shown for annotations including POS tags (Dickinson and Meurers, 2003), syntactic parses (Dickinson and Meurers, 2005; Dickinson, 2010; Dickinson and Smith, 2011), and p"
N19-1051,D18-2029,0,0.0379485,"Missing"
N19-1051,L18-1559,0,0.0232428,"t our representations to capture the semantic structure of the space for each class. An example that is far away from other examples in the set is therefore less semantically similar in some sense, making it an outlier. Importantly, it may be an outlier for two distinct reasons: (1) it is not a valid instance of this class (i.e., an error), or (2) it is an unusual example of the class (i.e., unique). This approach is applied independently to each class of data. As example applications we consider Finally, a related but distinct topic is novelty detection (Soboroff and Harman, 2005; Lee, 2015; Ghosal et al., 2018), in which two sets of documents are provided, one that is assumed to be known, and one that may contain new content. The task is to identify novel content in the second set. While outlier detection methods are often applied to this problem, the inclusion of the known document set makes the task fundamentally different from the problem we consider in this work. 518 two dialog system tasks: intent classification and slot-filling. For classification, data for each possible intent label is considered separately, meaning we find outliers in the data by considering one intent class at a time. For s"
N19-1051,guthrie-etal-2008-unsupervised,0,0.44261,"r substantially from the rest of the data. 2.2 For almost two decades, a body of work in NLP has investigated applying these ideas to data in order to identify annotation errors (Abney et al., 1999). Approaches have included the use of scores from trained models for POS tagging (Abney et al., 1999; Eskin, 2000; van Halteren, 2000; Dligach and Palmer, 2011; Fukumoto and Suzuki, 2004), count-based methods that compare examples from across the corpus (Nakagawa and Matsumoto, 2002; Hollenstein et al., 2016), characterizing data based on feature vectors projected down into a low-dimensional space (Guthrie et al., 2008), and tracking the difficulty of learning each example during training (Amiri et al., 2018). One particularly effective approach has been to find n-grams that match but have different labels, as shown for annotations including POS tags (Dickinson and Meurers, 2003), syntactic parses (Dickinson and Meurers, 2005; Dickinson, 2010; Dickinson and Smith, 2011), and predicate-argument relations (Dickinson and Lee, 2008). Our work instead uses continuous representations of text derived from neural networks. We build on prior work employing online crowd workers to create data by paraphrasing. In parti"
N19-1051,W00-1907,0,0.394733,"Missing"
N19-1051,P10-1075,0,0.0338103,"igach and Palmer, 2011; Fukumoto and Suzuki, 2004), count-based methods that compare examples from across the corpus (Nakagawa and Matsumoto, 2002; Hollenstein et al., 2016), characterizing data based on feature vectors projected down into a low-dimensional space (Guthrie et al., 2008), and tracking the difficulty of learning each example during training (Amiri et al., 2018). One particularly effective approach has been to find n-grams that match but have different labels, as shown for annotations including POS tags (Dickinson and Meurers, 2003), syntactic parses (Dickinson and Meurers, 2005; Dickinson, 2010; Dickinson and Smith, 2011), and predicate-argument relations (Dickinson and Lee, 2008). Our work instead uses continuous representations of text derived from neural networks. We build on prior work employing online crowd workers to create data by paraphrasing. In particular, we refine the idea of iteratively asking for paraphrases, where each round prompts workers with sentences from the previous round, leading to more diverse data (Negri et al., 2012; Jiang et al., 2017; Kang et al., 2018). We also apply the idea of a multi-stage process, in which a second set of workers check paraphrases t"
N19-1051,L16-1629,0,0.0821048,"019, pages 517–527 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics fer substantially from the rest of the data. 2.2 For almost two decades, a body of work in NLP has investigated applying these ideas to data in order to identify annotation errors (Abney et al., 1999). Approaches have included the use of scores from trained models for POS tagging (Abney et al., 1999; Eskin, 2000; van Halteren, 2000; Dligach and Palmer, 2011; Fukumoto and Suzuki, 2004), count-based methods that compare examples from across the corpus (Nakagawa and Matsumoto, 2002; Hollenstein et al., 2016), characterizing data based on feature vectors projected down into a low-dimensional space (Guthrie et al., 2008), and tracking the difficulty of learning each example during training (Amiri et al., 2018). One particularly effective approach has been to find n-grams that match but have different labels, as shown for annotations including POS tags (Dickinson and Meurers, 2003), syntactic parses (Dickinson and Meurers, 2005; Dickinson, 2010; Dickinson and Smith, 2011), and predicate-argument relations (Dickinson and Lee, 2008). Our work instead uses continuous representations of text derived fro"
N19-1051,dickinson-lee-2008-detecting,0,0.0428582,"mpare examples from across the corpus (Nakagawa and Matsumoto, 2002; Hollenstein et al., 2016), characterizing data based on feature vectors projected down into a low-dimensional space (Guthrie et al., 2008), and tracking the difficulty of learning each example during training (Amiri et al., 2018). One particularly effective approach has been to find n-grams that match but have different labels, as shown for annotations including POS tags (Dickinson and Meurers, 2003), syntactic parses (Dickinson and Meurers, 2005; Dickinson, 2010; Dickinson and Smith, 2011), and predicate-argument relations (Dickinson and Lee, 2008). Our work instead uses continuous representations of text derived from neural networks. We build on prior work employing online crowd workers to create data by paraphrasing. In particular, we refine the idea of iteratively asking for paraphrases, where each round prompts workers with sentences from the previous round, leading to more diverse data (Negri et al., 2012; Jiang et al., 2017; Kang et al., 2018). We also apply the idea of a multi-stage process, in which a second set of workers check paraphrases to ensure they are correct (Buzek et al., 2010; Burrows et al., 2013; Coucke et al., 2018"
N19-1051,N06-2015,0,0.0258009,"ess in Natural Language Processing (e.g. Marcus et al., 1993; Koehn, 2005). In order to be effective at producing an accurate and robust model, a dataset needs to be correct while also diverse enough to cover the full range of ways in which the phenomena it targets occur. Substantial research effort has considered dataset correctness (Eskin, 2000; Dickinson and Meurers, 2003; Rehbein and Ruppenhofer, 2017), particularly for crowdsourcing (Snow et al., 2008; Jiang et al., 2017), but addressing diversity in data has received less attention, with the exception of using data from diverse domains (Hovy et al., 2006). Outlier detection, the task of finding ex2 2.1 Background and Related Work Outlier Detection Outlier detection (Rousseeuw and Leroy, 1987), also called outlier analysis (Aggarwal, 2015) or anomaly detection (Chandola et al., 2009), is the task of identifying examples in a dataset that dif517 Proceedings of NAACL-HLT 2019, pages 517–527 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics fer substantially from the rest of the data. 2.2 For almost two decades, a body of work in NLP has investigated applying these ideas to data in order to identify an"
N19-1051,E03-1068,0,0.41042,"e at finding errors while our data collection pipeline yields highly diverse corpora that in turn produce more robust intent classification and slot-filling models. 1 Introduction High-quality annotated data is one of the fundamental drivers of progress in Natural Language Processing (e.g. Marcus et al., 1993; Koehn, 2005). In order to be effective at producing an accurate and robust model, a dataset needs to be correct while also diverse enough to cover the full range of ways in which the phenomena it targets occur. Substantial research effort has considered dataset correctness (Eskin, 2000; Dickinson and Meurers, 2003; Rehbein and Ruppenhofer, 2017), particularly for crowdsourcing (Snow et al., 2008; Jiang et al., 2017), but addressing diversity in data has received less attention, with the exception of using data from diverse domains (Hovy et al., 2006). Outlier detection, the task of finding ex2 2.1 Background and Related Work Outlier Detection Outlier detection (Rousseeuw and Leroy, 1987), also called outlier analysis (Aggarwal, 2015) or anomaly detection (Chandola et al., 2009), is the task of identifying examples in a dataset that dif517 Proceedings of NAACL-HLT 2019, pages 517–527 c Minneapolis, Minn"
N19-1051,P17-2017,1,0.918675,"obust intent classification and slot-filling models. 1 Introduction High-quality annotated data is one of the fundamental drivers of progress in Natural Language Processing (e.g. Marcus et al., 1993; Koehn, 2005). In order to be effective at producing an accurate and robust model, a dataset needs to be correct while also diverse enough to cover the full range of ways in which the phenomena it targets occur. Substantial research effort has considered dataset correctness (Eskin, 2000; Dickinson and Meurers, 2003; Rehbein and Ruppenhofer, 2017), particularly for crowdsourcing (Snow et al., 2008; Jiang et al., 2017), but addressing diversity in data has received less attention, with the exception of using data from diverse domains (Hovy et al., 2006). Outlier detection, the task of finding ex2 2.1 Background and Related Work Outlier Detection Outlier detection (Rousseeuw and Leroy, 1987), also called outlier analysis (Aggarwal, 2015) or anomaly detection (Chandola et al., 2009), is the task of identifying examples in a dataset that dif517 Proceedings of NAACL-HLT 2019, pages 517–527 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics fer substantially from the"
N19-1051,P05-1040,0,0.0462111,"2000; van Halteren, 2000; Dligach and Palmer, 2011; Fukumoto and Suzuki, 2004), count-based methods that compare examples from across the corpus (Nakagawa and Matsumoto, 2002; Hollenstein et al., 2016), characterizing data based on feature vectors projected down into a low-dimensional space (Guthrie et al., 2008), and tracking the difficulty of learning each example during training (Amiri et al., 2018). One particularly effective approach has been to find n-grams that match but have different labels, as shown for annotations including POS tags (Dickinson and Meurers, 2003), syntactic parses (Dickinson and Meurers, 2005; Dickinson, 2010; Dickinson and Smith, 2011), and predicate-argument relations (Dickinson and Lee, 2008). Our work instead uses continuous representations of text derived from neural networks. We build on prior work employing online crowd workers to create data by paraphrasing. In particular, we refine the idea of iteratively asking for paraphrases, where each round prompts workers with sentences from the previous round, leading to more diverse data (Negri et al., 2012; Jiang et al., 2017; Kang et al., 2018). We also apply the idea of a multi-stage process, in which a second set of workers ch"
N19-1051,E17-2068,0,0.0217779,"The main reason to increase dataset diversity is to construct more robust models. To directly evaluate that objective, we randomly divided the datasets collected by each pipeline into training and test sets (85-15 split). Our intuition is that a robust model should perform fairly well across all test sets. Training on a dataset that is not diverse will lead to a brittle model that only does well on data collected with the same seed sentences. For intent classification, we measure accuracy of two models: an SVM (Cortes and Vapnik, 1995) using bag of words feature representation, and FastText (Joulin et al., 2017), a neural network that averages across sentence embeddings and passes the result through feedforward layers. For slot-filling, we measure the F1 score of a bi-directional LSTM with word vectors that are trained, but initialized with GloVe 300dimensional embeddings. For all models, we average results across 10 runs. Approaches As well as our proposed data collection pipeline (unique), we consider a variant where the next seed is chosen randomly (random), and one where the seeds are the same in every round (same). The third case is equivalent to the standard pipeline from Figure 1a. All three p"
N19-1051,W11-2929,0,0.023678,"2011; Fukumoto and Suzuki, 2004), count-based methods that compare examples from across the corpus (Nakagawa and Matsumoto, 2002; Hollenstein et al., 2016), characterizing data based on feature vectors projected down into a low-dimensional space (Guthrie et al., 2008), and tracking the difficulty of learning each example during training (Amiri et al., 2018). One particularly effective approach has been to find n-grams that match but have different labels, as shown for annotations including POS tags (Dickinson and Meurers, 2003), syntactic parses (Dickinson and Meurers, 2005; Dickinson, 2010; Dickinson and Smith, 2011), and predicate-argument relations (Dickinson and Lee, 2008). Our work instead uses continuous representations of text derived from neural networks. We build on prior work employing online crowd workers to create data by paraphrasing. In particular, we refine the idea of iteratively asking for paraphrases, where each round prompts workers with sentences from the previous round, leading to more diverse data (Negri et al., 2012; Jiang et al., 2017; Kang et al., 2018). We also apply the idea of a multi-stage process, in which a second set of workers check paraphrases to ensure they are correct (B"
N19-1051,N18-3005,1,0.873439,"otations including POS tags (Dickinson and Meurers, 2003), syntactic parses (Dickinson and Meurers, 2005; Dickinson, 2010; Dickinson and Smith, 2011), and predicate-argument relations (Dickinson and Lee, 2008). Our work instead uses continuous representations of text derived from neural networks. We build on prior work employing online crowd workers to create data by paraphrasing. In particular, we refine the idea of iteratively asking for paraphrases, where each round prompts workers with sentences from the previous round, leading to more diverse data (Negri et al., 2012; Jiang et al., 2017; Kang et al., 2018). We also apply the idea of a multi-stage process, in which a second set of workers check paraphrases to ensure they are correct (Buzek et al., 2010; Burrows et al., 2013; Coucke et al., 2018). Most notably, by incorporating our outlier detection method, we are able to automate detecting detrimental data points while also prompting workers in subsequent rounds to paraphrase more unique examples. 3 Data Collection Outlier Detection We propose a new outlier detection approach using continuous representations of sentences. Using that approach, we explored two applications: (1) identifying errors"
N19-1051,W11-0408,0,0.0224011,"(Aggarwal, 2015) or anomaly detection (Chandola et al., 2009), is the task of identifying examples in a dataset that dif517 Proceedings of NAACL-HLT 2019, pages 517–527 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics fer substantially from the rest of the data. 2.2 For almost two decades, a body of work in NLP has investigated applying these ideas to data in order to identify annotation errors (Abney et al., 1999). Approaches have included the use of scores from trained models for POS tagging (Abney et al., 1999; Eskin, 2000; van Halteren, 2000; Dligach and Palmer, 2011; Fukumoto and Suzuki, 2004), count-based methods that compare examples from across the corpus (Nakagawa and Matsumoto, 2002; Hollenstein et al., 2016), characterizing data based on feature vectors projected down into a low-dimensional space (Guthrie et al., 2008), and tracking the difficulty of learning each example during training (Amiri et al., 2018). One particularly effective approach has been to find n-grams that match but have different labels, as shown for annotations including POS tags (Dickinson and Meurers, 2003), syntactic parses (Dickinson and Meurers, 2005; Dickinson, 2010; Dicki"
N19-1051,H05-1014,0,0.0454544,"behind this approach is that we expect our representations to capture the semantic structure of the space for each class. An example that is far away from other examples in the set is therefore less semantically similar in some sense, making it an outlier. Importantly, it may be an outlier for two distinct reasons: (1) it is not a valid instance of this class (i.e., an error), or (2) it is an unusual example of the class (i.e., unique). This approach is applied independently to each class of data. As example applications we consider Finally, a related but distinct topic is novelty detection (Soboroff and Harman, 2005; Lee, 2015; Ghosal et al., 2018), in which two sets of documents are provided, one that is assumed to be known, and one that may contain new content. The task is to identify novel content in the second set. While outlier detection methods are often applied to this problem, the inclusion of the known document set makes the task fundamentally different from the problem we consider in this work. 518 two dialog system tasks: intent classification and slot-filling. For classification, data for each possible intent label is considered separately, meaning we find outliers in the data by considering"
N19-1051,2005.mtsummit-papers.11,0,0.0308152,"bined with distance-based outlier detection. We also present a novel data collection pipeline built atop our detection technique to automatically and iteratively mine unique data samples while discarding erroneous samples. Experiments show that our outlier detection technique is effective at finding errors while our data collection pipeline yields highly diverse corpora that in turn produce more robust intent classification and slot-filling models. 1 Introduction High-quality annotated data is one of the fundamental drivers of progress in Natural Language Processing (e.g. Marcus et al., 1993; Koehn, 2005). In order to be effective at producing an accurate and robust model, a dataset needs to be correct while also diverse enough to cover the full range of ways in which the phenomena it targets occur. Substantial research effort has considered dataset correctness (Eskin, 2000; Dickinson and Meurers, 2003; Rehbein and Ruppenhofer, 2017), particularly for crowdsourcing (Snow et al., 2008; Jiang et al., 2017), but addressing diversity in data has received less attention, with the exception of using data from diverse domains (Hovy et al., 2006). Outlier detection, the task of finding ex2 2.1 Backgro"
N19-1051,I17-1026,0,0.0227064,"ence Encoder (USE; Cer et al., 2018) A Deep Averaging Network method, which averages word embeddings and passes the result through a feedforward network. The USE is trained using a range of supervised and unsupervised tasks. Smooth Inverse Frequency (SIF; Arora et al., 2017) A weighted average of word embeddings, with weights determined by word frequency within a corpus. We consider word embeddings from GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018). Average An unweighted average of word embeddings. While simple, this approach has been shown to be effective for classification (Zhang and Wallace, 2017) and other downstream tasks (Zhu et al., 2018). Again, we consider GloVe and ELMo word embeddings as inputs. 3.1.2 Model Combination In addition to ranked lists produced by using these core sentence embeddings, we also investigated aggregating the ranked lists using the Borda count, a rank aggregation technique that has previously been used for combining web search results (Dwork et al., 2001). The Borda count aggregates multiple ranked lists of the same set of items into a single ranked 1 We found similar results in experiments with a densitybased metric, Local Outlier Factor (Breunig et al.,"
N19-1051,P18-2100,0,0.0268851,"Missing"
N19-1051,D15-1067,0,0.0187258,"at we expect our representations to capture the semantic structure of the space for each class. An example that is far away from other examples in the set is therefore less semantically similar in some sense, making it an outlier. Importantly, it may be an outlier for two distinct reasons: (1) it is not a valid instance of this class (i.e., an error), or (2) it is an unusual example of the class (i.e., unique). This approach is applied independently to each class of data. As example applications we consider Finally, a related but distinct topic is novelty detection (Soboroff and Harman, 2005; Lee, 2015; Ghosal et al., 2018), in which two sets of documents are provided, one that is assumed to be known, and one that may contain new content. The task is to identify novel content in the second set. While outlier detection methods are often applied to this problem, the inclusion of the known document set makes the task fundamentally different from the problem we consider in this work. 518 two dialog system tasks: intent classification and slot-filling. For classification, data for each possible intent label is considered separately, meaning we find outliers in the data by considering one intent"
N19-1051,D17-1291,0,0.0583653,"Missing"
N19-1051,J93-2004,0,0.0669456,"ntence embeddings combined with distance-based outlier detection. We also present a novel data collection pipeline built atop our detection technique to automatically and iteratively mine unique data samples while discarding erroneous samples. Experiments show that our outlier detection technique is effective at finding errors while our data collection pipeline yields highly diverse corpora that in turn produce more robust intent classification and slot-filling models. 1 Introduction High-quality annotated data is one of the fundamental drivers of progress in Natural Language Processing (e.g. Marcus et al., 1993; Koehn, 2005). In order to be effective at producing an accurate and robust model, a dataset needs to be correct while also diverse enough to cover the full range of ways in which the phenomena it targets occur. Substantial research effort has considered dataset correctness (Eskin, 2000; Dickinson and Meurers, 2003; Rehbein and Ruppenhofer, 2017), particularly for crowdsourcing (Snow et al., 2008; Jiang et al., 2017), but addressing diversity in data has received less attention, with the exception of using data from diverse domains (Hovy et al., 2006). Outlier detection, the task of finding e"
N19-1051,C02-1101,0,0.0788144,"517 Proceedings of NAACL-HLT 2019, pages 517–527 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics fer substantially from the rest of the data. 2.2 For almost two decades, a body of work in NLP has investigated applying these ideas to data in order to identify annotation errors (Abney et al., 1999). Approaches have included the use of scores from trained models for POS tagging (Abney et al., 1999; Eskin, 2000; van Halteren, 2000; Dligach and Palmer, 2011; Fukumoto and Suzuki, 2004), count-based methods that compare examples from across the corpus (Nakagawa and Matsumoto, 2002; Hollenstein et al., 2016), characterizing data based on feature vectors projected down into a low-dimensional space (Guthrie et al., 2008), and tracking the difficulty of learning each example during training (Amiri et al., 2018). One particularly effective approach has been to find n-grams that match but have different labels, as shown for annotations including POS tags (Dickinson and Meurers, 2003), syntactic parses (Dickinson and Meurers, 2005; Dickinson, 2010; Dickinson and Smith, 2011), and predicate-argument relations (Dickinson and Lee, 2008). Our work instead uses continuous represen"
N19-1051,negri-etal-2012-chinese,0,0.0168046,"have different labels, as shown for annotations including POS tags (Dickinson and Meurers, 2003), syntactic parses (Dickinson and Meurers, 2005; Dickinson, 2010; Dickinson and Smith, 2011), and predicate-argument relations (Dickinson and Lee, 2008). Our work instead uses continuous representations of text derived from neural networks. We build on prior work employing online crowd workers to create data by paraphrasing. In particular, we refine the idea of iteratively asking for paraphrases, where each round prompts workers with sentences from the previous round, leading to more diverse data (Negri et al., 2012; Jiang et al., 2017; Kang et al., 2018). We also apply the idea of a multi-stage process, in which a second set of workers check paraphrases to ensure they are correct (Buzek et al., 2010; Burrows et al., 2013; Coucke et al., 2018). Most notably, by incorporating our outlier detection method, we are able to automate detecting detrimental data points while also prompting workers in subsequent rounds to paraphrase more unique examples. 3 Data Collection Outlier Detection We propose a new outlier detection approach using continuous representations of sentences. Using that approach, we explored t"
N19-1051,D14-1162,0,0.0839231,"a new pipeline, shown in Figure 1b that uses outlier detection to (1) reduce the number of sentences being checked, and (2) collect more diverse examples. Our new approach Universal Sentence Encoder (USE; Cer et al., 2018) A Deep Averaging Network method, which averages word embeddings and passes the result through a feedforward network. The USE is trained using a range of supervised and unsupervised tasks. Smooth Inverse Frequency (SIF; Arora et al., 2017) A weighted average of word embeddings, with weights determined by word frequency within a corpus. We consider word embeddings from GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018). Average An unweighted average of word embeddings. While simple, this approach has been shown to be effective for classification (Zhang and Wallace, 2017) and other downstream tasks (Zhu et al., 2018). Again, we consider GloVe and ELMo word embeddings as inputs. 3.1.2 Model Combination In addition to ranked lists produced by using these core sentence embeddings, we also investigated aggregating the ranked lists using the Borda count, a rank aggregation technique that has previously been used for combining web search results (Dwork et al., 2001). The Borda count"
N19-1051,N18-1202,0,0.00918108,"that uses outlier detection to (1) reduce the number of sentences being checked, and (2) collect more diverse examples. Our new approach Universal Sentence Encoder (USE; Cer et al., 2018) A Deep Averaging Network method, which averages word embeddings and passes the result through a feedforward network. The USE is trained using a range of supervised and unsupervised tasks. Smooth Inverse Frequency (SIF; Arora et al., 2017) A weighted average of word embeddings, with weights determined by word frequency within a corpus. We consider word embeddings from GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018). Average An unweighted average of word embeddings. While simple, this approach has been shown to be effective for classification (Zhang and Wallace, 2017) and other downstream tasks (Zhu et al., 2018). Again, we consider GloVe and ELMo word embeddings as inputs. 3.1.2 Model Combination In addition to ranked lists produced by using these core sentence embeddings, we also investigated aggregating the ranked lists using the Borda count, a rank aggregation technique that has previously been used for combining web search results (Dwork et al., 2001). The Borda count aggregates multiple ranked list"
N19-1051,P17-1107,0,0.0185879,"data collection pipeline yields highly diverse corpora that in turn produce more robust intent classification and slot-filling models. 1 Introduction High-quality annotated data is one of the fundamental drivers of progress in Natural Language Processing (e.g. Marcus et al., 1993; Koehn, 2005). In order to be effective at producing an accurate and robust model, a dataset needs to be correct while also diverse enough to cover the full range of ways in which the phenomena it targets occur. Substantial research effort has considered dataset correctness (Eskin, 2000; Dickinson and Meurers, 2003; Rehbein and Ruppenhofer, 2017), particularly for crowdsourcing (Snow et al., 2008; Jiang et al., 2017), but addressing diversity in data has received less attention, with the exception of using data from diverse domains (Hovy et al., 2006). Outlier detection, the task of finding ex2 2.1 Background and Related Work Outlier Detection Outlier detection (Rousseeuw and Leroy, 1987), also called outlier analysis (Aggarwal, 2015) or anomaly detection (Chandola et al., 2009), is the task of identifying examples in a dataset that dif517 Proceedings of NAACL-HLT 2019, pages 517–527 c Minneapolis, Minnesota, June 2 - June 7, 2019. 20"
N19-1051,D08-1027,0,0.033398,"Missing"
P10-1036,J99-2004,0,0.71339,"racy and speed improvements for Wikipedia and biomedical text. 1 Introduction In many NLP tasks and applications, e.g. distributional similarity (Curran, 2004) and question answering (Dumais et al., 2002), large volumes of text and detailed syntactic information are both critical for high performance. To avoid a tradeoff between these two, we need to increase parsing speed, but without losing accuracy. Parsing with lexicalised grammar formalisms, such as Lexicalised Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG; Steedman, 2000), can be made more efficient using a supertagger. Bangalore and Joshi (1999) call supertagging almost parsing because of the significant reduction in ambiguity which occurs once the supertags have been assigned. In this paper, we focus on the CCG parser and supertagger described in Clark and Curran (2007). 1 We use supertag and lexical category interchangeably. 345 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 345–355, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics I Using the parser to generate training data also has the advantage that it is not a domain specific process. Previous wo"
P10-1036,E99-1025,0,0.096565,"Missing"
P10-1036,W02-2236,0,0.0204483,"NP S NP &gt; < &gt; < Figure 1: Two CCG derivations with PP ambiguity. can be used to find the most probable supertag sequence. Alternatively the Forward-Backward algorithm can be used to efficiently sum over all sequences, giving a probability distribution over supertags for each word which is conditional only on the input sentence. Background Supertaggers can be made accurate enough for wide coverage parsing using multi-tagging (Chen et al., 1999), in which more than one supertag can be assigned to a word; however, as more supertags are supplied by the supertagger, parsing efficiency decreases (Chen et al., 2002), demonstrating the influence of lexical ambiguity on parsing complexity (Sarkar et al., 2000). Many statistical parsers use two stages: a tagging stage that labels each word with its grammatical role, and a parsing stage that uses the tags to form a parse tree. Lexicalised grammars typically contain a much smaller set of rules than phrase-structure grammars, relying on tags (supertags) that contain a more detailed description of each word’s role in the sentence. This leads to much larger tag sets, and shifts a large proportion of the search for an optimal derivation to the tagging component o"
P10-1036,M95-1004,0,0.192466,"idering a word correct if the correct tag is amongst any of the assigned tags. For the biomedical parser evaluation we have used the parsing model and grammatical relation conversion script from Rimell and Clark (2009). Our timing measurements are calculated in two ways. Overall times were measured using the C&C parser’s timers. Individual sentence measurements were made using the Intel timing registers, since standard methods are not accurate enough for the short time it takes to parse a single sentence. To check whether changes were statistically significant we applied the test described by Chinchor (1995). This measures the probability that two sets of responses are drawn from the same distribution, where a score below 0.05 is considered significant. Models were trained on an Intel Core2Duo 3GHz with 4GB of RAM. The evaluation was performed on a dual quad-core Intel Xeon 2.27GHz with 16GB of RAM. 5.1 6 Results We have performed four primary sets of experiments to explore the ability of an adaptive supertagger to improve parsing speed or accuracy. In the first two experiments, we explore performance on the newswire domain, which is the source of training data for the parsing model and the basel"
P10-1036,C04-1041,1,0.923426,"ng component of the parser. Figure 1 gives two sentences and their CCG derivations, showing how some of the syntactic ambiguity is transferred to the supertagging component in a lexicalised grammar. Note that the lexical category assigned to with is different in each case, reflecting the fact that the prepositional phrase attaches differently. Either we need a tagging model that can resolve this ambiguity, or both lexical categories must be supplied to the parser which can then attempt to resolve the ambiguity by eventually selecting between them. 2.1 ate NP (S NP)/NP NP ((S NP)(S NP))/NP Clark and Curran (2004) applied supertagging to CCG, using a flexible multi-tagging approach. The supertagger assigns to a word all lexical categories whose probabilities are within some factor, β, of the most probable category for that word. When the supertagger is integrated with the C&C parser, several progressively lower β values are considered. If a sentence is not parsed on one pass then the parser attempts to parse the sentence again with a lower β value, using a larger set of categories from the supertagger. Since most sentences are parsed at the first level (in which the average number of supertags assigned"
P10-1036,J07-4004,1,0.901182,"etailed syntactic information are both critical for high performance. To avoid a tradeoff between these two, we need to increase parsing speed, but without losing accuracy. Parsing with lexicalised grammar formalisms, such as Lexicalised Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG; Steedman, 2000), can be made more efficient using a supertagger. Bangalore and Joshi (1999) call supertagging almost parsing because of the significant reduction in ambiguity which occurs once the supertags have been assigned. In this paper, we focus on the CCG parser and supertagger described in Clark and Curran (2007). 1 We use supertag and lexical category interchangeably. 345 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 345–355, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics I Using the parser to generate training data also has the advantage that it is not a domain specific process. Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001). Using a newspapertrained parser, we constructed new training sets for Wikipedia and biomedical text. These were used to create new"
P10-1036,P07-1120,0,0.0547606,"Missing"
P10-1036,W03-0407,1,0.787353,"ng analysis cannot be found by the parser, the number of lexical categories supplied by the supertagger is increased. The supertagger-parser interaction influences speed in two ways: first, the larger the lexical ambiguity, the more derivations the parser must consider; second, each further pass is as costly as parsing a whole extra sentence. Our goal is to increase parsing speed without loss of accuracy. The technique we use is a form of self-training, in which the output of the parser is used to train the supertagger component. The existing literature on self-training reports mixed results. Clark et al. (2003) were unable to improve the accuracy of POS tagging using self-training. In contrast, McClosky et al. (2006a) report improved accuracy through self-training for a twostage parser and re-ranker. Here our goal is not to improve accuracy, only to maintain it, which we achieve through an adaptive supertagger. The adaptive supertagger produces lexical categories that the parser would have used in the final derivation when using the baseline model. However, it does so with much lower ambiguity levels, and potentially during an earlier pass, which means sentences are parsed faster. By increasing the"
P10-1036,W03-2401,0,0.0235798,"ed as they would be most similar in style to the evaluation corpus. In all experiments the sentences from 1989 were excluded to ensure no overlap occurred with CCGbank. As Wikipedia text we have used 794,024,397 tokens (51,673,069 sentences) from Wikipedia articles. This text was processed in the same way as the NANC data to produce parser-annotated training data. For supertagger evaluation, one thousand sentences were manually annotated with CCG lexical categories and POS tags. For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al., 2003) in the style of Briscoe and Carroll (2006). Both sets of annotations were produced by manually correcting the output of the baseline system. The annotation was performed by Stephen Clark and Laura Rimell. For the biomedical domain we have used several different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evaluation, grammatical relations from the BioInfer corpus were used (Pyysalo et al., 2007), with the same post-processing process as Rimell and Clark (2009) to convert th"
P10-1036,W02-1001,0,0.197012,"Missing"
P10-1036,J93-2004,0,0.0350434,"-20 14.04 17.41 39.2 21-40 28.76 29.27 49.4 41-250 49.73 86.73 10.2 All 24.83 152.15 100.0 0-4 2.81 0.60 22.4 5-20 11.64 21.56 48.9 21-40 28.02 28.48 24.3 41-250 49.69 77.70 4.5 All 15.33 154.57 100.0 0-4 2.98 0.75 0.9 5-20 14.54 15.14 41.3 21-40 28.49 29.34 48.0 41-250 49.17 68.34 9.8 All 24.53 139.35 100.0 Table 1: Statistics for sentences in the supertagger training data. Sentences containing more than 250 tokens were not included in our data sets. Training and accuracy evaluation We have used Sections 02-21 of CCGbank (Hockenmaier and Steedman, 2007), the CCG version of the Penn Treebank (Marcus et al., 1993), as training data for the newspaper domain. Sections 00 and 23 were used for development and test evaluation. A further 113,346,430 tokens (4,566,241 sentences) of raw data from the Wall Street Journal section of the North American News Corpus (Graff, 1995) were parsed to produce the training data for adaptation. This text was tokenised using the C&C tools tokeniser and parsed using our baseline models. For the smaller training sets, sentences from 1988 were used as they would be most similar in style to the evaluation corpus. In all experiments the sentences from 1989 were excluded to ensure"
P10-1036,N06-1020,0,0.0472281,"s increased. The supertagger-parser interaction influences speed in two ways: first, the larger the lexical ambiguity, the more derivations the parser must consider; second, each further pass is as costly as parsing a whole extra sentence. Our goal is to increase parsing speed without loss of accuracy. The technique we use is a form of self-training, in which the output of the parser is used to train the supertagger component. The existing literature on self-training reports mixed results. Clark et al. (2003) were unable to improve the accuracy of POS tagging using self-training. In contrast, McClosky et al. (2006a) report improved accuracy through self-training for a twostage parser and re-ranker. Here our goal is not to improve accuracy, only to maintain it, which we achieve through an adaptive supertagger. The adaptive supertagger produces lexical categories that the parser would have used in the final derivation when using the baseline model. However, it does so with much lower ambiguity levels, and potentially during an earlier pass, which means sentences are parsed faster. By increasing the ambiguity level of the adaptive models to match the baseline system, we can also slightly increase supertag"
P10-1036,de-marneffe-etal-2006-generating,0,0.0116686,"Missing"
P10-1036,P06-1043,0,0.0232405,"s increased. The supertagger-parser interaction influences speed in two ways: first, the larger the lexical ambiguity, the more derivations the parser must consider; second, each further pass is as costly as parsing a whole extra sentence. Our goal is to increase parsing speed without loss of accuracy. The technique we use is a form of self-training, in which the output of the parser is used to train the supertagger component. The existing literature on self-training reports mixed results. Clark et al. (2003) were unable to improve the accuracy of POS tagging using self-training. In contrast, McClosky et al. (2006a) report improved accuracy through self-training for a twostage parser and re-ranker. Here our goal is not to improve accuracy, only to maintain it, which we achieve through an adaptive supertagger. The adaptive supertagger produces lexical categories that the parser would have used in the final derivation when using the baseline model. However, it does so with much lower ambiguity levels, and potentially during an earlier pass, which means sentences are parsed faster. By increasing the ambiguity level of the adaptive models to match the baseline system, we can also slightly increase supertag"
P10-1036,U08-1013,1,0.808259,"r the biomedical domain we have used several different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evaluation, grammatical relations from the BioInfer corpus were used (Pyysalo et al., 2007), with the same post-processing process as Rimell and Clark (2009) to convert the C&C parser output to Stanford format grammatical relations (de Marneffe et al., 2006). For adaptive training we have used 1,900,618,859 tokens (76,739,723 sentences) from the MEDLINE abstracts tokenised by McIntosh and Curran (2008). These sentences were POS -tagged and parsed twice, once as for the newswire and Wikipedia data, and then again, using the bio-specific models developed by Rimell and Clark (2009). Statistics for the sentences in the training sets are given in Table 1. 4.2 Speed evaluation data For speed evaluation we held out three sets of sentences from each domain-specific corpus. Specifically, we used 30,000, 4,000 and 2,000 unique sentences of length 5-20, 21-40 and 41-250 tokens respectively. Speeds on these length controlled sets were combined to calculate an overall parsing speed for the text in each"
P10-1036,W01-0521,0,0.0241195,"ery case the new models perform worse than the baseline on domains other than the one they were trained on. In some cases the models in Table 7 are less accurate than those in Table 5. This is because as well as optimising the β levels we have changed training methods. All of the training methods were tried, but only the method with the best results in newswire is included here, which for F-score when trained on 400,000 sentences was GIS. The accuracy presented so far for the biomediCross-domain speed improvement When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001). In this experiment, we attempt to increase speed on out-of-domain data. Note that for some of the results presented here it may appear that the C&C parser does not lose speed when out of domain, since the Wikipedia and biomedical corpora contain shorter sentences on average than the news corpus. However, by testing on balanced sets it is clear that speed does decrease, particularly for longer sentences, as shown in Table 9. For our domain adaptation development experiments, we considered a collection of different models; here we only present results for the best set of models. For speed impr"
P10-1036,P07-1037,0,0.0368961,"Missing"
P10-1036,W07-1004,0,0.0185156,"red of these sentences were manually annotated with DepBank grammatical relations (King et al., 2003) in the style of Briscoe and Carroll (2006). Both sets of annotations were produced by manually correcting the output of the baseline system. The annotation was performed by Stephen Clark and Laura Rimell. For the biomedical domain we have used several different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evaluation, grammatical relations from the BioInfer corpus were used (Pyysalo et al., 2007), with the same post-processing process as Rimell and Clark (2009) to convert the C&C parser output to Stanford format grammatical relations (de Marneffe et al., 2006). For adaptive training we have used 1,900,618,859 tokens (76,739,723 sentences) from the MEDLINE abstracts tokenised by McIntosh and Curran (2008). These sentences were POS -tagged and parsed twice, once as for the newswire and Wikipedia data, and then again, using the bio-specific models developed by Rimell and Clark (2009). Statistics for the sentences in the training sets are given in Table 1. 4.2 Speed evaluation data For sp"
P10-1036,J07-3004,0,0.343689,"Missing"
P10-1036,D08-1050,1,0.880391,"ntences were manually annotated with CCG lexical categories and POS tags. For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al., 2003) in the style of Briscoe and Carroll (2006). Both sets of annotations were produced by manually correcting the output of the baseline system. The annotation was performed by Stephen Clark and Laura Rimell. For the biomedical domain we have used several different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evaluation, grammatical relations from the BioInfer corpus were used (Pyysalo et al., 2007), with the same post-processing process as Rimell and Clark (2009) to convert the C&C parser output to Stanford format grammatical relations (de Marneffe et al., 2006). For adaptive training we have used 1,900,618,859 tokens (76,739,723 sentences) from the MEDLINE abstracts tokenised by McIntosh and Curran (2008). These sentences were POS -tagged and parsed twice, once as for the newswire and Wikipedia data, and then again, using the bio-specific models developed by Rimell and Clark (2009)."
P10-1036,W00-1605,0,0.0316057,"ost probable supertag sequence. Alternatively the Forward-Backward algorithm can be used to efficiently sum over all sequences, giving a probability distribution over supertags for each word which is conditional only on the input sentence. Background Supertaggers can be made accurate enough for wide coverage parsing using multi-tagging (Chen et al., 1999), in which more than one supertag can be assigned to a word; however, as more supertags are supplied by the supertagger, parsing efficiency decreases (Chen et al., 2002), demonstrating the influence of lexical ambiguity on parsing complexity (Sarkar et al., 2000). Many statistical parsers use two stages: a tagging stage that labels each word with its grammatical role, and a parsing stage that uses the tags to form a parse tree. Lexicalised grammars typically contain a much smaller set of rules than phrase-structure grammars, relying on tags (supertags) that contain a more detailed description of each word’s role in the sentence. This leads to much larger tag sets, and shifts a large proportion of the search for an optimal derivation to the tagging component of the parser. Figure 1 gives two sentences and their CCG derivations, showing how some of the"
P10-1036,N01-1023,0,0.0924336,"Missing"
P10-1036,E03-1008,1,0.886743,"Missing"
P10-1036,E09-1093,0,0.063093,"Missing"
P10-1036,W09-3832,0,0.0147488,"age (Clark and Curran, 2004). Supertagging Supertaggers typically use standard linear-time tagging algorithms, and only consider words in the local context when assigning a supertag. The C&C supertagger is similar to the Ratnaparkhi (1996) tagger, using features based on words and POS tags in a five-word window surrounding the target word, and defining a local probability distribution over supertags for each word in the sentence, given the previous two supertags. The Viterbi algorithm Supertagging has since been effectively applied to other formalisms, such as HPSG (Blunsom and Baldwin, 2006; Zhang et al., 2009), and as an information source for tasks such as Statistical Machine Translation (Hassan et al., 2007). The use of parser output for supertagger training has been explored for LTAG by Sarkar (2007). However, the focus of that work was on improving parser and supertagger accuracy rather than speed. 346 Previously , watch imports S/S N , N /N N were N S [adj ]NP denied (S [dcl ]NP )/(S [pss]NP ) (S [pss]NP )/NP (S[dcl]NP)/NP S [pss]NP (S [dcl ]NP )/(S [adj ]NP ) (S [pss]NP )/NP such NP/NP duty-free treatment N/N N (N /N )/(N /N ) N /N (S [pt]NP )/NP (S[dcl]NP)/NP Figure 2: An example"
P10-1036,W96-0213,0,\N,Missing
P10-1036,W06-1620,0,\N,Missing
P10-1036,J05-1003,0,\N,Missing
P10-1036,P06-2006,0,\N,Missing
P12-2021,H91-1060,0,0.0702437,"Missing"
P12-2021,P11-1048,0,0.0449345,"Missing"
P12-2021,J08-1003,0,0.0563225,"Missing"
P12-2021,P05-1022,0,0.10656,"Missing"
P12-2021,P00-1058,0,0.0132874,"se errors are inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics S NP JJ VP NNS VBD S NP PRP$ NP NN NN DT Italian magistrates labeled his death"
P12-2021,J07-4004,1,0.831639,"Missing"
P12-2021,P09-2014,1,0.921488,"re not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics S NP JJ VP NNS VBD S NP PRP$ NP NN NN DT Italian magistrates labeled his death a suicide N /N N ((S [dcl ]NP )/NP )/NP NP [nb]/N N NP [nb]/N N &gt; N NP &gt; NP &gt;"
P12-2021,P97-1003,0,0.0734998,"Missing"
P12-2021,P10-1035,0,0.0303575,"Missing"
P12-2021,P03-1054,1,0.0215858,"Missing"
P12-2021,J93-2004,0,0.0443419,"Missing"
P12-2021,C08-1069,0,0.386976,"ch as QPs, NXs, and NACs. Many of these errors are inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics S NP JJ VP NNS VBD S NP PRP$ NP NN NN DT Italian magistr"
P12-2021,N07-1051,1,0.802091,"Missing"
P12-2021,N01-1023,0,0.0298084,"inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics S NP JJ VP NNS VBD S NP PRP$ NP NN NN DT Italian magistrates labeled his death a suicide N /N"
P12-2021,H01-1014,0,0.0421712,"he most common errors our approach makes involve nodes for clauses and rare spans such as QPs, NXs, and NACs. Many of these errors are inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Associa"
P13-2018,E09-1031,0,0.17099,"h error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges. 1 James R. Curran‡ Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modiﬁcations to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the PCTB. However, their manual analysis was limited in scope, covering only part of th"
P13-2018,D07-1027,0,0.0730843,"Missing"
P13-2018,P09-1059,0,0.08056,"lso investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges. 1 James R. Curran‡ Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modiﬁcations to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the PCTB. However, their manual analysis was limited in scope, c"
P13-2018,P03-1054,1,0.0191192,"g some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the PCTB. However, their manual analysis was limited in scope, covering only part of the parser output, and was unable to characterize the relative impact of the issues they uncovered. 2 Background The closest previous work is the detailed manual analysis performed by Levy and Manning (2003). While their focus was on issues faced by their factored PCFG parser (Klein and Manning, 2003b), the error types they identiﬁed are general issues presented by Chinese syntax in the PCTB. They presented several Chinese error types that are rare or absent in English, including noun/verb ambiguity, NP-internal structure and coordination ambiguity due to pro-drop, suggesting that closing the English-Chinese parsing gap demands techniques 1 The system described in this paper is available from http://code.google.com/p/berkeley-parser-analyser/ 98 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 98–103, c Sofia, Bulgaria, August 4-9 2013. 2013 A"
P13-2018,D12-1096,1,0.875836,"Missing"
P13-2018,P03-1056,0,0.0337462,"only addresses certain error types, leaving substantial outstanding challenges. 1 James R. Curran‡ Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modiﬁcations to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the PCTB. However, their manual analysis was limited in scope, covering only part of the parser output, and was unable to characterize the relative impact of the issues they uncovered. 2 Bac"
P13-2018,J93-2004,0,0.0433648,"Missing"
P13-2018,W09-3825,0,0.0367848,"ach Span Label Sense Edge Attach Attach Attach Other 0.76 0.72 0.21 0.30 0.05 0.21 0.26 0.22 0.18 1.87 1.48 1.68 1.06 1.02 0.88 0.55 0.50 0.44 0.44 4.11 Table 2: Error breakdown for the development set of PCTB 6. The area ﬁlled in for each bar indicates the average number of bracket errors per sentence attributed to that error type, where an empty bar is no errors and a full bar has the value indicated in the bottom row. The parsers are: the Berkeley parser with gold POS tags as input (Berk-G), the Berkeley product parser with two grammars (Berk-2), the Berkeley parser (Berk-1), the parser of Zhang and Clark (2009) (ZPAR), the Bikel parser (Bikel), the Stanford Factored parser (Stan-F), and the Stanford Unlexicalized PCFG parser (Stan-P). two categories (e.g. between Verb taking wrong args and NP Attachment). Differences in treebank annotations also present a challenge for cross-language error comparison. The most common error type in Chinese, NPinternal structure, is rare in the results of Kummerfeld et al. (2012), but the datasets are not comparable because the PTB has very limited NP-internal structure annotated. Further characterization of the impact of annotation differences on errors is beyond the"
P13-2018,P06-1055,1,0.686979,"Missing"
P13-2018,D12-1046,0,0.0984765,"se parsers, covering a broad range of error types for large sets of sentences, enabling the ﬁrst empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges. 1 James R. Curran‡ Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modiﬁcations to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis t"
P13-2018,N12-1030,1,0.873947,"as been a major part of several recent papers (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009). The Berk-G row of Table 2 shows the performance of the Berkeley parser when given gold POS tags.5 While the F1 improvement is unsurprising, for the ﬁrst time we can clearly show that the gains are only in a subset of the error types. In particular, tagging improvement will not help for two of the most signiﬁcant challenges: coordination scope errors, and verb argument selection. To see which tagging confusions contribute to which error reductions, we adapt the POS ablation approach of Tse and Curran (2012). We consider the POS tag pairs shown in Table 3. To isolate the effects of each confusion we start from the gold tags and introduce the output of the Stanford tagger whenever it returns one of the two tags being considered.6 We then feed these “semi-gold” tags Cross-parser analysis The previous section described the error types and their distribution for a single Chinese parser. Here we conﬁrm that these are general trends, by showing that the same pattern is observed for several different parsers on the PCTB 6 dev set.3 We include results for a transition-based parser (ZPAR; Zhang and Clark,"
P13-2018,I05-1007,0,0.0330528,"Missing"
P13-2018,N07-1051,1,\N,Missing
P13-2018,W00-1201,0,\N,Missing
P13-2018,N10-1003,0,\N,Missing
P17-2017,bouamor-etal-2012-contrastive,0,0.0212308,"there any four credit upper-level classes? We considered the above two questions as paraphrases since they are both requests for a list of classes, explicit and implicit, respectively, although the second one is a polar question and the first one is not. However: Expert and Automated Generation Finally, there are two general lines of research on paraphrasing not focused on using crowds. The first of these is the automatic collection of paraphrases from parallel data sources, such as translations of the same text or captions for the same image (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Bouamor et al., 2012; Pavlick et al., Prompt:Which is easier out of EECS 378 and EECS 280? Is EECS 378 easier than EECS 280? We did not consider the above two questions as paraphrases since the first one is requesting one of 104 Workflow We considered three variations to workflow. First, for each sentence, we either asked workers to provide two paraphrases (Baseline), or one (One Paraphrase). Asking for multiple paraphrases reduces duplication (since workers will not repeat themselves), but may result in lower diversity. Second, since our baseline prompt sentences are questions, we ran a condition with answers sh"
P17-2017,C14-1117,0,0.02605,"Missing"
P17-2017,W10-0735,0,0.153741,"Missing"
P17-2017,P11-1020,0,0.149989,"are four credits? Are there any four credit upper-level classes? We considered the above two questions as paraphrases since they are both requests for a list of classes, explicit and implicit, respectively, although the second one is a polar question and the first one is not. However: Expert and Automated Generation Finally, there are two general lines of research on paraphrasing not focused on using crowds. The first of these is the automatic collection of paraphrases from parallel data sources, such as translations of the same text or captions for the same image (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Bouamor et al., 2012; Pavlick et al., Prompt:Which is easier out of EECS 378 and EECS 280? Is EECS 378 easier than EECS 280? We did not consider the above two questions as paraphrases since the first one is requesting one of 104 Workflow We considered three variations to workflow. First, for each sentence, we either asked workers to provide two paraphrases (Baseline), or one (One Paraphrase). Asking for multiple paraphrases reduces duplication (since workers will not repeat themselves), but may result in lower diversity. Second, since our baseline prompt sentences are questions, we ran a con"
P17-2017,N12-1017,0,0.057929,"Missing"
P17-2017,P13-1158,0,0.0295304,"k design, providing guidance for future paraphrase generation procedures. 1 2 Introduction Related Work Previous work on crowdsourced paraphrase generation fits into two categories: work on modifying the creation process or workflow, and studying the effect of prompting or priming on crowd worker output. Beyond crowdsourced generation, other work has explored using experts or automated systems to generate paraphrases. Paraphrases are useful for a range of tasks, including machine translation evaluation (Kauchak and Barzilay, 2006), semantic parsing (Wang et al., 2015), and question answering (Fader et al., 2013). Crowdsourcing has been widely used as a scalable and cost-effective means of generating paraphrases (Negri et al., 2012; Wang et al., 2012; Tschirsich and Hintz, 2013), but there has been limited analysis of the factors influencing diversity and correctness of the paraphrases workers write. In this paper, we perform a systematic investigation of design decisions for crowdsourcing paraphrases, including the first exploration of worker incentives for paraphrasing. For worker incentives, we either provide a bonus payment when a paraphrase is novel (encouraging diversity) or 2.1 Workflows for Cr"
P17-2017,W14-5003,0,0.0701069,"raphrase/reword). For example: ’Which 400 level courses don’t have labs?’ could be rewritten as: • Of all the 400 level courses, which ones do not include labs? • What are the 400 level courses without lab sessions? BONUS: You will receive 5 cents bonus for each sentence you write that matches one written by another worker on the task. Figure 1: Baseline task instructions. The Effects of Priming When crowd workers perform a task, they are primed (influenced) by the examples, instructions, and context that they see. This priming can result in systematic variations in the resulting paraphrases. Mitchell et al. (2014) showed that providing context, in the form of previous utterances from a dialogue, only provides benefits once four or more are included. Kumaran et al. (2014) provided drawings as prompts, obtaining diverse paraphrases, but without exact semantic equivalence. When each sentence expresses a small set of slot-filler predicates, Wang et al. (2012) found that providing the list of predicates led to slightly faster paraphrasing than giving either a complete sentence or a short sentence for each predicate. We further expand on this work by exploring how the type of examples shown affects paraphras"
P17-2017,N13-1092,0,0.162926,"Missing"
P17-2017,negri-etal-2012-chinese,0,0.312852,"rowdsourced paraphrase generation fits into two categories: work on modifying the creation process or workflow, and studying the effect of prompting or priming on crowd worker output. Beyond crowdsourced generation, other work has explored using experts or automated systems to generate paraphrases. Paraphrases are useful for a range of tasks, including machine translation evaluation (Kauchak and Barzilay, 2006), semantic parsing (Wang et al., 2015), and question answering (Fader et al., 2013). Crowdsourcing has been widely used as a scalable and cost-effective means of generating paraphrases (Negri et al., 2012; Wang et al., 2012; Tschirsich and Hintz, 2013), but there has been limited analysis of the factors influencing diversity and correctness of the paraphrases workers write. In this paper, we perform a systematic investigation of design decisions for crowdsourcing paraphrases, including the first exploration of worker incentives for paraphrasing. For worker incentives, we either provide a bonus payment when a paraphrase is novel (encouraging diversity) or 2.1 Workflows for Crowd-Paraphrasing The most common approach to crowdsourcing paraphrase generation is to provide a sentence as a prompt and"
P17-2017,P02-1040,0,0.110543,"Missing"
P17-2017,N06-1058,0,0.0494414,"trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures. 1 2 Introduction Related Work Previous work on crowdsourced paraphrase generation fits into two categories: work on modifying the creation process or workflow, and studying the effect of prompting or priming on crowd worker output. Beyond crowdsourced generation, other work has explored using experts or automated systems to generate paraphrases. Paraphrases are useful for a range of tasks, including machine translation evaluation (Kauchak and Barzilay, 2006), semantic parsing (Wang et al., 2015), and question answering (Fader et al., 2013). Crowdsourcing has been widely used as a scalable and cost-effective means of generating paraphrases (Negri et al., 2012; Wang et al., 2012; Tschirsich and Hintz, 2013), but there has been limited analysis of the factors influencing diversity and correctness of the paraphrases workers write. In this paper, we perform a systematic investigation of design decisions for crowdsourcing paraphrases, including the first exploration of worker incentives for paraphrasing. For worker incentives, we either provide a bonus"
P17-2017,P15-2070,0,0.0359549,"Missing"
P17-2017,W13-2325,0,0.147601,"into two categories: work on modifying the creation process or workflow, and studying the effect of prompting or priming on crowd worker output. Beyond crowdsourced generation, other work has explored using experts or automated systems to generate paraphrases. Paraphrases are useful for a range of tasks, including machine translation evaluation (Kauchak and Barzilay, 2006), semantic parsing (Wang et al., 2015), and question answering (Fader et al., 2013). Crowdsourcing has been widely used as a scalable and cost-effective means of generating paraphrases (Negri et al., 2012; Wang et al., 2012; Tschirsich and Hintz, 2013), but there has been limited analysis of the factors influencing diversity and correctness of the paraphrases workers write. In this paper, we perform a systematic investigation of design decisions for crowdsourcing paraphrases, including the first exploration of worker incentives for paraphrasing. For worker incentives, we either provide a bonus payment when a paraphrase is novel (encouraging diversity) or 2.1 Workflows for Crowd-Paraphrasing The most common approach to crowdsourcing paraphrase generation is to provide a sentence as a prompt and request a single paraphrase from a worker. One"
P17-2017,P15-1129,0,0.142065,"rowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures. 1 2 Introduction Related Work Previous work on crowdsourced paraphrase generation fits into two categories: work on modifying the creation process or workflow, and studying the effect of prompting or priming on crowd worker output. Beyond crowdsourced generation, other work has explored using experts or automated systems to generate paraphrases. Paraphrases are useful for a range of tasks, including machine translation evaluation (Kauchak and Barzilay, 2006), semantic parsing (Wang et al., 2015), and question answering (Fader et al., 2013). Crowdsourcing has been widely used as a scalable and cost-effective means of generating paraphrases (Negri et al., 2012; Wang et al., 2012; Tschirsich and Hintz, 2013), but there has been limited analysis of the factors influencing diversity and correctness of the paraphrases workers write. In this paper, we perform a systematic investigation of design decisions for crowdsourcing paraphrases, including the first exploration of worker incentives for paraphrasing. For worker incentives, we either provide a bonus payment when a paraphrase is novel (e"
P17-2017,J93-2004,0,\N,Missing
P18-1033,W14-2402,0,0.0135844,"methodology apply broadly to the systems cited below. Within the DB community, systems commonly use pattern matching, grammar-based techniques, or intermediate representations of the query (Pazos Rangel et al., 2013). Recent work has explored incorporating user feedback to improve accuracy (Li and Jagadish, 2014). Unfortunately, none of these systems are publicly available, and many rely on domain-specific resources. In the NLP community, there has been extensive work on semantic parsing to logical representations that query a knowledge base (Zettlemoyer and Collins, 2005; Liang et al., 2011; Beltagy et al., 2014; Berant and Liang, 2014), while work on mapping to SQL has recently increased (Yih et al., 2015; Iyer et al., 2017; Zhong et al., 2017). One of the earliest statistical models for mapping text to SQL was the PRECISE system (Popescu et al., 2003, 2004), which achieved high precision on queries that met constraints linking tokens and database values, attributes, and relations, but did not attempt to generate SQL for questions outside this class. Later work considered generating queries based on relations extracted by a syntactic parser (Giordani and Moschitti, 2012) and applying techniques from"
P18-1033,P14-1133,0,0.0222958,"dly to the systems cited below. Within the DB community, systems commonly use pattern matching, grammar-based techniques, or intermediate representations of the query (Pazos Rangel et al., 2013). Recent work has explored incorporating user feedback to improve accuracy (Li and Jagadish, 2014). Unfortunately, none of these systems are publicly available, and many rely on domain-specific resources. In the NLP community, there has been extensive work on semantic parsing to logical representations that query a knowledge base (Zettlemoyer and Collins, 2005; Liang et al., 2011; Beltagy et al., 2014; Berant and Liang, 2014), while work on mapping to SQL has recently increased (Yih et al., 2015; Iyer et al., 2017; Zhong et al., 2017). One of the earliest statistical models for mapping text to SQL was the PRECISE system (Popescu et al., 2003, 2004), which achieved high precision on queries that met constraints linking tokens and database values, attributes, and relations, but did not attempt to generate SQL for questions outside this class. Later work considered generating queries based on relations extracted by a syntactic parser (Giordani and Moschitti, 2012) and applying techniques from logical parsing research"
P18-1033,D17-1151,0,0.0125685,"data. We call this a questionbased data split. However, many English questions may correspond to the same SQL query. If at least one copy of every SQL query appears in training, then the task evaluated is classification, not true semantic parsing, of the English questions. We can increase the number of distinct SQL queries by varying 5.1 Systems Recently, a great deal of work has used variations on the seq2seq model. We compare performance of a basic seq2seq model (Sutskever et al., 2014), and seq2seq with attention over the input (Bahdanau et al., 2015), implemented with TensorFlow seq2seq (Britz et al., 2017). We also extend that model to include an attention-based copying option, similar to Jia and Liang (2016). Our output vocabulary for the decoder includes a special token, COPY. If COPY has the highest probability at step t, we replace it with the input token with the 355 O O city0 O city1 Flight from Denver to Boston bidirectional LSTM provides a prediction for each word, either O if the word is not used in the final query, or a symbol such as city1 to indicate that it fills a slot. The hidden states of the LSTM at each end of the sentence are passed through a small feed-forward network to det"
P18-1033,P16-1004,0,0.431621,"aluation methodology for this task. In the process, we (1) introduce a new, challenging dataset, (2) standardize and fix many errors in existing datasets, and (3) propose a simple yet effective baseline system.1 ∗ The first two authors contributed equally to this work. Code and data is available at https://github. com/jkkummerfeld/text2sql-data/ 1 351 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 351–360 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics sumptions about the output structure (Dong and Lapata, 2016). One challenge for applying neural models to this task is annotating large enough datasets of question-query pairs. Recent work (Cai et al., 2017; Zhong et al., 2017) has automatically generated large datasets using templates to form random queries and corresponding natural-languagelike questions, and then having humans rephrase the question into English. Another option is to use feedback-based learning, where the system alternates between training and making predictions, which a user rates as correct or not (Iyer et al., 2017). Other work seeks to avoid the data bottleneck by using end-to-en"
P18-1033,C12-2040,0,0.151263,"Missing"
P18-1033,P17-1089,0,0.251203,"mputational Linguistics sumptions about the output structure (Dong and Lapata, 2016). One challenge for applying neural models to this task is annotating large enough datasets of question-query pairs. Recent work (Cai et al., 2017; Zhong et al., 2017) has automatically generated large datasets using templates to form random queries and corresponding natural-languagelike questions, and then having humans rephrase the question into English. Another option is to use feedback-based learning, where the system alternates between training and making predictions, which a user rates as correct or not (Iyer et al., 2017). Other work seeks to avoid the data bottleneck by using end-to-end approaches (Yin et al., 2016; Neelakantan et al., 2017), which we do not consider here. One key contribution of this paper is standardization of a range of datasets, to help address the challenge of limited data resources. challenging form of ambiguity from the task. In the process, we apply extensive effort to standardize datasets and fix a range of errors. Previous NLIDB work has led to impressive systems, but current evaluations provide an incomplete picture of their strengths and weaknesses. In this paper, we provide new a"
P18-1033,P16-1002,0,0.420886,"me SQL query. If at least one copy of every SQL query appears in training, then the task evaluated is classification, not true semantic parsing, of the English questions. We can increase the number of distinct SQL queries by varying 5.1 Systems Recently, a great deal of work has used variations on the seq2seq model. We compare performance of a basic seq2seq model (Sutskever et al., 2014), and seq2seq with attention over the input (Bahdanau et al., 2015), implemented with TensorFlow seq2seq (Britz et al., 2017). We also extend that model to include an attention-based copying option, similar to Jia and Liang (2016). Our output vocabulary for the decoder includes a special token, COPY. If COPY has the highest probability at step t, we replace it with the input token with the 355 O O city0 O city1 Flight from Denver to Boston bidirectional LSTM provides a prediction for each word, either O if the word is not used in the final query, or a symbol such as city1 to indicate that it fills a slot. The hidden states of the LSTM at each end of the sentence are passed through a small feed-forward network to determine the SQL template to use. This architecture is simple and enables a joint choice of the tags and th"
P18-1033,P17-2017,1,0.806873,"hers were written by CS students with knowledge of the database who were instructed to write questions they might ask in an academic advising appointment. The authors manually labeled the initial set of questions with SQL. To ensure high quality, at least two annotators scored each questionquery pair on a two-point scale for accuracy— did the query generate an accurate answer to the question?—and a three-point scale for helpfulness—did the answer provide the information the asker was probably seeking? Cases with low scores were fixed or removed from the dataset. We collected paraphrases using Jiang et al. (2017)’s method, with manual inspection to ensure accuracy. For a given sentence, this produced paraphrases with the same named entities (e.g. course number EECS 123). To add variation, we annotated entities in the questions and queries with their types—such as course name, department, or instructor—and substituted randomly-selected values of each type into each paraphrase and its corresponding query. This combination of paraphrasing and entity replacement means an original question of “For next semester, who is teaching EECS 123?” can give rise to “Who teaches MATH 456 next semester?” as well as “W"
P18-1033,W00-1317,0,0.498716,"text-to-SQL datasets, standardizing them to have a consistent SQL style. ATIS (Price, 1990; Dahl et al., 1994) User questions for a flight-booking task, manually annotated. We use the modified SQL from Iyer et al. (2017), which follows the data split from the logical form version (Zettlemoyer and Collins, 2007). GeoQuery (Zelle and Mooney, 1996) User questions about US geography, manually annotated with Prolog. We use the SQL version (Popescu et al., 2003; Giordani and Moschitti, 2012; Iyer et al., 2017), which follows the logical form data split (Zettlemoyer and Collins, 2005). Restaurants (Tang and Mooney, 2000; Popescu et al., 2003) User questions about restaurants, their food types, and locations. Scholar (Iyer et al., 2017) User questions about academic publications, with automatically generated SQL that was checked by asking the user if the output was correct. Academic (Li and Jagadish, 2014) Questions about the Microsoft Academic Search (MAS) database, derived by enumerating every logical query that could be expressed using the search page of the MAS website and writing sentences to match them. The domain is similar to that of Scholar, but their schemas differ. 352 Yelp and IMDB (Yaghmazadeh et"
P18-1033,P11-1060,0,0.0827939,"ns about evaluation methodology apply broadly to the systems cited below. Within the DB community, systems commonly use pattern matching, grammar-based techniques, or intermediate representations of the query (Pazos Rangel et al., 2013). Recent work has explored incorporating user feedback to improve accuracy (Li and Jagadish, 2014). Unfortunately, none of these systems are publicly available, and many rely on domain-specific resources. In the NLP community, there has been extensive work on semantic parsing to logical representations that query a knowledge base (Zettlemoyer and Collins, 2005; Liang et al., 2011; Beltagy et al., 2014; Berant and Liang, 2014), while work on mapping to SQL has recently increased (Yih et al., 2015; Iyer et al., 2017; Zhong et al., 2017). One of the earliest statistical models for mapping text to SQL was the PRECISE system (Popescu et al., 2003, 2004), which achieved high precision on queries that met constraints linking tokens and database values, attributes, and relations, but did not attempt to generate SQL for questions outside this class. Later work considered generating queries based on relations extracted by a syntactic parser (Giordani and Moschitti, 2012) and ap"
P18-1033,P15-1128,0,0.0259465,"pattern matching, grammar-based techniques, or intermediate representations of the query (Pazos Rangel et al., 2013). Recent work has explored incorporating user feedback to improve accuracy (Li and Jagadish, 2014). Unfortunately, none of these systems are publicly available, and many rely on domain-specific resources. In the NLP community, there has been extensive work on semantic parsing to logical representations that query a knowledge base (Zettlemoyer and Collins, 2005; Liang et al., 2011; Beltagy et al., 2014; Berant and Liang, 2014), while work on mapping to SQL has recently increased (Yih et al., 2015; Iyer et al., 2017; Zhong et al., 2017). One of the earliest statistical models for mapping text to SQL was the PRECISE system (Popescu et al., 2003, 2004), which achieved high precision on queries that met constraints linking tokens and database values, attributes, and relations, but did not attempt to generate SQL for questions outside this class. Later work considered generating queries based on relations extracted by a syntactic parser (Giordani and Moschitti, 2012) and applying techniques from logical parsing research (Poon, 2013). However, none of these earlier systems are publicly avai"
P18-1033,W16-0105,0,0.0149045,"e for applying neural models to this task is annotating large enough datasets of question-query pairs. Recent work (Cai et al., 2017; Zhong et al., 2017) has automatically generated large datasets using templates to form random queries and corresponding natural-languagelike questions, and then having humans rephrase the question into English. Another option is to use feedback-based learning, where the system alternates between training and making predictions, which a user rates as correct or not (Iyer et al., 2017). Other work seeks to avoid the data bottleneck by using end-to-end approaches (Yin et al., 2016; Neelakantan et al., 2017), which we do not consider here. One key contribution of this paper is standardization of a range of datasets, to help address the challenge of limited data resources. challenging form of ambiguity from the task. In the process, we apply extensive effort to standardize datasets and fix a range of errors. Previous NLIDB work has led to impressive systems, but current evaluations provide an incomplete picture of their strengths and weaknesses. In this paper, we provide new and improved data, a new baseline, and guidelines that complement existing metrics, supporting fu"
P18-1033,D07-1071,0,0.0602937,"as the lexicon used by PRECISE. More recent work has produced general purpose systems that are competitive with previous results and are also available, such as Iyer et al. (2017). We also adapt a logical form parser with a sequence to tree approach that makes very few as3 Data For our analysis, we study a range of text-to-SQL datasets, standardizing them to have a consistent SQL style. ATIS (Price, 1990; Dahl et al., 1994) User questions for a flight-booking task, manually annotated. We use the modified SQL from Iyer et al. (2017), which follows the data split from the logical form version (Zettlemoyer and Collins, 2007). GeoQuery (Zelle and Mooney, 1996) User questions about US geography, manually annotated with Prolog. We use the SQL version (Popescu et al., 2003; Giordani and Moschitti, 2012; Iyer et al., 2017), which follows the logical form data split (Zettlemoyer and Collins, 2005). Restaurants (Tang and Mooney, 2000; Popescu et al., 2003) User questions about restaurants, their food types, and locations. Scholar (Iyer et al., 2017) User questions about academic publications, with automatically generated SQL that was checked by asking the user if the output was correct. Academic (Li and Jagadish, 2014)"
P18-1033,P13-1092,0,0.0364262,"while work on mapping to SQL has recently increased (Yih et al., 2015; Iyer et al., 2017; Zhong et al., 2017). One of the earliest statistical models for mapping text to SQL was the PRECISE system (Popescu et al., 2003, 2004), which achieved high precision on queries that met constraints linking tokens and database values, attributes, and relations, but did not attempt to generate SQL for questions outside this class. Later work considered generating queries based on relations extracted by a syntactic parser (Giordani and Moschitti, 2012) and applying techniques from logical parsing research (Poon, 2013). However, none of these earlier systems are publicly available, and some required extensive engineering effort for each domain, such as the lexicon used by PRECISE. More recent work has produced general purpose systems that are competitive with previous results and are also available, such as Iyer et al. (2017). We also adapt a logical form parser with a sequence to tree approach that makes very few as3 Data For our analysis, we study a range of text-to-SQL datasets, standardizing them to have a consistent SQL style. ATIS (Price, 1990; Dahl et al., 1994) User questions for a flight-booking ta"
P18-1033,C04-1021,0,0.784096,"Missing"
P18-1033,H90-1080,0,0.212278,"12) and applying techniques from logical parsing research (Poon, 2013). However, none of these earlier systems are publicly available, and some required extensive engineering effort for each domain, such as the lexicon used by PRECISE. More recent work has produced general purpose systems that are competitive with previous results and are also available, such as Iyer et al. (2017). We also adapt a logical form parser with a sequence to tree approach that makes very few as3 Data For our analysis, we study a range of text-to-SQL datasets, standardizing them to have a consistent SQL style. ATIS (Price, 1990; Dahl et al., 1994) User questions for a flight-booking task, manually annotated. We use the modified SQL from Iyer et al. (2017), which follows the data split from the logical form version (Zettlemoyer and Collins, 2007). GeoQuery (Zelle and Mooney, 1996) User questions about US geography, manually annotated with Prolog. We use the SQL version (Popescu et al., 2003; Giordani and Moschitti, 2012; Iyer et al., 2017), which follows the logical form data split (Zettlemoyer and Collins, 2005). Restaurants (Tang and Mooney, 2000; Popescu et al., 2003) User questions about restaurants, their food t"
P19-1374,L16-1704,0,0.0477399,"Missing"
P19-1374,P17-1152,0,0.133362,"Missing"
P19-1374,P08-1095,0,0.0281157,"ons on websites. Even when structure is provided it often has limited depth, such as threads in Slack, which provide one layer of branching. In all of these cases, conversations are entangled: all messages appear together, with no indication of separate conversations. Automatic disentanglement could be used to provide more interpretable results when searching over chat logs, and to help users understand what is happening when they join a channel. Over a decade of research has considered conversation disentanglement (Shen et al., 2006), but using datasets that are either small (2,500 messages, Elsner and Charniak, 2008) or not released (Adams and Martell, 2008). ∗ We introduce a conversation disentanglement dataset of 77,563 messages of IRC manually annotated with reply-to relations between messages.1 Our data is sampled from a technical support channel at 173 points in time between 2004 and 2018, providing a diverse set of speakers and topics, while remaining in a single domain. Our data is the first to include context, which differentiates messages that start a conversation from messages that are responding to an earlier point in time. We are also the first to adjudicate disagreements in disentanglement an"
P19-1374,J10-3004,0,0.0384943,"Missing"
P19-1374,P11-1118,0,0.0583846,"Missing"
P19-1374,W09-1803,0,0.0582107,"Missing"
P19-1374,W19-4107,1,0.817564,"Missing"
P19-1374,N18-1164,0,0.0599443,"Missing"
P19-1374,W10-2923,0,0.0797568,"Missing"
P19-1374,P19-3002,1,0.859969,"Missing"
P19-1374,D15-1178,0,0.0496049,"Missing"
P19-1374,W12-1607,0,0.0502135,"Missing"
P19-1374,I17-1062,0,0.0359906,"Missing"
P19-1374,D11-1002,0,0.0671747,"Missing"
P19-1374,N09-1023,0,0.0391231,"Missing"
P19-1374,D14-1162,0,0.0811995,"Missing"
P19-1374,N10-1097,0,0.0753044,"Missing"
P19-1374,I13-1089,0,0.0656406,"Missing"
P19-3002,N03-4009,0,0.242563,"Missing"
P19-3002,W14-0612,0,0.0635986,"Missing"
P19-3002,P08-4004,0,0.0817535,"Missing"
P19-3002,N06-4006,0,0.307476,"Missing"
P19-3002,W03-2120,0,0.23535,"Missing"
P19-3002,N13-3004,0,0.0376772,"Missing"
P19-3002,petasis-2012-sync3,0,0.0285827,"Missing"
P19-3002,W15-0203,0,0.0678233,"Missing"
P19-3002,E17-3004,0,0.031768,"Missing"
P19-3002,W16-5808,0,0.045357,"Missing"
P19-3002,koeva-etal-2008-chooser,0,0.0801608,"Missing"
P19-3002,W12-2416,0,0.0439085,"Missing"
P19-3002,W11-0416,0,0.0585475,"Missing"
P19-3002,landragin-etal-2012-analec,0,0.0648657,"Missing"
P19-3002,P18-4006,0,0.0744695,"Missing"
P19-3002,bartalesi-lenzi-etal-2012-cat,0,0.0707176,"Missing"
P19-3002,P13-4001,0,0.0684475,"Missing"
P19-3002,marcinczuk-etal-2012-inforex,0,0.0793718,"Missing"
Q17-1031,P11-2037,0,0.180279,"sponds to the 1-EC pagenumber-2 space, a property that applies to this work as well2 . Parsing with Null Elements in the PTB has taken two general approaches. The first broadly effective system was Johnson (2002), which post-processed the output of a parser, inserting extra elements. This was effective for some types of structure, such as null complementizers, but had difficulty with long distance dependencies. The other common approach has been to thread a trace through the tree structure on the non-terminal symbols. Collins (1997)’s third model used this approach to recover wh-traces, while Cai et al. (2011) used it to recover null pronouns, and others have used it for a range of movement types (Dienes and Dubey 2003; Schmid 2006). These approaches have the disadvantage that each 1 To produce directed edges, their parser treats the direction as part of the edge label. 2 This is a topological space with two half-planes sharing a boundary. All edges are drawn on one of the two half-planes and each half-plane contains no crossings. 442 additional trace dramatically expands the grammar. Our representation is similar to LTAG-Spinal (Shen et al. 2007) but has the advantage that it can be converted back"
Q17-1031,P04-1082,0,0.296388,"Missing"
Q17-1031,P17-1193,0,0.0332959,"the space of all non-projective graphs is intractable. Fortunately, in practice almost all parses are covered by well-defined subsets of this space. For dependency parsing, recent work has defined algorithms for inference within various subspaces (G´omezRodr´ıguez and Nivre 2010; Pitler et al. 2013). We build upon these algorithms and adapt them to constituency parsing. For constituency parsing, a range of formalisms have been developed that are mildlycontext sensitive, such as CCG (Steedman 2000), LFG (Kaplan and Bresnan 1982), and LTAG (Joshi and Schabes 1997). Concurrently with this work, Cao et al. (2017) also proposed a graph version of Pitler et al. (2013)’s One-Endpoint Crossing (1-EC) algorithm. However, Cao’s algorithm does not consider the direction of edges1 and so it could produce cycles, or graphs with multiple root nodes. Their algorithm also has spurious ambiguity, with multiple derivations of the same parse structure permitted. One advantage of their algorithm is that by introducing a new item type it can handle some cases of the Locked-Chain we define below (specifically, when N is even), though in practise they also restrict their algorithm to ignore such cases. They also show th"
Q17-1031,W08-2102,0,0.229054,"k word labels in the state, we need to adjust every n by a factor of S, leading to O(S 4 n4 + ES 2 n2 ). 4 Parse Representation Our algorithm relies on the assumption that we can process the dependents to the left and right of a word independently and then combine the two halves. This means we need lexicalized structures, which the PTB does not provide. We define a new representation in which each non-terminal symbol is associated with a specific word (the head). Unlike dependency parsing, we retain all the information required to reconstruct the constituency parse. Our approach is related to Carreras et al. (2008) and Hayashi and Nagata (2016), with three key differences: (1) we encode non-terminals explicitly, rather than implicitly through adjunction operations, which can cause ambiguity, (2) we add representations of null elements and co-indexation, (3) we modify head rules to avoid problematic structures. Figure 3 shows a comparison of the PTB representation and ours. We add lexicalization, assigning each non-terminal to a word. The only other changes are visual notation, with non-terminals moved to be directly above the words to more clearly show the distinction between spines and edges. Spines: E"
Q17-1031,J07-2003,0,0.00752945,"bining items). Almost every template in Algorithm 1 generates some unnecessary rules, and no items of type B are needed. 4 One alternative is to count half of it on each end, removing the need for subtraction later. Another is to add it during the combination step. The remaining rules still have high coverage of the development set, missing only 15 rules, each applied once (out of 78,692 rule applications). By pruning in this way, we are considering the intersection of 1-EC graphs and the true space of structures used in language. Chart Pruning: To improve speed we use beams and cube pruning (Chiang 2007), discarding items based on their Viterbi inside score. We divide each beam into sub-beams based on aspects of the state. This ensures diversity and enables consideration of only compatible items during binary and ternary compositions. Coarse to Fine Pruning: Rather than parsing immediately with the full model we use several passes with progressively richer structure (Goodman 1997): (1) Projective parsing without traces or spines, and simultaneously a trace classifier, (2) Non-projective parsing without spines, and simultaneously a spine classifier, (3) Full structure parsing. Each pass prunes"
Q17-1031,P97-1003,0,0.199574,"nore such cases. They also show that the class of graphs they generate corresponds to the 1-EC pagenumber-2 space, a property that applies to this work as well2 . Parsing with Null Elements in the PTB has taken two general approaches. The first broadly effective system was Johnson (2002), which post-processed the output of a parser, inserting extra elements. This was effective for some types of structure, such as null complementizers, but had difficulty with long distance dependencies. The other common approach has been to thread a trace through the tree structure on the non-terminal symbols. Collins (1997)’s third model used this approach to recover wh-traces, while Cai et al. (2011) used it to recover null pronouns, and others have used it for a range of movement types (Dienes and Dubey 2003; Schmid 2006). These approaches have the disadvantage that each 1 To produce directed edges, their parser treats the direction as part of the edge label. 2 This is a topological space with two half-planes sharing a boundary. All edges are drawn on one of the two half-planes and each half-plane contains no crossings. 442 additional trace dramatically expands the grammar. Our representation is similar to LTA"
Q17-1031,P03-1055,0,0.600072,"ce edges that represent control structures, wh-movement and more. However, most parsers and the standard evaluation metric ignore these edges and all null elements. By leaving out parts of the structure, they fail to provide key relations to downstream tasks such as question answering. While there has been work on capturing some parts of this extra structure, it has generally either been through post-processing on trees (Johnson 2002; Jijkoun 2003; Campbell 2004; Levy and Manning 2004; Gabbard et al. 2006) or has only captured a limited set of phenomena via grammar augmentation (Collins 1997; Dienes and Dubey 2003; Schmid 2006; Cai et al. 2011). We propose a new general-purpose parsing algorithm that can efficiently search over a wide range of syntactic phenomena. Our algorithm extends a non-projective tree parsing algorithm (Pitler et al. 2013; Pitler 2014) to graph structures, with improvements to avoid derivational ambiguity while maintaining an O(n4 ) runtime. Our algorithm also includes an optional extension to ensure parses contain a directed projective tree of non-trace edges. Our algorithm cannot apply directly to constituency parses–it requires lexicalized structures similar to dependency pars"
Q17-1031,C96-1058,0,0.551031,"007; Hall and Nivre 2008; Fern´andezGonz´alez and Martins 2015; Kong et al. 2015). Kato and Matsubara (2016) described a new approach, modifying a transition-based parser to recover null elements and traces, with strong results, but using heuristics to determine trace referents. 3 Algorithm Our algorithm is a dynamic program, similar at a high level to CKY (Kasami 1966; Younger 1967; Cocke 1969). The states of our dynamic program (items) represent partial parses. Usually in CKY, items are defined as covering the n words in a sentence, starting and ending at the spaces between words. We follow Eisner (1996), defining items as covering the n−1 spaces in a sentence, starting and ending on words, as shown in Figure 1. This means that we process each word’s left and right dependents separately, then combine the two halves. We use three types of items: (1) a single edge, linking two words, (2) a continuous span, going from one word to another, representing all edges linking pairs of words within the span, (3) a span (as defined in 2) plus an additional word outside the span, enabling the inclusion of edges between that word and words in the span. Within the CKY framework, the key to defining our algo"
Q17-1031,P15-1147,0,0.0695872,"Missing"
Q17-1031,N06-1024,0,0.874605,"Missing"
Q17-1031,P10-1151,0,0.042706,"Missing"
Q17-1031,W97-0302,0,0.329732,"78,692 rule applications). By pruning in this way, we are considering the intersection of 1-EC graphs and the true space of structures used in language. Chart Pruning: To improve speed we use beams and cube pruning (Chiang 2007), discarding items based on their Viterbi inside score. We divide each beam into sub-beams based on aspects of the state. This ensures diversity and enables consideration of only compatible items during binary and ternary compositions. Coarse to Fine Pruning: Rather than parsing immediately with the full model we use several passes with progressively richer structure (Goodman 1997): (1) Projective parsing without traces or spines, and simultaneously a trace classifier, (2) Non-projective parsing without spines, and simultaneously a spine classifier, (3) Full structure parsing. Each pass prunes using parse max-marginals and classifier scores, tuned on the development set. The third pass also prunes spines that are not consistent with any unpruned edge from the second pass. For the spine classifier we use a bidirectional LSTM tagger, implemented in DyNet (Neubig et al. 2017). Speed: Parsing took an average of 8.6 seconds per sentence for graph parsing and 0.5 seconds when"
Q17-1031,W08-1007,0,0.0380596,"drawn on one of the two half-planes and each half-plane contains no crossings. 442 additional trace dramatically expands the grammar. Our representation is similar to LTAG-Spinal (Shen et al. 2007) but has the advantage that it can be converted back into the PTB representation. Hayashi and Nagata (2016) also incorporated null elements into a spinal structure but did not include a representation of co-indexation. In related work, dependency parsers have been used to assist in constituency parsing, with varying degrees of representation design, but only for trees (Hall, Nivre, and Nilsson 2007; Hall and Nivre 2008; Fern´andezGonz´alez and Martins 2015; Kong et al. 2015). Kato and Matsubara (2016) described a new approach, modifying a transition-based parser to recover null elements and traces, with strong results, but using heuristics to determine trace referents. 3 Algorithm Our algorithm is a dynamic program, similar at a high level to CKY (Kasami 1966; Younger 1967; Cocke 1969). The states of our dynamic program (items) represent partial parses. Usually in CKY, items are defined as covering the n words in a sentence, starting and ending at the spaces between words. We follow Eisner (1996), defining"
Q17-1031,W07-2444,0,0.0606674,"Missing"
Q17-1031,P16-2016,0,0.452837,"uns, and others have used it for a range of movement types (Dienes and Dubey 2003; Schmid 2006). These approaches have the disadvantage that each 1 To produce directed edges, their parser treats the direction as part of the edge label. 2 This is a topological space with two half-planes sharing a boundary. All edges are drawn on one of the two half-planes and each half-plane contains no crossings. 442 additional trace dramatically expands the grammar. Our representation is similar to LTAG-Spinal (Shen et al. 2007) but has the advantage that it can be converted back into the PTB representation. Hayashi and Nagata (2016) also incorporated null elements into a spinal structure but did not include a representation of co-indexation. In related work, dependency parsers have been used to assist in constituency parsing, with varying degrees of representation design, but only for trees (Hall, Nivre, and Nilsson 2007; Hall and Nivre 2008; Fern´andezGonz´alez and Martins 2015; Kong et al. 2015). Kato and Matsubara (2016) described a new approach, modifying a transition-based parser to recover null elements and traces, with strong results, but using heuristics to determine trace referents. 3 Algorithm Our algorithm is"
Q17-1031,P03-2006,0,0.126092,"Missing"
Q17-1031,P02-1018,0,0.768512,"gorithm also has spurious ambiguity, with multiple derivations of the same parse structure permitted. One advantage of their algorithm is that by introducing a new item type it can handle some cases of the Locked-Chain we define below (specifically, when N is even), though in practise they also restrict their algorithm to ignore such cases. They also show that the class of graphs they generate corresponds to the 1-EC pagenumber-2 space, a property that applies to this work as well2 . Parsing with Null Elements in the PTB has taken two general approaches. The first broadly effective system was Johnson (2002), which post-processed the output of a parser, inserting extra elements. This was effective for some types of structure, such as null complementizers, but had difficulty with long distance dependencies. The other common approach has been to thread a trace through the tree structure on the non-terminal symbols. Collins (1997)’s third model used this approach to recover wh-traces, while Cai et al. (2011) used it to recover null pronouns, and others have used it for a range of movement types (Dienes and Dubey 2003; Schmid 2006). These approaches have the disadvantage that each 1 To produce direct"
Q17-1031,P04-1042,0,0.350116,"Missing"
Q17-1031,P16-1088,0,0.585559,"442 additional trace dramatically expands the grammar. Our representation is similar to LTAG-Spinal (Shen et al. 2007) but has the advantage that it can be converted back into the PTB representation. Hayashi and Nagata (2016) also incorporated null elements into a spinal structure but did not include a representation of co-indexation. In related work, dependency parsers have been used to assist in constituency parsing, with varying degrees of representation design, but only for trees (Hall, Nivre, and Nilsson 2007; Hall and Nivre 2008; Fern´andezGonz´alez and Martins 2015; Kong et al. 2015). Kato and Matsubara (2016) described a new approach, modifying a transition-based parser to recover null elements and traces, with strong results, but using heuristics to determine trace referents. 3 Algorithm Our algorithm is a dynamic program, similar at a high level to CKY (Kasami 1966; Younger 1967; Cocke 1969). The states of our dynamic program (items) represent partial parses. Usually in CKY, items are defined as covering the n words in a sentence, starting and ending at the spaces between words. We follow Eisner (1996), defining items as covering the n−1 spaces in a sentence, starting and ending on words, as sho"
Q17-1031,N15-1080,0,0.0134479,"tains no crossings. 442 additional trace dramatically expands the grammar. Our representation is similar to LTAG-Spinal (Shen et al. 2007) but has the advantage that it can be converted back into the PTB representation. Hayashi and Nagata (2016) also incorporated null elements into a spinal structure but did not include a representation of co-indexation. In related work, dependency parsers have been used to assist in constituency parsing, with varying degrees of representation design, but only for trees (Hall, Nivre, and Nilsson 2007; Hall and Nivre 2008; Fern´andezGonz´alez and Martins 2015; Kong et al. 2015). Kato and Matsubara (2016) described a new approach, modifying a transition-based parser to recover null elements and traces, with strong results, but using heuristics to determine trace referents. 3 Algorithm Our algorithm is a dynamic program, similar at a high level to CKY (Kasami 1966; Younger 1967; Cocke 1969). The states of our dynamic program (items) represent partial parses. Usually in CKY, items are defined as covering the n words in a sentence, starting and ending at the spaces between words. We follow Eisner (1996), defining items as covering the n−1 spaces in a sentence, starting"
Q17-1031,D15-1032,1,0.90542,"Missing"
Q17-1031,J93-2004,0,0.061933,"Missing"
Q17-1031,P05-1012,0,0.132057,"Missing"
Q17-1031,Q14-1004,0,0.167306,"Missing"
Q17-1031,Q13-1002,0,0.374895,"to downstream tasks such as question answering. While there has been work on capturing some parts of this extra structure, it has generally either been through post-processing on trees (Johnson 2002; Jijkoun 2003; Campbell 2004; Levy and Manning 2004; Gabbard et al. 2006) or has only captured a limited set of phenomena via grammar augmentation (Collins 1997; Dienes and Dubey 2003; Schmid 2006; Cai et al. 2011). We propose a new general-purpose parsing algorithm that can efficiently search over a wide range of syntactic phenomena. Our algorithm extends a non-projective tree parsing algorithm (Pitler et al. 2013; Pitler 2014) to graph structures, with improvements to avoid derivational ambiguity while maintaining an O(n4 ) runtime. Our algorithm also includes an optional extension to ensure parses contain a directed projective tree of non-trace edges. Our algorithm cannot apply directly to constituency parses–it requires lexicalized structures similar to dependency parses. We extend and improve previous work on lexicalized constituent representations (Shen et al. 2007; Carreras et al. 2008; Hayashi and Nagata 2016) to handle traces. In this form, traces can create problematic structures such as direc"
Q17-1031,P06-1023,0,0.78761,"control structures, wh-movement and more. However, most parsers and the standard evaluation metric ignore these edges and all null elements. By leaving out parts of the structure, they fail to provide key relations to downstream tasks such as question answering. While there has been work on capturing some parts of this extra structure, it has generally either been through post-processing on trees (Johnson 2002; Jijkoun 2003; Campbell 2004; Levy and Manning 2004; Gabbard et al. 2006) or has only captured a limited set of phenomena via grammar augmentation (Collins 1997; Dienes and Dubey 2003; Schmid 2006; Cai et al. 2011). We propose a new general-purpose parsing algorithm that can efficiently search over a wide range of syntactic phenomena. Our algorithm extends a non-projective tree parsing algorithm (Pitler et al. 2013; Pitler 2014) to graph structures, with improvements to avoid derivational ambiguity while maintaining an O(n4 ) runtime. Our algorithm also includes an optional extension to ensure parses contain a directed projective tree of non-trace edges. Our algorithm cannot apply directly to constituency parses–it requires lexicalized structures similar to dependency parses. We extend"
Q17-1031,C12-1147,0,0.0517144,"Missing"
Q17-1031,Q14-1026,0,\N,Missing
U08-1008,W02-2001,0,0.1172,"ns accurately. Also, because these systems will be running during parsing, they need to be fast to prevent the creation of an additional bottleneck. Here we focus on Verb-Particle Constructions (VPCs), a type of MWE composed of a verb and a particle. Villavicencio (2003a) showed that VPCs are poorly covered by corpus data and constantly growing in number, making them a suitable candidate for an automatic classification system. Previous work on VPCs has mainly focused either on their compositionality (McCarthy et al., 2003), or on using sophisticated parsers to perform extraction from corpora (Baldwin and Villavicencio, 2002). While parser based methods have been very successful (Kim and Baldwin, 2006) they rely on contextual knowledge and complex processing. The Web has been used as a corpus previously by Villavicencio (2003b), who used search engines as a source of statistics for classification, but her aim was to create new resources, rather than a tool that can be used for identification. We have constructed a high-throughput query system for the Google Web1T data, which we use to collect information to perform the fronting linguistic constituency test. The results of this process are used to train a classifie"
U08-1008,W03-1809,0,0.0302881,"on compositionality as the two areas are closely linked. 2.1 Compositionality Determining how much the simplex meaning of individual words in a MWE contribute to the overall meaning is challenging, and important for producing semantically correct analysis of text. A range of methods have been considered to differentiate examples based on their degree of semantic idiosyncrasy. Initial work by Lin (1999) considered the compositionality of MWEs by comparing the distributional characteristics for a given MWE and potentially similar expressions formed by synonym substitution. This was followed by Bannard et al. (2003), who used human non-experts to construct a gold standard dataset of VPCs and their compositionality, which was used to construct a classifier that could judge whether the two words used contributed their simplex meaning. McCarthy et al. (2003) used an automatically acquired thesaurus in the calculation of statistics regarding the compositionality of VPCs and compared their results with statistics commonly used for extracting multiwords, such as latent semantic analysis. Bannard (2005) compared the lexical contexts of VPC s and their component words across a corpus to determine which words are"
U08-1008,P08-1002,0,0.0348507,"Missing"
U08-1008,W06-1207,0,0.0491799,"ing multiwords, such as latent semantic analysis. Bannard (2005) compared the lexical contexts of VPC s and their component words across a corpus to determine which words are contributing an independent meaning. Light Verb Constructions (LVCs) are another example of an MWE that has been considered in compositionality studies. The challenge of distinguishing LVCs from idioms was considered by Fazly et al. (2005), who proposed a set of statistical measures to quantify properties that relate to the compositionality of an MWE, ideas that were then extended in Fazly and Stevenson (2007). Recently, Cook and Stevenson (2006) addressed the question of which sense of the component words is being used in a particular VPC. They focused on the contribution by particles and constructed a feature set based on the properties of VPC s and compared their effectiveness with standard co-occurrence measurements. 2.2 Classification A range of methods for automatic classification of MWE s have been studied previously, in particular the use of parsers, applying heuristics to n-grams, and search engine queries. Possibly the earliest attempt at classification, was by Smadja (1993), who considered verbparticle pairs, separated by u"
U08-1008,W07-1102,0,0.0117343,"h statistics commonly used for extracting multiwords, such as latent semantic analysis. Bannard (2005) compared the lexical contexts of VPC s and their component words across a corpus to determine which words are contributing an independent meaning. Light Verb Constructions (LVCs) are another example of an MWE that has been considered in compositionality studies. The challenge of distinguishing LVCs from idioms was considered by Fazly et al. (2005), who proposed a set of statistical measures to quantify properties that relate to the compositionality of an MWE, ideas that were then extended in Fazly and Stevenson (2007). Recently, Cook and Stevenson (2006) addressed the question of which sense of the component words is being used in a particular VPC. They focused on the contribution by particles and constructed a feature set based on the properties of VPC s and compared their effectiveness with standard co-occurrence measurements. 2.2 Classification A range of methods for automatic classification of MWE s have been studied previously, in particular the use of parsers, applying heuristics to n-grams, and search engine queries. Possibly the earliest attempt at classification, was by Smadja (1993), who consider"
U08-1008,W05-1005,0,0.0135417,"McCarthy et al. (2003) used an automatically acquired thesaurus in the calculation of statistics regarding the compositionality of VPCs and compared their results with statistics commonly used for extracting multiwords, such as latent semantic analysis. Bannard (2005) compared the lexical contexts of VPC s and their component words across a corpus to determine which words are contributing an independent meaning. Light Verb Constructions (LVCs) are another example of an MWE that has been considered in compositionality studies. The challenge of distinguishing LVCs from idioms was considered by Fazly et al. (2005), who proposed a set of statistical measures to quantify properties that relate to the compositionality of an MWE, ideas that were then extended in Fazly and Stevenson (2007). Recently, Cook and Stevenson (2006) addressed the question of which sense of the component words is being used in a particular VPC. They focused on the contribution by particles and constructed a feature set based on the properties of VPC s and compared their effectiveness with standard co-occurrence measurements. 2.2 Classification A range of methods for automatic classification of MWE s have been studied previously, in"
U08-1008,U07-1008,0,0.0653415,"71 120 47 60 52 54 45 63 Table 1: N-grams in the Web1T corpus for the VPC ferret out. alphabetical characters. One approach to this dataset is to simply treat it as a normal collection of n-gram frequencies, scanning the relevant sections for answers, such as in Bergsma et al. (2008) and Yuret (2007). Another approach is used by Talbot and Brants (2008), pruning the corpus to a third of its size and quantising the rest, reducing the space used by frequency counts to eight bits each. These methods have the disadvantages of slow execution and only providing approximate frequencies respectively. Hawker et al. (2007) considered two methods for making practical queries possible, preprocessing of the data, and pre-processing of queries. The first approach is to reduce the size of the dataset by decreasing the resolution of measurements, and by accessing n-grams by implicit information based on their location in the compressed data, rather than their actual representation. While this avoids the cost of processing the entire dataset for each query, it does produce less accurate results. The second method described is intelligent batching queries, then performing a single pass through the data to answer them a"
U08-1008,W06-2110,0,0.0928112,"tra information about the context of the verb-particle pair being considered. In particular, the information needed to identify the head noun phrase (NP) for each potential VPC. Early work by Blaheta and Johnson (2001) used a parsed corpus and log-linear models to identify VPC s. This was followed by Baldwin and Villavicencio (2002) who used a range of parser outputs and other features to produce a better informed classifier. Other forms of linguistic and statistical unsupervised methods were considered by Baldwin (2005), such as Pointwise Mutual Information. This work was further extended by Kim and Baldwin (2006) to utilise the sentential context of verb-particle pairs and their associated NP to improve results. The closest work to our own in terms of the corpus used is that of Villavicencio (2003b), in which the web is used to test the validity of candidate VPC s. Also, like our work, Villavicencio does not consider the context of the verb and particle, unlike the parser based methods described above. A collection of verb-particle pairs was generated by combining verbs from Levin’s classes (Levin, 1993) with the particle up. Each potential VPC was passed to the search engine Google to obtain an appro"
U08-1008,P99-1041,0,0.0376084,"s work concerning VPCs can be broadly divided into two groups, compositionality analysis, and classification. Our work is entirely concerned with classification, but we will briefly describe previous work on compositionality as the two areas are closely linked. 2.1 Compositionality Determining how much the simplex meaning of individual words in a MWE contribute to the overall meaning is challenging, and important for producing semantically correct analysis of text. A range of methods have been considered to differentiate examples based on their degree of semantic idiosyncrasy. Initial work by Lin (1999) considered the compositionality of MWEs by comparing the distributional characteristics for a given MWE and potentially similar expressions formed by synonym substitution. This was followed by Bannard et al. (2003), who used human non-experts to construct a gold standard dataset of VPCs and their compositionality, which was used to construct a classifier that could judge whether the two words used contributed their simplex meaning. McCarthy et al. (2003) used an automatically acquired thesaurus in the calculation of statistics regarding the compositionality of VPCs and compared their results"
U08-1008,W03-1810,0,0.0939327,"Missing"
U08-1008,J93-1007,0,0.0648401,"Fazly and Stevenson (2007). Recently, Cook and Stevenson (2006) addressed the question of which sense of the component words is being used in a particular VPC. They focused on the contribution by particles and constructed a feature set based on the properties of VPC s and compared their effectiveness with standard co-occurrence measurements. 2.2 Classification A range of methods for automatic classification of MWE s have been studied previously, in particular the use of parsers, applying heuristics to n-grams, and search engine queries. Possibly the earliest attempt at classification, was by Smadja (1993), who considered verbparticle pairs, separated by up to four other words, but did not perform a rigorous evaluation, which prevents us from performing a comparison. Recent work has focused on using parsers over raw text to gain extra information about the context of the verb-particle pair being considered. In particular, the information needed to identify the head noun phrase (NP) for each potential VPC. Early work by Blaheta and Johnson (2001) used a parsed corpus and log-linear models to identify VPC s. This was followed by Baldwin and Villavicencio (2002) who used a range of parser outputs"
U08-1008,P08-1058,0,0.0163202,"ferret you out out a ferret out of ferret out the ferret ferret it all out ferret these people out ferret these projects out out of the ferret ferret lovers can ferret out out a needing shelter ferret Frequency 79728 74 52 342 43 54 1562 58 180 1582 232 58 148 100 63 71 120 47 60 52 54 45 63 Table 1: N-grams in the Web1T corpus for the VPC ferret out. alphabetical characters. One approach to this dataset is to simply treat it as a normal collection of n-gram frequencies, scanning the relevant sections for answers, such as in Bergsma et al. (2008) and Yuret (2007). Another approach is used by Talbot and Brants (2008), pruning the corpus to a third of its size and quantising the rest, reducing the space used by frequency counts to eight bits each. These methods have the disadvantages of slow execution and only providing approximate frequencies respectively. Hawker et al. (2007) considered two methods for making practical queries possible, preprocessing of the data, and pre-processing of queries. The first approach is to reduce the size of the dataset by decreasing the resolution of measurements, and by accessing n-grams by implicit information based on their location in the compressed data, rather than the"
U08-1008,W03-1808,0,0.675799,"es are crucial for a range of Natural Language Processing (NLP) tasks, such as accurate parsing. Identification of MWEs does not prevent the production of syntactically accurate parses, but the extra information can improve results. Since manually creating these resources is a challenge, systems are needed that can automatically classify expressions accurately. Also, because these systems will be running during parsing, they need to be fast to prevent the creation of an additional bottleneck. Here we focus on Verb-Particle Constructions (VPCs), a type of MWE composed of a verb and a particle. Villavicencio (2003a) showed that VPCs are poorly covered by corpus data and constantly growing in number, making them a suitable candidate for an automatic classification system. Previous work on VPCs has mainly focused either on their compositionality (McCarthy et al., 2003), or on using sophisticated parsers to perform extraction from corpora (Baldwin and Villavicencio, 2002). While parser based methods have been very successful (Kim and Baldwin, 2006) they rely on contextual knowledge and complex processing. The Web has been used as a corpus previously by Villavicencio (2003b), who used search engines as a s"
U08-1008,S07-1044,0,0.0309763,"hese out ferret things out ferret this out ferret you out out a ferret out of ferret out the ferret ferret it all out ferret these people out ferret these projects out out of the ferret ferret lovers can ferret out out a needing shelter ferret Frequency 79728 74 52 342 43 54 1562 58 180 1582 232 58 148 100 63 71 120 47 60 52 54 45 63 Table 1: N-grams in the Web1T corpus for the VPC ferret out. alphabetical characters. One approach to this dataset is to simply treat it as a normal collection of n-gram frequencies, scanning the relevant sections for answers, such as in Bergsma et al. (2008) and Yuret (2007). Another approach is used by Talbot and Brants (2008), pruning the corpus to a third of its size and quantising the rest, reducing the space used by frequency counts to eight bits each. These methods have the disadvantages of slow execution and only providing approximate frequencies respectively. Hawker et al. (2007) considered two methods for making practical queries possible, preprocessing of the data, and pre-processing of queries. The first approach is to reduce the size of the dataset by decreasing the resolution of measurements, and by accessing n-grams by implicit information based on"
U09-1009,E99-1025,0,0.0913013,"Missing"
U09-1009,W02-2236,0,0.060454,"Missing"
U09-1009,W03-1013,1,0.677757,"te in particular the change of tag for ‘with’ in the two examples and its affect on the subsequent rule applications. The decision made by the supertagger effectively decides which analysis will be found, or if both are provided the parser must consider more possible derivations. I ate pizza with cutlery NP (S NP )/NP NP ((S NP )(S NP ))/NP S NP > NP > (S NP )(S NP ) < S NP < S I ate pizza with anchovies NP (S NP )/NP NP (NP NP )/NP NP NP NP S NP S NP > < > < Figure 1: Two CCG derivations with PP ambiguity. The CCG parser and associated supertagger we have used is the C&C parser (Clark and Curran, 2003; Clark and Curran, 2007b). The supertagger applies categories to words using the forward backward algorithm, and the parser forms a derivation by applying the Cocke–Younger– Kasami (CKY) chart parsing algorithm (Younger, 1967; Kasami, 1967) and dynamic programming. 2.1 Supertagging Supertags were first proposed by Joshi and Bangalore (1994) for Lexicalized Tree-Adjoining Grammar (LTAG). Like POS tags, supertags are assigned to each word in the sentence prior to parsing, but supertags contain much more detailed syntactic information. This leads to tag sets that are up to two orders of magnitud"
U09-1009,W07-1202,1,0.831586,"nge of tag for ‘with’ in the two examples and its affect on the subsequent rule applications. The decision made by the supertagger effectively decides which analysis will be found, or if both are provided the parser must consider more possible derivations. I ate pizza with cutlery NP (S NP )/NP NP ((S NP )(S NP ))/NP S NP > NP > (S NP )(S NP ) < S NP < S I ate pizza with anchovies NP (S NP )/NP NP (NP NP )/NP NP NP NP S NP S NP > < > < Figure 1: Two CCG derivations with PP ambiguity. The CCG parser and associated supertagger we have used is the C&C parser (Clark and Curran, 2003; Clark and Curran, 2007b). The supertagger applies categories to words using the forward backward algorithm, and the parser forms a derivation by applying the Cocke–Younger– Kasami (CKY) chart parsing algorithm (Younger, 1967; Kasami, 1967) and dynamic programming. 2.1 Supertagging Supertags were first proposed by Joshi and Bangalore (1994) for Lexicalized Tree-Adjoining Grammar (LTAG). Like POS tags, supertags are assigned to each word in the sentence prior to parsing, but supertags contain much more detailed syntactic information. This leads to tag sets that are up to two orders of magnitude larger. The first supe"
U09-1009,J07-4004,1,0.74932,"nge of tag for ‘with’ in the two examples and its affect on the subsequent rule applications. The decision made by the supertagger effectively decides which analysis will be found, or if both are provided the parser must consider more possible derivations. I ate pizza with cutlery NP (S NP )/NP NP ((S NP )(S NP ))/NP S NP > NP > (S NP )(S NP ) < S NP < S I ate pizza with anchovies NP (S NP )/NP NP (NP NP )/NP NP NP NP S NP S NP > < > < Figure 1: Two CCG derivations with PP ambiguity. The CCG parser and associated supertagger we have used is the C&C parser (Clark and Curran, 2003; Clark and Curran, 2007b). The supertagger applies categories to words using the forward backward algorithm, and the parser forms a derivation by applying the Cocke–Younger– Kasami (CKY) chart parsing algorithm (Younger, 1967; Kasami, 1967) and dynamic programming. 2.1 Supertagging Supertags were first proposed by Joshi and Bangalore (1994) for Lexicalized Tree-Adjoining Grammar (LTAG). Like POS tags, supertags are assigned to each word in the sentence prior to parsing, but supertags contain much more detailed syntactic information. This leads to tag sets that are up to two orders of magnitude larger. The first supe"
U09-1009,W03-0407,1,0.837372,"re accurate it can further constrain the set of possible derivations by supplying fewer categories, leaving the parser with less to do. One means of improving the supertagger’s statistical model of language is to provide more evidence, in this case, more annotated text. However, creating a significant amount of extra gold standard annotated text is not feasible. An alternative approach is ‘semi-supervised training’, in which a small set of annotated data and a much larger set of unannotated data is used. Training a system directly on its own output, ‘self-training’, is not normally effective (Clark et al., 2003), but recently McClosky et al. (2006) demonstrated that parser output can be made useful for retraining by the application of a reranker. To enable the use of more training data we have parallelised the C&C parser’s supertagger training process and implemented perceptron–based algorithms for parameter estimation. In the process of this work we also modified the C&C parser’s use of a particular CCG rule, based on observations of its behaviour. Our unlabeled training data was part of the English section of Wikipedia, consisting of 47 million sentences. We used the C&C parser to label the data wi"
U09-1009,W02-2203,0,0.055938,"Missing"
U09-1009,P04-1015,0,0.0563999,"Missing"
U09-1009,W02-1001,0,0.152541,"Missing"
U09-1009,P96-1011,0,0.0690988,"xcess Backward Composition In the process of debugging the parser, we investigated the number of times particular pairs of categories were combined. We were surprised to discover that a very large number of backward compositions were being performed in the chart, even though backward composition rarely occurred in the parser output (or in the gold standard itself). Backward composition is normally used for non-constituent coordination between pairs of type-raised categories, but the parser was also using it for combining non-type-raised and typeraised categories. This is an instance where the Eisner (1996) normal form constraints have failed to stop non-normal form derivations, because Eisner’s constraints were not designed to work with type-raising. We added a constraint that only allows backward composition to occur if both children are type-raised. 4 Methodology 4.1 Data Evaluation has been performed using Section 00 of CCGBank, a translation of the Penn Treebank to CCG (Hockenmaier, 2003). Sections 02-21 were used as training data and are simply referred to as WSJ in the following section. The raw Wikipedia data was tokenised using Punkt (Kiss and Strunk, 2006) and the NLTK tokeniser (Bird"
U09-1009,C94-1024,0,0.0714572,"re of sentences. However, when they are the bottleneck in the data acquisition phase of a system simple solutions are to use less data, or not use a parser at all. If we can improve the speed of parsers this will be unnecessary. For lexicalised grammars such as Combinatory Categorial Grammar (CCG) (Steedman, 2000) the step in which words are labelled with lexical categories has great influence on parsing speed and accuracy. In these formalisms, the labels chosen constrain the set of possible derivations so much that the process of choosing them, supertagging, is described as ‘almost parsing’ (Joshi and Bangalore, 1994). If the supertagger is more accurate it can further constrain the set of possible derivations by supplying fewer categories, leaving the parser with less to do. One means of improving the supertagger’s statistical model of language is to provide more evidence, in this case, more annotated text. However, creating a significant amount of extra gold standard annotated text is not feasible. An alternative approach is ‘semi-supervised training’, in which a small set of annotated data and a much larger set of unannotated data is used. Training a system directly on its own output, ‘self-training’, i"
U09-1009,J06-4003,0,0.0133171,"egories. This is an instance where the Eisner (1996) normal form constraints have failed to stop non-normal form derivations, because Eisner’s constraints were not designed to work with type-raising. We added a constraint that only allows backward composition to occur if both children are type-raised. 4 Methodology 4.1 Data Evaluation has been performed using Section 00 of CCGBank, a translation of the Penn Treebank to CCG (Hockenmaier, 2003). Sections 02-21 were used as training data and are simply referred to as WSJ in the following section. The raw Wikipedia data was tokenised using Punkt (Kiss and Strunk, 2006) and the NLTK tokeniser (Bird et al., 2009), and parsed using the C&C parser and models version 1.022 . The WSJ sentences had an average length of 23.5 words and a variance of 122.0 while the Wikipedia sentences had an average length of 21.7 words and a variance of 151.0. 2 http://svn.ask.it.usyd.edu.au/trac/candc 4.2 Evaluation For a given beta level the number of categories assigned to each word by the supertagger will vary greatly between models. This presents a problem because as described in Section 3 the number of categories assigned has a large influence on parsing speed and accuracy. T"
U09-1009,N06-1020,0,0.0325602,"in the set of possible derivations by supplying fewer categories, leaving the parser with less to do. One means of improving the supertagger’s statistical model of language is to provide more evidence, in this case, more annotated text. However, creating a significant amount of extra gold standard annotated text is not feasible. An alternative approach is ‘semi-supervised training’, in which a small set of annotated data and a much larger set of unannotated data is used. Training a system directly on its own output, ‘self-training’, is not normally effective (Clark et al., 2003), but recently McClosky et al. (2006) demonstrated that parser output can be made useful for retraining by the application of a reranker. To enable the use of more training data we have parallelised the C&C parser’s supertagger training process and implemented perceptron–based algorithms for parameter estimation. In the process of this work we also modified the C&C parser’s use of a particular CCG rule, based on observations of its behaviour. Our unlabeled training data was part of the English section of Wikipedia, consisting of 47 million sentences. We used the C&C parser to label the data with supertags, producing training data"
U09-1009,W00-1605,0,0.0711674,"Missing"
U09-1009,N01-1023,0,0.0607919,"Missing"
U09-1009,P95-1026,0,0.35424,"Missing"
U09-1009,W09-3832,0,0.0404103,"Missing"
W11-1916,N10-1061,1,0.916509,"n Coreference resolution is concerned with identifying mentions of entities in text and determining which mentions are referring to the same entity. Previously the focus in the field has been on the latter task. Typically, mentions were considered correct if their span was within the true span of a gold mention, and contained the head word. This task (Pradhan et al., 2011) has set a harder challenge by only considering exact matches to be correct. Our system uses an unsupervised approach based on a generative model. Unlike previous work, we did not use the Bllip or Wikipedia data described in Haghighi and Klein (2010). This was necessary for the system to be eligible for the closed task. The system detects mentions by finding the maximal projection of every noun and pronoun. For the OntoNotes corpus this approach posed several problems. First, the annotation scheme explicitly rejects noun phrases in certain constructions. And second, it includes coreference for events as well as things. In preliminary experiments on the development set, we found that spurious mentions were our primary source of error. Using an oracle to exclude all spurious mentions at evaluation time yielded improvements ranging from five"
W11-1916,P06-1055,1,0.631097,"a spurious mention, or because it is not co-referent. Without manually annotating the singletons in the data, these two cases cannot be easily separated. 3.1 Baseline mention detection The standard approach used in the system to detect mentions is to consider each word and its maximal projection, accepting it only if the span is an NP or the word is a pronoun. This approach will introduce spurious mentions if the parser makes a mistake, or if the NP is not considered a mention in the OntoNotes corpus. In this work, we considered the provided parses and parses produced by the Berkeley parser (Petrov et al., 2006) trained on the provided training data. We added a set of filters based on the annotation scheme described by Pradhan et al. (2007). Some filters are applied before coreference resolution and others afterward, as described below. Data Set Dev Test Filters None Pre Post All All P 37.59 39.49 59.05 58.69 56.97 R 76.93 76.83 68.08 67.98 69.77 F 50.50 52.17 63.24 63.00 62.72 Filters None Pre Post All Table 1: Mention detection performance with various subsets of the filters. 3.2 Before Coreference Resolution The pre-resolution filters were based on three reliable features of spurious mentions: • A"
W11-1916,W11-1901,0,0.0603164,"system behavior on the development set. These changes led to improvements in coreference F-score of 10.06, 5.71, 6.78, 6.63 and 3.09 on the MUC, B3 , Ceaf-e, Ceaf-m and Blanc, metrics, respectively, and a final task score of 47.10. 1 Introduction Coreference resolution is concerned with identifying mentions of entities in text and determining which mentions are referring to the same entity. Previously the focus in the field has been on the latter task. Typically, mentions were considered correct if their span was within the true span of a gold mention, and contained the head word. This task (Pradhan et al., 2011) has set a harder challenge by only considering exact matches to be correct. Our system uses an unsupervised approach based on a generative model. Unlike previous work, we did not use the Bllip or Wikipedia data described in Haghighi and Klein (2010). This was necessary for the system to be eligible for the closed task. The system detects mentions by finding the maximal projection of every noun and pronoun. For the OntoNotes corpus this approach posed several problems. First, the annotation scheme explicitly rejects noun phrases in certain constructions. And second, it includes coreference for"
W11-1916,J03-4003,0,\N,Missing
W19-4107,W15-4640,0,0.602688,"al people acting in a university student advising scenario. The other dataset was formed by applying a new disentanglement method (Kummerfeld et al., 2019) to extract conversations from an IRC channel of technical help for the Ubuntu operating system. We structured the dialogue problem as next 2 Task This task pushed the state-of-the-art in goaloriented dialogue systems in four directions deemed necessary for practical automated agents, using two new datasets. We sidestepped the challenge of evaluating generated utterances by formulating the problem as next utterance selection, as proposed by Lowe et al. (2015). At test time, participants were provided with partial conversations, each paired with a set of utterances that could be 1 https://ibm.github.io/dstc7-noesis/ public/index.html 60 Proceedings of the 1st Workshop on NLP for Conversational AI, pages 60–67 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics the next utterance in the conversation. Systems needed to rank these options, with the goal of placing the true utterance first. Prior work used sets of 2 or 10 utterances. We make the task harder by expanding the size of the sets, and considered several advanced"
W19-4107,N18-1202,0,0.0300619,"team on each subtask, ordered by teams’ average rank. Table 4 presents the full set of results, including all metrics for all subtasks. Participants Table 5 presents a summary of approaches teams used. One clear trend was the use of the Enhanced LSTM model (ESIM, Chen et al., 2017), though each team modified it differently as they worked to improve performance on the task. Other approaches covered a wide range of neural model components: Convolutional Neural Networks, Memory Networks, the Transformer, Attention, and Recurrent Neural Network variants. Two teams used ELMo word representations (Peters et al., 2018), while three constructed ensembles. Several teams also incorporated more classical approaches, such as TF-IDF based ranking, as part of their system. We provided a range of data sources in the task, with the goal of enabling innovation in training methods. Six teams used the external data, while four teams used the raw form of the Advising data. The rules did not state whether the validation data could be used as additional training data at test time, and so we asked each team what they used. As Table 5 shows, only four teams trained their systems with the validation data. 4.2 Discussion Over"
W19-4107,P17-1152,0,0.0789744,"14 weeks to develop their systems with access to the training and validation data, plus the external resources we provided. Additional external resources were not permitted, with the exception of pre-trained embeddings that were publicly available prior to the release of the data. 4.1 4.3 Table 2 presents the overall scores for each team on each subtask, ordered by teams’ average rank. Table 4 presents the full set of results, including all metrics for all subtasks. Participants Table 5 presents a summary of approaches teams used. One clear trend was the use of the Enhanced LSTM model (ESIM, Chen et al., 2017), though each team modified it differently as they worked to improve performance on the task. Other approaches covered a wide range of neural model components: Convolutional Neural Networks, Memory Networks, the Transformer, Attention, and Recurrent Neural Network variants. Two teams used ELMo word representations (Peters et al., 2018), while three constructed ensembles. Several teams also incorporated more classical approaches, such as TF-IDF based ranking, as part of their system. We provided a range of data sources in the task, with the goal of enabling innovation in training methods. Six t"
W19-4107,P17-2017,1,0.820225,"in utterances. Tokens are based on splitting on whitespace. had taken, what classes were available, and which were suggested (based on aggregate statistics from real student records). The data was collected over a year, with some data collected as part of courses in NLP and social computing, and some collected with paid participants. In the shared task, we provide all of this information - student preferences, and course information - to participants. 815 conversations were collected, and then the data was expanded by collecting 82,094 paraphrases using the crowdsourcing approach described by Jiang et al. (2017). Of this data, 500 conversations were used for training, 100 for development, and 100 for testing. The remaining 115 conversations were used as a source of negative candidates in the candidate sets. For the test data, 500 conversations were constructed by cutting the conversations off at 5 points and using paraphrases to make 5 distinct conversations. The training data was provided in two forms. First, the 500 training conversations with a list of paraphrases for each utterance, which participants could use in any way. Second, 100,000 partial conversations generated by randomly selecting para"
W19-4107,P19-3002,1,0.730139,"Missing"
