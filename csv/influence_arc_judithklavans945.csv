2020.lrec-1.879,W02-0603,0,0.495662,"els are inefficient in detecting the morpheme wa due to its high degree of ambiguity. For the analysis of the common segmentation phenomena seen in both Mexicanero and Nahuatl, see Eskander et al. (2019). 5. Related Work Unsupervised morphological segmentation was first performed by expensive manual rule engineering. An early use of machine learning for morphological segmentation was proposed by Goldsmith (2001) through the use of the Minimum Description Length (MDL) approach. The approach, however, requires some manual work that makes it challenging to generalize across languages. Morfessor (Creutz and Lagus, 2002), is a commonly used unsupervised and semi-supervised morphologicalsegmentation framework that utilizes the MDL principal, along with an HMM model, where the morphemes have a hierarchical structure. Another variation of Morfessor is Morfessor FlatCat (Gr¨onroos et al., 2014), which predicts both segmentation and morpheme categories. Log-linear models have proved successful for the problem of unsupervised morphological segmentation (Poon et al., 2009) with the use of global and contextual features. Another log-linear model is proposed by Narasimhan et al. (2015), where they arrange the words in"
2020.lrec-1.879,N09-1019,0,0.0971274,"Missing"
2020.lrec-1.879,C16-1086,1,0.299902,"hological segmentation has been an active research topic for decades as it is beneficial for many natural language processing tasks. With the high cost of manually labeling data for morphology and the increasing interest in low-resource languages, unsupervised morphological segmentation has become essential for processing a typologically diverse set of languages, whether high-resource or low-resource. In this paper, we present and release MorphAGram, a publicly available framework for unsupervised morphological segmentation that uses Adaptor Grammars (AG) and is based on the work presented by Eskander et al. (2016). We conduct an extensive quantitative and qualitative evaluation of this framework on 12 languages and show that the framework achieves state-of-the-art results across languages of different typologies (from fusional to polysynthetic and from high-resource to low-resource). Keywords: Unsupervised Morphological Segmentation Framework, Low-Resource Languages, Qualitative and Qualitative Evaluation, Adaptor Grammars, Language Typology 1. Introduction Many natural language processing tasks profit from morphological segmentation, for example machine translation (Nguyen et al., 2010; Ataman et al.,"
2020.lrec-1.879,W19-4222,1,0.726857,"nds the set of distributions over linguistic structures that can be characterized by a grammar, better matching the occurrences of trees and sub-trees observed in actual corpora. AGs define a framework to implement Bayesian nonparametric learning of grammars and are usually trained in an unsupervised manner using sampling techniques. AGs have been used successfully for unsupervised morphological segmentation, where a grammar is a morphological grammar that specifies word structure (Sirts 1 https://github.com/rnd2110/MorphAGram and Goldwater, 2013; Eskander et al., 2016; Eskander et al., 2018; Eskander et al., 2019). AGs have been also applied to other NLP applications such as word segmentation (Johnson, 2008a; Johnson, 2008c; Johnson and Demuth, 2010), named-entity clustering (Elsner et al., 2009), transliteration of names (Huang et al., 2011) and native-language identification (Wong et al., 2012). In this paper, we release MorphAGram, a publicly available framework for unsupervised morphological segmentation. The framework is also suitable for semi-supervised learning setups where it allows linguistic knowledge to be specified at two levels: designing the grammars and using scholar-seeded knowledge in"
2020.lrec-1.879,J01-2001,0,0.656015,"hile the AG-LI and AG-SS models tend to over-segment in Yorem Nokki, MorphoChain under-segments most of the time, which is why it cannot produce many of the short common morphemes. Finally, all the models are inefficient in detecting the morpheme wa due to its high degree of ambiguity. For the analysis of the common segmentation phenomena seen in both Mexicanero and Nahuatl, see Eskander et al. (2019). 5. Related Work Unsupervised morphological segmentation was first performed by expensive manual rule engineering. An early use of machine learning for morphological segmentation was proposed by Goldsmith (2001) through the use of the Minimum Description Length (MDL) approach. The approach, however, requires some manual work that makes it challenging to generalize across languages. Morfessor (Creutz and Lagus, 2002), is a commonly used unsupervised and semi-supervised morphologicalsegmentation framework that utilizes the MDL principal, along with an HMM model, where the morphemes have a hierarchical structure. Another variation of Morfessor is Morfessor FlatCat (Gr¨onroos et al., 2014), which predicts both segmentation and morpheme categories. Log-linear models have proved successful for the problem"
2020.lrec-1.879,C14-1111,0,0.0487354,"Missing"
2020.lrec-1.879,P11-2094,0,0.0282008,"c learning of grammars and are usually trained in an unsupervised manner using sampling techniques. AGs have been used successfully for unsupervised morphological segmentation, where a grammar is a morphological grammar that specifies word structure (Sirts 1 https://github.com/rnd2110/MorphAGram and Goldwater, 2013; Eskander et al., 2016; Eskander et al., 2018; Eskander et al., 2019). AGs have been also applied to other NLP applications such as word segmentation (Johnson, 2008a; Johnson, 2008c; Johnson and Demuth, 2010), named-entity clustering (Elsner et al., 2009), transliteration of names (Huang et al., 2011) and native-language identification (Wong et al., 2012). In this paper, we release MorphAGram, a publicly available framework for unsupervised morphological segmentation. The framework is also suitable for semi-supervised learning setups where it allows linguistic knowledge to be specified at two levels: designing the grammars and using scholar-seeded knowledge in terms of known affixes (Section 3). We conduct an extensive quantitative and qualitative evaluation of this framework (Section 4) for a set of 12 languages across a language typology continuum (Section 2), namely English, German, Fin"
2020.lrec-1.879,C10-1060,0,0.0222975,"and sub-trees observed in actual corpora. AGs define a framework to implement Bayesian nonparametric learning of grammars and are usually trained in an unsupervised manner using sampling techniques. AGs have been used successfully for unsupervised morphological segmentation, where a grammar is a morphological grammar that specifies word structure (Sirts 1 https://github.com/rnd2110/MorphAGram and Goldwater, 2013; Eskander et al., 2016; Eskander et al., 2018; Eskander et al., 2019). AGs have been also applied to other NLP applications such as word segmentation (Johnson, 2008a; Johnson, 2008c; Johnson and Demuth, 2010), named-entity clustering (Elsner et al., 2009), transliteration of names (Huang et al., 2011) and native-language identification (Wong et al., 2012). In this paper, we release MorphAGram, a publicly available framework for unsupervised morphological segmentation. The framework is also suitable for semi-supervised learning setups where it allows linguistic knowledge to be specified at two levels: designing the grammars and using scholar-seeded knowledge in terms of known affixes (Section 3). We conduct an extensive quantitative and qualitative evaluation of this framework (Section 4) for a set"
2020.lrec-1.879,W08-0704,0,0.638478,"atching the occurrences of trees and sub-trees observed in actual corpora. AGs define a framework to implement Bayesian nonparametric learning of grammars and are usually trained in an unsupervised manner using sampling techniques. AGs have been used successfully for unsupervised morphological segmentation, where a grammar is a morphological grammar that specifies word structure (Sirts 1 https://github.com/rnd2110/MorphAGram and Goldwater, 2013; Eskander et al., 2016; Eskander et al., 2018; Eskander et al., 2019). AGs have been also applied to other NLP applications such as word segmentation (Johnson, 2008a; Johnson, 2008c; Johnson and Demuth, 2010), named-entity clustering (Elsner et al., 2009), transliteration of names (Huang et al., 2011) and native-language identification (Wong et al., 2012). In this paper, we release MorphAGram, a publicly available framework for unsupervised morphological segmentation. The framework is also suitable for semi-supervised learning setups where it allows linguistic knowledge to be specified at two levels: designing the grammars and using scholar-seeded knowledge in terms of known affixes (Section 3). We conduct an extensive quantitative and qualitative evalua"
2020.lrec-1.879,P08-1046,0,0.173545,"atching the occurrences of trees and sub-trees observed in actual corpora. AGs define a framework to implement Bayesian nonparametric learning of grammars and are usually trained in an unsupervised manner using sampling techniques. AGs have been used successfully for unsupervised morphological segmentation, where a grammar is a morphological grammar that specifies word structure (Sirts 1 https://github.com/rnd2110/MorphAGram and Goldwater, 2013; Eskander et al., 2016; Eskander et al., 2018; Eskander et al., 2019). AGs have been also applied to other NLP applications such as word segmentation (Johnson, 2008a; Johnson, 2008c; Johnson and Demuth, 2010), named-entity clustering (Elsner et al., 2009), transliteration of names (Huang et al., 2011) and native-language identification (Wong et al., 2012). In this paper, we release MorphAGram, a publicly available framework for unsupervised morphological segmentation. The framework is also suitable for semi-supervised learning setups where it allows linguistic knowledge to be specified at two levels: designing the grammars and using scholar-seeded knowledge in terms of known affixes (Section 3). We conduct an extensive quantitative and qualitative evalua"
2020.lrec-1.879,N18-1005,0,0.283831,"Missing"
2020.lrec-1.879,W10-2211,0,0.027971,"d subtrees that correspond to the adapted nonterminals in addition to the segmentation output of the input vocabulary. Text segmentation can then be performed in two different modes; transductive and inductive. In the transductive mode, the word should be present in the vocabulary list provided to the learner, where the segmentation output serves as a segmentation lookup. In contrast, the inductive mode is suitable for words that were not processed by the learner, Evaluation and Results Data The data for English, German, Finnish and Turkish is from the Morpho Challenge competition 2 (MC2010) (Kurimo et al., 2010), where we select the most frequent 50,000 words for training after filtering out those words that have foreign letters. In addition, the development sets are collected from all the years of the competition, where we filter out the German words in which the stem receives transformation. The Estonian training and development sets are the ones used by Sirts and Goldwater (2013) 3 , where we filter out words containing foreign letters. The data is based on the Sega corpus 4 , where the gold segmentation in the development set is collected from the Estonian Morphologically Disambiguated Corpus 5 ."
2020.lrec-1.879,D14-1095,0,0.349557,"itative and qualitative evaluation of this framework on 12 languages and show that the framework achieves state-of-the-art results across languages of different typologies (from fusional to polysynthetic and from high-resource to low-resource). Keywords: Unsupervised Morphological Segmentation Framework, Low-Resource Languages, Qualitative and Qualitative Evaluation, Adaptor Grammars, Language Typology 1. Introduction Many natural language processing tasks profit from morphological segmentation, for example machine translation (Nguyen et al., 2010; Ataman et al., 2017) and speech recognition (Narasimhan et al., 2014). Many of the languages of the world are low-resource and/or endangered, where they lack adequate morphologically annotated resources. Thus, open-source unsupervised morphological segmentation frameworks could be an important resource for the computational linguistics community. In addition, we argue that frameworks that enable the use of linguistic knowledge to guide the learning process could be particularly beneficial when working on low-resource or endangered languages, where even unsegmentated data might be minimal. We present MorphAGram 1 , an publicly available framework for unsupervise"
2020.lrec-1.879,Q15-1012,0,0.141547,"rStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu2a+SM Sch. PrStSu2a+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Table 3: The best language-independent (standard/cascaded) setup (AG-LI) and the best scholarseeded setup (AG-SS) per language. Std.=Standard, Casc.=Cascaded, and Sch.=Scholar-Seeded We conduct the evaluation in a transductive learning scenario, where the unsegmented test words are included in our training set, which is common in the evaluation of unsupervised morphological segmentation (Poon et al., 2009; Sirts and Goldwater, 2013; Narasimhan et al., 2015; Eskander et al., 2016). However, we do not see gains in the performance when using the inductive learning approach instead, where the unsegmented test words are separate from the training set. We run the learners for 500 iterations for all languages, and we compute the results as the average of five runs since the samplers are non-deterministic. No annealing is used as it does not improve the results, and all parameters are automatically inferred. 4.3. Language Ave L(M) Ave M/W Max M/W Ambiguity English 5.30 2.39 6 0.48 German 5.16 2.94 8 0.43 5.74 3.48 9 0.53 Finnish Estonian 5.63 1.93 7 0."
2020.lrec-1.879,C10-1092,0,0.0406524,"presented by Eskander et al. (2016). We conduct an extensive quantitative and qualitative evaluation of this framework on 12 languages and show that the framework achieves state-of-the-art results across languages of different typologies (from fusional to polysynthetic and from high-resource to low-resource). Keywords: Unsupervised Morphological Segmentation Framework, Low-Resource Languages, Qualitative and Qualitative Evaluation, Adaptor Grammars, Language Typology 1. Introduction Many natural language processing tasks profit from morphological segmentation, for example machine translation (Nguyen et al., 2010; Ataman et al., 2017) and speech recognition (Narasimhan et al., 2014). Many of the languages of the world are low-resource and/or endangered, where they lack adequate morphologically annotated resources. Thus, open-source unsupervised morphological segmentation frameworks could be an important resource for the computational linguistics community. In addition, we argue that frameworks that enable the use of linguistic knowledge to guide the learning process could be particularly beneficial when working on low-resource or endangered languages, where even unsegmentated data might be minimal. We"
2020.lrec-1.879,N09-1024,0,0.674735,"n in Table 3. Best AG-SS Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu2a+SM Sch. PrStSu2a+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Table 3: The best language-independent (standard/cascaded) setup (AG-LI) and the best scholarseeded setup (AG-SS) per language. Std.=Standard, Casc.=Cascaded, and Sch.=Scholar-Seeded We conduct the evaluation in a transductive learning scenario, where the unsegmented test words are included in our training set, which is common in the evaluation of unsupervised morphological segmentation (Poon et al., 2009; Sirts and Goldwater, 2013; Narasimhan et al., 2015; Eskander et al., 2016). However, we do not see gains in the performance when using the inductive learning approach instead, where the unsegmented test words are separate from the training set. We run the learners for 500 iterations for all languages, and we compute the results as the average of five runs since the samplers are non-deterministic. No annealing is used as it does not improve the results, and all parameters are automatically inferred. 4.3. Language Ave L(M) Ave M/W Max M/W Ambiguity English 5.30 2.39 6 0.48 German 5.16 2.94 8 0"
2020.lrec-1.879,C10-1116,0,0.0240946,"PrStSu+SM Casc. PrStSu+SM Casc. PrStSu+SM Casc. PrStSu+SM Std. PrStSu+SM Std. PrStSu+SM Casc. PrStSu+SM Std. PrStSu+SM Std. PrStSu+SM Std. PrStSu+SM Std. PrStSu+SM Evaluation Metrics We evaluate the performance of our morphologicalsegmentation framework using two metrics: Boundary Precision and Recall (BPR) and EMMA-2 (Virpioja et al., 2011). BPR is the classical evaluation method for morphological segmentation, where the boundaries in the proposed segmentation are compared to the boundaries in the reference. In contrast, EMMA-2 is based on matching the morphemes, and is a variation of EMMA (Spiegler and Monson, 2010). In EMMA, each proposed morpheme is matched to each morpheme in the gold segmentation through one-to-one mappings. However, EMMA-2 allows for shorter computation times as it replaces the one-to-one assignment problem in EMMA by two many-to-one assignment problems, where two or more proposed morphemes can be mapped to one reference morpheme. EMMA-2 also results in higher precision and recall as it tolerates failing to join two allomorphs or to distinguish between identical syncretic morphemes. 4.4. Baselines We evaluate our system versus two state-of-the-art baselines: Morfessor (Creutz and La"
2020.lrec-1.879,C10-1115,0,0.0768193,"Missing"
2020.lrec-1.879,D12-1064,0,0.0119657,"upervised manner using sampling techniques. AGs have been used successfully for unsupervised morphological segmentation, where a grammar is a morphological grammar that specifies word structure (Sirts 1 https://github.com/rnd2110/MorphAGram and Goldwater, 2013; Eskander et al., 2016; Eskander et al., 2018; Eskander et al., 2019). AGs have been also applied to other NLP applications such as word segmentation (Johnson, 2008a; Johnson, 2008c; Johnson and Demuth, 2010), named-entity clustering (Elsner et al., 2009), transliteration of names (Huang et al., 2011) and native-language identification (Wong et al., 2012). In this paper, we release MorphAGram, a publicly available framework for unsupervised morphological segmentation. The framework is also suitable for semi-supervised learning setups where it allows linguistic knowledge to be specified at two levels: designing the grammars and using scholar-seeded knowledge in terms of known affixes (Section 3). We conduct an extensive quantitative and qualitative evaluation of this framework (Section 4) for a set of 12 languages across a language typology continuum (Section 2), namely English, German, Finnish, Estonian, Georgian, Turkish, Arabic, Zulu, Mexica"
2021.findings-acl.347,2020.lrec-1.879,1,0.907959,"2013; Narasimhan et al., 2014; Eskander et al., 2016, 2018, 2019). In this work, we show how linguistic priors effectively boost morphological-segmentation performance in a minimally-supervised manner that does not require segmented words for training. We integrate our priors within Adaptor Grammars (Johnson et al., 2007), a type of nonparametric Bayesian models that generalize Probabilistic Context-Free Grammars (PCFGs). Adaptor Grammars have proved successful for unsupervised morphological segmentation, achieving state-of-the-art results across a variety of typologically diverse languages (Eskander et al., 2020). We introduce two types of linguistic priors: 1) grammar definition, where we design a languagespecific grammar that is tailored for the language of interest by modeling specific morphological phenomena, and 2) linguist-provided affixes, where an expert in the underlying language compiles a list of carefully selected affixes and seeds it into the grammars prior to training the segmentation model. We use Japanese and Georgian as case studies for priors 1 and 2, respectively. As our goal is to develop a robust approach that benefits low-resource and/or endangered languages of high morphological"
2021.findings-acl.347,W19-4222,1,0.807706,"vised and 1 The training and evaluation datasets, linguistic priors and models for both Japanese and Georgian are available at https://github.com/rnd2110/MorphAGram/data. 2 https://github.com/rnd2110/MorphAGram 3969 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3969–3974 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: Language-independent PrStSu+SM grammar (left side) vs. its Japanese cognate (right side) minimally-supervised morphological segmentation, outperforming the competing discriminative models (Sirts and Goldwater, 2013; Eskander et al., 2019, 2020). Adaptor Grammars are non-parametric Bayesian models that are composed of two main components: 1) a Probabilistic Context-Free Grammar (PCFG) whose definition relies on the underlying task (in the case of morphological segmentation, a PCFG models word structure); and 2) an adaptor that is based on the Pitman-Yor process (Pitman, 1995). The adaptor keeps the posterior probability of a subtree proportional to the number of times that subtree is utilized to parse the input data and manages the caching of the subtrees. The learning process is Markov Chain Monte Carlo sampling (MCMC) (Andri"
2021.findings-acl.347,C16-1086,1,0.906633,"a Probabilistic Context-Free Grammar (PCFG) whose definition relies on the underlying task (in the case of morphological segmentation, a PCFG models word structure); and 2) an adaptor that is based on the Pitman-Yor process (Pitman, 1995). The adaptor keeps the posterior probability of a subtree proportional to the number of times that subtree is utilized to parse the input data and manages the caching of the subtrees. The learning process is Markov Chain Monte Carlo sampling (MCMC) (Andrieu et al., 2003) that does the inference of the PCFG probabilities and the hyperparameters of the model. Eskander et al. (2016) define a set of languageindependent grammars and three learning settings for Adaptor Grammars: 1) Standard, fully unsupervised; 2) Scholar-Seeded, minimally-supervised by manually seeding affixes into the grammar prior to training the segmentation model, and 3) Cascaded, fully unsupervised by approximating the ScholarSeeded setting using automatically generated affixes from an initial round of learning. We next present two ways of including linguistic priors in Adaptor Grammars: 1) defining a language-specific grammar; and 2) using linguist-provided affixes in the Scholar-Seeded learning setu"
2021.findings-acl.347,D14-1095,0,0.0201565,"Missing"
2021.findings-acl.347,N09-1024,0,0.0336939,"tation models for Japanese in the Standard (STD) and Cascaded (CAS)5 settings, both with generic and language-specific (LS) grammar definitions. For Georgian, we evaluate our morphologicalsegmentation models in the Standard (STD), Cascaded (CAS) and Scholar-Seeded (SS) settings, in addition to the proposed Scholar-Seeded setting with linguist-provided affixes (SS-Ling). We perform the evaluation in a transductive manner, where the unsegmented words in the gold standard are part of the training sets; this is common in evaluating unsupervised and minimally-supervised morphological segmentation (Poon et al., 2009; Sirts and Goldwater, 2013; Narasimhan et al., 2014; Eskander et al., 2016, 2019, 2020). For the metrics, we use Boundary Precision and Recall (BPR) and EMMA-2 (Virpioja et al., 2011). BPR is the classical metric for evaluating morphological segmentation; it compares the boundaries in the proposed segmentation to those in the reference. EMMA-2 5 For the Cascaded setup, we use the high-precision grammar PrStSu2a+SM defined by Eskander et al. (2016) as the base grammar. 3971 Language Japanese Georgian Setting Prec. BPR Recall F1-Score Prec. EMMA-2 Recall F1-Score Morfessor AG STD AG CAS AG STD-"
2021.findings-acl.347,E14-4017,0,0.0160353,"Missing"
2021.findings-acl.347,Q13-1021,0,0.0875462,"oved successful for unsupervised and 1 The training and evaluation datasets, linguistic priors and models for both Japanese and Georgian are available at https://github.com/rnd2110/MorphAGram/data. 2 https://github.com/rnd2110/MorphAGram 3969 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3969–3974 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: Language-independent PrStSu+SM grammar (left side) vs. its Japanese cognate (right side) minimally-supervised morphological segmentation, outperforming the competing discriminative models (Sirts and Goldwater, 2013; Eskander et al., 2019, 2020). Adaptor Grammars are non-parametric Bayesian models that are composed of two main components: 1) a Probabilistic Context-Free Grammar (PCFG) whose definition relies on the underlying task (in the case of morphological segmentation, a PCFG models word structure); and 2) an adaptor that is based on the Pitman-Yor process (Pitman, 1995). The adaptor keeps the posterior probability of a subtree proportional to the number of times that subtree is utilized to parse the input data and manages the caching of the subtrees. The learning process is Markov Chain Monte Carlo"
2021.findings-acl.347,W18-5808,1,0.876435,"Missing"
2021.findings-acl.347,J01-2001,0,0.713923,"Missing"
2021.findings-acl.347,N18-1005,0,0.04374,"Missing"
A00-1042,P96-1003,1,0.819209,"l, a head makes a greater contribution to the syntax and semantics of a phrase than does a modifier. This linguistic insight can be extended to the document level. If, as a practical matter, it is necessary to rank the contribution to a whole document made by the sequence of words constituting an NP, the head should be ranked more highly than other words in the phrase. This distinction is important in linguistic theory; for example, [Jackendoff 1977] discusses the relationship of heads and modifiers in phrase structure. It is also important in NLP, where, for example, [Strzalkowski 1997] and [Evans and Zhai 1996] have used the distinction between heads and modifiers to add query terms to information retrieval systems. Powerful corpus processing techniques have been developed to measure deviance from an average occurrence or co-occurrence in the corpus. In this paper we chose to evaluate methods that depend only on document-internal data, independent of corpus, domain or genre. We therefore did not use, for example, tf*idf, the purely statistical technique that is the used by most information retrieval systems, or [Smadja 1993], a hybrid statistical and symbolic technique for identifying collocations."
A00-1042,W99-0625,1,\N,Missing
A00-1042,J93-1007,0,\N,Missing
A00-1042,A97-1044,0,\N,Missing
A00-1042,A88-1019,0,\N,Missing
C88-2166,C86-1051,0,0.0438092,"Missing"
C88-2166,P86-1019,1,0.847391,"o n s The initial impetus for building a computational lexicon arose from the needs of the CRITIQUE text-critiquing system (previously called EPISTLE, Ileidom et al. 1982). Basic syntactic information such as part of speech, subcategorization for verbs (e.g. trans, intrans, complement taking properties), irregular forms, some inherent semantic information (such as male, female for nouns), some graphemic, phonological, and stylistic B15 features were gathered from a range of (primarily) maelfine-readable sources. This system (called UDICT, the ultimate dictionary) is described in Byrd 1983 and Byrd et al. 1986. A modified version of the original dictionary is still in use by that project. Our experience in attempting to build a solid broad-coverage computational lexicon revealed to us the range of projects potentially in need of such a lexical resource. Unfortunately, it also revealed to us a range of problems. First, the projects: we received requests for information from NIA&apos; projects such as the experimental English-toGerman machine translation system I,MT /McCord 1988/, the natural language data hase query project T Q A / D a m e r a u et al. 1982, Johnson 1984/, the kind-types Knowledge Repres"
C88-2166,J87-3003,1,0.817528,"Missing"
C88-2166,P85-1037,0,0.0124704,"prellminmy to be reported. 4.~. CONIPLEX S~nucture Tile previous sections of this paper have described the limitations of UDICT. With this in .tnind, this section gives the information to be eon~ rained in COMPLEX. Currently, we draw on the following sources: 3 1. enhanced UDICT ([mxSys) 2. Brandeis Verb Lexicon 3. defirdfions and grammatical information fi&apos;om I,DOCF,/ix)ngman 1978/, We have, plans to use inlbrmation from: 1. definitions, synonyms, and etymologies from Webster&apos;s Seventh/Mcrriam 1963/, 2. taxonomy files created from Webster&apos;s Seventh /Merriam 1963/ using techniques reported in Chodorow et al. 1985, 3. synonyms from the Collins Thesaurus /Collins 1984/, 4. Collins bilingual dictionaries for English/Italian, English/French, English/Spanisla, and English/German 5. text corpora We too arc using tile sense distinctions cm LDOCII&apos;;, although we are aware of its limitations. (See also Michiels 1982). Our system is not hard,, wired into I,I)OCE. Ccmsider the design fer one sense of the verb ""bring"": --Lcxicai Systems Analysis (MORPH(INFLECTION(PAST brought))) (PASTPART brought))) (PIION(AXNT)) (SYNTACTIC(CONSTRUCTION (MWESTART))) (INHERENT (INF))) (IRREG))) (NUMBER (PLUR))) (SUBCAT (DITRAN)))"
C88-2166,A88-1020,0,0.0274634,"Missing"
C88-2166,A88-1012,0,\N,Missing
C88-2166,J89-1003,0,\N,Missing
C90-3031,C88-1016,0,0.286625,"on construction, Catizone el: al. 1989 take two corresponding texts (English and German) and develop aigoritluns to deternffne lexical alignments by using statistical methods over texts combined with the optional support of an MRD. In contrast, Sadler 1989 proposes parsing aligned corpora into dependency trees, which form the structures upon which lexieal correspondences are suggested to the user. The early stages of the construction of the Bilingual Knowledge Base (BKB) rely heavily on human input but gradually becomes more automatic as data is collected. Using purely statistical techniques, Brown et al. 1988 make use of the Itansard bilingual corpus for the purpose of building a machine translation system. Such a system is a good example of using exclusively statistical non-linguistic methods to induce translations. drift, dance, commute, emigrate, immigrate, ascend, descend, circle, sail and glide. The probe string was used to search in CR; both for translations and collocations under the entry itself, and also for French headwords in the French side of the dictionary with the probe as a translation. The extracted corpora, consisting of the set of English citations containing the probe string (h"
C90-3031,P89-1010,0,0.0248766,"Missing"
C90-3031,C88-2166,1,0.846816,"ench words from the Canadian Parliamentary Proceedings (the ttansard corpus). Of this, 75 million French and 70 milfion I;nglish words are aligned by sentence (Brown ctal. 1988). For example: 1. Background. As NLP systems become more robust, large lexicons are required, providing a wide range of information including syntactic, semantic, pragmatic, naorphological and phonological. There are difficulties in constructing these large lexicons, first in their design, and then in providing them with the necessary and sufficient data. These problems have recently been the topic of intense research (Klavans 1988, Boguraev a n d Briscoe 1989, Boguraev et al. 1989, Zemick 1990). Moreover, an important sub-area of computational lexicon building that has barely been approached is that of bilingual lexicon construction (Caholari and Picchi 1986, Rizk 1989). SENTENCE ~: 3S7748 The a . ~ , a s s a d o r &apos; s c o n ~ r l b u ~ i o n w a s one s m a l l p a r l y a~ w h i c h a r&apos;mu~er o f us e n d e d u p d a n c i n g on a l a b l e . L &apos; a p p o r ~ de l &apos; a m b a s s a d e u r s &apos; e s ~ r e s u m e a une p e t i t e f;~e ou nous avons fini par danser sup une table. Figure One : Sample C i ~ a l i o n Some"
C90-3031,A88-1012,0,0.0276343,"translations and collocations under the entry itself, and also for French headwords in the French side of the dictionary with the probe as a translation. The extracted corpora, consisting of the set of English citations containing the probe string (ha any morphological shape) and the corresponding French sentence, is called a ""probe corpus"". A statistical tagger (Tzoukermann and Merialdo 1989) was used to assign a part of speech to the English side of the corpora. Translations and collocations were abstracted automatically from the parsed version of CR (see Neff and Boguraev 1989) using LQL (Neff et al. 1988). For illustration, a partial entry for dance is: +-bdw: dance +-superhom +-Im~og~aph 5. The B I C O R D System - Bilingual Corpus-based Dictionary. Our approach involves a combination of standard linguistic methodology using MRD&apos;s, enhanced with some statistical techniques. Dictionaries are often discounted because they are built on basis of introspective intuition rather than purely :t +-homr.Jm: 2 +-pos : v~ +-Iransla~ I +-argxJlm~r1~:: leal~z e~c I +-+*ord: danser J .o, For example, the A C L Data Collection Initiative ( A C L / D C I ) coordinated by Dr. Mark l.iberman at A.T.& T. Bell La"
C90-3031,P89-1012,0,0.0267667,"was used to search in CR; both for translations and collocations under the entry itself, and also for French headwords in the French side of the dictionary with the probe as a translation. The extracted corpora, consisting of the set of English citations containing the probe string (ha any morphological shape) and the corresponding French sentence, is called a ""probe corpus"". A statistical tagger (Tzoukermann and Merialdo 1989) was used to assign a part of speech to the English side of the corpora. Translations and collocations were abstracted automatically from the parsed version of CR (see Neff and Boguraev 1989) using LQL (Neff et al. 1988). For illustration, a partial entry for dance is: +-bdw: dance +-superhom +-Im~og~aph 5. The B I C O R D System - Bilingual Corpus-based Dictionary. Our approach involves a combination of standard linguistic methodology using MRD&apos;s, enhanced with some statistical techniques. Dictionaries are often discounted because they are built on basis of introspective intuition rather than purely :t +-homr.Jm: 2 +-pos : v~ +-Iransla~ I +-argxJlm~r1~:: leal~z e~c I +-+*ord: danser J .o, For example, the A C L Data Collection Initiative ( A C L / D C I ) coordinated by Dr. Mark"
C90-3031,J90-1003,0,\N,Missing
C90-3031,W89-0240,0,\N,Missing
C90-3031,H89-2012,0,\N,Missing
C92-4177,J88-2003,0,0.807507,"Missing"
C92-4177,J80-1003,0,\N,Missing
C94-2156,C90-3031,1,0.897079,"nd accuracy of a speech system. Since a dictionary entry consists of several fields of information, naturally, each will bc userid for different applications [1]. Among the standard fields are prommciation, etymology, subjcct field notes, definition fields, synonym and antonym cross references, semantic and syntactic comments, run-on forms, conjugational class and inflectional information where relevant, and translation for the I)ilingual dictionaries. Each of these fields has proven usefifl for different applications, such as for building semantic taxouomies [3], [13] and machine translation [12]. The most directly useflfl for TTS is the pronunciation field [4], [11]. Equally usefifl for TTS, but less dir6ctly acces971 sible, are data from run-on fields, conjugational class information, and part-of-speech. 1 To illustrate, the following partial entries from Webster&apos;s Seventh (W7) [15] illustrate typical pronunciation, definition, and run-on fields: (l) (2) (3) (4) h a . y e n / &apos; h . ~ - v 0 n / n 1: IIAnBOR, POLO&apos; 2 : a place of safety : ASYLUM h a v e n vt bi.son/&apos;brs-on, &apos;t&gt;iz-/ n ... ho.m,,.ge.neous /-&apos;j~-ne-0s,-ny0s/ ... den.tic.u.late/den-&apos;tik-y0-1~t/ or d e n . t i c . u . l a"
C94-2156,P85-1037,0,0.0768069,"significantly impact efficiency and accuracy of a speech system. Since a dictionary entry consists of several fields of information, naturally, each will bc userid for different applications [1]. Among the standard fields are prommciation, etymology, subjcct field notes, definition fields, synonym and antonym cross references, semantic and syntactic comments, run-on forms, conjugational class and inflectional information where relevant, and translation for the I)ilingual dictionaries. Each of these fields has proven usefifl for different applications, such as for building semantic taxouomies [3], [13] and machine translation [12]. The most directly useflfl for TTS is the pronunciation field [4], [11]. Equally usefifl for TTS, but less dir6ctly acces971 sible, are data from run-on fields, conjugational class information, and part-of-speech. 1 To illustrate, the following partial entries from Webster&apos;s Seventh (W7) [15] illustrate typical pronunciation, definition, and run-on fields: (l) (2) (3) (4) h a . y e n / &apos; h . ~ - v 0 n / n 1: IIAnBOR, POLO&apos; 2 : a place of safety : ASYLUM h a v e n vt bi.son/&apos;brs-on, &apos;t&gt;iz-/ n ... ho.m,,.ge.neous /-&apos;j~-ne-0s,-ny0s/ ... den.tic.u.late/den-&apos;tik-"
C94-2156,P89-1012,0,0.0670154,"rm &quot;denticulated&quot; in tile early part of the entry, and the pronunciation of t h a t run-on is related to the main entry, but the user must decide how to strip and append the given syllables. 2 While these types of reasoning are not difficult for humans, for whom the dictionary was written, they are quite difficult for programs, and thus are not straighforward to perform automatically. 2.1 Using the MRD pronunciation field Extracting the prommciation field from an MRD is one of the most obvious uses of a dictionary. Nevertheless, parsing dictionaries in general can be a very complex operation ([16]) and even the extraction of one field, such as prommciation, can pose problems. Similar to W7, in the Robert French dictionary [9], which contains a b o u t 89,000 entries, several pronunciations can be given for a headword and the choice of one must be made. Moreover, because of the rich morphology of French 1Notice, however, that the fifll Collins Spanish-English dictionary [7], as opposed to the other bilinguals, does not contain any prommciatlon information. Although this is rather surprising taking into account that the smaller versi. . . . . . h as the paperback and g.... ([8], [lO]) do"
C94-2156,C90-3049,1,0.781021,"ixes and suffixes which are listed in the dictionary without pronunciations, and which should not be used in isolation in arty 3 Methodology and Results ease. 3.1 2.2 Using the MRD for morphology Even though an MRI) may not list complete intlectional paradigms, it contains useful inflectional information. For example in the Collins SpanishEnglish dictionary, verb entries are listed with an index pointing to the conjugation chess and table, listed at the end of the dictionary. Using this i n f e r mation, a finite-state transducer for morphological analysis and generation was built for Spanish [20]. From the original list of over 50,000 words, a few million words have been generated. These forms can then be used as tile input to the graphemeto-phoneme conversion module, in ;t Spanish TTS system. 2.3 Using Run-on&apos;s A run-on is defined as a morphological variant of a headword, included in the entry. Run-on&apos;s are problematic data in MRI)s [16], and they can be found nearly anywhere in the entry. In example (4), the run-on occurs at the beginning of the entry, and consists of a fitll form with suffix. More commonly, run-on&apos;s occur towards the end of the entry, and tend to consist of predict"
C98-1108,W97-0703,0,0.0180665,"ormation extraction systems. For example, given the noun phrases fares and US Air that occur within a particular article, the reader will know what the story is about, i.e. fares and US Air. However, the reader will not know the [EVENT], i.e. what happened to the fares or to US Air. Did airfare prices rise, fall or stabilize? These are the verbs most typically applicable to prices, and which embody the event. 1.1 Focus on t h e N o u n Many natural language analysis systems focus on nouns and noun phrases in order to identify information on who, what, and where. For example, in summarization, Barzilay and Elhadad (1997) and Lin and Hovy (1997) focus oll multiword noun phrases. For information extraction tasks, such as the DARPA-sponsored Message Understanding Conferences (1992), only a few projects use verb phrases (events), e.g. Appelt et al. (1993), Lin (1993). In contrast, the named entity task, which identifies nouns and noun phrases, has generated numerous projects as evidenced by a host of papers in recent conferences, (e.g. Wacholder et al. 1997, Pahner and Day 1997, Neumann et al. 1997). Although rich infbrmation on nominal participants, actors, and other entities is provided, the nmned entity task p"
C98-1108,P94-1002,0,0.0227587,"Missing"
C98-1108,C94-2174,0,0.0144944,"a breaking news article would feature a higher percentage of motion verbs rather than verbs of commmfication. 1.3 On Genre Detection Verbs are an important factor in providing an event profile, which in turn might be used in categorizing articles into different genres, f l e m i n g to the literature in genre classification, Biber (1989) outlines five dimensions which can be used to characterize genre. Properties for distinguishing dimensions include verbal features such as tense, agentless passives and infinitives. Biber also refers to three verb classes: private, public, and suasive verbs. Karlgren and Cutting (1994) take a comtmtationally tractable set of these properties and use them to compute a score to recognize text genre using discriminant analysis. Tlm only verbal feature used in their study is present-tense verb count. As Karlgren and Cutting show, their techniques are effective in genre categorization, but, they do not claim to show how genres differ. Kessler et al. (1997) discuss some of the complexities in automatic detection of genre using a set of computationally efficient cues, such as punctuation, abbreviations, or presence of Latinate suffixes. The taxonomy of genres and facets developed"
C98-1108,P97-1005,0,0.07283,"used to characterize genre. Properties for distinguishing dimensions include verbal features such as tense, agentless passives and infinitives. Biber also refers to three verb classes: private, public, and suasive verbs. Karlgren and Cutting (1994) take a comtmtationally tractable set of these properties and use them to compute a score to recognize text genre using discriminant analysis. Tlm only verbal feature used in their study is present-tense verb count. As Karlgren and Cutting show, their techniques are effective in genre categorization, but, they do not claim to show how genres differ. Kessler et al. (1997) discuss some of the complexities in automatic detection of genre using a set of computationally efficient cues, such as punctuation, abbreviations, or presence of Latinate suffixes. The taxonomy of genres and facets developed in Kessler et al. is useful for a wide range of types, such as found in the Brown corpus. Although some of their discriminators could be useful for news articles (e.g. presence of second person pronoun tends to indicate a letter to the editor), the indicators do not appear to be directly applicable to a finer classification of&apos; news articles. News articles can be divided"
C98-1108,A97-1042,0,0.0142071,"example, given the noun phrases fares and US Air that occur within a particular article, the reader will know what the story is about, i.e. fares and US Air. However, the reader will not know the [EVENT], i.e. what happened to the fares or to US Air. Did airfare prices rise, fall or stabilize? These are the verbs most typically applicable to prices, and which embody the event. 1.1 Focus on t h e N o u n Many natural language analysis systems focus on nouns and noun phrases in order to identify information on who, what, and where. For example, in summarization, Barzilay and Elhadad (1997) and Lin and Hovy (1997) focus oll multiword noun phrases. For information extraction tasks, such as the DARPA-sponsored Message Understanding Conferences (1992), only a few projects use verb phrases (events), e.g. Appelt et al. (1993), Lin (1993). In contrast, the named entity task, which identifies nouns and noun phrases, has generated numerous projects as evidenced by a host of papers in recent conferences, (e.g. Wacholder et al. 1997, Pahner and Day 1997, Neumann et al. 1997). Although rich infbrmation on nominal participants, actors, and other entities is provided, the nmned entity task provides no information o"
C98-1108,M93-1022,0,0.0134553,"fares or to US Air. Did airfare prices rise, fall or stabilize? These are the verbs most typically applicable to prices, and which embody the event. 1.1 Focus on t h e N o u n Many natural language analysis systems focus on nouns and noun phrases in order to identify information on who, what, and where. For example, in summarization, Barzilay and Elhadad (1997) and Lin and Hovy (1997) focus oll multiword noun phrases. For information extraction tasks, such as the DARPA-sponsored Message Understanding Conferences (1992), only a few projects use verb phrases (events), e.g. Appelt et al. (1993), Lin (1993). In contrast, the named entity task, which identifies nouns and noun phrases, has generated numerous projects as evidenced by a host of papers in recent conferences, (e.g. Wacholder et al. 1997, Pahner and Day 1997, Neumann et al. 1997). Although rich infbrmation on nominal participants, actors, and other entities is provided, the nmned entity task provides no information on w h a t h a p p e n e d in the document, i.e. the e v e n t or a c t i o n . Less progress has been made on ways to utilize verbal information efficiently. In earlier systems with stemming, many of the verbal and nonfinal"
C98-1108,J91-1002,0,0.0246762,"n document type discrimination; we show how article types can be successfully classified within the news domain using verb semantic classes. 2 Verb T y p e communication support remainder and EVCA Since our first intuition of the data suggested that articles with a preponderance of verbs of 682 a certain semantic type might reveal aspects of document type, we tested the hypothesis that verbs could be used as a predictor in providing an event profile. We developed two algo~ rithms to: (1) explore WordNet (WN-Yerber) to cluster related verbs and build a set of verb chains in a document, much as Morris and Hirst (1991) used Roget&apos;s Thesaurus or like Hirst and St. Onge (1998) used WordNet to build noun chains; (2) classify verbs according to a semantic classification system, in this case, using Levin&apos;s (1993) English Verb Classes and Alternations (EVCA-Verber) as a basis. For source material, we used the manually-parsed Linguistic Data Consortium&apos;s Wall Street Journal (WSJ) corpus from which we extracted main and complement of communication verbs to test the algorithms on. Using WordNet. Our first technique was to use WordNet to build links between verbs and to provide a semantic profile of the document. Wor"
C98-1108,A97-1031,0,0.0126875,"ns and noun phrases in order to identify information on who, what, and where. For example, in summarization, Barzilay and Elhadad (1997) and Lin and Hovy (1997) focus oll multiword noun phrases. For information extraction tasks, such as the DARPA-sponsored Message Understanding Conferences (1992), only a few projects use verb phrases (events), e.g. Appelt et al. (1993), Lin (1993). In contrast, the named entity task, which identifies nouns and noun phrases, has generated numerous projects as evidenced by a host of papers in recent conferences, (e.g. Wacholder et al. 1997, Pahner and Day 1997, Neumann et al. 1997). Although rich infbrmation on nominal participants, actors, and other entities is provided, the nmned entity task provides no information on w h a t h a p p e n e d in the document, i.e. the e v e n t or a c t i o n . Less progress has been made on ways to utilize verbal information efficiently. In earlier systems with stemming, many of the verbal and nonfinal forms were conflated, sometimes erroneously. With the development of more sophisticated tools, such as part of speech taggers, more accurate verb phrase identification is possible. We present in this paper an effective way to utilize ve"
C98-1108,A97-1028,0,0.0387044,"Missing"
C98-1108,A97-1030,0,0.00771795,"tural language analysis systems focus on nouns and noun phrases in order to identify information on who, what, and where. For example, in summarization, Barzilay and Elhadad (1997) and Lin and Hovy (1997) focus oll multiword noun phrases. For information extraction tasks, such as the DARPA-sponsored Message Understanding Conferences (1992), only a few projects use verb phrases (events), e.g. Appelt et al. (1993), Lin (1993). In contrast, the named entity task, which identifies nouns and noun phrases, has generated numerous projects as evidenced by a host of papers in recent conferences, (e.g. Wacholder et al. 1997, Pahner and Day 1997, Neumann et al. 1997). Although rich infbrmation on nominal participants, actors, and other entities is provided, the nmned entity task provides no information on w h a t h a p p e n e d in the document, i.e. the e v e n t or a c t i o n . Less progress has been made on ways to utilize verbal information efficiently. In earlier systems with stemming, many of the verbal and nonfinal forms were conflated, sometimes erroneously. With the development of more sophisticated tools, such as part of speech taggers, more accurate verb phrase identification is possible. We present i"
C98-1108,W98-1123,1,\N,Missing
C98-1108,H94-1020,0,\N,Missing
J87-3003,P85-1034,1,0.853071,"Missing"
J87-3003,P86-1019,1,0.893026,"Missing"
J87-3003,P84-1036,1,0.60533,"Missing"
J87-3003,P84-1095,1,0.870618,"Missing"
J87-3003,P85-1037,1,0.785797,"ist of one or more &quot; g e n u s &quot; terms, which identify superordinate categories of the defined word, and &quot;differentia&quot; which distinguish this instance of the superordinate categories from other instances. By manually extracting and disambiguating genus terms for a pocket dictionary, Amsler demonstrated the feasibility of generating semantic hierarchies. It has been our goal to automate the genus extraction and disambiguation processes so that hierarchies can be generated from full-sized dictionaries, such as Webster's Seventh New Collegiate Dictionary. These efforts are described in detail in Chodorow, et al. (1985). They begin with Head Finding. In the definition of car as &quot; a vehicle moving on wheels&quot;, the word vehicle serves as the genus term, while &quot;moving on wheels&quot; differentiates cars from some other types of vehicles. Taken as a group, all of the word/genus pairs contained in a normal dictionary for words o f a given part-of-speech form what Amsler(1980) calls a &quot;tangled hierarchy&quot;. In this hierarchy, each word constitutes a node whose subordinate Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. N"
J87-3003,C86-1051,0,0.0305603,"Missing"
J87-3003,J83-3002,0,0.0390277,"Missing"
J87-3003,P86-1018,0,0.0795249,"ord search technique turned up 527 more candidate words. Subtracting the 153 words c o m m o n to both lists, there remained 844 words to be checked. Half of these passed the criteria in all senses, so a total of 422 words are marked with the [+unit] feature. In addition, a subset of unit nouns (n=128) were marked with a feature [+currency], of which 26 have irregular plural forms, yielding a total of 154 tokens marked. This was an unexpected but useful result particularly for judging the grammaticality of constructions like: Rice is four dollars a pound. 4 . 1 . 3 . ACTIVE AND STATIVE VERBS. Markowitz et al. (1986) present linguistic motivations for extracting an &quot; a c t i v e &quot; and &quot; s t a t i v e &quot; feature for verbs, and suggest methods for using MRD's as sources for finding active/stative information for verbs. We decided to try to carry out their suggestions using our analysis tools and our morphological analyzer on our dictionary resources. Our attempt was both a success 232 Tools and Methods and a failure. The success arises from the design and use of our tools. We achieved clear results, although the results were not what we had expected. The disappointment arises from the fact that the active/st"
J87-3003,A88-1012,1,0.901385,"Missing"
kan-etal-2002-using,J98-3005,1,\N,Missing
kan-etal-2002-using,P96-1025,0,\N,Missing
klavans-etal-2000-evaluation,J93-2004,0,\N,Missing
klavans-etal-2000-evaluation,A97-1044,0,\N,Missing
klavans-etal-2000-evaluation,A88-1019,0,\N,Missing
klavans-etal-2000-evaluation,P96-1003,1,\N,Missing
muresan-klavans-2002-method,W01-0513,0,\N,Missing
muresan-klavans-2002-method,A00-2018,0,\N,Missing
muresan-klavans-2002-method,J93-1007,0,\N,Missing
muresan-klavans-2002-method,J87-3003,1,\N,Missing
N04-3001,P99-1044,0,0.0343031,"to English text are replaced (or augmented with) the similar English sentence. This first system using full machine translation over the sentences and English similarity detection will be extended using simple features for multilingual similarity detection in SimFinder MultiLingual (SimFinderML), a multilingual version of SimFinder (Hatzivassiloglou et al., 2001). We also plan an experiment evaluating the usefulness of noun phrase detection and noun phrase variant detection as a primitive for multilingual similarity detection, using tools such as Christian Jacquemin’s FASTR (Jacquemin, 1994; Jacquemin, 1999). 4.2 Summary presentation Multilingual Newsblaster presents multiple views of a cluster of documents to the user, broken down by language and by country. Summaries are generated for the entire cluster, as well as sub-sets of the articles based on the country of origin and language of the original articles. Users are first presented with a summary of the entire cluster using all documents, and then have the ability to focus on countries or languages of their choosing. We also allow the user to view two summaries side-by-side so they can easily compare differences between summaries from differe"
N04-3001,1999.mtsummit-1.45,0,0.0216909,"iments. The phases in the multilingual version of Columbia Newsblaster have been modified to take language and character encoding into account, and a new phase, translation, has been added. Figure 1 depicts the multilingual Columbia Newsblaster architecture. We will describe the system, in particular a method using machine learning to extract article text from web pages that is applicable to different languages, and a baseline approach to multilingual multi-document summarization. 1.1 Related Research Previous work in multilingual document summarization, such as the SUMMARIST system (Hovy and Lin, 1999) 1 http://newsblaster.cs.columbia.edu/ Figure 1: Architecture of the multilingual Columbia Newsblaster system. extracts sentences from documents in a variety of languages, and translates the resulting summary. This system has been applied to Information Retrieval in the MuST System (Lin, 1999) which uses query translation to allow a user to search for documents in a variety of languages, summarize the documents using SUMMARIST, and translate the summary. The Keizei system (Ogden et al., 1999) uses query translation to allow users to search Japanese and Korean documents in English, and displays"
N04-3001,N03-2024,1,0.738908,"o one of two multidocument summarization systems based on the similarity of the documents in the cluster. If the documents are highly similar, the Multigen summarization system (McKeown et al., 1999) is used. Multigen clusters sentences based on similarity, and then parses and fuses information from similar sentences to form a summary. The second summarization system used is DEMS, the Dissimilarity Engine for Multi-document Summarization (Schiffman et al., 2002), which uses a sentence extraction approach to summarization. The resulting summary is then run through a named entity recovery tool (Nenkova and McKeown, 2003), which repairs named entity references in the summary by making the first reference descriptive, and shortening subsequent reference mentions in the summary. Using an unmodified version of DEMS, summaries might contain sentences from translated documents which are not grammatically correct. The DEMS summarization system was modified to prefer choosing a sentence from an English article if there are sentences that express similar content in multiple languages. By setting different weight penalties we can take the quality of the translation system for a given language pair into Figure 2: A scre"
N04-3001,W97-0704,0,\N,Missing
P86-1019,P85-1037,1,0.813091,"Missing"
P86-1019,P84-1038,0,\N,Missing
P86-1019,W83-0114,0,\N,Missing
P97-1004,E95-1014,0,0.0137872,"verage of multi-word terms for indexing and retrieval. Final results are evaluated for precision and recall, and implications for indexing and retrieval are discussed. 1 2 Background and Introduction NLP techniques have been applied to extraction of information from corpora for tasks such as free indexing (extraction of descriptors from corpora), (Metzler and Haas, 1989; Schwarz, 1990; Sheridan and Smeaton, 1992; Strzalkowski, 1996), term acquisition (Smadja and McKeown, 1991; Bourigault, 1993; Justeson and Katz, 1995; Dallle, 1996), or extraction of lin9uistic information e.g. support verbs (Grefenstette and Teufel, 1995), and event structure of verbs (Klavans and Chodorow, 1992). Although useful, these approaches suffer from two weaknesses which we address. First is the issue of filtering term lists; this has been dealt with by constraints on processing and by post-processing overgenerated lists. Second is the problem of difficulties in identifying related terms across parts of speech. We address these limitations through the use of controlled indexing, that is, indexing with reference to previously available authoritative terms lists, such as (NLM, 1995). Our approach is fully automatic, but permits effectiv"
P97-1004,A94-1019,1,0.839638,"netic, genome, genotoxic, genetically, etc. Second, the term list is dynamically expanded through syntactic transformations which allow the retrieval of term variants. For example, genic expressions, genes were expressed, expression of this gene, etc. are extracted as variants of gene expression. This system relies on a full-fledged unification formalism and thus is well adapted to a fine-grained identification of terms related in syntactically and morphologically complex ways. The same system has been effectively applied both to English and French, although this paper focuses on French (see (Jacquemin, 1994) for the case of syntactic variants in English). All evaluation experiments were performed on two corpora: a training corpus [ECI] (ECI, 1989 and 1990) used for the tuning of the metagrammar and a test corpus [AGR] (AGR, 1995) used for evaluation. [ECI] is a subset of the European Corpus Initiative data composed of 1.3 million words of the French newspaper ""Le Monde""; [AGR] is a set of abstracts of scientific papers in the agricultural domain from INIST/CNRS (1.1 million words). A list of terms is associated with each corpus: the terms corresponding to [ECI] were automatically extracted by LEX"
P97-1004,E93-1011,0,0.528203,"the successful combination of parsing over a seed term list coupled with derivational morphology to achieve greater coverage of multi-word terms for indexing and retrieval. Final results are evaluated for precision and recall, and implications for indexing and retrieval are discussed. 1 2 Background and Introduction NLP techniques have been applied to extraction of information from corpora for tasks such as free indexing (extraction of descriptors from corpora), (Metzler and Haas, 1989; Schwarz, 1990; Sheridan and Smeaton, 1992; Strzalkowski, 1996), term acquisition (Smadja and McKeown, 1991; Bourigault, 1993; Justeson and Katz, 1995; Dallle, 1996), or extraction of lin9uistic information e.g. support verbs (Grefenstette and Teufel, 1995), and event structure of verbs (Klavans and Chodorow, 1992). Although useful, these approaches suffer from two weaknesses which we address. First is the issue of filtering term lists; this has been dealt with by constraints on processing and by post-processing overgenerated lists. Second is the problem of difficulties in identifying related terms across parts of speech. We address these limitations through the use of controlled indexing, that is, indexing with ref"
P97-1004,C92-4177,1,0.814823,"Missing"
P97-1004,C90-3049,1,0.788249,"ral disambiguation over internal phrase structure, these important syntactic distinctions would be incorrectly overlooked. 4 Part of Speech Disambiguation and Morphology The morphological analysis performed in this study is detailed in (Tzoukermann, Klavans, and Jacquemin, 1997). It is more complete and linguistically more accurate than simple stemming for the following reasons: First, i n f l e c t i o n a l m o r p h o l o g y is performed in order to get the different analyses of word forms. Inflectional morphology is implemented with finite-state transducers on the model used for Spanish (Tzoukermann and Liberman, 1990). The theoretical principles underlying this approach are based on generative morphology (Aronoff, 1976; Selkirk, 1982). The system consists of precomputing stems, extracted from a large dictionary of French (Boyer, 1993) enhanced with newspaper corpora, a total of over 85,000 entries. • Allomorphy is accounted for by listing the set of its possible allomorphs for each word. A1lomorphies are obtained through multiple verb stems, e.g. ]abriqu-, ]abric- (fabricate) or additional allomorphic rules. • Concatenation of several suffixes is accounted for by rule ordering mechanisms. Furthermore, we h"
P97-1004,W96-0101,1,0.80118,"set of its possible allomorphs for each word. A1lomorphies are obtained through multiple verb stems, e.g. ]abriqu-, ]abric- (fabricate) or additional allomorphic rules. • Concatenation of several suffixes is accounted for by rule ordering mechanisms. Furthermore, we have devised a method for guessing possible suffix combinations from a lexicon and a corpus. This empirical method reported in (Jacquemin, 1997) ensures that suffixes which are related within specific domains are considered. Second, a f i n i t e - s t a t e p a r t o f s p e e c h t a g g e r (Tzoukermann, Radev, and Gale, 1995; Tzoukermann and Radev, 1996) performs the morphosyntactic disambiguation of words. The tagger takes the output of inflectional morphological analysis and through a combination of linguistic and statistical techniques, outputs a unique part of speech for each word in context. Reducing the ambiguity of part of speech tags eliminates ambiguity in local parsing. Furthermore, part of speech ambiguity resolution permits construction of correct derivational links. • Derivational morphology is built with the perspective of overgeneration. The nature of the semantic links between a word and its derivational forms is not checked a"
P98-1112,W97-0703,0,0.0422981,"Missing"
P98-1112,P94-1002,0,0.0244337,"Missing"
P98-1112,C94-2174,0,0.0153799,"sults, a breaking news article would feature a higher percentage of motion verbs rather than verbs of communication. 1.3 On Genre Detection Verbs are an important factor in providing an event profile, which in turn might be used in categorizing articles into different genres. Turning to the literature in genre classification, Biber (1989) outlines five dimensions which can be used to characterize genre. Properties for distinguishing dimensions include verbal features such as tense, agentless passives and infinitives. Biber also refers to three verb classes: private, public, and suasive verbs. Karlgren and Cutting (1994) take a computationally tractable set of these properties and use them to compute a score to recognize text genre using discriminant analysis. The only verbal feature used in their study is present-tense verb count. As Karlgren and Cutting show, their techniques are effective in genre categorization, but they do not claim to show how genres differ. Kessler et al. (1997) discuss some of the complexities in automatic detection of genre using a set of computationally efficient cues, such as punctuation, abbreviations, or presence of Latinate suffixes. The taxonomy of genres and facets developed i"
P98-1112,P97-1005,0,0.081528,"used to characterize genre. Properties for distinguishing dimensions include verbal features such as tense, agentless passives and infinitives. Biber also refers to three verb classes: private, public, and suasive verbs. Karlgren and Cutting (1994) take a computationally tractable set of these properties and use them to compute a score to recognize text genre using discriminant analysis. The only verbal feature used in their study is present-tense verb count. As Karlgren and Cutting show, their techniques are effective in genre categorization, but they do not claim to show how genres differ. Kessler et al. (1997) discuss some of the complexities in automatic detection of genre using a set of computationally efficient cues, such as punctuation, abbreviations, or presence of Latinate suffixes. The taxonomy of genres and facets developed in Kessler et al. is useful for a wide range of types, such as found in the Brown corpus. Although some of their discriminators could be useful for news articles (e.g. presence of second person pronoun tends to indicate a letter to the editor), the indicators do not appear to be directly applicable to a finer classification of news articles. News articles can be divided"
P98-1112,A97-1042,0,0.0254018,"Missing"
P98-1112,M93-1022,0,0.0332853,"Missing"
P98-1112,J91-1002,0,0.0217012,"in document type discrimination; we show how article types can be successfully classified within the news domain using verb semantic classes. 2 Verb T y p e communication support remainder and EVCA Since our first intuition of the data suggested that articles with a preponderance of verbs of 682 a certain semantic type might reveal aspects of document type, we tested the hypothesis that verbs could be used as a predictor in providing an event profile. We developed two algorithms to: (1) explore WordNet (WN-Verber) to cluster related verbs and build a set of verb chains in a document, much as Morris and Hirst (1991) used Roget's Thesaurus or like Hirst and St. Onge (1998) used WordNet to build noun chains; (2) classify verbs according to a semantic classification system, in this case, using Levin's (1993) English Verb Classes and Alternations (EVCA-Yerber) as a basis. For source material, we used the manually-parsed Linguistic Data Consortium's Wall Street Journal (WSJ) corpus from which we extracted main and complement of communication verbs to test the algorithms on. Using WordNet. Our first technique was to use WordNet to build links between verbs and to provide a semantic profile of the document. Wor"
P98-1112,A97-1031,0,0.0174454,"Missing"
P98-1112,A97-1028,0,0.0374578,"Missing"
P98-1112,A97-1030,0,0.0180049,"Missing"
P98-1112,W98-1123,1,\N,Missing
P98-1112,H94-1020,0,\N,Missing
passonneau-etal-2006-climb,A97-1054,0,\N,Missing
passonneau-etal-2006-climb,A97-1051,0,\N,Missing
passonneau-etal-2008-relation,passonneau-etal-2006-inter,1,\N,Missing
passonneau-etal-2008-relation,passonneau-2006-measuring,1,\N,Missing
passonneau-etal-2008-relation,J02-4002,0,\N,Missing
passonneau-etal-2008-relation,L06-1000,0,\N,Missing
W01-0719,W95-0107,0,0.0212699,"ses, verb phrases, and prepositional phrases, we assume that noun phrases (NPs) carry the most contentful information about the document, a well-supported hypothesis (Smeaton, 1999; Wacholder, 1998). As considered by Wacholder (1998), the simple NPs are the maximal NPs that contain premodifiers but not post-nominal constituents such as prepositions or clauses. We chose simple NPs for content representation because they are semantically and syntactically coherent and they are less ambiguous than complex NPs. For extracting simple noun phrases we first used Ramshaw and Marcus’s base NP chunker (Ramshaw and Marcus, 1995). The base NP is either a simple NP or a coordination of simple NPs. We used heuristics based on POS tags to automatically split the coordinate NPs into simple ones, properly assigning the premodifiers. Table 1 presents some coordinate NPs (CNP) encountered in our data collection and the results of our algorithm which split them into simple NPs (SNP1 and SNP2). 2.2 Features used for Classification The choice of features used to represent the candidate phrases has a strong impact on the accuracy of the classifiers (e.g. the number of examples needed to obtain a given accuracy on the test data,"
W01-0719,W98-0610,0,0.292856,"es/NNS scientific/JJ articles/NNS technical/JJ articles/NNS scientific/JJ thesauri/NNS and databases/NNS scientific/JJ thesauri/NNS scientific/JJ databases/NNS physics/NN and/CC biology/NN skilled/JJ researchers/NNS physics/NN skilled/JJ researchers/NNS biology/NN skilled/JJ researchers/NNS Table 1: Resolving Coordination of NPs 2.1 Candidate Phrases Of the major syntactic constituents of a sentence, e.g. noun phrases, verb phrases, and prepositional phrases, we assume that noun phrases (NPs) carry the most contentful information about the document, a well-supported hypothesis (Smeaton, 1999; Wacholder, 1998). As considered by Wacholder (1998), the simple NPs are the maximal NPs that contain premodifiers but not post-nominal constituents such as prepositions or clauses. We chose simple NPs for content representation because they are semantically and syntactically coherent and they are less ambiguous than complex NPs. For extracting simple noun phrases we first used Ramshaw and Marcus’s base NP chunker (Ramshaw and Marcus, 1995). The base NP is either a simple NP or a coordination of simple NPs. We used heuristics based on POS tags to automatically split the coordinate NPs into simple ones, properl"
W01-0813,C00-1007,0,0.0268105,"C ENTRIFUSER is fully implemented; it produces the sample summary in Figure 1. We have concentrated on implementing the most commonly occuring document feature, topicality, and have additionally incorporated three other document features into our framework (document-derived Content Types and Special Content and the Title metadata). Future work will include extending our document feature analysis to model context (to model adding features only when appropriate), as well as incorporating additional document features. We are also exploring the use of stochastic corpus modeling (Langkilde, 2000; Bangalore and Rambow, 2000) to replace our template-based realizer with a probabilistic one that can produce felicitous sentence patterns based on contextual analysis. 8 Conclusion We have presented a model for indicative multidocument summarization based on natural language generation. In our model, summary content is based on document features describing topic and structure instead of extracted text. Given these features, a generation model uses a text plan, derived from analysis of naturally occurring indicative summaries plus guidelines for summarization, to guide the system in describing document topics as typical,"
W01-0813,W98-1123,1,0.900195,"Missing"
W01-0813,A00-2023,0,0.0400379,"s and future work C ENTRIFUSER is fully implemented; it produces the sample summary in Figure 1. We have concentrated on implementing the most commonly occuring document feature, topicality, and have additionally incorporated three other document features into our framework (document-derived Content Types and Special Content and the Title metadata). Future work will include extending our document feature analysis to model context (to model adding features only when appropriate), as well as incorporating additional document features. We are also exploring the use of stochastic corpus modeling (Langkilde, 2000; Bangalore and Rambow, 2000) to replace our template-based realizer with a probabilistic one that can produce felicitous sentence patterns based on contextual analysis. 8 Conclusion We have presented a model for indicative multidocument summarization based on natural language generation. In our model, summary content is based on document features describing topic and structure instead of extracted text. Given these features, a generation model uses a text plan, derived from analysis of naturally occurring indicative summaries plus guidelines for summarization, to guide the system in describin"
W01-0813,P97-1013,0,0.0223606,"A topic tree for an article about coronary artery disease from The Merck manual of medical information, constructed automatically from its section headers. care article. Each document in the collection is represented by such a tree, which breaks each document’s topic into subtopics. We build these document topic trees automatically for structured documents using a simple approach that utilizes section headers, which suffices for our current domain and genre. Other methods such as layout identification (Hu et al., 1999) and text segmentation / rhetorical parsing (Yaari, 1999; Kan et al., 1998; Marcu, 1997) can serve as the basis for constructing such trees in both structured and unstructured documents, respectively. 4.1 Normative topicality as composite topic trees As stated in rule 1, the summarizer needs normative values calculated for each document feature to properly compute differences between documents. The composite topic tree embodies this paradigm. It is a data structure that compiles knowledge about all possible topics and their structure in articles of the same intersection of domain and genre, (i.e., rule 1’s notion of “document type”). Figure 4 shows a partial view of such a tree c"
W01-0813,W94-0319,0,0.0106391,"multidocument summary generation. We address the problem of “what to say” in Section 2, by examining what document features are important for indicative summaries, starting from a single document context and generalizing to a multidocument, querybased context. This yields two rules-of-thumb for guiding content calculation: 1) reporting differences from the norm and 2) reporting information relevent to the query. We have implemented these rules as part of the content planning module of our C ENTRIFUSER summarization system. The summarizer’s architecture follows the consensus NLG architecture (Reiter, 1994), including the stages of content calculation and content planning. We follow the generation of a sample indicative multidocument query-based summary, shown in the bottom half of Figure 1, focusing on these two stages in the remainder of the paper. 2 Document features as potential summary content Information about topics and structure of the document may be based on higher-level document features. Such information typically does not occur as strings in the document text. Our approach, therefore, is to identify and extract the document features that are relevant for indicative summaries. These"
W01-0906,C00-1018,0,0.106643,"Missing"
W01-0906,P97-1035,0,0.0587805,"Missing"
W01-0906,J00-4006,0,\N,Missing
W01-1011,H92-1022,0,0.0194125,"he message content. 2.2 Candidate Simple Noun Phrase Extraction and Filtering Unit This module performs shallow text processing for extraction and filtering of simple NP candidates, consisting of a pipeline of three modules: text tokenization, NP extraction, and NP filtering. Since the tool was created to preprocess email for speech output, some of the text tokenization suitable for speech is not accurate for text processing and some modifications needed to be implemented (e.g. email preprocessor splits acronyms like DLI2 into DLI 2). The noun phrase extraction module uses Brill&apos;s POS tagger [Brill (1992)]and a base NP chunker [Ramshaw and Marcus (1995)]. After analyzing some of these errors, we augmented the tagger lexicon from our training data and we added lexical and contextual rules to deal mainly with incorrect tagging of gerund endings. In order to improve the accuracy of classifiers we perform linguistic filtering, as discussed in detail in Section 3.1.2. 2.3 Machine Learning Unit The first component of the ML unit is the feature selection module to compute NP vectors. In the training phase, a model for identifying salient simple NPs is created. The training data consist of a list of f"
W01-1011,klavans-etal-2000-evaluation,1,0.794805,"tten et al (1999), Turney (1999)]. These approaches select a set of candidate phrases (sequence of one, two or three consecutive stemmed, non-stop words) and then apply machine learning techniques to classify them as key phrases or not. But dealing only with n-grams does not always provide good output in terms of a summary (see discussion in Section 5.4). Wacholder (1998) proposes a linguisticallymotivated method for the representation of the document aboutness: ‘head clustering’. A list of simple noun phrases is first extracted, clustered by head and then ranked by the frequency of the head. Klavans et al (2000) report on the evaluation of ‘usefulness’ of head clustering in the context of browsing applications, in terms of quality and coverage. Other researchers have used noun-phrases quite successfully for information retrieval task [Strzalkowski et al (1999), Sparck-Jones (1999)]. Strzalkowski et al (1999) uses head + HPDLO PHVVDJH modifier pairs as part of a larger system which constitutes the “stream model” that is used for information retrieval. They treat the head-modifier relationship as an ”ordered relation between otherwise equal elements”, emphasizing that for some tasks, the syntactic head"
W01-1011,P98-1112,1,0.841177,"zed speech. The focus of this paper is on extracting content with GISTIT, although presentation is a topic for future research. 3 Combining Linguistic Knowledge Machine Learning for Email Gisting and We combine symbolic machine learning and linguistic processing in order to extract the salient phrases of a document. Out of the large syntactic constituents of a sentence, e.g. noun phrases, verb phrases, and prepositional phrases, we assume that noun phrases (NPs) carry the most contentful information about the document, even if sometimes the verbs are important too, as reported in the work by [Klavans and Kan (1998)]. The problem is that no matter the size of a document, the number of informative noun phrases is very small comparing with the number of all noun phrases, making selection a necessity. Indeed, in the context of gisting, generating and presenting the list of all noun phrases, even with adequate linguistic filtering, may be overwhelming. Thus, we define the extraction of important noun phrases as a classification task, applying machine learning techniques to determine which features associated with the candidate NPs classify them as salient vs. non-salient. We represent the document -- in this"
W01-1011,W01-0719,1,0.824032,"Missing"
W01-1011,W95-0107,0,0.0587534,"mple Noun Phrase Extraction and Filtering Unit This module performs shallow text processing for extraction and filtering of simple NP candidates, consisting of a pipeline of three modules: text tokenization, NP extraction, and NP filtering. Since the tool was created to preprocess email for speech output, some of the text tokenization suitable for speech is not accurate for text processing and some modifications needed to be implemented (e.g. email preprocessor splits acronyms like DLI2 into DLI 2). The noun phrase extraction module uses Brill&apos;s POS tagger [Brill (1992)]and a base NP chunker [Ramshaw and Marcus (1995)]. After analyzing some of these errors, we augmented the tagger lexicon from our training data and we added lexical and contextual rules to deal mainly with incorrect tagging of gerund endings. In order to improve the accuracy of classifiers we perform linguistic filtering, as discussed in detail in Section 3.1.2. 2.3 Machine Learning Unit The first component of the ML unit is the feature selection module to compute NP vectors. In the training phase, a model for identifying salient simple NPs is created. The training data consist of a list of feature vectors already classified as salient/nons"
W01-1011,W98-0610,0,0.279332,"uraev and Kennedy (1999)], the meaning of ‘summary’ should be adjusted depending on the information management task for which it is used. Key phrases, for example, can be seen as semantic metadata that summarize and characterize documents [Witten et al (1999), Turney (1999)]. These approaches select a set of candidate phrases (sequence of one, two or three consecutive stemmed, non-stop words) and then apply machine learning techniques to classify them as key phrases or not. But dealing only with n-grams does not always provide good output in terms of a summary (see discussion in Section 5.4). Wacholder (1998) proposes a linguisticallymotivated method for the representation of the document aboutness: ‘head clustering’. A list of simple noun phrases is first extracted, clustered by head and then ranked by the frequency of the head. Klavans et al (2000) report on the evaluation of ‘usefulness’ of head clustering in the context of browsing applications, in terms of quality and coverage. Other researchers have used noun-phrases quite successfully for information retrieval task [Strzalkowski et al (1999), Sparck-Jones (1999)]. Strzalkowski et al (1999) uses head + HPDLO PHVVDJH modifier pairs as part of"
W07-0904,1992.tmi-1.9,0,0.0288438,"Missing"
W07-2312,P06-1049,0,0.613813,". O’Learya,b Judith D. Schlesingerd a Department of Computer Science b Institute for Advanced Computer Studies University of Maryland, College Park {nmadnani,nfa,bonnie,oleary}@cs.umd.edu c Center for Computational Learning Systems, Columbia University becky@cs.columbia.edu d IDA/Center for Computing Sciences {conroy,judith}@super.org e College of Information Studies, University of Maryland, College Park jklavans@umd.edu While a good ordering is essential for summary comprehension (Barzilay et al., 2002), and recent The issue of sentence ordering is an important one work on sentence ordering (Bollegala et al., 2006) for natural language tasks such as multi-document does show promise, it is important to note that desummarization, yet there has not been a quantitatermining an optimal sentence ordering for a given tive exploration of the range of acceptable sentence summary may not be feasible. The question for orderings for short texts. We present results of a evaluation of ordering is whether there is a single sentence reordering experiment with three experibest ordering that humans will converge on, or that mental conditions. Our findings indicate a very high would lead to maximum reading comprehension,"
W07-2312,W98-1507,0,0.0368907,"timates of the expected values within each cell. Given a confusion matrix where the cells on the matrix diagonal are denoted as nii , the row marginals as ni+ , the column marginals as n+i and the matrix total as n++ , the formula for κ is: Variability across Experimental Conditions To measure the variability across the experimental conditions, we developed two methods that assign a global score to each set of reorderings by comparing them to a particular reference point. 4.1 Method 1: Confusion Matrices and κ In NLP evaluation, confusion matrices have typically been used in annotation tasks (Bruce and Wiebe (1998), Tomuro (2001)) where the matrix represents the comparison of two judges, and the κ inter-annotator agreement metric value (Cohen, 1960) gives a measure of the amount of agreement between the two judges, after factoring out chance. However, κ has been used to quantify the observed distribution in confusion matrices of other types in a range of other fields and applications (e.g., assessing map accuracy (Hardin, 1999), or optical recognition (Ross et al., 2002)). Here we use it to quantify variability within sets of reorderings for a summary. Given a representation of each summary as a set of"
W07-2312,W05-1621,0,0.241703,"ber is a significant factor in predicting mean τ scores, we can conclude that the 9 summaries differ from each other in terms of the variability among individuals. As in the earlier ANOVA presented in Table 1, we use Tukey’s HSD to determine the magnitude of the difference in means that is necessary for statistical significance, and use this to identify which summaries have significant differences in the amount of similarity among subjects’ reorderings. Applying Tukey’s method to summary number as a factor yields the differences shown in Table 2. 6 Related Work on Evaluating Sentence Ordering Karamanis and Mellish (2005) also measure the amount of variability between human subjects. However, there are several dimensions of contrast between our experiment and theirs: Their experiment operates in a very distinct domain (archaeology) and genre (descriptions of museum artifacts) whereas we use domain-independent multidocument summaries derived from news articles. We use ordinary, English-speaking volunteers as compared to the domain and genre experts that they employ (archaeologists trained in museum labeling). In terms of the experimental design, we use a Latin square design with three experimental condi86 tions"
W07-2312,P03-1069,0,0.678408,"; Lin and Hovy, 2002). Barzilay et al. (2002) were the first to discuss the impact of sentence ordering in the context of multi-document summarization in the news genre. They used an augmented chronological ordering algorithm that first identified and clustered related sentences, then imposed an ordering as directed by the chronology. Okazaki et al. (2004) further improved the chronological ordering algorithm by first arranging sentences in simple chronological order, then performing local reorderings. More recent work includes probabilistic approaches that try to model the structure of text (Lapata, 2003) and algorithms that use large corpora to learn an ordering and then apply it to the summary under consideration (Bollegala et al., 2005). Conroy et al. (2006) treat sentence ordering as a Traveling Salesperson Problem (TSP), similar to Althaus et al. (2004). Starting from a designated first sentence, they reorder the other sentences so that the sum of the distances between adjacent sentences is minimized. The distance (cjk ) between any pair of sentences j and k is computed by first obtaining a similarity score (bjk ) for the pair, and then normalizing this score: bjk , (cjj = 0) cjk = 1 − p"
W07-2312,J06-4002,0,0.267326,"sum to 6, κ ranges from 1 to 0, with 1 indicating that the set of reorderings all reproduce the initial ordering, and 0 indicating that the set of reorderings conforms to chance. 1 2 6 6 2 2 6 4 2 3 1 1 3 3 1 2 3 4 2 2 4 4 2 3 4 5 3 3 5 5 3 4 5 6 4 4 6 6 4 5 6 1 5 5 1 1 5 3 Figure 3: A hypothetical example illustrating Means Vectors compute means vectors for each condition for each summary, giving 27 such vectors. We compare each means vector representing a set of reorderings to each initial ordering O, R and T using three correlation coefficients: Pearson’s r, Spearman’s ρ, and Kendall’s τ (Lapata, 2006). The three correlation coefficients test the closeness of two series of numbers, or two variables x and y, in different ways. Pearson’s r is a parametric test of whether there is a perfect linear relation between the two variables. Spearman’s ρ and Kendall’s τ are non-parametric tests. Spearman’s ρ is computed by replacing the variable values by their rank and computing the correlation. Kendall’s τ is based on counting the number of pairs xi , xi+1 and yi , yi+1 where the deltas of both pairs have the same sign. In sum, the three metrics test whether x and y are in a linear relation, a rank-p"
W07-2312,C04-1108,0,0.578699,"in Section 8. 2 Sentence Ordering Algorithms A number of approaches have been applied to sentence ordering for multi-document summarization (Radev and McKeown, 1999). The first techniques exploited chronological information in the documents (McKeown et al., 1999; Lin and Hovy, 2002). Barzilay et al. (2002) were the first to discuss the impact of sentence ordering in the context of multi-document summarization in the news genre. They used an augmented chronological ordering algorithm that first identified and clustered related sentences, then imposed an ordering as directed by the chronology. Okazaki et al. (2004) further improved the chronological ordering algorithm by first arranging sentences in simple chronological order, then performing local reorderings. More recent work includes probabilistic approaches that try to model the structure of text (Lapata, 2003) and algorithms that use large corpora to learn an ordering and then apply it to the summary under consideration (Bollegala et al., 2005). Conroy et al. (2006) treat sentence ordering as a Traveling Salesperson Problem (TSP), similar to Althaus et al. (2004). Starting from a designated first sentence, they reorder the other sentences so that t"
W07-2312,P04-1051,0,0.118959,"Missing"
W07-2312,J98-3005,0,\N,Missing
W07-2312,I05-1055,0,\N,Missing
W18-1921,2015.iwslt-papers.18,0,0.113297,"Missing"
W18-1921,J09-3007,0,0.0246341,"mily), Canada, Mexico (Nahuatl, UtoAztecan family), and Central Chile (Mapudungun, Araucanian), as well as in Australia (Nunggubuyu, Macro-Gunwinyguan family), Northeastern Siberia (Chukchi and Koryak, both from the Chukotko-Kamchatkan family), and India (Sora, Munda family), as shown in the map below (Figure 1). 2 https://www.nytimes.com/2017/10/04/world/africa/special-forces-killed-niger.html Abbreviations follow the Leipzig Glossing Rules; additional glosses are spelled out in full. 4 In fact, the majority of the languages spoken in the world today are endangered and disappearing fast (See Bird, 2009). Estimates are that, of the approximately 7000 languages in the world today, at least one disappears every day (https://www.ethnologue.com). 3 Proceedings of AMTA 2018, vol. 2: MT Users' Track Boston, March 17 - 21, 2018 |Page 284 Figure 1: Polysynthetic Languages5 Although there are many definitions of polysynthesis, there is often confusion on what constitutes the exact criteria and phenomena (Mithun 2017). Even authoritative sources categorize languages in conflicting ways.6 Typically, polysynthetic languages demonstrate holophrasis, i.e. the ability of an entire sentence to be expressed i"
W18-1921,P86-1019,1,0.256019,"both theoretical and practical reasons, as discussed more fully in the next section. On the theoretical side, these languages offer a potentially unique window into human cognition and language capabilities as well as into language acquisition (Mithun 1989; Greenberg 1960; Comrie 1981; Fortescue 1994; Fortescue et al. 2017). On the practical side, they offer significant obstacles to accurate linguistic analysis as well as to computational modeling. 3. Some Computational Challenges of Polysynthetic Languages Polysynthetic languages pose unique challenges for traditional computational systems (Byrd et al. 1986). Even in allegedly cross-linguistic or typological analyses of specific phenomena, e.g. in forming a theory of clitics and cliticization (Klavans 1995), finding the full range of language types on which to test hypotheses proves difficult. Often, the data is simply not available so claims cannot neither refuted nor supported fully. One of the underlying causes of this difficulty is that there are many languages for which a clear lexical division between nouns and verbs has been challenged; these languages are characterized by a large class of roots that are used either nominally or verbally,"
W18-1921,W17-0102,0,0.0301653,"tal recurrent neural networks (Kong et al. 2015, Micher 2017) and byte-pair encoding Sennrich, Haddow, Birch 2015) to several challenging problems for polysynthetic language analysis and processing. For ASR, we have implemented adaptive learning for iterative ASR, incorporating principles from the Kaldi toolkit9 with modifications as required by different workflows and tasks. 5. Corpus Collection - Electronically-available resources Only recently have researchers started collecting well-designed corpora for polysynthetic languages, e.g. for Circassian (Arkhangelskiy & Lander 2016) or Arapaho (Kazeminejad et al. 2017). There is an urgent need for documentation, archiving, creation of corpora and teaching materials that are specific to polysynthetic languages. Documentation and corpus-building challenges arise for many languages, but the complex morphological makeup of polysynthetic languages makes consistent documentation particularly difficult. The more language data that is gathered and accurately analyzed, the deeper cross-linguistic analyses can be conducted which in turn will contribute to a range of fields including linguistic theory, language teaching and lexicography. For example, in examining cros"
W18-1921,W17-0106,0,0.0294692,"research-programs/material and LORELEI, http://www.darpa.mil/program/low-resource-languages-for-emergent-incidents, respectively. Proceedings of AMTA 2018, vol. 2: MT Users' Track Boston, March 17 - 21, 2018 |Page 288 Concomitant with the collection and cataloging of corpora, we are working with colleagues especially from the the NSF-funded EL-STEC Shared Task Evaluation Campaign project13 on a future shared task in order to bring linguists and computational linguists together around the common area: accuracy in data analysis. We aim to formulate a shared task that meets the goals outlined in Levow, et al. (2017), namely, to “align the interests of the speech and language processing communities with those of … language documentation communities….”, guided by their design principles of realism, typological diversity, accessibility of the shared task, accessibility of the resulting software, extensibility and nuanced evaluation. 6. Future Research and Applications Our next steps involve a two-phase approach, one on the ASR input and then one on the MT side (as shown in Figure 2.) On the ASR side, we plan to use Multi-Task Learning (MTL) (Caruana 1997), using corpora from multiple languages. Multitask Le"
W18-1921,W17-0114,1,0.829109,"of each component and discuss further the novel methodological contributions of the research. Figure 2: Overview of Speech-MT Polysynthetic Language Architecture 8 https://www..gov/index.php/research-programs/babel Proceedings of AMTA 2018, vol. 2: MT Users' Track Boston, March 17 - 21, 2018 |Page 287 ARL has demonstrated leading technologies in the field with critical expertise. We are planning on developing systems, capable of performing speech translation. We are applying machine learning techniques using neural network approaches e.g. segmental recurrent neural networks (Kong et al. 2015, Micher 2017) and byte-pair encoding Sennrich, Haddow, Birch 2015) to several challenging problems for polysynthetic language analysis and processing. For ASR, we have implemented adaptive learning for iterative ASR, incorporating principles from the Kaldi toolkit9 with modifications as required by different workflows and tasks. 5. Corpus Collection - Electronically-available resources Only recently have researchers started collecting well-designed corpora for polysynthetic languages, e.g. for Circassian (Arkhangelskiy & Lander 2016) or Arapaho (Kazeminejad et al. 2017). There is an urgent need for documen"
W18-4801,2015.iwslt-papers.18,0,0.0290869,"Missing"
W18-4801,J09-3007,0,0.216429,"s and communities struggling to preserve their linguistic heritage. In particular, polysynthetic languages can be found in the US Southwest (Southern Tiwa, Kiowa Tanoan family), Canada, Mexico (Nahuatl, Uto-Aztecan family), and Central Chile (Mapudungun, This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/. Abbreviations follow the Leipzig Glossing Rules; additional glosses are spelled out in full. fact, the majority of the languages spoken in the world today are endangered and disappearing fast (See Bird, 2009). Estimates are that, of the approximately 7000 languages in the world today, at least one disappears every day (https://www.ethnologue.com). 1 2 In 1 Proceedings of Workshop on Polysynthetic Languages, pages 1–11 Santa Fe, New Mexico, USA, August 20-26, 2018. Araucanian), as well as in Australia (Nunggubuyu, Macro-Gunwinyguan family), Northeastern Siberia (Chukchi and Koryak, both from the Chukotko-Kamchatkan family), and India (Sora, Munda family), as shown in the map below (Figure 1). This workshop addresses the needs for documentation, archiving, creation of corpora and teaching materials"
W18-4801,P86-1019,1,0.224246,"h polysynthesis. Here we will mention just two of these properties: rich agreement (with the subject, direct object, indirect object, and applied objects if present) and omission of free-standing arguments (pro-drop). Polysynthetic languages are of interest for both theoretical and practical reasons. On the theoretical side, these languages offer a potentially unique window into human cognition and language capabilities as well as into language acquisition (Mithun 1989; Greenberg 1960; Comrie 1981; Fortescue et al. 2017). They also pose unique challenges for traditional computational systems (Byrd et al. 1986). Even in allegedly cross-linguistic or typological analyses of specific phenomena, e.g. in forming a theory of clitics and cliticization (Klavans 1995), finding the full range of language types on which to test hypotheses proves difficult. Often, the data is simply not available so claims cannot be either refuted or supported fully. On the applied side, many morphologically complex languages are crucial to purposes in domains ranging from health care,5 search and rescue, to the maintenance of cultural history. Add to this the interest in low-resource languages (from Inuktitut and Yup’ik in th"
W18-4801,W17-0102,0,0.171047,"Missing"
W18-4801,W17-0106,0,0.123802,"vis & Matthewson 2009, Watanabe 2017 for Salish). Without a clear definition of what counts as a verb and what as a noun, there is no reliable way to compute significant correlations. Thus, a deeper understanding of polysynthetic phenomena may well contribute to a more nuanced understanding of cross-language comparisons and generalizations and enable researchers to pose meaningful and answerable questions about comparative features across languages. One of the goals of the workshop is to identify and build new resources, with annotation that is effective for a range of efforts, as outlined in Levow et al. (2017). We will ensure that all materials resulting from this workshop are listed in the LDC catalog with adequate metadata giving descriptions, pointers, terms and conditions and other facts necessary for use. What we have found is that there are corpora in many different places by different types of community actors, and often they are difficult to locate and obtain. Building models and theoretical descriptions can be challenging without adequate data, and this is a gap we plan to address along with the many others involved in this endeavor. While collections of annotated corpora (spoken and writt"
W19-4222,W18-4801,1,0.740057,"-Aztecan family. Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages. 1 Smaranda Muresan Columbia University Data Science Institute smara@columbia.edu Introduction Computational morphology of polysynthetic languages is an emerging field of research. Polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations (Homola, 2011; Mager et al., 2018d; Klavans, 2018a). Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017), hybrid models (Mager et al., 2018b; Moeller et al., 2018), and supervised machine learning, particularly deep learning approaches (Micher, 2017; Kann et al., 2018). While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009), or Arapaho (Littell, 2018; Moeller et al., 2018)), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages. We propose an unsupervised approac"
W19-4222,W18-4803,0,0.0390868,"to supervised methods, outperforming them on two of the four languages. 1 Smaranda Muresan Columbia University Data Science Institute smara@columbia.edu Introduction Computational morphology of polysynthetic languages is an emerging field of research. Polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations (Homola, 2011; Mager et al., 2018d; Klavans, 2018a). Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017), hybrid models (Mager et al., 2018b; Moeller et al., 2018), and supervised machine learning, particularly deep learning approaches (Micher, 2017; Kann et al., 2018). While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009), or Arapaho (Littell, 2018; Moeller et al., 2018)), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages. We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor Grammars (Johnson 2 Languages and Da"
W19-4222,C18-1006,0,0.229421,"anguages from the Uto-Aztecan family. Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages. 1 Smaranda Muresan Columbia University Data Science Institute smara@columbia.edu Introduction Computational morphology of polysynthetic languages is an emerging field of research. Polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations (Homola, 2011; Mager et al., 2018d; Klavans, 2018a). Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017), hybrid models (Mager et al., 2018b; Moeller et al., 2018), and supervised machine learning, particularly deep learning approaches (Micher, 2017; Kann et al., 2018). While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009), or Arapaho (Littell, 2018; Moeller et al., 2018)), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages. We propose an unsu"
W19-4222,W18-4808,0,0.136534,"tric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016, 2018). Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal. We use the datasets introduced by Kann et al. (2018) in an unsupervised fashion (unsegmented words). We design several AG learning setups: 1) use the best-on-average AG setup from Eskander et al. (2016); 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from Kann et al. (2018); 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3). We show that the AG-based approaches outperform other unsupervised methods — M orf essor (Creutz and Lagus, 2007) and M orphoChain (Narasimhan et"
W19-4222,R11-1079,0,0.026377,"nt with four languages from the Uto-Aztecan family. Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages. 1 Smaranda Muresan Columbia University Data Science Institute smara@columbia.edu Introduction Computational morphology of polysynthetic languages is an emerging field of research. Polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations (Homola, 2011; Mager et al., 2018d; Klavans, 2018a). Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017), hybrid models (Mager et al., 2018b; Moeller et al., 2018), and supervised machine learning, particularly deep learning approaches (Micher, 2017; Kann et al., 2018). While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009), or Arapaho (Littell, 2018; Moeller et al., 2018)), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages"
W19-4222,W08-0704,0,0.440282,"source Polysynthetic Languages Ramy Eskander Columbia University Dept. of Computer Science rnd2110@columbia.edu Judith L. Klavans University of Maryland UMIACS jklavans@umd.edu Abstract et al., 2007). We experiment with four UtoAztecan languages: Mexicanero (MX), Nahuatl (NH), Wixarika (WX) and Yorem Nokki (YN) (Kann et al., 2018). Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016, 2018). Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal. We use the datasets introduced by Kann et al. (2018) in an unsupervised fashion (unsegmented words). We design several AG learning setups: 1) use the best-on-average AG setup from Eskander et al. (2016); 2) optimize for language using just the small training vocabulary (unsegmented) and d"
W19-4222,W17-0114,0,0.0209778,"tional morphology of polysynthetic languages is an emerging field of research. Polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations (Homola, 2011; Mager et al., 2018d; Klavans, 2018a). Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017), hybrid models (Mager et al., 2018b; Moeller et al., 2018), and supervised machine learning, particularly deep learning approaches (Micher, 2017; Kann et al., 2018). While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009), or Arapaho (Littell, 2018; Moeller et al., 2018)), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages. We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor Grammars (Johnson 2 Languages and Datasets Typically, polysynthetic languages demonstrate holophrasis, i.e. the ability of an entire sentence to be expressed as what is considered by native speakers to be jus"
W19-4222,W18-4802,0,0.0212637,"anda Muresan Columbia University Data Science Institute smara@columbia.edu Introduction Computational morphology of polysynthetic languages is an emerging field of research. Polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations (Homola, 2011; Mager et al., 2018d; Klavans, 2018a). Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017), hybrid models (Mager et al., 2018b; Moeller et al., 2018), and supervised machine learning, particularly deep learning approaches (Micher, 2017; Kann et al., 2018). While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009), or Arapaho (Littell, 2018; Moeller et al., 2018)), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages. We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor Grammars (Johnson 2 Languages and Datasets Typically, polysynthetic languages demonstrate holophrasis, i.e. the ability of"
W19-4222,N18-1005,0,0.0817705,"Missing"
W19-4222,Q15-1012,0,0.298007,"et al. (2018) in an unsupervised fashion (unsegmented words). We design several AG learning setups: 1) use the best-on-average AG setup from Eskander et al. (2016); 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from Kann et al. (2018); 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3). We show that the AG-based approaches outperform other unsupervised methods — M orf essor (Creutz and Lagus, 2007) and M orphoChain (Narasimhan et al., 2015)) —, and that for two of the languages (NH and YN), the best AG-based approaches outperform the best supervised methods (Section 4). Polysynthetic languages pose a challenge for morphological analysis due to the rootmorpheme complexity and to the word class “squish”. In addition, many of these polysynthetic languages are low-resource. We propose unsupervised approaches for morphological segmentation of low-resource polysynthetic languages based on Adaptor Grammars (AG) (Eskander et al., 2016). We experiment with four languages from the Uto-Aztecan family. Our AG-based approaches outperform oth"
W19-4222,W17-0102,0,0.0306569,"ethods, outperforming them on two of the four languages. 1 Smaranda Muresan Columbia University Data Science Institute smara@columbia.edu Introduction Computational morphology of polysynthetic languages is an emerging field of research. Polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations (Homola, 2011; Mager et al., 2018d; Klavans, 2018a). Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017), hybrid models (Mager et al., 2018b; Moeller et al., 2018), and supervised machine learning, particularly deep learning approaches (Micher, 2017; Kann et al., 2018). While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009), or Arapaho (Littell, 2018; Moeller et al., 2018)), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages. We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor Grammars (Johnson 2 Languages and Datasets Typically, polysynth"
W19-4222,Q13-1021,0,0.122423,"hetic Languages Ramy Eskander Columbia University Dept. of Computer Science rnd2110@columbia.edu Judith L. Klavans University of Maryland UMIACS jklavans@umd.edu Abstract et al., 2007). We experiment with four UtoAztecan languages: Mexicanero (MX), Nahuatl (NH), Wixarika (WX) and Yorem Nokki (YN) (Kann et al., 2018). Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016, 2018). Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal. We use the datasets introduced by Kann et al. (2018) in an unsupervised fashion (unsegmented words). We design several AG learning setups: 1) use the best-on-average AG setup from Eskander et al. (2016); 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) f"
W98-1123,P94-1002,0,0.398315,"t, tended to have their own prior segmentation markings consisting of headers or bullets, so these were excluded. We thus concentrated our work on a corpus of shorter articles, averaging roughly 800-1500 words in length: 15 from the Wall Street Journal in the Linguistic Data Consortium's 1988 collection, and 5 from the on-line The Economist from 1997. We constructed an evaluation standard from human segmentation judgments to test our output. 1 SEGMENTER:L i n e a r S e g m e n t a t i o n For the purposes of discourse structure identification, we follow a formulation of the problem similar to Hearst (1994), in which zero or more segment boundaries are found at various paragraph separations, which identify one or more topical text segments. Our segmentation is linear, rather than hierarchical (Marcu 1997 and Yaari 1997), i.e. the input article is divided into a linear sequence of adjacent segments. Our segmentation methodology has three distinct phases (Figure 1), which are executed sequentially. We will describe each of these phases in detail. [. &quot; - &quot; ~ - ~ &quot; - ~ W e i g h :'t [Terms I ITerrn I I [Links ~--~Score I ISegment I:egnts ] [Boundarie] Figure 1. SEGMENTERArchitecture 1,1 E x t r a c"
W98-1123,J97-1003,0,0.395035,"boundary heavily, but instead place the emphasis on the rear. Front: a paragraph in which a link begins. During: a paragraph in which a link occurs, but is not a front paragraph. Rear: a paragraph in which a link just stopped occurring the paragraph before. No link: any remaining paragraphs. paras 1 2 3 4 5 7 8 sents 12345678901234567890123456789012345 wine : Ixxl ix21 t y p e :n f d r n f d Figure 2zt A term &quot;wine&quot;, and its occurrences and type. We also tried to semantically cluster terms by using Miller et al. (1990)'s WordNet 1.5 with edge counting to determine relatedness, as suggested by Hearst (1997). However, results showed only minor improvement in precision and over a tenfold increase in execution time. 199 1.2.3. Zero Sum Normalization When we iterate the weighting process described above over each term, and total the scores assigned, we come up with a numerical score for an indication of which paragraphs are more likely to beh a topical boundary. The higher the numerical score, the higher the likelihood that the paragraph is a beginning of a new topical segment. The question then is what should the threshold be? paras 1 2 3 4 5 7 8 sents 12345678901234567890123456789012345 wine : ixx"
W98-1123,P93-1041,0,0.756536,"Missing"
W98-1123,P97-1013,0,0.0278967,"words in length: 15 from the Wall Street Journal in the Linguistic Data Consortium's 1988 collection, and 5 from the on-line The Economist from 1997. We constructed an evaluation standard from human segmentation judgments to test our output. 1 SEGMENTER:L i n e a r S e g m e n t a t i o n For the purposes of discourse structure identification, we follow a formulation of the problem similar to Hearst (1994), in which zero or more segment boundaries are found at various paragraph separations, which identify one or more topical text segments. Our segmentation is linear, rather than hierarchical (Marcu 1997 and Yaari 1997), i.e. the input article is divided into a linear sequence of adjacent segments. Our segmentation methodology has three distinct phases (Figure 1), which are executed sequentially. We will describe each of these phases in detail. [. &quot; - &quot; ~ - ~ &quot; - ~ W e i g h :'t [Terms I ITerrn I I [Links ~--~Score I ISegment I:egnts ] [Boundarie] Figure 1. SEGMENTERArchitecture 1,1 E x t r a c t i n g Useful T o k e n s The task of determining segmentation b r e ~ s depends fundamentally on extracting useful topic information from the text. We extract three categories of information, which r"
W98-1123,C94-2121,0,0.1839,"Missing"
W98-1123,P93-1020,0,0.0629062,"Missing"
W98-1123,P94-1050,0,0.488043,"Missing"
W98-1123,W97-0304,0,\N,Missing
W98-1123,W97-0703,0,\N,Missing
W98-1123,C94-1042,0,\N,Missing
W98-1123,J96-2004,0,\N,Missing
W99-0625,M95-1012,0,0.0128247,"Missing"
W99-0625,P99-1071,0,0.0886417,"Missing"
W99-0625,J96-2004,0,0.00591901,"Missing"
W99-0625,J93-1007,0,0.0385544,"Missing"
W99-0625,W98-0610,0,0.0471508,"Missing"
