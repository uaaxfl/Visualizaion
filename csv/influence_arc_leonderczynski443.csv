2020.coling-tutorials.4,aker-etal-2017-simple,1,0.794399,"n authoritative source. Emerging claims may not yet be mentioned in resources like WikiData, due to lag; and claims around entities that are not notable enough for inclusion in knowledge bases will remain difficult to ground. However, the stance that social media users take to claims can serve to predict the claim’s accuracy (Dungs et al., 2018; Qazvinian et al., 2011). In this part, we introduce the problem of stance prediction, discuss multiple framings of the problem, and describe the state-of-the-art in stance prediction with both neural and non-neural techniques (Augenstein et al., 2016; Aker et al., 2017). We go on to describe how stance predictions can be combined through multi-spaced HMM to predict veracity based on conversational stances (Dungs et al., 2018), and how models for this can be transferred across languages (Lillie et al., 2019). 1.6 Style analysis for deception detection Many verification practices concentrate on determining precisely what the claim is, and then finding whether or not that claim is false. This misses an important and much closer source: the way in which the claim itself is written. Style tells us a lot about an author and their intent, and more candid the writin"
2020.coling-tutorials.4,D15-1075,0,0.0162862,"ke to the claim, or information stored in external knowledge bases. An assessment of veracity is stronger if accompanied by supporting evidence. This is the approach taken in the FEVER challenge (Thorne et al., 2018), where supporting text must be provided alongside a judgment. We introduce techniques including word and entity matching between claim and article (Yoneda et al., 2018; Luken et al., 2018), neural whole-sentence comparison using Enhanced LSTM (Chen et al., 2016) or Decomposable Attention (Parikh et al., 2016), and the pros and cons of NLI-based techniques for rumour verification (Bowman et al., 2015; Rockt¨aschel et al., 2016). 1.5 Using stance to verify claims It’s not always possible to ground a claim to an authoritative source. Emerging claims may not yet be mentioned in resources like WikiData, due to lag; and claims around entities that are not notable enough for inclusion in knowledge bases will remain difficult to ground. However, the stance that social media users take to claims can serve to predict the claim’s accuracy (Dungs et al., 2018; Qazvinian et al., 2011). In this part, we introduce the problem of stance prediction, discuss multiple framings of the problem, and describe"
2020.coling-tutorials.4,C18-1284,0,0.0120644,"M (Chen et al., 2016) or Decomposable Attention (Parikh et al., 2016), and the pros and cons of NLI-based techniques for rumour verification (Bowman et al., 2015; Rockt¨aschel et al., 2016). 1.5 Using stance to verify claims It’s not always possible to ground a claim to an authoritative source. Emerging claims may not yet be mentioned in resources like WikiData, due to lag; and claims around entities that are not notable enough for inclusion in knowledge bases will remain difficult to ground. However, the stance that social media users take to claims can serve to predict the claim’s accuracy (Dungs et al., 2018; Qazvinian et al., 2011). In this part, we introduce the problem of stance prediction, discuss multiple framings of the problem, and describe the state-of-the-art in stance prediction with both neural and non-neural techniques (Augenstein et al., 2016; Aker et al., 2017). We go on to describe how stance predictions can be combined through multi-spaced HMM to predict veracity based on conversational stances (Dungs et al., 2018), and how models for this can be transferred across languages (Lillie et al., 2019). 1.6 Style analysis for deception detection Many verification practices concentrate o"
2020.coling-tutorials.4,P12-2034,0,0.0274629,"lse. This misses an important and much closer source: the way in which the claim itself is written. Style tells us a lot about an author and their intent, and more candid the writing is, the more information an author leaks about themself. When people seek to deceive, they adopt a certain set of behaviors. These behaviors can affect how they use language, thus providing a stylistic clue to identifying deception and deceptive intent. This section of the tutorial introduces these techniques, explaining typical linguistic cues and outlining how they can be mined automatically (Zhou et al., 2004; Feng et al., 2012; Fitzpatrick et al., 2015), as well as introducing caveats for applying this technology in the context of fact checking. 2 Type This is a “cutting edge” tutorial; none on the topic have been presented before at the relevant venues. 3 Structure outline Introduction A contextualisation of the course material, including a brief history of fact checking and a challenging quiz around modern claims, at each step delineating the scope of what NLP-based methods can do to address each kind of misinformation. Identifying and tracking check-worthy claims Define checkworthiness and modern techniques for"
2020.coling-tutorials.4,S19-2147,1,0.820072,"ercises. Familiarity with the Twitter API is a bonus, and one should have an account before starting the exercises (we don’t need to know what the account is, it’s simply for getting a developer API key). 7 Presenters One UK, one Denmark (two male). Leon Derczynski is an assistant professor of Computer Science at the IT University of Copenhagen, Denmark. He was program co-chair for COLING 2018, co-author of an EU project on misinformation (Derczynski et al., 2015), P HEME, co-investigator of an EC Horizon 2020 RIA, Comrades, and has co-organised SemEval shared tasks in 2013/15/16/17/19 (e.g. (Gorrell et al., 2019)). He has published repeatedly in the area of rumour identification and processing and given talks on the topics at a wide range of venues, and teaches a course on NLP-based verification and misinformation detection at Innopolis University, Russia. Leon has taught full university courses in the UK, USA, Denmark, Russia, and China. ld@itu.dk www.derczynski.com. Arkaitz Zubiaga is an assistant professor at Queen Mary University of London, UK. He is on the editorial boards of the Information Processing & Management, Online Social Networks and Media, PeerJ Computer Science, and Information journal"
2020.coling-tutorials.4,W19-6122,1,0.726117,"tance that social media users take to claims can serve to predict the claim’s accuracy (Dungs et al., 2018; Qazvinian et al., 2011). In this part, we introduce the problem of stance prediction, discuss multiple framings of the problem, and describe the state-of-the-art in stance prediction with both neural and non-neural techniques (Augenstein et al., 2016; Aker et al., 2017). We go on to describe how stance predictions can be combined through multi-spaced HMM to predict veracity based on conversational stances (Dungs et al., 2018), and how models for this can be transferred across languages (Lillie et al., 2019). 1.6 Style analysis for deception detection Many verification practices concentrate on determining precisely what the claim is, and then finding whether or not that claim is false. This misses an important and much closer source: the way in which the claim itself is written. Style tells us a lot about an author and their intent, and more candid the writing is, the more information an author leaks about themself. When people seek to deceive, they adopt a certain set of behaviors. These behaviors can affect how they use language, thus providing a stylistic clue to identifying deception and dece"
2020.coling-tutorials.4,W18-5526,0,0.0248599,"Missing"
2020.coling-tutorials.4,D16-1244,0,0.0955906,"Missing"
2020.coling-tutorials.4,N10-1021,0,0.07545,"Missing"
2020.coling-tutorials.4,D11-1147,0,0.0395659,") or Decomposable Attention (Parikh et al., 2016), and the pros and cons of NLI-based techniques for rumour verification (Bowman et al., 2015; Rockt¨aschel et al., 2016). 1.5 Using stance to verify claims It’s not always possible to ground a claim to an authoritative source. Emerging claims may not yet be mentioned in resources like WikiData, due to lag; and claims around entities that are not notable enough for inclusion in knowledge bases will remain difficult to ground. However, the stance that social media users take to claims can serve to predict the claim’s accuracy (Dungs et al., 2018; Qazvinian et al., 2011). In this part, we introduce the problem of stance prediction, discuss multiple framings of the problem, and describe the state-of-the-art in stance prediction with both neural and non-neural techniques (Augenstein et al., 2016; Aker et al., 2017). We go on to describe how stance predictions can be combined through multi-spaced HMM to predict veracity based on conversational stances (Dungs et al., 2018), and how models for this can be transferred across languages (Lillie et al., 2019). 1.6 Style analysis for deception detection Many verification practices concentrate on determining precisely w"
2020.coling-tutorials.4,N18-1074,0,0.0161783,"2020. 1.4 Grounding claims against knowledge bases Having found a claim on the web, the next step is to determine its veracity – to test if it is true or not. Depending on what the claim involves, and the context of the claim, there are a variety of techniques available. This section discusses those techniques, analyzing the claim itself and comparing with external factors – like the stance others take to the claim, or information stored in external knowledge bases. An assessment of veracity is stronger if accompanied by supporting evidence. This is the approach taken in the FEVER challenge (Thorne et al., 2018), where supporting text must be provided alongside a judgment. We introduce techniques including word and entity matching between claim and article (Yoneda et al., 2018; Luken et al., 2018), neural whole-sentence comparison using Enhanced LSTM (Chen et al., 2016) or Decomposable Attention (Parikh et al., 2016), and the pros and cons of NLI-based techniques for rumour verification (Bowman et al., 2015; Rockt¨aschel et al., 2016). 1.5 Using stance to verify claims It’s not always possible to ground a claim to an authoritative source. Emerging claims may not yet be mentioned in resources like Wik"
2020.coling-tutorials.4,W18-5515,0,0.0113457,"on what the claim involves, and the context of the claim, there are a variety of techniques available. This section discusses those techniques, analyzing the claim itself and comparing with external factors – like the stance others take to the claim, or information stored in external knowledge bases. An assessment of veracity is stronger if accompanied by supporting evidence. This is the approach taken in the FEVER challenge (Thorne et al., 2018), where supporting text must be provided alongside a judgment. We introduce techniques including word and entity matching between claim and article (Yoneda et al., 2018; Luken et al., 2018), neural whole-sentence comparison using Enhanced LSTM (Chen et al., 2016) or Decomposable Attention (Parikh et al., 2016), and the pros and cons of NLI-based techniques for rumour verification (Bowman et al., 2015; Rockt¨aschel et al., 2016). 1.5 Using stance to verify claims It’s not always possible to ground a claim to an authoritative source. Emerging claims may not yet be mentioned in resources like WikiData, due to lag; and claims around entities that are not notable enough for inclusion in knowledge bases will remain difficult to ground. However, the stance that soc"
2020.fever-1.6,S19-1028,0,0.35197,"dkˆ : d ∈ D||D|−1 ) s (5) Where s is a scaling factor corresponding to the estimated exponent of the features’ power law frequency distribution. For English, s = 3 gives reasonable results (i.e. taking the cube root). These two are combined taking their squared product to form DCI: DCI = p λh × λf (6) A note regarding language: in this case, we consider 1, 2, and 3-grams, with skips in the range of [0, 2]. This is suitable for English; other languages might benefit from broader skip ranges. 43 3.2 Cue a is in the was Cue Productivity and Coverage Probes We follow the approach of Niven and Kao (2019) in determining a productivity and coverage score for each cue in the data. As the structure of their dataset is fundamentally different from the dataset presented in Thorne et al. (2018), we have made amendments to their methodology in order to attain comparable results. As in Niven and Kao (2019), we consider any uni- or bigram a potential cue. We extract these cues from the claims in the dataset and take note of the associated label. This allows us to calculate the applicability of a given cue (αk ), which represents the absolute number of claims in the dataset that contain the cue irrespec"
2020.fever-1.6,guthrie-etal-2006-closer,0,0.0475897,"ldom, should not receive a high value. On the other hand, knowing that features in language typically follow a Zipfian frequency distribution (Montemurro, 2001), one should still have useful resolution beyond the most-frequent items. Thus we specify a frequency-based scaling factor λf as a root of the scaled frequency weight: Dataset-weighted Cue Information The first metric we propose is a simple information theoretic measure of how much a pattern contributes to a classification. In this case, patterns are extracted using skip-grams. These capture a good amount of information about a corpus (Guthrie et al., 2006) while also giving a way of ignoring the typically-rare named entities that are rich in FEVER claims and focusing on the surrounding language. The metric is the weighted inverse information gain of a skip-gram relative to a pair of classes. Weighting is determined by the frequency of documents bearing the skip-gram in the corpus, which normalises skew from highly imbalanced but rare phrases. For dataset D and cue k, where cues are e.g. skip-gram features: 1 λf = (|dkˆ : d ∈ D||D|−1 ) s (5) Where s is a scaling factor corresponding to the estimated exponent of the features’ power law frequency"
2020.fever-1.6,N18-1175,0,0.0611899,"Missing"
2020.fever-1.6,P19-1459,0,0.102305,"res: 1 λf = (|dkˆ : d ∈ D||D|−1 ) s (5) Where s is a scaling factor corresponding to the estimated exponent of the features’ power law frequency distribution. For English, s = 3 gives reasonable results (i.e. taking the cube root). These two are combined taking their squared product to form DCI: DCI = p λh × λf (6) A note regarding language: in this case, we consider 1, 2, and 3-grams, with skips in the range of [0, 2]. This is suitable for English; other languages might benefit from broader skip ranges. 43 3.2 Cue a is in the was Cue Productivity and Coverage Probes We follow the approach of Niven and Kao (2019) in determining a productivity and coverage score for each cue in the data. As the structure of their dataset is fundamentally different from the dataset presented in Thorne et al. (2018), we have made amendments to their methodology in order to attain comparable results. As in Niven and Kao (2019), we consider any uni- or bigram a potential cue. We extract these cues from the claims in the dataset and take note of the associated label. This allows us to calculate the applicability of a given cue (αk ), which represents the absolute number of claims in the dataset that contain the cue irrespec"
2020.fever-1.6,sabou-etal-2014-corpus,1,0.841487,"Missing"
2020.fever-1.6,D19-1340,0,0.0187339,"information (LMI) reveals some 45 References n-grams that are strongly-associated with negative examples, and are able to predict claim veracity based on claims alone. The phrases that Schuster et al. find match those top-ranked by our DCI metric. We can therefore see that mutual informationbased measures (LMI, DCI) find different biases to frequency-associative measures, such as those use to find cues in the ARCT task. It may be worth applying e.g. LMI or DCI to the ARCT data to see if complementary results emerge. Note that we examine all n- and skip-grams in the dataset, without smoothing. Suntwal et al. (2019) experiment with removing named entities and rare noun-phrases from their dataset when training models. While this is likely to reduce variances in the data representation, enhancing the signal, the goal of this work is to find the strongest signals, and go down from there, rather than remove noise in a “bottom-up” fashion. This is not the first investigation into biases related to crowdsourcing and human annotation: Belinkov et al. (2019) find patterns in corpora for inference. Sabou et al. (2014) and Bontcheva et al. (2017) discuss best practices in crowdsourcing for corpus creation. Notably"
2020.fever-1.6,N18-1074,0,0.0171309,"sonable results (i.e. taking the cube root). These two are combined taking their squared product to form DCI: DCI = p λh × λf (6) A note regarding language: in this case, we consider 1, 2, and 3-grams, with skips in the range of [0, 2]. This is suitable for English; other languages might benefit from broader skip ranges. 43 3.2 Cue a is in the was Cue Productivity and Coverage Probes We follow the approach of Niven and Kao (2019) in determining a productivity and coverage score for each cue in the data. As the structure of their dataset is fundamentally different from the dataset presented in Thorne et al. (2018), we have made amendments to their methodology in order to attain comparable results. As in Niven and Kao (2019), we consider any uni- or bigram a potential cue. We extract these cues from the claims in the dataset and take note of the associated label. This allows us to calculate the applicability of a given cue (αk ), which represents the absolute number of claims in the dataset that contain the cue irrespective of their label. Let T be the set of all cues and n the number of claims. αk = n h X i 1 ∃k ∈ T Cue not only πk = h i=1 1 ∃j, k ∈ Tj Pn αk Productivity 0.86 0.90 Coverage 0.04 0.04 Ta"
2020.lrec-1.303,D13-1106,0,0.0200032,"cability in NLP. We present efficient implementations of Brown clustering and the alternative Exchange clustering as well as a number of methods to accelerate the computation of both hierarchical and flat clusters. We show empirically that clusters obtained with the accelerated method match the performance of clusters computed using the original methods. Keywords: word clusters, word representations, efficient computation 1. Introduction Word clusters have been successfully used in NLP tasks over the past decades, contributing to advances especially in machine translation (Brown et al., 1993; Auli et al., 2013), named entity recognition (Ratinov and Roth, 2009; Ritter et al., 2011), parsing (Koo et al., 2008; Kong et al., 2014), and processing noisy text (Owoputi et al., 2012). They remain a competitive representation useful for many tasks (Dyer et al., 2016; Choi, 2016; Lukasik et al., 2015), yielding superior extrinsic performance in particular when limited data is available (Qu et al., 2015) – which is the case for the majority of languages. More recently, word clusters induced using the Brown clustering algorithm have been shown to be highly effective in encoding syntactic information (Ciosici e"
2020.lrec-1.303,P14-2131,0,0.0242995,"MI value with varied beam width / cluster count c. Time is wallclock seconds. ditional Brown using the implementation of Liang (2005). This is run with min-occur set to 1, in order to preserve information and provide a fair, higher-AMI clustering that incorporates a maximum amount of corpus knowledge. In other words, we do not eliminate low frequency words from the vocabulary. Comparing Brown, Exchange, or the hybrid method with other word representations is out of scope for this work. For comparisons of Brown or Exchange with other word representation methods, in particular word vectors, see Bansal et al. (2014), and Qu et al. (2015); for characterization of syntactic information encoded by Brown clusters see Ciosici et al. (2019). 5. 5.1. Results and Analysis Computation and AMI performance We measure the reduction in computation time and improvement in language modeling. For this first experiment we focus on English, using one million words from Reuters Corpus (RCV) (Rose et al., 2002). We use a machine with dual Intel 8176 and 512GB RAM. Runtime results (mean of three runs) are presented in Table 1. Our hybrid method provides a higher peak AMI and much lower run-time than Brown, in every case. The"
2020.lrec-1.303,J92-4003,0,0.583008,"014), and processing noisy text (Owoputi et al., 2012). They remain a competitive representation useful for many tasks (Dyer et al., 2016; Choi, 2016; Lukasik et al., 2015), yielding superior extrinsic performance in particular when limited data is available (Qu et al., 2015) – which is the case for the majority of languages. More recently, word clusters induced using the Brown clustering algorithm have been shown to be highly effective in encoding syntactic information (Ciosici et al., 2019) and have been used for unsupervised Part-of-Speech tagging (Cardenas et al., 2019). Brown clustering (Brown et al., 1992) is a commonly used word clustering algorithm, performing a bottom-up, windowed, hard hierarchical clustering based on the global objective of maximized Average Mutual Information (AMI), which is equivalent to the Maximum Likelihood Estimate (MLE) of the underlying language model (Martin et al., 1998). The tension between local merges and the global optimization goal makes the algorithm parallelize hard. Simultaneously, the number of merges considered at any one time directly affects the quality of the final word clustering (Derczynski and Chester, 2016). While Brown clustering often provides"
2020.lrec-1.303,J93-2003,0,0.223147,", limits their applicability in NLP. We present efficient implementations of Brown clustering and the alternative Exchange clustering as well as a number of methods to accelerate the computation of both hierarchical and flat clusters. We show empirically that clusters obtained with the accelerated method match the performance of clusters computed using the original methods. Keywords: word clusters, word representations, efficient computation 1. Introduction Word clusters have been successfully used in NLP tasks over the past decades, contributing to advances especially in machine translation (Brown et al., 1993; Auli et al., 2013), named entity recognition (Ratinov and Roth, 2009; Ritter et al., 2011), parsing (Koo et al., 2008; Kong et al., 2014), and processing noisy text (Owoputi et al., 2012). They remain a competitive representation useful for many tasks (Dyer et al., 2016; Choi, 2016; Lukasik et al., 2015), yielding superior extrinsic performance in particular when limited data is available (Qu et al., 2015) – which is the case for the majority of languages. More recently, word clusters induced using the Brown clustering algorithm have been shown to be highly effective in encoding syntactic in"
2020.lrec-1.303,N19-1252,0,0.0237557,"parsing (Koo et al., 2008; Kong et al., 2014), and processing noisy text (Owoputi et al., 2012). They remain a competitive representation useful for many tasks (Dyer et al., 2016; Choi, 2016; Lukasik et al., 2015), yielding superior extrinsic performance in particular when limited data is available (Qu et al., 2015) – which is the case for the majority of languages. More recently, word clusters induced using the Brown clustering algorithm have been shown to be highly effective in encoding syntactic information (Ciosici et al., 2019) and have been used for unsupervised Part-of-Speech tagging (Cardenas et al., 2019). Brown clustering (Brown et al., 1992) is a commonly used word clustering algorithm, performing a bottom-up, windowed, hard hierarchical clustering based on the global objective of maximized Average Mutual Information (AMI), which is equivalent to the Maximum Likelihood Estimate (MLE) of the underlying language model (Martin et al., 1998). The tension between local merges and the global optimization goal makes the algorithm parallelize hard. Simultaneously, the number of merges considered at any one time directly affects the quality of the final word clustering (Derczynski and Chester, 2016)."
2020.lrec-1.303,N16-1031,0,0.0186618,"erated method match the performance of clusters computed using the original methods. Keywords: word clusters, word representations, efficient computation 1. Introduction Word clusters have been successfully used in NLP tasks over the past decades, contributing to advances especially in machine translation (Brown et al., 1993; Auli et al., 2013), named entity recognition (Ratinov and Roth, 2009; Ritter et al., 2011), parsing (Koo et al., 2008; Kong et al., 2014), and processing noisy text (Owoputi et al., 2012). They remain a competitive representation useful for many tasks (Dyer et al., 2016; Choi, 2016; Lukasik et al., 2015), yielding superior extrinsic performance in particular when limited data is available (Qu et al., 2015) – which is the case for the majority of languages. More recently, word clusters induced using the Brown clustering algorithm have been shown to be highly effective in encoding syntactic information (Ciosici et al., 2019) and have been used for unsupervised Part-of-Speech tagging (Cardenas et al., 2019). Brown clustering (Brown et al., 1992) is a commonly used word clustering algorithm, performing a bottom-up, windowed, hard hierarchical clustering based on the global"
2020.lrec-1.303,N19-1157,1,0.890819,"l., 2013), named entity recognition (Ratinov and Roth, 2009; Ritter et al., 2011), parsing (Koo et al., 2008; Kong et al., 2014), and processing noisy text (Owoputi et al., 2012). They remain a competitive representation useful for many tasks (Dyer et al., 2016; Choi, 2016; Lukasik et al., 2015), yielding superior extrinsic performance in particular when limited data is available (Qu et al., 2015) – which is the case for the majority of languages. More recently, word clusters induced using the Brown clustering algorithm have been shown to be highly effective in encoding syntactic information (Ciosici et al., 2019) and have been used for unsupervised Part-of-Speech tagging (Cardenas et al., 2019). Brown clustering (Brown et al., 1992) is a commonly used word clustering algorithm, performing a bottom-up, windowed, hard hierarchical clustering based on the global objective of maximized Average Mutual Information (AMI), which is equivalent to the Maximum Likelihood Estimate (MLE) of the underlying language model (Martin et al., 1998). The tension between local merges and the global optimization goal makes the algorithm parallelize hard. Simultaneously, the number of merges considered at any one time direct"
2020.lrec-1.303,N16-1139,0,0.0351678,"Missing"
2020.lrec-1.303,W15-4306,1,0.801916,"of items (i.e. |V |, which is typically large for NLP applications). This behavior is preserved in larger corpora, see Table 2 for the full RCV corpus of 114M tokens. 5.2. Downstream performance As higher AMI can result from overfitting of the language model to the training corpus, we supplement the experiment with extrinsic downstream evaluation in Named Entity Recognition (NER). The aim is to compare the performance of Brown clusters with that of hybrid clusters in a downstream task to study the information encoded. We perform NER using a classifier based on Conditional Random Fields (CRF) (Derczynski et al., 2015a) with per token cluster bitstring IDs from the cluster hierarchy and character skip-2-gram features over the English newswire train and test splits of the CoNLL 2003 shared task (Tjong c= Brown Hybrid 10 0.565 0.694 40 1.143 1.271 160 1.671 1.747 640 2.154 2.189 2560 2.631 2.650 Table 2: Peak AMI for Exchange and Brown over the complete RCV1 dataset, varying c; i = 10. c= Brown Hybrid 10 72.86 72.44 40 73.41 74.04 160 73.99 74.08 320 74.72 74.98 640 74.60 75.35 2560 74.32 74.16 Table 3: Extrinsic F1 on CoNLL 2003 NER. Kim Sang and De Meulder, 2003). We choose this simple setup in order to fo"
2020.lrec-1.303,R15-1016,1,0.824743,"of items (i.e. |V |, which is typically large for NLP applications). This behavior is preserved in larger corpora, see Table 2 for the full RCV corpus of 114M tokens. 5.2. Downstream performance As higher AMI can result from overfitting of the language model to the training corpus, we supplement the experiment with extrinsic downstream evaluation in Named Entity Recognition (NER). The aim is to compare the performance of Brown clusters with that of hybrid clusters in a downstream task to study the information encoded. We perform NER using a classifier based on Conditional Random Fields (CRF) (Derczynski et al., 2015a) with per token cluster bitstring IDs from the cluster hierarchy and character skip-2-gram features over the English newswire train and test splits of the CoNLL 2003 shared task (Tjong c= Brown Hybrid 10 0.565 0.694 40 1.143 1.271 160 1.671 1.747 640 2.154 2.189 2560 2.631 2.650 Table 2: Peak AMI for Exchange and Brown over the complete RCV1 dataset, varying c; i = 10. c= Brown Hybrid 10 72.86 72.44 40 73.41 74.04 160 73.99 74.08 320 74.72 74.98 640 74.60 75.35 2560 74.32 74.16 Table 3: Extrinsic F1 on CoNLL 2003 NER. Kim Sang and De Meulder, 2003). We choose this simple setup in order to fo"
2020.lrec-1.303,N16-1024,0,0.0186487,"ined with the accelerated method match the performance of clusters computed using the original methods. Keywords: word clusters, word representations, efficient computation 1. Introduction Word clusters have been successfully used in NLP tasks over the past decades, contributing to advances especially in machine translation (Brown et al., 1993; Auli et al., 2013), named entity recognition (Ratinov and Roth, 2009; Ritter et al., 2011), parsing (Koo et al., 2008; Kong et al., 2014), and processing noisy text (Owoputi et al., 2012). They remain a competitive representation useful for many tasks (Dyer et al., 2016; Choi, 2016; Lukasik et al., 2015), yielding superior extrinsic performance in particular when limited data is available (Qu et al., 2015) – which is the case for the majority of languages. More recently, word clusters induced using the Brown clustering algorithm have been shown to be highly effective in encoding syntactic information (Ciosici et al., 2019) and have been used for unsupervised Part-of-Speech tagging (Cardenas et al., 2019). Brown clustering (Brown et al., 1992) is a commonly used word clustering algorithm, performing a bottom-up, windowed, hard hierarchical clustering based on"
2020.lrec-1.303,D14-1108,0,0.0115822,"ll as a number of methods to accelerate the computation of both hierarchical and flat clusters. We show empirically that clusters obtained with the accelerated method match the performance of clusters computed using the original methods. Keywords: word clusters, word representations, efficient computation 1. Introduction Word clusters have been successfully used in NLP tasks over the past decades, contributing to advances especially in machine translation (Brown et al., 1993; Auli et al., 2013), named entity recognition (Ratinov and Roth, 2009; Ritter et al., 2011), parsing (Koo et al., 2008; Kong et al., 2014), and processing noisy text (Owoputi et al., 2012). They remain a competitive representation useful for many tasks (Dyer et al., 2016; Choi, 2016; Lukasik et al., 2015), yielding superior extrinsic performance in particular when limited data is available (Qu et al., 2015) – which is the case for the majority of languages. More recently, word clusters induced using the Brown clustering algorithm have been shown to be highly effective in encoding syntactic information (Ciosici et al., 2019) and have been used for unsupervised Part-of-Speech tagging (Cardenas et al., 2019). Brown clustering (Brow"
2020.lrec-1.303,P08-1068,0,0.0914792,"e clustering as well as a number of methods to accelerate the computation of both hierarchical and flat clusters. We show empirically that clusters obtained with the accelerated method match the performance of clusters computed using the original methods. Keywords: word clusters, word representations, efficient computation 1. Introduction Word clusters have been successfully used in NLP tasks over the past decades, contributing to advances especially in machine translation (Brown et al., 1993; Auli et al., 2013), named entity recognition (Ratinov and Roth, 2009; Ritter et al., 2011), parsing (Koo et al., 2008; Kong et al., 2014), and processing noisy text (Owoputi et al., 2012). They remain a competitive representation useful for many tasks (Dyer et al., 2016; Choi, 2016; Lukasik et al., 2015), yielding superior extrinsic performance in particular when limited data is available (Qu et al., 2015) – which is the case for the majority of languages. More recently, word clusters induced using the Brown clustering algorithm have been shown to be highly effective in encoding syntactic information (Ciosici et al., 2019) and have been used for unsupervised Part-of-Speech tagging (Cardenas et al., 2019). Br"
2020.lrec-1.303,D15-1311,0,0.0137233,"d match the performance of clusters computed using the original methods. Keywords: word clusters, word representations, efficient computation 1. Introduction Word clusters have been successfully used in NLP tasks over the past decades, contributing to advances especially in machine translation (Brown et al., 1993; Auli et al., 2013), named entity recognition (Ratinov and Roth, 2009; Ritter et al., 2011), parsing (Koo et al., 2008; Kong et al., 2014), and processing noisy text (Owoputi et al., 2012). They remain a competitive representation useful for many tasks (Dyer et al., 2016; Choi, 2016; Lukasik et al., 2015), yielding superior extrinsic performance in particular when limited data is available (Qu et al., 2015) – which is the case for the majority of languages. More recently, word clusters induced using the Brown clustering algorithm have been shown to be highly effective in encoding syntactic information (Ciosici et al., 2019) and have been used for unsupervised Part-of-Speech tagging (Cardenas et al., 2019). Brown clustering (Brown et al., 1992) is a commonly used word clustering algorithm, performing a bottom-up, windowed, hard hierarchical clustering based on the global objective of maximized"
2020.lrec-1.303,K15-1009,0,0.119572,"entations, efficient computation 1. Introduction Word clusters have been successfully used in NLP tasks over the past decades, contributing to advances especially in machine translation (Brown et al., 1993; Auli et al., 2013), named entity recognition (Ratinov and Roth, 2009; Ritter et al., 2011), parsing (Koo et al., 2008; Kong et al., 2014), and processing noisy text (Owoputi et al., 2012). They remain a competitive representation useful for many tasks (Dyer et al., 2016; Choi, 2016; Lukasik et al., 2015), yielding superior extrinsic performance in particular when limited data is available (Qu et al., 2015) – which is the case for the majority of languages. More recently, word clusters induced using the Brown clustering algorithm have been shown to be highly effective in encoding syntactic information (Ciosici et al., 2019) and have been used for unsupervised Part-of-Speech tagging (Cardenas et al., 2019). Brown clustering (Brown et al., 1992) is a commonly used word clustering algorithm, performing a bottom-up, windowed, hard hierarchical clustering based on the global objective of maximized Average Mutual Information (AMI), which is equivalent to the Maximum Likelihood Estimate (MLE) of the un"
2020.lrec-1.303,W09-1119,0,0.217007,"tations of Brown clustering and the alternative Exchange clustering as well as a number of methods to accelerate the computation of both hierarchical and flat clusters. We show empirically that clusters obtained with the accelerated method match the performance of clusters computed using the original methods. Keywords: word clusters, word representations, efficient computation 1. Introduction Word clusters have been successfully used in NLP tasks over the past decades, contributing to advances especially in machine translation (Brown et al., 1993; Auli et al., 2013), named entity recognition (Ratinov and Roth, 2009; Ritter et al., 2011), parsing (Koo et al., 2008; Kong et al., 2014), and processing noisy text (Owoputi et al., 2012). They remain a competitive representation useful for many tasks (Dyer et al., 2016; Choi, 2016; Lukasik et al., 2015), yielding superior extrinsic performance in particular when limited data is available (Qu et al., 2015) – which is the case for the majority of languages. More recently, word clusters induced using the Brown clustering algorithm have been shown to be highly effective in encoding syntactic information (Ciosici et al., 2019) and have been used for unsupervised P"
2020.lrec-1.303,D11-1141,0,0.0455826,"Missing"
2020.lrec-1.303,rose-etal-2002-reuters,0,0.075843,"ring Brown, Exchange, or the hybrid method with other word representations is out of scope for this work. For comparisons of Brown or Exchange with other word representation methods, in particular word vectors, see Bansal et al. (2014), and Qu et al. (2015); for characterization of syntactic information encoded by Brown clusters see Ciosici et al. (2019). 5. 5.1. Results and Analysis Computation and AMI performance We measure the reduction in computation time and improvement in language modeling. For this first experiment we focus on English, using one million words from Reuters Corpus (RCV) (Rose et al., 2002). We use a machine with dual Intel 8176 and 512GB RAM. Runtime results (mean of three runs) are presented in Table 1. Our hybrid method provides a higher peak AMI and much lower run-time than Brown, in every case. The gap in AMI closes as the number of clusters generated rises. This is expected; the information present in a clustering rises as the number of clusters increases from 1, though drops as the number of clusters approaches the number of items (i.e. |V |, which is typically large for NLP applications). This behavior is preserved in larger corpora, see Table 2 for the full RCV corpus o"
2020.lrec-1.303,W03-0419,0,0.147556,"Missing"
2020.lrec-1.303,P08-1086,0,0.230201,"a windowed approximation (Brown et al., 1992), as well as lack of fast implementations for the community to use. This is partially due to the use of a global rather than local metric as the objective agglomerative clustering function. In fact, the only available open-source implementation of Brown is over 15 years old (Liang, 2005) and does not include an implementation of Exchange as per the original paper. Most research aimed at runtime performance of Brown so far is limited to relaxations of the underlying language model in the interest of speed (Dehdari et al., 2016; Stratos et al., 2014; Uszkoreit and Brants, 2008). In this paper, we demonstrate that Brown and Exchange can be combined to speed up computation of Brown clusters while yielding similar quality clusters, in less time, and retaining the tree-based features of the generally slower and more involved Brown algorithm. An added advantage is the ability to move away from the local maxima Brown’s greedy algorithm is prone to. We further retro-fit Exchange with stochastic merging, to allow escape from local maxima in Exchange. We contribute our code to the NLP community as the only fast implementation of both Brown and Exchange. It is written in mode"
2020.lrec-1.430,L18-1008,0,0.162838,"large corpus of news samples. Secondly, they use word2vec (Mikolov et al., 2013) to generate word embeddings using their own corpus of text samples. We use both approaches. Both the pretrained and word2vec models represent each word as a 200 dimensional distributed real number vector. Lastly, they develop a comment2vec model Le and Mikolov (2014). Their results show that the comment2vec and the word2vec models provide the most predictive features (Nobata et al., 2016). Badjatiya et al. (2017) experiment with pre-trained GloVe embeddings (Pennington et al., 2014), learned FastText embeddings (Mikolov et al., 2018), and randomly initialized learned embeddings; interestingly, the randomly initialized embeddings slightly outperform the others (Badjatiya et al., 2017). Sentiment Scores Sentiment scores are a common feature in systems dealing with offensive and hateful speech. We experiment with these by including these scores as features in some models. To compute these sentiment score features our systems use two libraries: VADER (Hutto and Gilbert, 2014) and AFINN (Nielsen, 2011). Our models use the compound attribute, giving a normalized sum of sentiment scores over all words in the sample. This ranges"
2020.lrec-1.430,D14-1162,0,0.0881415,"Missing"
2020.lrec-1.430,W17-1101,0,0.680958,"ocial media platforms has been shown to be in correlation with hate crimes in real life settings (Müller and Schwarz, 2018). It can be quite hard to distinguish between generally offensive language and hate speech as few universal definitions exist (Davidson et al., 2017). There does, however, seem to be a general consensus that hate speech can be defined as language that targets a group with the intent to be harmful or to cause social chaos. This targeting is usually done on the basis of some characteristics such as race, color, ethnicity, gender, sexual orientation, nationality or religion (Schmidt and Wiegand, 2017). Offensive language, on the other hand, is a more general category containing any type of profanity or insult. Hate speech can, therefore, be classified as a subset of offensive language. (Zampieri et al., 2019a) propose guidelines for classifying offensive language as well as the type and the target of offensive language. These guidelines capture the characteristics of generally offensive language, hate speech and other types of targeted offensive language such as cyberbullying. However, despite offensive language detection being a burgeoning field, no dataset yet exists for Danish (Kirkedal"
2020.lrec-1.430,P16-1162,0,0.00614146,"d insults such as WalkAwayFromAllDemocrats. It seems to rely too highly on the presence of profanity, misclassifying samples containing terms such as bitch, fuck, shit, etc. as targeted insults. The issue of the data quality is also concerning in this sub-task, as we discover samples containing clear targeted insults such as HillaryForPrison being labeled as untargeted in the test set. This fairly typical use mode of social media (Derczynski et al., 2013) needs some extra effort to handle, as might be afforded by tokenization at the level of hashtags (Maynard and Greenwood, 2014) or subwords (Sennrich et al., 2016). Figure 2 (a) shows the model fits aggressively after just 10 epochs while the training loss approaches zero. A possible 7.3. Task C - Target identification Misclassification based on obfuscated terms as discussed earlier also seems to be an issue for sub-task C. This problem of obfuscated terms could be tackled by introducing character-level features such as character level n-grams. Figure 3 shows training and validation loss curves for each language. There are no indications of early overfitting for English. The classifier Danish behaves similarly to the Learned-BiLSTM classifier for Englis"
2020.lrec-1.430,R15-1086,0,0.0393976,"Missing"
2020.lrec-1.430,W19-3509,0,0.0297242,"latforms and its implications has been gaining attention over the last couple of years. This interest is sparked by the fact that many of the online social media platforms have come under scrutiny on how this type of content should be detected and dealt with. It is, however, far from trivial to deal with this type of language directly due to the gigantic amount of user-generated content created every day. For this reason, automatic methods are required, using natural language processing (NLP) and machine learning techniques. The task of finding these poses a pressing and formidable challenge (Vidgen et al., 2019). Given the fact that the research on offensive language detection has to a large extent been focused on the English language, we set out to explore the design of models that can successfully be used for both English and Danish. To accomplish this, an appropriate dataset must be constructed, annotated with the guidelines described in Zampieri et al. (2019a). We furthermore set out to analyze relevant linguistic features by analyzing the patterns that prove hard to detect. 2. Background Offensive language varies greatly, ranging from simple profanity to much more severe types of language. One o"
2020.lrec-1.430,N16-2013,0,0.0605495,"ion schemas and the definitions used, since it makes it difficult to effectively compare results to existing work, as pointed out by several authors (Nobata et al., 2016; Schmidt and Wiegand, 2017; Waseem et al., 2017; Zampieri et al., 2019a). These issues become clear when comparing the work of Van Hee et al. (2015b), where racist and sexist remarks are classified as a subset of insults, to the work of Nobata et al. (2016), where similar remarks are split into two categories; hate speech and derogatory language. Another clear example of conflicting definitions becomes visible when comparing (Waseem and Hovy, 2016), where hate speech is considered without any consideration of overlaps with the more general type of offensive language, to (Davidson et al., 2017) where a clear distinction is made between the two, by classifying posts as either Hate speech, Offensive or Neither. This lack of consensus led (Waseem et al., 2017) to propose annotation guidelines and introduce a typology. (Zampieri et al., 2019b) argue that these guidelines do not effectively capture both the type and target of the offensive language. 3. Dataset In this section we give a comprehensive overview of the structure of the task and d"
2020.lrec-1.430,W17-3012,0,0.37415,"be able to effectively distinguish between generally offensive language and the more severe hate speech. The dataset is constructed by gathering data from Twitter, using a hate speech lexicon to query the data with crowdsourced annotations. 2.3. Contradicting definitions It becomes clear that one of the key challenges in doing meaningful research on the topic are the differences in both the annotation schemas and the definitions used, since it makes it difficult to effectively compare results to existing work, as pointed out by several authors (Nobata et al., 2016; Schmidt and Wiegand, 2017; Waseem et al., 2017; Zampieri et al., 2019a). These issues become clear when comparing the work of Van Hee et al. (2015b), where racist and sexist remarks are classified as a subset of insults, to the work of Nobata et al. (2016), where similar remarks are split into two categories; hate speech and derogatory language. Another clear example of conflicting definitions becomes visible when comparing (Waseem and Hovy, 2016), where hate speech is considered without any consideration of overlaps with the more general type of offensive language, to (Davidson et al., 2017) where a clear distinction is made between the"
2020.lrec-1.430,N19-1144,0,0.122922,"amount of user-generated content created every day. For this reason, automatic methods are required, using natural language processing (NLP) and machine learning techniques. The task of finding these poses a pressing and formidable challenge (Vidgen et al., 2019). Given the fact that the research on offensive language detection has to a large extent been focused on the English language, we set out to explore the design of models that can successfully be used for both English and Danish. To accomplish this, an appropriate dataset must be constructed, annotated with the guidelines described in Zampieri et al. (2019a). We furthermore set out to analyze relevant linguistic features by analyzing the patterns that prove hard to detect. 2. Background Offensive language varies greatly, ranging from simple profanity to much more severe types of language. One of the more troublesome types of language is hate speech and the presence of hate speech on social media platforms has been shown to be in correlation with hate crimes in real life settings (Müller and Schwarz, 2018). It can be quite hard to distinguish between generally offensive language and hate speech as few universal definitions exist (Davidson et al."
2020.lrec-1.430,S19-2010,0,0.0786723,"amount of user-generated content created every day. For this reason, automatic methods are required, using natural language processing (NLP) and machine learning techniques. The task of finding these poses a pressing and formidable challenge (Vidgen et al., 2019). Given the fact that the research on offensive language detection has to a large extent been focused on the English language, we set out to explore the design of models that can successfully be used for both English and Danish. To accomplish this, an appropriate dataset must be constructed, annotated with the guidelines described in Zampieri et al. (2019a). We furthermore set out to analyze relevant linguistic features by analyzing the patterns that prove hard to detect. 2. Background Offensive language varies greatly, ranging from simple profanity to much more severe types of language. One of the more troublesome types of language is hate speech and the presence of hate speech on social media platforms has been shown to be in correlation with hate crimes in real life settings (Müller and Schwarz, 2018). It can be quite hard to distinguish between generally offensive language and hate speech as few universal definitions exist (Davidson et al."
2020.semeval-1.188,2020.semeval-1.206,0,0.0947199,"Missing"
2020.semeval-1.188,C18-1139,0,0.018965,"ømberg-Derczynski et al., 2020), etc. 4 1428 Many teams also used context-independent embeddings from word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), including language-specific embeddings such as Mazajak (Farha and Magdy, 2019) for Arabic. Some teams used other techniques: word n-grams, character n-grams, lexicons for sentiment analysis, and lexicon of offensive words. Other representations included emoji priors extracted from the weakly supervised SOLID dataset for English, and sentiment analysis using NLTK (Bird et al., 2009), Vader (Hutto and Gilbert, 2014), and FLAIR (Akbik et al., 2018). Machine learning models In terms of machine learning models, most teams used some kind of pretrained Transformers: typically BERT, but RoBERTa, XLM-RoBERTa (Conneau et al., 2020), ALBERT (Lan et al., 2019), and GPT-2 (Radford et al., 2019) were also popular. Other popular models included CNNs (Fukushima, 1980), RNNs (Rumelhart et al., 1986), and GRUs (Cho et al., 2014). Older models such as SVMs (Cortes and Vapnik, 1995) were also used, typically as part of ensembles. 5 English Track A total of 87 teams made submissions for the English track (23 of them participated in the 2019 edition of th"
2021.acl-long.247,2020.semeval-1.206,0,0.0168356,"guage understanding in Danish, finetuned on our dataset. Model: We follow the suggested parameters from Mosbach et al. (2020) for fine-tuning (learning rate 2e-5, weight decay 0.01, AdamW optimizer without bias correction). Class imbalance is handled by weighted sampling and data split for train/test 80/20. Experiments are conducted with batch size 32 using Tesla V100 GPU. Preprocessing: Our initial pre-processing of the unstrucutured posts included converting emojis to text, url replacement, limit @USER and punctuation occurrences and adding special tokens for upper case letters adopted from Ahn et al. (2020). Classification: Since the effect of applying multitask-learning might not conditionally improve performance (Mulki and Ghanem, 2021), the classification is evaluated on a subset of the dataset for each subtask (see Table 6) including all posts of the target label (e.g. misogyny) and stratified sampling of the non-target classes (e.g. for non-misogynistic: abusive and non-abusive posts) with 10k posts for each experiment. Results are reported when the model reached stabilized per class f1 scores for all classes on the test set (± 0.01/20). The results indicate the expected challenge of accura"
2021.acl-long.247,2020.alw-1.21,0,0.0210083,"annotation, Waseem (2016) show that the quality of amateur annotators is competitive with expert annotations when several amateurs agree. Facing the trade-off between training annotators intensely and the number of involved annotators, we continued with the trained annotators and group discussions/ individual revisions for flagged content and disagreements (Section 5.4). 3.3 Mitigating Biases Prior work demonstrates that biases in datasets can occur through the training and selection of annotators or selection of posts to annotate (Geva et al., 2019; Wiegand et al., 2019; Sap et al., 2019; Al Kuwatly et al., 2020; Ousidhoum et al., 2020). Selection biases: Selection biases for abusive language can be seen in the sampling of text, for instance when using keyword search (Wiegand et al., 2019), topic dependency (Ousidhoum et al., 2020), users (Wiegand et al., 2019), domain (Wiegand et al., 2019), time (Florio et al., 2020) and lack of linguistic variety (Vidgen and Derczynski, 2020). Label biases: Label biases can be caused by, for instance, non-representative annotator selection, lack in training/domain expertise, preconceived notions, or pre-held stereotypes. These biases are treated in relation to abu"
2021.acl-long.247,J08-4004,0,0.203801,"of special character and stopwords, notion:(token, tf-idf) 5.4 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 Inter-Annotator Agreement (IAA) discussion round 69 (+125 pilot) Table 5: Solved disagreements/flagged content The final overall Fleiss’ Kappa (Fleiss (1971)) for individual subtasks are: abusive/not: 0.58, targeted: 0.54, misogyny/not: 0.54. It is notable here that the dataset is significantly more skewed than prior work which upsampled to 1:1 class balances. Chance-corrected measurements are sensitive to agreement on rare categories and higher agreement is needed to reach reliability, as shown in Artstein and Poesio (2008). 5.5 Table 4: Top-3 word frequencies group solv. 169 Annotator disagreement analysis Based on the discussion rounds, the following types of posts were the most challenging to annotate: 1. Interpretation of the author’s intention (irony, sarcasm, jokes, and questions) E.g. Haha! Virksomheder i Danmark: Vi ansætter per iteration accumelated aldrig en kvinde igen... (Haha! Companies in Denmark: We will never hire a woman again ...) 0.65 0.61 0.4 0 0.7 0.7 0.71 0.71 (I don’t believe sexism and sexual liberalism are the 0.46 5 sexisme og seksuelt frisind er da vist ikke det samme? same?) 10 15 20"
2021.acl-long.247,2020.alw-1.16,0,0.0303377,"ampieri et al. (2019) lang. da,en, gr,ar,tu Waseem and Hovy (2016) Anzovino et al. (2018) en en,it,es Jha and Mamidi (2017) en labels Offensive (OFF)/Not offensive (NOT) Targeted Insult (TIN)/Untargeted (UNT)/ Individual (IND)/Group (GRP)/Other (OTH) Sexism, Racism Discredit, Stereotype, Objectification, Sexual Harassm., Dominance, Derailing Benevolent extension Table 2: Established taxonomies and their use for the misogyny detection task and mapping entities in a way that demonstrates their natural relationship, e.g. Schmidt and Wiegand (2017); Anzovino et al. (2018); Zampieri et al. (2019); Banko et al. (2020). Their application is especially clear in shared tasks, as for multilingual sexism detection against women, SemEval 2019 (Basile et al., 2019). On one hand, it should be an aim of a taxonomy that it is easily understandable and applicable for annotators from various background and with different expertise levels. On the other hand, a taxonomy is only useful if it is also correct and comprehensive, i.e. a good representation of the world. Therefore, we have aimed to integrate definitions from several sources of previous research (deductive approach) as well as categories resulting from discuss"
2021.acl-long.247,2020.lrec-1.175,0,0.0320348,"detecting and defining abuse, but there exists no objective framing for what constitutes abuse and what does not. In this work, we focus on a specific category of online abuse, namely misogyny. 2.1 Online misogyny and existing datasets Misogyny can be categorised as a subbranch of hate speech and is described as hateful content targeting women (Waseem, 2016). The degree of toxicity depends on complicated subjective measures, for instance, the receiver’s perception of the dialect of the speaker (Sap et al., 2019). Annotating misogyny typically requires more than a binary present/absent label. Chiril et al. (2020), for instance, use three categories to classify misogyny in French: direct sexist content (directly addressed to a woman or a group of women), descriptive sexist content (describing a woman or women in general) or reporting sexist content (a report of a sexism experience or a denunciation of a sexist behaviour). This categorization does not, however, specify the type of misogyny. Jha and Mamidi (2017) distinguish between harsh and benevolvent sexism, building on the data from the work of Waseem and Hovy (2016). While harsh sexism (hateful or negative views of women) is the more recognized typ"
2021.acl-long.247,P19-1271,0,0.0402705,"Missing"
2021.acl-long.247,W19-3504,0,0.0133922,"nd et al., 2019), time (Florio et al., 2020) and lack of linguistic variety (Vidgen and Derczynski, 2020). Label biases: Label biases can be caused by, for instance, non-representative annotator selection, lack in training/domain expertise, preconceived notions, or pre-held stereotypes. These biases are treated in relation to abusive language datasets by several sources, e.g. general sampling and annotators biases (Waseem, 2016; Al Kuwatly et al., 2020), biases towards minority identity mentions based for example on gender or race (Davidson et al., 2017; Dixon et al., 2018; Park et al., 2018; Davidson et al., 2019), and political annotator biases (Wich et al., 2020). Other qualitative biases comprise, for instance, demographic bias, over-generalization, topic exposure as social biases (Hovy and Spruit, 2016). Systematic measurement of biases in datasets remains an open research problem. Friedman and Nissenbaum (1996) discuss “freedom from biases” as an ideal for good computer systems, and state that methods applied during data creation influence the quality of the resulting dataset quality with which systems are later trained. Shah et al. (2020) showed that half of biases are caused by the methodology d"
2021.acl-long.247,P16-2096,0,0.0735075,"Missing"
2021.acl-long.247,D19-1174,0,0.0660763,"tegories: Discredit, Harassment & Threats of Violence, Derailing, Stereotype & Objectification, and Dominance. They also distinguish between if the abuse is active or passive towards the target. These labels appear to apply well to other languages, and quantitative representation of labels differ by language. For example, Spanish shows a stronger presence of Dominance, Italian of Stereotype & Objectification, and English of Discredit. As we see variance across languages, building terminology for labeling misogyny correctly is therefore a key challenge in being able to detect it automatically. Parikh et al. (2019) take a multi-label approach to categorizing posts from the “Everyday Sexism Project”, where as many as 23 different categories are not mutually exclusive. The types of sexism identified in their dataset include body shaming, gaslighting, and mansplaining. While the categories of this work are extremely detailed and socially useful, several studies have demonstrated the challenge for human annotators to use labels that are intuitively unclear (Chatzakou et al., 2017; Vidgen et al., 2019) or closely related to each other (Founta et al., 2018). Guest et al. (2021) suggest a novel taxonomy for mi"
2021.acl-long.247,D18-1302,0,0.0151094,"019), domain (Wiegand et al., 2019), time (Florio et al., 2020) and lack of linguistic variety (Vidgen and Derczynski, 2020). Label biases: Label biases can be caused by, for instance, non-representative annotator selection, lack in training/domain expertise, preconceived notions, or pre-held stereotypes. These biases are treated in relation to abusive language datasets by several sources, e.g. general sampling and annotators biases (Waseem, 2016; Al Kuwatly et al., 2020), biases towards minority identity mentions based for example on gender or race (Davidson et al., 2017; Dixon et al., 2018; Park et al., 2018; Davidson et al., 2019), and political annotator biases (Wich et al., 2020). Other qualitative biases comprise, for instance, demographic bias, over-generalization, topic exposure as social biases (Hovy and Spruit, 2016). Systematic measurement of biases in datasets remains an open research problem. Friedman and Nissenbaum (1996) discuss “freedom from biases” as an ideal for good computer systems, and state that methods applied during data creation influence the quality of the resulting dataset quality with which systems are later trained. Shah et al. (2020) showed that half of biases are cau"
2021.acl-long.247,W19-6141,1,0.867359,"Missing"
2021.acl-long.247,W18-4401,0,0.0470972,"Missing"
2021.acl-long.247,P19-1163,0,0.113423,"nd Waseem et al. (2017). Research in this field has produced both data, taxonomies, and methods for detecting and defining abuse, but there exists no objective framing for what constitutes abuse and what does not. In this work, we focus on a specific category of online abuse, namely misogyny. 2.1 Online misogyny and existing datasets Misogyny can be categorised as a subbranch of hate speech and is described as hateful content targeting women (Waseem, 2016). The degree of toxicity depends on complicated subjective measures, for instance, the receiver’s perception of the dialect of the speaker (Sap et al., 2019). Annotating misogyny typically requires more than a binary present/absent label. Chiril et al. (2020), for instance, use three categories to classify misogyny in French: direct sexist content (directly addressed to a woman or a group of women), descriptive sexist content (describing a woman or women in general) or reporting sexist content (a report of a sexism experience or a denunciation of a sexist behaviour). This categorization does not, however, specify the type of misogyny. Jha and Mamidi (2017) distinguish between harsh and benevolvent sexism, building on the data from the work of Wase"
2021.acl-long.247,W17-1101,0,0.0266438,"he purpose is categorizing Abusive Language Hate speech Misogyny reference Zampieri et al. (2019) lang. da,en, gr,ar,tu Waseem and Hovy (2016) Anzovino et al. (2018) en en,it,es Jha and Mamidi (2017) en labels Offensive (OFF)/Not offensive (NOT) Targeted Insult (TIN)/Untargeted (UNT)/ Individual (IND)/Group (GRP)/Other (OTH) Sexism, Racism Discredit, Stereotype, Objectification, Sexual Harassm., Dominance, Derailing Benevolent extension Table 2: Established taxonomies and their use for the misogyny detection task and mapping entities in a way that demonstrates their natural relationship, e.g. Schmidt and Wiegand (2017); Anzovino et al. (2018); Zampieri et al. (2019); Banko et al. (2020). Their application is especially clear in shared tasks, as for multilingual sexism detection against women, SemEval 2019 (Basile et al., 2019). On one hand, it should be an aim of a taxonomy that it is easily understandable and applicable for annotators from various background and with different expertise levels. On the other hand, a taxonomy is only useful if it is also correct and comprehensive, i.e. a good representation of the world. Therefore, we have aimed to integrate definitions from several sources of previous resea"
2021.acl-long.247,2020.acl-main.468,0,0.0153428,"n et al., 2017; Dixon et al., 2018; Park et al., 2018; Davidson et al., 2019), and political annotator biases (Wich et al., 2020). Other qualitative biases comprise, for instance, demographic bias, over-generalization, topic exposure as social biases (Hovy and Spruit, 2016). Systematic measurement of biases in datasets remains an open research problem. Friedman and Nissenbaum (1996) discuss “freedom from biases” as an ideal for good computer systems, and state that methods applied during data creation influence the quality of the resulting dataset quality with which systems are later trained. Shah et al. (2020) showed that half of biases are caused by the methodology design, and presented a first approach of classifying a broad range of predictive biases under one umbrella in NLP. We applied several measures to mitigate biases occurring through the annotation design and execution: First, we selected labels grounded in existing, peer-reviewed research from more than one field. Second, we aimed for diversity in annotator profiles in terms of age, gender, dialect, and background. Third, we recruited a facilitator with a background in ethnographic studies and provided intense annotator training. Fourth,"
2021.acl-long.247,W19-3609,0,0.0327509,"ation does not, however, specify the type of misogyny. Jha and Mamidi (2017) distinguish between harsh and benevolvent sexism, building on the data from the work of Waseem and Hovy (2016). While harsh sexism (hateful or negative views of women) is the more recognized type of sexism, benevolent sexism (“a subjectively positive view towards men or women”), often exemplified as a compliment using a positive stereotypical picture, is still discriminating (Glick and Fiske, 1996). Other categorisations of harassment towards women have distinguished between physical, sexual and indirect occurrences (Sharifirad and Jacovi, 2019). Anzovino et al. (2018) classify misogyny more segregated in five subcategories: Discredit, Harassment & Threats of Violence, Derailing, Stereotype & Objectification, and Dominance. They also distinguish between if the abuse is active or passive towards the target. These labels appear to apply well to other languages, and quantitative representation of labels differ by language. For example, Spanish shows a stronger presence of Dominance, Italian of Stereotype & Objectification, and English of Discredit. As we see variance across languages, building terminology for labeling misogyny correctly"
2021.acl-long.247,2020.lrec-1.430,1,0.844745,"Missing"
2021.acl-long.247,K19-1088,0,0.0343432,"Missing"
2021.acl-long.247,W19-3509,0,0.256711,"7). Automatic detection of abusive language can help identify and report harmful accounts and acts, and allows counter narratives (Chung et al., 2019; Garland et al., 2020; Ziems et al., 2020). Due to the volume of online text and the mental impact on humans who are employed to moderate online abusive language - moderators of abusive online content have been shown to develop serious PTSD and depressive symptoms (Casey Newton, 2020) it is urgent to develop systems to automate the detection and moderation of online abusive language. Automatic detection, however, presents significant challenges (Vidgen et al., 2019). Abusive language is linguistically diverse (Vidgen and Derczynski, 2020), both explicitly, in the form of swear words or profanities; implicitly, in the form of sarcasm or humor (Waseem et al., 2017); and subtly, in the form of attitudes and opinions. Recognizing distinctions between variants of misogyny is challenging for humans, let alone computers. Systems for automatic detection are usually created using labeled training data (Kiritchenko et al., 2020), hence, their performance depends on the quality and representativity of the available datasets and their labels. We currently lack trans"
2021.acl-long.247,W16-5618,0,0.276107,"ruins from the Roman empire (Wallace, 2005). Automatic processing of abusive text is far more recent, early work including e.g. Davidson et al. (2017) and Waseem et al. (2017). Research in this field has produced both data, taxonomies, and methods for detecting and defining abuse, but there exists no objective framing for what constitutes abuse and what does not. In this work, we focus on a specific category of online abuse, namely misogyny. 2.1 Online misogyny and existing datasets Misogyny can be categorised as a subbranch of hate speech and is described as hateful content targeting women (Waseem, 2016). The degree of toxicity depends on complicated subjective measures, for instance, the receiver’s perception of the dialect of the speaker (Sap et al., 2019). Annotating misogyny typically requires more than a binary present/absent label. Chiril et al. (2020), for instance, use three categories to classify misogyny in French: direct sexist content (directly addressed to a woman or a group of women), descriptive sexist content (describing a woman or women in general) or reporting sexist content (a report of a sexism experience or a denunciation of a sexist behaviour). This categorization does n"
2021.acl-long.247,W17-3012,0,0.0154197,", 2021), making this study contextually relevant. Further, the lack of language resources available for Danish (Kirkedal et al., 2019) coupled with its lexical complexity (Bleses et al., 2008) make it an intricate research objective for natural language processing. 2 Background and related work Abusive language is as ancient a phenomenon as written language itself. Written profanities and insults about others are found as old as graffiti on ruins from the Roman empire (Wallace, 2005). Automatic processing of abusive text is far more recent, early work including e.g. Davidson et al. (2017) and Waseem et al. (2017). Research in this field has produced both data, taxonomies, and methods for detecting and defining abuse, but there exists no objective framing for what constitutes abuse and what does not. In this work, we focus on a specific category of online abuse, namely misogyny. 2.1 Online misogyny and existing datasets Misogyny can be categorised as a subbranch of hate speech and is described as hateful content targeting women (Waseem, 2016). The degree of toxicity depends on complicated subjective measures, for instance, the receiver’s perception of the dialect of the speaker (Sap et al., 2019). Anno"
2021.acl-long.247,N16-2013,0,0.501984,"019). Annotating misogyny typically requires more than a binary present/absent label. Chiril et al. (2020), for instance, use three categories to classify misogyny in French: direct sexist content (directly addressed to a woman or a group of women), descriptive sexist content (describing a woman or women in general) or reporting sexist content (a report of a sexism experience or a denunciation of a sexist behaviour). This categorization does not, however, specify the type of misogyny. Jha and Mamidi (2017) distinguish between harsh and benevolvent sexism, building on the data from the work of Waseem and Hovy (2016). While harsh sexism (hateful or negative views of women) is the more recognized type of sexism, benevolent sexism (“a subjectively positive view towards men or women”), often exemplified as a compliment using a positive stereotypical picture, is still discriminating (Glick and Fiske, 1996). Other categorisations of harassment towards women have distinguished between physical, sexual and indirect occurrences (Sharifirad and Jacovi, 2019). Anzovino et al. (2018) classify misogyny more segregated in five subcategories: Discredit, Harassment & Threats of Violence, Derailing, Stereotype & Objectif"
2021.acl-long.247,2020.alw-1.7,0,0.0180991,"linguistic variety (Vidgen and Derczynski, 2020). Label biases: Label biases can be caused by, for instance, non-representative annotator selection, lack in training/domain expertise, preconceived notions, or pre-held stereotypes. These biases are treated in relation to abusive language datasets by several sources, e.g. general sampling and annotators biases (Waseem, 2016; Al Kuwatly et al., 2020), biases towards minority identity mentions based for example on gender or race (Davidson et al., 2017; Dixon et al., 2018; Park et al., 2018; Davidson et al., 2019), and political annotator biases (Wich et al., 2020). Other qualitative biases comprise, for instance, demographic bias, over-generalization, topic exposure as social biases (Hovy and Spruit, 2016). Systematic measurement of biases in datasets remains an open research problem. Friedman and Nissenbaum (1996) discuss “freedom from biases” as an ideal for good computer systems, and state that methods applied during data creation influence the quality of the resulting dataset quality with which systems are later trained. Shah et al. (2020) showed that half of biases are caused by the methodology design, and presented a first approach of classifying"
2021.acl-long.247,N19-1060,0,0.015911,"tators intensely and the number of involved annotators, we continued with the trained annotators and group discussions/ individual revisions for flagged content and disagreements (Section 5.4). 3.3 Mitigating Biases Prior work demonstrates that biases in datasets can occur through the training and selection of annotators or selection of posts to annotate (Geva et al., 2019; Wiegand et al., 2019; Sap et al., 2019; Al Kuwatly et al., 2020; Ousidhoum et al., 2020). Selection biases: Selection biases for abusive language can be seen in the sampling of text, for instance when using keyword search (Wiegand et al., 2019), topic dependency (Ousidhoum et al., 2020), users (Wiegand et al., 2019), domain (Wiegand et al., 2019), time (Florio et al., 2020) and lack of linguistic variety (Vidgen and Derczynski, 2020). Label biases: Label biases can be caused by, for instance, non-representative annotator selection, lack in training/domain expertise, preconceived notions, or pre-held stereotypes. These biases are treated in relation to abusive language datasets by several sources, e.g. general sampling and annotators biases (Waseem, 2016; Al Kuwatly et al., 2020), biases towards minority identity mentions based for e"
2021.acl-long.247,N19-1144,0,0.0967924,"ense annotator training. Fourth, we engaged in weekly group discussions, iteratively improving the codebook and integrating edge cases. Fifth, the selection of platforms from which we sampled data is based on local user representation in Denmark, rather than convenience. Sixth, diverse sampling methods for data collection reduced selection biases. 4 A taxonomy and codebook for labeling online misogyny Good language taxonomies systematically bring together definitions and describe general principles of each definition. The purpose is categorizing Abusive Language Hate speech Misogyny reference Zampieri et al. (2019) lang. da,en, gr,ar,tu Waseem and Hovy (2016) Anzovino et al. (2018) en en,it,es Jha and Mamidi (2017) en labels Offensive (OFF)/Not offensive (NOT) Targeted Insult (TIN)/Untargeted (UNT)/ Individual (IND)/Group (GRP)/Other (OTH) Sexism, Racism Discredit, Stereotype, Objectification, Sexual Harassm., Dominance, Derailing Benevolent extension Table 2: Established taxonomies and their use for the misogyny detection task and mapping entities in a way that demonstrates their natural relationship, e.g. Schmidt and Wiegand (2017); Anzovino et al. (2018); Zampieri et al. (2019); Banko et al. (2020)."
2021.bsnlp-1.3,Q17-1010,0,0.00566964,"nnot be converted to initial form due to distortion through slang/word formation. An example of this is the following: 9. Train-test split Randomly split the ready data into train and test sets with 80/20 proportion. 4.2 В чем проблема? Деградируй до неандертальца и х*ярь (heavily distorted slang) п*дарасов (misspelling) Feature Extraction Additional features beyond the text itself are included. Since abusive or hateful comments are anticipated to be also negative in sentiment, sentiment analysis is included. The sentiment was automatically predicted for the RTC dataset, for which a FastText (Bojanowski et al., 2017) embedding induced over RuSentiment (Rogers et al., 2018) was used, achieving F1 of 0.71, high for sentiment classifiers for Russian. Upper-casing full words is a popular toneindicating technique (Derczynski et al., 2015). Since one cannot “shout” in the internet, the intent of a higher-tone is expressed with upper-casing. Therefore, the number of fully-uppercased words is counted for each sample. We also count the number of offensive words (from our lexicon) contained in a sentence. This feature is expected to be important, since abusive language is often combined with profanity, though this"
2021.bsnlp-1.3,P18-4021,0,0.0213184,"Missing"
2021.bsnlp-1.3,N19-1423,0,0.0536922,"recall and precision on both datasets without stopword filtering (a) with RTC data Watched Burnt by the Sun 2. Turns out it’s a pretty good movie, a high-budget arthouse-ish film, the only downside possible is that most of the budget has been corruptly-stolen and THE PLOT IS NOT REALISTIC. F*ck those critics. The review texts are longer than the movie itself, jokes are worse than &lt;humor in Russian-produced comedies&gt;, f*cked up hate and f*cking nagging about small errors. (b) no RTC data Figure 4: Performance without giving balancing instance weights 4.6 4.6.2 mBERT mBERT is multilingual BERT (Devlin et al., 2019), again trained on Wikipedia pages of over a hundred languages, mainly of non-Latin alphabets. Russian is Cyrillic, thus the model has the potential in Russian hate-speech recognition domain. The fine-tuning is the same as for RuBERT. The results (Figure 5) showed worse performance than RuBERT, up to 0.76 F1-score. The reason for the lower performance is probably in the concept of generalisation of BERT to multiple languages, as opposed to RuBERT, which is trained exclusively on Russian language. The following is an example of a sample which has been incorrectly classified as no-hate with both"
2021.bsnlp-1.3,C18-1064,0,0.015405,"lang/word formation. An example of this is the following: 9. Train-test split Randomly split the ready data into train and test sets with 80/20 proportion. 4.2 В чем проблема? Деградируй до неандертальца и х*ярь (heavily distorted slang) п*дарасов (misspelling) Feature Extraction Additional features beyond the text itself are included. Since abusive or hateful comments are anticipated to be also negative in sentiment, sentiment analysis is included. The sentiment was automatically predicted for the RTC dataset, for which a FastText (Bojanowski et al., 2017) embedding induced over RuSentiment (Rogers et al., 2018) was used, achieving F1 of 0.71, high for sentiment classifiers for Russian. Upper-casing full words is a popular toneindicating technique (Derczynski et al., 2015). Since one cannot “shout” in the internet, the intent of a higher-tone is expressed with upper-casing. Therefore, the number of fully-uppercased words is counted for each sample. We also count the number of offensive words (from our lexicon) contained in a sentence. This feature is expected to be important, since abusive language is often combined with profanity, though this kind of sampling is not without bias (Vidgen and Derczyns"
2021.bsnlp-1.3,sabou-etal-2014-corpus,1,0.692027,"e.1 Thus, there is a critical need for abusive language recognition systems, which would help social networks and forums filter abusive language. Moreover, with platforms taking increased control over which content to surface, automatic abuse recognition is more important than ever. One problem arises when the subjectivity of the matter is considered. Abusive language is hard for humans to recognize universally (Waseem, 2016). This indicates that the collection and labeling of data should be thorough and objective, which could be reached through e.g. large-scale crowd-sourced data annotation (Sabou et al., 2014). NLP research in the area is nascent, with existing solutions oriented mostly towards English language (Vidgen and Derczynski, 2020), which, despite sometimes being mistakenly considered as “universal” (Bender, 2019), is very different 3 Dataset 3.1 Data collection We searched for publicly available datasets containing considerable amounts of abusive language. Russian Troll Tweets is a repository consisting of 3 million tweets.2 This was filtered to only Cyrillic texts. This data is not labeled, thus a subset of the data was labeled manually for use in this research. During labeling, the data"
2021.bsnlp-1.3,W16-5618,0,0.0158703,"an aggressive environment for users. This can include cyber-bullying or threats towards individuals and groups. Reducing this content is difficult: it is harmful for humans to moderate.1 Thus, there is a critical need for abusive language recognition systems, which would help social networks and forums filter abusive language. Moreover, with platforms taking increased control over which content to surface, automatic abuse recognition is more important than ever. One problem arises when the subjectivity of the matter is considered. Abusive language is hard for humans to recognize universally (Waseem, 2016). This indicates that the collection and labeling of data should be thorough and objective, which could be reached through e.g. large-scale crowd-sourced data annotation (Sabou et al., 2014). NLP research in the area is nascent, with existing solutions oriented mostly towards English language (Vidgen and Derczynski, 2020), which, despite sometimes being mistakenly considered as “universal” (Bender, 2019), is very different 3 Dataset 3.1 Data collection We searched for publicly available datasets containing considerable amounts of abusive language. Russian Troll Tweets is a repository consistin"
2021.bsnlp-1.3,N16-2013,0,0.0303923,"ent potential difficulties for this task, which is addressed using a variety of machine learning approaches. We present a dataset and baselines for this task. 1 2 Introduction Abusive Language Definition In this case, we use the OLID annotation definition of abusive language (Zampieri et al., 2019). This covers profanity, and targeted and untargeted insults and threats, against both groups and individuals. Specifically, in accordance this scheme, we consider the use of racial and other group-targeted slurs abusive. Unfortunately, hate speech and abusive language are prevalent on the internet (Waseem and Hovy, 2016), often creating an aggressive environment for users. This can include cyber-bullying or threats towards individuals and groups. Reducing this content is difficult: it is harmful for humans to moderate.1 Thus, there is a critical need for abusive language recognition systems, which would help social networks and forums filter abusive language. Moreover, with platforms taking increased control over which content to surface, automatic abuse recognition is more important than ever. One problem arises when the subjectivity of the matter is considered. Abusive language is hard for humans to recogni"
2021.bsnlp-1.3,2020.alw-1.8,0,0.0526663,"Missing"
2021.hcinlp-1.16,J91-2004,0,0.257561,"es could readily improve NLP error analyses. New abstractions. HCI is a broad field with many interactions and concepts within its aegis. These can lead to new abstractions regarding the use and structure of NLP technology. For example, the abstract concept of a design material – a conceptual, tangible, or other item used in or by a design process – can be applied to machine learning, improving understanding of how machine learning (or ML-based NLP) can be used or useful (Dove et al., 2017). Similarly, the role played by linguistic actions (e.g. conversation analysis (Norman and Thomas, 1991; Hirst, 1991)) may be differently understood in various HCI frameworks, giving new interfaces that may lead to a deeper problem understanding. A concrete example of such work is VoxML’s use of affordances and embodiment for semantic disambiguation (Pustejovsky and Krishnaswamy, 2020). Extra data layers. HCI research produces vast amounts of data, a lot of it transcribed. Interviews, field notes, observations, and video transcripts could all serve as readily accessible data for NLP training datasets and analyses. This novel modality gives an interface for applying NLP to support HCI research while presentin"
2021.hcinlp-1.16,C16-2012,0,0.0303258,"Missing"
2021.hcinlp-1.16,E14-1078,0,0.0162796,"of the process. 102 Another challenge in annotation processes is assessing quality: annotator disagreement is often a valuable signal, indicating e.g. rich, multiphenomena instances (Das et al., 2017; Sommerauer et al., 2020) or cases where only an annotator minority has the knowledge to complete the task (Derczynski et al., 2016; Vidgen and Derczynski, 2020). This disagreement, which signals a problem in the interface between human annotator and the dataset goal, should be investigated: its sources are likely to be interesting, and at the least, disagreement can help downstream applications (Plank et al., 2014). HCI offers the tools for exploring and better understanding that interface. HCI, concerned with assessing qualitative data, offers numerous methods, discussions, and reflections on reliability and when to use (and when not to use) inter-rater reliability (IRR) as a measure (McDonald et al., 2019). Analysis work in HCI is often iterative, with several steps of testing and inclusion of relevant stakeholders (Zimmerman et al., 2007). In HCI, there is no one correct way to assess reliability – different studies invite different frames and methods. Therefore, HCI research must be transparent and"
2021.hcinlp-1.16,J08-3001,0,0.0475732,"ve” translation performance. Annotation processes. The creation of NLP datasets hinges on quality human annotation. A core challenge in NLP annotation is to develop schemas that allow computation, that model a realworld phenomenon, and that are understandable by annotators (Pustejovsky and Stubbs, 2012; Ferro et al., 2005). These schemas and their description benefit from having a situated understanding the phenomena well and having clear interactions around it. After all, machine learning models tend to learn the behaviours represented their training data – which may not always be desirable (Reidsma and Carletta, 2008; Bender et al., 2021). However, while these schemas are typically developed with both linguistic and computational constraints in mind, the fact that they are to be applied to language by humans in order to generate data is seldom recognised, despite this being the critical part of the process. 102 Another challenge in annotation processes is assessing quality: annotator disagreement is often a valuable signal, indicating e.g. rich, multiphenomena instances (Das et al., 2017; Sommerauer et al., 2020) or cases where only an annotator minority has the knowledge to complete the task (Derczynski"
2021.hcinlp-1.16,C14-3005,0,0.0152006,"with specific technical requirements for storage and access. Data storage significantly influences the way data is retrieved, which is crucial to enabling different kinds of analyses by different researchers. Output risks Biases in data. All data is biased – by the way it is sampled, by the goal behind gathering and annotating it, by the individual implementing its assembly. These biases may present as: class overrepresentation and underrepresentation; missing phenomena, such as languages, entity names, lexical items, or syntactic structures; skew in treatment of borderline cases; and so on (Søgaard et al., 2014). The important part is to label the biases, so they may be addressed and communicated. While NLP researchers and HCI researchers have the expertise and experiential knowledge to be capable of recognising and documenting the biases within their own field, it is harder to properly understand the biases present in data from another discipline. This is a challenge faced by people on both sides of the HCI/NLP interface. We should be each cognisant of our potential lack of insight into the other field, and aware that data from it might be mis-applied, or that assumptions may not port well from one"
2021.hcinlp-1.16,2020.coling-main.422,0,0.011096,"o learn the behaviours represented their training data – which may not always be desirable (Reidsma and Carletta, 2008; Bender et al., 2021). However, while these schemas are typically developed with both linguistic and computational constraints in mind, the fact that they are to be applied to language by humans in order to generate data is seldom recognised, despite this being the critical part of the process. 102 Another challenge in annotation processes is assessing quality: annotator disagreement is often a valuable signal, indicating e.g. rich, multiphenomena instances (Das et al., 2017; Sommerauer et al., 2020) or cases where only an annotator minority has the knowledge to complete the task (Derczynski et al., 2016; Vidgen and Derczynski, 2020). This disagreement, which signals a problem in the interface between human annotator and the dataset goal, should be investigated: its sources are likely to be interesting, and at the least, disagreement can help downstream applications (Plank et al., 2014). HCI offers the tools for exploring and better understanding that interface. HCI, concerned with assessing qualitative data, offers numerous methods, discussions, and reflections on reliability and when to"
2021.nodalida-main.47,Q18-1041,0,0.0262959,"for creating the claims as well as the content of the articles used from Den Store Danske. The remaining articles from Den Store Danske are not included (due to rights), and all articles should be considered to be iid.for modelling. The format of the dataset can be found in Appendix A.1. 3.1 Dataset Goal DAN FEVER can be used for research and implementation of multi-lingual claim-detection. The dataset can be used for bench-marking models on a small language, as well as for fine-tuning when applying such models on Danish data. 3.2 Data Statement The following is a data-statement as defined by Bender and Friedman (2018). The dataset consists of a text corpus and a set of annotated claims. The annotated part contains 6407 claims, with labels and information about what articles can be used to verify them. Curation Rationale A dump of the Danish Wikipedia of 13 February 2020 was stored as well as the relevant articles from Den Store Danske (subset of site to adhere to rights). Two teams of two people independently sampled evidence, and created and annotated claims from these two sites (more detail in section 3.3). Speaker Demographic Den Store Danske is written by professionals and is funded by various foundati"
2021.nodalida-main.47,2020.acl-main.747,0,0.0937456,"Missing"
2021.nodalida-main.47,N19-1423,0,0.00996756,"Supported, Refuted or NotEnoughInfo, based on the claim and the subset of sentences. To provide baseline performance for future research to benchmark against, we trained a baseline model on the final task; recognizing textual entailment. Since there are no evidence extracts for the NotVerifiable samples, we apply the random-sampling method from the original E N FEVER paper, where evidence is randomly assigned from the data to each of these samples. We trained classifiers on the resulting 3-class problem. 1 Interestingly the most mentioned location is Finland The transformer based model BERT (Devlin et al., 2019) has shown promising performance for claim verification (Soleimani et al., 2020), and the team (DOMLIN) with highest FEVER-score in the FEVER2.0 competition used a BERTbased system (Thorne et al., 2019). Using the transformers repository from HuggingFace (Wolf et al., 2020) we test; mBERT (Feng et al., 2020) (tag: bert-base-multilingual-cased), XLM-RoBERTa Small and XLM-RoBERTa Large (Conneau et al., 2020; Liu et al., 2019) (tags: xlm-roberta-base and xlm-roberta-large), and the Danish NordicBERT (BotXO, 2019). We use BERT’s sentence-pair representation for claims and evidence extracts. The cl"
2021.nodalida-main.47,2020.lrec-1.565,0,0.0370846,"ush Frederik Den Store Obama Eastwood Jens Grant Rodiek # 110 73 44 36 24 16 15 13 9 8 Organization Aalborg Universitet FN DR Københavns Universitet Electronics Art FC Barcelona Apollo Rejser Bananarama EU MTV # 11 11 10 9 9 9 8 8 8 7 Table 5: Most frequent entities and number of occurrences. 4.1 Named Entities in Claims The entities mentioned frequently in a corpus can give insight into popular themes in the data. In this case, the topic of the claims is particularly relevant. We present an automatic survey of DAN FEVER’s entities. Entities in claims were identified using the DaNLP NER tool (Hvingelby et al., 2020), which identifies location (LOC), person (PER), and organization (ORG) entities. Those most frequently named are shown in Table 5.1 . 5 Baseline: Recognizing Textual Entailment The FEVER task consists of verifying claims based on a text corpus. One common strategy is to split the task into three components (as in the original work (Thorne et al., 2018a)) 1. Document Retrieval: Retrieve a useful subset of documents from the corpora, based on the claim. 2. Sentence Retrieval: Retrieve a useful subset of sentences from those documents, based on the claim. 3. Recognize Textual Entailment: Classif"
2021.nodalida-main.47,W19-6141,1,0.80237,", 2017). It is Leon Derczynski IT University of Copenhagen leod@itu.dk however evident that multilingual models are essential if automation is to assist in multilingual regions like Europe. A possible approach for multilingual verification is to use translation systems for existing methods (Dementieva and Panchenko, 2020), but relevant datasets in more languages are necessary for testing multilingual models’ performance within each language, and ideally also for training. This paper presents DAN FEVER, a dataset and baseline for the FEVER task in Danish, a language with shortage of resources (Kirkedal et al., 2019). While DAN FEVER enables improved automatic verification for Danish, an important task (Derczynski et al., 2019), it is also, to our knowledge, the first non-English dataset on the FEVER task, and so paves the way for multilingual fact verification systems. DAN FEVER is openly available at https: //figshare.com/articles/dataset/ DanFEVER_claim_verification_ dataset_for_Danish/14380970 2 English FEVER The Fact Extraction and VERification dataset and task (FEVER) is aimed at automatic claim verification in English (Thorne et al., 2018a). When comparing we will stylize the original FEVER dataset"
2021.nodalida-main.47,2021.ccl-1.108,0,0.0501903,"Missing"
2021.nodalida-main.47,N18-1074,0,0.389739,"iaga et al., 2018). One approach to this task is to break down information content into verifiable claims, which can subsequently be fact-checked by automated systems. Automated fact checking can be framed as a machine learning task, where a model is trained to verify a claim. Applying machine learning requires training and validation data that is representative of the task and is annotated for the desired behaviour. A model should then attempt to generalise over the labeled data. One dataset supporting automatic verification is the Fact Extraction and VERification dataset (FEVER) in English (Thorne et al., 2018a), which supports the FEVER task (Thorne et al., 2018b; Thorne and Vlachos, 2019). The dataset is aimed both at claim detection and verification. While the misinformation problem spans both geography and language, much work in the field has focused on English. There have been suggestions on strategies for alleviating the misinformation problem (Hellman and Wagnsson, 2017). It is Leon Derczynski IT University of Copenhagen leod@itu.dk however evident that multilingual models are essential if automation is to assist in multilingual regions like Europe. A possible approach for multilingual verif"
2021.nodalida-main.47,W18-5501,0,0.0370337,"Missing"
2021.nodalida-main.47,2020.emnlp-demos.6,0,0.0189759,"for the NotVerifiable samples, we apply the random-sampling method from the original E N FEVER paper, where evidence is randomly assigned from the data to each of these samples. We trained classifiers on the resulting 3-class problem. 1 Interestingly the most mentioned location is Finland The transformer based model BERT (Devlin et al., 2019) has shown promising performance for claim verification (Soleimani et al., 2020), and the team (DOMLIN) with highest FEVER-score in the FEVER2.0 competition used a BERTbased system (Thorne et al., 2019). Using the transformers repository from HuggingFace (Wolf et al., 2020) we test; mBERT (Feng et al., 2020) (tag: bert-base-multilingual-cased), XLM-RoBERTa Small and XLM-RoBERTa Large (Conneau et al., 2020; Liu et al., 2019) (tags: xlm-roberta-base and xlm-roberta-large), and the Danish NordicBERT (BotXO, 2019). We use BERT’s sentence-pair representation for claims and evidence extracts. The classification embedding is then passed to a single-hidden-layer, fullyconnected neural network for prediction. We first train the prediction layer, while freezing the weights of the language model, and consecutively fine-tune them both. We do this in a 10-fold cross-validati"
2021.sustainlp-1.12,N19-1423,0,0.076431,"Missing"
2021.sustainlp-1.12,2021.ccl-1.108,0,0.0654624,"Missing"
2021.sustainlp-1.12,2020.findings-emnlp.298,0,0.0165208,"htly more complex calculation to the environmental impact it can have, such as for self-attention, this suggests that it doesn’t have accelerating rising sea levels (Veng and Andersen, a huge impact on energy consumption, while it has 2020), and as training NLP models has an actual ima noticeable impact on perplexity. Relative key pact on the environment due to high energy usage, query introduces another 143K parameters but is reducing their consumption is, as a topic, both imrelatively insignificant when compared to the 103M portant and very unexplored. This also means that BERT already has (Huang et al., 2020). While our dataset contains 107 models with “relative key we, as the authors, have to stress the importance of noting that this is not a complete guide on how query” as the positional embedding type, when looking at the worst-performing models, the dis- to create low-power, low-perplexity transformer tribution looks close to uniform, at least when look- models. As an example, reducing the number of hidden layers might have a positive effect in certain ing at perplexity and PEP. Furthermore, Table 13 models, but not necessarily in others - one has to shows the distribution in the first cluster"
2021.sustainlp-1.12,P19-1355,0,0.346163,"ct to Strubell et al. (2019), we must cut CO2 emissions by half to slow natural disasters. However, much research in the field ignores the perspective of energy efficiency. When looking at papers from three top AI conferences, namely ACL, NeurIPS, and CVPR, work tends to focus on accuracy rather than efficiency, or a mixture (Schwartz et al., 2019). An added benefit to developing more energyefficient models is a reduced barrier of entry to NLP research. Researchers with good ideas may not be able to execute those ideas, given that state-of-theart results are locked behind large-scale compute (Strubell et al., 2019; Bender et al., 2021). This study investigates how to reduce power consumption in training transformer language models. We seek to address the issue of high-power models by analysing the resulting models’ hyperparameters, energy consumption, and perplexity and providing initial parameter guidelines for low-power, highperformance transformers, and an opening into the research of low-power transformers. Our research question is: How can we reduce the energy consumption of models to both lower the barrier of entry and reduce CO2 emissions, while still keeping an effective model? Following Strube"
2021.vardial-1.8,P12-3005,0,0.269199,"specifically on the group of Nordic languages, leaving users of those languages without high quality automatically-extracted single language corpora (Derczynski et al., 2020). This is particularly disadvantageous for some Nordic language pairs, such as Danish/Norwegian and Faroese/Icelandic, where general-purpose many-language systems fall down (Toftrup et al., 2021). Thus, we focus specifically on data for this language and baseline methods. Introduction Automatic language identification is a core problem in NLP but remains a difficult task (Caswell et al., 2020), especially across domains (Lui and Baldwin, 2012; Derczynski et al., 2013). Discriminating between closely related languages is often a particularly difficult subtask of this problem (Zampieri et al., 2014). Language technology for Scandinavian languages is in a nascent phase (e.g. Kirkedal et al. (2019)). One problem is acquiring enough text with which to train e.g. large language models. Good quality language ID is critical to this data sourcing, though leading models often confuse similar Nordic languages. This paper presents data and baselines for automatic language identification between six closelyrelated Nordic languages: Danish, Swe"
2021.vardial-1.8,2020.coling-main.579,0,0.0599425,"Missing"
2021.vardial-1.8,W18-1605,0,0.023095,"inating between Similar Languages (DSL) shared task&quot;. Over the two editions of the DSL shared task different teams competed to develop the best machine learning algorithms to discriminate between the languages in a corpus consisting of 20K sentences in each of the languages: Bosnian, Croatian, Serbian, Indonesian, Malaysian, Czech, Slovak, Brazil Portuguese, European Portuguese, Argentine Spanish, Peninsular Spanish, Bulgarian and Macedonian. Similar work has included (Toftrup et al., 2021), who include Nordic languages in a larger exercise in reproducing a commercial language ID system; and (Rangel et al., 2018), who attempt native language extraction, a task complex in the Nordic context which is rich in cognates and shared etymologies. However, no prior work has focused specifically on the group of Nordic languages, leaving users of those languages without high quality automatically-extracted single language corpora (Derczynski et al., 2020). This is particularly disadvantageous for some Nordic language pairs, such as Danish/Norwegian and Faroese/Icelandic, where general-purpose many-language systems fall down (Toftrup et al., 2021). Thus, we focus specifically on data for this language and baselin"
2021.vardial-1.8,W19-6138,1,0.885569,"Missing"
2021.vardial-1.8,2021.eacl-srw.6,0,0.0948571,"Missing"
2021.vardial-1.8,W14-5307,0,0.0267485,"nski et al., 2020). This is particularly disadvantageous for some Nordic language pairs, such as Danish/Norwegian and Faroese/Icelandic, where general-purpose many-language systems fall down (Toftrup et al., 2021). Thus, we focus specifically on data for this language and baseline methods. Introduction Automatic language identification is a core problem in NLP but remains a difficult task (Caswell et al., 2020), especially across domains (Lui and Baldwin, 2012; Derczynski et al., 2013). Discriminating between closely related languages is often a particularly difficult subtask of this problem (Zampieri et al., 2014). Language technology for Scandinavian languages is in a nascent phase (e.g. Kirkedal et al. (2019)). One problem is acquiring enough text with which to train e.g. large language models. Good quality language ID is critical to this data sourcing, though leading models often confuse similar Nordic languages. This paper presents data and baselines for automatic language identification between six closelyrelated Nordic languages: Danish, Swedish, Norwegian (Nynorsk), Norwegian (Bokmål), Faroese and Icelandic. Further, we investigate feature extraction methods for Nordic language identification an"
2021.vardial-1.8,W15-5401,0,0.0196001,"atic language identification is a challenging problem. Discriminating between closely related languages is especially difficult. This paper presents a machine learning approach for automatic language identification for the Nordic languages, which often suffer miscategorisation by existing state-of-the-art tools. Concretely we will focus on discrimination between six Nordic languages: Danish, Swedish, Norwegian (Nynorsk), Norwegian (Bokmål), Faroese and Icelandic. 1 2 Related Work The problem of discriminating between similar languages has been investigated in recent work (Goutte et al., 2016; Zampieri et al., 2015) which discuss the results from two editions of the “Discriminating between Similar Languages (DSL) shared task&quot;. Over the two editions of the DSL shared task different teams competed to develop the best machine learning algorithms to discriminate between the languages in a corpus consisting of 20K sentences in each of the languages: Bosnian, Croatian, Serbian, Indonesian, Malaysian, Czech, Slovak, Brazil Portuguese, European Portuguese, Argentine Spanish, Peninsular Spanish, Bulgarian and Macedonian. Similar work has included (Toftrup et al., 2021), who include Nordic languages in a larger ex"
2021.vardial-1.8,L16-1284,0,0.0212795,"across domains. Automatic language identification is a challenging problem. Discriminating between closely related languages is especially difficult. This paper presents a machine learning approach for automatic language identification for the Nordic languages, which often suffer miscategorisation by existing state-of-the-art tools. Concretely we will focus on discrimination between six Nordic languages: Danish, Swedish, Norwegian (Nynorsk), Norwegian (Bokmål), Faroese and Icelandic. 1 2 Related Work The problem of discriminating between similar languages has been investigated in recent work (Goutte et al., 2016; Zampieri et al., 2015) which discuss the results from two editions of the “Discriminating between Similar Languages (DSL) shared task&quot;. Over the two editions of the DSL shared task different teams competed to develop the best machine learning algorithms to discriminate between the languages in a corpus consisting of 20K sentences in each of the languages: Bosnian, Croatian, Serbian, Indonesian, Malaysian, Czech, Slovak, Brazil Portuguese, European Portuguese, Argentine Spanish, Peninsular Spanish, Bulgarian and Macedonian. Similar work has included (Toftrup et al., 2021), who include Nordic"
2021.vardial-1.8,W18-5408,0,0.0663029,"Missing"
2021.vardial-1.8,W19-6141,1,0.638418,"nish/Norwegian and Faroese/Icelandic, where general-purpose many-language systems fall down (Toftrup et al., 2021). Thus, we focus specifically on data for this language and baseline methods. Introduction Automatic language identification is a core problem in NLP but remains a difficult task (Caswell et al., 2020), especially across domains (Lui and Baldwin, 2012; Derczynski et al., 2013). Discriminating between closely related languages is often a particularly difficult subtask of this problem (Zampieri et al., 2014). Language technology for Scandinavian languages is in a nascent phase (e.g. Kirkedal et al. (2019)). One problem is acquiring enough text with which to train e.g. large language models. Good quality language ID is critical to this data sourcing, though leading models often confuse similar Nordic languages. This paper presents data and baselines for automatic language identification between six closelyrelated Nordic languages: Danish, Swedish, Norwegian (Nynorsk), Norwegian (Bokmål), Faroese and Icelandic. Further, we investigate feature extraction methods for Nordic language identification and evaluates the performance of a selection of baseline models. Finally, we test the models on a dat"
2021.vardial-1.8,W02-0109,0,0.139009,"the experiments presented in this paper, the CBOW and Skipgram encodings have the following settings: We use individual words (uni-grams) augmented with character level n-grams of size 2-5 with a context window of 5. The encoding result in fixed-length vectors in R100 . Data Cleaning and encoding. This section describes how the data set is initially cleaned and how sentences are extracted from the raw data. Extracting Sentences The first pass in sentence tokenisation is splitting by line breaks. We then extract shorter sentences with the sentence tokenizer (sent_tokenize) function from NLTK (Loper and Bird, 2002). This does a better job than just splitting by ’.’ due to the fact that abbreviations, which can appear in a legitimate sentence, typically include a period symbol. Cleaning characters The initial data set has many characters that do not belong to the alphabets of the languages we work with. Often the Wikipedia pages for people or places contain names in foreign languages. For example a summary might contain Chinese or Russian characters which are not strong signals for the purpose of discriminating between the target languages. Further, it can be that some characters in the target languages"
2021.vardial-1.8,I11-1062,0,0.0382804,"e raw data is converted to lowercase and stripped of all characters which are not part of the standard alphabet of the six languages. In this way we only accept the characters: 4 4.1 Baselines langid.py We compare with an off-the-shelf language identification system, langid.py (Lui and Baldwin, 2012). langid.py comes with a pretrained model which covers 97 languages. The data for langid.py comes from five different domains: government documents, software documentation, newswire, online encyclopedia and an internet crawl. Features are selected for cross-domain stability using the LD heuristic (Lui and Baldwin, 2011). We evaluated how well langid.py performed on the Nordic DSL data set. It is a peculiar feature of 68 Model Knn Log-Reg Naive Bayes SVM Knn Log-Reg Naive Bayes SVM Knn Log-Reg Naive Bayes SVM Knn Log-Reg Naive Bayes SVM the Norwegian language that there exist two different written languages but three different language codes. Since langid.py also returned the language id “no&quot; (Norwegian) on some of the data points we restrict langid.py to only be able to return either “nn&quot; (Nynorsk) or “nb&quot; (Bokmål) as predictions. Figure 1: Confusion matrix with results from langid.py on the full Wikipedia d"
aker-etal-2017-simple,P12-1091,0,\N,Missing
aker-etal-2017-simple,D14-1082,0,\N,Missing
aker-etal-2017-simple,P14-1023,0,\N,Missing
aker-etal-2017-simple,D13-1170,0,\N,Missing
aker-etal-2017-simple,S16-1003,0,\N,Missing
aker-etal-2017-simple,P16-2064,1,\N,Missing
aker-etal-2017-simple,S17-2087,0,\N,Missing
aker-etal-2017-simple,S17-2085,0,\N,Missing
aker-etal-2017-simple,S17-2080,0,\N,Missing
aker-etal-2017-simple,S17-2081,0,\N,Missing
aker-etal-2017-simple,D11-1147,0,\N,Missing
C16-1111,R13-1011,1,0.323241,"ial media, creating topic shifts of both greater magnitude and higher frequency than other text types. We focus on Twitter, using this as the “model organism” of social media text (Tufekci, 2014), to assemble a corpus that is capable of catching these variances. 2.1 Annotation Scheme The BTC corpus is divided into segments, where the documents within each segment share a common theme (see Table 2). The documents consist of the social media message – i.e. tweet – complete with its JSON metadata. The text of the tweet is then annotated with into sentences and tokens, using the TwitIE tokenizer (Bontcheva et al., 2013). Tokenisation presents some issues in tweets. Classic schemas like PTB do not work well with constructs like smilies or URLs. To address this, we use the TwitIE tokeniser (Bontcheva et al., 2013) which is roughly based on TweetMotif and the twokeniser tool (O’Connor et al., 2010). Of note, we separate the preceding symbol in mentions and hashtags (the @ or # characters) as a distinct token, but still include this in entity spans. The main question was which entity classes should be covered in the corpus. Some Twitter corpora have used ten top-level Freebase categories (Bollacker et al., 2008;"
C16-1111,E14-2025,1,0.661946,"le 1). For expert-based annotation methodologies Hovy (2010) recommend at most ten, ideally seven, categories. In crowdsourcing, successful tasks tend to present even fewer choices – in most cases between two and five categories (Sabou et al., 2014). Therefore, we chose to focus on the three most widely used entity categories: Person, Location, and Organization (Table 1). Apart from being well understood by annotators, these three categories offer straightforward mapping to the other existing Twitter datasets, so all could be used in combination, as training data, if needed. An initial pilot (Bontcheva et al., 2014a) also included a fourth class, “Product”, but crowd workers struggled to annotate these correctly and with good recall, so they were dropped. For polysemous entities, our guidelines instructed annotators to assign the entity class that corresponds to the correct entity class in the given context. For example, in “We’re driving to Manchester”, Manchester is a location, but in “Manchester are in the final tonight”, it is a sports club – an organization. Special attention is given to username mentions. Where other corpora have blocked these out (Rowe et al., 2013) or classified them universally"
C16-1111,W10-0701,0,0.0426189,"luded. Segment H is the most varied. To balance the UK bias of segment B, this segment excludes tweets of UK origin (according to the Twitter metadata). The segment is stratified for month of year, time of day, and day of week, giving an even spread over many temporal cycle types in the collection period. 3 Annotation Process To make annotation scalable and of high quality, while ensuring sufficient annotator variety, corpus annotation was carried out using a mix of NLP experts and paid-for crowdsourcing. The annotation process follows general best practices in crowdsourced corpus annotation (Callison-Burch and Dredze, 2010; Alonso and Lease, 2011; Sabou et al., 2014). For example, task design is kept clean (a critical factor, more important than e.g. interface language – (Khanna et al., 2010)), and the process developed over pilot and refinement iterations. Tasks were built in GATE and jobs automatically managed through through Crowdflower (Bontcheva et al., 2014b). First segments were entirely annotated and adjudicated by experts as calibration. To maximize annotator focus, images attached to tweets or featuring in content (e.g. news stories) linked to from tweets are shown alongside the task, for worker primi"
C16-1111,W14-3207,0,0.0326691,"esses, governments, and communities increasingly need real-time information from dynamic, large-volume media data streams, such as blogs, Facebook, and Twitter. In particular, the automatic detection of mentions of people, organizations, locations, and other entities (i.e. Named Entity Recognition) is a key step in numerous social media analysis applications, e.g. competitor and brand monitoring (Mostafa, 2013), debate and election analysis (Mascaro and Goggins, 2012; Tumasjan et al., 2010), disaster response (Kedzie et al., 2015; Neubig et al., 2011), and health- and well-being applications (Coppersmith et al., 2014; Choudhury et al., 2013). NER methods (typically trained on longer texts, such as news articles), have been shown to perform poorly on shorter and noisier social media content (Ritter et al., 2011). Therefore, recent Twitter NER work (Ritter et al., 2011; Liu et al., 2011; Derczynski et al., 2015) has focused on improving the state-ofthe-art, through new methods. The challenges come from named entities (NEs) typically being out-ofvocabulary (OOV) as compared to the training newswire data; the shorter context; and lack of sufficiently large NE annotated social media datasets. In more detail, T"
C16-1111,N13-1037,0,0.0669534,"peaking world. Finally, it is partially socially segmented, including reactions to news stories, non-professional content, and text from the “twitterati”. The corpus is made freely available in various formats; the source text is included under Twitter’s revised 2015 licensing guidelines, as are the intermediate annotations. 2 Corpus Construction The goal of the corpus is to provide a representative example of named entities in social media. Social media is said to contain more variance than some other text types, like newswire. It certainly is authored by a broader demographic than newswire (Eisenstein, 2013), and contains a variety of styles and formality registers, unlike other user-generated content such as Youtube comments or SMS (Hu et al., 2013). Changes in general discourse subject are also said to present more quickly in social media, creating topic shifts of both greater magnitude and higher frequency than other text types. We focus on Twitter, using this as the “model organism” of social media text (Tufekci, 2014), to assemble a corpus that is capable of catching these variances. 2.1 Annotation Scheme The BTC corpus is divided into segments, where the documents within each segment share"
C16-1111,W10-0713,0,0.0210569,"Missing"
C16-1111,fromreide-etal-2014-crowdsourcing,0,0.0745323,"Missing"
C16-1111,N13-1132,0,0.0179668,"origin tweet had better recall on these entities. Conversely, annotators working on documents from other countries had lower recall. As matched geographic contexts tend to produce better results, during annotation, the geographically stratified parts of corpora were issued only to crowd workers in the same region, in order to maximize recall and local knowledge. 3.2 Adjudication Adjudication is an important step in refining annotator data. In the case of crowdsourced annotations, further economies of scale can be afforded by automating adjudication; tools already exist for this, such as MACE (Hovy et al., 2013). However, auto-adjudication is poorly equipped to handle exceptional circumstances; further, it is hard to judge its impact without human intervention. The construction of the BTC involved a combination of automatic and human adjudication. Primarily, we found there were problems with recall. Often, only a single crowd worker or expert would annotate a given (correct) entity. Under traditional agreement-based measures, this singleton annotation would be in the minority, and so likely removed in a typical adjudication step. Given our and others’ experiences with annotator recall and diversity ("
C16-1111,P15-1155,0,0.0676664,"sed openly, including source text and intermediate annotations. 1 Introduction Businesses, governments, and communities increasingly need real-time information from dynamic, large-volume media data streams, such as blogs, Facebook, and Twitter. In particular, the automatic detection of mentions of people, organizations, locations, and other entities (i.e. Named Entity Recognition) is a key step in numerous social media analysis applications, e.g. competitor and brand monitoring (Mostafa, 2013), debate and election analysis (Mascaro and Goggins, 2012; Tumasjan et al., 2010), disaster response (Kedzie et al., 2015; Neubig et al., 2011), and health- and well-being applications (Coppersmith et al., 2014; Choudhury et al., 2013). NER methods (typically trained on longer texts, such as news articles), have been shown to perform poorly on shorter and noisier social media content (Ritter et al., 2011). Therefore, recent Twitter NER work (Ritter et al., 2011; Liu et al., 2011; Derczynski et al., 2015) has focused on improving the state-ofthe-art, through new methods. The challenges come from named entities (NEs) typically being out-ofvocabulary (OOV) as compared to the training newswire data; the shorter cont"
C16-1111,P11-1037,0,0.0289405,"ntity Recognition) is a key step in numerous social media analysis applications, e.g. competitor and brand monitoring (Mostafa, 2013), debate and election analysis (Mascaro and Goggins, 2012; Tumasjan et al., 2010), disaster response (Kedzie et al., 2015; Neubig et al., 2011), and health- and well-being applications (Coppersmith et al., 2014; Choudhury et al., 2013). NER methods (typically trained on longer texts, such as news articles), have been shown to perform poorly on shorter and noisier social media content (Ritter et al., 2011). Therefore, recent Twitter NER work (Ritter et al., 2011; Liu et al., 2011; Derczynski et al., 2015) has focused on improving the state-ofthe-art, through new methods. The challenges come from named entities (NEs) typically being out-ofvocabulary (OOV) as compared to the training newswire data; the shorter context; and lack of sufficiently large NE annotated social media datasets. In more detail, Table 1 shows that there are less than 90 thousand tokens of publicly available NEannotated tweet datasets, and even those have shortcomings in terms of annotation methodology (e.g. singly annotated), low inter-annotator agreement, and stripping of important entity-bearing"
C16-1111,P12-3005,0,0.0172216,"inds of Twitter users. For instance, verbal communication behaviors such as g-dropping are often copied into typed social media messages (Eisenstein et al., 2010). To try to capture these, the corpus collects data from different segments, explicitly taking in content from well-known public figures, news outlets, wellknown social media figures, plus a large volume of randomly-selected posts. 2.3 Corpus Segmentation The dataset is organized into multiple segments (Table 2) to reflect the diversity criteria and annotation approach (expert vs. crowd). English tweets were filtered using langid.py (Lui and Baldwin, 2012). Segment A comprises a random sample of UK tweets, collected after New Year, annotated by multiple NLP experts. This data was used for calibration, so includes both expert input and crowd correction.4 Segment B is similar to segment A. In this segment we focused on non-directed tweets – i.e. those that are not private replies and so do not begin with a username mention. These were found to be more likely to contain a named entity based on sampling the Ritter et al. (2011) corpus. Segment E is a small sample focused on a specific event, the crash of flight MH17 over Ukraine. It contains commen"
C16-1111,I11-1108,0,0.0650068,"source text and intermediate annotations. 1 Introduction Businesses, governments, and communities increasingly need real-time information from dynamic, large-volume media data streams, such as blogs, Facebook, and Twitter. In particular, the automatic detection of mentions of people, organizations, locations, and other entities (i.e. Named Entity Recognition) is a key step in numerous social media analysis applications, e.g. competitor and brand monitoring (Mostafa, 2013), debate and election analysis (Mascaro and Goggins, 2012; Tumasjan et al., 2010), disaster response (Kedzie et al., 2015; Neubig et al., 2011), and health- and well-being applications (Coppersmith et al., 2014; Choudhury et al., 2013). NER methods (typically trained on longer texts, such as news articles), have been shown to perform poorly on shorter and noisier social media content (Ritter et al., 2011). Therefore, recent Twitter NER work (Ritter et al., 2011; Liu et al., 2011; Derczynski et al., 2015) has focused on improving the state-ofthe-art, through new methods. The challenges come from named entities (NEs) typically being out-ofvocabulary (OOV) as compared to the training newswire data; the shorter context; and lack of suffi"
C16-1111,D11-1141,0,0.0829466,"mentions of people, organizations, locations, and other entities (i.e. Named Entity Recognition) is a key step in numerous social media analysis applications, e.g. competitor and brand monitoring (Mostafa, 2013), debate and election analysis (Mascaro and Goggins, 2012; Tumasjan et al., 2010), disaster response (Kedzie et al., 2015; Neubig et al., 2011), and health- and well-being applications (Coppersmith et al., 2014; Choudhury et al., 2013). NER methods (typically trained on longer texts, such as news articles), have been shown to perform poorly on shorter and noisier social media content (Ritter et al., 2011). Therefore, recent Twitter NER work (Ritter et al., 2011; Liu et al., 2011; Derczynski et al., 2015) has focused on improving the state-ofthe-art, through new methods. The challenges come from named entities (NEs) typically being out-ofvocabulary (OOV) as compared to the training newswire data; the shorter context; and lack of sufficiently large NE annotated social media datasets. In more detail, Table 1 shows that there are less than 90 thousand tokens of publicly available NEannotated tweet datasets, and even those have shortcomings in terms of annotation methodology (e.g. singly annotated)"
C16-1111,rose-etal-2002-reuters,0,0.0453131,"picture of globally common entities to make inter-regional comparisons meaningful. The side effect a disproportionate novel part. 5 Entities in Social Media Having examined diversity in the underlying text, we next analyze characteristics of entities. We qualitatively examine surface forms, and compare entity distribution in social media to that in newswire. 5.1 Common Surface Forms Table 12 presents the most frequent surface forms in our corpus and also in the CoNLL’03 NER annotated data. The latter comes from news, based on the RCV1 corpus, which is largely US-based newswire from the 1990s (Rose et al., 2002) written by white working-age men (Eisenstein, 2013). Temporal concept drift (Masud et al., 2010) is evident here. For example, the most frequentlymentioned person entities have different surface forms, while referring to the same concept. The lexical representation of “the President of the US” has changed from Clinton to Obama. Similarly, the leader of Russia is present but with a different word; Yeltsin in the older newswire, Putin in modern social media. The top locations mentioned remain largely the same level and granularity, being countries that are major actors on the global scale or in"
C16-1111,sabou-etal-2014-corpus,1,0.491815,"ote, we separate the preceding symbol in mentions and hashtags (the @ or # characters) as a distinct token, but still include this in entity spans. The main question was which entity classes should be covered in the corpus. Some Twitter corpora have used ten top-level Freebase categories (Bollacker et al., 2008; Ritter et al., 2011)1 or have included products (see Table 1). For expert-based annotation methodologies Hovy (2010) recommend at most ten, ideally seven, categories. In crowdsourcing, successful tasks tend to present even fewer choices – in most cases between two and five categories (Sabou et al., 2014). Therefore, we chose to focus on the three most widely used entity categories: Person, Location, and Organization (Table 1). Apart from being well understood by annotators, these three categories offer straightforward mapping to the other existing Twitter datasets, so all could be used in combination, as training data, if needed. An initial pilot (Bontcheva et al., 2014a) also included a fourth class, “Product”, but crowd workers struggled to annotate these correctly and with good recall, so they were dropped. For polysemous entities, our guidelines instructed annotators to assign the entity"
C16-1111,P11-1137,0,\N,Missing
C16-1111,P10-5004,0,\N,Missing
C16-1182,D08-1073,0,0.0160791,"is a highly structured task, with local and global dependencies. Not only should local relation constraints (e.g. through transitivity and commutativity) be taken into account, as noted by Hovy et al. (2012); also, the annotation of a well-formed document should be globally temporally consistent, otherwise it will detail an impossible sequence of events.5 This interdependent aspect of temporal relations is hard to model, partially due to the underlying computational complexity (Vilain and Kautz, 1986). Nevertheless, including features for local dependencies give modest accuracy improvements (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). 5 Evaluating Learning of Relation Sets Using a variety of temporal relation sets, machine learning of temporal relations is evaluated over the same data. This is achieved by mapping TimeML gold-standard relations to other relation sets thus: Simple – before: BEFORE, IBEFORE; after: AFTER, IAFTER; overlap: everything else. STAG – add inverses of the before and includes types, to make five relations. before: BEFORE, IBEFORE ; after: AFTER , IAFTER ; simultaneous: SIMULTANEOUS , DURING , DURING INV , IDENTITY ; is-included: IS - INCLUDED; includes: everything else. Alle"
C16-1182,C10-1029,0,0.0183944,"indicates that these containers can be annotated by humans sufficiently well to learn an initial automated approach. However, in the absence of a large general-purpose corpus annotated with them, this paper does not include narrative containers. For a general overview and history of temporal annotation standards, see (Str¨otgen and Gertz, 2016). 3 Analysing temporal relation sets This section investigates selected sets of temporal relation types. The sets are chosen to demonstrate key differences, though the list is not exhaustive. A graph-based comparison of three sets can also be found in (Denis and Muller, 2010). 3.1 Expressiveness and Specificity We qualitatively describe temporal relation sets in two dimensions: The first dimension, expressiveness vs. simplicity details the range of different combinations of event orderings they can capture. The second, specificity vs. laxness details how much constraint the relation set’s types imply, and how much one needs to know before typing a relation. 1940 Figure 1: The Opera problem: Linguistic ambiguity leads to inability to choose a single interval relation. Here, we know the time of arrival at the opera but not of the departure, making it hard to relate"
C16-1182,P13-2114,1,0.89239,"Missing"
C16-1182,W13-0107,1,0.897392,"Missing"
C16-1182,D12-1062,0,0.0506534,"Missing"
C16-1182,P13-2139,0,0.0513057,"Missing"
C16-1182,E12-1019,0,0.0270545,"ally different. TimeML’s “immediately after” relation, IAFTER, is more like AFTER than it is INCLUDES. In fact, this non-orthogonality is critical to the construction and behaviour of Freksa’s conceptual neighbourhood relations. The connected nature of relation types offers nuance in training, classification and evaluation of machine learning approaches to temporal relation typing. Secondly, relation typing is a highly structured task, with local and global dependencies. Not only should local relation constraints (e.g. through transitivity and commutativity) be taken into account, as noted by Hovy et al. (2012); also, the annotation of a well-formed document should be globally temporally consistent, otherwise it will detail an impossible sequence of events.5 This interdependent aspect of temporal relations is hard to model, partially due to the underlying computational complexity (Vilain and Kautz, 1986). Nevertheless, including features for local dependencies give modest accuracy improvements (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). 5 Evaluating Learning of Relation Sets Using a variety of temporal relation sets, machine learning of temporal relations is evaluated over the same data."
C16-1182,W13-1903,0,0.0340169,"Missing"
C16-1182,I11-1007,0,0.0304552,"simplifying the problem, and increasing available training data. Taking transitive closures is omitted, as it does not help (Mani et al., 2007). 6.1 Folding Many relation types used in both TimeML and Allen’s relation sets have a corresponding inverse relation. Mapping between these can be achieved by inverting the relation type and the argument order. For example, BEFORE(Monday, Tuesday) is equivalent to AFTER(Tuesday, Monday). This relation folding simplifies classification by reducing the number of target classes, and is common in temporal relation classification, e.g. Mani et al. (2007); Mirroshandel et al. (2011). 6.2 Doubling Doubling the reverse of folding: instead of using a relation type mapping to reduce the number of classes, use the mappings to double the number of examples. In controlled evaluation, it is possible to reverse the order of arguments in the evaluation set so the set only contains relation types the classifier has seen in folded training data. This is not possible where the relation type is never known, as one does not have control over argument order. E.g. if all AFTER relations are removed from the training data by swapping their arguments and changing them to BEFORE, when faced"
C16-1182,W11-0419,0,0.0565898,"Missing"
C16-1182,pustejovsky-etal-2010-iso,0,0.392769,"Missing"
C16-1182,sabou-etal-2014-corpus,1,0.804457,"out, is that the presence of parallels between natural language and a given representation is not always indicative of its suitability for automated reasoning. Freksa’s relation set may be a better target for human annotation: it is intended to better reflect the usage of temporal relation expression in natural language, and only requires judgment over pairs of points rather than intervals, for construction. For example, one may decompose interval relations into point relations, and ask “Does A begin before B ends?”. We already know that simpler questions yield better results from annotators (Sabou et al., 2014), and so a decomposed, less constrained approach to annotation may be fruitful. Certainly, the 13-way Allen (or 14-way TimeML) task has been tough, and it is reasonable that some disagreements come from annotators having to chose a particular relation type when in fact many (or none) seem appropriate to them. 9 Conclusion This paper has investigated choices in representation of temporal relations. Human annotation agreement is often low in relation typing, and the task is also difficult for automatic systems. The expressivity of a representation has an inverse correlation with automatic tempor"
C16-1182,setzer-gaizauskas-2000-annotating,0,0.242444,"Missing"
C16-1182,S13-2001,1,0.785468,"Missing"
C16-1182,P09-1046,0,0.0292348,"with local and global dependencies. Not only should local relation constraints (e.g. through transitivity and commutativity) be taken into account, as noted by Hovy et al. (2012); also, the annotation of a well-formed document should be globally temporally consistent, otherwise it will detail an impossible sequence of events.5 This interdependent aspect of temporal relations is hard to model, partially due to the underlying computational complexity (Vilain and Kautz, 1986). Nevertheless, including features for local dependencies give modest accuracy improvements (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). 5 Evaluating Learning of Relation Sets Using a variety of temporal relation sets, machine learning of temporal relations is evaluated over the same data. This is achieved by mapping TimeML gold-standard relations to other relation sets thus: Simple – before: BEFORE, IBEFORE; after: AFTER, IAFTER; overlap: everything else. STAG – add inverses of the before and includes types, to make five relations. before: BEFORE, IBEFORE ; after: AFTER , IAFTER ; simultaneous: SIMULTANEOUS , DURING , DURING INV , IDENTITY ; is-included: IS - INCLUDED; includes: everything else. Allen – One change: TimeML ID"
C16-1182,Q14-1022,0,\N,Missing
C16-1182,S16-1165,1,\N,Missing
derczynski-etal-2012-massively,llorens-etal-2012-timen,1,\N,Missing
derczynski-etal-2012-massively,de-marneffe-etal-2006-generating,0,\N,Missing
derczynski-etal-2012-massively,D10-1089,0,\N,Missing
derczynski-etal-2012-massively,derczynski-gaizauskas-2010-analysing,1,\N,Missing
derczynski-etal-2012-massively,W01-1311,0,\N,Missing
derczynski-etal-2012-massively,S10-1010,0,\N,Missing
derczynski-etal-2012-massively,pustejovsky-etal-2010-iso,0,\N,Missing
derczynski-etal-2012-massively,S10-1063,1,\N,Missing
derczynski-etal-2012-massively,ide-romary-2002-standards,0,\N,Missing
derczynski-etal-2012-massively,strassel-etal-2008-linguistic,0,\N,Missing
derczynski-gaizauskas-2010-analysing,C08-3012,0,\N,Missing
derczynski-gaizauskas-2010-analysing,S07-1098,1,\N,Missing
derczynski-gaizauskas-2010-analysing,P06-1095,0,\N,Missing
E14-2016,W10-1817,0,0.0157495,"h, but also the same actual word. A language processing toolkit for Danish must exhibit sensitivity to these variances. In addition, Danish has some word boundary considerations. Compound nouns are common (e.g. kvindeh˚andboldlandsholdet for “the women’s national handball team”), as are hyphenated constructions (fugle-fotografering for “bird photography”) which are often treated as single tokens. Finally, abbreviations are common in Danish, and its acronyms can be difficult to disambiguate without the right context and language resource (e.g. OB for Odense Boldklub, a football club). 3 (CDT) (Buch-Kromann and Korzen, 2010), which built on and included previously-released corpora for Danish. This 200K-token corpus is taken from news articles and editorials, and includes document structure, tokenisation, lemma, part-ofspeech and dependency relation information. The application demonstrated, DKIE, draws only on open corpus resources for annotation, and the annotations over these corpora are released openly. Further, the application is also made opensource, with each component having similar or better performance when compared with the stateof-the-art. 4 Information Extraction Pipeline This section details each ste"
E14-2016,R13-1026,1,0.806179,"al features distinct from part-of-speech data. This data may then be used by later work to train a morphological analyser, or by other tools that rely on morphological information. We combine PAROLE annotations with the reduced tagset employed by the Danish Dependency Treebank (DDT) (Kromann, 2003). This has 25 tags. We adapted the tagger to Danish by including internal automatic mapping of æ, ø and a˚ to two-letter diphthongs when both training and labelling, by adding extra sets of features for handling words and adjusting our unknown word threshold to compensate for the small corpus (as in Derczynski et al. (2013)), and by specifying the closed-class tags for this set and language. We also prefer a CRF-based classifier in order to get better whole-sequence accuracy, providing greater opportunities for later-stage tools such as dependency parsers to accurately process more of the corpus. Results are given in Table 1, comparing tokenand sentence-level accuracy to other work using the DDT and the TnT tagger (Brants, 2000). Stateof-the-art performance is achieved, with wholesentence tagging accuracy comparable to that of leading English taggers. 4.3 expression ---------igaa Num._jul Secondly, entities outs"
E14-2016,P05-1045,0,0.0155285,"Missing"
E14-2016,P13-1166,0,0.0227731,"Missing"
E14-2016,llorens-etal-2012-timen,1,0.830105,"at it permits flexible temporal anchors outside of reference time (Reichenbach, 1947) and the default structure of a calendar. For example, while in English one may use numbers to express a distance in days (two days from now) or into a month (the second of March), Danish permits these offsets from any agreed time. As a result, it is common to see expressions of the form 2. juledag, which in this case is the second christmas day and refers to 26th December. For this pipeline, we use finite state transducers to define how Danish timexes may be recognised. We then use the general-purpose TIMEN (Llorens et al., 2012) timex normalisation tool to provide calendar or TIMEX3 values for these expressions. Example rules are shown in Figure 2. Gazetteers High precision entity recognition can be achieved with gazetteer-based named entity recognition. This is a low-cost way of quickly getting decent performance out of existing toolkits. We include two special kinds of gazetteer for Danish. Firstly, it is important to annotation the names of entities specific to Denmark (e.g. Danish towns). 2 See https://en.wikipedia.org/wiki/List of Danes, minus musicians due to stage names 63 4.5 Named entities plied natural lang"
E14-2016,N03-1033,0,0.0429126,"Missing"
E14-2016,bick-2004-named,0,0.657205,"total – is converted to the single token i alt. A set list of these group formations is given in the Danish PAROLE guidelines. Another key difference is in the treatment of quoted phrases and hyphenation. Phrases connected in this way are often treated as single tokens. For example, the phrase “Se og hør”læserne (readers of “See and Hear”, a magazine) is treated as a single token under this scheme. Background The state of the art in Danish information extraction is not very interoperable or open compared to that for e.g. English. Previous work, while highperformance, is not available freely (Bick, 2004), or domain-restricted.1 This makes results difficult to reproduce (Fokkens et al., 2013), and leads to sub-optimal interoperability (Lee et al., 2010). Even recent books focusing on the topic are heavily licensed and difficult for the average academic to access. Further, prior tools are often in the form of discrete components, hard to extend or to integrate with other systems. Some good corpus resources are available, most recently the Copenhagen Dependency Treebank 4.2 Part-of-Speech tagger We use a machine-learning based tagger (Toutanova et al., 2003) for Danish partof-speech labelling. T"
E14-2016,A00-1031,0,0.274838,"thongs when both training and labelling, by adding extra sets of features for handling words and adjusting our unknown word threshold to compensate for the small corpus (as in Derczynski et al. (2013)), and by specifying the closed-class tags for this set and language. We also prefer a CRF-based classifier in order to get better whole-sequence accuracy, providing greater opportunities for later-stage tools such as dependency parsers to accurately process more of the corpus. Results are given in Table 1, comparing tokenand sentence-level accuracy to other work using the DDT and the TnT tagger (Brants, 2000). Stateof-the-art performance is achieved, with wholesentence tagging accuracy comparable to that of leading English taggers. 4.3 expression ---------igaa Num._jul Secondly, entities outside of Denmark sometimes have different names specific to the Danish language (e.g. Lissabon for Lisboa / Lisbon). As well as a standard strict-matching gazetteer, we include a “fuzzy” gazetteer specific to Danish that tolerates vowel orthography variation and the other changes introduced in the 1948 spelling reform. For locations, we extracted data for names of Danish towns from DBpedia and a local gazetteer,"
E14-2025,W10-0701,0,0.0146216,"t of this section discusses in more detail where reusable components and infrastructural support for automatic data mapping and user interface generation are necessary, in order to reduce the overhead of crowdsourcing NLP corpora. Introduction Annotation science (Hovy, 2010; Stede and Huang, 2012) and general purpose corpus annotation tools (e.g. Bontcheva et al. (2013)) have evolved in response to the need for creating highquality NLP corpora. Crowdsourcing is a popular collaborative approach that has been applied to acquiring annotated corpora and a wide range of other linguistic resources (Callison-Burch and Dredze, 2010; Fort et al., 2011; Wang et al., 2012). Although the use of this approach is intensifying, especially paid-for crowdsourcing, the reuse of annotation guidelines, task designs, and user interfaces between projects is still problematic, since these are generally not made available, despite their important role in result quality (Khanna et al., 2010). A big outstanding challenge for crowdsourcing projects is that the cost to define a single 2.1 Project Definition An important part of project definition is the mapping of the NLP problem into one or more crowdsourcing tasks, which are sufficiently"
E14-2025,P02-1022,1,0.483231,"ists of three kinds of tasks: task workflow and management, contributor management (including profiling and retention), and quality control. Paid-for marketplaces like Amazon Mechanical Turk and CrowdFlower already provide this support. As with conventional corpus annotation, quality control is particularly challenging, and additional NLP-specific infrastructural support can help. 2.4 3.2 Data Evaluation and Aggregation The plugin expects documents to be presegmented into paragraphs, sentences and word tokens, using a tokeniser, POS tagger, and sentence splitter – e.g. those built in to GATE (Cunningham et al., 2002). The GATE Crowdsourcing plugin allows choice between these of which to use as the crowdsourcing task unit; e.g., to show one sentence per unit or one paragraph. In the demonstration we will show both automatic mapping at sentence level (for named entity annotation) and at paragraph level (for named entity disambiguation). In this phase, additional NLP-specific, infrastructural support is needed for evaluating and aggregating the multiple contributor inputs into a complete linguistic resource, and in assessing the resulting overall quality. Next we demonstrate how these challenges have been ad"
E14-2025,J11-2010,0,0.00831923,"re detail where reusable components and infrastructural support for automatic data mapping and user interface generation are necessary, in order to reduce the overhead of crowdsourcing NLP corpora. Introduction Annotation science (Hovy, 2010; Stede and Huang, 2012) and general purpose corpus annotation tools (e.g. Bontcheva et al. (2013)) have evolved in response to the need for creating highquality NLP corpora. Crowdsourcing is a popular collaborative approach that has been applied to acquiring annotated corpora and a wide range of other linguistic resources (Callison-Burch and Dredze, 2010; Fort et al., 2011; Wang et al., 2012). Although the use of this approach is intensifying, especially paid-for crowdsourcing, the reuse of annotation guidelines, task designs, and user interfaces between projects is still problematic, since these are generally not made available, despite their important role in result quality (Khanna et al., 2010). A big outstanding challenge for crowdsourcing projects is that the cost to define a single 2.1 Project Definition An important part of project definition is the mapping of the NLP problem into one or more crowdsourcing tasks, which are sufficiently simple to be carri"
E14-2025,ide-etal-2000-xces,0,0.0552745,"ion This stage, in particular, can benefit significantly from infrastructural support and reusable components, in order to collect the data (e.g. crawl the web, download samples from Twitter), preprocess it with linguistic tools (e.g. tokenisation, POS tagging, entity recognition), and then map automatically from documents and sentences to crowdsourcing micro-tasks. 2.3 Figure 1: Classification UI Configuration et al., 2002), which was chosen for its support for overlapping annotations and the wide range of automatic pre-processing tools available. GATE also has support for the XCES standard (Ide et al., 2000) and others (e.g. CoNLL) if preferred. Annotations are grouped in separate annotation sets: one for the automatically pre-annotated annotations, one for the crowdsourced judgements, and a consensus set, which can be considered as the final resulting corpus annotation layer. In this way, provenance is fully tracked, which makes it possible to experiment with methods that consider more than one answer as potentially correct. Running the Crowdsourcing Project This is the main phase of each crowdsourcing project. It consists of three kinds of tasks: task workflow and management, contributor manage"
E14-2025,P10-5004,0,\N,Missing
E14-4014,I13-1041,0,0.0369275,"Missing"
E14-4014,P02-1022,1,0.66554,"Missing"
E14-4014,P11-1037,0,0.0243396,"this system as our baseline. 3 Experimental Setup 3.1 Corpus The experiments combine person annotations from three openly-available datasets: Ritter et al. (2011), UMBC (Finin et al., 2010) and MSM2013 (Basave et al., 2013). In line with previous research (Ritter et al., 2011), annotations on @mentions are filtered out. The placeholder tokens in MSM data (i.e. MENTION , HASHTAG , URL ) are replaced with @Mention, #hashtag, and http://url/, respectively, to give case and character n-grams more similar to the original values. The total corpus has 4 285 tweets, around a third the size of that in Liu et al. (2011). This dataset contains 86 352 tokens with 1 741 entity mentions. Person entity recognition was chosen as it is a challenging entity type. Names of persons popular on Twitter change more frequently than e.g. locations. Person names also tend to have a long tail, not being confined to just public figures. Lastly, although all three corpora cover different entity types, they all have Person annotations. 3.2 Figure 1: Training curve for lem. Diagonal cross (blue) is CRF/PA, vertical cross (red) SVM/UM. lem: with added lemmas, lower-case versions of tokens, word shape, and neighbouring lemmas (in"
E14-4014,N13-1037,0,0.07521,"Missing"
E14-4014,W10-0713,0,0.0556158,"Computational Linguistics, pages 69–73, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics LDA and vocabularies: Ritter et al. (2011)’s T-NER system uses 2,400 labelled tweets, unlabelled data and Linked Data vocabularies (Freebase), as well as co-training. These techniques helped but did not bring person recognition accuracy above the supervised MaxEnt baseline in their experiments. We use this system as our baseline. 3 Experimental Setup 3.1 Corpus The experiments combine person annotations from three openly-available datasets: Ritter et al. (2011), UMBC (Finin et al., 2010) and MSM2013 (Basave et al., 2013). In line with previous research (Ritter et al., 2011), annotations on @mentions are filtered out. The placeholder tokens in MSM data (i.e. MENTION , HASHTAG , URL ) are replaced with @Mention, #hashtag, and http://url/, respectively, to give case and character n-grams more similar to the original values. The total corpus has 4 285 tweets, around a third the size of that in Liu et al. (2011). This dataset contains 86 352 tokens with 1 741 entity mentions. Person entity recognition was chosen as it is a challenging entity type. Names of persons popular on Twitt"
E14-4014,W09-1119,0,0.372934,"Missing"
E14-4014,D11-1141,0,0.196657,"trates that entity recognition using noise-resistant sequence labeling outperforms state-of-the-art Twitter NER, although we find that recall is consistently lower than precision. Secondly, to remedy this, we introduce a method for automatically post-editing the resulting entity annotations by using a discriminative classifier. This improves recall and precision. 69 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 69–73, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics LDA and vocabularies: Ritter et al. (2011)’s T-NER system uses 2,400 labelled tweets, unlabelled data and Linked Data vocabularies (Freebase), as well as co-training. These techniques helped but did not bring person recognition accuracy above the supervised MaxEnt baseline in their experiments. We use this system as our baseline. 3 Experimental Setup 3.1 Corpus The experiments combine person annotations from three openly-available datasets: Ritter et al. (2011), UMBC (Finin et al., 2010) and MSM2013 (Basave et al., 2013). In line with previous research (Ritter et al., 2011), annotations on @mentions are filtered out. The placeholder t"
E14-4014,P05-1045,0,0.0344035,"Missing"
E14-4014,W03-0419,0,0.154486,"Missing"
L16-1040,P98-1029,0,0.559775,"example changes in feature representation – two systems may reach the same F-score, but on very different examples, depending on what information they use. If systems make the same mistakes, there is little uniqueness that sets the systems apart, and little to be gained by combining them. Conversely, if they make reasonably different mistakes, one may be able to profit from e.g. combining them and reducing the number of cases where they are both wrong, and conducting analyses. This difference, hidden by F-score, can be measured in terms of complementarity. 2. Complementarity Complementarity (Brill and Wu, 1998) is a measure of the difference in decisions made by two systems that assign labels. It represents the amount of times where one system is wrong that the other is correct. This is referred to as the complementary rate, and works as follows. Given systems A and B, Comp(A, B) is the proportion of cases in which A is wrong and B is correct. From Brill’s paper: Comp(A, B) = 1 − ( common errors ) errors only in A (1) In this case, Comp(A, B) shows the proportion of errors in A that could be corrected by using B’s responses. Another way of looking at it is that Comp(A, B) shows the maximum improveme"
L16-1040,P02-1022,0,0.0151981,"andard LTG 85.1 TRIPS 85.1 Complementary Comp(LTG, TRIPS) 32.5 Comp(TRIPS, TRIOS) 15.5 Recall F1 82.2 85.1 83.6 85.1 6.37 4.55 10.7 7.03 Table 2: Temporal expression annotation: first standard precision, recall and F-score, and then complementary. organisations in text. It is a long-standing challenge in NLP with many applications (Nadeau and Sekine, 2007). For this example, we look look at location names present in the CoNLL 2003 dataset (Tjong Kim Sang and De Meulder, 2003). We compare two approaches based on different technologies: ANNIE, which uses gazetteers and finite state transducers (Cunningham et al., 2002); and Open NLP, which uses statistical learning to perform sequence labelling (Baldridge, 2005). Results are shown in Table 1. The first row of complementary figures are for the maximum improvement that OpenNLP offers over ANNIE, and the second for the maximum improvement ANNIE offers over OpenNLP. From these results, we can see that OpenNLP offers strong precision improvement potential over ANNIE. Also, from the high complementary recall that ANNIE offers over OpenNLP, adding ANNIE’s results is likely to offer strong recall improvements. However, from PComp (OpenN LP, AN N IE) being only 20.0"
L16-1040,P09-1079,0,0.0192777,"partof-speech tagging there is no argument over which tokens should be labelled: every token gets a label. This is a classical classification scenario, where for a set of instances I, each instance i ∈ I is assigned a class label from inventory C. The could also be something like assigning a spatial relation, or determining the language of a text. When combining approaches, it is useful to know how different their performances are. The idea of combining classifiers into ensembles is not new to NLP or in general: there is considerable research in supervised (Zhou et al., 2005) semi-supervised (Dasgupta and Ng, 2009) and unsupervised (Zimek et al., 2013) ensembles. Complementarity 261 has uses in building ensembles: intuitively, such constructions require diversity to function, and this intuition is borne out in practice (Zhou et al., 2005; Uzuner et al., 2011). Regardless of how voting is conducted, at least one contributing system should get the right answer, and complementarity indicates when this happens. Complementarity has three key behaviours. Namely: • High disagreement is a strong indicator of error. This is a byproduct of how approaches to many tasks that use similar information will achieve sim"
L16-1040,S10-1074,0,0.0256294,"t likely to be made. 5.2. Temporal Expression Extraction Temporal expressions are another kind of entity prevalent in text, this time describing a time or period of time. Recognising these entities is difficult and has been the subject of much recent research, including shared challenges. To evaluate performance over these expressions, We use the TempEval-2 dataset (Verhagen et al., 2010). In the challenge based on this dataset, one system was used as a baseline for others: a good scenario for applying complementary precision and recall. We compare a system based on semantic information, LTG (Grover et al., 2010), to an advanced parser-based system, TRIPS (UzZaman and Allen, 2010). Results are given in Table 2. In this instance, we can see that despite having similar F-scores in the overall evaluation, the systems score well on quite different results. Both can offer the other an increase in precision, with TRIPS offering higher quality results than LTG. However, only minor changes in recall are 264 possible, suggesting that both systems make similar errors in terms of false negatives. 6. Related Work Cohen’s kappa (Cohen and others, 1960) is often used to measure the difference between responses of r"
L16-1040,W03-0419,0,0.299263,"Missing"
L16-1040,S10-1062,0,0.0245654,"expressions are another kind of entity prevalent in text, this time describing a time or period of time. Recognising these entities is difficult and has been the subject of much recent research, including shared challenges. To evaluate performance over these expressions, We use the TempEval-2 dataset (Verhagen et al., 2010). In the challenge based on this dataset, one system was used as a baseline for others: a good scenario for applying complementary precision and recall. We compare a system based on semantic information, LTG (Grover et al., 2010), to an advanced parser-based system, TRIPS (UzZaman and Allen, 2010). Results are given in Table 2. In this instance, we can see that despite having similar F-scores in the overall evaluation, the systems score well on quite different results. Both can offer the other an increase in precision, with TRIPS offering higher quality results than LTG. However, only minor changes in recall are 264 possible, suggesting that both systems make similar errors in terms of false negatives. 6. Related Work Cohen’s kappa (Cohen and others, 1960) is often used to measure the difference between responses of raters (be they human or automatic). This provides a method of measuri"
L16-1040,S10-1010,0,\N,Missing
L16-1587,P13-1009,0,0.0128299,"of temporal entity annotation, TimeML annotations are preferred, requiring TIMEX3. In addition, DANTE is a monolingual tool for processing English only. Thus, it is less suitable than HeidelTime. One option for later work is to use a flexible, compositional temporal expression annotation system, i.e., separate tools for the extraction and the normalization of temporal expressions. This can prove especially useful for cases where rule-based interpretations cannot cover the given expression. Examples of such systems use contextfree grammars (Bethard, 2013) or language-independent latent parses (Angeli and Uszkoreit, 2013). This line of research has begun to reconsider the TIMEX annotation standards (Bethard and Parker, 2016). However, the available systems – while achieving superior performance on some of the common news-style test sets – can be either slow or restricted to just English. So, for the scope of GATE-Time, we prefer a high-accuracy, high-speed temporal tagger that works across multiple languages and domains. 3.4. HeidelTime GATE Wrapper The multilingual and domain-sensitive temporal tagger HeidelTime (Str¨otgen and Gertz, 2013) was initially developed within the UIMA framework (Ferrucci and Lally,"
L16-1587,D13-1078,0,0.0166094,"Since GATE-Time should be suitable for the full task of temporal entity annotation, TimeML annotations are preferred, requiring TIMEX3. In addition, DANTE is a monolingual tool for processing English only. Thus, it is less suitable than HeidelTime. One option for later work is to use a flexible, compositional temporal expression annotation system, i.e., separate tools for the extraction and the normalization of temporal expressions. This can prove especially useful for cases where rule-based interpretations cannot cover the given expression. Examples of such systems use contextfree grammars (Bethard, 2013) or language-independent latent parses (Angeli and Uszkoreit, 2013). This line of research has begun to reconsider the TIMEX annotation standards (Bethard and Parker, 2016). However, the available systems – while achieving superior performance on some of the common news-style test sets – can be either slow or restricted to just English. So, for the scope of GATE-Time, we prefer a high-accuracy, high-speed temporal tagger that works across multiple languages and domains. 3.4. HeidelTime GATE Wrapper The multilingual and domain-sensitive temporal tagger HeidelTime (Str¨otgen and Gertz, 2013) was"
L16-1587,chang-manning-2012-sutime,0,0.0323009,"ocuments) SUTime∗ 89.4 91.3 90.3 67.4 HeidelTime 1.8∗ 93.1 87.7 90.3 77.6 HeidelTime 2.0 93.1 88.4 90.7 78.1 WikiWars (narrative Wikipedia documents) SUTime (new) 94.5 88.0 91.1 50.4 HeidelTime 2.0 98.3 86.1 91.8 83.1 Temporal Tagging In this section, we discuss some existing tools for temporal tagging, explain the importance of performing domainsensitive temporal tagging, motivate our choice of adding HeidelTime to GATE-Time, and present the new HeidelTime GATE wrapper. 3.1. Popular Tools for Temporal Tagging For the task of temporal tagging, two popular, publicly available tools are SUTime (Chang and Manning, 2012) and HeidelTime (Str¨otgen and Gertz, 2013). At TempEval3, SUTime achieved the best results for the extraction of temporal expressions in English, while HeidelTime won the full task of temporal tagging (correct extraction and normalization of temporal expressions) for English and Spanish (UzZaman et al., 2013). When processing news (and news-style) documents, SUTime and HeidelTime perform similarly. However, two major differences between them are that HeidelTime is multilingual, and that it applies different normalization strategies depending on the domain of the documents that are to be proce"
L16-1587,P02-1022,1,0.465859,"ed to an eight-year interval. An understanding of time in natural language text is critical to effective communication and analysis and must be accounted for in natural language processing and understanding. Over recent years, research has been published in the area of temporal annotation resulting in several publicly available tools for temporal information extraction. Unfortunately, many of the tools rely on license-restricted linguistic preprocessing components. In this paper, we present GATE-Time, an ISOTimeML (Pustejovsky et al., 2010) approach to temporal information extraction in GATE (Cunningham et al., 2002) which brings together a number of these existing tools, as well as an upgraded approach to event annotation, within GATE. We use HeidelTime, a state-of-the-art, actively maintained, multilingual and domain-sensitive temporal expression tagger, breaking it out of its prepackaged pipeline and making the core component available in GATE. We present a novel event annotation tool, based on the earlier Evita system, using statistical learning with uneven margins; this yields state-of-the-art results. As GATE-Time can make use of whatever linguistic preprocessing annotations are provided in GATE, it"
L16-1587,S13-2004,0,0.142658,". The EVITA system (Saur´ı et al., 2005) is a freely available tool for TimeML event recognition, and has achieved good results on the official task. It uses linguistic pre-processing and shallow syntactic information as features for machine learning. It requires a corpus annotated with tokens, sentences, POS tags, NP and VP chunks, possessive modifiers, and heads of noun phrases (NPs). TIPSem (Llorens et al., 2010) takes a different tack, using semantic role labelling to identify temporal uses of language, and then building this into a structured learning approach to event recognition. ATT1 (Jung and Stent, 2013) achieved the best scores in event extraction at TempEval-3. This system relies on both semantic and also syntactic information to perform event extraction, but it relies on lexical rather than semantic role features. It takes a sequence labelling approach to event extraction, using BIO tags, though it labels them using MaxEnt, a non-structured classifier with mild independence bias – quite different to the TIPSem approach. We decided to use the Evita system as our starting point for event extraction, as it was most compatible with GATE. As with HeidelTime, we could theoretically have just wri"
L16-1587,W05-0610,0,0.0167019,"ormation from the context surrounding the current token, features from different tokens can be weighted differently, based on their position in the context. The weighting scheme we use is the reciprocal scheme, which weights the surrounding tokens reciprocally to the distance to the token in the centre of the context window. This reflects the intuition that the nearer a neighbouring token is, the more important it is for classifying the given token. Previous experiments have shown that such a weighting scheme typically obtains better results than the commonly used equal weighting of features (Li et al., 2005a). In our experiments, the same number of left and right tokens was taken as a context window. The window size was set to 4, which means that the algorithm uses features derived from 9 tokens: the 4 preceding, the current, and the 4 following tokens. Due to the use of a context window, the input vector is the combination of the feature vector of the current token and those of its neighbouring tokens. We also experimented with the use of some additional features for event recognition, although these did not make a vast difference to the results. In future work, we plan to experiment with combi"
L16-1587,C10-1082,0,0.0271345,"g Turkey’s Progress Report. 4.1. Popular Tools for Event Extraction Prior work on event extraction encompasses both rulebased and statistical work, though there are not many examples of existing systems. The EVITA system (Saur´ı et al., 2005) is a freely available tool for TimeML event recognition, and has achieved good results on the official task. It uses linguistic pre-processing and shallow syntactic information as features for machine learning. It requires a corpus annotated with tokens, sentences, POS tags, NP and VP chunks, possessive modifiers, and heads of noun phrases (NPs). TIPSem (Llorens et al., 2010) takes a different tack, using semantic role labelling to identify temporal uses of language, and then building this into a structured learning approach to event recognition. ATT1 (Jung and Stent, 2013) achieved the best scores in event extraction at TempEval-3. This system relies on both semantic and also syntactic information to perform event extraction, but it relies on lexical rather than semantic role features. It takes a sequence labelling approach to event extraction, using BIO tags, though it labels them using MaxEnt, a non-structured classifier with mild independence bias – quite diff"
L16-1587,D10-1089,0,0.161517,"eidelTime and SUTime performance on TempEval-3 platinum and WikiWars. On WikiWars, HeidelTime is used with its narrative-style normalization strategy. ∗ official results reported by (UzZaman et al., 2013). use tense information, while a chronology assumption between the reference time and an underspecified expression in a narrative text is often valid (Str¨otgen, 2015). 3.3. Temporal Tagger Selection To compare SUTime and HeidelTime, we performed an evaluation on two publicly available corpora, the news corpus TempEval-3 platinum (UzZaman et al., 2013) and the narrative-style corpus WikiWars (Mazur and Dale, 2010), which contains Wikipedia articles about important wars in history. The evaluation results shown in Table 1 demonstrate the importance of applying domain-dependent normalization strategies. HeidelTime uses domain-dependent strategies and achieves high-quality normalization results on both corpora, while SUTime applies the same normalization strategy independent of the domain of the documents and shows a significant decrease in the normalization quality on the narrative corpus. Note that the WikiWars corpus contains annotations in TIMEX2 format and that both HeidelTime and SUTime extract tempo"
L16-1587,pustejovsky-etal-2010-iso,0,0.0337753,"or example, the sky was not always blue; George W. Bush’s presidency was confined to an eight-year interval. An understanding of time in natural language text is critical to effective communication and analysis and must be accounted for in natural language processing and understanding. Over recent years, research has been published in the area of temporal annotation resulting in several publicly available tools for temporal information extraction. Unfortunately, many of the tools rely on license-restricted linguistic preprocessing components. In this paper, we present GATE-Time, an ISOTimeML (Pustejovsky et al., 2010) approach to temporal information extraction in GATE (Cunningham et al., 2002) which brings together a number of these existing tools, as well as an upgraded approach to event annotation, within GATE. We use HeidelTime, a state-of-the-art, actively maintained, multilingual and domain-sensitive temporal expression tagger, breaking it out of its prepackaged pipeline and making the core component available in GATE. We present a novel event annotation tool, based on the earlier Evita system, using statistical learning with uneven margins; this yields state-of-the-art results. As GATE-Time can make"
L16-1587,H05-1088,0,0.121383,"Missing"
L16-1587,strotgen-gertz-2012-temporal,1,0.883429,"Missing"
L16-1587,S13-2001,1,0.880681,"he importance of performing domainsensitive temporal tagging, motivate our choice of adding HeidelTime to GATE-Time, and present the new HeidelTime GATE wrapper. 3.1. Popular Tools for Temporal Tagging For the task of temporal tagging, two popular, publicly available tools are SUTime (Chang and Manning, 2012) and HeidelTime (Str¨otgen and Gertz, 2013). At TempEval3, SUTime achieved the best results for the extraction of temporal expressions in English, while HeidelTime won the full task of temporal tagging (correct extraction and normalization of temporal expressions) for English and Spanish (UzZaman et al., 2013). When processing news (and news-style) documents, SUTime and HeidelTime perform similarly. However, two major differences between them are that HeidelTime is multilingual, and that it applies different normalization strategies depending on the domain of the documents that are to be processed. This is paramount when processing narrative-style documents such as Wikipedia articles (Str¨otgen and Gertz, 2012), as will be detailed in the following section. 3.2. Domain-sensitive Temporal Tagging In general, date and time expressions can be either explicit (e.g., January 2016), implicit (e.g., Saint"
L16-1587,verhagen-pustejovsky-2012-tarsqi,0,0.0183756,"nguages and used for a variety of applications (Derczynski et al., 2013). Major progress in temporal annotation has been stimulated by the TempEval shared task. This comprises a subset of decomposed TimeML annotation tasks – for example, typing the temporal relations between events in the same sentence, or identifying temporal expression boundaries. The three main tasks have all attracted attention, with several temporal annotation tasks at SemEval’15. With regard to integrated systems for temporal annotation: the earliest are Tango & Callisto (Verhagen et al., 2006); later, there was Tarsqi (Verhagen and Pustejovsky, 2012). Time and event annotation technology has moved on significantly since the last iteration of these, and so an update is required. Further, prior efforts are often standalone and sometimes closed-source tools, unlike GATE, an established open-source community tool maintained by a team of active developers. Our approach to extract events builds on the earlier approaches to replicating Evita in a statistical learning environment (Demidova et al., 2013). 3702 3. relaxed extr. extr. & norm. p r f1 value f1 TE-3 platinum (news documents) SUTime∗ 89.4 91.3 90.3 67.4 HeidelTime 1.8∗ 93.1 87.7 90.3 77"
L16-1587,verhagen-etal-2006-annotation,0,0.0411244,"eme has been a great success, ported into many languages and used for a variety of applications (Derczynski et al., 2013). Major progress in temporal annotation has been stimulated by the TempEval shared task. This comprises a subset of decomposed TimeML annotation tasks – for example, typing the temporal relations between events in the same sentence, or identifying temporal expression boundaries. The three main tasks have all attracted attention, with several temporal annotation tasks at SemEval’15. With regard to integrated systems for temporal annotation: the earliest are Tango & Callisto (Verhagen et al., 2006); later, there was Tarsqi (Verhagen and Pustejovsky, 2012). Time and event annotation technology has moved on significantly since the last iteration of these, and so an update is required. Further, prior efforts are often standalone and sometimes closed-source tools, unlike GATE, an established open-source community tool maintained by a team of active developers. Our approach to extract events builds on the earlier approaches to replicating Evita in a statistical learning environment (Demidova et al., 2013). 3702 3. relaxed extr. extr. & norm. p r f1 value f1 TE-3 platinum (news documents) SUT"
L16-1587,L16-1599,0,\N,Missing
llorens-etal-2012-timen,S10-1062,0,\N,Missing
llorens-etal-2012-timen,derczynski-gaizauskas-2010-analysing,1,\N,Missing
llorens-etal-2012-timen,S10-1071,0,\N,Missing
llorens-etal-2012-timen,P00-1010,0,\N,Missing
llorens-etal-2012-timen,W03-0502,0,\N,Missing
llorens-etal-2012-timen,P05-3021,0,\N,Missing
llorens-etal-2012-timen,S10-1010,0,\N,Missing
llorens-etal-2012-timen,S10-1063,1,\N,Missing
N19-1157,P14-2131,0,0.4378,"tures. 1 Introduction Distributionally generated word classes (often referred to as word clusters) are hard clusters, containing all word types observed in a corpus, allocated to clusters based on contextual information observed in the corpus. They have found wide use in Natural Language Processing (NLP) systems as an alternative to word embeddings such as word2vec (Mikolov et al., 2013). Word clusters differentiate themselves from word embeddings by requiring estimation of many fewer parameters, and by their ability to derive qualitative representations from smaller corpora (Qu et al., 2015; Bansal et al., 2014). Brown Clusters (Brown et al., 1992) are a wellknown approach based on hard, hierarchical, distributionally derived groups of word types observed in a corpus of unstructured text, with Average Mutual Information (AMI) as the optimizaIra Assent Department of Computer Science Aarhus University Aarhus, Denmark ira@cs.au.dk tion goal. Exchange Clusters are an alternative approach obtained by applying the Exchange Algorithm (Kneser and Ney, 1993) to the same optimization goal. Unlike Brown, Exchange outputs a flat clustering, with no hierarchy (Martin et al., 1998). When only the bottom of the hie"
N19-1157,D10-1056,0,0.0266009,"). Most of this work, like (Swain and Cole, 2016) uses the word clusters as sources of features which are combined with hand-designed ones. While word clusters derived using Exchange and Brown clustering have found wide use in NLP systems, their use has been based on the assumption that they encode morphosyntactic and semantic information rather that a principled use. In relation to Parts of Speech, early on Martin et al. (1998) concluded that initializing Exchange with PoS-homogeneous clusters has no effect on final clustering AMI, but that it does help accelerate convergence. More recently, Christodoulopoulos et al. (2010) found that Brown clusters match the performance of more sophisticated clustering methods, despite their simple algorithmic construction. The study focused on using word clustering algorithms as sources of prototypal information to prototype-driven learning models for classification. In this paper, we study the amount of morphosyntactic information encoded in Brown and Exchange word clusters with the goal of providing empirical results for a principled use of such clusters in downstream tasks. 3 Metric selection In order to determine the amount of morphosyntactic information encoded in Brown a"
N19-1157,N16-1139,0,0.14461,"Missing"
N19-1157,R15-1016,1,0.799004,"enmark ira@cs.au.dk tion goal. Exchange Clusters are an alternative approach obtained by applying the Exchange Algorithm (Kneser and Ney, 1993) to the same optimization goal. Unlike Brown, Exchange outputs a flat clustering, with no hierarchy (Martin et al., 1998). When only the bottom of the hierarchy is used, like in this paper, Exchange and Brown clusters are interchangeable. Both Brown and Exchange clusters have been used as word representations for various Natural Language Processing tasks such as Part of Speech tagging in clean and noisy text (Swain and Cole, 2016; Owoputi et al., 2013; Derczynski et al., 2015), dependency parsing (Koo et al., 2008; Bansal et al., 2014), Chinese Word Segmentation (Liang, 2005), and Named Entity Recognition (Swain and Cole, 2016; Derczynski et al., 2015; Liang, 2005). Word clusters distinguish themselves from word embedding models by their ability to learn from little data (Bansal et al., 2014; Qu et al., 2015); for example, in cases like (Bansal et al., 2014), word clusters outperform other kinds of representations, including word embeddings. In the literature, it is often observed that word clusters seem to encode a considerable amount of morphosyntactic and semant"
N19-1157,2005.mtsummit-papers.11,0,0.0223437,"ng.” is transformed into “words NOUN have VERB meaning NOUN . PUNCT”. Both word clustering algorithms studied in this paper are insensitive to the appended PoS tags as they operate at word and not character level. The appended tags allow us to evaluate the quality of word clusters using the measures described in the previous section. We replace all numbers, dates, times, URLs and emails with placeholders in order to reduce vocabulary size. Universal Dependencies is the largest manually annotated corpus we have access to. For experiments on larger corpora, we use the unlabeled EuroParl corpus (Koehn, 2005). More specifically, the English-French and English-Czech pairs. Since manually annotated PoS tags are not available for Europarl, we append automatically assigned PoS tags, obtained by using UDPipe (Straka and Strakov´a, 2017) pretrained on manually annotated corpora from Universal Dependencies. We use flat clusters from the Exchange clustering algorithm for all experiments reported in this section as they outperform the flat clustering resulting from Brown in terms of Average Mutual Information (their optimization goal), Adjusted Mutual Information (AdjMI) and cluster purity. All observation"
N19-1157,P08-1068,0,0.411235,"Missing"
N19-1157,E17-5001,0,0.0567493,"Missing"
N19-1157,N13-1039,0,0.117535,"Missing"
N19-1157,K15-1009,0,0.446964,"word cluster features. 1 Introduction Distributionally generated word classes (often referred to as word clusters) are hard clusters, containing all word types observed in a corpus, allocated to clusters based on contextual information observed in the corpus. They have found wide use in Natural Language Processing (NLP) systems as an alternative to word embeddings such as word2vec (Mikolov et al., 2013). Word clusters differentiate themselves from word embeddings by requiring estimation of many fewer parameters, and by their ability to derive qualitative representations from smaller corpora (Qu et al., 2015; Bansal et al., 2014). Brown Clusters (Brown et al., 1992) are a wellknown approach based on hard, hierarchical, distributionally derived groups of word types observed in a corpus of unstructured text, with Average Mutual Information (AMI) as the optimizaIra Assent Department of Computer Science Aarhus University Aarhus, Denmark ira@cs.au.dk tion goal. Exchange Clusters are an alternative approach obtained by applying the Exchange Algorithm (Kneser and Ney, 1993) to the same optimization goal. Unlike Brown, Exchange outputs a flat clustering, with no hierarchy (Martin et al., 1998). When only"
N19-1157,W18-3802,0,0.0286208,"Missing"
N19-1157,K17-3009,0,0.0716764,"Missing"
N19-1157,P08-1086,0,0.488872,"r followed two major directions: algorithm improvements and applications in Natural Language Processing. In contrast little focus has been placed on understanding and evaluating the information content of the clusters. In the direction of algorithm improvements, work has been done on the effect of greedy merge choices in Brown Clustering (Derczynski and Chester, 2016; Ciosici, 2015) and extension of AMI to n-grams (Martin et al., 1998). Model relaxations, particularly to Exchange, aim to improve computational performance by reducing the effect of words swapping clusters (Dehdari et al., 2016; Uszkoreit and Brants, 2008). As mentioned earlier, both Brown and Exchange clusters have seen many applications in Natural Language Processing (NLP) systems: PoS tagging (Swain and Cole, 2016; Owoputi et al., 2013; Derczynski et al., 2015), dependency parsing (Koo et al., 2008; Bansal et al., 2014), Chinese Word Segmentation (Liang, 2005), and Named Entity Recognition (Swain and Cole, 2016; Derczynski et al., 2015; Liang, 2005). Most of this work, like (Swain and Cole, 2016) uses the word clusters as sources of features which are combined with hand-designed ones. While word clusters derived using Exchange and Brown clus"
P13-2114,A00-2031,0,0.158814,"Missing"
P13-2114,P07-2044,0,0.0762794,"detailed analysis of the performance of a set of labelling techniques when using temporal signals. 3 class, aspect, modality, tense, polarity, part of speech; and, for times: value, type, function in document, mod, quant. To these are added same-tense and same-aspect features, as well as the string values of events/times. The feature groups we use here are: • Base – The attributes of TimeML annotations involved (includes tense, aspect, polarity and so on as above), as with previous approaches. • Argument Ordering – Two features: a boolean set if both arguments are in the same sentence (as in Chambers et al. (2007)), and the text order of argument intervals (as in Hepple et al. (2007)). Experimental Setup We only approach the relation typing task, and we use existing signal annotations – that is, we do not attempt to automatically identify temporal signals. The corpus used is the signal-curated version of TimeBank (Pustejovsky et al., 2003). This corpus, TBsig,1 adds extra events, times and relations to TimeBank, in an effort to correct signal under-annotation in the original corpus (Derczynski and Gaizauskas, 2011). Like the original TimeBank corpus, it comprises 183 documents. In these, we are interes"
P13-2114,W13-0107,1,0.845944,"Missing"
P13-2114,D12-1062,0,0.120211,"Missing"
P13-2114,R11-1004,0,0.0603566,"Missing"
P13-2114,S07-1098,1,0.899661,"Missing"
P13-2114,S13-2001,1,0.872271,"Missing"
P13-2114,P03-1054,0,0.0200137,"Missing"
P13-2114,P06-1095,0,0.0907548,"Missing"
P13-2114,P10-1073,0,0.0608226,"Missing"
P13-2114,S10-1063,0,0.152025,"Missing"
P13-2114,P09-1046,0,0.0706006,"Missing"
P13-2114,S07-1046,0,0.072222,"Missing"
R13-1011,P05-1045,0,0.00426474,"Missing"
R13-1011,P11-2008,0,0.0145767,"Missing"
R13-1011,P11-1038,0,0.144722,"Missing"
R13-1011,P02-1022,1,0.360301,"tter is a cascade of finite-state transducers which segments text into sentences. This module is required for the POS tagger. The ANNIE sentence splitter is reused without modification, although when processing tweets, it is also possible to just use the text of the tweet as one sentence, without further analysis. The normaliser, the adapted POS tagger, and named entity recognition are discussed in detail in Sections 3.4, 3.5, and 3.6 respectively. The TwitIE IE Pipeline The open-source GATE NLP framework (Cunningham et al., 2013) comes pre-packaged with the ANNIE general purpose IE pipeline (Cunningham et al., 2002). ANNIE consists of the following main processing resources: tokeniser, sentence splitter, POS tagger, gazetteer lists, finite state transducer (based on GATE’s built-in regular expressions over annotations language), orthomatcher and coreference resolver. The resources communicate via GATE’s annotation API, which is a directed graph of arcs bearing arbitrary feature/value data, and nodes rooting this data into document content. The ANNIE components can be used individually or coupled together with new modules in order to create new applications. TwitIE re-uses the sentence splitter and name g"
R13-1011,D12-1039,0,0.00994453,"allenges to existing tools, being rich in previously unseen tokens, elision of words, and unusual grammar. Normalisation is commonly proposed as a solution for overcoming or reducing linguistic noise (Sproat et al., 2001). The task is generally approached in two stages: first, the identification of orthographic errors in an input discourse, and second, the correction of these errors. The TwitIE Normaliser is a combination of a generic spelling-correction dictionary and a spelling correction dictionary, specific to social media. The latter contains entries such as “2moro” and “brb”, similar to Han et al. (2012). Figure 4 shows an example tweet, where the abbreviation “Govt” has been normalised to government. Instead of a fixed list of variations, it is also possible to use a heuristic to suggest correct spellings. Both 86 text edit distance and phonetic distance can be used to find candidate matches for words identified as misspelled. (Han and Baldwin, 2011) achieved good corrections in many cases by using a combination of Levenshtein distance and double-metaphone distance between known words and words identified as incorrectly entered. We also experimented with this normalisation approach in TwitIE"
R13-1011,P12-1055,0,0.100531,"Missing"
R13-1011,P07-1033,0,0.0100292,"Missing"
R13-1011,J93-2004,0,0.042879,"Missing"
R13-1011,P12-1086,0,0.0400909,"Missing"
R13-1011,R13-1026,1,0.446745,"ess grammatical than longer posts, contain unorthodox capitalisation, and make frequent use of emoticons, abbreviations and hashtags, which can form an important part of the meaning. To combat these problems, research has focused on microblog-specific information extraction algorithms (e.g. named entity recognition for Twitter using CRFs (Ritter et al., 2011), Wikipedia-based topic and entity disambiguation (van Erp et al., 2013)). Particular attention is given to microtext normalisation, as a way of removing some of the linguistic noise prior to part-of-speech tagging and entity recognition (Derczynski et al., 2013a; Han and Baldwin, 2011; Han et al., 2012). Named entity recognition of longer texts, such as news, is a very well studied problem (cf. (Nadeau and Sekine, 2007; Roberts et al., 2008; Marrero et al., 2009)). For Twitter, some approaches have been proposed but often they are not freely available. Ritter et al. (Ritter et al., 2011) take a pipeline approach performing first tokenisation and POS tagging before using topic models to find named entities. Liu (Liu et al., 2012) Introduction Researchers have started recently to study the problem of mining social media content automatically (e.g. (Ro"
R13-1011,pak-paroubek-2010-twitter,0,0.0210778,"Missing"
R13-1011,D11-1141,0,0.385284,"er news articles, there is a low amount of discourse information per microblog document, and threaded structure is fragmented across multiple documents, flowing in multiple directions. Second, microtexts also exhibit much more language variation, tend to be less grammatical than longer posts, contain unorthodox capitalisation, and make frequent use of emoticons, abbreviations and hashtags, which can form an important part of the meaning. To combat these problems, research has focused on microblog-specific information extraction algorithms (e.g. named entity recognition for Twitter using CRFs (Ritter et al., 2011), Wikipedia-based topic and entity disambiguation (van Erp et al., 2013)). Particular attention is given to microtext normalisation, as a way of removing some of the linguistic noise prior to part-of-speech tagging and entity recognition (Derczynski et al., 2013a; Han and Baldwin, 2011; Han et al., 2012). Named entity recognition of longer texts, such as news, is a very well studied problem (cf. (Nadeau and Sekine, 2007; Roberts et al., 2008; Marrero et al., 2009)). For Twitter, some approaches have been proposed but often they are not freely available. Ritter et al. (Ritter et al., 2011) take"
R13-1011,roberts-etal-2008-combining,0,0.00920463,"Missing"
R13-1011,N03-1033,0,0.00963143,"Missing"
R13-1015,derczynski-etal-2012-massively,1,0.845999,"However, as with many natural language processing problems, diminishing returns are being seen in the field. Therefore, next efforts must address the temporal expressions that we cannot yet already detect and interpret. It is of interest to consider the automatic extraction of named timex resolution rules, perhaps using the most important timexes (Str¨otgen et al., 2012) from articles describing the corresponding occasion. It is also relevant to merge our named timex corpus with existing timex corpora See https://en.wikipedia.org/wiki/Computus 3 119 See http://derczynski.com/sheffield/ (e.g. Derczynski et al. (2012)), after annotating the conventional timexes in our named timex training data. Such a corpus could be extended by extracting sentences that cite the Wikipedia or DBpedia entries corresponding to named timexes. Evaluation against such a resource is less likely to overreport the variety of expressions recognised by timex annotation systems, and can provide a solid base for future wide-coverage approaches to temporal expression recognition. Decomposing the complex temporal annotation task so that it can be reliably crowdsourced would enable the construction of more resources. Using human computat"
R13-1015,P05-1045,0,0.00613788,"861 sentences (117 060 tokens) were extracted from English Gigaword v5 (Graff et al., 2003), containing 4 180 named timex annotations. The training split contained 1 053 of these sentences. The entire corpus construction method requires no human intervention aside from supplying source Wikipedia pages. Regarding the NTE recognisers, we adapted three entity recognition approaches to the task by discarding their default models and rebuilding new models based solely on this NTE corpus. The recognition tools were CRF-based: a multipurpose system incorporating non-local information, Stanford NER (Finkel et al., 2005); one for temporal entity recognition that uses semantic role information, TIPSem (Llorens et al., 2012b); and TIPSem-B, a baseline temporal entity recognition variant of TIPSem. Recognisers were learned from the training split and evaluated on the test split. As we are attempting to recognise named timexes only, we do not do comparison against tools designed for standard timex recognition, as these are designed for a different task. A na¨ıve gazetteer-matching baseline was used, based on timex strings found in existing resources (TimeBank and the AQUAINT TimeML annotations). This behaved exac"
R13-1015,N12-1049,0,0.0198686,"enEval Discovering such interpretations is a difficult task. For example, based on text, it is difficult to automatically learn or infer the link between “New Year’s Day” and 1st January, or the associations between north/south hemisphere and which months fall in summer, especially given the cost of temporal annotation and resulting scarcity of annotated resources. This often leaves the task of developing such interpretations to human computation (Sabou et al., 2012). The closest computational method for solving this problem uses a more flexible compositional approach to timex interpretation (Angeli et al., 2012), though it is prone to floundering and failing on completely new expressions, such as named timexes. As the named timexes mined from Wikipedia were generally accompanied by a textual description of the time (e.g., as in Figure 3), we used these descriptions to work out how to interpret the expression. We created a custom parser that worked well with the majority of uncurated, natural language descriptions of named timex dates. Having gathered information from Wikipedia, we then encoded it as rules in a popular timex interpretation system, TIMEN (Llorens et al., 2012a). TIMEN operates using ex"
R13-1015,J06-4003,0,0.0190813,"n of named temporal expressions, we moved on to the task of NTE discovery. Our approach was to first develop a statistical tagger adapted to NTE recognition, and then apply it to new data, to observe what expressions it annotates beyond those in the collection extracted from Wikipedia. The collection was used to construct a corpus and then a statistical named temporal expression recogniser. The corpus was constructed as follows. Using our list of monosemous named timexes, we searched the Gigaword corpus to retrieve paragraphs containing the timexes. These paragraphs were split into sentences (Kiss and Strunk, 2006), and the sentences matching any NTE were extracted; the sentences were then broken down into lists of tokens. We marked all monosemous named timexes in the sentences as target entities. Some NTEs are polysemous, having both temporal and non-temporal sense. Observation of a small part of the corpus suggested that these polysemous NTEs generally occurred in a temporal sense when in the same sentence as other temporal phrases. Rather than excluding any sentence containing a polysemous NTE from the corpus on grounds of ambiguity, based on this observation, we adopted a simple heuristic: polysemou"
R13-1015,llorens-etal-2012-timen,1,0.868446,"cato University of Bologna Leon Derczynski University of Sheffield matteo.brucato@studio.unibo.it leon@dcs.shef.ac.uk Hector Llorens Nuance Communications Kalina Bontcheva University of Sheffield Christian S. Jensen Aarhus University hector.llorens@nuance.com kalina@dcs.shef.ac.uk csj@cs.au.dk Abstract Phrases that explicitly describe certain periods of time, or temporal expressions, are particularly useful. They may be calendar dates, mentions of months, relative expressions like “tomorrow”, and so on. In-depth accounts of temporal expressions – timexes – are given by Ferro et al. (2005) and Llorens et al. (2012a). In this paper, we discuss a new class of timexes that signify a date or range of dates, but that do not explicitly include information about which dates these are (e.g., October 31 vs. Halloween). Following the description of expressions that clearly identify one entity from a set of others by use of a proper noun as named entities, we call these named temporal expressions (or NTEs). As with many linguistic phenomena, the phrases used as timexes have a power law-like frequency distribution in text. A few forms of expression make up for the bulk of occurrences of temporal expressions. Howev"
R13-1015,S10-1071,0,0.0811201,"Missing"
R13-1015,S13-2001,1,0.889807,"e named temporal expressions (or NTEs). As with many linguistic phenomena, the phrases used as timexes have a power law-like frequency distribution in text. A few forms of expression make up for the bulk of occurrences of temporal expressions. However, existing research has been typically evaluated on only a small corpus of hand-annotated temporal expressions. With such resources, it is difficult to build or evaluate tools for recognising or interpreting the lessfrequent temporal expressions, and this is reflected in the performance plateau of recent TempEval exercises (Verhagen et al., 2010; UzZaman et al., 2013). Existing temporal expression recognition tools are typically rule-based (Str¨otgen and Gertz, 2010). These perform reasonably well on existing datasets, achieving F-scores of around 0.90, and improving them is an active area of research. However, as temporal annotation is expensive, existing datasets are not particularly large, and therefore do not contain as challenging a variety of forms of expression as general, unannotated text. Therefore, evaluations using these resources This paper introduces a new class of temporal expression – named temporal expressions – and methods for recognising"
R13-1015,S10-1010,0,\N,Missing
R13-1026,R13-1011,1,0.223646,"Missing"
R13-1026,A00-1031,0,0.0624805,"work on bootstrapped PoS tagging is that of Clark et al. (2003), who use a cotraining approach to improve tagger performance using unlabeled data. 3 do not occur in the training data. Further, as per Manning (2011) we report the rate of getting whole sentences right, since “a single bad mistake in a sentence can greatly throw off the usefulness of a tagger to downstream tasks”.3 We evaluated four state-of-the-art trainable and publicly available PoS taggers that used the Penn Treebank tagsettrereetagger: SVMTool (Gim´enez and Marquez, 2004), the Stanford Tagger (Toutanova et al., 2003), TnT (Brants, 2000) and a transformation-based learning (TBL) tagger (Brill, 1995) supported by sequential n-gram backoff. The NLTK implementations of TnT and TBL were used (Bird et al., 2009). The ‘left3words’ model was used with the Stanford tagger, and ‘M0’ with SVMTool. For initial comparison, taggers were tested on standard newswire text from the Penn Treebank (Marcus et al., 1993),4 training with Wall Street Journal (WSJ) sections 0-18 and evaluating on sections 19-21. The base performance for each tagger is given in Table 1. 3.2 Three PoS-labeled microblog datasets are currently available. The T-Pos corpu"
R13-1026,J95-4004,0,0.451368,", who use a cotraining approach to improve tagger performance using unlabeled data. 3 do not occur in the training data. Further, as per Manning (2011) we report the rate of getting whole sentences right, since “a single bad mistake in a sentence can greatly throw off the usefulness of a tagger to downstream tasks”.3 We evaluated four state-of-the-art trainable and publicly available PoS taggers that used the Penn Treebank tagsettrereetagger: SVMTool (Gim´enez and Marquez, 2004), the Stanford Tagger (Toutanova et al., 2003), TnT (Brants, 2000) and a transformation-based learning (TBL) tagger (Brill, 1995) supported by sequential n-gram backoff. The NLTK implementations of TnT and TBL were used (Bird et al., 2009). The ‘left3words’ model was used with the Stanford tagger, and ‘M0’ with SVMTool. For initial comparison, taggers were tested on standard newswire text from the Penn Treebank (Marcus et al., 1993),4 training with Wall Street Journal (WSJ) sections 0-18 and evaluating on sections 19-21. The base performance for each tagger is given in Table 1. 3.2 Three PoS-labeled microblog datasets are currently available. The T-Pos corpus of 15K tokens introduced by Ritter et al. (2011) uses a tagse"
R13-1026,gimenez-marquez-2004-svmtool,0,0.0991477,"Missing"
R13-1026,W03-0407,0,0.0502941,"Missing"
R13-1026,P11-2008,0,0.690806,"Missing"
R13-1026,E03-1009,0,0.0208576,"Missing"
R13-1026,W02-2006,0,0.249811,"Missing"
R13-1026,P11-1038,0,0.117493,"Missing"
R13-1026,P02-1022,1,0.755704,"Missing"
R13-1026,P12-1109,0,0.017024,"Missing"
R13-1026,J93-2004,0,0.0447464,"downstream tasks”.3 We evaluated four state-of-the-art trainable and publicly available PoS taggers that used the Penn Treebank tagsettrereetagger: SVMTool (Gim´enez and Marquez, 2004), the Stanford Tagger (Toutanova et al., 2003), TnT (Brants, 2000) and a transformation-based learning (TBL) tagger (Brill, 1995) supported by sequential n-gram backoff. The NLTK implementations of TnT and TBL were used (Bird et al., 2009). The ‘left3words’ model was used with the Stanford tagger, and ‘M0’ with SVMTool. For initial comparison, taggers were tested on standard newswire text from the Penn Treebank (Marcus et al., 1993),4 training with Wall Street Journal (WSJ) sections 0-18 and evaluating on sections 19-21. The base performance for each tagger is given in Table 1. 3.2 Three PoS-labeled microblog datasets are currently available. The T-Pos corpus of 15K tokens introduced by Ritter et al. (2011) uses a tagset based on the Penn Treebank tagset,plus four new tags for URLs (URL), hashtags (HT), username mentions (USR) and retweet signifiers (RT). The DCU dataset of 14K tokens (Foster et al., 2011) is also based on the Penn Treebank (PTB) set, but does not have the same new tags as TPos, and uses slightly differe"
R13-1026,D11-1141,1,0.750123,"arning (TBL) tagger (Brill, 1995) supported by sequential n-gram backoff. The NLTK implementations of TnT and TBL were used (Bird et al., 2009). The ‘left3words’ model was used with the Stanford tagger, and ‘M0’ with SVMTool. For initial comparison, taggers were tested on standard newswire text from the Penn Treebank (Marcus et al., 1993),4 training with Wall Street Journal (WSJ) sections 0-18 and evaluating on sections 19-21. The base performance for each tagger is given in Table 1. 3.2 Three PoS-labeled microblog datasets are currently available. The T-Pos corpus of 15K tokens introduced by Ritter et al. (2011) uses a tagset based on the Penn Treebank tagset,plus four new tags for URLs (URL), hashtags (HT), username mentions (USR) and retweet signifiers (RT). The DCU dataset of 14K tokens (Foster et al., 2011) is also based on the Penn Treebank (PTB) set, but does not have the same new tags as TPos, and uses slightly different tokenisation. The ARK corpus of 39K tokens (Gimpel et al., 2011) uses a novel tagset, which, while suitable for the microblog genre, is somewhat less descriptive than the PTB sets on many points. For example, its V tag corresponds to any verb, conflating PTB’s VB, VBD, VBG, VB"
R13-1026,N03-1033,0,0.114204,"t al., 2011). Finally, classic work on bootstrapped PoS tagging is that of Clark et al. (2003), who use a cotraining approach to improve tagger performance using unlabeled data. 3 do not occur in the training data. Further, as per Manning (2011) we report the rate of getting whole sentences right, since “a single bad mistake in a sentence can greatly throw off the usefulness of a tagger to downstream tasks”.3 We evaluated four state-of-the-art trainable and publicly available PoS taggers that used the Penn Treebank tagsettrereetagger: SVMTool (Gim´enez and Marquez, 2004), the Stanford Tagger (Toutanova et al., 2003), TnT (Brants, 2000) and a transformation-based learning (TBL) tagger (Brill, 1995) supported by sequential n-gram backoff. The NLTK implementations of TnT and TBL were used (Bird et al., 2009). The ‘left3words’ model was used with the Stanford tagger, and ‘M0’ with SVMTool. For initial comparison, taggers were tested on standard newswire text from the Penn Treebank (Marcus et al., 1993),4 training with Wall Street Journal (WSJ) sections 0-18 and evaluating on sections 19-21. The base performance for each tagger is given in Table 1. 3.2 Three PoS-labeled microblog datasets are currently availa"
R13-1026,W12-0601,0,\N,Missing
R15-1016,P11-1087,0,0.0176414,"clusters nearby in the binary tree. This is an unsupervised learned representation of language from the input corpus (Bengio et al., 2013). In the main implementation of Brown clustering (Liang, 2005), mutual information is measured at the bigram level. The resulting structure of word types can be used as feature representations in many NLP tasks, leading to quick, solid performance increases (Turian et al., 2010). In fact, as well as producing effective discriminative features, unsupervised hierarchical clusterings like Brown often lead to better taggers than models developed 20 years later (Blunsom and Cohn (2011), Owoputi et al. (2013)). Introduction Brown clustering (Brown et al., 1992) uses distributional information to group similar words. Unsupervised, it induces a hierarchical clustering over words to form a binary tree (e.g. Figure 1). This hierarchical clustering has recently been used in thousands of computational linguistics papers, often for feature generation. However, no work exists describing the behaviour and hyper-parametre tuning effects of Brown clustering; even the original paper concentrates on implementation rather than its behaviour. Except for a few forays off the beaten track (e"
R15-1016,J92-4003,0,0.829579,"on of language from the input corpus (Bengio et al., 2013). In the main implementation of Brown clustering (Liang, 2005), mutual information is measured at the bigram level. The resulting structure of word types can be used as feature representations in many NLP tasks, leading to quick, solid performance increases (Turian et al., 2010). In fact, as well as producing effective discriminative features, unsupervised hierarchical clusterings like Brown often lead to better taggers than models developed 20 years later (Blunsom and Cohn (2011), Owoputi et al. (2013)). Introduction Brown clustering (Brown et al., 1992) uses distributional information to group similar words. Unsupervised, it induces a hierarchical clustering over words to form a binary tree (e.g. Figure 1). This hierarchical clustering has recently been used in thousands of computational linguistics papers, often for feature generation. However, no work exists describing the behaviour and hyper-parametre tuning effects of Brown clustering; even the original paper concentrates on implementation rather than its behaviour. Except for a few forays off the beaten track (e.g. Christodoulopoulos et al. (2010), Owoputi et al. (2012), Derczynski et a"
R15-1016,N13-1039,0,0.136364,"Missing"
R15-1016,D10-1056,0,0.0482913,"oputi et al. (2013)). Introduction Brown clustering (Brown et al., 1992) uses distributional information to group similar words. Unsupervised, it induces a hierarchical clustering over words to form a binary tree (e.g. Figure 1). This hierarchical clustering has recently been used in thousands of computational linguistics papers, often for feature generation. However, no work exists describing the behaviour and hyper-parametre tuning effects of Brown clustering; even the original paper concentrates on implementation rather than its behaviour. Except for a few forays off the beaten track (e.g. Christodoulopoulos et al. (2010), Owoputi et al. (2012), Derczynski et al. (2015a)), default parametres dominate; either 800 or 1000 Brown 110 Proceedings of Recent Advances in Natural Language Processing, pages 110–117, Hissar, Bulgaria, Sep 7–9 2015. Bit path 00111001 001011111001 Word types can cn cann caan cannn ckan shalll ccan caaan cannnn caaaan ii id ion iv ll iii ud wd uma ul idnt provoking hed 1+1 ididnt hast ine 2+2 idw #thingsblackpeopledo iiii #onlywhitepeople dost doan uon apt-get Table 1: Sample Brown clusters over English tweets.1 Each set of terms is a leaf in the hierarchy. Figure 2: Expected cluster qualit"
R15-1016,C14-1168,0,0.0135599,"cy at token level. The “oct27” training and test splits are used. Terminals We note that Brown et al. (1992) assume a corpus long enough (T → ∞) that the final term in Equation 1 tends to 1, and so P r(c1 |c2 ) tends to the relative frequency of consecutive classes c1 c2 . P r(c1 |c2 ) = Evaluation 5.2 Named Entity Recognition We simplify NER to isolate the impact of c and |T |. A CRF (Okazaki, 2007) is used to train and classify NER models. The only features are Brown cluster path prefices of length [4,6,10,20] for newswire, as per Ratinov and Roth (2009), and [2,4,8,16] for newswire, as per Plank et al. (2014). For newswire, we train and evaluate on the CoNLL data (Tjong Kim Sang and De Meulder, 2003) taking RCV clusters as input. For social media, we use the CRF with passive-aggressive updates to overcome some social media noise (Derczynski and Bontcheva, 2014), and train and eval(1) When corpora are composed of long, structured documents, bigrams are unlikely to cross the boundaries of unrelated sentences. However, in social media corpora there is little running discourse: each document is ≤ 140 characters and usually just one sentence. Running discourse only occurs when consecutive messages are"
R15-1016,E14-4014,1,0.827855,"consecutive classes c1 c2 . P r(c1 |c2 ) = Evaluation 5.2 Named Entity Recognition We simplify NER to isolate the impact of c and |T |. A CRF (Okazaki, 2007) is used to train and classify NER models. The only features are Brown cluster path prefices of length [4,6,10,20] for newswire, as per Ratinov and Roth (2009), and [2,4,8,16] for newswire, as per Plank et al. (2014). For newswire, we train and evaluate on the CoNLL data (Tjong Kim Sang and De Meulder, 2003) taking RCV clusters as input. For social media, we use the CRF with passive-aggressive updates to overcome some social media noise (Derczynski and Bontcheva, 2014), and train and eval(1) When corpora are composed of long, structured documents, bigrams are unlikely to cross the boundaries of unrelated sentences. However, in social media corpora there is little running discourse: each document is ≤ 140 characters and usually just one sentence. Running discourse only occurs when consecutive messages are from the 112 T = 1M SM F1 19.5 19.5 19.5 19.8 21.6 23.5 34.2 37.0 34.9 41.2 37.7 37.8 T NW F1 8K 21.5 24.4 16K 32K 28.5 62.5K 29.9 125K 30.6 250K 31.8 500K 35.5 1M 36.5 T = 8M T = 62.5k SM F1 NW F1 NW F1 19.5 12.1 16.73 19.5 16.0 16.65 19.5 18.3 17.51 20.6"
R15-1016,K15-1009,0,0.208449,"nd to clusters that are similar to each other. clusters are generated in nearly every published use. Few experiments use other configurations, and we are not aware of any prior work on hyperparametre tuning for Brown clustering. This paper addresses this information gap, providing practitioners with principled insights into the algorithm. We provide an analysis of how Brown clustering adds information over input, and, based on this, describe models for the effect that corpus size and cluster count have on the quality of results. These models are then tested in two sequence labeling tasks, cf. Qu et al. (2015). Finally, we compare the initial analysis to observations, leading to concrete advice for practitioners. 2 Background Brown clustering uses mutual information to determine distributional similarity, placing similar words in the same cluster and similar clusters nearby in the binary tree. This is an unsupervised learned representation of language from the input corpus (Bengio et al., 2013). In the main implementation of Brown clustering (Liang, 2005), mutual information is measured at the bigram level. The resulting structure of word types can be used as feature representations in many NLP tas"
R15-1016,W09-1119,0,0.0459936,"ew clusters and measure the tagger’s resultant tagging accuracy at token level. The “oct27” training and test splits are used. Terminals We note that Brown et al. (1992) assume a corpus long enough (T → ∞) that the final term in Equation 1 tends to 1, and so P r(c1 |c2 ) tends to the relative frequency of consecutive classes c1 c2 . P r(c1 |c2 ) = Evaluation 5.2 Named Entity Recognition We simplify NER to isolate the impact of c and |T |. A CRF (Okazaki, 2007) is used to train and classify NER models. The only features are Brown cluster path prefices of length [4,6,10,20] for newswire, as per Ratinov and Roth (2009), and [2,4,8,16] for newswire, as per Plank et al. (2014). For newswire, we train and evaluate on the CoNLL data (Tjong Kim Sang and De Meulder, 2003) taking RCV clusters as input. For social media, we use the CRF with passive-aggressive updates to overcome some social media noise (Derczynski and Bontcheva, 2014), and train and eval(1) When corpora are composed of long, structured documents, bigrams are unlikely to cross the boundaries of unrelated sentences. However, in social media corpora there is little running discourse: each document is ≤ 140 characters and usually just one sentence. Run"
R15-1016,D11-1141,0,0.0424835,"Missing"
R15-1016,rose-etal-2002-reuters,0,0.166572,"Missing"
R15-1016,P12-3005,0,0.0226812,"Missing"
R15-1016,W03-0419,0,0.481404,"Missing"
R15-1016,P14-5010,0,0.00732874,"Missing"
R15-1016,P10-1040,0,0.0176243,"vations, leading to concrete advice for practitioners. 2 Background Brown clustering uses mutual information to determine distributional similarity, placing similar words in the same cluster and similar clusters nearby in the binary tree. This is an unsupervised learned representation of language from the input corpus (Bengio et al., 2013). In the main implementation of Brown clustering (Liang, 2005), mutual information is measured at the bigram level. The resulting structure of word types can be used as feature representations in many NLP tasks, leading to quick, solid performance increases (Turian et al., 2010). In fact, as well as producing effective discriminative features, unsupervised hierarchical clusterings like Brown often lead to better taggers than models developed 20 years later (Blunsom and Cohn (2011), Owoputi et al. (2013)). Introduction Brown clustering (Brown et al., 1992) uses distributional information to group similar words. Unsupervised, it induces a hierarchical clustering over words to form a binary tree (e.g. Figure 1). This hierarchical clustering has recently been used in thousands of computational linguistics papers, often for feature generation. However, no work exists desc"
R15-1017,S13-2001,1,0.803658,"nd us. When we automatically extract temporal information, we are often concerned with events and times – referred to collectively as temporal intervals. We might ask, for example, “Who is the current President of the USA?.” In order to extract a single contemporary answer to this question, we need to identify events related to persons becoming president and the times of those events. Crucially, however, we also need to identify the ordering between these events and times, by assigning a temporal relation type (from e.g. Allen (1983)). This last task, temporal relation typing, is challenging (UzZaman et al., 2013; Bethard et al., 2015), and is the focus of this paper. When events are expressed as verbs, tense and aspect are used to convey temporal features of these events. Thus, it is intuitive that tense and aspect will be of value in determining the type of temporal relation that holds between two verb events, and evidence in human-annotated corpora supports this intuition. 1 TimeBank is a corpus semantically annotated for temporal information in TimeML (Pustejovsky et al., 2003; Pustejovsky et al., 2004) 2 In TimeML v1.2, the tense attribute of events has values that are conflated with verb form. T"
R15-1017,Q14-1022,0,0.0613993,"b event orderings based on the Reichenbachian tenses that map directly to those in TimeML. Cell values describe the e1 [rel] e2 relationship. 3 In Example 1, the reference point is determined positionally with an explicit time (10 o’clock). TimeML provides some of the information that Reichenbach’s framework alone does not cater for and vice versa. A combination of the two may lead to better labelling performance, but relying on Reichenbach’s framework alone for rule-based temporal relation label constraint is insufficient. However, the framework has shown to inform prior systems effectively (Chambers et al., 2014). The situations we examine are those where two verb events occur in the same temporal context, where a timex directly influences a verb event, and also verb events that report other verb events. Reichenbach’s framework is used as a linguistic model that generates temporal ordering features, which are added to a base feature set. The base features are those as in Mani et al. (2007), i.e.: (1) It was 10 o’clock, and Sarah had brushed her teeth. The verb group had brushed is anterior past tense; that is, E &lt; R &lt; S. The event is complete before the reference time – that is, at any point until 10"
R15-1017,W13-0107,1,0.794775,"brushed is anterior past tense; that is, E &lt; R &lt; S. The event is complete before the reference time – that is, at any point until 10 o’clock – and so the relation between the event and timex can be determined (brushed BE FORE 10 o’clock). 2.3 The Framework in TLINK Typing Feature Extraction Two interpretations of the model are used in feature extraction. Firstly, a simple view is taken assuming permanence of the reference point. This provides a constraint dependent on the pairing of Reichenbachian tenses used, and is detailed in Table 2. Secondly, an advanced interpretation is used, following Derczynski and Gaizauskas (2013). This approach fully populates all Reichenbachian tense combinations using Freksa’s temporal semi-interval algebra (Freksa, 1992) to derive a (large) temporal constraint table, which for space reasons is omitted here. In all cases, the gold standard tense and aspect features annotated on the events in TimeBank are used as the basis for Reichenbachian representations. For each event: text; TimeML tense and aspect; modality; cardinality; polarity; event class; partof-speech tag. For each event pair: booleans for: are events in the same sentence; are events in adjacent sentences; do events have"
R15-1017,P06-1095,0,0.0934359,"Missing"
R15-1017,W13-1903,0,0.0540506,"Missing"
R15-1017,S15-2136,1,\N,Missing
R15-1018,aker-etal-2012-assessing,0,0.0219974,"is useful during corpus creation, in this section we examine how to apply it with an increasingly popular new annotation method: crowdsourcing. Crowdsourcing annotation works by presenting a many microtasks to non-expert workers. They typically make their judgements over short texts, after reading a short set of instructions (Sabou et al., 2014). Such judgments are often simpler than those in linguistic annotation by experts; for example, workers might be asked to annotate only a single class of entity at a time. Through crowdsourcing, quality annotations can be gathered quickly and at scale (Aker et al., 2012). There also tends to be a larger variance in reliability over crowd workers than in expert annotators (Hovy et al., 2013). For this reason, crowdsourced annotation microtasks are often all performed by at least two different workers. E.g., every sentence would be examined for each entity type by at least two different non-expert workers. We investigate entity pre-empting of crowdsourced corpora for a challenging genre: social media. Newswire corpora are not too hard to come by, especially for English, and the genre is somewhat biased in style, mostly being written or created by working-age mi"
R15-1018,E14-2025,1,0.477481,"fficult for existing NER tools to achieve good accuracy on (Derczynski et al., 2013; Derczynski et al., 2015) and having no large NE annotated corpora. In our setup, we subdivide the annotation task according to entity type. Workers perform best with light cognitive loads, so asking them to annotate one kind of thing at a time increases their agreement and accuracy (Krug, 2009; Khanna et al., 2010). Person, location and organisation entities are annotated, giving three annotation sub-tasks, following Bontcheva et al. (2015). Jobs were created automatically using the GATE crowdsourcing plugin (Bontcheva et al., 2014). An example sub-task is shown in Figure 1. This Dataset Base: 500 messages 500 msgs + 1k without entities 500 msgs + 1k random 500 msgs + 1k with entities P 70.39 85.00 76.14 71.21 R 31.66 25.15 44.38 54.14 F1 43.67 38.81 56.07 61.51 Table 8: Adding entity-less vs. entity-bearing data to a 500-message base training set means that we must pre-empt according to entity type, instead of just pre-empting whether or not an excerpt contains any entities at all, which has the additional effect of changing entitybearing/entity-free class distributions. We use two sources that share entity classificati"
R15-1018,E14-4014,1,0.82129,"Missing"
R15-1018,N13-1037,0,0.0141674,"ds to be a larger variance in reliability over crowd workers than in expert annotators (Hovy et al., 2013). For this reason, crowdsourced annotation microtasks are often all performed by at least two different workers. E.g., every sentence would be examined for each entity type by at least two different non-expert workers. We investigate entity pre-empting of crowdsourced corpora for a challenging genre: social media. Newswire corpora are not too hard to come by, especially for English, and the genre is somewhat biased in style, mostly being written or created by working-age middle-class men (Eisenstein, 2013), and in topic, being related to major events around unique entities that one might refer to by a special name. In contrast, social media text has broad stylistic variance (Hu et al., 2013) while also being difficult for existing NER tools to achieve good accuracy on (Derczynski et al., 2013; Derczynski et al., 2015) and having no large NE annotated corpora. In our setup, we subdivide the annotation task according to entity type. Workers perform best with light cognitive loads, so asking them to annotate one kind of thing at a time increases their agreement and accuracy (Krug, 2009; Khanna et"
R15-1018,D11-1141,0,0.0976123,"1k with entities P 70.39 85.00 76.14 71.21 R 31.66 25.15 44.38 54.14 F1 43.67 38.81 56.07 61.51 Table 8: Adding entity-less vs. entity-bearing data to a 500-message base training set means that we must pre-empt according to entity type, instead of just pre-empting whether or not an excerpt contains any entities at all, which has the additional effect of changing entitybearing/entity-free class distributions. We use two sources that share entity classification schemas: the UMBC twitter NE annotations (Finin et al., 2010), and the MSM2013 twitter annotations (Rowe et al., 2013). We also add the Ritter et al. (2011) dataset, mapping its geo-location and facility classes to location, and company, sports team and band to organisation. Mixing datasets reduces the impact of any single corpus’ sampling bias on final results. In total, this gives 3 854 twitter messages (tweets). Table 7 shows the entity distribution over this corpus. From this we separated a 500 tweet training set, used as base NER training data and pre-empting training data, and another set of 500 tweets for evalution. Note that each message can contain more than one type of entity, and that names of people are the most common class of entity"
R15-1018,W10-0713,0,0.0321873,"his Dataset Base: 500 messages 500 msgs + 1k without entities 500 msgs + 1k random 500 msgs + 1k with entities P 70.39 85.00 76.14 71.21 R 31.66 25.15 44.38 54.14 F1 43.67 38.81 56.07 61.51 Table 8: Adding entity-less vs. entity-bearing data to a 500-message base training set means that we must pre-empt according to entity type, instead of just pre-empting whether or not an excerpt contains any entities at all, which has the additional effect of changing entitybearing/entity-free class distributions. We use two sources that share entity classification schemas: the UMBC twitter NE annotations (Finin et al., 2010), and the MSM2013 twitter annotations (Rowe et al., 2013). We also add the Ritter et al. (2011) dataset, mapping its geo-location and facility classes to location, and company, sports team and band to organisation. Mixing datasets reduces the impact of any single corpus’ sampling bias on final results. In total, this gives 3 854 twitter messages (tweets). Table 7 shows the entity distribution over this corpus. From this we separated a 500 tweet training set, used as base NER training data and pre-empting training data, and another set of 500 tweets for evalution. Note that each message can con"
R15-1018,sabou-etal-2014-corpus,1,0.867686,"Missing"
R15-1018,P05-1045,0,0.0119251,"ces - 2k without entities - 2k with entities P 85.70 84.89 85.43 R 84.08 84.41 83.17 F1 84.88 84.65 84.29 ∆ F1 -0.23 -0.59 Table 2: Removing data from our training set base corpus of quality annotated data and intends to expand this corpus. 2.1 Experimental Setup For English newswire, we use the CoNLL 2003 dataset (Tjong Kim Sang and Meulder, 2003). The training part of this dataset has 14 040 sentences; of these, 11 131 contain at least one entity and so 2 909 have no entities. We evaluate against the more challenging testb part of this corpus, which contains 5 652 entity annotations. We use Finkel et al. (2005)’s statistical machine learning-based NER system. 2.2 Pre-empting Entity Presence Validation Results Results are shown in Table 1. Adding 2 000 entitybearing sentences gives the largest improvement in F1, and is better than adding 2 000 randomly chosen sentences – the case without pre-empting. Adding only entity-free text decreases overall performance, especially recall. To double check, we try removing training data instead of adding it. In this case, removing content without entities should hurt performance less than removing content with entities. From all 14k sentences of English training"
R15-1018,P04-1075,0,0.0330421,"st to researchers, who often go to great lengths to avoid it. For example, recently, Garrette and Baldridge (2013) demon128 we only outline an approach using 1 000 examples, up to 15 000 have been annotated and made publicly available for some entity types. For future work, the pre-empting feature set could be first adapted to morphologically rich languages, and then also to languages that do not necessarily compose tokens from individual letters, such as Mi’kmaq or Chinese. strated the impressive construction of a part-ofspeech tagger based on just two hours’ annotation. Similar to our work, Shen et al. (2004) proposed active learning for named entity recognition annotation, reducing annotation load without hurting NER performance, based on three metrics for each text batch and an iterative process. We differ from Shen et al. by giving a one-shot approach which does not need iterative re-training and is simple to implement in an annotation workflow, although we do not reduce annotation load as much. Our simplification means that pre-empting is easy to integrate into an annotation process, especially important for e.g. crowdsourced annotation, which is cheap and effective but gives a lot less contro"
R15-1018,N13-1014,0,0.018652,"entropy classifier implementation used allows output of the most informative features. These are reported – for newswire – in Table 12. In this case, the model was trained on 10 000 examples, and is the one for which results were given in Table 3, that achieved an F-score of 96.88. Word shape features are the strongest indicators of named entity presence, and the strongest indicators of entity absence are all character grams. 6 Related Work Avoiding needless annotation is a constant theme in NLP, and of interest to researchers, who often go to great lengths to avoid it. For example, recently, Garrette and Baldridge (2013) demon128 we only outline an approach using 1 000 examples, up to 15 000 have been annotated and made publicly available for some entity types. For future work, the pre-empting feature set could be first adapted to morphologically rich languages, and then also to languages that do not necessarily compose tokens from individual letters, such as Mi’kmaq or Chinese. strated the impressive construction of a part-ofspeech tagger based on just two hours’ annotation. Similar to our work, Shen et al. (2004) proposed active learning for named entity recognition annotation, reducing annotation load with"
R15-1018,N13-1132,0,0.0202626,"ethod: crowdsourcing. Crowdsourcing annotation works by presenting a many microtasks to non-expert workers. They typically make their judgements over short texts, after reading a short set of instructions (Sabou et al., 2014). Such judgments are often simpler than those in linguistic annotation by experts; for example, workers might be asked to annotate only a single class of entity at a time. Through crowdsourcing, quality annotations can be gathered quickly and at scale (Aker et al., 2012). There also tends to be a larger variance in reliability over crowd workers than in expert annotators (Hovy et al., 2013). For this reason, crowdsourced annotation microtasks are often all performed by at least two different workers. E.g., every sentence would be examined for each entity type by at least two different non-expert workers. We investigate entity pre-empting of crowdsourced corpora for a challenging genre: social media. Newswire corpora are not too hard to come by, especially for English, and the genre is somewhat biased in style, mostly being written or created by working-age middle-class men (Eisenstein, 2013), and in topic, being related to major events around unique entities that one might refer"
R15-1018,szarvas-etal-2006-highly,0,0.0660604,"Missing"
R15-1018,W03-0419,0,0.147535,"Missing"
R15-1018,W02-2024,0,0.213211,"8.6 78.6 90.58 92.12 93.28 96.06 94.22 96.46 SVM + Cost, j = 5 79 79 78.6 78.6 86.33 86.53 92.12 92.36 94.15 94.25 R F1 Language 82.28 80.55 92.14 92.22 Dutch Spanish Hungarian 83 97.5 94.85 96.44 97.20 84 90.1 93.94 96.35 96.88 100 100 96.25 95.36 96.18 88 88.0 91.34 94.65 95.33 100 100 97.84 98.09 98.57 88 88.0 86.43 92.24 94.20 Dutch Spanish Hungarian Training data F1 71.33 72.01 3.3 98.2 100 99.9 93.9 86.5 82.6 F1 55.22 52.27 64.90 69.82 71.75 66.65 Other Languages Pre-empting is not restricted to just English. Similar NER datasets are available for Dutch, Spanish and Hungarian (Tjong Kim Sang, 2002; Szarvas et al., 2006). Results regarding the effectiveness of an SVM pre-empter for these languages are presented in Table 5. In each case, we train with 1 000 sentences and evaluate against a 4 000-sentence evaluation partition. Strong above-baseline performance was achieved for each language. For Dutch and Spanish, this pre-empting approach performs in the same class as for English, with a low error rate. The error rate is markedly higher in Hungarian, a morphologically-rich language. This could be attributed to the use of token n-gram features; one would expect these to be sparser in a la"
R15-1018,N03-1033,0,0.00459272,"hims, 1999); we experiment with cost-weighted SVM in order to achieve high recall (Morik et al., 1999). The second is to declare sentences containing proper nouns as entitybearing. We use a random baseline that predicts NE presence based on the prior proportion of entity-bearing to entity-free sentences (≈4.8:1, entity-bearing is the dominant class, for any entity type). For the machine learning approach, we use the following feature representations: character 1,2,3grams; compressed word shape 1,2,3 grams;1 and token 1,2,3 grams. For the proper noun-based approach, we use the Stanford tagger (Toutanova et al., 2003) to label sentences. This is trained on Wall Street Journal data which does not overlap with the Reuters data in our NER corpus. As data we use a base set of sentences as training examples, which are a mixture of entitybearing and entity-free. We experiment with various sizes of base set. Evaluation is performed over a separate 4 000-sentence set, labelled as either having or not having any entities. Table 1: Adding entity-less vs. entity-bearing data to a 2 000-sentence base training set Dataset Base: All sentences - 2k without entities - 2k with entities P 85.70 84.89 85.43 R 84.08 84.41 83."
R15-1018,D11-1143,0,0.0525993,"Missing"
R15-1018,C00-2137,0,0.160373,"Missing"
R15-1018,W09-1119,0,\N,Missing
S10-1075,S07-1052,0,0.0357776,"Missing"
S10-1075,S07-1098,1,0.930026,"Missing"
S10-1075,P00-1010,0,0.572047,"oral Expresions and TLINKs for TempEval-2 Robert Gaizauskas Dept of Computer Science University of Sheffield Regent Court 211 Portobello Sheffield S1 4DP, UK robertg@dcs.shef.ac.uk Leon Derczynski Dept of Computer Science University of Sheffield Regent Court 211 Portobello Sheffield S1 4DP, UK leon@dcs.shef.ac.uk Abstract from the TempEval-2 training data annotation, augmented by features that can be directly derived from the annotated texts. There are two main aims of this work: (1) to create a rule-based temporal expression annotator that includes knowledge from work published since GUTime (Mani and Wilson, 2000) and measure its performance, and (2) to measure the performance of a classifier that includes features based on temporal signals. Our entry to the challenge, USFD2, is a successor to USFD (Hepple et al., 2007). In the rest of this paper, we will describe how USFD2 is constructed (Section 2), and then go on to discuss its overall performance and the impact of some internal parameters on specific TempEval tasks. Regarding classifiers, we found that despite using identical feature sets across relation classification tasks, performance varied significantly. We also found that USFD2 performance tr"
S10-1075,P06-1095,0,0.113693,"Missing"
S10-1075,C08-1070,0,0.0339013,"Missing"
S10-1075,S07-1046,0,0.0399903,"Missing"
S10-1075,W09-2418,0,0.0368921,"Missing"
S10-1075,P09-1046,0,0.0656148,"Missing"
S13-2001,derczynski-gaizauskas-2010-analysing,1,0.706855,"ems (silver). The TempEval-3 platinum evaluation corpus was annotated/reviewed by the organizers, who are experts in the area. This process used the TimeML Annotation Guidelines v1.2.1 (Saur´ı et al., 2006). Every file was annotated independently by at least two expert annotators, and a third was dedicated to adjudicating between annotations and merging the final result. Some annotators based their work on TIPSem annotation suggestions (Llorens et al., 2012b). The GATE Annotation Diff tool was used for merging (Cunningham et al., 2013), a custom TimeML validator ensured integrity,3 and CAVaT (Derczynski and Gaizauskas, 2010) was used to determine various modes of TimeML mis-annotation and inconsistency that are inexpressable via XML schema. Post-exercise, that corpus (TempEval-3 Platinum with around 6K tokens, on completely new text) is released for the community to review 3 See and improve.4 Inter-annotator agreement (measured with F1, as per Hripcsak and Rothschild (2005)) and the number of annotation passes per document were higher than in existing TimeML corpora, hence the name. Details are given in Table 1. Attribute value scores are given based on the agreed entity set. These are for exact matches. The Temp"
S13-2001,S10-1063,1,0.726158,"13 shows the results from event extraction. In this case, the previous state-of-the-art is not improved. Table 14 only shows the results obtained in temporal awareness by the state-of-the-art system since there were not participants on this task. We observe that TIPSemB-F approach offers competitive results, which is comparable to results obtained in TE3 English test set. 6.1 Comparison with TempEval-2 TempEval-2 Spanish test set is included as a subset of this TempEval-3 test set. We can therefore compare the performance across editions. Furthermore, we can include the full-featured TIPSem (Llorens et al., 2010), which unlike TIPSemB-F used the AnCora (Taul´e et al., 2008) corpus annotations as features including semantic roles. For timexes, as can be seen in Table 15, the original TIPSem obtains better results for timex extraction, which favours the hypothesis that machine learning systems are very well suited for this task (if the training data is sufficiently representative). However, for normalization (value F1), HeidelTime – a rule-engineered system – obtains better results. This indicates that rule-based approaches have the upper hand in this task. TIPSem uses FSS-TimEx TIPSemB-F TIPSem F1 59.0"
S13-2001,padro-stanilovsky-2012-freeling,0,0.0212522,"Missing"
S13-2001,S10-1062,1,0.680883,"ompletely new text) is released for the community to review 3 See and improve.4 Inter-annotator agreement (measured with F1, as per Hripcsak and Rothschild (2005)) and the number of annotation passes per document were higher than in existing TimeML corpora, hence the name. Details are given in Table 1. Attribute value scores are given based on the agreed entity set. These are for exact matches. The TempEval-3 silver evaluation corpus is a 600K word corpus collected from Gigaword (Parker et al., 2011). We automatically annotated this corpus by TIPSem, TIPSem-B (Llorens et al., 2013) and TRIOS (UzZaman and Allen, 2010). These systems were retrained on the corrected TimeBank and AQUAINT corpus to generate the original TimeML temporal relation set. We then merged these three state-of-the-art system outputs using our merging algorithm (Llorens et al., 2012a). In our selected merged configuration all entities and relations suggested by the best system (TIPSem) are added in the merged output. Suggestions from other systems (TRIOS and TIPSem-B) are added in the merged output, only if they are also supported by another system. The weights considered in our configuration are: TIPSem 0.36, TIPSemB 0.32, TRIOS 0.32."
S13-2001,P11-2061,1,0.562387,"the system identified both the entity and attribute (attr) together. Attribute Recall = |{∀x |x∈(Sysentity ∩Refentity )∧Sysattr (x)==Refattr (x)}| |Refentity | Attribute Precision = |{∀x |x∈(Sysentity ∩Refentity )∧Sysattr (x)==Refattr (x)}| |Sysentity | Attribute F1-score = 2∗p∗r p+r Attribute (Attr) accuracy, precision and recall can be calculated as well from the above information. Attr Accuracy = Attr F1 / Entity Extraction F1 Attr R = Attr Accuracy * Entity R Attr P = Attr Accuracy * Entity P 4.2 Temporal Relation Processing To evaluate relations, we use the evaluation metric presented by UzZaman and Allen (2011).5 This metric captures the temporal awareness of an annotation in terms of precision, recall and F1 score. Temporal awareness is defined as the performance of an annotation as identifying and categorizing temporal relations, which implies the correct recognition and classification of the temporal entities involved in the relations. Unlike TempEval2 relation score, where only categorization is evaluated for relations, this metric evaluates how well pairs of entities are identified, how well the relations are categorized, and how well the events and temporal expressions are extracted. + |Sys− ∩"
S13-2001,S10-1010,1,\N,Missing
S13-2001,taule-etal-2008-ancora,0,\N,Missing
S15-2101,cieliebak-etal-2014-meta,1,0.883569,"Missing"
S15-2101,S14-2062,1,0.891647,"Missing"
S15-2101,N13-1039,0,0.0528107,"Missing"
S15-2101,W02-1011,0,0.016848,"Missing"
S15-2101,D14-1162,0,0.0806836,"for each class - total score over all tokens for each class - + score of last token regardless of the class - + maximum score over all tokens for all classes together - + total score over all tokens For the 2-class lexicons, we flip the score of tokens occurring in the negation scope. The 3class lexicons are already trained with marked negations (Jaggi et al., 2014). • + lemma n-grams: presence of lemma n-grams (n = 1...4), by using the Standford Core NLP lemmatizer. • + cluster unigram: whether a word from each cluster in the CMU tweet clusters occurs or not • + GloVe: GloVe word embeddings (Pennington et al., 2014) are a newer version of the word2vec embedding by (Mikolov et al., 2013), using a matrix factorization instead of deep learning. We used the sum, minimum and maximum of the GloVe-vectors for the tokens occuring in the tweet. 3.3 Subsystems For the subsystems we used different linear classifier variants trained using the LibLinear package (Fan et al., 2008), all being multi-class classifiers for the three classes in a one-against-all setting. Subsystem 1. We combined all features to a single feature vector using an `1 -regularized squared loss SVM classifer and flipout regularization as describ"
S15-2101,S15-2078,0,0.124481,"Missing"
S15-2101,S14-2105,1,\N,Missing
S15-2136,N13-3004,0,0.0310777,"RE P OST E XP or S ET 4. Adjudicators revised and finalized the temporal relations More details on the corpus annotation process are documented in a separate article (Styler et al., 2014a). Because the data contained incompletely deidentified clinical data (the time expressions were retained), participants were required to sign a data use agreement with the Mayo Clinic to obtain the raw text of the clinical notes and pathology reports.1 The event, time and temporal relation annotations were distributed separately from the text, in an open source repository2 using the Anafora standoff format (Chen and Styler, 2013). • Identifying event expressions (EVENT annotations in the THYME corpus) consisting of the following components: – The spans (character offsets) of the expression in the text – Contextual Modality: ACTUAL, H YPO THETICAL, H EDGED or G ENERIC – Degree: M OST, L ITTLE or N/A – Polarity: P OS or N EG – Type: A SPECTUAL, E VIDENTIAL or N/A 1 The details of this process are described at http://thyme. healthnlp.org/ 2 https://github.com/stylerw/thymedata 807 Train Dev 293 147 38890 20974 3833 2078 11176 6173 3 Normalized time values (e.g. 2015-02-05) were originally planned, but annotation was not"
S15-2136,W11-0419,1,0.496993,"AL, E VIDENTIAL or N/A 1 The details of this process are described at http://thyme. healthnlp.org/ 2 https://github.com/stylerw/thymedata 807 Train Dev 293 147 38890 20974 3833 2078 11176 6173 3 Normalized time values (e.g. 2015-02-05) were originally planned, but annotation was not completed in time. • Identifying temporal relations between events and times, focusing on the following types: – Relations between events and the document creation time (B EFORE, OVER LAP , B EFORE -OVERLAP or A FTER ), represented by D OC T IME R EL annotations in the THYME corpus – Narrative container relations (Pustejovsky and Stubbs, 2011) between events and/or times, represented by T LINK annotations with T YPE =C ONTAINS in the THYME corpus The evaluation was run in two phases: 1. Systems were given access only to the raw text, and were asked to identify time expressions, event expressions and temporal relations 2. Systems were given access to the raw text and the manual event and time annotations, and were asked to identify only temporal relations 4 Evaluation Metrics All of the tasks were evaluated using the standard metrics of precision (P ), recall (R) and F1 : P = |S ∩ H| |S| R= |S ∩ H| |H| F1 = 2·P ·R P +R where S is th"
S15-2136,Q14-1012,1,0.497138,"Missing"
S15-2136,P11-2061,0,0.0272505,"ated by dividing the F1 on the attribute by the F1 on identifying the spans: A= attribute F1 span F1 For the narrative container relations, additional metrics were included that took into account temporal closure, where additional relations can be deterministically inferred from other relations (e.g., A C ON - TAINS B and B C ONTAINS C, so A C ONTAINS C): Pclosure = Rclosure |S ∩ closure(H)| |S| 6 |closure(S) ∩ H| = |H| Fclosure = Participating Systems Three research teams submitted a total of 13 runs: 2 · Pclosure · Rclosure Pclosure + Rclosure These measures take the approach of prior work (UzZaman and Allen, 2011) and TempEval 2013 (UzZaman et al., 2013), following the intuition that precision should measure the fraction of system-predicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of humanannotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure). 5 was predicted to be a narrative container, containing only the closest event expression to it in the text. Ba"
S15-2136,S13-2001,1,0.954536,"l of 13 system runs, with the best systems achieving near-human performance on identifying events and times, but with a large performance gap still remaining for temporal relations. 1 April 23, 2014: The patient did not have any postoperative bleeding so we will resume chemotherapy with a larger bolus on Friday even if there is slight nausea. And output annotations over the text that capture the following kinds of information: Introduction The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations. However, the TempEval campaigns to date have focused primarily on in-document timelines derived from news articles. Clinical TempEval brings these temporal information extraction tasks to the clinical domain, using clinical notes and pathology reports from the Mayo Clinic. This follows recent interest in temporal information extraction for the clinical domain, e.g., the i2b2 2012 shared task (Sun et al., 2013), and broadens our understan"
S15-2136,S07-1014,1,0.916093,"Missing"
S15-2136,S10-1010,1,\N,Missing
S15-2141,hovy-etal-2014-pos,0,0.0270377,"Missing"
S15-2141,pustejovsky-etal-2010-iso,0,0.172033,"Missing"
S15-2141,S13-2003,0,0.0548707,"Missing"
S15-2141,W15-0211,1,0.780543,"Missing"
S15-2141,S13-2001,1,0.884284,"Missing"
S15-2141,S15-2136,1,\N,Missing
S15-2141,Q14-1012,0,\N,Missing
S16-1165,S16-1195,0,0.0501484,"Missing"
S16-1165,S16-1196,0,0.0406396,"Missing"
S16-1165,S15-2136,1,0.542156,"Missing"
S16-1165,S16-1193,0,0.0677741,"Missing"
S16-1165,N13-3004,1,0.0547857,"Missing"
S16-1165,S16-1192,0,0.064286,"Missing"
S16-1165,S16-1198,0,0.0757468,"Missing"
S16-1165,S16-1190,0,0.0552623,"Missing"
S16-1165,S16-1200,0,0.0379618,"Missing"
S16-1165,S16-1201,0,0.0940692,"Missing"
S16-1165,S16-1199,0,0.0614168,"Missing"
S16-1165,P14-5010,1,0.0104397,"Missing"
S16-1165,W11-0419,1,0.613193,"event expressions (EVENT annotations in the THYME corpus) consisting of the following components: – The span (character offsets) of the expression in the text – Contextual Modality: ACTUAL, H YPO THETICAL, H EDGED or G ENERIC – Degree: M OST, L ITTLE or N/A – Polarity: P OS or N EG – Type: A SPECTUAL, E VIDENTIAL or N/A • Identifying temporal relations between events and times, focusing on the following types: – Relations between events and the document creation time (B EFORE, OVER LAP , B EFORE -OVERLAP or A FTER ), represented by D OC T IME R EL annotations. – Narrative container relations (Pustejovsky and Stubbs, 2011), which indicate that an event or time is temporally contained in (i.e., occurred during) another event or time, represented by T LINK annotations with T YPE =C ONTAINS. The evaluation was run in two phases: 1. Systems were provided access only to the raw text, and were asked to identify time expressions, event expressions and temporal relations 2. Systems were provided access to the raw text and the manual event and time annotations, and were asked to identify only temporal relations 4 Evaluation Metrics All of the tasks were evaluated using the standard metrics of precision (P ), recall (R)"
S16-1165,Q14-1012,1,0.456145,"Missing"
S16-1165,P11-2061,0,0.0570732,"w a comparison across systems for assigning attribute values, even when different systems produce different numbers of events and times. This metric is calculated by dividing the F1 on the attribute by the F1 on identifying the spans: A= attribute F1 span F1 For narrative container relations, the P and R definitions were modified to take into account temporal closure, where additional relations are deterministically inferred from other relations (e.g., A C ONTAINS B and B C ONTAINS C, so A C ONTAINS C): P = |S ∩ closure(H)| |S| R= |closure(S) ∩ H| |H| Similar measures were used in prior work (UzZaman and Allen, 2011) and TempEval 2013 (UzZaman et al., 2013), following the intuition that precision should measure the fraction of system-predicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of human-annotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure). 5 Baseline Systems Two rule-based systems were used as baselines to compare the participating systems against."
S16-1165,S13-2001,1,0.208832,"on a corpus of clinical and pathology notes from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain. 14 teams submitted a total of 40 system runs, with the best systems achieving near-human performance on identifying events and times. On identifying temporal relations, there was a gap between the best systems and human performance, but the gap was less than half the gap of Clinical TempEval 2015. 1 Introduction The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations. However, the TempEval campaigns to date have focused primarily on in-document timelines derived from news articles. In recent years, the community has moved toward testing such information extraction systems on clinical data (Sun et al., 2013; Bethard et al., 2015) to broaden our understanding of the language of time beyond newswire expressions and structure. Clinical TempEval focuses on discrete, welldefined tasks which allow rapid, rel"
S16-1165,S07-1014,1,0.223174,"Missing"
S17-2006,S17-2082,0,0.162573,"Missing"
S17-2006,S17-2084,0,0.0230442,"Missing"
S17-2006,S17-2083,1,0.818965,"hin the conversations were performed through crowdsourcing – as performed to satisfactory quality already with the existing training data (Zubiaga et al., 2015a). 3 Score 0.635 0.778 0.641 0.701 0.749 0.709 0.784 0.780 0.741 0.391 Table 2: Results for Task A: port/deny/query/comment classification. supTask A, we also introduce a baseline excluding the common, low-impact “comment” class, considering accuracy over only support, deny and query. This is included as the SDQ baseline. 4 Participant Systems and Results We have had 13 system submissions at RumourEval, eight submissions for Subtask A (Kochkina et al., 2017; Bahuleyan and Vechtomova, 2017; Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Garc´ıa Lozano et al., 2017; Enayet and ElBeltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Enayet and El-Beltagy, 2017), the rumour veracity classification task, with participant teams coming from four continents (Europe: Germany, Sweden, UK; North America: Canada; Asia: China, India, Taiwan; Africa: Egypt), showing the global reach of the issue of"
S17-2006,W11-1701,0,0.0223707,"ce a subtask where the goal is to label the type of interaction between a given statement (rumourous tweet) and a reply tweet (the latter can be either direct or nested replies). Secondly, participants need to determine the type of response towards a rumourous tweet from a tree-structured conversation, where each tweet is not necessarily sufficiently descriptive on its own, but needs to be viewed in the context of an aggregate discussion consisting of tweets preceding it in the thread. This is more closely aligned with stance classification as defined in other domains, such as public debates (Anand et al., 2011). The latter also relates somewhat to the SemEval-2015 Task 3 on Answer Selection in Community Question Answering (Moschitti et al., 2015), where the task was to determine the quality of responses in tree-structured threads in CQA platforms. Responses to questions are classified as ‘good’, ‘potential’ or ‘bad’. Both tasks are related to textual entailment and textual similarity. However, Semeval-2015 Task3 is clearly a question answering task, the platform itself supporting a QA format in contrast with the more free-form format of conversations in Twitter. Moreover, as a question answering tas"
S17-2006,D15-1311,1,0.840265,"gorised into one of the following four categories, following Procter et al. (2013b): • Support: the author of the response supports the veracity of the rumour. • Deny: the author of the response denies the veracity of the rumour. • Query: the author of the response asks for additional evidence in relation to the veracity of the rumour. • Comment: the author of the response makes their own comment without a clear contribution to assessing the veracity of the rumour. Prior work in the area has found the task difficult, compounded by the variety present in language use between different stories (Lukasik et al., 2015; Zubiaga et al., 2017). This indicates it is challenging enough to make for an interesting SemEval shared task. 1.2 1.3 Impact Identifying the veracity of claims made on the web is an increasingly important task (Zubiaga et al., 2015b). Decision support, digital journalism and disaster response already rely on picking out such claims (Procter et al., 2013b). Additionally, web and social media are a more challenging environment than e.g. newswire, which has traditionally provided the mainstay of similar tasks (such as RTE (Bentivogli et al., 2011)). Last year we ran a workshop at WWW 2015, Rum"
S17-2006,S16-1003,0,0.0771763,"Missing"
S17-2006,S17-2080,0,0.0322263,"Missing"
S17-2006,D11-1147,0,0.479793,"y participants on these challenges. 1 Introduction and Motivation Rumours are rife on the web. False claims affect people’s perceptions of events and their behaviour, sometimes in harmful ways. With the increasing reliance on the Web – social media, in particular – as a source of information and news updates by individuals, news professionals, and automated systems, the potential disruptive impact of rumours is further accentuated. The task of analysing and determining veracity of social media content has been of recent interest to the field of natural language processing. After initial work (Qazvinian et al., 2011), increasingly advanced systems and annotation schemas have been developed to support the analysis of rumour and misinformation in text (Kumar and Geethakumari, 2014; Zhang et al., 2015; Shao et al., 2016; Zubiaga et al., 2016b). Veracity judgment can be decomposed intuitively in terms of a comparison between assertions made in – and entailments from – a candidate text, and external world knowledge. Intermediate linguistic cues have also been 1.1 Subtask A - SDQC Support/ Rumour stance classification Related to the objective of predicting a rumour’s veracity, Subtask A deals with the complemen"
S17-2006,S17-2087,0,0.0127388,"lready with the existing training data (Zubiaga et al., 2015a). 3 Score 0.635 0.778 0.641 0.701 0.749 0.709 0.784 0.780 0.741 0.391 Table 2: Results for Task A: port/deny/query/comment classification. supTask A, we also introduce a baseline excluding the common, low-impact “comment” class, considering accuracy over only support, deny and query. This is included as the SDQ baseline. 4 Participant Systems and Results We have had 13 system submissions at RumourEval, eight submissions for Subtask A (Kochkina et al., 2017; Bahuleyan and Vechtomova, 2017; Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Garc´ıa Lozano et al., 2017; Enayet and ElBeltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Enayet and El-Beltagy, 2017), the rumour veracity classification task, with participant teams coming from four continents (Europe: Germany, Sweden, UK; North America: Canada; Asia: China, India, Taiwan; Africa: Egypt), showing the global reach of the issue of rumour veracity on social media. Most participants tackled Subtask A, which involves classifying"
S17-2006,S17-2085,0,0.0202189,"ing – as performed to satisfactory quality already with the existing training data (Zubiaga et al., 2015a). 3 Score 0.635 0.778 0.641 0.701 0.749 0.709 0.784 0.780 0.741 0.391 Table 2: Results for Task A: port/deny/query/comment classification. supTask A, we also introduce a baseline excluding the common, low-impact “comment” class, considering accuracy over only support, deny and query. This is included as the SDQ baseline. 4 Participant Systems and Results We have had 13 system submissions at RumourEval, eight submissions for Subtask A (Kochkina et al., 2017; Bahuleyan and Vechtomova, 2017; Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Garc´ıa Lozano et al., 2017; Enayet and ElBeltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Enayet and El-Beltagy, 2017), the rumour veracity classification task, with participant teams coming from four continents (Europe: Germany, Sweden, UK; North America: Canada; Asia: China, India, Taiwan; Africa: Egypt), showing the global reach of the issue of rumour veracity on social media. Most participants tackle"
S17-2006,S17-2086,0,0.0386461,"Missing"
S18-1179,P17-2057,0,0.0119826,"ommonsense knowledge into a question-answering task (Ostermann et al., 2018b). In this case, questions are either directly or indirectly related to given English texts; some questions may be answered using the text while others require background (commonsense) knowledge. The challenge is to use this knowledge in such a way as to enhance the quality of chosen answers. There are many approaches to question answering including using structured knowledge (Yao and Durme, 2014), knowledge databases (Yih et al., 2015), deep learning methods (Minaee and Liu, 2017) and hybrid methods (Xu et al., 2016; Das et al., 2017). The present task accepts any method or any source of background knowledge. The training data consists of 1469 texts covering more than 100 topics. The number of questions per text varies from 1 to 14 and there are two answer options. The development data has 219 texts and the test data has 430. (Ostermann et al., 2018a) The main idea behind the method proposed in this paper is to use the given texts as potential sources of knowledge. Texts from training and development data can be divided into topics using existing clustering algorithms (e.g. k-Means, Hierarchical, Grid-based or Density-base"
S18-1179,L18-1564,0,0.0653533,"1, on machine comprehension using commonsense knowledge. First, clustering and topic modeling are used to divide given texts into topics. Then, during the answering phase, other texts of the same topic are retrieved and used as commonsense knowledge. Finally, the answer is selected. While clustering itself shows good results, finding an answer proves to be more challenging. This paper reports the results of system evaluation and suggests potential improvements. 1 Introduction The goal of SemEval-2018 Task 11 is to find a way to incorporate commonsense knowledge into a question-answering task (Ostermann et al., 2018b). In this case, questions are either directly or indirectly related to given English texts; some questions may be answered using the text while others require background (commonsense) knowledge. The challenge is to use this knowledge in such a way as to enhance the quality of chosen answers. There are many approaches to question answering including using structured knowledge (Yao and Durme, 2014), knowledge databases (Yih et al., 2015), deep learning methods (Minaee and Liu, 2017) and hybrid methods (Xu et al., 2016; Das et al., 2017). The present task accepts any method or any source of bac"
S18-1179,C16-1226,0,0.0198923,"to incorporate commonsense knowledge into a question-answering task (Ostermann et al., 2018b). In this case, questions are either directly or indirectly related to given English texts; some questions may be answered using the text while others require background (commonsense) knowledge. The challenge is to use this knowledge in such a way as to enhance the quality of chosen answers. There are many approaches to question answering including using structured knowledge (Yao and Durme, 2014), knowledge databases (Yih et al., 2015), deep learning methods (Minaee and Liu, 2017) and hybrid methods (Xu et al., 2016; Das et al., 2017). The present task accepts any method or any source of background knowledge. The training data consists of 1469 texts covering more than 100 topics. The number of questions per text varies from 1 to 14 and there are two answer options. The development data has 219 texts and the test data has 430. (Ostermann et al., 2018a) The main idea behind the method proposed in this paper is to use the given texts as potential sources of knowledge. Texts from training and development data can be divided into topics using existing clustering algorithms (e.g. k-Means, Hierarchical, Grid-ba"
S18-1179,P14-1090,0,0.0311043,"Missing"
S18-1179,P15-1128,0,0.0120528,"ntial improvements. 1 Introduction The goal of SemEval-2018 Task 11 is to find a way to incorporate commonsense knowledge into a question-answering task (Ostermann et al., 2018b). In this case, questions are either directly or indirectly related to given English texts; some questions may be answered using the text while others require background (commonsense) knowledge. The challenge is to use this knowledge in such a way as to enhance the quality of chosen answers. There are many approaches to question answering including using structured knowledge (Yao and Durme, 2014), knowledge databases (Yih et al., 2015), deep learning methods (Minaee and Liu, 2017) and hybrid methods (Xu et al., 2016; Das et al., 2017). The present task accepts any method or any source of background knowledge. The training data consists of 1469 texts covering more than 100 topics. The number of questions per text varies from 1 to 14 and there are two answer options. The development data has 219 texts and the test data has 430. (Ostermann et al., 2018a) The main idea behind the method proposed in this paper is to use the given texts as potential sources of knowledge. Texts from training and development data can be divided int"
S19-2147,N16-1138,0,0.0499239,"n propagation. Initial work on rumour detection and stance classification (Qazvinian et al., 2011) was succeeded by more elaborate systems and annotation schemas (Kumar and Geethakumari, 2014; Zhang et al., 2015; Shao et al., 2016; Zubiaga et al., 2016). Vosoughi (2015) demonstrated the value of making use of propagation information, i.e. the ensuing discussion, in rumour verification. Stance detection is the task of classifying a text according to the position it takes with respect to a statement. Research supports the importance of this subtask as a first step to 6 veracity identification. (Ferreira and Vlachos, 2016; Enayet and El-Beltagy, 2017). Crowd response, stance and the details of rumour propagation feature in the work by Chen et al. (2016) as well as the most successful system in RumourEval 2017 (Enayet and El-Beltagy, 2017), and the highest performing systems in RumourEval 2019. 1.2 Datasets for rumour verification The UK fact-checking charity Full Fact provides a roadmap7 for development of automated fact checking. They cite open and shared evaluation as one of their five principles for international collaboration, demonstrating the continuing relevance of shared tasks in this area. Shared data"
S19-2147,S17-2083,1,0.672765,"4 (15) 0.4895 (4) 0.1272 (19) 0.3267 (16) 0.3537 (14) 0.3875 (10) 0.4384 (6) 0.3927 (9) 0.6067 (2) 0.4792 (5) 0.3699 (12) 0.4298 (8) 0.3326 0.6846 0.2165 0.1845 0.7857 0.2530 0.3364 0.7806 0.4929 0.3089 0.7698 - 0.2241 0.7115 0.2234 uses the same features as the stance classification system but produces a single output per branch. The veracity prediction for the thread is then decided using majority voting over per-branch outcomes. Stance classification baseline For subtask A we released a Keras (Chollet et al., 2015) implementation of branchLSTM, the winning system of RumourEval 2017 Task A (Kochkina et al., 2017). This system uses the conversation structure by splitting it into linear branches. It is a neural network architecture that uses LSTM layer(s) to process sequences of tweets, outputting a stance label at each time step. Each tweet is represented by the average of its word vectors 11 concatenated with a number of extra features. This baseline was outperformed by 3 submitted systems (BLCU NLP, BUT-FIT, eventAI). 4.2 Subtask B, RMSE 0.6078 (1) 0.7642 (2) 0.8012 (3) 0.8179 (5) 0.8081 (4) 0.8623 (7) 0.8623 (7) 0.8623 (7) 0.8678 (8) 0.8264 (6) 0.9148 (9) - Table 5: Results table. Ranking is in brac"
S19-2147,C18-1288,1,0.869514,"Missing"
S19-2147,P18-1184,0,0.0196785,"cite open and shared evaluation as one of their five principles for international collaboration, demonstrating the continuing relevance of shared tasks in this area. Shared datasets are a crucial part of the joint endeavour. Datasets for rumour resolution are still relatively few, and likely to be in increasing demand. In addition to the data from RumourEval 2017, the dataset released by Kwon et al. (2017) is also suitable for veracity classification. It includes 51 true rumours and 60 false rumours, where each rumour includes a stream of tweets associated with it. Twitter 15 and 16 datasets (Ma et al., 2018) contain claim propagation trees and combine tasks of rumour detection and verification in one four-way classification task (Non-rumour, True, False, Unverified). A Sina Weibo corpus is also available (Wu et al., 2015), in which 5000 posts are classified for veracity, but responses are not available. Partially generated statistical claim checking data is now becoming available in the context of the FEVER shared task, mentioned above, but is not suitable for this type of work. Twitter continues to be a highly relevant platform for rumour verification, being popular with the public as well as po"
S19-2147,N18-1202,0,0.0133727,"t al., 2018) 10 https://github.com/kochkinaelena/ RumourEval2019 11 We are using word2vec (Mikolov et al., 2013) model pretrained on the GoogleNews dataset (300d) 851 training of bidirectional representations to provide additional context. They experiment with different parameter settings and if the model increased overall performance it was added to the classifier. Interestingly the best performing system in task A (BLCU-NLP) and the third best (CLEARumor) also use pre-trained contextual embedding representations with BLCU-NLP using OpenAI GPT (Radford et al., 2018) and ClEARumor using ELMo (Peters et al., 2018). While most systems use single tweets or pairs of tweets (sourceresponse) as their underlying structure to operate on, BLCU-NLP employ an inference chain-based system for this paper. Thus they consider the conversation thread starting with a source tweet, followed by replies, in which each one responds to an earlier one in time sequence. They take each conversation thread as an inference chain and concentrate on utilizing it to solve the problem of class imbalance in subtask A and training data scarcity in subtask B. They also have augmented the training data with external public datasets. Ot"
S19-2147,D11-1147,0,0.461343,"ient skepticism and/or anxiety so as to motivate finding out the actual truth” (Zubiaga et al., 2016). One can distinguish several component to a rumour resolution pipeline such as rumour detection, rumour tracking and stance classification, leading to the final outcome of determining the veracity of a rumour (Zubiaga et al., 2018). Thus what characterises rumour verification compared to other types of fact checking is time sensitivity and the importance of dynamic interactions between users, their stance and information propagation. Initial work on rumour detection and stance classification (Qazvinian et al., 2011) was succeeded by more elaborate systems and annotation schemas (Kumar and Geethakumari, 2014; Zhang et al., 2015; Shao et al., 2016; Zubiaga et al., 2016). Vosoughi (2015) demonstrated the value of making use of propagation information, i.e. the ensuing discussion, in rumour verification. Stance detection is the task of classifying a text according to the position it takes with respect to a statement. Research supports the importance of this subtask as a first step to 6 veracity identification. (Ferreira and Vlachos, 2016; Enayet and El-Beltagy, 2017). Crowd response, stance and the details o"
S19-2147,D15-1312,0,0.023098,"Hostage-taker in supermarket siege killed, reports say. #ParisAttacks LINK [true] Veracity prediction. Example 2: u1: OMG. #Prince rumoured to be performing in Toronto today. Exciting! [false] Table 1: Examples of source tweets with veracity value of local features. Fact checking is a broad complex task, challenging the resourcefulness of even a human expert. Claims such as ”we send the EU 350 million a week” which is partially true would need to be decomposed into statements to be checked against knowledge bases and multiple sources. Ways of automating fact checking has inspired researchers (Vlachos and Riedel, 2015) and has resulted in a new shared task FEVER.6 Other research has focused on stylistic tells of untrustworthiness in the source itself (Conroy et al., 2015; Singhania et al., 2017). Rumour verification is a particular case of fact checking. Rumours are “circulating stories of questionable veracity, which are apparently credible but hard to verify, and produce sufficient skepticism and/or anxiety so as to motivate finding out the actual truth” (Zubiaga et al., 2016). One can distinguish several component to a rumour resolution pipeline such as rumour detection, rumour tracking and stance classi"
sabou-etal-2014-corpus,W10-0705,0,\N,Missing
sabou-etal-2014-corpus,W10-1807,0,\N,Missing
sabou-etal-2014-corpus,W10-1839,0,\N,Missing
sabou-etal-2014-corpus,W10-0712,0,\N,Missing
sabou-etal-2014-corpus,W10-0717,0,\N,Missing
sabou-etal-2014-corpus,scharl-etal-2012-leveraging,1,\N,Missing
sabou-etal-2014-corpus,W10-0702,0,\N,Missing
sabou-etal-2014-corpus,rosenthal-etal-2010-towards,0,\N,Missing
sabou-etal-2014-corpus,W10-0719,0,\N,Missing
sabou-etal-2014-corpus,D08-1027,0,\N,Missing
sabou-etal-2014-corpus,W10-0718,0,\N,Missing
sabou-etal-2014-corpus,W02-0817,0,\N,Missing
sabou-etal-2014-corpus,N06-2015,0,\N,Missing
sabou-etal-2014-corpus,D11-1062,0,\N,Missing
sabou-etal-2014-corpus,D11-1065,0,\N,Missing
sabou-etal-2014-corpus,J11-2010,0,\N,Missing
sabou-etal-2014-corpus,P11-1122,0,\N,Missing
sabou-etal-2014-corpus,W10-0728,0,\N,Missing
sabou-etal-2014-corpus,W11-1510,0,\N,Missing
sabou-etal-2014-corpus,W10-0734,0,\N,Missing
sabou-etal-2014-corpus,E14-2025,1,\N,Missing
sabou-etal-2014-corpus,N13-1132,0,\N,Missing
sabou-etal-2014-corpus,W10-0703,0,\N,Missing
sabou-etal-2014-corpus,P10-5004,0,\N,Missing
sabou-etal-2014-corpus,aker-etal-2012-assessing,0,\N,Missing
sabou-etal-2014-corpus,D11-1143,0,\N,Missing
sabou-etal-2014-corpus,W11-0404,0,\N,Missing
sabou-etal-2014-corpus,W10-0713,0,\N,Missing
sabou-etal-2014-corpus,W10-0723,0,\N,Missing
sabou-etal-2014-corpus,abekawa-etal-2010-community,0,\N,Missing
W08-1805,H05-1020,0,0.0239307,"pared to those which had proven to be helpful as extension words. Relevance Feedback Relevance feedback is a widely explored technique for query expansion. It is often done using a specific measure to select terms using a limited set of ranked documents of size r; using a larger set will bring term distribution closer to values over the whole corpus, and away from ones in documents relevant to query terms. Techniques are used to identify phrases relevant to a query topic, in order to reduce noise (such as terms with a low corpus frequency that relate to only a single article) and query drift (Roussinov and Fan, 2005; Allan, 1996). In the context of QA, Pizzato (2006) employs blind RF using the AQUAINT corpus in an attempt to improve performance when answering factoid questions on personal names. This is a similar approach to some content in this paper, though limited to the study of named entities, and does not attempt to examine extensions from the existing answer data. Monz (2003) finds a negative result when applying blind feedback for QA in TREC 9, 10 and 11, and a neutral result for TREC 7 and 8’s ad hoc retrieval tasks. Monz’s experiment, using r = 10 and standard Rocchio term weighting, also found"
W08-1805,U06-1013,0,\N,Missing
W13-0107,E12-1027,0,0.0502818,"Missing"
W13-0107,W10-4205,0,0.0221874,"s framework not only offers a means of formally describing the tenses of verbs, but also rules for temporally arranging the events related by these verbs, using the its three abstract points. This can, for a subset of cases, form a basis for describing the temporal ordering of these events. The framework is currently used in approaches to many computational linguistics problems. These include language generation, summarisation, and the interpretation of temporal expressions. When automatically creating text, it is necessary to make decisions on when to shift tense to properly describe events. Elson and McKeown (2010) relate events based on a “perspective” which is calculated from the reference and event times of two verbs that each describe events. They construct a natural language generation system that uses reference times in order to correctly write stories. Further, reference point management is critical to medical summary generation. In order to helpfully unravel the meanings of tense shifts in minute-by-minute patient reports, Portet et al. (2009) required understanding of the reference point. The framework also helps interpret linguistic expressions of time (timexes). Reference time is required to"
W13-0107,S07-1098,1,0.875554,"r accurate reference time management is so persistent that state of the art systems for converting times expressed in natural language to machine-readable format now contain extra layers solely for handling reference time (Llorens et al., 2012). Given the difficulty of automatically determining the orderings, or temporal relations, between events, and the suggested ability of Reichenbach’s framework to provide information for this, it is natural to apply this framework to the temporal ordering task. Although tense has played a moderately useful part in machine learning approaches to the task (Hepple et al., 2007), its exact role in automatic temporal annotation is not fully understood. Further, though it was not the case when the framework was originally proposed, there now exist resources annotated with some temporal semantics, using TimeML (Pustejovsky et al., 2005). Comparing the explicit temporal annotations within these resources with the modes of interaction proposed by Reichenbach’s framework permits an evaluation of the validity of this established account of tense and aspect. This paper addresses the following questions: 1. How can Reichenbach’s framework be related to a modern temporal annot"
W13-0107,llorens-etal-2012-timen,1,0.831033,"eference time is required to interpret anaphoric expressions such as “last April”. Creation of recent timex corpora prompted the comment that there is a “need to develop sophisticated methods for temporal focus tracking if we are to extend current time-stamping technologies” (Mazur and Dale, 2010) – focus as a rˆole filled by Reichenbach’s reference point. In fact, demand for accurate reference time management is so persistent that state of the art systems for converting times expressed in natural language to machine-readable format now contain extra layers solely for handling reference time (Llorens et al., 2012). Given the difficulty of automatically determining the orderings, or temporal relations, between events, and the suggested ability of Reichenbach’s framework to provide information for this, it is natural to apply this framework to the temporal ordering task. Although tense has played a moderately useful part in machine learning approaches to the task (Hepple et al., 2007), its exact role in automatic temporal annotation is not fully understood. Further, though it was not the case when the framework was originally proposed, there now exist resources annotated with some temporal semantics, usi"
W13-0107,D10-1089,0,0.0285059,". Further, reference point management is critical to medical summary generation. In order to helpfully unravel the meanings of tense shifts in minute-by-minute patient reports, Portet et al. (2009) required understanding of the reference point. The framework also helps interpret linguistic expressions of time (timexes). Reference time is required to interpret anaphoric expressions such as “last April”. Creation of recent timex corpora prompted the comment that there is a “need to develop sophisticated methods for temporal focus tracking if we are to extend current time-stamping technologies” (Mazur and Dale, 2010) – focus as a rˆole filled by Reichenbach’s reference point. In fact, demand for accurate reference time management is so persistent that state of the art systems for converting times expressed in natural language to machine-readable format now contain extra layers solely for handling reference time (Llorens et al., 2012). Given the difficulty of automatically determining the orderings, or temporal relations, between events, and the suggested ability of Reichenbach’s framework to provide information for this, it is natural to apply this framework to the temporal ordering task. Although tense h"
W13-0107,P87-1021,0,0.656234,", S2 , ..., Sn to be interpreted as a narrative discourse, the reference time of each sentence Si (for i such that 1 &lt; i − n) is interpreted to be: (a) a time consistent with the definite time adverbials in Si , if there are any; (b) otherwise, a time which immediately follows the reference time of the previous sentence Si−1 . The TDIP accounts for a set of sentences which share a reference and speech point. However, as with other definitions of temporal context, this principle involves components that are difficult to automatically determine (e.g. “consistent with definite time adverbials”). Webber (1987) introduces a listener model, incorporating R as a means of determining temporal focus. Her focus resumption and embedded discourse heuristics capture the nesting behaviour of temporal contexts. Further, Eijck and Kamp (2010) describe context-bounding, tense-based rules for applicability of Reichenbach’s framework. These comprise a qualitative model of temporal context. As described in Chapter 4 of Hornstein (1990), permanence of the reference point does not apply between main verb events and those in embedded phrases, relative clauses or quoted speech. These latter events occur within a separ"
W15-0211,S15-2136,1,0.844922,"n standards and corpora is to use human annotations to test an annotation model (Pustejovsky and Moszkowicz, 2012). This paper provides an analysis of temporal expression annotation in one such corpus, in an effort to gather information on the underlying model and to improve future annotation efforts. Our analysis is based on the corpus and standard that backed a recent shared annotation exercise in SemEval (Semantic Evaluation) 2015.1 SemEval is a series of evaluations that aims to verify the effectivenesses of existing approaches to semantic analysis. SemEval-2015 Task 6, Clinical TempEval (Bethard et al., 2015), was a temporal information extraction task over the clinical domain, using clinical notes and pathology reports, focused on identification of spans and features for time expressions (TIMEX), and based on specific annotation guidelines. Clinical TempEval temporal expression 1 http://alt.qcri.org/semeval2015/ results2 were given in terms of Precision, Recall and F1-score for identifying spans and classes of temporal expressions. The identification of expressions should be based on a set of provided guidelines. The clarity of guidelines, skill of annotators and quality of annotated resource can"
W15-0211,N06-2015,0,0.0313874,"for time expressions (TIMEX), and based on specific annotation guidelines. Clinical TempEval temporal expression 1 http://alt.qcri.org/semeval2015/ results2 were given in terms of Precision, Recall and F1-score for identifying spans and classes of temporal expressions. The identification of expressions should be based on a set of provided guidelines. The clarity of guidelines, skill of annotators and quality of annotated resource can be estimated by measuring agreement between annotators. It is recommended that the target inter-annotator agreement for linguistic resources be at or above 0.90 (Hovy et al., 2006). Clinical TempEval’s timex annotations had an IAA of 0.80 (or 0.79) (Styler et al., 2014), suggesting that these can be improved. To investigate the quality of the dataset and annotation standard in Clinical TempEval, we have used a rule-based system using JAPE (Cunningham et al., 2011) based as closely as possible on the annotation guidelines, and referring to the corpus for guidance in edge cases. When evaluated using the Clinical TempEval scoring software, this system obtained good Recall (0.795 for timex spans and 0.756 for timex classes) but low precision ranging from 0.29 to 0.49. These"
W15-0211,W09-2422,0,0.020492,"006; Pustejovsky and Stubbs, 2012). Within the previous SemEval evaluation, TempEval-3’s Task “A” (UzZaman et al., 2013) examined temporal information extraction and normalisation using the complete set of TimeML temporal relations. Most of the participant systems achieved over 0.70 in Precision and Recall, and best approaches achieved 0.82 and 0.77 for strict F1-score on identifying span and value of timexes. TempEval and TempEval2 (Verhagen et al., 2009, 2010) also included temporal annotation tasks, of which both were followed by informative analyses of the corpora and participant results (Lee and Katz, 2009; Derczynski, 2013), which led to a better understanding of the task as framed in these exercises. Other researchers have annotated temporal information in clinical text. For example, the CLEF Project (Roberts et al., 2009) semantically annotated a corpus to assist in the extraction of clinical information from text. It used two different schemas to annotate a) clinical entities and relations between them, and b) time expressions and their temporal relations with the clinical entities in the text. The i2b2 Natural Language Processing Challenge for Clinical Records focused on the temporal relat"
W15-0211,pustejovsky-etal-2010-iso,0,0.123783,"gy reports for cancer patients provided by Mayo Clinic.3 Clinical TempEval focuses on identification of: spans and features for timexes, event expressions, and narrative container relations. For time expressions, participants identified expression spans within the text and their corresponding classes: DATE, TIME, DURATION, QUANTIFIER, PREPOSTEXP or SET.4 Participating systems had to annotate timexes according to the guidelines for the annotation of times, events and temporal relations in clinical notes – THYME Annotation Guidelines (Styler et al., 2014) –, which is an extension of ISO TimeML (Pustejovsky et al., 2010) developed by the THYME project.5 Further, ISO TimeML extends two other guidelines: a) TimeML Annotation Guidelines (Sauri et al., 2006), and b) TIDES 2005 Standard for the Annotation of Temporal Expressions (Ferro et al., 2005). For Clinical TempEval two datasets were provided. The first was a training dataset comprising 293 documents with a total number 3818 annotated time expressions. The second dataset comprised 150 documents with a total of 2078 annotations. This was used for evaluation and was then made available to participants, after evaluations were completed. Each annotation identifi"
W15-0211,pustejovsky-moszkowicz-2012-role,0,0.0188162,"standards include: a) how to formally represent the elements that describe temporal concepts; and b) what procedures should be performed by an algorithm, in order to deal with the set of temporal reasoning operations that humans seem to perform relatively easily (Caselli, 2009). The sub-problem of automatic recognition of temporal expressions within natural language text is a particularly challenging and active area in computational linguistics (Pustejovsky et al., 2003). One way of iteratively improving annotation standards and corpora is to use human annotations to test an annotation model (Pustejovsky and Moszkowicz, 2012). This paper provides an analysis of temporal expression annotation in one such corpus, in an effort to gather information on the underlying model and to improve future annotation efforts. Our analysis is based on the corpus and standard that backed a recent shared annotation exercise in SemEval (Semantic Evaluation) 2015.1 SemEval is a series of evaluations that aims to verify the effectivenesses of existing approaches to semantic analysis. SemEval-2015 Task 6, Clinical TempEval (Bethard et al., 2015), was a temporal information extraction task over the clinical domain, using clinical notes a"
W15-0211,Q14-1012,0,0.0831817,"Missing"
W15-0211,S13-2001,1,0.879892,"d the annotation standard aided the evolution of both. For example, Boguraev and Ando (2007) presented an extensive analysis of the TimeBank reference corpus in terms of development support of TimeML-compliant analytics, which helped advance the state of the art in temporal annotation. Indeed, iterative application of an annotation standard and examination of the resulting annotated data are critical steps in the MATTER development cycle, used for construction annotation standards (Pustejovsky, 2006; Pustejovsky and Stubbs, 2012). Within the previous SemEval evaluation, TempEval-3’s Task “A” (UzZaman et al., 2013) examined temporal information extraction and normalisation using the complete set of TimeML temporal relations. Most of the participant systems achieved over 0.70 in Precision and Recall, and best approaches achieved 0.82 and 0.77 for strict F1-score on identifying span and value of timexes. TempEval and TempEval2 (Verhagen et al., 2009, 2010) also included temporal annotation tasks, of which both were followed by informative analyses of the corpora and participant results (Lee and Katz, 2009; Derczynski, 2013), which led to a better understanding of the task as framed in these exercises. Oth"
W15-0211,S10-1010,0,\N,Missing
W15-4306,J92-4003,0,0.384216,"Missing"
W15-4306,R13-1015,1,0.89064,"Missing"
W15-4306,N15-1075,0,0.11961,"CRF L-BFGS provided the best performance on our dataset for the ten-types task. 3.5 Training Data In our final system, we included the dev 2015 data, to combat drift present in the corpus. We anticipated that the test set would be from 2015. The original dataset was harvested in 2010, long enough ago to be demonstrably disadvantaged when compared with modern data (Fromreide et al., 2014), and so it was critical to include something more. The compensate for the size imbalance – the dev 2015 data is 0.175 the size of the 2010 data – we weighted down the older dataset to by 0.7, as suggested by (Cherry and Guo, 2015), implemented by uniformly scaling individual feature values on older instances. This successfully reduced the negative impact of the inevitable drift. 4 5 5.1 Analysis Features In terms of features, we looked at the strongestweighted observations in the notypes model, to see what the general indicators are of named entities in tweets. The largest of these are shown in Table 3. Of note is that features indicating URLs, hashtags and usernames indicate against an entity; lowercase words including punctuation, or comprising only punctuation, are not entities; being proceeded by at indicates being"
W15-4306,P02-1022,1,0.731761,"ained on more consistent, longer documents, such as newswire, mostly impotent (Derczynski et al., 2015b). Suffering from a sustained dearth of annotated Twitter datasets, it may be useful to understand what makes this genre tick, and how our existing techniques and resources can be generalised better to fit such a challenging text source. This paper has focused on introducing our Named Entity Recognition (NER) entry to the WNUT evaluation challenge (Baldwin et al., 2015), which builds on our earlier experiments with Twitter and news NER (Derczynski and Bontcheva, 2014; Bontcheva et al., 2013; Cunningham et al., 2002). In particular, we push data sources and representations, using what is know about Twitter so far to construct a model that informs our choices. Specifically, we attempt to compensate for entity drift; to harness unsupervised word clustering in a principled fashion; to bring in large-scale gazetteers; to attenuate the impact of terms frequent in this text type; and to pick and choose targeted gazetteers for specific entity types. 3 Method The WNUT Twitter NER task required us to address many data sparsity challenges. Firstly, the datasets involved are simply very small, making it hard to gene"
W15-4306,N13-1037,0,0.0143652,"ge-scale gazetteers; to attenuate the impact of terms frequent in this text type; and to pick and choose targeted gazetteers for specific entity types. 3 Method The WNUT Twitter NER task required us to address many data sparsity challenges. Firstly, the datasets involved are simply very small, making it hard to generalise in supervised learning, and meaning that effect sizes cannot be reliably measured. Secondly, Twitter language is arguably one of the noisiest and idiosyncratic text genres, which manifests as a large number of word types, and very large vocabularies due to lexical variation (Eisenstein, 2013). Thirdly, the language and especially entities found in tweets change over time, which is commonly referred to as drift. The majority of the WNUT training data is from 2010, and only a small amount from 2015, leading to a sparsity in examples of modern language. Therefore, in our machine learning approach, many of the features we introduce are there to combat sparsity. 48 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 48–53, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics 3.1 Unsupervised Clustering NE type company We use an unsupervis"
W15-4306,fromreide-etal-2014-crowdsourcing,0,0.0851378,"h structured learning. Out of CRF using L-BFGS updates, CRF with passive-aggressive updates to combat Twitter noise (Derczynski and Bontcheva, 2014), and structured perceptron (also useful on Twitter noise (Johannsen et al., 2014)), CRF L-BFGS provided the best performance on our dataset for the ten-types task. 3.5 Training Data In our final system, we included the dev 2015 data, to combat drift present in the corpus. We anticipated that the test set would be from 2015. The original dataset was harvested in 2010, long enough ago to be demonstrably disadvantaged when compared with modern data (Fromreide et al., 2014), and so it was critical to include something more. The compensate for the size imbalance – the dev 2015 data is 0.175 the size of the 2010 data – we weighted down the older dataset to by 0.7, as suggested by (Cherry and Guo, 2015), implemented by uniformly scaling individual feature values on older instances. This successfully reduced the negative impact of the inevitable drift. 4 5 5.1 Analysis Features In terms of features, we looked at the strongestweighted observations in the notypes model, to see what the general indicators are of named entities in tweets. The largest of these are shown"
W15-4306,S14-1001,0,0.0267027,"Missing"
W15-4306,P08-1068,0,0.0672496,"the text type. 250 million tweets from 20102012 were used to generate 2,000 word classes using Brown clustering (Brown et al., 1992). Typically 1,000 or fewer are used; the larger number of classes was chosen because it helpfully increased the expressivity of the representation (Derczynski et al., 2015a), while retaining a useful sparsity reduction. These hierarchical classes were represented using bit depths of 3-10 inclusive, and then 12, 14, 16, 18 and 20, one feature per depth. The typical levels are 4, 6, 10 and 20, though selection of bit depths to use often yields brittle feature sets (Koo et al., 2008), and so we leave it to the classifier to decide which ones are useful. These choices are examined in our post-exercise investigations into the model, Section 5.1, and the clusters provided with this paper. Finally, we also include the Brown class paths for the previous token. To aid in filtering out common tokens and reducing the impact they may have as e.g. spurious gazetteer matches, we incorporate a term frequency from our language model. This is applied to terms that are in the top 50,000 found in our garden hose sample, and represented as a feature having a value scaled in proportion to"
W15-4306,P12-3005,0,0.0355868,"Missing"
W15-4306,D11-1141,0,0.712406,"ly corresponding types. To build gazetteers, we therefore retrieved all Freebase types for all entities in the training corpus and selected the most prominent Freebase types per entity type in the gold standard. The list of Freebase types corresponding to each entity type in the gold standard is listed in Table 1. For each Freebase type, separate gazetteers were created for entity names and alternative names (aliases), since the latter tend to be of lower quality. There were several other gazetteer sources that we tried but which did not work very well: IMDb dumps,2 Ritter’s LabeledLDA lists (Ritter et al., 2011) (duplicated in the baseline system), and ANNIE’s other Morpho-Syntactic Features To model context, we used reasonably conventional features: the token itself, the uni- and bigrams in a [−2, 2] offset window from the current token, and both wordshape (e.g. London becomes Xxxxxx) and reduced wordshape (London to Xx) features. We also included a part-of-speech tag for each token. These were automatically generated by a custom tweet PoS tagger using an extension of the PTB tagset (Derczynski et al., 2013b). To capture orthographic information, we take suffix and prefix features of length [1..3]."
W15-4306,W06-2918,0,0.213294,"nal: the all-types and multiple-types tasks are effectively similar when contrasted with the single-types task, in that they require the recognition of many different kinds of named entity. Finally, we found that other gazetteer types were not helpful to performance; taking for example all of the ANNIE gazetteers, gazetteers from IMDb dumps, entity names extracted from other Twitter NER corpora, or entities generated through LLDA (Ritter et al., 2011) all decreased performance. We suspect this is due to their swamping already-small input dataset with too great a profusion of information, c.f. Smith and Osborne (2006). In addition, we tried generating semi-supervised data using vote-constrained bootstrapping, but this was not helpful either – presumably due to the initially low performance of machine-learning based tools on Twitter NER making it hard to develop semi-supervised bootstrapped training data, no matter how stringent the filtering of autogenerated examples. For the final run, we were faced with a decision about fitting. We could either choose a configuration that minimised training loss on all the available training data (train + dev + dev 2015), but risked overfitting to it. Alternatively, we c"
W15-4306,W15-0211,1,0.807824,"r B-company B-geo-loc B-person B-facility B-facility B-sportsteam B-tvshow B-person B-product B-other B-geo-loc B-person B-geo-loc B-person B-company Terms -0.571505 -0.585369 -0.604976 -0.620909 -0.655420 0.699101 0.699101 0.709865 0.714127 -0.717037 0.747492 0.774895 0.804635 -0.894333 0.895203 0.950866 1.044984 5.2 Gold standard When developing the system, we encountered several problems and inconsistencies in the gold standard. These issues are partly a general problem of developing gold standards, i.e. the more complicated the task is, the more humans tend to disagree on correct answers (Tissot et al., 2015). For Twitter NERC with 10 types, some of the tokens are very difficult to label because the context window is very small (140 characters), which then also leads to acronyms being used very frequently to save space, and because world knowledge about sports, music etc. is required. In particular, the following groups of problems in the gold standard training corpus were identified: Table 4: Largest-weighted Brown cluster features in 10-types task numbers rarely start entities; and being matched by an entry in the video games gazetteer suggests being an entity. One cluster prefix was indicative"
W15-4306,E14-4014,1,\N,Missing
W15-4306,R13-1026,1,\N,Missing
W15-4306,R13-1011,1,\N,Missing
W16-3928,W16-3930,0,0.0746246,"96.3 2121.6 849.8 2075.7 3413.3 2232.8 3333.9 5503.0 5586.8 0.534 0.524 0.640 0.638 0.502 0.631 0.417 0.254 0.106 0.088 0.090 0.0 0.0 12.9 13.0 0.0 13.7 85.7 689.2 2289.3 6229.9 6514.4 1325.6 1108.9 1861.2 2021.3 1318.8 2409.0 2835.6 3487.6 4670.9 6819.8 6519.8 Table 4: User-level results comparison for pre 2016 (left) and 2016 (right) test sets method in the validation stage, and if no results are found, backing off to the next classifier. As highly accurate methods rely on location-based services or reliable social network information, this ensemble approach is primarily precision-oriented. Chi et al. (2016) (“IBM”) applied multinomial naive Bayes methods over different sets of features. Unlike other teams, they only used tweet text data in their system. The system used an existing set of location-indicative words, gazetted location names from GeoNames, hashtags, users mentions, and a combination of all of these features. In addition, a frequency-based method was used to filter the combined feature set. Experimental results show feature selection over the methods combined achieved the best results across all metrics. Tweet-level and user-level results are shown in Table 1 and Table 2, respectivel"
W16-3928,N16-1122,0,0.0251185,"Missing"
W16-3928,C12-1064,1,0.679011,"Missing"
W16-3928,W16-3929,0,0.0594753,"among which 3 teams submitted system description papers. Miura et al. (2016) (“F UJI X EROX”) applied vectorised inputs in linear models for both tweet-level and user-level geotagging tasks. They used tweet text, user self-declared locations, timezone values, and user self-descriptions as input sources. Each source of input is transformed from a one-hot bag-ofword representation into a vector. Vectors from the same source are averaged, and vectors from different sources then are concatenated and used as the input to linear models. A softmax function is used to select the most probable class. Jayasinghe et al. (2016) (“CSIRO”) adopted ensemble learning methods with carefully extracted features from various available sources in both text and metadata. Namely, the authors implemented label propagation methods among tweet posts, location name mappings, text retrieval methods that assume similar text comes from the same region, and language-based classifiers. Having obtained outputs from each of the individual methods, the authors combined the resultant features in different ways. The best accuracy is achieved by using accuracy-ordered predictions, i.e. taking the prediction from the best 215 SUBMISSION ACCUR"
W16-3928,W16-3931,0,0.191324,"30.6 262.7 630.2 1711.1 4000.2 5714.9 1122.3 963.8 1928.8 2071.5 1084.3 2242.4 3124.4 2860.2 4002.4 6161.4 6053.3 Table 2: User-level results ranked by median error distance wildly-wrong predictions. Specifically, as our distance-based metrics, we measure (in kilometres): (a) the median error distance; and (b) the mean error distance. Although distance-based metrics are more intuitive, class-based predictions such as city labels are often easier to use in downstream applications. 4 Systems and Results In total, 5 teams uploaded 21 runs, among which 3 teams submitted system description papers. Miura et al. (2016) (“F UJI X EROX”) applied vectorised inputs in linear models for both tweet-level and user-level geotagging tasks. They used tweet text, user self-declared locations, timezone values, and user self-descriptions as input sources. Each source of input is transformed from a one-hot bag-ofword representation into a vector. Vectors from the same source are averaged, and vectors from different sources then are concatenated and used as the input to linear models. A softmax function is used to select the most probable class. Jayasinghe et al. (2016) (“CSIRO”) adopted ensemble learning methods with car"
W17-4418,W15-4319,0,0.0194738,"Missing"
W17-4418,P06-4018,0,0.0191442,"as belize Bermuda botswana virginislands Guam isleofman jamaica TrinidadandTobago Anglosphere, high-traffic: usa unitedkingdom canada ireland newzealand australia southafrica Cities: cincinnati seattle leeds bristol vancouver calgary cork galway wellington sydney perth johannesburg montegobay To ensure that comments would be likely to include named entities, the data was preempted (Derczynski and Bontcheva, 2015) using proper nouns as an entity-bearing signal. Documents were filtered to include only those between 20 and 400 characters in length, split into sentences, and tagged with the NLTK (Bird, 2006) and Stanford CoreNLP (Manning et al., 2014) (using the GATE English Twitter model (Cunningham et al., 2012; Derczynski et al., 2013)) POS taggers. Only sentences with at least one word that was tagged as NNP by both taggers were kept. Data To focus the task on emerging and rare entities, we set out to assemble a dataset where very few surface forms occur in regular training data and very few surface forms occur more than once. Ideally, none of the surface forms would be shared between the training data and test data, but this was too ambitious in the time available. 3.1 Sources and Selection"
W17-4418,E14-2025,1,0.827081,"ter both named entity recognition and typing, so it fits well in this use case. Table 1: The emerging entity dataset statistics creative-work – Names of creative works (e.g. Bohemian Rhapsody). Include punctuation in the middle of names. The work should be created by a human, and referred to by its specific name. group – Names of groups (e.g. Nirvana, San Diego Padres). Don’t mark groups that don’t have a specific, unique name, or companies (which should be marked corporation). 3.5 Annotation Once selected and preprocessed, annotations were taken from the crowd. The GATE crowdsourcing plugin (Bontcheva et al., 2014) provided effective mediation with CrowdFlower for this. Three annotators were allocated per document/sentence, and all sentences were multiply annotated. Annotators were selected from the UK, USA, Australia, New Zealand, Ireland, Canada, Jamaica and Botswana. Once gathered, crowd annotations were processed using max-recall automatic adjudication, which has proven effective for social media text (Derczynski et al., 2016). The authors performed a final manual annotation over the resulting corpus, to compensate for crowd noise. 3.6 5 Results of the evaluation are given in Table 2. Note that surf"
W17-4418,Q16-1026,1,0.359235,"makes disambiguation hard, too, as the spatial, temporal, conversational 7 Related Work Named entity recognition has a long standing tradition of shared tasks, with the most prominent being the multilingual named entity recognition tasks organised at CoNLL in 2002 and 2003 (Sang, 2002; Tjong Kim Sang and Meulder, 2003). However, these, as well as follow-up tasks such as ACE (LDC, 2005) focused on formal and relatively clean texts such as newswire. This remains a difficult task, especially with the addition of the OntoNotes dataset, with modern work still pushing forward the state of the art (Chiu and Nichols, 2016). Since 2011, Twitter has been gaining attention as a rich source for information extraction challenges such as (Ritter et al., 2011) and the Making Sense of Microposts challenge series starting in 2013 (Rizzo et al., 2017). Emerging entities have received some attention entity linking approaches (Hoffart et al., 2014; far, 2016; NIST, 2017). In particular for entity linking, identifying whether an entity is present in a knowledge base to prevent an erroneous link from being created is a key problem. Rare entities are an even less researched problem. Recasens et al. (2013) attempt to identify"
W17-4418,R15-1018,1,0.852484,"ountry- and citylevel subreddits were included, as well as nongeospecific forums like /r/restaurants. The full list used was: Global: politics worldnews news sports soccer restaurants Anglosphere, low-traffic: bahamas belize Bermuda botswana virginislands Guam isleofman jamaica TrinidadandTobago Anglosphere, high-traffic: usa unitedkingdom canada ireland newzealand australia southafrica Cities: cincinnati seattle leeds bristol vancouver calgary cork galway wellington sydney perth johannesburg montegobay To ensure that comments would be likely to include named entities, the data was preempted (Derczynski and Bontcheva, 2015) using proper nouns as an entity-bearing signal. Documents were filtered to include only those between 20 and 400 characters in length, split into sentences, and tagged with the NLTK (Bird, 2006) and Stanford CoreNLP (Manning et al., 2014) (using the GATE English Twitter model (Cunningham et al., 2012; Derczynski et al., 2013)) POS taggers. Only sentences with at least one word that was tagged as NNP by both taggers were kept. Data To focus the task on emerging and rare entities, we set out to assemble a dataset where very few surface forms occur in regular training data and very few surface f"
W17-4418,C16-1111,1,0.363808,"non-corporate organisations) Note that the data is of mixed domains, and that the proportions of the mixture are not the same in dev and test data. This is intended to provide a maximally adverse machine learning environment. The underlying goal is to improve NER in a novel and emerging situation, where there is a high degree of drift. This challenges systems to generalise as best they can, instead of e.g. memorising or relying on stable context- or sub-wordlevel cues. Additionally, we know that entities mentioned vary over time, as does the linguistic context in which entities are situated (Derczynski et al., 2016). Changing the particular variant of noisy, user-generated text somewhat between partitions helps create this environment, high in diversity, and helps represent the constant variation found in the wild. 3.2 Data Splits The following guidelines were used for each class. person – Names of people (e.g. Virginia Wade). Don’t mark people that don’t have their own name. Include punctuation in the middle of names. Fictional people can be included, as long as they’re referred to by name (e.g. Harry Potter). location – Names that are locations (e.g. France). Don’t mark locations that don’t have their"
W17-4418,P14-5010,0,0.0263576,"slands Guam isleofman jamaica TrinidadandTobago Anglosphere, high-traffic: usa unitedkingdom canada ireland newzealand australia southafrica Cities: cincinnati seattle leeds bristol vancouver calgary cork galway wellington sydney perth johannesburg montegobay To ensure that comments would be likely to include named entities, the data was preempted (Derczynski and Bontcheva, 2015) using proper nouns as an entity-bearing signal. Documents were filtered to include only those between 20 and 400 characters in length, split into sentences, and tagged with the NLTK (Bird, 2006) and Stanford CoreNLP (Manning et al., 2014) (using the GATE English Twitter model (Cunningham et al., 2012; Derczynski et al., 2013)) POS taggers. Only sentences with at least one word that was tagged as NNP by both taggers were kept. Data To focus the task on emerging and rare entities, we set out to assemble a dataset where very few surface forms occur in regular training data and very few surface forms occur more than once. Ideally, none of the surface forms would be shared between the training data and test data, but this was too ambitious in the time available. 3.1 Sources and Selection In this section, we detail the dataset creat"
W17-4418,R13-1026,1,0.475716,"ingdom canada ireland newzealand australia southafrica Cities: cincinnati seattle leeds bristol vancouver calgary cork galway wellington sydney perth johannesburg montegobay To ensure that comments would be likely to include named entities, the data was preempted (Derczynski and Bontcheva, 2015) using proper nouns as an entity-bearing signal. Documents were filtered to include only those between 20 and 400 characters in length, split into sentences, and tagged with the NLTK (Bird, 2006) and Stanford CoreNLP (Manning et al., 2014) (using the GATE English Twitter model (Cunningham et al., 2012; Derczynski et al., 2013)) POS taggers. Only sentences with at least one word that was tagged as NNP by both taggers were kept. Data To focus the task on emerging and rare entities, we set out to assemble a dataset where very few surface forms occur in regular training data and very few surface forms occur more than once. Ideally, none of the surface forms would be shared between the training data and test data, but this was too ambitious in the time available. 3.1 Sources and Selection In this section, we detail the dataset creation. Training data – Following the WNUT15 task (Baldwin et al., 2015), the dataset from e"
W17-4418,N06-2015,0,0.0207417,"ties recognition task, we aim to establish a new benchmark dataset and current state-of-the-art for the recognition of entities in the long tail. Most language expressions form a Zipfian distribution (Zipf, 1949; Montemurro, 2001) wherein a small number of very frequent observations occur and a very long tail of less frequent observations. Our research community’s benchmark datasets, representing only a small sample of all language expressions, often follow a similar distribution if a standard sample is taken. Recently, an awareness of the limitations of current evaluation datasets has risen (Hovy et al., 2006; van Erp et al., 2016; Postma et al., 2016). Due to this bias and the way many NLP apNamed Entity Recognition (NER) is the task of finding in text special, unique names for specific concepts. For example, in “Going to San Diego”, “San Diego” refers to a specific instance of a location; compare with “Going to the city”, where the destination isn’t named, but rather a generic city. NER is sometimes described as a solved task due to high reported scores on well-known datasets, but in fact the systems that achieve these scores tend to fail on rarer or previously-unseen entities, making the majori"
W17-4418,W16-6004,1,0.887458,"Missing"
W17-4418,N13-1071,0,0.0318245,"Missing"
W17-4418,W17-4420,0,0.0694885,"Missing"
W17-4418,D11-1141,0,0.0937672,"with at least one word that was tagged as NNP by both taggers were kept. Data To focus the task on emerging and rare entities, we set out to assemble a dataset where very few surface forms occur in regular training data and very few surface forms occur more than once. Ideally, none of the surface forms would be shared between the training data and test data, but this was too ambitious in the time available. 3.1 Sources and Selection In this section, we detail the dataset creation. Training data – Following the WNUT15 task (Baldwin et al., 2015), the dataset from earlier Twitter NER exercises (Ritter et al., 2011) comprised this task’s training data. This dataset is made up of 1,000 annotated tweets, totaling 65,124 tokens. YouTube The corpus includes YouTube comments. These are drawn from the all-time top 100 videos across all categories, within certain parts of the anglosphere (specifically the US, the US, Canada, Ireland, New Zealand, Australia, Jamaica, Botswana, South Africa and Singapore) during April 2017. One hundred top-level comments were drawn from each video. Non-English comments were removed with langid.py (Lui and Baldwin, 2012). Finally, in an attempt to cut out trite comments and diatri"
W17-4418,P85-1022,0,0.165194,"but in “Amazon Web Services” it is part of a product name. The ‘White House’ can both be a location and a corporation, which requires the systems to distinguish between subtle contextual differences in use of the term. The difficulty in detecting entities of class creative-work can often be explained by the fact that these entities contain person names (e.g. “Grimm”) , common words (e.g. “Demolition Man”, “Rogue One”) and can be quite long (e.g. “Miss Peregrine’s Home for Peculiar Children”). Annotation still remains hard; some entities in the corpus, if we co-opt Kripke’s “rigid designator” (Kripke, 1972) to define that role, are hard to fit into a single category. There were also other types of entity in the data; we did not attempt to define a comprehensive classification schema. The shortness of texts often makes disambiguation hard, too, as the spatial, temporal, conversational 7 Related Work Named entity recognition has a long standing tradition of shared tasks, with the most prominent being the multilingual named entity recognition tasks organised at CoNLL in 2002 and 2003 (Sang, 2002; Tjong Kim Sang and Meulder, 2003). However, these, as well as follow-up tasks such as ACE (LDC, 2005) f"
W17-4418,W17-4421,0,0.0125859,"Missing"
W17-4418,W02-2024,0,0.388564,"so associated to five topics (including movies, politics, physics, scifi and security) were downloaded from archive.org3 . From these title posts and comments, 400 samples were uniformly drawn for each topic. Note that title posts and comments that are shorter than 20 characters or longer than 500 characters were excluded, in order to keep the task feasible but still challenging. On average the length of title posts and comments is 118.73 with a standard deviation of 100.89. 3.4 Annotation Guidelines Various named entity annotation schemes are available for named entity annotation (cf. CoNLL (Sang, 2002), ACE (LDC, 2005), MSM (Rizzo et al., 2016)). Based on these, we annotate the following entity types: 1. 2. 3. 4. person location (including GPE, facility) corporation product (tangible goods, or well-defined services) 5. creative-work (song, movie, book and so on) 6. group (subsuming music band, sports team, and non-corporate organisations) Note that the data is of mixed domains, and that the proportions of the mixture are not the same in dev and test data. This is intended to provide a maximally adverse machine learning environment. The underlying goal is to improve NER in a novel and emergi"
W17-4418,P11-1037,0,0.0211706,"(Lin et al., 2017) SpinningBytes (von D¨aniken and Cieliebak, 2017) UH-RiTUAL (Aguilar et al., 2017) F1 (entity) 39.98 26.30 38.35 37.06 40.42 40.78 41.86 F1 (surface) 37.77 25.26 36.31 34.25 37.62 39.33 40.24 Table 2: Results of the emerging entity extraction task. and topical context which a human reader relies on to interpret texts are all hidden under this model of annotation. Twitter accounts can also fall into a number of different classes, and rather than instruct annotators on this, we left behavior up to them. Much prior work has avoided assigning tags to these (Ritter et al., 2011; Liu et al., 2011) though accounts often represent not only a person, also organizations, regions, buildings and so on. Therefore, much of our data carries these labels on Twitter account names, where the annotator has specified it. Locations that contain elements that are also common in person names present an obstacle for the participating systems, for example in the detection of “Smith Tower” or “Crystal Palace” where “Smith” and “Crystal” are sometimes recognised as person names. Names originating from other languages such as “Leyonhjelm” or “Zlatan” for persons or “Sonmarg” and “Mahazgund” for locations of"
W17-4418,W17-4424,0,0.0288559,"Missing"
W17-4418,P12-3005,0,0.00999431,"et al., 2015), the dataset from earlier Twitter NER exercises (Ritter et al., 2011) comprised this task’s training data. This dataset is made up of 1,000 annotated tweets, totaling 65,124 tokens. YouTube The corpus includes YouTube comments. These are drawn from the all-time top 100 videos across all categories, within certain parts of the anglosphere (specifically the US, the US, Canada, Ireland, New Zealand, Australia, Jamaica, Botswana, South Africa and Singapore) during April 2017. One hundred top-level comments were drawn from each video. Non-English comments were removed with langid.py (Lui and Baldwin, 2012). Finally, in an attempt to cut out trite comments and diatribes, comments were filtered for length: min 10, max 200 characters. Development and test data – Whilst Twitter is a rich source for noisy user-generated data, we also sought to include texts that were longer than 140 characters as these exhibit different writing styles and characteristics. To align some of the development and test data with the training data, we included Twitter as a source, but additional comments were mined from Reddit, YouTube and StackExchange. These sources were chosen because they are large and samples can be m"
W17-4418,W16-3919,0,0.0596652,"Missing"
W17-4418,L16-1693,1,0.361861,"Missing"
W17-4418,W17-4422,0,0.0331358,"Missing"
W17-4418,W17-4423,0,0.0472582,"Missing"
W17-4418,W17-4419,0,\N,Missing
W19-6121,aker-etal-2017-simple,1,0.687368,"(Zeng et al., 2016; Tutek et al., 2016). Recently, deep learning approaches have shown promise at this task. The two top performing teams of SemEval 2016 Task 6 both applied deep learning models (Zarrella and Marsh, 2016; Wei et al., 2016) as did those in RumourEval 2019 (Gorrell et al., 2019). The task of stance detection has been applied widely within political analysis, both analyzing the stance of politicians towards a given topic (Lai et al., 2016; Skeppstedt et al., 2017), as is the task within this paper, and also to identify the stance of individuals towards some politician or policy (Aker et al., 2017; Augenstein et al., 2016; Mohammad et al., 2016; Johnson and Goldwasser, 2016; Iyyer et al., 2014). For several of these cases, the stance target has been mentioned explicitly in the data. This is not necessarily the case for the dataset generated for this paper, increasing the difficulty of the task significantly. Furthermore, all of these examples perform stance detection for English, whereas the dataset generated for this data is in Danish. This further increases the difficulty of the task, as fewer resources are available. Enevoldsen and Hansen (2017) perform sentiment analysis in Danish"
W19-6121,D16-1084,0,0.192286,"Missing"
W19-6121,Q18-1041,0,0.0724089,"019), and only very recently has any work been done for stance in Danish in the first place – just Lillie et al. (2019), published at the same time as this paper. 3 Data We assembled a dataset of quotes from Danish politicians, extracted from articles from the Danish media outlet Ritzau. Considerations were made regarding the objectivity of the collected data, and seeing as Ritzau is owned by a conglomerate of media outlets from all areas of the political spectrum (Ritzau, 2019), it is assumed that articles from the media outlet will not contain bias towards any given party. A data statement (Bender and Friedman, 2018) is in the appendix. A shortlist of possible topics to include in the dataset was attained based on an opinion poll executed by Kvalvik (2017), seeking to identify the topics most important to the Danish population, when voting in the next election. Here, the five most important topics were identified as health policy, social policy, immigration policy, crime and justice policy and finally environment and climate policy. Immigration policy was chosen as the topic to be included in the dataset, due to alternative topics being defined too broadly to easily allow a clear definition of annotation"
W19-6121,S17-2006,1,0.733158,"Missing"
W19-6121,S19-2147,1,0.775065,"earch within the field of NLP in Danish. 2 Related Work Stance detection has been addressed through a number of different model approaches, including probabilistic classifiers (Qazvinian et al., 2011), kernel-based classifiers (Mohammad et al., 2017; Enayet and El-Beltagy, 2017; Collins and Duffy, 2001) and ensemble learners (Zeng et al., 2016; Tutek et al., 2016). Recently, deep learning approaches have shown promise at this task. The two top performing teams of SemEval 2016 Task 6 both applied deep learning models (Zarrella and Marsh, 2016; Wei et al., 2016) as did those in RumourEval 2019 (Gorrell et al., 2019). The task of stance detection has been applied widely within political analysis, both analyzing the stance of politicians towards a given topic (Lai et al., 2016; Skeppstedt et al., 2017), as is the task within this paper, and also to identify the stance of individuals towards some politician or policy (Aker et al., 2017; Augenstein et al., 2016; Mohammad et al., 2016; Johnson and Goldwasser, 2016; Iyyer et al., 2014). For several of these cases, the stance target has been mentioned explicitly in the data. This is not necessarily the case for the dataset generated for this paper, increasing t"
W19-6121,P82-1020,0,0.692457,"Missing"
W19-6121,P14-1105,0,0.0210386,"this task. The two top performing teams of SemEval 2016 Task 6 both applied deep learning models (Zarrella and Marsh, 2016; Wei et al., 2016) as did those in RumourEval 2019 (Gorrell et al., 2019). The task of stance detection has been applied widely within political analysis, both analyzing the stance of politicians towards a given topic (Lai et al., 2016; Skeppstedt et al., 2017), as is the task within this paper, and also to identify the stance of individuals towards some politician or policy (Aker et al., 2017; Augenstein et al., 2016; Mohammad et al., 2016; Johnson and Goldwasser, 2016; Iyyer et al., 2014). For several of these cases, the stance target has been mentioned explicitly in the data. This is not necessarily the case for the dataset generated for this paper, increasing the difficulty of the task significantly. Furthermore, all of these examples perform stance detection for English, whereas the dataset generated for this data is in Danish. This further increases the difficulty of the task, as fewer resources are available. Enevoldsen and Hansen (2017) perform sentiment analysis in Danish using newspaper articles, using the AFINN dictionary over sentiment of ˚ Danish words (Arup Nielsen"
W19-6121,C16-1279,0,0.016359,"proaches have shown promise at this task. The two top performing teams of SemEval 2016 Task 6 both applied deep learning models (Zarrella and Marsh, 2016; Wei et al., 2016) as did those in RumourEval 2019 (Gorrell et al., 2019). The task of stance detection has been applied widely within political analysis, both analyzing the stance of politicians towards a given topic (Lai et al., 2016; Skeppstedt et al., 2017), as is the task within this paper, and also to identify the stance of individuals towards some politician or policy (Aker et al., 2017; Augenstein et al., 2016; Mohammad et al., 2016; Johnson and Goldwasser, 2016; Iyyer et al., 2014). For several of these cases, the stance target has been mentioned explicitly in the data. This is not necessarily the case for the dataset generated for this paper, increasing the difficulty of the task significantly. Furthermore, all of these examples perform stance detection for English, whereas the dataset generated for this data is in Danish. This further increases the difficulty of the task, as fewer resources are available. Enevoldsen and Hansen (2017) perform sentiment analysis in Danish using newspaper articles, using the AFINN dictionary over sentiment of ˚ Danis"
W19-6121,W19-6141,1,0.423881,"ng the difficulty of the task significantly. Furthermore, all of these examples perform stance detection for English, whereas the dataset generated for this data is in Danish. This further increases the difficulty of the task, as fewer resources are available. Enevoldsen and Hansen (2017) perform sentiment analysis in Danish using newspaper articles, using the AFINN dictionary over sentiment of ˚ Danish words (Arup Nielsen, 2011), performing ternary classification of articles using for, against and neutral labels. However, no research has been done within political stance detection in Danish (Kirkedal et al., 2019), and only very recently has any work been done for stance in Danish in the first place – just Lillie et al. (2019), published at the same time as this paper. 3 Data We assembled a dataset of quotes from Danish politicians, extracted from articles from the Danish media outlet Ritzau. Considerations were made regarding the objectivity of the collected data, and seeing as Ritzau is owned by a conglomerate of media outlets from all areas of the political spectrum (Ritzau, 2019), it is assumed that articles from the media outlet will not contain bias towards any given party. A data statement (Bend"
W19-6121,W19-6122,1,0.712552,"whereas the dataset generated for this data is in Danish. This further increases the difficulty of the task, as fewer resources are available. Enevoldsen and Hansen (2017) perform sentiment analysis in Danish using newspaper articles, using the AFINN dictionary over sentiment of ˚ Danish words (Arup Nielsen, 2011), performing ternary classification of articles using for, against and neutral labels. However, no research has been done within political stance detection in Danish (Kirkedal et al., 2019), and only very recently has any work been done for stance in Danish in the first place – just Lillie et al. (2019), published at the same time as this paper. 3 Data We assembled a dataset of quotes from Danish politicians, extracted from articles from the Danish media outlet Ritzau. Considerations were made regarding the objectivity of the collected data, and seeing as Ritzau is owned by a conglomerate of media outlets from all areas of the political spectrum (Ritzau, 2019), it is assumed that articles from the media outlet will not contain bias towards any given party. A data statement (Bender and Friedman, 2018) is in the appendix. A shortlist of possible topics to include in the dataset was attained ba"
W19-6121,L16-1623,0,0.168562,"ently, deep learning approaches have shown promise at this task. The two top performing teams of SemEval 2016 Task 6 both applied deep learning models (Zarrella and Marsh, 2016; Wei et al., 2016) as did those in RumourEval 2019 (Gorrell et al., 2019). The task of stance detection has been applied widely within political analysis, both analyzing the stance of politicians towards a given topic (Lai et al., 2016; Skeppstedt et al., 2017), as is the task within this paper, and also to identify the stance of individuals towards some politician or policy (Aker et al., 2017; Augenstein et al., 2016; Mohammad et al., 2016; Johnson and Goldwasser, 2016; Iyyer et al., 2014). For several of these cases, the stance target has been mentioned explicitly in the data. This is not necessarily the case for the dataset generated for this paper, increasing the difficulty of the task significantly. Furthermore, all of these examples perform stance detection for English, whereas the dataset generated for this data is in Danish. This further increases the difficulty of the task, as fewer resources are available. Enevoldsen and Hansen (2017) perform sentiment analysis in Danish using newspaper articles, using the AFINN dictio"
W19-6121,D11-1147,0,0.467558,"Missing"
W19-6121,S16-1074,0,0.0206351,"tic labelling of quotes, as well as to be used as a benchmark for further research within the field of NLP in Danish. 2 Related Work Stance detection has been addressed through a number of different model approaches, including probabilistic classifiers (Qazvinian et al., 2011), kernel-based classifiers (Mohammad et al., 2017; Enayet and El-Beltagy, 2017; Collins and Duffy, 2001) and ensemble learners (Zeng et al., 2016; Tutek et al., 2016). Recently, deep learning approaches have shown promise at this task. The two top performing teams of SemEval 2016 Task 6 both applied deep learning models (Zarrella and Marsh, 2016; Wei et al., 2016) as did those in RumourEval 2019 (Gorrell et al., 2019). The task of stance detection has been applied widely within political analysis, both analyzing the stance of politicians towards a given topic (Lai et al., 2016; Skeppstedt et al., 2017), as is the task within this paper, and also to identify the stance of individuals towards some politician or policy (Aker et al., 2017; Augenstein et al., 2016; Mohammad et al., 2016; Johnson and Goldwasser, 2016; Iyyer et al., 2014). For several of these cases, the stance target has been mentioned explicitly in the data. This is not n"
W19-6121,S16-1075,0,0.0267833,"neutral. The goal of this work is to create a model which can perform this task, both to be used as a tool for political analysis and to expand the generated dataset by automatic labelling of quotes, as well as to be used as a benchmark for further research within the field of NLP in Danish. 2 Related Work Stance detection has been addressed through a number of different model approaches, including probabilistic classifiers (Qazvinian et al., 2011), kernel-based classifiers (Mohammad et al., 2017; Enayet and El-Beltagy, 2017; Collins and Duffy, 2001) and ensemble learners (Zeng et al., 2016; Tutek et al., 2016). Recently, deep learning approaches have shown promise at this task. The two top performing teams of SemEval 2016 Task 6 both applied deep learning models (Zarrella and Marsh, 2016; Wei et al., 2016) as did those in RumourEval 2019 (Gorrell et al., 2019). The task of stance detection has been applied widely within political analysis, both analyzing the stance of politicians towards a given topic (Lai et al., 2016; Skeppstedt et al., 2017), as is the task within this paper, and also to identify the stance of individuals towards some politician or policy (Aker et al., 2017; Augenstein et al., 2"
W19-6121,S16-1062,0,0.0283474,"s well as to be used as a benchmark for further research within the field of NLP in Danish. 2 Related Work Stance detection has been addressed through a number of different model approaches, including probabilistic classifiers (Qazvinian et al., 2011), kernel-based classifiers (Mohammad et al., 2017; Enayet and El-Beltagy, 2017; Collins and Duffy, 2001) and ensemble learners (Zeng et al., 2016; Tutek et al., 2016). Recently, deep learning approaches have shown promise at this task. The two top performing teams of SemEval 2016 Task 6 both applied deep learning models (Zarrella and Marsh, 2016; Wei et al., 2016) as did those in RumourEval 2019 (Gorrell et al., 2019). The task of stance detection has been applied widely within political analysis, both analyzing the stance of politicians towards a given topic (Lai et al., 2016; Skeppstedt et al., 2017), as is the task within this paper, and also to identify the stance of individuals towards some politician or policy (Aker et al., 2017; Augenstein et al., 2016; Mohammad et al., 2016; Johnson and Goldwasser, 2016; Iyyer et al., 2014). For several of these cases, the stance target has been mentioned explicitly in the data. This is not necessarily the case"
W19-6122,aker-etal-2017-simple,1,0.160037,"rumours? Research has tried to tackle this problem (Qazvinian et al., 2011), but automated rumour veracity prediction is still maturing (Gorrell et al., 2019). This project investigates stance classification as a step for automatically determining the veracity of a rumour. Previous research has shown that the stance of a crowd is a strong indicator for veracity (Dungs et al., 2018), but that it is a difficult task to build a reliable classifier (Derczynski et al., 2017). Moreover a study has shown that careful feature engineering can have substantial influence on the accuracy of a classifier (Aker et al., 2017). A system able to verify or refute rumours is typically made up of four components: rumour detection, rumour tracking, stance classification, and veracity classification (Zubiaga et al., 2018). This project will mainly be concerned with stance classification and rumour veracity classification. Current research is mostly concerned with the English language, and in particular data from Twitter is used as data source because of its availability and relevant news content (Derczynski et al., 2017; Gorrell et al., 2019). To our knowledge no research within this area has been carried out in a Danish"
W19-6122,W13-3520,0,0.0719281,"Missing"
W19-6122,D16-1084,0,0.122306,"Missing"
W19-6122,Q18-1041,0,0.0456825,"lect reality as the truth value of rumours may stay unverified. The amount of unverified rumours does however warrant more investigation in order to use all of the rumourous submissions for rumour veracity classification. Further details about the approach to unverified rumours are covered in Section 4. In total the dataset contains 3,007 Reddit posts distributed across 33 submissions respectively grouped into 16 events. The tools9 and annotated corpora (Lillie and Middelboe, 2019a) are openly released with this paper in GDPR-compliant, non-identifying format. See appendix for data statement (Bender and Friedman, 2018). 4 Method Our veracity prediction approach depends on two components: a stance classifier and a veracity classification component (Zubiaga et al., 2018). 4.1 Stance Classification For stance classification two different approaches have been used, one being an LSTM classifier in9 github.com/danish-stance-detectors spired by (Kochkina et al., 2017) and the other employing a number of classic machine learning models with a focus on feature engineering as presented in Aker et al. (2017). LSTM classifier: The LSTM model is widely used for tasks where the sequence of data and earlier elements in se"
W19-6122,S17-2006,1,0.91694,"Missing"
W19-6122,C18-1284,0,0.512947,"or may not be true (Huang et al., 2015). This has lead to the concept of fake news, or misinformation, where the spreading of a misleading rumour is intentional (Shu et al., 2017). Can we somehow automatically predict the veracity of rumours? Research has tried to tackle this problem (Qazvinian et al., 2011), but automated rumour veracity prediction is still maturing (Gorrell et al., 2019). This project investigates stance classification as a step for automatically determining the veracity of a rumour. Previous research has shown that the stance of a crowd is a strong indicator for veracity (Dungs et al., 2018), but that it is a difficult task to build a reliable classifier (Derczynski et al., 2017). Moreover a study has shown that careful feature engineering can have substantial influence on the accuracy of a classifier (Aker et al., 2017). A system able to verify or refute rumours is typically made up of four components: rumour detection, rumour tracking, stance classification, and veracity classification (Zubiaga et al., 2018). This project will mainly be concerned with stance classification and rumour veracity classification. Current research is mostly concerned with the English language, and in"
W19-6122,S17-2082,0,0.0249037,"Missing"
W19-6122,S19-2192,0,0.0214438,"Missing"
W19-6122,S19-2147,1,0.77425,"∗: These authors contributed to the paper equally. Leon Derczynski ITU Copenhagen Denmark ld@itu.dk Twitter. However these phenomena create a platform for the spread of rumours, that is, stories with unverified claims, which may or may not be true (Huang et al., 2015). This has lead to the concept of fake news, or misinformation, where the spreading of a misleading rumour is intentional (Shu et al., 2017). Can we somehow automatically predict the veracity of rumours? Research has tried to tackle this problem (Qazvinian et al., 2011), but automated rumour veracity prediction is still maturing (Gorrell et al., 2019). This project investigates stance classification as a step for automatically determining the veracity of a rumour. Previous research has shown that the stance of a crowd is a strong indicator for veracity (Dungs et al., 2018), but that it is a difficult task to build a reliable classifier (Derczynski et al., 2017). Moreover a study has shown that careful feature engineering can have substantial influence on the accuracy of a classifier (Aker et al., 2017). A system able to verify or refute rumours is typically made up of four components: rumour detection, rumour tracking, stance classificatio"
W19-6122,W19-6141,1,0.370974,"lities for true and false rumours respectively. The best scoring model, which include both stance labels and tweet times, scores an F1 of 0.804, while the HMM with only stance labels scores 0.756 F1 . The use of automatic stance labels from (Aker et al., 2017) is also applied, which does not change performance much, proving the method to have practical applications. It is also shown that using the model for rumour veracity prediction is still useful when limiting the number of tweets to e.g. 5 and 10 tweets respectively. Danish: While Danish is not a privileged language in terms of resources (Kirkedal et al., 2019), there is stance classification work on political quotes (Lehmann and Derczynski, 2019). However, this is over a different text genre, and does not focus on veracity prediction as its final goal. Comprehensive reviews of automatic veracity and rumour analysis from an NLP perspective include Zubiaga et al. (2018), Atanasova et al. (2019), and Lillie and Middelboe (2019b). 3 Dataset Because of various limitations on big social media platforms including Facebook and Twitter, Reddit is used as platform for the dataset.3 This is a novel approach; prior research has typically relied on Twitter (Moh"
W19-6122,S17-2083,0,0.275022,"distributed across 33 submissions respectively grouped into 16 events. The tools9 and annotated corpora (Lillie and Middelboe, 2019a) are openly released with this paper in GDPR-compliant, non-identifying format. See appendix for data statement (Bender and Friedman, 2018). 4 Method Our veracity prediction approach depends on two components: a stance classifier and a veracity classification component (Zubiaga et al., 2018). 4.1 Stance Classification For stance classification two different approaches have been used, one being an LSTM classifier in9 github.com/danish-stance-detectors spired by (Kochkina et al., 2017) and the other employing a number of classic machine learning models with a focus on feature engineering as presented in Aker et al. (2017). LSTM classifier: The LSTM model is widely used for tasks where the sequence of data and earlier elements in sequences are important (Goldberg, 2016). The temporal sequence of tweets was one of the motivations for Kochkina et al. (2017) to use the LSTM model for branches of tweets, as well as for the bidirectional conditional LSTM for Augenstein et al. (2016). While the results from both the Bi-LSTM in Augenstein et al. (2016) and Branch-LSTM in Kochkina e"
W19-6122,W19-6121,1,0.712552,"lude both stance labels and tweet times, scores an F1 of 0.804, while the HMM with only stance labels scores 0.756 F1 . The use of automatic stance labels from (Aker et al., 2017) is also applied, which does not change performance much, proving the method to have practical applications. It is also shown that using the model for rumour veracity prediction is still useful when limiting the number of tweets to e.g. 5 and 10 tweets respectively. Danish: While Danish is not a privileged language in terms of resources (Kirkedal et al., 2019), there is stance classification work on political quotes (Lehmann and Derczynski, 2019). However, this is over a different text genre, and does not focus on veracity prediction as its final goal. Comprehensive reviews of automatic veracity and rumour analysis from an NLP perspective include Zubiaga et al. (2018), Atanasova et al. (2019), and Lillie and Middelboe (2019b). 3 Dataset Because of various limitations on big social media platforms including Facebook and Twitter, Reddit is used as platform for the dataset.3 This is a novel approach; prior research has typically relied on Twitter (Mohammad et al., 2016; Derczynski et al., 2017; Gorrell et al., 2019). Data sampling: The d"
W19-6122,D11-1147,0,0.658771,"Missing"
W19-6122,W18-5501,0,0.035158,"Missing"
W19-6138,Q17-1010,0,0.0309326,"thography used is unusually consistent and each story is translated to (somewhat old fashioned) standard Danish, more or less sentence by sentence. Although not identical, the orthographic principles used by Kuhre are very similar to those used in the BO dictionary project. A data statement (Bender and Friedman, 2018) for these resources is given in the appendices. The data used in and produced by the dictionary project will be published under CC BY-SA. 3 Embeddings and Alignment Given some text in Bornholmsk, we attempted to induce distributional word embeddings. For this, we chose FastText (Bojanowski et al., 2017). As Bornholmsk is a low-resource language, it is important to be able to connect it to other languages easily. Standard FastText embeddings are available for many languages. FastText supports subword embeddings, which are likely to be useful in a language like Bornholmsk that has a relatively small alphabet, and also have some chance of compensating for the high data sparsity. Embeddings are induced with 300 dimensions, in order to be compatible with the public Common Crawl-based FastText embeddings. Having induced these embeddings for Bornholmsk ebornholmsk , they are then aligned into the e"
W19-6138,E14-2016,1,0.803777,"and on making it possible for Bornholmsk-speakers to work digitally in Bornholmsk instead of Danish. Danish: Det er Mads og han er en god dreng. Model output: Ded e slæføre a˚ hajn e en go majn. Correct Bornholmsk: De(d) e Mads a˚ hajn e ejn goer horra. Danish: København er en af de større byer – faktisk den største . Model output: København e en majed r˚aganat ! Correct Bornholmsk: Kjøvvenhawn e ejn a˚ di storre byana – fakta dæjn storsta . 6 Related Work There is no former work that we are aware of on NLP for Bornholmsk. The closest resource is an openly-available toolkit for Danish, DKIE (Derczynski et al., 2014), which is designed for the GATE platform (Cunningham et al., 2012), though even for Danish work is scarce (Kirkedal et al., 2019). Written Bornholmsk corpora are also rare; these exist almost entirely in smaller collections, some of which have been built with great care. Two other Scandinavian tongues as small as Bornholmsk have had quite different stories. Faroese (ISO639: fao; BCP-47: fo-FO) is spoken by about 72000 people, many of whom live in the Faroes; it has a fairly long written tradition and is actively published in. It has some NLP visibility, being present in the Universal Dependen"
W19-6138,W19-6141,1,0.779643,"en god dreng. Model output: Ded e slæføre a˚ hajn e en go majn. Correct Bornholmsk: De(d) e Mads a˚ hajn e ejn goer horra. Danish: København er en af de større byer – faktisk den største . Model output: København e en majed r˚aganat ! Correct Bornholmsk: Kjøvvenhawn e ejn a˚ di storre byana – fakta dæjn storsta . 6 Related Work There is no former work that we are aware of on NLP for Bornholmsk. The closest resource is an openly-available toolkit for Danish, DKIE (Derczynski et al., 2014), which is designed for the GATE platform (Cunningham et al., 2012), though even for Danish work is scarce (Kirkedal et al., 2019). Written Bornholmsk corpora are also rare; these exist almost entirely in smaller collections, some of which have been built with great care. Two other Scandinavian tongues as small as Bornholmsk have had quite different stories. Faroese (ISO639: fao; BCP-47: fo-FO) is spoken by about 72000 people, many of whom live in the Faroes; it has a fairly long written tradition and is actively published in. It has some NLP visibility, being present in the Universal Dependencies treebanks, and a steady if slow stream of NLP research includes the language (e.g. Richter et al. (2018)). In contrast, Scand"
W19-6138,D14-1162,0,0.0820619,"anslations of a few songs and poems can be found, which are parallel line-by-line. Snippets of words giving example uses in various informal 1:1 word-level dictionaries are also available – as well as the word mappings themselves. We used Kuhre’s folk stories as parallel DanishBornholmsk text. Further, we used entries from the nascent Bornholmsk Ordbog, which includes a number of genuine examples of how the language might be used. Noisier and non-canonical web data were included, to improve vocabulary coverage. The monolingual corpora is the basis for word embeddings, in this case with GloVe (Pennington et al., 2014) in 50 dimensions. The Kuhre text is in an older form of Danish, some spelling reforms ago. Specifically, vowels are annotated differently (aa and ee vs. a˚ and e´ ), 6 https://github.com/bplank/bilstm-aux. These entries contain optional terms that are both expanded & omitted to create additional training data. 7 and nouns have a capital initial. This data is copied with case removed, and with the vowels converted to the modern format, so that the resulting model is not too surprised by modern Danish. The Bornholmsk Ordbog is a work in progress, i.a. containing usage examples such as: <bællana"
W19-6138,P17-4012,0,0.0414084,"l training data. 7 and nouns have a capital initial. This data is copied with case removed, and with the vowels converted to the modern format, so that the resulting model is not too surprised by modern Danish. The Bornholmsk Ordbog is a work in progress, i.a. containing usage examples such as: <bællana hadde aˆ gebakka hærudanforr i vijnters {børnene havde kælkebakke herudenfor (huset) i vinters}&gt; These are converted into plaintext and used as supporting parallel examples. Table 3 gives an overview of the parallel text used. 5.1 Experimental Setup We trained a translation model with OpenNMT (Klein et al., 2017) using all parallel text. The Bornholmsk side of this was combined with the Bornholmsk monolingual texts to build a language model and embeddings. Test and validation data were both 500 pairs taken from the input data. Parameters included: Glorot initialization, locked to the encoding vectors, dropout at 0.4, an average decay of 1e-4, and validation every 4000 steps. 5.2 Pilot Results The translation performed reasonably, given the very small training data size. Examples: Danish: der stod en lys sky p˚a en mørk baggrund . Output: dær sto en art sjy p˚a ejn aˆ zstæl . Reference: dær sto et lyst"
W19-6138,P16-2067,0,0.0423066,"es can be posed to the tagger as if they were in lsource . This requires that embeddings for both languages, esource and etarget , are aligned to the general embeddings space e. There is also an assumption that lsource and ltarget will be sufficiently distributionally and grammatically similar. One is more likely to encounter new words during tagging when training data is limited, so a PoS tagger that tolerates previously-unseen words is preferable. The structbilty tagger6 uses a bidirectional LSTM with language modelling as auxiliary loss function and achieves good accuracy on unknown words (Plank et al., 2016). The source language evaluated is Danish and training and validation data is taken from the Universal Dependencies corpus (Nivre et al., 2016). Sans PoS-annotated Bornholmsk, we give example tagged sentences. Many structures and words picked up correctly, despite absent training data and a very small monolingual dataset for embedding induction. However, basic structures are occasionally missing (cf. #3). 1) Hanj/PROPN fijk/VERB dask/NOUN p˚a/ADP sinj/ADJ luzagˆada/NOUN 2) de/PRON ska/VERB varra/X s˚a/ADV galed/ADJ ,/PUNCT sa/SCONJ de/PRON ammar/VERB ijkkje/ADV ./PUNCT 2) Hon/PROPN ve/X hˆa/X"
W19-6138,L18-1369,0,0.0285419,"ish work is scarce (Kirkedal et al., 2019). Written Bornholmsk corpora are also rare; these exist almost entirely in smaller collections, some of which have been built with great care. Two other Scandinavian tongues as small as Bornholmsk have had quite different stories. Faroese (ISO639: fao; BCP-47: fo-FO) is spoken by about 72000 people, many of whom live in the Faroes; it has a fairly long written tradition and is actively published in. It has some NLP visibility, being present in the Universal Dependencies treebanks, and a steady if slow stream of NLP research includes the language (e.g. Richter et al. (2018)). In contrast, Scandoromani (ISO639: rmg/rmu) has many fewer speakers than Bornholmsk; its original grammar has been overtaken 7 Conclusion Acknowledgments This research was carried out with support from the Bornholmsk Ordbog project sponsored by Sparekassen Bornholms Fond, Brødrene E., S. & A. Larsens Legat, Kunst- og Kulturhistorisk R˚ad under Bornholms Regionskommune and Bornholms Brandforsikring, and with thanks to the NEIC/NordForsk NLPL project. Titan X GPUs used in this research were donated by the NVIDIA Corporation. We are grateful to those who have gathered and published resources o"
W19-6141,cieri-etal-2004-fisher,0,0.019469,"ibed data and creating more data rather than correcting existing transcriptions provides better performance (Sperber et al., 2016; Novotney and Callison-Burch, 7 11 8 12 See https://github.com/facebookresearch/LASER See https://visl.sdu.dk/visl/da/tools/ 9 See http://opus.nlpl.eu/ 10 See www.clarin.eu/resource-families /parallel-corpora 13 14 See github.com/fnielsen/awesome-danish for links. github.com/kaldi-asr/kaldi/tree/master/egs/sprakbanken. Offline means it cannot recognise speech in real-time. https://www.dictus.dk 2010). This was used to create the Fisher corpus, a standard benchmark (Cieri et al., 2004). We recommend this approach, coupled with release of publicly-owned parallel data (e.g. subtitles & audio from Danmarks Radio archives; Danish parliament speeches with transcriptions). 5.2 Speech synthesis. The synthesisers available online are eSpeak and Responsive Voice.15 Spr˚akbanken contains a section of data that can be used to train a speech synthesiser. Recently, toolkits to train DNNbased speech synthesisers have become available online16 because they can be trained on aligned speech and text data like ASR systems, but we are not aware of any systems or recipes to train Danish speech"
W19-6141,W19-6138,1,0.835945,"Missing"
W19-6141,W17-4418,1,0.700825,"Missing"
W19-6141,E14-2016,1,0.813569,"thers, the technology is less advanced. It is a more coarse-grained task than sense tagging which has received attention in Danish (Alonso et al., 2015; Pedersen et al., 2015). Many NER results for Danish are outdated and based on closed, systems. E.g., Bick (2004) offers details of a system trained on 43K tokens but reports no F1. One has to pay for this tool and the data is not open. Johannessen et al. (2005) mention efforts in Danish NER but the research lies behind a paywall that the authors do not have access through, and we failed to find other artefacts of this research. More recently, Derczynski et al. (2014) describe a dataset used to train a recognizer that is openly available in GATE (Cunningham et al., 2012). Current efforts focus on addressing the problem of data sparsity and on providing accessible tools (Plank, 2019; Derczynski, 2019), including as part of the ITU Copenhagen open tool set for Danish NLP.5 In contrast, for English, F1 scores are in the mid-90s (e.g., 94.03 from Chiu and Nichols (2016)). Researchers have since moved on to more exotic challenges, such as nested entities, emerging entities, and clinical information extraction (Katiyar and Cardie, 2018; Derczynski et al., 2017;"
W19-6141,W14-2602,1,0.834,"xt, which presents its own hurdles (Balahur and Jacquet, 2015), leading to a series of shared tasks (Rosenthal et al., 2015). The afinn tool (Nielsen, 2011) performs sentiment analysis using a lexicon consisting of 3552 words, labelled with a value between -5 (very negative) and 5 (very positive). This approach only considers the individual words in the input, and therefore the context is lost. Full-text annotations for sentiment in Danish have appeared in previous multilingual work, including systems reaching F-scores of 0.924 on same-domain Trustpilot reviews and 0.462 going across domains (Elming et al., 2014). Alexandra Institute offer a model6 based on Facebook’s 6 See https://github.com/alexandrainst/danlp LASER multilingual sentiment tool.7 This is total of Danish sentiment text tools, and all are included incidentally as part of multilingual efforts. 4 Machine Translation Machine translation (MT) is the automatic translation from one language to another. MT typically thrives on sentence-aligned data, where sentences in the source language are paired with their translation in the target language. Tools specifically designed in Denmark for Danish are not open and often only translate one way;8 t"
W19-6141,C16-1084,0,0.0503077,"Missing"
W19-6141,N18-1079,0,0.0186264,"s research. More recently, Derczynski et al. (2014) describe a dataset used to train a recognizer that is openly available in GATE (Cunningham et al., 2012). Current efforts focus on addressing the problem of data sparsity and on providing accessible tools (Plank, 2019; Derczynski, 2019), including as part of the ITU Copenhagen open tool set for Danish NLP.5 In contrast, for English, F1 scores are in the mid-90s (e.g., 94.03 from Chiu and Nichols (2016)). Researchers have since moved on to more exotic challenges, such as nested entities, emerging entities, and clinical information extraction (Katiyar and Cardie, 2018; Derczynski et al., 2017; Wang et al., 2018). To improve Danish NER seems simple: we need open tools and annotated data. Fortunately, the landscape for Danish NER is somewhat barren, and so first movers have an advantage. Openly contributing such a dataset to a shared resource would mean that Danish NER would be included in multilingual NER exercises, thus enabling the rest of the world to also work on improving entity recognition for Danish. Danish clinical NLP lags behind that for other languages, even when English is taken out of the picture, with for example four times as many Pubmed refe"
W19-6141,L16-1262,0,0.0310059,"Missing"
W19-6141,N10-1024,0,0.0506236,"Missing"
W19-6141,P16-2067,1,0.864069,"Missing"
W19-6141,S15-2078,0,0.0151706,"bly be able to use belles lettres in English to better process belles lettres in Danish, the idiosyncracies in clinical notes mean that one language’s clinical note data is unlikely to hugely help understanding clinical notes in other languages. 5 See nlp.itu.dk/resources/ and github.com/ITUnlp Sentiment Extraction Sentiment analysis is a long-standing NLP task for predicting the sentiment of an utterance, in general or related to a target (Liu, 2012). It has been investigated for non-formal text, which presents its own hurdles (Balahur and Jacquet, 2015), leading to a series of shared tasks (Rosenthal et al., 2015). The afinn tool (Nielsen, 2011) performs sentiment analysis using a lexicon consisting of 3552 words, labelled with a value between -5 (very negative) and 5 (very positive). This approach only considers the individual words in the input, and therefore the context is lost. Full-text annotations for sentiment in Danish have appeared in previous multilingual work, including systems reaching F-scores of 0.924 on same-domain Trustpilot reviews and 0.462 going across domains (Elming et al., 2014). Alexandra Institute offer a model6 based on Facebook’s 6 See https://github.com/alexandrainst/danlp LA"
W19-6141,L16-1314,0,0.0127276,"nre is important for acoustic model performance, so language models trained on newswire, Wikipedia, Twitter data or similar will not work as well as language models trained on speech transcriptions. Dictus Sun has access to 11 years of transcribed speeches and so may work well for monologues in that domain, but we have not been able to test the system and cannot know its performance on spontaneous speech. A lot of medium quality transcribed data is better than a little perfectly transcribed data and creating more data rather than correcting existing transcriptions provides better performance (Sperber et al., 2016; Novotney and Callison-Burch, 7 11 8 12 See https://github.com/facebookresearch/LASER See https://visl.sdu.dk/visl/da/tools/ 9 See http://opus.nlpl.eu/ 10 See www.clarin.eu/resource-families /parallel-corpora 13 14 See github.com/fnielsen/awesome-danish for links. github.com/kaldi-asr/kaldi/tree/master/egs/sprakbanken. Offline means it cannot recognise speech in real-time. https://www.dictus.dk 2010). This was used to create the Fisher corpus, a standard benchmark (Cieri et al., 2004). We recommend this approach, coupled with release of publicly-owned parallel data (e.g. subtitles & audio fro"
W19-6141,D18-1334,0,0.0223992,", where sentences in the source language are paired with their translation in the target language. Tools specifically designed in Denmark for Danish are not open and often only translate one way;8 this makes them impossible to benchmark. On the other hand, it is rare that translation tools include Danish in evaluations. Popular pairs are en-fr, en-de, en-zh and en-ja, which tend to be present in most large-scale research exercises (Johnson et al., 2017; Chen et al., 2018). When Danish does appear, it is typically in order to make a linguistic point, rather than improve MT for Danish-speakers (Vanmassenhove et al., 2018). However, even given that, there is a relatively large amount of Danish parallel text (that MT relies on): Opus9 reports 63M sentences for English-Danish, 70M for English-Swedish, 117M for English-German, and 242M for EnglishFrench. A large amount of the Danish data comes from colloquial, crowdsourced sites like OpenSubtitles.net and Tatoeba. Just as it’s incidental that Danish is included in these (i.e. their translations is not purpose-created for Danish, which is a signal of quality), there are also no dedicated Danish parallel texts listed on CLARIN.eu.10 The result is thus that Danish MT"
W19-6141,K18-2001,0,0.0230321,"Missing"
