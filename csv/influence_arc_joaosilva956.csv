2020.lrec-1.106,barreto-etal-2006-open,1,0.649652,"tokens, which only partly coincide with the texts in BDCam˜oes (6 texts, with about 159,000 words). LT Corpus—Corpus de Textos Liter´arios (G´en´ereux et al., 2012) is a literary corpus containing 70 documents published between the mid-19th century and the 1940’s of the 20th century. While similar in design to and complementing BDCam˜oes, it covers a shorter time span, has less variety of genres, fewer authors, and is smaller in size, with 1.8 million words, which only partly coincide with the texts in BDCam˜oes (23 texts, with about 897,000 words). CINTIL—Corpus Internacional do Portuguˆes (Barreto et al., 2006) is a linguistically interpreted corpus with 1 million tokens, mostly from anonymized excerpts of news articles, but also including some works of fiction, and transcriptions of formal and informal speech. It is annotated with a variety of manually verified linguistic information, including morphological information and part-of-speech tags. Its texts are only from a recent period and it lacks some metadata items, such as information on the author, that would be necessary for some types of studies. 3 http://beta.clul.ul.pt/teitok/cta/ 3. 3.1. Corpus description Documents gathering methodology Th"
2020.lrec-1.106,E06-2024,1,0.653703,"vailable in Figure 1: XML structure of a document in BDCam˜oes the Digital Library of Cam˜oes, making them available for various types of studies. Researchers interested in a particular set of authors, genre or time period will then be able to take the BDCam˜oes corpus as a resource in which the relevant documents may be found. 3.3. Metadata and linguistic annotation All the documents written in Modern Portuguese, or which are older but whose edition has been transcribed into that orthographic norm, have been automatically parsed with state-of-the-art language processing tools for Portuguese (Branco and Silva, 2006), and thus annotated with linguistic information that follows from the design of these tools and that can be found in detail in their guidelines and documentation (Branco et al., 2015). This subcorpus forms the BDCam˜oes Treebank with 4,495,379 tokens, of which 3,456,396 belong to the public domain part and 1,038,983 belong to restricted part of the corpus. The resulting linguistic annotation comprises part-ofspeech tags (e.g. PREP, ADV, etc.), morphology (lemmas for words from the open categories; gender and number for words from nominal categories; tense, aspect, person and number for verbs)"
2020.lrec-1.106,de-marneffe-etal-2014-universal,0,0.0296811,"Missing"
2020.lrec-1.598,2019.gwc-1.32,1,0.722388,"n was performed only for a small sample of the relations. Both these wordnets are very large, by virtue of the automatic processes used to gather their contents, but on the 4860 flip side they lack the thorough validation only granted by a manual process. Another wordnet for Portuguese, WordNet.PT (Marrafa et al., 2006), presents opposite characteristics. Developed at CLUL, the Center for Linguistics of the University of Lisbon, it has been manually built, but is reported to contain only 19,000 expressions. Besides this, this wordnet is not distributed for reuse. To the best of our knowledge (Branco et al., 2019; De Paiva et al., 2016), MWN.PT differs from other wordnets for Portuguese in that it is the only high quality, manually validated and cross-lingually integrated, large-scale wordnet available for this language. 3. Projection The wordnet being presented in this paper has been built according to the expand model along two major devlopment campaigns. A first construction phase took place in the context of the MultiWordNet project (Pianta et al., 2002), leading to an initial, small sized version that is part of the collection of wordnets that resulted from that project. That inaugural version co"
2020.lrec-1.598,C12-3044,0,0.0334482,"Missing"
2020.lrec-1.598,2016.gwc-1.12,0,0.0477374,"Missing"
2020.lrec-1.598,magnini-cavaglia-2000-integrating,0,0.184054,"Missing"
2020.lrec-1.622,D15-1075,0,0.020107,"out of the current paper. These reports may, however, be found at (NLX, 2019). GIST (Choi and Lee, 2018) was the best system on the ARCT, with 0.712 accuracy, 0.175 over the baseline. The reference paper described the development score, the use of a distributed score and the best hyper-parameters. The system consists of LSTMs neural networks and makes use of 10 Note that three metrics are not shown in the table, namely Infrastructure, Empirical run-time and Hyper-parameter search trials, as none of the systems reported them. transfer learning from the natural language inference corpora SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018). The source code was made available through a public code repository (GitHub). Although the supporting library requirements were described in the repository, the specific versions of the libraries were not reported. This made the reproduction difficult, due to conflicts with the Theano GPU library (Theano Development Team, 2016) which forced us to resort to a CPU-compatible library instead, with which we were able to run the system (though certainly taking much longer than it would have on a GPU). It took one person less than one working day to reproduce t"
2020.lrec-1.622,S18-1192,0,0.0460678,"Missing"
2020.lrec-1.622,P17-1152,0,0.017888,"hich we attribute to non-deterministic steps in the process, such as weight initialization. Although the reference paper lacked a description of the infrastructure, the number of trials, and the hyper-parameter bounds and choice method, we consider the GIST system to be reproducible, mainly due to the availability of the source code. BLCU NLP (Zhao et al., 2018) was the second-best system on ARCT, with 0.606 accuracy, 0.106 points below the first system. The reference paper reported the development score, the score distribution and the best hyper-parameters. The system is an ensemble of ESIM (Chen et al., 2017) models, which are enhanced LSTM networks that incorporate syntactic information. Source code was made available through a public code repository (GitHub). Although the source code contained a minor problem in its instructions (pointing to a different script for execution) no other problems occur regarding running the source code. The reference paper mentions the use of the best five models for an ensemble majority vote, however, it does not mention the total number of models from which the five best models were selected. We decided to run the system ten times and choose the best five models t"
2020.lrec-1.622,S18-1194,0,0.0225858,"n paper, 7 gave only a summary of the system and 1 system (Joker) did not provide any description. Only 5 participants made the source code available. Overall, the participating systems show little variability regarding the machine learning methods resorted to, given that all the systems were based on neural networks, except one classifier that used support-vector machines. As for the 7 GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018), ECNU (Tian et al., 2018), NLITrans (Niven and Kao, 2018), Joker, YNU Deep (Ding and Zhou, 2018), mingyan, ArcNet, UniMelb (Joshi et al., 2018), TRANSRW (Chen et al., 2018), lyb3b (Li and Zhou, 2018), SNU iDS (Kim et al., 2018), ArgEnsGRU, ITNLP-ARC (Liu et al., 2018), YNU-HPCC (Zhang et al., 2018), TakeLab (Brassard et al., 2018), HHU (Liebeck et al., 2018), Deepfinder, ART, RW2C and ztangfdu. Systems that lack a citation did not provide a reference paper. 8 If participants with extensive prior knowledge of the task are used, human accuracy jumps to 0.909, with 30 participants solving the task perfectly. 9 http://alt.qcri.org/semeval2018/index. php?id=papers underlying model, 14 systems used a long short-term memory (LSTM) model, 1 a gated recurrent unit (GRU)"
2020.lrec-1.622,S18-1122,0,0.277472,", the trial, the training and development sets with gold labels were made available. In the second phase, the test with the unlabeled test set instances was made available. Competing with the baseline provided by a naive 50-50 random classifier, 21 systems participated.7 The ranking and scores of the systems are presented in Table 2. An upper bound for performance was found by having humans solving the task for 10 instances. A set of 173 crowdsourced participants set human accuracy at 0.798.8 The naive baseline of 50-50 random choice achieved an accuracy of 0.527. The top three systems, GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018) and ECNU (Tian et al., 2018), achieved an accuracy of 0.712, 0.606 and 0.604, respectively. These results lead the ARCT organizers to conclude that the identification of warrants for arguments is a feasible NLP task. It is important to note that the organizers mentioned the existence of artifacts in the data set that could bias the classifiers, such as negation cues, and provided a solution to fix this problem on the existing corpus. However, these fixes were not implemented for ARCT, and their impact on the results was not fully realised until after the ARCT was"
2020.lrec-1.622,N19-1423,0,0.0865159,"18 Task 12, the Argument Reasoning Comprehension Task (ARCT) (Habernal et al., 2018b), where the top 3 systems, with very good, close to human performance, were included. In a nutshell, the task consists of a binary decision among two input candidate sentences of which only one is fit to be a premise in the input argument. We were able to reproduce most systems, though with varying degrees of difficulty, and provide a detailed report for each case. On the other hand, we take notice of a problem that was found by Niven and Kao (2019) in the original data set of ARCT when it was used with BERT (Devlin et al., 2019), and of spurious statistical cues that have been shown to bias the results obtained. As a cleaned ARCT data set that fixes this bias problem has been released, we also run the reproduced systems on this revised data set. Given the sharp drop in performance that resulted — to very modest scores below to naive random choice baseline —, the present paper leads to a reassessment of the state-ofthe-art for this task. While highlighting the importance of reproducing previous work, we believe it will help also to foster the revival of the ARCT task. 2. Related Work Scientific reproduction. The chall"
2020.lrec-1.622,S18-1189,0,0.0132018,"oduce From the 21 participating systems, 13 were accompanied with a description paper, 7 gave only a summary of the system and 1 system (Joker) did not provide any description. Only 5 participants made the source code available. Overall, the participating systems show little variability regarding the machine learning methods resorted to, given that all the systems were based on neural networks, except one classifier that used support-vector machines. As for the 7 GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018), ECNU (Tian et al., 2018), NLITrans (Niven and Kao, 2018), Joker, YNU Deep (Ding and Zhou, 2018), mingyan, ArcNet, UniMelb (Joshi et al., 2018), TRANSRW (Chen et al., 2018), lyb3b (Li and Zhou, 2018), SNU iDS (Kim et al., 2018), ArgEnsGRU, ITNLP-ARC (Liu et al., 2018), YNU-HPCC (Zhang et al., 2018), TakeLab (Brassard et al., 2018), HHU (Liebeck et al., 2018), Deepfinder, ART, RW2C and ztangfdu. Systems that lack a citation did not provide a reference paper. 8 If participants with extensive prior knowledge of the task are used, human accuracy jumps to 0.909, with 30 participants solving the task perfectly. 9 http://alt.qcri.org/semeval2018/index. php?id=papers underlying model, 14 systems"
2020.lrec-1.622,D19-1224,0,0.0803662,"de available; and (iii) subjective attributes such as the use of toy problems, paper readability and algorithm difficulty, among others. The attributes that showed a significant positive impact on raising the level of reproducibility were: existence of a formal proof, paper readability, algorithm difficulty, availability of pseudo-code, primary topic, whether the hyper-parameters are specified, amount of computation needed, whether the authors reply to questions, number of equations and number of tables. Machine Learning. Upon assessing the comparability of different Machine Learning systems, Dodge et al. (2019) claim that simply reporting the score on the test set, as it is common practice, is insufficient to deem some systems better than others. In order to address this issue, a reproducibility checklist (based on the NeurIPS Machine Learning Reproducibility Checklist3 ) is presented along with a metric to measure the expected validation performance. Through this checklist, it is possible to arrive at recommendations to enhance the level of reproducibility and replicability. Regarding the reproducibility of experimental results, it is recommended to include the description of the computing infrastr"
2020.lrec-1.622,P13-1166,0,0.40265,"Missing"
2020.lrec-1.622,N18-1175,0,0.372026,"co et al., 2017): Replication, the practice of independently implementing scientific experiments to validate specific findings, is the cornerstone of discovering scientific truth. Related to replication is reproducibility, which is the calculation of quantitative scientific results by independent scientists using the original data sets and methods. The goal of this paper is twofold. On the one hand, we report on the challenges faced and the insights gained from our endeavour in reproducing several systems submitted to the SemEval-2018 Task 12, the Argument Reasoning Comprehension Task (ARCT) (Habernal et al., 2018b), where the top 3 systems, with very good, close to human performance, were included. In a nutshell, the task consists of a binary decision among two input candidate sentences of which only one is fit to be a premise in the input argument. We were able to reproduce most systems, though with varying degrees of difficulty, and provide a detailed report for each case. On the other hand, we take notice of a problem that was found by Niven and Kao (2019) in the original data set of ARCT when it was used with BERT (Devlin et al., 2019), and of spurious statistical cues that have been shown to bias"
2020.lrec-1.622,S18-1121,0,0.234483,"co et al., 2017): Replication, the practice of independently implementing scientific experiments to validate specific findings, is the cornerstone of discovering scientific truth. Related to replication is reproducibility, which is the calculation of quantitative scientific results by independent scientists using the original data sets and methods. The goal of this paper is twofold. On the one hand, we report on the challenges faced and the insights gained from our endeavour in reproducing several systems submitted to the SemEval-2018 Task 12, the Argument Reasoning Comprehension Task (ARCT) (Habernal et al., 2018b), where the top 3 systems, with very good, close to human performance, were included. In a nutshell, the task consists of a binary decision among two input candidate sentences of which only one is fit to be a premise in the input argument. We were able to reproduce most systems, though with varying degrees of difficulty, and provide a detailed report for each case. On the other hand, we take notice of a problem that was found by Niven and Kao (2019) in the original data set of ARCT when it was used with BERT (Devlin et al., 2019), and of spurious statistical cues that have been shown to bias"
2020.lrec-1.622,S18-1190,0,0.0615341,"Missing"
2020.lrec-1.622,S18-1182,0,0.15718,"m (Joker) did not provide any description. Only 5 participants made the source code available. Overall, the participating systems show little variability regarding the machine learning methods resorted to, given that all the systems were based on neural networks, except one classifier that used support-vector machines. As for the 7 GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018), ECNU (Tian et al., 2018), NLITrans (Niven and Kao, 2018), Joker, YNU Deep (Ding and Zhou, 2018), mingyan, ArcNet, UniMelb (Joshi et al., 2018), TRANSRW (Chen et al., 2018), lyb3b (Li and Zhou, 2018), SNU iDS (Kim et al., 2018), ArgEnsGRU, ITNLP-ARC (Liu et al., 2018), YNU-HPCC (Zhang et al., 2018), TakeLab (Brassard et al., 2018), HHU (Liebeck et al., 2018), Deepfinder, ART, RW2C and ztangfdu. Systems that lack a citation did not provide a reference paper. 8 If participants with extensive prior knowledge of the task are used, human accuracy jumps to 0.909, with 30 participants solving the task perfectly. 9 http://alt.qcri.org/semeval2018/index. php?id=papers underlying model, 14 systems used a long short-term memory (LSTM) model, 1 a gated recurrent unit (GRU) model, 2 a convolutional neural network (CNN), and 1 a"
2020.lrec-1.622,S18-1193,0,0.019482,"ary of the system and 1 system (Joker) did not provide any description. Only 5 participants made the source code available. Overall, the participating systems show little variability regarding the machine learning methods resorted to, given that all the systems were based on neural networks, except one classifier that used support-vector machines. As for the 7 GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018), ECNU (Tian et al., 2018), NLITrans (Niven and Kao, 2018), Joker, YNU Deep (Ding and Zhou, 2018), mingyan, ArcNet, UniMelb (Joshi et al., 2018), TRANSRW (Chen et al., 2018), lyb3b (Li and Zhou, 2018), SNU iDS (Kim et al., 2018), ArgEnsGRU, ITNLP-ARC (Liu et al., 2018), YNU-HPCC (Zhang et al., 2018), TakeLab (Brassard et al., 2018), HHU (Liebeck et al., 2018), Deepfinder, ART, RW2C and ztangfdu. Systems that lack a citation did not provide a reference paper. 8 If participants with extensive prior knowledge of the task are used, human accuracy jumps to 0.909, with 30 participants solving the task perfectly. 9 http://alt.qcri.org/semeval2018/index. php?id=papers underlying model, 14 systems used a long short-term memory (LSTM) model, 1 a gated recurrent unit (GRU) model, 2 a convolutional ne"
2020.lrec-1.622,S18-1188,0,0.017024,"show little variability regarding the machine learning methods resorted to, given that all the systems were based on neural networks, except one classifier that used support-vector machines. As for the 7 GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018), ECNU (Tian et al., 2018), NLITrans (Niven and Kao, 2018), Joker, YNU Deep (Ding and Zhou, 2018), mingyan, ArcNet, UniMelb (Joshi et al., 2018), TRANSRW (Chen et al., 2018), lyb3b (Li and Zhou, 2018), SNU iDS (Kim et al., 2018), ArgEnsGRU, ITNLP-ARC (Liu et al., 2018), YNU-HPCC (Zhang et al., 2018), TakeLab (Brassard et al., 2018), HHU (Liebeck et al., 2018), Deepfinder, ART, RW2C and ztangfdu. Systems that lack a citation did not provide a reference paper. 8 If participants with extensive prior knowledge of the task are used, human accuracy jumps to 0.909, with 30 participants solving the task perfectly. 9 http://alt.qcri.org/semeval2018/index. php?id=papers underlying model, 14 systems used a long short-term memory (LSTM) model, 1 a gated recurrent unit (GRU) model, 2 a convolutional neural network (CNN), and 1 a supportvector machine (SVM) classifier. A total of 12 systems report using pre-trained distributional semantic models, the majority o"
2020.lrec-1.622,S18-1183,0,0.0142222,". Only 5 participants made the source code available. Overall, the participating systems show little variability regarding the machine learning methods resorted to, given that all the systems were based on neural networks, except one classifier that used support-vector machines. As for the 7 GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018), ECNU (Tian et al., 2018), NLITrans (Niven and Kao, 2018), Joker, YNU Deep (Ding and Zhou, 2018), mingyan, ArcNet, UniMelb (Joshi et al., 2018), TRANSRW (Chen et al., 2018), lyb3b (Li and Zhou, 2018), SNU iDS (Kim et al., 2018), ArgEnsGRU, ITNLP-ARC (Liu et al., 2018), YNU-HPCC (Zhang et al., 2018), TakeLab (Brassard et al., 2018), HHU (Liebeck et al., 2018), Deepfinder, ART, RW2C and ztangfdu. Systems that lack a citation did not provide a reference paper. 8 If participants with extensive prior knowledge of the task are used, human accuracy jumps to 0.909, with 30 participants solving the task perfectly. 9 http://alt.qcri.org/semeval2018/index. php?id=papers underlying model, 14 systems used a long short-term memory (LSTM) model, 1 a gated recurrent unit (GRU) model, 2 a convolutional neural network (CNN), and 1 a supportvector machine (SVM) classifier. A"
2020.lrec-1.622,P14-5010,0,0.00247417,"od; and best: hyper-parameter settings for the best model. dependencies such as numpy, but these dependencies, and their precise versions, had to be determined by trial and error as the paper and source code documentation did not specify that information and using the most recent versions yielded errors. After several version regressions, we settled for a working Tensorflow (1.0.0) and Keras (2.2.4) version. The Python version was also not reported, thus we used 3.6.9 (the most recent stable version at the time of running our experiment). The system makes use of the Stanford CoreNLP pipeline (Manning et al., 2014) to parse its input but, again, the precise version is not specified and had to be determined through inspection of the source code. The system relies on pre-trained Word2Vec embeddings, but their source was not described so we assumed them to be the standard GoogleNews pre-trained vectors (Mikolov et al., 2013). The source code implements several models, but since the documentation does not specify which are used in the ensemble, these had to be determined by inspecting the source code.11 The experiments were ran on 2 Intel(R) Xeon(R) Gold 6152 CPU’s. We obtained a score of 0.583, 0.021 point"
2020.lrec-1.622,C18-1097,0,0.0195849,". Keywords: Reproduction, Replication, Argument Mining, Argument Reasoning, Argument Comprehension, ARCT, SemEval. 1. Introduction The ability to repeat experiments and to reproduce their results is a cornerstone of scientific work and it is necessary to properly validate the findings that are published. Several examples of failed replication efforts, however, have been documented in different scientific fields (Branco et al., 2017). For Natural Language Processing (NLP), failure to reproduce results has been reported for WordNet similarity measures (Fokkens et al., 2013), sentiment analysis (Moore and Rayson, 2018), PoS tagging and named entity recognition (Reimers and Gurevych, 2017), among others. The importance of open challenges that foster the reproduction of research results has been underlined (Fokkens et al., 2013; Branco, 2013) as part of the solution for the so-called replication crisis (Hutson, 2018). The community gathered around the science and technology of language has responded through initiatives such as the 4REAL workshops (Branco et al., 2017; Branco et al., 2018), and the forthcoming ReproLang cooperative shared task (ReproLang, 2019) of the LREC2020 conference. The discussion regard"
2020.lrec-1.622,S18-1185,0,0.137288,"ur system”. 4.1. Selecting what to reproduce From the 21 participating systems, 13 were accompanied with a description paper, 7 gave only a summary of the system and 1 system (Joker) did not provide any description. Only 5 participants made the source code available. Overall, the participating systems show little variability regarding the machine learning methods resorted to, given that all the systems were based on neural networks, except one classifier that used support-vector machines. As for the 7 GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018), ECNU (Tian et al., 2018), NLITrans (Niven and Kao, 2018), Joker, YNU Deep (Ding and Zhou, 2018), mingyan, ArcNet, UniMelb (Joshi et al., 2018), TRANSRW (Chen et al., 2018), lyb3b (Li and Zhou, 2018), SNU iDS (Kim et al., 2018), ArgEnsGRU, ITNLP-ARC (Liu et al., 2018), YNU-HPCC (Zhang et al., 2018), TakeLab (Brassard et al., 2018), HHU (Liebeck et al., 2018), Deepfinder, ART, RW2C and ztangfdu. Systems that lack a citation did not provide a reference paper. 8 If participants with extensive prior knowledge of the task are used, human accuracy jumps to 0.909, with 30 participants solving the task perfectly. 9 http://alt.qcri.org/semeval2018/index. php"
2020.lrec-1.622,P19-1459,0,0.125652,"ined from our endeavour in reproducing several systems submitted to the SemEval-2018 Task 12, the Argument Reasoning Comprehension Task (ARCT) (Habernal et al., 2018b), where the top 3 systems, with very good, close to human performance, were included. In a nutshell, the task consists of a binary decision among two input candidate sentences of which only one is fit to be a premise in the input argument. We were able to reproduce most systems, though with varying degrees of difficulty, and provide a detailed report for each case. On the other hand, we take notice of a problem that was found by Niven and Kao (2019) in the original data set of ARCT when it was used with BERT (Devlin et al., 2019), and of spurious statistical cues that have been shown to bias the results obtained. As a cleaned ARCT data set that fixes this bias problem has been released, we also run the reproduced systems on this revised data set. Given the sharp drop in performance that resulted — to very modest scores below to naive random choice baseline —, the present paper leads to a reassessment of the state-ofthe-art for this task. While highlighting the importance of reproducing previous work, we believe it will help also to foste"
2020.lrec-1.622,D14-1162,0,0.0895797,"Missing"
2020.lrec-1.622,D17-1035,0,0.120817,"asoning, Argument Comprehension, ARCT, SemEval. 1. Introduction The ability to repeat experiments and to reproduce their results is a cornerstone of scientific work and it is necessary to properly validate the findings that are published. Several examples of failed replication efforts, however, have been documented in different scientific fields (Branco et al., 2017). For Natural Language Processing (NLP), failure to reproduce results has been reported for WordNet similarity measures (Fokkens et al., 2013), sentiment analysis (Moore and Rayson, 2018), PoS tagging and named entity recognition (Reimers and Gurevych, 2017), among others. The importance of open challenges that foster the reproduction of research results has been underlined (Fokkens et al., 2013; Branco, 2013) as part of the solution for the so-called replication crisis (Hutson, 2018). The community gathered around the science and technology of language has responded through initiatives such as the 4REAL workshops (Branco et al., 2017; Branco et al., 2018), and the forthcoming ReproLang cooperative shared task (ReproLang, 2019) of the LREC2020 conference. The discussion regarding the reproduction of scientific experiments faces an additional, non"
2020.lrec-1.622,S18-1184,0,0.15679,"els were made available. In the second phase, the test with the unlabeled test set instances was made available. Competing with the baseline provided by a naive 50-50 random classifier, 21 systems participated.7 The ranking and scores of the systems are presented in Table 2. An upper bound for performance was found by having humans solving the task for 10 instances. A set of 173 crowdsourced participants set human accuracy at 0.798.8 The naive baseline of 50-50 random choice achieved an accuracy of 0.527. The top three systems, GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018) and ECNU (Tian et al., 2018), achieved an accuracy of 0.712, 0.606 and 0.604, respectively. These results lead the ARCT organizers to conclude that the identification of warrants for arguments is a feasible NLP task. It is important to note that the organizers mentioned the existence of artifacts in the data set that could bias the classifiers, such as negation cues, and provided a solution to fix this problem on the existing corpus. However, these fixes were not implemented for ARCT, and their impact on the results was not fully realised until after the ARCT was concluded. We will come back to this issue in Section 5. 4"
2020.lrec-1.622,N18-1101,0,0.167395,"ports may, however, be found at (NLX, 2019). GIST (Choi and Lee, 2018) was the best system on the ARCT, with 0.712 accuracy, 0.175 over the baseline. The reference paper described the development score, the use of a distributed score and the best hyper-parameters. The system consists of LSTMs neural networks and makes use of 10 Note that three metrics are not shown in the table, namely Infrastructure, Empirical run-time and Hyper-parameter search trials, as none of the systems reported them. transfer learning from the natural language inference corpora SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018). The source code was made available through a public code repository (GitHub). Although the supporting library requirements were described in the repository, the specific versions of the libraries were not reported. This made the reproduction difficult, due to conflicts with the Theano GPU library (Theano Development Team, 2016) which forced us to resort to a CPU-compatible library instead, with which we were able to run the system (though certainly taking much longer than it would have on a GPU). It took one person less than one working day to reproduce this system. Running the system with t"
2020.lrec-1.622,S18-1040,0,0.0126281,"e source code available. Overall, the participating systems show little variability regarding the machine learning methods resorted to, given that all the systems were based on neural networks, except one classifier that used support-vector machines. As for the 7 GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018), ECNU (Tian et al., 2018), NLITrans (Niven and Kao, 2018), Joker, YNU Deep (Ding and Zhou, 2018), mingyan, ArcNet, UniMelb (Joshi et al., 2018), TRANSRW (Chen et al., 2018), lyb3b (Li and Zhou, 2018), SNU iDS (Kim et al., 2018), ArgEnsGRU, ITNLP-ARC (Liu et al., 2018), YNU-HPCC (Zhang et al., 2018), TakeLab (Brassard et al., 2018), HHU (Liebeck et al., 2018), Deepfinder, ART, RW2C and ztangfdu. Systems that lack a citation did not provide a reference paper. 8 If participants with extensive prior knowledge of the task are used, human accuracy jumps to 0.909, with 30 participants solving the task perfectly. 9 http://alt.qcri.org/semeval2018/index. php?id=papers underlying model, 14 systems used a long short-term memory (LSTM) model, 1 a gated recurrent unit (GRU) model, 2 a convolutional neural network (CNN), and 1 a supportvector machine (SVM) classifier. A total of 12 systems report usi"
2020.lrec-1.622,S18-1186,0,0.039955,"Missing"
2020.lrec-1.680,P18-2005,0,0.0604696,"Missing"
2020.lrec-1.680,P18-3010,0,0.0204747,"Missing"
2020.lrec-1.680,P18-1246,0,0.0988842,"Missing"
2020.lrec-1.680,D16-1034,0,0.0641556,"Missing"
2020.lrec-1.680,P18-1197,0,0.0391306,"Missing"
2020.lrec-1.680,P18-1031,0,0.0492424,"Missing"
2020.lrec-1.680,W18-3026,0,0.0278988,"Missing"
2020.lrec-1.680,P17-2014,0,0.0241643,"Missing"
2020.lrec-1.680,S18-1112,0,0.196409,"Missing"
2020.lrec-1.680,W18-0515,0,0.0590939,"Missing"
2021.emnlp-main.113,2020.acl-main.703,0,0.486964,"s 1504–1521 c November 7–11, 2021. 2021 Association for Computational Linguistics lect four prominent commonsense reasoning tasks, namely Argument Reasoning Comprehension Task (ARCT) (Habernal et al., 2018), AI2 Reasoning Challenge (ARC) (Clark et al., 2018), Physical IQA (PIQA) (Bisk et al., 2020), and CommonsenseQA (CSQA) (Talmor et al., 2019)). We adopt the successful Transformer architecture and resort to prominent pre-trained language models: the encoder-only RoBERTa (Liu et al., 2019b), the decoder-only GPT-2 (Radford et al., 2019), the encoder-decoder T5 (Raffel et al., 2020) and BART (Lewis et al., 2020), and the neurosymbolic COMET(BART) (Hwang et al., 2021)). And we set on to gain insight with stress experiments that seek to find evidence that hopefully permits to advance in answering questions like the following: Are the models taking into account the actual input as a whole? Or could the models be only looking at certain parts of the input, therefore not performing the underlying task but some derivative. How robust are the models? Can they withstand adversarial attacks on the basis of powerful generalization they gained during training, or are they brittle and crumble under attack? How w"
2021.emnlp-main.113,2021.ccl-1.108,0,0.0825837,"Missing"
2021.emnlp-main.113,2020.findings-emnlp.341,0,0.02943,"Missing"
2021.emnlp-main.113,2020.emnlp-demos.16,0,0.0987421,"Missing"
2021.emnlp-main.113,P19-1459,0,0.280322,"tate-of-the-art NLP deep learnto the problem at stake or just taking advaning models genuinely grasp the underlying tasks tage of incidental shortcuts in the data items. they are handling is a debated topic. Recently, an The results obtained indicate that most increasing number of published experiments indidatasets experimented with are problematic, cate that models may be latching at spurious cues with models resorting to non-robust features present in the data (Zech et al., 2018; Geirhos et al., and appearing not to be learning and generalizing towards the overall tasks intended to be 2020; Niven and Kao, 2019), implying that they conveyed or exemplified by the datasets. will severely lack generalization capacity when presented with out-of-distribution data. 1 Introduction As they become more refined, pre-trained language models are continuously closing the gap to Reasoning helps humans to cope with their experience, whether in a complex situation such as de- humans in commonsense reasoning tasks (Zhou vising a plan to solve a pandemic or in a simple, et al., 2020; Tamborrino et al., 2020; Lourie et al., intuitive inference like what will happen to a cof- 2021). In light of the skepticism about the"
2021.emnlp-main.113,S18-2023,0,0.0581942,"Missing"
barreto-etal-2006-open,A00-1031,0,\N,Missing
barreto-etal-2006-open,branco-silva-2004-evaluating,1,\N,Missing
branco-etal-2008-lx,branco-silva-2004-evaluating,1,\N,Missing
branco-etal-2010-developing,W00-1908,0,\N,Missing
branco-etal-2010-developing,J93-2004,0,\N,Missing
branco-etal-2010-developing,brants-2000-inter,0,\N,Missing
branco-etal-2010-developing,C02-2025,0,\N,Missing
branco-etal-2010-developing,P06-4017,0,\N,Missing
branco-etal-2010-developing,W08-2224,1,\N,Missing
branco-etal-2010-developing,J08-4004,0,\N,Missing
branco-etal-2010-developing,J05-1004,0,\N,Missing
branco-etal-2010-developing,simov-etal-2002-building,0,\N,Missing
branco-etal-2012-propbank,J93-2004,0,\N,Missing
branco-etal-2012-propbank,S07-1018,0,\N,Missing
branco-etal-2012-propbank,W02-1502,0,\N,Missing
branco-etal-2012-propbank,W05-0620,0,\N,Missing
branco-etal-2012-propbank,J05-1004,0,\N,Missing
E06-2024,A00-1031,0,0.0895081,"Missing"
E06-2024,C90-3030,0,0.0434006,"Missing"
E06-2024,branco-silva-2004-evaluating,1,\N,Missing
L16-1246,ballesteros-nivre-2012-maltoptimizer-system,0,0.0495771,"Missing"
L16-1246,barreto-etal-2006-open,1,0.760318,"Missing"
L16-1246,W06-2920,0,0.120533,"Missing"
L16-1246,de-marneffe-etal-2006-generating,0,0.156842,"Missing"
L16-1246,de-marneffe-etal-2014-universal,0,0.0285395,"Missing"
L16-1246,E06-1011,0,0.0407046,"ices and Parsing Dependency banks usually represent the dependency structure of a sentence through a tree whose root is the main verb of the sentence, although there are some phenomena whose linguistic analysis might be best represented through a graph structure that is not a tree, like relative structures or the coordination with shared dependents, which is represented using multiple heads. The reason why dependency banks are usually restricted to tree structures is not linguistic, but algorithmic, since finding a parse, if such non-tree structures are allowed, can be an intractable problem (McDonald and Pereira, 2006). Though there has been some research done on extending 1554 Figure 3: A complex predicate: Deveriam ficar em aquele pedestal em o qual foram postas sem nos deixarem aproximar. / They should stay in that pedestal in which they were placed on without letting us approach. Figure 4: Emphatic duplication: Eles v˜ao l´a amanh˜a a o quartel. / Tomorrow they will go there to the barracks. parsing algorithms to handle these more complex structures (McDonald and Pereira, 2006), the available dependency parsers cannot handle them. As such, these sentences are removed from the corpus before training and"
L16-1246,H05-1066,0,0.0238118,"Missing"
L16-1246,petrov-etal-2012-universal,0,0.0360236,"Missing"
L16-1246,P13-4001,0,0.0435072,"Missing"
L16-1246,zeman-2008-reusable,0,0.049108,"Missing"
L16-1330,S15-1002,0,0.0120685,"nsor Network(Socher et al., 2013b). This Recursive NN uses layers of a tensor V to compose each dimension of the parent vector. 3.3. 5 C C X X 1 W( wt−i + wt+i ) 2C i=1 i=1 Recurrent Neural Network Recurrent NNs (Figure 3), just like the Recursive NN, can handle variable sized inputs. The idea behind this kind of 2082 network is to keep some kind of memory of previous results as features for the next input. And just like the Recursive NN, this NN is a good way to capture semantic aspects of sentences. This kind of network has great results with language models(Mikolov, 2012), compositionality(Le and Zuidema, 2015) and more. The simplest case for Recurrent NN is when the recurrence function is: ht = σ(Wh ht−1 + Wx xt + b) where Wh and Wx are weight matrices, ht−1 is the result of the previous recurrence and xt is the current input. Other more complicated recurrence functions exists, the Long Short Term Memory(Hochreiter and Schmidhuber, 1997) being a particularly popular choice. 4. NNBlocks Two things that all of models presented above have in common are: (a) they have some very different aspects from common Multilayer Perceptrons and (b) they all have a lot of architectural variations depending on the"
L16-1330,D12-1110,0,0.0177716,"ing the tree structure. This kind of NN has great results in compositionality tasks. The simplest Recursive NN is the one where a parent vector p is   c p = σ(W 1 + b) (1) c2 where c1 and c2 are child vectors, W is a weight matrix, b is a bias vector and σ is an activation function, e.g. the sigmoid function. The result of this recursive process, the last parent vector, can then be used as a semantic feature of the sentence. These kind of compositional vectors have great results in parsing(Socher et al., 2013a), paraphrase detection(Socher et al., 2011), entities relationship classification(Socher et al., 2012), sentiment detection(Socher et al., 2013b) and more. Normally, the simple composition function in (1) is not enough to fully capture semantic relationships between words or phrases. One Recursive NN that tries to overcome this is the Recursive Neural Tensor Network(Socher et al., 2013b). This Recursive NN uses layers of a tensor V to compose each dimension of the parent vector. 3.3. 5 C C X X 1 W( wt−i + wt+i ) 2C i=1 i=1 Recurrent Neural Network Recurrent NNs (Figure 3), just like the Recursive NN, can handle variable sized inputs. The idea behind this kind of 2082 network is to keep some ki"
L16-1330,P13-1045,0,0.0134537,"ees of sentences and word vectors as leafs. The model recursively applies a composition function following the tree structure. This kind of NN has great results in compositionality tasks. The simplest Recursive NN is the one where a parent vector p is   c p = σ(W 1 + b) (1) c2 where c1 and c2 are child vectors, W is a weight matrix, b is a bias vector and σ is an activation function, e.g. the sigmoid function. The result of this recursive process, the last parent vector, can then be used as a semantic feature of the sentence. These kind of compositional vectors have great results in parsing(Socher et al., 2013a), paraphrase detection(Socher et al., 2011), entities relationship classification(Socher et al., 2012), sentiment detection(Socher et al., 2013b) and more. Normally, the simple composition function in (1) is not enough to fully capture semantic relationships between words or phrases. One Recursive NN that tries to overcome this is the Recursive Neural Tensor Network(Socher et al., 2013b). This Recursive NN uses layers of a tensor V to compose each dimension of the parent vector. 3.3. 5 C C X X 1 W( wt−i + wt+i ) 2C i=1 i=1 Recurrent Neural Network Recurrent NNs (Figure 3), just like the Recu"
L16-1330,D13-1170,0,0.00537085,"ees of sentences and word vectors as leafs. The model recursively applies a composition function following the tree structure. This kind of NN has great results in compositionality tasks. The simplest Recursive NN is the one where a parent vector p is   c p = σ(W 1 + b) (1) c2 where c1 and c2 are child vectors, W is a weight matrix, b is a bias vector and σ is an activation function, e.g. the sigmoid function. The result of this recursive process, the last parent vector, can then be used as a semantic feature of the sentence. These kind of compositional vectors have great results in parsing(Socher et al., 2013a), paraphrase detection(Socher et al., 2011), entities relationship classification(Socher et al., 2012), sentiment detection(Socher et al., 2013b) and more. Normally, the simple composition function in (1) is not enough to fully capture semantic relationships between words or phrases. One Recursive NN that tries to overcome this is the Recursive Neural Tensor Network(Socher et al., 2013b). This Recursive NN uses layers of a tensor V to compose each dimension of the parent vector. 3.3. 5 C C X X 1 W( wt−i + wt+i ) 2C i=1 i=1 Recurrent Neural Network Recurrent NNs (Figure 3), just like the Recu"
L16-1483,agerri-etal-2014-ixa,0,0.0139978,"us into a pivot language, English, and this into the remaining six languages. The current annotated corpus covers the first 2,000 sentences of the QTLeap corpus, which have been used to train the MT systems in the project. 3. Annotation Tools In this section, we describe the NERC, NED, WSD and coreference tools used to annotate the corpora. We have chosen the tools based on their performance and their ease of use. We also describe the annotation formats. 3.1. Named-entity recognition and classification Basque, English and Spanish ixa-pipe-nerc is a multilingual NERC tagger, part of IXA pipes (Agerri et al., 2014). Every model has been trained with the averaged Perceptron algorithm as described in Collins (2002) and as implemented in Apache OpenNLP. The datasets used for training the models are the following: Egunkaria dataset for Basque, a combination of Ontonotes 4.0, CoNLL 2003 and MUC 7 for English, and CoNLL 2002 for Spanish. Bulgarian The Bulgarian NERC is a rule-based module. It uses a gazetteer with names categorized in four types: Person, Location, Organization, Other. The identification of new names is based on two factors – sure positions in the text and classifying contextual information, s"
L16-1483,E09-1005,1,0.857409,"ese extraction of DBpedia Spotlight on a local server, then takes an input text pre-processed with lemmas, Part of Speech tags and named entities using the LX-Suite (Branco and Silva, 2006) and converts it to the ’spotted’ format understood by Spotlight. This spotted input text is then disambiguated using DBpedia Spotlight, returning among other information links to existing Portuguese DBpedia resource pages for each named entity discovered. 3024 3.3. Word-sense disambiguation Basque, English and Spanish ixa-pipe-wsd-ukb is based on UKB, a collection of programs for performing graphbased WSD (Agirre and Soroa, 2009). It applies the socalled Personalized PageRank on a Lexical Knowledge Base (LKB) to rank the vertices of the LKB and thus perform disambiguation. WordNet 3.0 is the LKB used for this processing. Bulgarian The basic version of Bulgarian WSD is implemented on the assumption of one sense per discourse and bigram statistics. Czech Two different approaches were used for Czech WSD. The first approach based on the work of Duˇsek et al. (2015) focuses on verbal WSD. The second approach followed for the annotation is a straightforward way of achieving compatibility with English WordNet IDs. Since the"
L16-1483,barreto-etal-2006-open,1,0.822658,"Missing"
L16-1483,bojar-etal-2012-joy,1,0.897675,"Missing"
L16-1483,E06-2024,1,0.746535,"It offers the “disambiguate” and “candidates” service endpoints. The former takes the spotted text input and it returns the DBpedia resource page for each entity. The later is similar to disambiguate, but returns a ranked list of candidates. Portuguese The NED module for Portuguese, LX-NED, uses DBpedia Spotlight to find links to resources about entities identified in pre-processed input text. It creates a process to run a Portuguese extraction of DBpedia Spotlight on a local server, then takes an input text pre-processed with lemmas, Part of Speech tags and named entities using the LX-Suite (Branco and Silva, 2006) and converts it to the ’spotted’ format understood by Spotlight. This spotted input text is then disambiguated using DBpedia Spotlight, returning among other information links to existing Portuguese DBpedia resource pages for each named entity discovered. 3024 3.3. Word-sense disambiguation Basque, English and Spanish ixa-pipe-wsd-ukb is based on UKB, a collection of programs for performing graphbased WSD (Agirre and Soroa, 2009). It applies the socalled Personalized PageRank on a Lexical Knowledge Base (LKB) to rank the vertices of the LKB and thus perform disambiguation. WordNet 3.0 is the"
L16-1483,W02-2004,0,0.181331,"Missing"
L16-1483,W02-1001,0,0.104764,"covers the first 2,000 sentences of the QTLeap corpus, which have been used to train the MT systems in the project. 3. Annotation Tools In this section, we describe the NERC, NED, WSD and coreference tools used to annotate the corpora. We have chosen the tools based on their performance and their ease of use. We also describe the annotation formats. 3.1. Named-entity recognition and classification Basque, English and Spanish ixa-pipe-nerc is a multilingual NERC tagger, part of IXA pipes (Agerri et al., 2014). Every model has been trained with the averaged Perceptron algorithm as described in Collins (2002) and as implemented in Apache OpenNLP. The datasets used for training the models are the following: Egunkaria dataset for Basque, a combination of Ontonotes 4.0, CoNLL 2003 and MUC 7 for English, and CoNLL 2002 for Spanish. Bulgarian The Bulgarian NERC is a rule-based module. It uses a gazetteer with names categorized in four types: Person, Location, Organization, Other. The identification of new names is based on two factors – sure positions in the text and classifying contextual information, such as, titles for persons, types of geographical objects or organizations, etc. The disambiguation"
L16-1483,W15-2111,1,0.864612,"Missing"
L16-1483,hajic-etal-2012-announcing,1,0.885975,"Missing"
L16-1483,W03-1901,0,0.0627303,"n to comparing the cores and the morphological information (gender and number) of the two expressions. As such, we found it easier to directly implement equivalent tests in-code instead of having to feed the extracted features to the Weka J48 classifier proper. 3.5. Annotation formats Basque, Bulgarian, Czech, English and Spanish These corpora are annotated in the NAF format. The NAF format (Fokkens et al., 2014) is a linguistic annotation format designed for complex NLP pipelines that combines strengths of the Linguistic Annotation Framework (LAF) and the NLP Interchange Formats described by Ide and Romary (2003). Because of its layered extensible format, it can easily be incorporated in a variety of NLP modules that may require different linguistic information as their input. Portuguese The corpus for Portuguese is divided into 4 text files - the raw corpus, and one file for the output of each of the three tools used to process it (WSD, NED and coreference). For each of the three tools output is provided in a standoff annotation format, consisting of one token per line (ID of each token in a markable pair in the case of the coreference tool), the appropriate output element of the respective tools (wo"
L16-1483,2005.mtsummit-papers.11,0,0.506663,"ces, the less language-specific differences will remain between the representations of the meaning of the source and target texts. As a result, chances of success are expected to increase considerably by MT systems that are based on deeper semantic engineering approaches. Following this assumption, one of the approaches taken by the QTLeap project1 is to enrich MT training resources with lexico-semantic information. In this work, we present a solid effort to build multilingual parallel corpora annotated at multiple semantic levels. Our overall goal is to enrich two parallel corpora, Europarl (Koehn, 2005) and the QTLeap corpus (Agirre et al., 2015b), with token, lemma, part-of-speech (POS), namedentity recognition and classification (NERC), named-entity disambiguation (NED), word-sense disambiguation (WSD) and coreference for six languages covered in the QTLeap project, namely, Basque (EU), Bulgarian (BG), Czech (CS), English (EN), Portuguese (PT) and Spanish (ES). Specifically, this paper presents the first release of such corpora, which includes NERC, NED, WSD and coreferencelevel annotation for these six languages. Additionally, some languages have extra annotations, such as wikification (E"
L16-1483,W11-1902,0,0.0321161,"ce in Czech. English and Spanish ixa-pipe-coref is loosely based on the Stanford Multi Sieve Pass system (Lee et al., 2013). The system consists of a number of rule-based sieves. Each sieve pass is applied in a deterministic manner, reusing the information generated by the previous sieve and the mention processing. The order in which the sieves are applied favors a highest precision approach and aims at improving the recall with the subsequent application of each of the sieve passes. This is illustrated by the evaluation results of the CoNLL 2011 Coreference Evaluation task (Lee et al., 2013; Lee et al., 2011), in which the Stanford system obtained the best results. The results show a pattern which has also been shown in other results reported with other evaluation sets (Raghunathan et al., 2010), namely, the fact that a large part of the performance of the multi-pass sieve system is based on a set of significant sieves. Thus, this module so far focuses on a subset of sieves only, namely, Speaker Match, Exact Match, Precise Constructs, Strict Head Match and Pronoun Match (Lee et al., 2013). Portuguese For the Portuguese coreference tool, a decision tree classifier was experimented with. Given a pai"
L16-1483,J13-4004,0,0.0359801,"owed for the annotation is a straightforward way of achieving compatibility with English WordNet IDs. Since the Czech corpus contains the same sentences as the English corpus, the English WordNet ID annotation from this corpus is projected onto Czech words using GIZA++ word alignment. Portuguese The Portuguese WSD tool, LX-WSD, is also based on UKB. The LKB from which UKB returns word senses within the pipeline has been generated from an extraction of the Portuguese MultiWordNet6 . 3.4. Coreference Basque ixa-pipe-coref-eu is an adaptation of the Stanford Deterministic Coreference Resolution (Lee et al., 2013), which gives state-of-the art performance for English. The original system applies a succession of ten independent deterministic coreference models or sieves. During the adaptation process, firstly, a baseline system has been created which receives as input texts processed by Basque analysis tools and uses specifically adapted static lists to identify language dependent features like gender, animacy or number. Afterwards, improvements over the baseline system have been applied, adapting and replacing some of the original sieves (Soraluze et al., 2015), taking into account that morphosyntactic"
L16-1483,S07-1008,0,0.017093,"Missing"
L16-1483,S07-1006,0,0.117234,"Missing"
L16-1483,W11-1901,0,0.087359,"Missing"
L16-1483,D10-1048,0,0.0133306,"h sieve pass is applied in a deterministic manner, reusing the information generated by the previous sieve and the mention processing. The order in which the sieves are applied favors a highest precision approach and aims at improving the recall with the subsequent application of each of the sieve passes. This is illustrated by the evaluation results of the CoNLL 2011 Coreference Evaluation task (Lee et al., 2013; Lee et al., 2011), in which the Stanford system obtained the best results. The results show a pattern which has also been shown in other results reported with other evaluation sets (Raghunathan et al., 2010), namely, the fact that a large part of the performance of the multi-pass sieve system is based on a set of significant sieves. Thus, this module so far focuses on a subset of sieves only, namely, Speaker Match, Exact Match, Precise Constructs, Strict Head Match and Pronoun Match (Lee et al., 2013). Portuguese For the Portuguese coreference tool, a decision tree classifier was experimented with. Given a pair of expressions, the classifier returns a true or false value that indicates whether those expressions are coreferent. The classifier was trained over the Summit Corpus (Collovini et al., 2"
L16-1483,P14-5003,1,0.902631,"Missing"
L16-1483,tiedemann-2012-parallel,0,0.0371283,"with corresponding document IDs. Then, sentence boundaries were identified and aligned (for further collection and processing information, see Koehn (2005)). The Europarl corpus consists of monolingual data as well as bilingual parallel data with English as pivot language. In our effort, we have annotated the BG, CS, ES and PT parts of the corpus separately while the EN side of the ESEN language pair was used as pivot language to link all six languages. Given that Europarl does not include Basque, we annotated an alternative publicly available Basque-English parallel corpus, the GNOME corpus (Tiedemann, 2012), which includes GNOME localization files. 2.2. QTLeap corpus The QTLeap corpus consists of 4,000 pairs of questions and respective answers in the domain of IT troubleshooting http://hdl.handle.net/11234/1-1477 4 http://www.statmt.org/europarl/ 3023 for both hardware and software, distributed in four 1,000pair batches (Gaudio et al., 2016). This material was collected using a real-life, commercial online support service via chat. The QTLeap corpus is a unique resource in that it is a multilingual data set with parallel utterances in different languages (Basque, Bulgarian, Czech, Dutch, English"
L16-1483,E14-4045,0,0.0404676,"Missing"
L16-1483,M95-1005,0,0.280516,"Missing"
L18-1513,S12-1051,0,0.046219,"he performance of resolvers for segments of other types. Keywords: semantic text similarity, paraphrase detection, duplicate question detection 1. Introduction Semantic Text Similarity (STS) is a Natural Language Processing (NLP) task whereby a system, given two input text segments, assigns to them a similarity score in a discrete or continuous scale that ranges from representing total similarity—for semantically equivalent segments—to representing total dissimilarity—for segments that are semantically independent. The STS task has been part of the SemEval competitive shared tasks since 2012 (Agirre et al., 2012), together with other challenges for a wide variety of other tasks, such as plagiarism detection, sentiment analysis or relation extraction, to name but a few. More recently, SemEval embraced STS challenges that concern more focused tasks, like paraphrase detection, which consists of a binary decision on whether two input sentences are paraphrases of each other and, starting in 2016, a task on Duplicate Question Detection (DQD) (Nakov et al., 2016). DQD appears as a special case of paraphrase detection, where the focus is on interrogative sentences: this task consists of a binary decision on w"
L18-1513,K15-1013,0,0.0166638,"question should be directed to the already existing question. Duplicate questions are manually flagged by the users, but this effort quickly becomes unwieldy as the site grows in popularity, driving the need for automatic procedures for DQD. Though the interest in DQD may be seen as relatively recent, there is an accumulated body of lessons learned about this task and the expected performance of systems tackling it, some of them being quite in line with what is known about data-driven approaches in general, while some others are more specific for this task. From existing work on DQD, such as (Bogdanova et al., 2015) (Rodrigues et al., 2017) and (Saedi et al., 2017), one learned that (i) training and evaluating over a specific domain with less data, rather than over a generic one with more data, will likely lead to better performance; (ii) training on as much data as possible, gathered from all different domains, and evaluating on a specific domain yields little more than random choice performance; (iii) when training on data sets of interrogative sentences, differences in the average length or in the level of grammaticality of sentences have little impact on performance; (iv) the differences in performan"
L18-1513,S16-1083,0,0.074292,"total dissimilarity—for segments that are semantically independent. The STS task has been part of the SemEval competitive shared tasks since 2012 (Agirre et al., 2012), together with other challenges for a wide variety of other tasks, such as plagiarism detection, sentiment analysis or relation extraction, to name but a few. More recently, SemEval embraced STS challenges that concern more focused tasks, like paraphrase detection, which consists of a binary decision on whether two input sentences are paraphrases of each other and, starting in 2016, a task on Duplicate Question Detection (DQD) (Nakov et al., 2016). DQD appears as a special case of paraphrase detection, where the focus is on interrogative sentences: this task consists of a binary decision on whether two input interrogatives sentences are a duplicate of each other. The motivation for the increasing interest in DQD, and the inclusion in SemEval of challenges dedicated to DQD, comes from the increasing popularity of on-line Community Question Answering (CQA) forums, such as Stack Exchange1 or Quora2 . These forums are quite open in allowing any user to post questions (and answer questions from other users) but from this arises a potential"
L18-1513,S17-1030,1,0.922196,"ed to the already existing question. Duplicate questions are manually flagged by the users, but this effort quickly becomes unwieldy as the site grows in popularity, driving the need for automatic procedures for DQD. Though the interest in DQD may be seen as relatively recent, there is an accumulated body of lessons learned about this task and the expected performance of systems tackling it, some of them being quite in line with what is known about data-driven approaches in general, while some others are more specific for this task. From existing work on DQD, such as (Bogdanova et al., 2015) (Rodrigues et al., 2017) and (Saedi et al., 2017), one learned that (i) training and evaluating over a specific domain with less data, rather than over a generic one with more data, will likely lead to better performance; (ii) training on as much data as possible, gathered from all different domains, and evaluating on a specific domain yields little more than random choice performance; (iii) when training on data sets of interrogative sentences, differences in the average length or in the level of grammaticality of sentences have little impact on performance; (iv) the differences in performance between the major type"
L18-1513,I11-1112,0,0.0280301,"ly competitive performance for DQD, in order to explain away a possible justification for the difference in performance between the two types of sentences based on the putative weakness of the methods used vis a vis interrogatives. In this paper we provide a short summary of each approach and direct the reader to the articles cited above for further information. Jaccard The Jaccard index is a straightforward statistic based on the count of common of n-grams between the two segments being compared. It is used as a simple baseline that previous work has shown to nonetheless be very competitive (Wu et al., 2011), especially for small sized data sets below 30,000 pairs (Saedi et al., 2017). All n-grams, with n ranging from 1 to 4, are used. SVM Support Vector Machine classifiers have been used with success in many NLP tasks and are able to cope with a great variety of features. The set of features used in this work is formed by (i) two vectors with the one-hot encodings of n-gram occurrences in each segment; (ii) the Jaccard index scores for 1, 2, 3 and 4grams; (iii) the counts of negative words (e.g. never, nothing, etc.) in each segment; (iv) the number of nouns that are common to both segments; and"
L18-1513,I05-5002,0,0.296278,"Missing"
L18-1513,S15-2001,0,0.0483771,"Missing"
L18-1722,P13-1133,0,0.0859157,"eton wordnet (Fellbaum, 1998), is publicly available to be reused. It is designed to browse through synsets and senses in any wordnet compatible with the Princeton format. However, like the previous example, it does not support connections among wordnets of different languages. • Visuwords (Critchfield, 2017) is an online graphical wordnet browser that employs colors and shapes to distinguish between synsets in various parts of speeches and types of semantic relations. Although very user friendly, this browser does not support multilingual wordnet browsing either. • Open Multilingual Wordnet (Bond and Foster, 2013) connects a large number of wordnets from different languages while using the English Princeton wordnet as the pivot one. The wordnets it resorts to have permissive licences for derivatives and redistribution and searching through the browser shows results in all their languages. However the source code of the browser itself is not available to be reused, and it is a browser that in any case offers no options to peruse wordnets on the basis of, direct or transitive, semantic relations. • Multi-WordNet (Pianta et al., 2002) is a well-known project aiming at aligning wordnets of different langua"
L18-1722,P09-4002,1,0.687014,"Missing"
L18-1722,elkateb-etal-2006-building,0,0.101522,"Missing"
L18-1722,finthammer-cramer-2008-exploring,0,0.0818219,"Missing"
L18-1722,isahara-etal-2008-development,0,0.0641716,"Missing"
L18-1722,C10-2097,0,0.0366018,"Missing"
L18-1722,Y11-1027,0,0.0349276,"Missing"
L18-1722,W09-3420,0,0.0585876,"Missing"
L18-1722,tufis-etal-2008-racais,0,0.0755107,"Missing"
L18-1722,W13-4302,0,0.0329857,"Missing"
P09-4002,A00-1031,0,0.0423582,"Missing"
P09-4002,branco-etal-2008-lx,1,\N,Missing
S17-1030,N13-1090,0,0.00847526,"Missing"
S17-1030,S16-1103,0,0.0247811,"Missing"
S17-1030,D16-1244,0,0.0446491,"Missing"
S17-1030,P06-4018,0,0.0108849,"Missing"
S17-1030,P15-1173,0,0.0244336,"Missing"
S17-1030,I11-1112,0,0.429916,"Missing"
silva-etal-2010-top,varadi-etal-2008-clarin,0,\N,Missing
silva-etal-2010-top,J03-4003,0,\N,Missing
silva-etal-2010-top,P06-1055,0,\N,Missing
silva-etal-2010-top,branco-etal-2008-lx,1,\N,Missing
silva-etal-2012-dealing,N03-2016,0,\N,Missing
silva-etal-2012-dealing,N01-1020,0,\N,Missing
silva-etal-2012-dealing,W02-0902,0,\N,Missing
silva-etal-2012-dealing,D07-1092,0,\N,Missing
silva-etal-2012-dealing,mulloni-pekar-2006-automatic,0,\N,Missing
W12-3409,W05-1008,0,0.0342651,"e features it uses, are described in Section 4. Section 5 covers empirical evaluation and comparison with other approaches. Finally, Section 6 concludes with some final remarks. 2 Background and Related Work The construction of a hand-crafted lexicon for a deep grammar is a time-consuming task requiring trained linguists. More importantly, such lexica are invariably incomplete since they often do not cover specialized domains and are slow to incorporate new words. Accordingly, much research in this area has been focused on automatic lexical acquisition (Brent, 1991; Briscoe and Carroll, 1997; Baldwin, 2005). That is, approaches that try to discover all the lexical types a given unknown word may occur with, thus effectively creating a new lexical entry. However, at run-time, it is still up to the grammar using the newly acquired lexical entry to choose which of those lexical types is the correct one for each particular occurrence of that word; and, ultimately, one can only acquire the lexicon entries for those words that are present in the corpus. Thus, any system that is constantly exposed to new text—e.g. parsing text from the Web—will eventually come across some unknown word that has not yet b"
W12-3409,C94-1024,0,0.107758,"Missing"
W12-3409,J99-2004,0,0.0857291,"Missing"
W12-3409,A00-1031,0,0.00867165,".5 The second feature tree, labeled with D as root, encodes the target word—again marked with an asterisk—and the word it is dependent on. In the example shown in Figure 2, since the target word is the main verb of the sentence, the feature tree has no other nodes apart from that of the target word. 5 Evaluation The following evaluation results were obtained following a standard 10-fold cross-validation approach, where the folds were taken from a random shuffle of the sentences in the corpus. We compare the performance of our tree kernel (TK) approach with two other automatic annotators, TnT (Brants, 2000) and SVMTool (Gim´enez and M`arquez, 2004). TnT is a statistical POS tagger, well known for its efficiency—in terms of training and tagging speed—and for achieving state-of-the-art results despite having a quite simple underlying 5 POS tags in the example: V (verb), PREP (preposition) and CN (common noun). C(de, viagem) ADV(dia, de) PRD(dia, segundo) SP(dia, o) PRD(golfinhos, primeiros) SP(golfinhos, os) C(a, dia) TMP(encontr´amos, a) DO(encontr´amos, golfinhos) Figure 1: Dependency representation D H * DO TMP * V CN PREP V encontr´amos we-found golfinhos dolphins a by encontr´amos we-found Fi"
W12-3409,P91-1027,0,0.0999595,"lassifier. The classifier itself, and the features it uses, are described in Section 4. Section 5 covers empirical evaluation and comparison with other approaches. Finally, Section 6 concludes with some final remarks. 2 Background and Related Work The construction of a hand-crafted lexicon for a deep grammar is a time-consuming task requiring trained linguists. More importantly, such lexica are invariably incomplete since they often do not cover specialized domains and are slow to incorporate new words. Accordingly, much research in this area has been focused on automatic lexical acquisition (Brent, 1991; Briscoe and Carroll, 1997; Baldwin, 2005). That is, approaches that try to discover all the lexical types a given unknown word may occur with, thus effectively creating a new lexical entry. However, at run-time, it is still up to the grammar using the newly acquired lexical entry to choose which of those lexical types is the correct one for each particular occurrence of that word; and, ultimately, one can only acquire the lexicon entries for those words that are present in the corpus. Thus, any system that is constantly exposed to new text—e.g. parsing text from the Web—will eventually come"
W12-3409,A97-1052,0,0.0583635,"e classifier itself, and the features it uses, are described in Section 4. Section 5 covers empirical evaluation and comparison with other approaches. Finally, Section 6 concludes with some final remarks. 2 Background and Related Work The construction of a hand-crafted lexicon for a deep grammar is a time-consuming task requiring trained linguists. More importantly, such lexica are invariably incomplete since they often do not cover specialized domains and are slow to incorporate new words. Accordingly, much research in this area has been focused on automatic lexical acquisition (Brent, 1991; Briscoe and Carroll, 1997; Baldwin, 2005). That is, approaches that try to discover all the lexical types a given unknown word may occur with, thus effectively creating a new lexical entry. However, at run-time, it is still up to the grammar using the newly acquired lexical entry to choose which of those lexical types is the correct one for each particular occurrence of that word; and, ultimately, one can only acquire the lexicon entries for those words that are present in the corpus. Thus, any system that is constantly exposed to new text—e.g. parsing text from the Web—will eventually come across some unknown word th"
W12-3409,J07-4004,0,0.0898872,"Missing"
W12-3409,P02-1034,0,0.14639,"Missing"
W12-3409,gimenez-marquez-2004-svmtool,0,0.0510017,"Missing"
W12-3409,E06-1015,0,0.0185769,"est.2 This grammar-supported approach to corpus annotation ensures that the various linguistic annotation layers—morphological, syntactic and semantic—are consistent. The corpus that was used is composed mostly by a subset of the sentences in CETEMP´ublico, a corpus of plain text excerpts from the P´ublico newspaper. After running LXGram and manually disambiguating the parse forests, we were left with a dataset consisting of 5,422 sentences annotated with all the linguistic information provided by LXGram. 4 Classifier and Feature Extraction For training and classification we use SVM-light-TK (Moschitti, 2006), an extension to the widely-used SVM-light (Joachims, 1999) software for SVMs that adds a function implementing the tree kernel introduced in Section 2.2. With SVM-light-TK one can 2 In our setup, two annotators work in a double-blind scheme, where those cases where they disagree are adjudicated by a third annotator. Inter-annotator agreement is 0.86. directly provide one or more tree structures as features (using the standard parenthesis representation of trees) together with the numeric feature vectors that are already accepted by SVM-light. Given that the task at stake is a multi-class cla"
W12-3409,H91-1067,0,\N,Missing
W15-0208,E09-1005,0,0.149443,"ll also be addressed. 1 Introduction Annotated corpora are a cornerstone of Natural Language Processing (NLP), supporting the analysis of large quantities of text across a wide variety of contexts (Leech, 2004) and the development and evaluation of processing tools. There has been an increased interest in “high quality linguistic annotations of corpora” at the semantic level, with word senses in particular being “both elusive and central to many areas of NLP” (Passonneau et al., 2012). Sense annotated corpora are useful, for example, as training data for Word Sense Disambiguation (WSD) tools (Agirre and Soroa, 2009), many of which are based on the Princeton WordNet approach to the lexical semantics of nouns, verbs, adjectives and adverbs (Fellbaum, 1998). This format is widely used to build sense-annotated corpora in a variety of languages—examples include parallel corpora such as the English/Italian MultiSemCor (Bentivogli and Pianta, 2002) and corpora in languages such as Japanese, Bulgarian, German, Polish and many more (Global WordNet Association, 2013). Despite the need for these corpora to train and test new and developing WSD approaches (Wu et al., 2007), tools for manual sense-annotation are not"
W15-0208,bentivogli-pianta-2002-opportunistic,0,0.0509746,"stic annotations of corpora” at the semantic level, with word senses in particular being “both elusive and central to many areas of NLP” (Passonneau et al., 2012). Sense annotated corpora are useful, for example, as training data for Word Sense Disambiguation (WSD) tools (Agirre and Soroa, 2009), many of which are based on the Princeton WordNet approach to the lexical semantics of nouns, verbs, adjectives and adverbs (Fellbaum, 1998). This format is widely used to build sense-annotated corpora in a variety of languages—examples include parallel corpora such as the English/Italian MultiSemCor (Bentivogli and Pianta, 2002) and corpora in languages such as Japanese, Bulgarian, German, Polish and many more (Global WordNet Association, 2013). Despite the need for these corpora to train and test new and developing WSD approaches (Wu et al., 2007), tools for manual sense-annotation are not easy to come by. Finding any information at all about such tools is difficult, and those that are described are often done so in the context of the specific purposes for which they were developed. For example, the tools used to manually annotate the English MASC Corpus (Passonneau et al., 2012) and Chinese Word Sense Annotated Cor"
W15-0208,E06-2024,1,0.761276,"on-style WordNet. We are using this tool to produce a gold-standard corpus annotated with senses from our Portuguese WordNet for use in our own WSD tasks, and in this paper describe how its usability and flexibility make it well-suited to similar manual annotation tasks using source texts and WordNet-based lexicons in a variety of different languages. 2 Importing Text The current implementation of LX-SenseAnnotator is designed for the import of text files that have already been tagged and morphologically analyzed (in particular, POS-tagged and lemmatized) in an existing pipeline of NLP tools (Branco and Silva, 2006). POS-tagging in particular makes the separation of the input text according to which words are and are not sense-taggable (as described in the next section) very straightforward. It is of course assumed that the preprocessed tags in the input text have been verified and are correct. A goal for LX-SenseAnnotator is to support the import of source text in a variety of different formats. The code that currently reads and interprets input text is stored in a stand-alone C++ function, making it easy for the tool to be tuned to allow texts pre-processed using different types of tagsets to be import"
W15-0208,P08-4004,0,0.0601329,"Missing"
W15-0208,passonneau-etal-2012-masc,0,0.0844132,"f LX-SenseAnnotator, including the support of a variety of languages and the handling of pre-processed texts with different tagsets, will also be addressed. 1 Introduction Annotated corpora are a cornerstone of Natural Language Processing (NLP), supporting the analysis of large quantities of text across a wide variety of contexts (Leech, 2004) and the development and evaluation of processing tools. There has been an increased interest in “high quality linguistic annotations of corpora” at the semantic level, with word senses in particular being “both elusive and central to many areas of NLP” (Passonneau et al., 2012). Sense annotated corpora are useful, for example, as training data for Word Sense Disambiguation (WSD) tools (Agirre and Soroa, 2009), many of which are based on the Princeton WordNet approach to the lexical semantics of nouns, verbs, adjectives and adverbs (Fellbaum, 1998). This format is widely used to build sense-annotated corpora in a variety of languages—examples include parallel corpora such as the English/Italian MultiSemCor (Bentivogli and Pianta, 2002) and corpora in languages such as Japanese, Bulgarian, German, Polish and many more (Global WordNet Association, 2013). Despite the ne"
W15-0208,W07-1521,0,0.0172516,"rd Sense Disambiguation (WSD) tools (Agirre and Soroa, 2009), many of which are based on the Princeton WordNet approach to the lexical semantics of nouns, verbs, adjectives and adverbs (Fellbaum, 1998). This format is widely used to build sense-annotated corpora in a variety of languages—examples include parallel corpora such as the English/Italian MultiSemCor (Bentivogli and Pianta, 2002) and corpora in languages such as Japanese, Bulgarian, German, Polish and many more (Global WordNet Association, 2013). Despite the need for these corpora to train and test new and developing WSD approaches (Wu et al., 2007), tools for manual sense-annotation are not easy to come by. Finding any information at all about such tools is difficult, and those that are described are often done so in the context of the specific purposes for which they were developed. For example, the tools used to manually annotate the English MASC Corpus (Passonneau et al., 2012) and Chinese Word Sense Annotated Corpus (Wu et al., 2007) both seem intrinsically tied to those particular corpora. Such examples demonstrate the need for a more open, flexible solution for manual word-sense annotation that is more “readily adaptable to differ"
W15-4101,E06-2024,1,0.776568,"Europarl (Koehn, 2005). Each pair of parallel sentences, one in English and one in Portuguese, are analyzed by Treex up to the t-layer level, where each pair of trees are fed into the model. Analysis Analysis proceeds in two stages. The first stage is a shallow syntactic analysis that takes us from the surface string to what in the Treex framework is called the a-layer (analytical layer), which is a grammatical dependency graph. The second stage is a deep syntactic analysis that takes us from the a-layer to the t-layer (tectogrammatical layer). 2.1.1 Getting the a-layer We resort to LX-Suite (Branco and Silva, 2006), a set of pre-existing shallow processing tools for Portuguese that include a sentence segmenter, a tokenizer, a POS tagger, a morphological analyser and a dependency parser, all with state-of-the-art performance. Treex blocks were created to call and interface with these tools. After running the shallow processing tools, the dependency output of the parser is converted into Universal Dependencies (UD, (de Marneffe et al., 2014)). These dependencies are then converted into the a-layer tree (a-tree) in a second step. Both steps are implemented as rule-based Treex blocks. Taking this two-tiered"
W15-4101,de-marneffe-etal-2014-universal,0,0.0825014,"Missing"
W15-4101,2005.mtsummit-papers.11,0,0.0109181,"pro-dropped pronouns.1 2.2 Transfer Transfer is handled by a tree-to-tree maximum entropy translation model (Mareˇcek et al., 2010) working at the deep syntactic level of Tecto trees. This transfer model assumes that the source and target trees are isomorphic. This limitation is rarely a problem since at the Tecto level, as one would expect from a deep syntactic representation, the source and target trees are often isomorphic. Since the trees are isomorphic, the model is concerned only with learning mappings between t-tree nodes. The model was trained over 1.9 million sentences from Europarl (Koehn, 2005). Each pair of parallel sentences, one in English and one in Portuguese, are analyzed by Treex up to the t-layer level, where each pair of trees are fed into the model. Analysis Analysis proceeds in two stages. The first stage is a shallow syntactic analysis that takes us from the surface string to what in the Treex framework is called the a-layer (analytical layer), which is a grammatical dependency graph. The second stage is a deep syntactic analysis that takes us from the a-layer to the t-layer (tectogrammatical layer). 2.1.1 Getting the a-layer We resort to LX-Suite (Branco and Silva, 2006"
W15-4101,W10-1730,0,0.561457,"Missing"
W15-4101,W08-0325,0,0.305184,"as hierarchical methods and tree-to-tree mappings, but these methods have been unable to clearly improve on the phrase-based state-of-the-art. There is a growing opinion that the previous approaches to SMT may be reaching a performance ceiling and that pushing beyond it will require approaches that are more linguistically informed and that are able to bring semantics into the process. 2 Translation pipeline Our pipeline is built upon the Treex system (Popel ˇ and Zabokrtsk´ y, 2010), a modular NLP framework used mostly for MT and the most recent ˇ incarnation of the TectoMT system (Zabokrtsk´ y et al., 2008). Treex uses an analysis-transfersynthesis architecture, with transfer being done at the deep syntactic level, where a Tectogrammatical (Tecto) formal description is used. 1 Proceedings of the ACL 2015 Fourth Workshop on Hybrid Approaches to Translation (HyTra), pages 1–5, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics 2.1.2 The choice of Treex as the supporting framework was motivated by several reasons. Firstly, Treex is a tried and tested framework that has been shown to achieve very good results in English to Czech translation, on a par with phrasebased SMT"
W15-4101,P07-2045,0,\N,Missing
W15-4101,W13-2201,0,\N,Missing
W15-5503,agirre-soroa-2008-using,0,0.0220661,"using the smaller, languagespecific Portuguese MultiWordNet (MultiWordNet, nd) as the underlying lexical knowledge base (LKB) for the WSD, and 2) translating open-class words in the input text from Portuguese to English in order to run WSD using the much larger English WordNet as the underlying LKB. The contributions from our results are twofold: Progress in knowledge-based WSD has largely been driven by the development of graph-based disambiguation methods, as pioneered by a number of researchers (Navigli and Velardi, 2005; Mihalcea, 2005; Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2008). Graph-based methods allow LKBs such as WordNets to be represented as weighted graphs, where word senses correspond to nodes and the relationships or dependencies between pairs of senses correspond to the edges between nodes. The strength of the edge between two nodes, corresponding to the relationship or dependency between two synsets, can then be calculated using semantic similarity measures such as the Lesk algorithm (Lesk, 1986). • Performing graph-based WSD using a smaller, language-specific LKB (Portuguese MultiWordNet) provides better results than translating terms to English in order"
W15-5503,E09-1005,0,0.237514,"e some related work (Section 2), before describing an implementation of graphbased WSD for Portuguese (Section 3). Next, we present our evaluation of the two approaches to WSD in Portuguese, using a gold-standard, human-annotated corpus for comparison (Section 4). Finally, we discuss the possible ramifications of our findings in the context of LOD (Section 5), before presenting our conclusions (Section 6). 2 2.1 For WSD tasks, graph-based representations of LKBs can then be used to choose the most likely sense of a word in a given context, based on the dependencies between nodes in the graph (Agirre and Soroa, 2009). Algorithms such as PageRank (Brin and Page, 1998) allow for the weights and probabilities of directed links between target words and words in their local context to be spread over the entirety of the graph (Agirre and Soroa, 2009). Nodes (senses) ‘recommend’ each other based on their own importance – with the importance of any given node being higher or lower depending on the importance of other nodes which recommend it – and then follow a ‘random walk’ over the rest of the graph based on the importance of the nodes to whose edges they are attached (Mihalcea, 2005; Agirre and Soroa, 2009). A"
W15-5503,S07-1008,0,0.0635913,"Missing"
W15-5503,J14-1003,0,0.0222562,"Missing"
W15-5503,H05-1052,0,0.0794691,"evenson et al., 2011; Preiss and Stevenson, 2013). in Portuguese; 1) using the smaller, languagespecific Portuguese MultiWordNet (MultiWordNet, nd) as the underlying lexical knowledge base (LKB) for the WSD, and 2) translating open-class words in the input text from Portuguese to English in order to run WSD using the much larger English WordNet as the underlying LKB. The contributions from our results are twofold: Progress in knowledge-based WSD has largely been driven by the development of graph-based disambiguation methods, as pioneered by a number of researchers (Navigli and Velardi, 2005; Mihalcea, 2005; Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2008). Graph-based methods allow LKBs such as WordNets to be represented as weighted graphs, where word senses correspond to nodes and the relationships or dependencies between pairs of senses correspond to the edges between nodes. The strength of the edge between two nodes, corresponding to the relationship or dependency between two synsets, can then be calculated using semantic similarity measures such as the Lesk algorithm (Lesk, 1986). • Performing graph-based WSD using a smaller, language-specific LKB (Portuguese Mult"
W15-5503,atserias-etal-2004-spanish,0,0.0475901,"some tasks (Agirre et al., 2014). Related Work Knowledge and graph-based WSD While WSD has traditionally delivered its best results using supervised and unsupervised machine learning methods, domain-specific knowledgebased WSD can now perform as well or better than a more generic, supervised machine learningbased WSD approach (Agirre et al., 2009). For example, in the medical domain good results have been obtained in WSD tasks by creating an 7 2.2 in the context of WSD. Agirre and Soroa (2009) evaluated their graph-based WSD algorithm using the Spanish WordNet of approximately 67,000 senses (Atserias et al., 2004) as their LKB. They obtained promising results that approach those reported using the supervised ‘most frequent sense’ (MFS) baseline system for the SemEval-2007 Task 09 dataset (M`arquez et al., 2007). More recently, graph-based WSD performed over Spanish Babelnet senses as the LKB was shown to improve over the MFS baseline in the Multilingual Word Sense Disambiguation task at SemEval2013 (Navigli et al., 2013). These results are encouraging for the case of Portuguese, demonstrating that knowledge-based WSD produces good results using LKBs specific to similar languages. For Portuguese, it wou"
W15-5503,barreto-etal-2006-open,1,0.765626,"Missing"
W15-5503,E06-2024,1,0.741074,"when building the graph, which our own experimentation and previous reporting of results using UKB (Agirre and Soroa, 2009; Agirre et al., 2014) have both shown to result in more accurate WSD. UKB first accepts input texts in a ‘context’ format, where each sentence in a text is treated as an individual context containing the target word and all other open-class words (nouns, verbs, adjectives and adverbs) from the original sentence. This context file can be easily extracted and arranged from input texts pre-tagged with lemmas and partof-speech (PoS) tags, which we produce using the LX-Suite (Branco and Silva, 2006), a collection of shallow processing tools for Portuguese. UKB then performs WSD for each sentence in the context file, using a PageRank-based (Brin and Page, 1998) random walk to return the probability of each node (synset) in a given graph being semantically related to a target word, and returning the appropriate synset identifier for the most likely node. It is this use of the words surrounding a target word in the context file – which are also included as nodes in the graph and whose relevance thus affects the final decision on which sense to assign – that separates UKB from similar algori"
W15-5503,S07-1006,0,0.0236199,"with the terminology that is accounted for being specific to the language in question. A glance at the original and translated context files used in our comparison shows that in many Table 2 compares the performance of UKB over the Portuguese MultiWordNet with the results obtained by Agirre et al. (2014), who most recently reported on the performance of UKB as F1 over four different datasets – the Senseval-2 (Palmer et al., 2001), Senseval-3 (Snyder and Palmer, 2004), Semeval-2007 fine-grained (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007) and Semeval-2007 coarse-graned (Navigli et al., 2007) English all-words tasks. Although the results they present cover various disambiguation options within UKB, we focus here on the results they obtained using the ppr w2w UKB method (as we have). We also assume that they continue using version 3.0 of the English WordNet (complete with information on the semantic relationships between glosses) as their underlying LKB, as they have reported in previous evaluations (Agirre and Soroa, 2009). This combination of UKB option and underlying LKB is comparable with our own evaluation of UKB over the Portuguese MultiWordNet. The 19,700 verified synsets fr"
W15-5503,W15-0208,1,0.830706,"roximately 1 million tokens manually annotated with lemmas, part-of-speech, inflection, and named entities, which are compatible with the input and output formats of the tools in the LXSuite. The corpus contains data from both written sources and transcriptions of spoken Portuguese – we have used the data from the written part, sourced mainly from newspaper articles and short novels and comprising approximately 700,000 tokens, of which 193,443 are open class words. Word senses were manually chosen and assigned to open-class words by a team of human annotators using the LX-SenseAnnotator tool (Neale et al., 2015), a graphical user interface for assigning senses from WordNet-style lexicons to pretagged input texts. The lexicon from which annotators were able to choose senses was the same Portuguese MultiWordNet (approximately 19,700 verified synsets) used in the evaluation. Because annotators were only able to select from the words and synets present in the Portuguese MultiWordNet, not all of the open-class words in the corpus were able to be annotated. 4.2 Performance for Portuguese Running the UKB algorithm over the manually disambiuated CINTIL corpus, we can see how well the two approaches – disambi"
W15-5503,P07-1006,0,0.021086,"Missing"
W15-5503,S01-1005,0,0.0598606,"ese that have much more specific categories in English (N´obrega and Pardo, 2014). While their coverage may be less due to their smaller size, language-specific LKBs limit such problems, with the terminology that is accounted for being specific to the language in question. A glance at the original and translated context files used in our comparison shows that in many Table 2 compares the performance of UKB over the Portuguese MultiWordNet with the results obtained by Agirre et al. (2014), who most recently reported on the performance of UKB as F1 over four different datasets – the Senseval-2 (Palmer et al., 2001), Senseval-3 (Snyder and Palmer, 2004), Semeval-2007 fine-grained (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007) and Semeval-2007 coarse-graned (Navigli et al., 2007) English all-words tasks. Although the results they present cover various disambiguation options within UKB, we focus here on the results they obtained using the ppr w2w UKB method (as we have). We also assume that they continue using version 3.0 of the English WordNet (complete with information on the semantic relationships between glosses) as their underlying LKB, as they have reported in previous evaluatio"
W15-5503,P06-3010,0,0.0205559,"described in this paper, we use UKB, a collection of tools and algorithms (Agirre and Soroa, 2009; Agirre et al., 2014) for performing graph-based WSD over a pre-existing knowledge base. We use UKB for two reasons: Current state of WSD in Portuguese Portuguese-specific WSD has also followed the knowledge-based trend. Early work focused on the automatic generation of disambiguation rules based on representations of meaning in preannotated corpora (Specia et al., 2005), before exploring hybrid approaches that leverage the relationships between different knowledge sources to support such rules (Specia, 2006; Specia et al., 2007). More recent work has focused on graph-based methods, leveraging WordNets as LKBs (N´obrega and Pardo, 2014). However, this work assumes that translating Portuguese terms into English and then querying the English WordNet is sufficient for representing most of the senses found in Portuguese texts. Spanish, which shares a degree of similarity with Portuguese, has been more widely explored • UKB includes tools for automatically creating graph-based representations of LKBs in WordNet-style formats. • The algorithm used by UKB for performing WSD over the graph itself has bee"
W15-5503,S07-1016,0,0.0143434,"ler size, language-specific LKBs limit such problems, with the terminology that is accounted for being specific to the language in question. A glance at the original and translated context files used in our comparison shows that in many Table 2 compares the performance of UKB over the Portuguese MultiWordNet with the results obtained by Agirre et al. (2014), who most recently reported on the performance of UKB as F1 over four different datasets – the Senseval-2 (Palmer et al., 2001), Senseval-3 (Snyder and Palmer, 2004), Semeval-2007 fine-grained (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007) and Semeval-2007 coarse-graned (Navigli et al., 2007) English all-words tasks. Although the results they present cover various disambiguation options within UKB, we focus here on the results they obtained using the ppr w2w UKB method (as we have). We also assume that they continue using version 3.0 of the English WordNet (complete with information on the semantic relationships between glosses) as their underlying LKB, as they have reported in previous evaluations (Agirre and Soroa, 2009). This combination of UKB option and underlying LKB is comparable with our own evaluation of UKB over the P"
W15-5503,N13-3001,0,0.0198652,"example, the word ‘bank’ could be interpreted in the sense of the financial institution or as the slope of land at the side of a river, depending on the context in which it is used. Target words are disambiguated based on their context (determined based on the words surrounding 6 Proceedings of the Second Workshop on Natural Language Processing and Linked Open Data, pages 6–15, Hissar, Bulgaria, 11 September 2015. LKB from the Unified Medical Language System (UMLS) Metathesarurus, a collection of more than one million biomedical concepts and five million concept names (Stevenson et al., 2011; Preiss and Stevenson, 2013). in Portuguese; 1) using the smaller, languagespecific Portuguese MultiWordNet (MultiWordNet, nd) as the underlying lexical knowledge base (LKB) for the WSD, and 2) translating open-class words in the input text from Portuguese to English in order to run WSD using the much larger English WordNet as the underlying LKB. The contributions from our results are twofold: Progress in knowledge-based WSD has largely been driven by the development of graph-based disambiguation methods, as pioneered by a number of researchers (Navigli and Velardi, 2005; Mihalcea, 2005; Sinha and Mihalcea, 2007; Navigli"
W15-5503,W04-0811,0,0.0572555,"ategories in English (N´obrega and Pardo, 2014). While their coverage may be less due to their smaller size, language-specific LKBs limit such problems, with the terminology that is accounted for being specific to the language in question. A glance at the original and translated context files used in our comparison shows that in many Table 2 compares the performance of UKB over the Portuguese MultiWordNet with the results obtained by Agirre et al. (2014), who most recently reported on the performance of UKB as F1 over four different datasets – the Senseval-2 (Palmer et al., 2001), Senseval-3 (Snyder and Palmer, 2004), Semeval-2007 fine-grained (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007) and Semeval-2007 coarse-graned (Navigli et al., 2007) English all-words tasks. Although the results they present cover various disambiguation options within UKB, we focus here on the results they obtained using the ppr w2w UKB method (as we have). We also assume that they continue using version 3.0 of the English WordNet (complete with information on the semantic relationships between glosses) as their underlying LKB, as they have reported in previous evaluations (Agirre and Soroa, 2009). This comb"
W16-2332,N03-1017,0,0.00891535,"provide several translation options for each node along with their estimated probability. The best options are then selected using a Hidden Markov Tree Model (HMTM) ˇ with a target-language tree model (Zabokrtsk´ y and Popel, 2009). For this specific task, where we need to work on a specific domain, an extended version of TectoMT was used allowing interpolation of multiple TMs (Rosa et al., 2015). Moses All the systems submitted that were based on Moses have been trained on a phrase-based model by Giza++ or mGiza with “grow-diag-finaland” symmetrization and “msd-bidirectional-fe” reordering (Koehn et al., 2003). For the language pairs where big quantities of domain-specific monolingual data were available along with the generic domain data, separate language models (domain-specific and generic) were interpolated against our ICT domain-specific development set. For LM training and interpolation, the SRILM toolkit (Stolcke, 2002) was used. The method of truecasing has been adopted for several language pairs where it proved useful. 3 TectoMT The deep translation is based on the TectoMT system, an open-source MT system based on the Treex platform for general natural-language processing. TectoMT uses a c"
W16-2332,C10-3009,0,0.0136209,"obabilities in both directions, lexical weightings in both directions, a phrase length penalty, a ”phrase-mslr-fe” lexicalized reordering model and a target language model. As for the language model, a 5-gram model was trained. The weights for the different components were adjusted to optimize BLEU using MERT tuning over the Batch1 development set, with an n-best list of size 100. For the TectoMT system, EU-Treex existing tools were used in order to get the a-layer. Eustagger is a robust and wide coverage morphological analyzer and POS tagger. The dependency parser is based on the MATE-tools (Bjrkelund et al., 2010). Basque models have been trained using the Basque Dependency Treebank (BDT) corpus (Aduriz et al., 2003). Transformation from the a-level analysis into t-level is partially performed with language-independent blocks thanks to the support of Interset (Zeman, 2008). The English-to-Basque TectoMT system uses the PaCo2 and the Batch1 corpora to train two separate translation models, and they are used to create an interpolated list of translation candidates. In addition to that, the terminological equivalences extracted from the localization PO files (VLC, LO and KDE) as well as the domain terms e"
W16-2332,W16-2334,1,0.789389,"Missing"
W16-2332,2005.mtsummit-papers.11,0,0.0123228,"PO files (VLC, LO and KDE) as well as the domain terms extracted from Wikipedia are used to identify domain terms before syntactic analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used to identify the elements that should be maintained untranslated on the output. Both systems were trained using the same training corpora: the 7th version of the Europarl corpus was used for both translation and language modDutch The Moses system for Dutch was trained on the third version of the Europarl corpus (Koehn, 2005) and the in-domain KDE4 Localization data (Tiedemann, 2012). Words are aligned with GIZA++ and tuning was done with MERT. The applied heuristics for the Dutch baselines were set to “grow-diag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first"
W16-2332,P14-5010,0,0.00318097,"ed Moses with the following factors: ENWordForm-BGLemma|Lemma|BGPOStag, where ENWordForm-BGLemma is an English word form when there is no appropriate Bulgarian one, or the Bulgarian lemma; BGPOStag is the appropriate Bulgarian tag representing grammatical features like number, tense, etc. adaptation and MERT training. Batch2 domain corpus was used for testing during development. The Moses system, EU-Moses, uses factored models to allow lemma-based word-alignment. After word alignment, the rest of the training process is based on lowercased word-forms and standard parameters: Stanford CoreNLP (Manning et al., 2014) and Eustagger (Alegria et al., 2002) tools are used for tokenization and lemmatization, MGIZA for word alignment with the ”growdiag-final-and” symmetrization heuristic, a maximum length of 75 tokens per sentence and 5 tokens per phrase, translation probabilities in both directions, lexical weightings in both directions, a phrase length penalty, a ”phrase-mslr-fe” lexicalized reordering model and a target language model. As for the language model, a 5-gram model was trained. The weights for the different components were adjusted to optimize BLEU using MERT tuning over the Batch1 development se"
W16-2332,W15-4101,1,0.800257,"was performed by the Moses tokenizer. No lemmatization or compound splitting was used and the casing was obtained with the Moses truecaser. For the training, a phrase-based model was used with a language model order of 5, with Kneser-Ney smoothing, which was interpolated using the SRILM tool. The word alignment was done with Giza++ on full forms and the final tuning was done using MERT. The Europarl corpus was used for the training data, both as monolingual data for training language models and as parallel data for training the phrase-table. Regarding the English-to-Portuguese TectoMT system (Silva et al., 2015)(Rodrigues et al., 2016a), PT-Treex, in order to get the a-layer the Portuguese system resorted to LX-Suite (Branco and Silva, 2006), a set of pre-existing shallow processing tools for Portuguese that include a sentence segmenter, a tokenizer, a POS tagger, a morphological analyser and a dependency parser, all with state-of-the-art performance. Treex blocks were created to be called and interfaced with these tools. After running the shallow processing tools, the dependency output of the parser is converted into Universal Dependencies (UD) (de Marneffe et al., 2014). These dependencies are then"
W16-2332,W10-1730,1,0.894585,"Missing"
W16-2332,W15-5712,1,0.718195,"ansfer, and synthesis 4 Basque Both English-Basque submissions are trained on the same training corpora. That is, the PaCO2eneu corpus for translation and language modeling, and the in-domain Batch1 corpus for domain 436 tors retrieved from POS tagged, lemmatized parallel corpora; and BG-DeepMoses — a system that also is based on standard factored Moses but the translation is done in two steps: (1) semanticsbased translation of the source language text to a mixed source-target language text which is then (2) translated to the target language via Moses. The latter system builds on Simov et al. (2015). As training data for both systems the following corpora were used: the Setimes parallel corpus, the Europarl parallel corpus and a corpus created on the basis of the documentation of LibreOffice. The corpora are linguistically processed with the IXA2 pipeline for the English part and the BTB pipeline for the Bulgarian. The analyses include POS tagging, lemmatization and WSD, using the UKB system,3 which provides graph-based methods for Word Sense Disambiguation and lexical similarity measurements. For the BG-Moses system, the following factors have been constructed: WordForm|Lemma|POStag. Fo"
W16-2332,H05-1066,0,0.184414,"Missing"
W16-2332,P14-5003,0,0.0466518,"Missing"
W16-2332,2006.jeptalnrecital-invite.2,1,0.754357,"Missing"
W16-2332,tiedemann-2012-parallel,0,0.0377489,"extracted from Wikipedia are used to identify domain terms before syntactic analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used to identify the elements that should be maintained untranslated on the output. Both systems were trained using the same training corpora: the 7th version of the Europarl corpus was used for both translation and language modDutch The Moses system for Dutch was trained on the third version of the Europarl corpus (Koehn, 2005) and the in-domain KDE4 Localization data (Tiedemann, 2012). Words are aligned with GIZA++ and tuning was done with MERT. The applied heuristics for the Dutch baselines were set to “grow-diag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first model with over 1.9 million sentences from Europarl (Koehn,"
W16-2332,L16-1094,1,0.833385,"ag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first model with over 1.9 million sentences from Europarl (Koehn, 2005) and the second model composed of the Batch1, the Microsoft Terminology Collection and ˇ the LibreOffice localization data (Stajner et al., 2016). Each pair of parallel sentences, one in English and one in Portuguese, are analyzed by Treex up to the t-layer level, where each pair of trees are fed into the model. The TectoMT synthesis (Rodrigues et al., 2016b) included other two lexical-semanticsrelated modules, the HideIT and gazetteers. The HideIT module handles entities that do not require translation such as URLs and shell commands. The gazetteers are specialized lexicons that handle the translation of named entities from the ITdomain such as menu items and button names. Finally, synset IDs were used as additional contextual feature"
W16-2332,P09-2037,1,0.925288,"uage-specific additions and distinguishes two levels of syntactic description: and Spanish, Charles University in Prague for Czech, by University of Groningen for Dutch, by University of Lisbon for Portuguese and by IICTBAS of the Bulgarian Academy of Sciences for Bulgarian. For each language two different systems were submitted, corresponding to different phases of the project, namely a phrase-based MT system built using Moses (Koehn et al., 2007), and a system exploiting deep language engineering approaches, that in all the languages but Bulgarian was imˇ plemented using TectoMT (Zabokrtsk´ y and Popel, 2009). For Bulgarian, its second MT system is not based on TectoMT, but on exploiting deep factors in Moses. All 12 systems are constrained, that is trained only on the data provided by the WMT16 IT-task organizers. We present briefly the Moses common setting and the TectoMT structure and then more detailed information for each language system are provided. In the last Section, results based on BLEU and TrueSkill are given and discussed. 2 • Surface dependency syntax (a-layer) – surface dependency trees containing all the tokens in the sentence. • Deep syntax (t-layer) – dependency trees that conta"
W16-2332,W08-0325,0,0.300026,"Missing"
W16-2332,L16-1438,1,0.826183,"Missing"
W16-2332,zeman-2008-reusable,0,0.0263149,"djusted to optimize BLEU using MERT tuning over the Batch1 development set, with an n-best list of size 100. For the TectoMT system, EU-Treex existing tools were used in order to get the a-layer. Eustagger is a robust and wide coverage morphological analyzer and POS tagger. The dependency parser is based on the MATE-tools (Bjrkelund et al., 2010). Basque models have been trained using the Basque Dependency Treebank (BDT) corpus (Aduriz et al., 2003). Transformation from the a-level analysis into t-level is partially performed with language-independent blocks thanks to the support of Interset (Zeman, 2008). The English-to-Basque TectoMT system uses the PaCo2 and the Batch1 corpora to train two separate translation models, and they are used to create an interpolated list of translation candidates. In addition to that, the terminological equivalences extracted from the localization PO files (VLC, LO and KDE) as well as the domain terms extracted from Wikipedia are used to identify domain terms before syntactical analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used, to identify the elements that s"
W16-2332,W15-5711,1,0.91741,"Missing"
W16-2332,de-marneffe-etal-2014-universal,0,\N,Missing
W16-2332,E06-2024,1,\N,Missing
W16-2332,P07-2045,0,\N,Missing
W16-2332,W13-2208,0,\N,Missing
W16-2332,bojar-etal-2012-joy,1,\N,Missing
W16-2332,L16-1441,1,\N,Missing
W16-6405,W12-3132,0,0.045366,"Missing"
W16-6405,W15-3009,0,0.0248943,"Missing"
W16-6405,W08-0325,0,0.0248875,"sults with simple techniques, e.g. force-translating domain-specific expressions. In such approaches, multiword entries are translated as if they were a single token-with-spaces, failing to represent the internal structure which makes TectoMT a powerful translation engine. In this work we enrich source and target multiword terms with syntactic structure, and seamlessly integrate them in the tree-based transfer phase of TectoMT. Our experiments on the IT domain using the Microsoft terminological resource show improvement in Spanish, Basque and Portuguese. 1 Introduction ˇ ˇ TectoMT (Zabokrtsk´ y et al., 2008; Popel and Zabokrtsk´ y, 2010) has emerged as an architecture to develop deep-transfer systems, where the translation step is done a deep level of analysis, in contrast to methods based on surface sequences of words. TectoMT combines linguistic knowledge and statistical techniques, particularly during transfer, and it aims at transfer on the so-called tectogrammatical layer (Hajiˇcov´a, 2000), a layer of deep syntactic dependency trees. In domain adaptation of machine translation, a typical scenario is as follows: there is an MT system trained on large general-domain data, and there is a bili"
W16-6405,zesch-etal-2008-extracting,0,0.0608747,"Missing"
W18-2801,W10-0603,0,0.0151401,"tic representations of words; and that (ii) lexical semantics can be captured by cooccurrence statistics, the assumption underlying semantics space models of the lexicon. 3 the sake of the comparability of the performance scores obtained. In an initial period, different authors sought to explore the experimental space of the task by focusing on different ways to set up the features. Devereux et al. (2010) find that choosing the set of verbs used for the semantic features under an automatic approach can lead to predictions that are equally good as when using the manually selected set of verbs. Jelodar et al. (2010) use the same set of 25 features to represent a word, but instead of basing the features on co-occurrence counts they resort to relatedness measures based on WordNet. Fernandino et al. (2015) use instead a set of features with 5 sensory-motor experience based attributes (sound, color, visual motion, shape, and manipulation). The relatedness scores between the stimulus word and the attributes are based on human ratings instead of corpus data. Subsequently, as distributional semantics became increasingly popular, authors moved from feature-based representations of the meaning of words to experim"
W18-2801,W18-0107,0,0.645643,"dings. They find the best results with dependency-based embeddings, where words inside the context window are extended with grammatical functions. Binder et al. (2016) use word representations based on 65 experiential attributes with relatedness scores crowdsourced from over 1,700 participants. Xu et al. (2016) present BrainBench, a workbench to test embedding models on both behavioral and brain imaging data sets. Anderson et al. (2017) use a linguistic model based on word2vec embeddings and a visual model built with a deep convolutional neural network on the Google Images data set. Recently, Abnar et al. (2018) evaluated 8 different embeddings regarding their usefulness in predicting neural activation patterns: the cooccurrence embeddings of (Mitchell et al., 2008); the experiential embeddings of (Binder et al., 2016); the non-distributional feature-based embeddings of (Faruqui and Dyer, 2015); and 5 different distributional embeddings, namely word2vec (Mikolov et al., 2013), Fasttext (Bojanowski et al., 2016), dependency-based word2vec (Levy and Goldberg, 2014), GloVe (Pennington et al., 2014) and LexVec (Salle et al., 2016). These authors found that dependencyRelated work Several authors have addr"
W18-2801,P14-2050,0,0.0395971,"ic model based on word2vec embeddings and a visual model built with a deep convolutional neural network on the Google Images data set. Recently, Abnar et al. (2018) evaluated 8 different embeddings regarding their usefulness in predicting neural activation patterns: the cooccurrence embeddings of (Mitchell et al., 2008); the experiential embeddings of (Binder et al., 2016); the non-distributional feature-based embeddings of (Faruqui and Dyer, 2015); and 5 different distributional embeddings, namely word2vec (Mikolov et al., 2013), Fasttext (Bojanowski et al., 2016), dependency-based word2vec (Levy and Goldberg, 2014), GloVe (Pennington et al., 2014) and LexVec (Salle et al., 2016). These authors found that dependencyRelated work Several authors have addressed this brain activation prediction task, keeping up with its basic assumptions and resorting to the same data sets for 1 The verbs are: approach, break, clean, drive, eat, enter, fear, fill, hear, lift, listen, manipulate, move, near, open, push, ride, rub, run, say, see, smell, taste, touch, and wear. 2 The 60 pairs are composed of 5 items from each of the 12 concrete semantic categories (animals, body parts, buildings, building parts, clothing, furni"
W18-2801,Q17-1002,0,0.0150808,"meaning of words to experiment with different vector based representation models (aka word embeddings). Murphy et al. (2012) compare different corpusbased models to derive word embeddings. They find the best results with dependency-based embeddings, where words inside the context window are extended with grammatical functions. Binder et al. (2016) use word representations based on 65 experiential attributes with relatedness scores crowdsourced from over 1,700 participants. Xu et al. (2016) present BrainBench, a workbench to test embedding models on both behavioral and brain imaging data sets. Anderson et al. (2017) use a linguistic model based on word2vec embeddings and a visual model built with a deep convolutional neural network on the Google Images data set. Recently, Abnar et al. (2018) evaluated 8 different embeddings regarding their usefulness in predicting neural activation patterns: the cooccurrence embeddings of (Mitchell et al., 2008); the experiential embeddings of (Binder et al., 2016); the non-distributional feature-based embeddings of (Faruqui and Dyer, 2015); and 5 different distributional embeddings, namely word2vec (Mikolov et al., 2013), Fasttext (Bojanowski et al., 2016), dependency-b"
W18-2801,S12-1019,0,0.0231684,"he features on co-occurrence counts they resort to relatedness measures based on WordNet. Fernandino et al. (2015) use instead a set of features with 5 sensory-motor experience based attributes (sound, color, visual motion, shape, and manipulation). The relatedness scores between the stimulus word and the attributes are based on human ratings instead of corpus data. Subsequently, as distributional semantics became increasingly popular, authors moved from feature-based representations of the meaning of words to experiment with different vector based representation models (aka word embeddings). Murphy et al. (2012) compare different corpusbased models to derive word embeddings. They find the best results with dependency-based embeddings, where words inside the context window are extended with grammatical functions. Binder et al. (2016) use word representations based on 65 experiential attributes with relatedness scores crowdsourced from over 1,700 participants. Xu et al. (2016) present BrainBench, a workbench to test embedding models on both behavioral and brain imaging data sets. Anderson et al. (2017) use a linguistic model based on word2vec embeddings and a visual model built with a deep convolutiona"
W18-2801,D14-1162,0,0.103859,"ings and a visual model built with a deep convolutional neural network on the Google Images data set. Recently, Abnar et al. (2018) evaluated 8 different embeddings regarding their usefulness in predicting neural activation patterns: the cooccurrence embeddings of (Mitchell et al., 2008); the experiential embeddings of (Binder et al., 2016); the non-distributional feature-based embeddings of (Faruqui and Dyer, 2015); and 5 different distributional embeddings, namely word2vec (Mikolov et al., 2013), Fasttext (Bojanowski et al., 2016), dependency-based word2vec (Levy and Goldberg, 2014), GloVe (Pennington et al., 2014) and LexVec (Salle et al., 2016). These authors found that dependencyRelated work Several authors have addressed this brain activation prediction task, keeping up with its basic assumptions and resorting to the same data sets for 1 The verbs are: approach, break, clean, drive, eat, enter, fear, fill, hear, lift, listen, manipulate, move, near, open, push, ride, rub, run, say, see, smell, taste, touch, and wear. 2 The 60 pairs are composed of 5 items from each of the 12 concrete semantic categories (animals, body parts, buildings, building parts, clothing, furniture, insects, kitchen items, too"
W18-2801,W18-3016,1,0.632324,"erent sorts. While most approaches to this task have resorted to feature-based models or to semantic spaces (aka word embeddings), here we address the task of prediciting the brain activation triggred by nouns rather by using a semantic network, thus providing further evidence for the cognitive plausibility of this approach to model lexical meaning. In this paper, we report on the competitive results of resolving the brain activation task by taking a mainstream lexical semantics network, WordNet (Fellbaum, 1998), and resorting to intermediate word embeddings obatined with a novel methodology (Saedi et al., 2018) for generating semantic spaces from semantic networks. The task of taking a semantic representation of a noun and predicting the brain activity triggered by it in terms of fMRI spatial patterns was pioneered by Mitchell et al. (2008). That seminal work used word co-occurrence features to represent the meaning of the nouns. Even though the task does not impose any specific type of semantic representation, the vast majority of subsequent approaches resort to featurebased models or to semantic spaces (aka word embeddings). We address this task, with competitive results, by using instead a semant"
W18-2801,W10-0609,0,0.0214274,"ith all individual accuracies significantly above chance. These results support the plausibility of the two key assumptions underlying the task, namely that (i) brain activation patterns can be predicted from semantic representations of words; and that (ii) lexical semantics can be captured by cooccurrence statistics, the assumption underlying semantics space models of the lexicon. 3 the sake of the comparability of the performance scores obtained. In an initial period, different authors sought to explore the experimental space of the task by focusing on different ways to set up the features. Devereux et al. (2010) find that choosing the set of verbs used for the semantic features under an automatic approach can lead to predictions that are equally good as when using the manually selected set of verbs. Jelodar et al. (2010) use the same set of 25 features to represent a word, but instead of basing the features on co-occurrence counts they resort to relatedness measures based on WordNet. Fernandino et al. (2015) use instead a set of features with 5 sensory-motor experience based attributes (sound, color, visual motion, shape, and manipulation). The relatedness scores between the stimulus word and the att"
W18-2801,P16-2068,0,0.0228355,"deep convolutional neural network on the Google Images data set. Recently, Abnar et al. (2018) evaluated 8 different embeddings regarding their usefulness in predicting neural activation patterns: the cooccurrence embeddings of (Mitchell et al., 2008); the experiential embeddings of (Binder et al., 2016); the non-distributional feature-based embeddings of (Faruqui and Dyer, 2015); and 5 different distributional embeddings, namely word2vec (Mikolov et al., 2013), Fasttext (Bojanowski et al., 2016), dependency-based word2vec (Levy and Goldberg, 2014), GloVe (Pennington et al., 2014) and LexVec (Salle et al., 2016). These authors found that dependencyRelated work Several authors have addressed this brain activation prediction task, keeping up with its basic assumptions and resorting to the same data sets for 1 The verbs are: approach, break, clean, drive, eat, enter, fear, fill, hear, lift, listen, manipulate, move, near, open, push, ride, rub, run, say, see, smell, taste, touch, and wear. 2 The 60 pairs are composed of 5 items from each of the 12 concrete semantic categories (animals, body parts, buildings, building parts, clothing, furniture, insects, kitchen items, tools, vegetables, vehicles, and ot"
W18-2801,D16-1213,0,0.0363587,"Missing"
W18-3016,E12-1004,0,0.298302,"from laypersons; and word2vec is built on the basis of the co-occurrence frequency of lexical units in a collection of documents. Even when motivated in the first place by psycholinguistic research goals, these repositories of lexical knowledge have been extraordinarily important for language technology. They have been instrumental for major advances in language processing tasks and applications such as word sense disambiguation, part-of-speech tagging, named entity recognition, sentiment analysis (e.g. (Li and Jurafsky, 2015)), parsing (e.g. (Socher et al., 2013)), textual entailment (e.g. (Baroni et al., 2012)), discourse analysis (e.g. (Ji and Eisenstein, 2014)), among many others.1 The proliferation of different types of representation for the same object of research is common in science, and searching for a unified rendering of a given research domain has been a major goal in many disciplines. To a large extent, such search focuses on finding ways of converting from one type of representation into another. Once this is made possible, it brings not only the theoretical satisfaction of getting a better unified insight into the research object, but also important instrumental rewards of reapplying"
W18-3016,C92-2082,0,0.305341,"sed our best settings, with a random 60k subgraph, and our second best settings, with the best model with a specific 13k subgraph, cf. Subsection 4.3. 5 Related work From semantic spaces to semantic networks: There has been a long research tradition on semantic networks enhanced with information extracted from text, including distributional vectors, which in the limit may encompass semantic networks obtained from semantic spaces. As a way of illustration, among many others, this includes the work on semantic relations determined from patterns based on regular expressions, either hand crafted (Hearst, 1992), or learned from corpora (Snow et al., 2005); work on semantic relations predicted by classifiers running over distributional vectors (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014); work on semantic relations obtained with deep learning that integrates distributional information and patterns of grammatical dependency relations (Shwartz et al., 2016), including the hard task of distinguishing synonymy from antonymy (Nguyen et al., 2017); etc. While being highly relevant for a unified account of lexical semantics, this line of research addresses the conversion direction, from se"
W18-3016,P12-1015,0,0.0374131,"rdNet under n POS categories. Word ambiguity rate of the whole WordNet is 1.3. 126 4.4 displayed in Table 4.12 Testing data and metrics To assess the robustness of the results obtained, experiments were undertaken with: (i) yet another evaluation metric, namely Pearson’s correlation coefficient; (ii) further evaluation data sets for semantic similarity, namely RG1965 (Rubenstein and Goodenough, 1965) and Wordsim-353Similarity (Agirre et al., 2009); (iii) and testing over another task, namely semantic relatedness, with the evaluation data sets Wordsim-353Relatedness (Agirre et al., 2009), MEN (Bruni et al., 2012) and MTurk-771 (Halawi et al., 2012). In these experiments we used our best settings, with a random 60k subgraph, and our second best settings, with the best model with a specific 13k subgraph, cf. Subsection 4.3. 5 Related work From semantic spaces to semantic networks: There has been a long research tradition on semantic networks enhanced with information extracted from text, including distributional vectors, which in the limit may encompass semantic networks obtained from semantic spaces. As a way of illustration, among many others, this includes the work on semantic relations determined fr"
W18-3016,J06-1003,0,0.147804,". Distances in a semantic graph: The task of determining the semantic similarity between two words can be performed not only on the basis of the distance of their respective vectors in a semantic space, but also on the basis of the distance of the respective concepts in a lexical semantic network, like WordNet. There has been a long research tradition on this issue whose major proposals include (Jiang and Conrath, 1997), (Lin, 1998), (Leacock and Chodorow, 1998), (Hirst and St-Onge, 1998),(Resnik, 1999), among others, which received nice comparative assessments in (Ferlez and Gams, 2004) and (Budanitsky and Hirst, 2006), including their correlation with human judgments. In this context, it is worth of note the work by (Hughes and Ramage, 2007), which resorts to random graph walks over WordNet edges. Differently from our approach, its goal is to obtain word-specific stationary probability distributions — such that the semantic affinity of two words is based on the similarity of their probability distributions —, rather than to obtain vectorial representations for words in a shared distributional sethe relevant semantic relations; (Nickel and Kiela, 2017) that experimented with computing embeddings not in Eucl"
W18-3016,N15-1059,0,0.0240857,"not the major focus of this paper. From semantic networks to semantic spaces: Work towards the conversion direction that is of interest here is more recent. As a way of illustration, among others, one can mention (Faruqui et al., 2015), which explored retrofitting to refine distributional representations using relational information, and (Yu and Dredze, 2014), which focused also on refining word embeddings with lexical knowledge, but which are not addressing the goal of obtaining semantic spaces solely on the basis of semantic networks as we do here. That is the aim also of recent work like (Camacho-Collados et al., 2015) who improve the embeddings built from data sets made of selected Wikipedia pages by resorting to the local, oneedge relations of each relevant word in the WordNet graph. Further recent works worth mentioning include (Vendrov et al., 2015) that resorted to order embeddings, which however do not preserve distance and/or do not preserve directionality under Additional metric: The evaluation scores obtained over SimLex-999 with the Pearson’s coefficient are basically aligned with the scores already obtained with Spearman’s coefficient, confirming the superiority of the WordNet embeddings. Additio"
W18-3016,D07-1061,0,0.116955,"he basis of the distance of their respective vectors in a semantic space, but also on the basis of the distance of the respective concepts in a lexical semantic network, like WordNet. There has been a long research tradition on this issue whose major proposals include (Jiang and Conrath, 1997), (Lin, 1998), (Leacock and Chodorow, 1998), (Hirst and St-Onge, 1998),(Resnik, 1999), among others, which received nice comparative assessments in (Ferlez and Gams, 2004) and (Budanitsky and Hirst, 2006), including their correlation with human judgments. In this context, it is worth of note the work by (Hughes and Ramage, 2007), which resorts to random graph walks over WordNet edges. Differently from our approach, its goal is to obtain word-specific stationary probability distributions — such that the semantic affinity of two words is based on the similarity of their probability distributions —, rather than to obtain vectorial representations for words in a shared distributional sethe relevant semantic relations; (Nickel and Kiela, 2017) that experimented with computing embeddings not in Euclidean but in hyperbolic space, namely the Poincaré ball model. A shortcoming with these proposals is that their outcome is not"
W18-3016,P14-1002,0,0.103507,"Missing"
W18-3016,C16-1175,0,0.105528,"Missing"
W18-3016,O97-1002,0,0.577729,"dNet; this “artificial text” is a partial and contingent reflection of the semantic network and is used to obtain distributional vectors by resorting to typical word embeddings techniques based on text. Distances in a semantic graph: The task of determining the semantic similarity between two words can be performed not only on the basis of the distance of their respective vectors in a semantic space, but also on the basis of the distance of the respective concepts in a lexical semantic network, like WordNet. There has been a long research tradition on this issue whose major proposals include (Jiang and Conrath, 1997), (Lin, 1998), (Leacock and Chodorow, 1998), (Hirst and St-Onge, 1998),(Resnik, 1999), among others, which received nice comparative assessments in (Ferlez and Gams, 2004) and (Budanitsky and Hirst, 2006), including their correlation with human judgments. In this context, it is worth of note the work by (Hughes and Ramage, 2007), which resorts to random graph walks over WordNet edges. Differently from our approach, its goal is to obtain word-specific stationary probability distributions — such that the semantic affinity of two words is based on the similarity of their probability distributions"
W18-3016,N15-1184,0,0.0646122,"Missing"
W18-3016,D15-1200,0,0.0594503,"Missing"
W18-3016,N15-1165,0,0.155556,"Missing"
W18-3016,W18-2801,1,0.596089,"relations; (Nickel and Kiela, 2017) that experimented with computing embeddings not in Euclidean but in hyperbolic space, namely the Poincaré ball model. A shortcoming with these proposals is that their outcome is not easily plugged into neural models. Also they are not fit to evaluation on external tasks, like the semantic similarity task, with their evaluation being rather based on their ability to complete missing edges from ontological graphs. In contrats, an example of the sutability of wnet2vec to be plugged into neural models and of its application in a downstream task is reported in (Rodrigues et al., 2018), where these embeddings support the predicition of brain activation based on neural networks. There has been also a long tradition of research on learning vector embeddings from multirelational data of which, among many others, one can refer (Bordes et al., 2013), (Lin et al., 2015), and (Nickel et al., 2016). Though to a large extent these are generic approaches for graph to vectors conversion, also here the major focus has been on exploring these models on their ability to complete missing relations in knowledge bases rather than to experiment them on natural language processing and lexical"
W18-3016,C14-1097,0,0.0248139,"Missing"
W18-3016,P16-1226,0,0.0237672,"Missing"
W18-3016,P13-1045,0,0.0575887,"Missing"
W18-3016,C14-1212,0,0.0189738,"emantic networks: There has been a long research tradition on semantic networks enhanced with information extracted from text, including distributional vectors, which in the limit may encompass semantic networks obtained from semantic spaces. As a way of illustration, among many others, this includes the work on semantic relations determined from patterns based on regular expressions, either hand crafted (Hearst, 1992), or learned from corpora (Snow et al., 2005); work on semantic relations predicted by classifiers running over distributional vectors (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014); work on semantic relations obtained with deep learning that integrates distributional information and patterns of grammatical dependency relations (Shwartz et al., 2016), including the hard task of distinguishing synonymy from antonymy (Nguyen et al., 2017); etc. While being highly relevant for a unified account of lexical semantics, this line of research addresses the conversion direction, from semantic spaces to semantic networks, that is not the major focus of this paper. From semantic networks to semantic spaces: Work towards the conversion direction that is of interest here is more rece"
W18-3016,P14-2089,0,0.0322672,"ard task of distinguishing synonymy from antonymy (Nguyen et al., 2017); etc. While being highly relevant for a unified account of lexical semantics, this line of research addresses the conversion direction, from semantic spaces to semantic networks, that is not the major focus of this paper. From semantic networks to semantic spaces: Work towards the conversion direction that is of interest here is more recent. As a way of illustration, among others, one can mention (Faruqui et al., 2015), which explored retrofitting to refine distributional representations using relational information, and (Yu and Dredze, 2014), which focused also on refining word embeddings with lexical knowledge, but which are not addressing the goal of obtaining semantic spaces solely on the basis of semantic networks as we do here. That is the aim also of recent work like (Camacho-Collados et al., 2015) who improve the embeddings built from data sets made of selected Wikipedia pages by resorting to the local, oneedge relations of each relevant word in the WordNet graph. Further recent works worth mentioning include (Vendrov et al., 2015) that resorted to order embeddings, which however do not preserve distance and/or do not pres"
W19-3510,W17-3013,0,0.0438216,"Missing"
W19-3510,W17-6615,0,0.0397333,"Missing"
W19-3510,W18-4401,0,0.116503,"Missing"
W19-3510,W16-3638,0,0.0472051,"Missing"
W19-3510,L18-1443,0,0.387562,"2016; Davidson et al., 2017; Nobata et al., 2016; Jigsaw, 2018), see also the overview by Schmidt and Wiegand (2017). As a result, also many more annotated datasets, which are the precondition for the use of supervised machine learning, are available for English (e.g., Waseem and Hovy (2016); Davidson et al. (2017); Nobata et al. (2016); Jigsaw (2018)) than for other languages. However, hate speech is not a phenomenon that is observed only in English discourse; it is notorious in online media in other languages as well; cf., e.g., Spanish (Fersini et al., 2018), Italian (Poletto et al., 2017; Sanguinetti et al., 2018), or German (Ross et al., 2016). In this work, we aim to contribute to the field of hate speech detection. Our contribution is twofold: (i) diversification of the research on hate speech by provision of a new dataset of hate speech in another language than English, namely Portuguese; (ii) introduction of a novel fine-grained hate speech typology that improves on the common state-of-the-art used typologies, which tend to disregard the existence of subtypes of hate speech and either consider hate speech recognition as a binary classification task, or take into account only a few classes, such as"
W19-3510,W17-1101,0,0.0328408,"self-image and social exclusion of the targeted individuals, groups or populations, and incites violence against them. A clear example of the extreme harm that can be caused by hate speech is the 1994 Rwandan genocide; see Schabas (2000) for a detailed analysis. The detection of online hate speech is thus a pressing problem that calls for solutions. Over the last decade, a considerable number of supervised machine learning-based works tackled the problem. Most of them focused on English (Waseem and Hovy, 2016; Davidson et al., 2017; Nobata et al., 2016; Jigsaw, 2018), see also the overview by Schmidt and Wiegand (2017). As a result, also many more annotated datasets, which are the precondition for the use of supervised machine learning, are available for English (e.g., Waseem and Hovy (2016); Davidson et al. (2017); Nobata et al. (2016); Jigsaw (2018)) than for other languages. However, hate speech is not a phenomenon that is observed only in English discourse; it is notorious in online media in other languages as well; cf., e.g., Spanish (Fersini et al., 2018), Italian (Poletto et al., 2017; Sanguinetti et al., 2018), or German (Ross et al., 2016). In this work, we aim to contribute to the field of hate sp"
W19-3510,W12-2103,0,0.297983,"nd 30 messages while others had more than 5 We use the term “search instance” to refer to profiles, keywords or hashtags used for the Twitter search. 97 4.2 Hierarchical annotation following properties: When studying hate speech, it is possible to distinguish between different categories of it, like ‘racism’, ‘sexism’, or ‘homophobia’. A more finegrained view can be useful in hate speech classification because each category has a specific vocabulary and ways to be expressed, such that creating a language model for each category may be helpful to improve the automatic detection of hate speech (Warner and Hirschberg, 2012). Another phenomenon we can observe when analyzing different categories of hate speech is their intersectionality. This concept appeared as an answer to the historical exclusion of black women from early women’s rights movements often concerned with the struggles of white women alone. Intersectionality brings attention to the experiences of people who are subjected to multiple forms of discrimination within a society (e.g., being woman and black) (Collins, 2015). Waseem (2016) introduce a hate speech labeling scheme that follows an intersectional approach. In addition to ‘racism’, ‘sexism’, an"
W19-3510,W17-3006,0,0.0437384,"Missing"
W19-3510,W16-5618,0,0.112572,"ng a language model for each category may be helpful to improve the automatic detection of hate speech (Warner and Hirschberg, 2012). Another phenomenon we can observe when analyzing different categories of hate speech is their intersectionality. This concept appeared as an answer to the historical exclusion of black women from early women’s rights movements often concerned with the struggles of white women alone. Intersectionality brings attention to the experiences of people who are subjected to multiple forms of discrimination within a society (e.g., being woman and black) (Collins, 2015). Waseem (2016) introduce a hate speech labeling scheme that follows an intersectional approach. In addition to ‘racism’, ‘sexism’, and ‘neither’, they use the label “both” arguing that the intersection of multiple oppression categories can differ from the forms of oppression it consists of (Crenshaw, 2018). To better take into account different hate speech categories from an intersectional perspective, we approach the definition of the hate speech annotation schema in terms of a hierarchical structure of classes. • The ‘hate speech’ class corresponds to the root of the graph. • If hate speech can be divided"
W19-3510,N16-2013,0,0.435001,"ne of these observed negative phenomena is the propagation of hate speech. Hate speech leads to a negative self-image and social exclusion of the targeted individuals, groups or populations, and incites violence against them. A clear example of the extreme harm that can be caused by hate speech is the 1994 Rwandan genocide; see Schabas (2000) for a detailed analysis. The detection of online hate speech is thus a pressing problem that calls for solutions. Over the last decade, a considerable number of supervised machine learning-based works tackled the problem. Most of them focused on English (Waseem and Hovy, 2016; Davidson et al., 2017; Nobata et al., 2016; Jigsaw, 2018), see also the overview by Schmidt and Wiegand (2017). As a result, also many more annotated datasets, which are the precondition for the use of supervised machine learning, are available for English (e.g., Waseem and Hovy (2016); Davidson et al. (2017); Nobata et al. (2016); Jigsaw (2018)) than for other languages. However, hate speech is not a phenomenon that is observed only in English discourse; it is notorious in online media in other languages as well; cf., e.g., Spanish (Fersini et al., 2018), Italian (Poletto et al., 2017; Sang"
