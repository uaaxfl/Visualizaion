2020.acl-main.273,W18-6402,0,0.192307,"n multi-modal NMT. 4 Related Work Multi-modal NMT Huang et al. (2016) first incorporate global or regional visual features into attention-based NMT. Calixto and Liu (2017) also study the effects of incorporating global visual features into different NMT components. Elliott and K´ad´ar (2017) share an encoder between a translation model and an image prediction model to learn visually grounded representations. Besides, the most common practice is to use attention mechanisms to extract visual contexts for multimodal NMT (Caglayan et al., 2016; Calixto et al., 2017; Delbrouck and Dupont, 2017a,b; Barrault et al., 2018). Recently, Ive et al. (2019) propose a translate-and-refine approach and Calixto et al. (2019) employ a latent variable model to capture the multi-modal interactions for multi-modal NMT. Apart from model design, Elliott (2018) reveal that visual information seems to be ignored by the multimodal NMT models. Caglayan et al. (2019) conduct a systematic analysis and show that visual information can be better leveraged under limited textual context. Different from the above-mentioned studies, we first represent the input sentence-image pair as a unified graph, where various semantic relationships"
2020.acl-main.273,P18-1026,0,0.039539,"Missing"
2020.acl-main.273,W17-4746,0,0.343479,"Missing"
2020.acl-main.273,N19-1422,0,0.199371,"model and an image prediction model to learn visually grounded representations. Besides, the most common practice is to use attention mechanisms to extract visual contexts for multimodal NMT (Caglayan et al., 2016; Calixto et al., 2017; Delbrouck and Dupont, 2017a,b; Barrault et al., 2018). Recently, Ive et al. (2019) propose a translate-and-refine approach and Calixto et al. (2019) employ a latent variable model to capture the multi-modal interactions for multi-modal NMT. Apart from model design, Elliott (2018) reveal that visual information seems to be ignored by the multimodal NMT models. Caglayan et al. (2019) conduct a systematic analysis and show that visual information can be better leveraged under limited textual context. Different from the above-mentioned studies, we first represent the input sentence-image pair as a unified graph, where various semantic relationships between multi-modal semantic units can be effectively captured for multi-modal NMT. Benefiting from the multi-modal graph, we further introduce an extended GNN to conduct graph encoding via multi-modal semantic interactions. Note that if we directly adapt the approach proposed by Huang et al. (2016) into Transformer, the model (O"
2020.acl-main.273,D17-1105,0,0.0280843,"boarding ramp” is not translated correctly by all baselines, while our model correctly translates it. This reveals that our encoder is able to learn more accurate representations. 3.6 Results on the En⇒Fr Translation Task We also conduct experiments on the EN⇒Fr dataset. From Table 3, our model still achieves better performance compared to all baselines, which demonstrates again that our model is effective and general to different language pairs in multi-modal NMT. 4 Related Work Multi-modal NMT Huang et al. (2016) first incorporate global or regional visual features into attention-based NMT. Calixto and Liu (2017) also study the effects of incorporating global visual features into different NMT components. Elliott and K´ad´ar (2017) share an encoder between a translation model and an image prediction model to learn visually grounded representations. Besides, the most common practice is to use attention mechanisms to extract visual contexts for multimodal NMT (Caglayan et al., 2016; Calixto et al., 2017; Delbrouck and Dupont, 2017a,b; Barrault et al., 2018). Recently, Ive et al. (2019) propose a translate-and-refine approach and Calixto et al. (2019) employ a latent variable model to capture the multi-m"
2020.acl-main.273,P17-1175,0,0.136326,"Missing"
2020.acl-main.273,P19-1642,0,0.690464,"translation, since the visual context helps to resolve ambiguous multi-sense words (Ive et al., 2019). Apparently, how to fully exploit visual information is one of the core issues in multi-modal NMT, which directly impacts the model performance. To this end, a lot of efforts have been made, roughly consisting of: (1) encoding each input image into a global feature vector, which can be used to initialize different components of multi-modal NMT models, or as additional source tokens (Huang et al., 2016; Calixto et al., 2017), or to learn the joint multi-modal representation (Zhou et al., 2018; Calixto et al., 2019); (2) extracting object-based image features to initialize the model, or supplement source sequences, or generate attention-based visual context (Huang et al., 2016; Ive et al., 2019); and (3) representing each image as spatial features, which can be exploited as extra context (Calixto et al., 2017; Delbrouck and Dupont, 2017a; Ive et al., 2019), or a supplement to source semantics (Delbrouck and Dupont, 2017b) via an attention mechanism. Despite their success, the above studies do not fully exploit the fine-grained semantic correspondences between semantic units within an input sentence-image"
2020.acl-main.273,D17-1095,0,0.143417,"Missing"
2020.acl-main.273,W14-3348,0,0.106532,"el tends to be over-fitting, we first perform a small grid search to obtain a set of hyper-parameters on the En⇒De validation set. Specifically, the word embedding dimension and hidden size are 128 and 256 respectively. The decoder has Ld =4 layers4 and the number of attention heads is 4. The dropout is set to 0.5. Each batch consists of approximately 2,000 source and target tokens. We apply the Adam optimizer with a scheduled learning rate to optimize various models, and we use other same settings as (Vaswani et al., 2017). Finally, we use the metrics BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) to evaluate the quality of translations. Particularly, we run all models three times for each experiment and report the average results. 40.4 BLEU 3.1 39.6 39.2 http://www.statmt.org/wmt18/multimodal-task.html There is no parsing failure for this dataset. If no noun is detected for a sentence, the object representations will be set to zero vectors and the model will degenerate to Transformer. 4 The encoder of the text-based Transformer also has 4 layers. 1 2 3 Le 4 5 Figure 3: Results on the En⇒De validation set regarding the number Le of graph-based multi-modal fusion layers. Baseline Models"
2020.acl-main.273,D18-1329,0,0.234241,"into different NMT components. Elliott and K´ad´ar (2017) share an encoder between a translation model and an image prediction model to learn visually grounded representations. Besides, the most common practice is to use attention mechanisms to extract visual contexts for multimodal NMT (Caglayan et al., 2016; Calixto et al., 2017; Delbrouck and Dupont, 2017a,b; Barrault et al., 2018). Recently, Ive et al. (2019) propose a translate-and-refine approach and Calixto et al. (2019) employ a latent variable model to capture the multi-modal interactions for multi-modal NMT. Apart from model design, Elliott (2018) reveal that visual information seems to be ignored by the multimodal NMT models. Caglayan et al. (2019) conduct a systematic analysis and show that visual information can be better leveraged under limited textual context. Different from the above-mentioned studies, we first represent the input sentence-image pair as a unified graph, where various semantic relationships between multi-modal semantic units can be effectively captured for multi-modal NMT. Benefiting from the multi-modal graph, we further introduce an extended GNN to conduct graph encoding via multi-modal semantic interactions. No"
2020.acl-main.273,W16-3210,0,0.251902,"Missing"
2020.acl-main.273,I17-1014,0,0.363334,"Missing"
2020.acl-main.273,P18-1150,0,0.107975,"rge-scale pretraining, while we utilize visual grounding to capture explicit cross-modal correspondences. (3) We focus on multi-modal NMT rather than visionand-language reasoning in (Tan and Bansal, 2019). Graph Neural Networks Recently, GNNs (Marco Gori and Scarselli, 2005) including gated graph neural network (Li et al., 2016), graph convolutional network (Duvenaud et al., 2015; Kipf and Welling, 2017) and graph attention network (Velickovic et al., 2018) have been shown effective in many tasks such as VQA (Teney et al., 2017; Norcliffe-Brown et al., 2018; Li et al., 2019), text generation (Gildea et al., 2018; Becky et al., 2018; Song et al., 2018b, 2019) and text representation (Zhang et al., 2018; Yin et al., 2019; Song et al., 3032 Source: A boy riding a skateboard on a skateboarding ramp . Reference: Ein junge fährt skateboard auf einer skateboardrampe . Tranformer: Ein junge fährt auf einem skateboard auf einer rampe . Doubly-att(TF): Ein junge fährt mit einem skateboard auf einer rampe . Enc-att(TF): Ein junge fährt ein skateboard auf einer rampe . ObjectAsToken(TF): Ein junge fährt auf einem skateboard auf einer rampe . Our model: Ein junge fährt auf einem skateboard auf einer skateboardram"
2020.acl-main.273,W18-6441,0,0.530768,"Missing"
2020.acl-main.273,W16-2360,0,0.23828,"ce and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects). We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model. 1 Introduction Multi-modal neural machine translation (NMT) (Huang et al., 2016; Calixto et al., 2017) has become an important research direction in machine translation, due to its research significance in multimodal deep learning and wide applications, such as translating multimedia news and web product information (Zhou et al., 2018). It significantly extends the conventional text-based machine translation by taking images as additional inputs. The assumption behind this is that the translation is expected to be more accurate compared to purely text-based ∗ This work is done when Yongjing Yin was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. †"
2020.acl-main.273,P19-1653,0,0.649723,"Missing"
2020.acl-main.273,P16-1162,0,0.0622395,"dal English⇒German (En⇒De) and English⇒French (En⇒Fr) translation tasks. 3028 40.8 Setup Datasets We use the Multi30K dataset (Elliott et al., 2016), where each image is paired with one English description and human translations into German and French. Training, validation and test sets contain 29,000, 1,014 and 1,000 instances respectively. In addition, we evaluate various models on the WMT17 test set and the ambiguous MSCOCO test set, which contain 1,000 and 461 instances respectively. Here, we directly use the preprocessed sentences 2 and segment words into subwords via byte pair encoding (Sennrich et al., 2016) with 10,000 merge operations. Visual Features We first apply the Stanford parser to identify noun phrases from each source sentence, and then employ the visual ground toolkit released by Yang et al. (2019) to detect associated visual objects of the identified noun phrases. For each phrase, we keep the visual object with the highest prediction probability, so as to reduce negative effects of abundant visual objects. In each sentence, the average numbers of objects and words are around 3.5 and 15.0 respectively. 3 Finally, we compute 2,048-dimensional features for these objects with the pre-tra"
2020.acl-main.273,Q19-1002,1,0.876509,"Missing"
2020.acl-main.273,D19-1514,0,0.177044,"r” semantically corresponds to the blue dashed region. The neglect of this important clue may be due to two big challenges: 1) how to construct a unified representation to bridge the semantic gap between two different modalities, and 2) how to achieve semantic interactions based on the unified representation. However, we believe that such semantic correspondences can be exploited to refine multimodal representation learning, since they enable the representations within one modality to incorporate cross-modal information as supplement during multi-modal semantic interactions (Lee et al., 2018; Tan and Bansal, 2019). 3025 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3025–3035 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Multi-modal Graph Image ??? ??? Text Two boys are playing with a toy car ??? ??? ??? ??? ??? ??? ??? Two boys are playing with a ??? ??? toy car Figure 1: The multi-modal graph for an input sentence-image pair. The blue and green solid circles denote textual nodes and visual nodes respectively. An intra-modal edge (dotted line) connects two nodes in the same modality, and an inter-modal edge (solid line) links two no"
2020.acl-main.273,P02-1040,0,0.110641,"corpus is small and the trained model tends to be over-fitting, we first perform a small grid search to obtain a set of hyper-parameters on the En⇒De validation set. Specifically, the word embedding dimension and hidden size are 128 and 256 respectively. The decoder has Ld =4 layers4 and the number of attention heads is 4. The dropout is set to 0.5. Each batch consists of approximately 2,000 source and target tokens. We apply the Adam optimizer with a scheduled learning rate to optimize various models, and we use other same settings as (Vaswani et al., 2017). Finally, we use the metrics BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) to evaluate the quality of translations. Particularly, we run all models three times for each experiment and report the average results. 40.4 BLEU 3.1 39.6 39.2 http://www.statmt.org/wmt18/multimodal-task.html There is no parsing failure for this dataset. If no noun is detected for a sentence, the object representations will be set to zero vectors and the model will degenerate to Transformer. 4 The encoder of the text-based Transformer also has 4 layers. 1 2 3 Le 4 5 Figure 3: Results on the En⇒De validation set regarding the number Le of graph-based mul"
2020.acl-main.273,P18-1030,0,0.0280822,"respondences. (3) We focus on multi-modal NMT rather than visionand-language reasoning in (Tan and Bansal, 2019). Graph Neural Networks Recently, GNNs (Marco Gori and Scarselli, 2005) including gated graph neural network (Li et al., 2016), graph convolutional network (Duvenaud et al., 2015; Kipf and Welling, 2017) and graph attention network (Velickovic et al., 2018) have been shown effective in many tasks such as VQA (Teney et al., 2017; Norcliffe-Brown et al., 2018; Li et al., 2019), text generation (Gildea et al., 2018; Becky et al., 2018; Song et al., 2018b, 2019) and text representation (Zhang et al., 2018; Yin et al., 2019; Song et al., 3032 Source: A boy riding a skateboard on a skateboarding ramp . Reference: Ein junge fährt skateboard auf einer skateboardrampe . Tranformer: Ein junge fährt auf einem skateboard auf einer rampe . Doubly-att(TF): Ein junge fährt mit einem skateboard auf einer rampe . Enc-att(TF): Ein junge fährt ein skateboard auf einer rampe . ObjectAsToken(TF): Ein junge fährt auf einem skateboard auf einer rampe . Our model: Ein junge fährt auf einem skateboard auf einer skateboardrampe . Figure 6: A translation example of different multi-modal NMT models. The baseline mode"
2020.acl-main.273,D18-1400,0,0.102639,"ctions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model. 1 Introduction Multi-modal neural machine translation (NMT) (Huang et al., 2016; Calixto et al., 2017) has become an important research direction in machine translation, due to its research significance in multimodal deep learning and wide applications, such as translating multimedia news and web product information (Zhou et al., 2018). It significantly extends the conventional text-based machine translation by taking images as additional inputs. The assumption behind this is that the translation is expected to be more accurate compared to purely text-based ∗ This work is done when Yongjing Yin was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. † Corresponding author. translation, since the visual context helps to resolve ambiguous multi-sense words (Ive et al., 2019). Apparently, how to fully exploit visual information is one of the core issues in multi-modal NMT, which directly impacts the model p"
2020.acl-main.28,D17-1098,0,0.0259676,"y, where their groundtruth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in fact a greedy algorithm. In NLP, beam search (BS, Tillmann et al. 1997) is widely applied to sentence generation. BS maintains a k-best list in a partially greedy fashion during left-to-right (or right-to-left) decoding (Anderson et al., 2017; Zhou and Rush, 2019). By contrast, UPSA is local search with distributed edits over the entire sentence. Moreover, UPSA is able to make use of the original sentence as an initial state of searching, whereas BS usually works in the decoder of a Seq2Seq model and is not applicable to unsupervised paraphrasing. 3 Approach In this section, we present our novel UPSA framework that uses simulated annealing (SA) for unsupervised paraphrasing. In particular, we first present the general SA algorithm and then design our searching objective and searching actions (i.e., candidate sentence generator) fo"
2020.acl-main.28,P19-1602,1,0.921322,"s a result, unsupervised methods would largely benefit paraphrase generation as no parallel data are 1 Code and data available at: https://github.com/ Liuxg16/UPSA 2 https://www.kaggle.com/c/quora-question-pairs 302 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 302–312 c July 5 - 10, 2020. 2020 Association for Computational Linguistics needed. With the help of deep learning, researchers are able to generate paraphrases by sampling from a neural network-defined probabilistic distribution, either in a continuous latent space (Bowman et al., 2016; Bao et al., 2019) or directly in the word space (Miao et al., 2019). However, the meaning preservation and expression diversity of those generated paraphrases are less “controllable” in such probabilistic sampling procedures. To this end, we propose a novel approach to Unsupervised Paraphrasing by Simulated Annealing (UPSA). Simulated annealing (SA) is a stochastic searching algorithm towards an objective function, which can be flexibly defined. In our work, we design a sophisticated objective function, considering semantic preservation, expression diversity, and language fluency of paraphrases. SA searches to"
2020.acl-main.28,N03-1003,0,0.228491,"14 English-German dataset contains 4.5M sentence pairs (Neidert et al., 2014). However, the training corpora for paraphrasing are usually small. The widely-used Quora dataset2 only contains 140K pairs of paraphrases; constructing such human-written paraphrase pairs is expensive and labor-intensive. Further, existing paraphrase datasets are domain-specific: the Quora dataset only contains question sentences, and thus, supervised paraphrase models do not generalize well to new domains (Li et al., 2019). On the other hand, researchers synthesize pseudo-paraphrase pairs by clustering news events (Barzilay and Lee, 2003), crawling tweets of the same topic (Lan et al., 2017), or translating bi-lingual datasets (Wieting and Gimpel, 2017), but these methods typically yield noisy training sets, leading to low paraphrasing performance (Li et al., 2018). As a result, unsupervised methods would largely benefit paraphrase generation as no parallel data are 1 Code and data available at: https://github.com/ Liuxg16/UPSA 2 https://www.kaggle.com/c/quora-question-pairs 302 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 302–312 c July 5 - 10, 2020. 2020 Association for Compu"
2020.acl-main.28,K16-1002,0,0.740759,"(Li et al., 2018). As a result, unsupervised methods would largely benefit paraphrase generation as no parallel data are 1 Code and data available at: https://github.com/ Liuxg16/UPSA 2 https://www.kaggle.com/c/quora-question-pairs 302 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 302–312 c July 5 - 10, 2020. 2020 Association for Computational Linguistics needed. With the help of deep learning, researchers are able to generate paraphrases by sampling from a neural network-defined probabilistic distribution, either in a continuous latent space (Bowman et al., 2016; Bao et al., 2019) or directly in the word space (Miao et al., 2019). However, the meaning preservation and expression diversity of those generated paraphrases are less “controllable” in such probabilistic sampling procedures. To this end, we propose a novel approach to Unsupervised Paraphrasing by Simulated Annealing (UPSA). Simulated annealing (SA) is a stochastic searching algorithm towards an objective function, which can be flexibly defined. In our work, we design a sophisticated objective function, considering semantic preservation, expression diversity, and language fluency of paraphra"
2020.acl-main.28,C04-1051,0,0.266948,"annealing to address rare words. • We achieve the state-of-the-art performance on four benchmark datasets compared with previous unsupervised paraphrase generators, largely reducing the performance gap between unsupervised and supervised paraphrasing. We outperform most domain-adapted paraphrase generators, and even a supervised one on the Wikianswers dataset. 2 Related Work In early years, paraphrasing was typically accomplished by exploiting linguistic knowledge (Mckeown, 1983; Ellsworth and Janin, 2007; Narayan et al., 2016) and statistical machine translation methods (Quirk et al., 2004; Dolan et al., 2004). Recently, deep neural networks have become a prevailing approach to text generation, where paraphrasing is often formulated as a supervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample sentences from a learned latent space (Bowman et al., 2016; Zhang et al., 2019; Bao et al., 2019). But the ge"
2020.acl-main.28,D17-1158,0,0.0234271,"ed Seq2Seq paraphrase generators: ResidualLSTM (Prakash et al., 2016), VAE-SVG-eq (Gupta et al., 2018), Pointer-generator (See et al., 2017), the Transformer (Vaswani et al., 2017), and the decomposable neural paraphrase generator (DNPG, Li et al., 2019). DNPG has been reported as the state-of-the-art supervised paraphrase generator. To better compare UPSA with all paraphrasing settings, we also include domain-adapted supervised paraphrase generators that are trained in a source domain but tested in a target domain, including shallow fusion (Gulcehre et al., 2015) and multitask learning (MTL, Domhan and Hieber 2017). We adopt BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores as automatic metrics to evaluate model performance. Sun and Zhou (2012) observe that BLEU and ROUGE could not measure the diversity between the generated and the original sentences, and propose the iBLEU variant by penalizing by the similarity with the original sentence. Therefore, we regard the iBLEU score as our major metric, which is also adopted in Li et al. (2019). In addition, we also conduct human evaluation in our experiments (detailed later). 4.3 Implementation Details Our method involves unsupervised language modeli"
2020.acl-main.28,P19-1331,0,0.0459871,"asing performance. The main difference between their work and ours is that UPSA imposes the annealing temperature into the sampling process for better convergence to an optimum. In addition, we define our searching objective involving not only semantic similarity and language fluency, but also the expression diversity; we further propose a copy mechanism in our searching process. Recently, a few studies have applied editingbased approaches to sentence generation. Guu et al. (2018) propose a heuristic delete-retrieve-generate component for a supervised sequence-to-sequence 303 (Seq2Seq) model. Dong et al. (2019) learn the deletion and insertion operations for text simplification in a supervised way, where their groundtruth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in fact a greedy algorithm. In NLP, beam search (BS, Tillmann et al. 1997) is widely applied to sentence generation. BS maintains a k-bes"
2020.acl-main.28,W07-1424,0,0.0599355,"ersity between a paraphrase and the input. • We propose a copy mechanism as one of our search actions of simulated annealing to address rare words. • We achieve the state-of-the-art performance on four benchmark datasets compared with previous unsupervised paraphrase generators, largely reducing the performance gap between unsupervised and supervised paraphrasing. We outperform most domain-adapted paraphrase generators, and even a supervised one on the Wikianswers dataset. 2 Related Work In early years, paraphrasing was typically accomplished by exploiting linguistic knowledge (Mckeown, 1983; Ellsworth and Janin, 2007; Narayan et al., 2016) and statistical machine translation methods (Quirk et al., 2004; Dolan et al., 2004). Recently, deep neural networks have become a prevailing approach to text generation, where paraphrasing is often formulated as a supervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample s"
2020.acl-main.28,P13-1158,0,0.0824825,"Missing"
2020.acl-main.28,P16-1154,0,0.0272601,"eplace = top- Kw∗ p− LM i − (w∗ , wt,k+1 , . . . , wt,lt ) . p← LM 4 (9) 4.1 For word insertion, the top-K vocabulary Wt,insert is computed in a similar way (except that the position of w∗ is slightly different). Details are not repeated. In our experiments, K is set to 50. Copy Mechanism. We observe that name entities and rare words are sometimes deleted or replaced during SA stochastic sampling. They are difficult to be recovered because they usually have a low language model-suggested probability. Therefore, we propose a copy mechanism for SA sampling, inspired by that in Seq2Seq learning (Gu et al., 2016). Specifically, we allow the candidate sentence generator to copy the words from the original sentence x0 for word replacement and insertion. This is essentially enlarging the top-K sampling vocabulary with the words in x0 , given by ft,op = Wt,op ∪ {w0,1 , . . . , w0,l } W 0 (10) ft,op is the where op ∈ {replace,insert}. Thus, W actual vocabulary from which SA samples the word w∗ for replacement and insertion operation. While such vocabulary reduces the proposal space, it works well empirically because other low-ranked candidate words are either irrelevant or make the sentence disfluent; they"
2020.acl-main.28,Q18-1031,0,0.0349776,"19) use Metropolis–Hastings sampling (1953) for constrained sentence generation, achieving the state-of-the-art unsupervised paraphrasing performance. The main difference between their work and ours is that UPSA imposes the annealing temperature into the sampling process for better convergence to an optimum. In addition, we define our searching objective involving not only semantic similarity and language fluency, but also the expression diversity; we further propose a copy mechanism in our searching process. Recently, a few studies have applied editingbased approaches to sentence generation. Guu et al. (2018) propose a heuristic delete-retrieve-generate component for a supervised sequence-to-sequence 303 (Seq2Seq) model. Dong et al. (2019) learn the deletion and insertion operations for text simplification in a supervised way, where their groundtruth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in f"
2020.acl-main.28,2020.acl-main.707,1,0.78785,"ence generation. Guu et al. (2018) propose a heuristic delete-retrieve-generate component for a supervised sequence-to-sequence 303 (Seq2Seq) model. Dong et al. (2019) learn the deletion and insertion operations for text simplification in a supervised way, where their groundtruth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in fact a greedy algorithm. In NLP, beam search (BS, Tillmann et al. 1997) is widely applied to sentence generation. BS maintains a k-best list in a partially greedy fashion during left-to-right (or right-to-left) decoding (Anderson et al., 2017; Zhou and Rush, 2019). By contrast, UPSA is local search with distributed edits over the entire sentence. Moreover, UPSA is able to make use of the original sentence as an initial state of searching, whereas BS usually works in the decoder of a Seq2Seq model and is not applicable to unsupervised paraphrasing. 3 Approach In this section, we p"
2020.acl-main.28,D17-1126,0,0.0410463,"Missing"
2020.acl-main.28,D18-1421,0,0.0287354,"an-written paraphrase pairs is expensive and labor-intensive. Further, existing paraphrase datasets are domain-specific: the Quora dataset only contains question sentences, and thus, supervised paraphrase models do not generalize well to new domains (Li et al., 2019). On the other hand, researchers synthesize pseudo-paraphrase pairs by clustering news events (Barzilay and Lee, 2003), crawling tweets of the same topic (Lan et al., 2017), or translating bi-lingual datasets (Wieting and Gimpel, 2017), but these methods typically yield noisy training sets, leading to low paraphrasing performance (Li et al., 2018). As a result, unsupervised methods would largely benefit paraphrase generation as no parallel data are 1 Code and data available at: https://github.com/ Liuxg16/UPSA 2 https://www.kaggle.com/c/quora-question-pairs 302 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 302–312 c July 5 - 10, 2020. 2020 Association for Computational Linguistics needed. With the help of deep learning, researchers are able to generate paraphrases by sampling from a neural network-defined probabilistic distribution, either in a continuous latent space (Bowman et al., 201"
2020.acl-main.28,J83-1001,0,0.644195,"s of editing operations (i.e., insertion, replacement, and deletion). At each step, UPSA proposes a candidate modification of the sentence, which is accepted or rejected according to a certain acceptance rate (only accepted modifications are shown). Although sentences are discrete, we make an analogue in the continuous real x-axis where the distance of two sentences is roughly given by the number of edits. Introduction Paraphrasing aims to restate one sentence as another with the same meaning, but different wordings. It constitutes a corner stone in many NLP tasks, such as question answering (Mckeown, 1983), information retrieval (Knight and Marcu, 2000), and dialogue systems (Shah et al., 2018). However, automatically generating accurate and different-appearing paraphrases is a still challenging research problem, due to the complexity of natural language. Conventional approaches (Prakash et al., 2016; Gupta et al., 2018) model the paraphrase generation as a supervised encoding-decoding problem, inspired by machine translation systems. Usually, such models require massive parallel samples for training. In machine translation, for example, the WMT 2014 English-German dataset contains 4.5M sentenc"
2020.acl-main.28,W16-6625,0,0.0225211,"and the input. • We propose a copy mechanism as one of our search actions of simulated annealing to address rare words. • We achieve the state-of-the-art performance on four benchmark datasets compared with previous unsupervised paraphrase generators, largely reducing the performance gap between unsupervised and supervised paraphrasing. We outperform most domain-adapted paraphrase generators, and even a supervised one on the Wikianswers dataset. 2 Related Work In early years, paraphrasing was typically accomplished by exploiting linguistic knowledge (Mckeown, 1983; Ellsworth and Janin, 2007; Narayan et al., 2016) and statistical machine translation methods (Quirk et al., 2004; Dolan et al., 2004). Recently, deep neural networks have become a prevailing approach to text generation, where paraphrasing is often formulated as a supervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample sentences from a learned"
2020.acl-main.28,W14-3316,0,0.0682153,"Missing"
2020.acl-main.28,P02-1040,0,0.114331,"oses a candimultiplicative. date word w∗ for the kth step, the resulting canExpression Diversity. The expression diversity didate sentence becomes x∗ = (wt,1 , . . . , wt,k−1 , scoring function computes the lexical difference of w∗ , wt,k+1 . . . , wt,lt ). The insertion operation two sentences. We adopt a BLEU-induced function works similarly. to penalize the repetition of the words and phrases Here, the candidate word is sampled from a probin the input sentence: abilistic distribution, induced by the objective function (2): fexp (x∗ , x0 ) = (1 − BLEU(x∗ , x0 ))S , (5) where the BLEU score (Papineni et al., 2002) computes a length-penalized geometric mean of n-gram precision (n = 1, · · · , 4). S coordinates the importance of fexp (xt , x0 ) in the objective function (2). Language Fluency. Despite semantic preservation and expression diversity, the candidate paraphrase should be a fluent sentence by itself. We use a separately trained (forward) language model −→ (denoted as LM) to compute the likelihood of the candidate paraphrase as our fluency scoring function: fflu (x∗ ) = k=l Y∗ → (w∗,k |w∗,1 , . . . , w∗,k−1 ), (6) p− LM k=1 where l∗ is the length of x∗ and w∗,1 , . . . , w∗,l are words of x∗ . H"
2020.acl-main.28,D14-1162,0,0.0841998,"cluding semantic preservation fsem , expression diversity fexp , and language fluency fflu . Thus, our searching objective is to maximize f (x) = fsem (x, x0 ) · fexp (x, x0 ) · fflu (x), (2) where x0 is the input sentence. Semantic Preservation. A paraphrase is expected to capture all the key semantics of the original sentence. Thus, we leverage the cosine function of keyword embeddings to measure if the key focus of the candidate paraphrase is the same as the input. Specifically, we extract the keywords of the input sentence x0 by the Rake system (Rose et al., 2010) and embed them by GloVE (Pennington et al., 2014). For each keyword, we find the closest word in the candidate paraphrase x∗ in terms of the cosine similarity. Our keyword-based semantic preservation score is given by the lowest cosine similarity among all the keywords, i.e., the least matched keyword: fsem,key (x∗ , x0 ) = If the proposal is accepted, xt+1 = x∗ , or otherwise, xt+1 = xt . 304 min max{cos(w∗,j , e)}, e∈keywords(x0 ) j (3) where w∗,j is the jth word in the sentence x∗ ; e is an extracted keyword of x0 . Bold letters indicate embedding vectors. In addition to keyword embeddings, we also adopt a sentence-level similarity functi"
2020.acl-main.28,C16-1275,0,0.518552,"analogue in the continuous real x-axis where the distance of two sentences is roughly given by the number of edits. Introduction Paraphrasing aims to restate one sentence as another with the same meaning, but different wordings. It constitutes a corner stone in many NLP tasks, such as question answering (Mckeown, 1983), information retrieval (Knight and Marcu, 2000), and dialogue systems (Shah et al., 2018). However, automatically generating accurate and different-appearing paraphrases is a still challenging research problem, due to the complexity of natural language. Conventional approaches (Prakash et al., 2016; Gupta et al., 2018) model the paraphrase generation as a supervised encoding-decoding problem, inspired by machine translation systems. Usually, such models require massive parallel samples for training. In machine translation, for example, the WMT 2014 English-German dataset contains 4.5M sentence pairs (Neidert et al., 2014). However, the training corpora for paraphrasing are usually small. The widely-used Quora dataset2 only contains 140K pairs of paraphrases; constructing such human-written paraphrase pairs is expensive and labor-intensive. Further, existing paraphrase datasets are domai"
2020.acl-main.28,W04-3219,0,0.160841,"actions of simulated annealing to address rare words. • We achieve the state-of-the-art performance on four benchmark datasets compared with previous unsupervised paraphrase generators, largely reducing the performance gap between unsupervised and supervised paraphrasing. We outperform most domain-adapted paraphrase generators, and even a supervised one on the Wikianswers dataset. 2 Related Work In early years, paraphrasing was typically accomplished by exploiting linguistic knowledge (Mckeown, 1983; Ellsworth and Janin, 2007; Narayan et al., 2016) and statistical machine translation methods (Quirk et al., 2004; Dolan et al., 2004). Recently, deep neural networks have become a prevailing approach to text generation, where paraphrasing is often formulated as a supervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample sentences from a learned latent space (Bowman et al., 2016; Zhang et al., 2019; Bao et a"
2020.acl-main.28,P19-1332,0,0.592499,"lly, such models require massive parallel samples for training. In machine translation, for example, the WMT 2014 English-German dataset contains 4.5M sentence pairs (Neidert et al., 2014). However, the training corpora for paraphrasing are usually small. The widely-used Quora dataset2 only contains 140K pairs of paraphrases; constructing such human-written paraphrase pairs is expensive and labor-intensive. Further, existing paraphrase datasets are domain-specific: the Quora dataset only contains question sentences, and thus, supervised paraphrase models do not generalize well to new domains (Li et al., 2019). On the other hand, researchers synthesize pseudo-paraphrase pairs by clustering news events (Barzilay and Lee, 2003), crawling tweets of the same topic (Lan et al., 2017), or translating bi-lingual datasets (Wieting and Gimpel, 2017), but these methods typically yield noisy training sets, leading to low paraphrasing performance (Li et al., 2018). As a result, unsupervised methods would largely benefit paraphrase generation as no parallel data are 1 Code and data available at: https://github.com/ Liuxg16/UPSA 2 https://www.kaggle.com/c/quora-question-pairs 302 Proceedings of the 58th Annual M"
2020.acl-main.28,W04-1013,0,0.0290802,"-SVG-eq (Gupta et al., 2018), Pointer-generator (See et al., 2017), the Transformer (Vaswani et al., 2017), and the decomposable neural paraphrase generator (DNPG, Li et al., 2019). DNPG has been reported as the state-of-the-art supervised paraphrase generator. To better compare UPSA with all paraphrasing settings, we also include domain-adapted supervised paraphrase generators that are trained in a source domain but tested in a target domain, including shallow fusion (Gulcehre et al., 2015) and multitask learning (MTL, Domhan and Hieber 2017). We adopt BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores as automatic metrics to evaluate model performance. Sun and Zhou (2012) observe that BLEU and ROUGE could not measure the diversity between the generated and the original sentences, and propose the iBLEU variant by penalizing by the similarity with the original sentence. Therefore, we regard the iBLEU score as our major metric, which is also adopted in Li et al. (2019). In addition, we also conduct human evaluation in our experiments (detailed later). 4.3 Implementation Details Our method involves unsupervised language modeling (forward and backward), realized by two-layer LSTM with 30"
2020.acl-main.28,P19-1605,0,0.0197168,"upervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample sentences from a learned latent space (Bowman et al., 2016; Zhang et al., 2019; Bao et al., 2019). But the generated sentences are less controllable and suffer from the error accumulation problem in VAE’s decoding phase (Miao et al., 2019). Roy and Grangier (2019) introduce an unsupervised model based on vector-quantized autoencoders (Van den Oord et al., 2017). But their work mainly focuses on generating sentences for data augmentation instead of paraphrasing itself. Miao et al. (2019) use Metropolis–Hastings sampling (1953) for constrained sentence generation, achieving the state-of-the-art unsupervised paraphrasing performance. The main difference between their work and ours is that UPSA imposes the annealing temperature into the sampling process for better convergence to an optimum. In addition, we define our searching objective involving not only"
2020.acl-main.28,2020.acl-main.452,1,0.877759,"ased approaches to sentence generation. Guu et al. (2018) propose a heuristic delete-retrieve-generate component for a supervised sequence-to-sequence 303 (Seq2Seq) model. Dong et al. (2019) learn the deletion and insertion operations for text simplification in a supervised way, where their groundtruth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in fact a greedy algorithm. In NLP, beam search (BS, Tillmann et al. 1997) is widely applied to sentence generation. BS maintains a k-best list in a partially greedy fashion during left-to-right (or right-to-left) decoding (Anderson et al., 2017; Zhou and Rush, 2019). By contrast, UPSA is local search with distributed edits over the entire sentence. Moreover, UPSA is able to make use of the original sentence as an initial state of searching, whereas BS usually works in the decoder of a Seq2Seq model and is not applicable to unsupervised paraphrasing. 3 Approach"
2020.acl-main.28,P17-1099,0,0.0403138,"eported to be the state-of-the-art VAE. We adopted the published source code and generated paraphrases for comparison. CGMH. Miao et al. (2019) use Metropolis– Hastings sampling in the word space for constrained sentence generation. It is shown to outperform latent space sampling as in VAE, and is the state-of-the-art unsupervised paraphrasing approach. We also adopted the published source code and generated paraphrases for comparison. We further compare UPSA with supervised Seq2Seq paraphrase generators: ResidualLSTM (Prakash et al., 2016), VAE-SVG-eq (Gupta et al., 2018), Pointer-generator (See et al., 2017), the Transformer (Vaswani et al., 2017), and the decomposable neural paraphrase generator (DNPG, Li et al., 2019). DNPG has been reported as the state-of-the-art supervised paraphrase generator. To better compare UPSA with all paraphrasing settings, we also include domain-adapted supervised paraphrase generators that are trained in a source domain but tested in a target domain, including shallow fusion (Gulcehre et al., 2015) and multitask learning (MTL, Domhan and Hieber 2017). We adopt BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores as automatic metrics to evaluate model performan"
2020.acl-main.28,N18-3006,0,0.0495463,"Missing"
2020.acl-main.28,P12-2008,0,0.229831,"e Transformer (Vaswani et al., 2017), and the decomposable neural paraphrase generator (DNPG, Li et al., 2019). DNPG has been reported as the state-of-the-art supervised paraphrase generator. To better compare UPSA with all paraphrasing settings, we also include domain-adapted supervised paraphrase generators that are trained in a source domain but tested in a target domain, including shallow fusion (Gulcehre et al., 2015) and multitask learning (MTL, Domhan and Hieber 2017). We adopt BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores as automatic metrics to evaluate model performance. Sun and Zhou (2012) observe that BLEU and ROUGE could not measure the diversity between the generated and the original sentences, and propose the iBLEU variant by penalizing by the similarity with the original sentence. Therefore, we regard the iBLEU score as our major metric, which is also adopted in Li et al. (2019). In addition, we also conduct human evaluation in our experiments (detailed later). 4.3 Implementation Details Our method involves unsupervised language modeling (forward and backward), realized by two-layer LSTM with 300 hidden units and trained specifically on each dataset with non-parallel sente"
2020.acl-main.28,P19-1199,0,0.0870125,"methods (Quirk et al., 2004; Dolan et al., 2004). Recently, deep neural networks have become a prevailing approach to text generation, where paraphrasing is often formulated as a supervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample sentences from a learned latent space (Bowman et al., 2016; Zhang et al., 2019; Bao et al., 2019). But the generated sentences are less controllable and suffer from the error accumulation problem in VAE’s decoding phase (Miao et al., 2019). Roy and Grangier (2019) introduce an unsupervised model based on vector-quantized autoencoders (Van den Oord et al., 2017). But their work mainly focuses on generating sentences for data augmentation instead of paraphrasing itself. Miao et al. (2019) use Metropolis–Hastings sampling (1953) for constrained sentence generation, achieving the state-of-the-art unsupervised paraphrasing performance. The main difference between their work"
2020.acl-main.28,P19-1503,0,0.0213944,"uth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in fact a greedy algorithm. In NLP, beam search (BS, Tillmann et al. 1997) is widely applied to sentence generation. BS maintains a k-best list in a partially greedy fashion during left-to-right (or right-to-left) decoding (Anderson et al., 2017; Zhou and Rush, 2019). By contrast, UPSA is local search with distributed edits over the entire sentence. Moreover, UPSA is able to make use of the original sentence as an initial state of searching, whereas BS usually works in the decoder of a Seq2Seq model and is not applicable to unsupervised paraphrasing. 3 Approach In this section, we present our novel UPSA framework that uses simulated annealing (SA) for unsupervised paraphrasing. In particular, we first present the general SA algorithm and then design our searching objective and searching actions (i.e., candidate sentence generator) for paraphrasing. 3.1 Th"
2020.acl-main.563,N19-1423,0,0.0940548,"Missing"
2020.acl-main.563,P19-1546,0,0.408049,"´c et al., 2017; Zhong et al., 2018; Chao and Lane, 2019) which neglects the use of dialogue history. Recent researches attempt to address this problem through introducing historical dialogue information into the prediction of slot-value pairs. Most of them leverage a naive attention between slots and concatenated historical utterances (Wu et al., 2019; Zhou and Small, 2019; Gao et al., 2019b; Zhang et al., 2019; Le et al., 2020a,b) or only utilize partial history (Ren et al., 2019; Kim et al., 2019; Sharma et al., 2019) or lack direct interactions between slots and history (Ren et al., 2018; Lee et al., 2019; Goel et al., 2019). Briefly, these methods are deficient in exploiting relevant context from dialogue history. 6322 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6322–6333 c July 5 - 10, 2020. 2020 Association for Computational Linguistics This paper was retracted. For more information, see https://aclanthology.org/2020.acl-main.563. adjusting the weight of each slot. To the best of our knowledge, our method is the first to address the slot imbalance problem in DST. • Experimental results show that our model achieves state-of-the-art performan"
2020.acl-main.563,P18-1133,0,0.0934332,"redictions while SUMBT fails. guage understanding modules to predict the current dialogue state (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014) or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons besides the ontology, and they are hard to extend and scale to new domains. Recent neural network models are proposed for further improvements (Mrkˇsi´c et al., 2015; Hori et al., 2016; Mrkˇsi´c et al., 2017; Lei et al., 2018; Xu and Hu, 2018; Zhong et al., 2018; Nouri and Hosseini-Asl, 2018; Wu et al., 2019; Ren et al., 2019; Balaraman and Magnini, 2019). Ren et al. (2018) and Lee et al. (2019) use an RNN to encode the slot-related information of each turn, where slots can not attend to relevant information of past turns directly. Sharma et al. (2019) employ a heuristic rule to extract partial dialogue history and then integrate the historical information into prediction in a coarse manner. Goel et al. (2019) encode the dialogue history into a hidden state and then simply combine it with the slot to make decision"
2020.acl-main.563,N19-1057,0,0.207776,"rs accomplish tasks through spoken interactions (Young, 2002; Young et al., 2013; Gao et al., 2019a). Dialogue state tracking (DST) is an essential part of dialogue management in task-oriented dialogue systems. Given current utterances and dialogue history, DST aims to determine the set of † Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. ∗ Yang Feng is the corresponding author. 1 Code is available at https://github.com/ictnlp/CHAN-DST As Table 1 shows, the dialogue state is usually dependent on relevant context in the dialogue history, which is proven in previous studies (Sharma et al., 2019; Wu et al., 2019). However, traditional DST models usually determine dialogue states by considering only utterances at current turn (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018; Chao and Lane, 2019) which neglects the use of dialogue history. Recent researches attempt to address this problem through introducing historical dialogue information into the prediction of slot-value pairs. Most of them leverage a naive attention between slots and concatenated historical utterances (Wu et al., 2019; Zhou and Small, 2019; Gao et al., 2019b; Zhang et al., 2019; Le et al., 2020a,b"
2020.acl-main.563,P18-1158,0,0.0295864,"estination restaurant-food restaurant-book-people hotel-stars attraction-area hotel-price-range hotel-type attraction-type hotel-area restaurant-price-range restaurant-area train-leave-at hotel-internet hotel-parking hotel-name hotel-book-stay hotel-book-people hotel-book-day restaurant-book-time restaurant-book-day -0.2 taxi-arrive-by taxi-leave-at taxi-departure taxi-destination attraction-name train-book-people restaurant-name train-arrive-by -0.1 Percentage 6329 This paper was retracted. For more information, see https://aclanthology.org/2020.acl-main.563. et al., 2016; Ying et al., 2018; Wang et al., 2018; Xing et al., 2018; Aujogue and Aussem, 2019; Naik et al., 2018; Liu and Chen, 2019). Conclusion Acknowledgments We thank the anonymous reviewers for their insightful comments. This work was supported by National Key R&D Program of China (NO. 2017YFE0192900). References Jianfeng Gao, Michel Galley, Lihong Li, et al. 2019a. Neural approaches to conversational ai. FoundaR in Information Retrieval, 13(2tions and Trends 3):127–298. Shuyang Gao, Abhishek Sethi, Sanchit Agarwal, Tagyoung Chung, Dilek Hakkani-Tur, and Amazon Alexa AI. 2019b. Dialog state tracking: A neural reading comprehension appr"
2020.acl-main.563,P19-1543,0,0.0185449,"-price-range hotel-type attraction-type hotel-area restaurant-price-range restaurant-area train-leave-at hotel-internet hotel-parking hotel-name hotel-book-stay hotel-book-people hotel-book-day restaurant-book-time restaurant-book-day -0.2 taxi-arrive-by taxi-leave-at taxi-departure taxi-destination attraction-name train-book-people restaurant-name train-arrive-by -0.1 Percentage 6329 This paper was retracted. For more information, see https://aclanthology.org/2020.acl-main.563. et al., 2016; Ying et al., 2018; Wang et al., 2018; Xing et al., 2018; Aujogue and Aussem, 2019; Naik et al., 2018; Liu and Chen, 2019). Conclusion Acknowledgments We thank the anonymous reviewers for their insightful comments. This work was supported by National Key R&D Program of China (NO. 2017YFE0192900). References Jianfeng Gao, Michel Galley, Lihong Li, et al. 2019a. Neural approaches to conversational ai. FoundaR in Information Retrieval, 13(2tions and Trends 3):127–298. Shuyang Gao, Abhishek Sethi, Sanchit Agarwal, Tagyoung Chung, Dilek Hakkani-Tur, and Amazon Alexa AI. 2019b. Dialog state tracking: A neural reading comprehension approach. In 20th Annual Meeting of the Special Interest Group on Discourse and Dialogue,"
2020.acl-main.563,D18-1299,0,0.0611422,"Missing"
2020.acl-main.563,P15-2130,0,0.0471484,"Missing"
2020.acl-main.563,P17-1163,0,0.102841,"Missing"
2020.acl-main.563,D19-1196,0,0.612489,"Missing"
2020.acl-main.563,W13-4067,0,0.165105,"Missing"
2020.acl-main.563,E17-1042,0,0.108691,"Missing"
2020.acl-main.563,W13-4065,0,0.225425,"Missing"
2020.acl-main.563,P19-1078,0,0.472121,"Missing"
2020.acl-main.563,N16-1174,0,0.0826114,"Missing"
2020.acl-main.563,N16-1000,0,0.223513,"Missing"
2020.acl-main.563,P18-1134,0,0.104761,"UMBT fails. guage understanding modules to predict the current dialogue state (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014) or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons besides the ontology, and they are hard to extend and scale to new domains. Recent neural network models are proposed for further improvements (Mrkˇsi´c et al., 2015; Hori et al., 2016; Mrkˇsi´c et al., 2017; Lei et al., 2018; Xu and Hu, 2018; Zhong et al., 2018; Nouri and Hosseini-Asl, 2018; Wu et al., 2019; Ren et al., 2019; Balaraman and Magnini, 2019). Ren et al. (2018) and Lee et al. (2019) use an RNN to encode the slot-related information of each turn, where slots can not attend to relevant information of past turns directly. Sharma et al. (2019) employ a heuristic rule to extract partial dialogue history and then integrate the historical information into prediction in a coarse manner. Goel et al. (2019) encode the dialogue history into a hidden state and then simply combine it with the slot to make decisions. These models a"
2020.acl-main.563,P18-1135,0,0.22343,"systems. Given current utterances and dialogue history, DST aims to determine the set of † Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. ∗ Yang Feng is the corresponding author. 1 Code is available at https://github.com/ictnlp/CHAN-DST As Table 1 shows, the dialogue state is usually dependent on relevant context in the dialogue history, which is proven in previous studies (Sharma et al., 2019; Wu et al., 2019). However, traditional DST models usually determine dialogue states by considering only utterances at current turn (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018; Chao and Lane, 2019) which neglects the use of dialogue history. Recent researches attempt to address this problem through introducing historical dialogue information into the prediction of slot-value pairs. Most of them leverage a naive attention between slots and concatenated historical utterances (Wu et al., 2019; Zhou and Small, 2019; Gao et al., 2019b; Zhang et al., 2019; Le et al., 2020a,b) or only utilize partial history (Ren et al., 2019; Kim et al., 2019; Sharma et al., 2019) or lack direct interactions between slots and history (Ren et al., 2018; Lee et al., 2019; Goel et al., 2019"
2020.emnlp-main.275,D18-2029,0,0.0135609,"t training and the prior distribution k ∼ πθ (Kt ) at inference. Figure 1 clearly shows this discrepancy which will cause the decoder to have to generate with knowledge selected from the unfamiliar prior distribution. These issues lead to the gap between prior and posterior knowledge selection, which we try to deal with in this paper. Sentence Encoding. For any sentence sentt with Nw words at t-th turn, SKT uses a shared BERT (Devlin et al., 2019) to obtain the context aware word representations Hsent with d dims and t then converts them into the sentence representation hsent by mean pooling (Cer et al., 2018): t Hsent = BERT (sentt ) ∈ RNw ×d t .  hsent = Mean Hsent ∈ Rd t t (3) As a result, we obtain Hxt and hxt for the message kl xt , Hyt and hyt for the response2 yt , and Ht t and kl ht t for any knowledge sentence ktl ∈ Kt . Knowledge Selection. To utilize the dialogue history and selection history, two GRUs (Cho et al., 2014) are used to summarize them as corresponding states shist and skh t t with zero initialization:   d shist = GRUdial [hxt ; hyt ] , shist t t−1 ∈ R  sel  , (4) kt kh d skh t = GRUsel ht , st−1 ∈ R ksel where hxt , hyt and ht t are sentence vectors of message xt , resp"
2020.emnlp-main.275,K18-1048,0,0.0282681,"erage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical M"
2020.emnlp-main.275,P17-1171,0,0.0217147,"Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3426–3437, c November 16–20, 2020. 2020 Association for Computational Linguistics nan et al., 2019) since the inappropriate knowledge selection may prevent the model from leveraging the knowledge accurately (Lian et al., 2019), or even lead to an inappropri"
2020.emnlp-main.275,P19-1258,1,0.851583,"p their selection decisions in turn. The prior knowledge selection only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into tw"
2020.emnlp-main.275,N19-1423,0,0.100024,"see that the selected knowledge for response generation at training and inference is drawn from different distributions, i.e., the posterior distribution k ∼ qφ (Kt ) at training and the prior distribution k ∼ πθ (Kt ) at inference. Figure 1 clearly shows this discrepancy which will cause the decoder to have to generate with knowledge selected from the unfamiliar prior distribution. These issues lead to the gap between prior and posterior knowledge selection, which we try to deal with in this paper. Sentence Encoding. For any sentence sentt with Nw words at t-th turn, SKT uses a shared BERT (Devlin et al., 2019) to obtain the context aware word representations Hsent with d dims and t then converts them into the sentence representation hsent by mean pooling (Cer et al., 2018): t Hsent = BERT (sentt ) ∈ RNw ×d t .  hsent = Mean Hsent ∈ Rd t t (3) As a result, we obtain Hxt and hxt for the message kl xt , Hyt and hyt for the response2 yt , and Ht t and kl ht t for any knowledge sentence ktl ∈ Kt . Knowledge Selection. To utilize the dialogue history and selection history, two GRUs (Cho et al., 2014) are used to summarize them as corresponding states shist and skh t t with zero initialization:   d shi"
2020.emnlp-main.275,P16-1154,0,0.0375727,"on is based on the knowledge selected by the prior module, which is guided by the well-trained teacher, and we only update the green blocks at this stage. Finally, the knowledge ktsel is selected by sampling from the posterior distribution qφ (Kt ) = post at (Kt ) at training while selected with the highest probability over the prior distribution πθ (Kt ) = prior at (Kt ) at inference. Generation with Knowledge. SKT takes the concatenation of message xt and selected knowledge sentence ktsel as input and generates responses by the Transformer decoder (Vaswani et al., 2017) with copy mechanism (Gu et al., 2016). Though there are various models studying how to improve the generation quality based on the given knowledge, here, we simply follow the decoder of SKT and mainly focus on the knowledge selection issue. 3 Approach In this section, we show how to bridge the gap between prior and posterior knowledge selection in knowledge-grounded dialogue. Firstly, we design the Posterior Information Prediction Module (PIPM) to enhance the prior selection module with the necessary posterior information. Secondly, we design a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the k"
2020.emnlp-main.275,P84-1044,0,0.723364,"Missing"
2020.emnlp-main.275,P19-1002,1,0.84127,"election only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generati"
2020.emnlp-main.275,P18-1136,0,0.0236704,"nt responses which help their selection decisions in turn. The prior knowledge selection only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompos"
2020.emnlp-main.275,P18-1160,0,0.0174108,"However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3426–3437, c November 16–20, 2020. 2020 Association for Computational Linguistics nan et al., 2019) since the inappropriate knowledge selection may prevent the model from leveraging the knowledge accurately (Lian et al., 2019), or even lead to an inappropriate response. The"
2020.emnlp-main.275,D18-1255,0,0.46899,"jad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowl"
2020.emnlp-main.275,D19-1258,0,0.0160588,"lly use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3426–3437, c November 16–20, 2020. 2020 Association for Computational Linguistics nan et al., 2019) since the inappropriate knowledge selection may prevent the model from leveraging the knowledge accurately (Lian et al., 2019), or even lead to an inappropriate response. The example in Table 1"
2020.emnlp-main.275,D19-1187,0,0.0502611,"Missing"
2020.emnlp-main.275,W19-5917,0,0.0915067,"Missing"
2020.emnlp-main.275,D18-1073,0,0.0249398,"e to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Proce"
2020.emnlp-main.275,P18-1205,0,0.0287203,"dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with e"
2020.emnlp-main.275,P19-1539,0,0.0176836,"ich leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-g"
2020.emnlp-main.275,P19-1426,1,0.820016,"Missing"
2020.emnlp-main.275,N19-1123,0,0.304558,"nowledge sentences to generate different responses which help their selection decisions in turn. The prior knowledge selection only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently"
2020.emnlp-main.275,P17-1061,0,0.121102,"ge selection in knowledge-grounded dialogue. Firstly, we design the Posterior Information Prediction Module (PIPM) to enhance the prior selection module with the necessary posterior information. Secondly, we design a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection. 3.1 information for knowledge selection at inference. Following the typical setting in latent variable models (Lian et al., 2019; Kim et al., 2020), we use the response in bag-of-words (BOW) format (Zhao et al., 2017) as the posterior information. Here we take the dialogue context and the knowledge pool as input to generate the posterior information. We firstly summarize as the query of  histthe xcontext  this module qPI = s ; h and use it to get the t t t−1 PI attention distribution at (Kt ) over the knowledge pool Kt by Equation 6. Then, we summarize the knowledge representation in the knowledge pool with the weights in aPI t (Kt ) considered: h 1 i kt ktL d (7) hPI = h , · · · , h · aPI t t (Kt ) ∈ R . t t Secondly, we take the summarization of the dialogue context and the knowledge pool as input and"
2020.emnlp-main.275,D19-1193,0,0.0263512,"nowledge sentences to generate different responses which help their selection decisions in turn. The prior knowledge selection only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently"
2020.emnlp-main.76,P17-2061,0,0.0657001,"Missing"
2020.emnlp-main.76,P05-1066,0,0.263519,"Missing"
2020.emnlp-main.76,N19-1312,1,0.860959,"nce. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity. To alleviate this problem, we investigated existing adaptive objectives for other tasks and then proposed two heuristic criteria based on the"
2020.emnlp-main.76,P16-1014,0,0.0417886,"Missing"
2020.emnlp-main.76,P15-1001,0,0.160488,"ent frequencies, which roughly obey the Zipf’s Law (Zipf, 1949). Table 1 shows that there is a serious imbalance between high-frequency tokens and lowfrequency tokens. NMT models rarely have the opportunity to learn and generate those groundtruth low-frequency tokens in the training process. 1035 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1035–1046, c November 16–20, 2020. 2020 Association for Computational Linguistics Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary (Luong et al., 2015; Jean et al., 2015; Li et al., 2016; Pham et al., 2018) or adding extra components (G¨ulc¸ehre et al., 2016; Zhao et al., 2018), which bring in extra training complexity and computing expense. Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model (Luong and Manning, 2016), BPE-based model (Sennrich et al., 2016) and word-piece-based model (Wu et al., 2016). These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models. Although sub-word based NMT models hav"
2020.emnlp-main.76,W18-5712,0,0.0629753,"Missing"
2020.emnlp-main.76,D13-1176,0,0.243187,"Missing"
2020.emnlp-main.76,kocmi-bojar-2017-curriculum,0,0.0399389,"divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity."
2020.emnlp-main.76,D18-1149,0,0.0285784,"onsists of 1.25M sentence pairs from LDC corpora which has 27.9M Chinese words and 34.5M English words, respectively 2 . The data set MT02 was used as validation and MT03, MT04, MT05, MT06, MT08 were used for the test. We tokenized and lowercased English sentences using the Moses scripts3 , and segmented the Chinese sentences with the Stanford Segmentor4 . The two sides were further segmented into subword units using Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 30K merge operations separately. EN→RO. We used the preprocessed version of the WMT2016 English-Romanian dataset released by Lee et al. (2018) which includes 0.6M sentence pairs. We used news-dev 2016 for validation and news-test 2016 for the test. The two languages shared the same vocabulary generated with 40K merge operations of BPE. EN→DE. The training data is from WMT2016 which consists of about 4.5M sentences pairs with 118M English words and 111M German words. We chose the news test-2013 for validation and newstest 2014 for the test. 32K merge operations BPE were performed on both sides jointly. 4.2 • Baseline. The baseline system was implemented as the base model configuration in Vaswani et al. (2017) strictly. Since our meth"
2020.emnlp-main.76,2015.iwslt-evaluation.11,0,0.0427153,"airs with 118M English words and 111M German words. We chose the news test-2013 for validation and newstest 2014 for the test. 32K merge operations BPE were performed on both sides jointly. 4.2 • Baseline. The baseline system was implemented as the base model configuration in Vaswani et al. (2017) strictly. Since our method is further trained based on the pre-trained model at a low learning rate, we also trained another baseline model following the same procedures as our methods have except that all the target tokens share equal weights in the objective, denoted as Baseline-FT. • Fine Tuning (Luong and Manning, 2015). This model was first trained with all the training sentence pairs and then further trained with sentences containing more low-frequency tokens. To filter out sentences containing more low-frequency tokens, the method in Platanios et al. (2019) was adopted as our judging metric with a small modification: Systems We used the open-source toolkit called Fairseqpy (Edunov et al., 2017) released by Facebook as our Transformer system. 2 The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 3 http://www.statmt.org/moses/ 4 https://nlp.stan"
2020.emnlp-main.76,P16-1100,0,0.0343734,"Missing"
2020.emnlp-main.76,P15-1002,0,0.168608,"s appear with different frequencies, which roughly obey the Zipf’s Law (Zipf, 1949). Table 1 shows that there is a serious imbalance between high-frequency tokens and lowfrequency tokens. NMT models rarely have the opportunity to learn and generate those groundtruth low-frequency tokens in the training process. 1035 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1035–1046, c November 16–20, 2020. 2020 Association for Computational Linguistics Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary (Luong et al., 2015; Jean et al., 2015; Li et al., 2016; Pham et al., 2018) or adding extra components (G¨ulc¸ehre et al., 2016; Zhao et al., 2018), which bring in extra training complexity and computing expense. Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model (Luong and Manning, 2016), BPE-based model (Sennrich et al., 2016) and word-piece-based model (Wu et al., 2016). These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models. Although sub-word b"
2020.emnlp-main.76,J14-3004,0,0.0273182,"al., 2018), or the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019). The methods of trying to solve this can be divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we f"
2020.emnlp-main.76,P02-1040,0,0.106664,"ve (Equation 1), where all the target tokens have the same training weights. Then the model was further trained with the adaptive objective at a low learning rate. The weights were produced by the Exponential form (Equation 8). k) For computing stability, we used Count(y Cmedian instead of Count(yk ) in the weighting function, where Cmedian is the median of the token frequency. • Our K2. This system was trained following the same procedure as system Our Exp except that the training weights were produced by the Chi-Square form (Equation 9). The translation quality was evaluated by 4-gram BLEU (Papineni et al., 2002) with the multi-bleu.pl script. Besides, we used beam search with a beam size of 4 and a length penalty of 0.6 during the decoding process. 4.3 Hyperparameters There are two hyperparameters in our weighting functions, A and T. In our experiments, we fixed A to narrow search space and the overall weight range is [1, e]. We tuned another hyperparameter T on the validation data sets under the criteria proposed in section 3.2. The results are shown in Table 3. According to the results, the best hyperparameters differed across different language pairs. It is affected by the proportion of low-freque"
2020.emnlp-main.76,C18-1265,0,0.0163854,"et al., 2018). In contrast, our methods can improve the translation performance without extra cost and can be combined with other techniques. Class Imbalance. Class imbalance means the total number of some classes of data is far less than the total number of other classes. This problem can be observed in various tasks (Wei et al., 2013; Johnson and Khoshgoftaar, 2019). In NMT, the class imbalance problem might be the underlying cause of, among others, the gender-biased output problem (Vanmassenhove et al., 2019a), the inability of MT system to handle morphologically richer language correctly (Passban et al., 2018), or the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019). The methods of trying to solve this can be divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al.,"
2020.emnlp-main.76,W19-6622,0,0.043297,"slation of the rare words with the help of the memory network or the pointer network (Zhao et al., 2018; Pham et al., 2018). In contrast, our methods can improve the translation performance without extra cost and can be combined with other techniques. Class Imbalance. Class imbalance means the total number of some classes of data is far less than the total number of other classes. This problem can be observed in various tasks (Wei et al., 2013; Johnson and Khoshgoftaar, 2019). In NMT, the class imbalance problem might be the underlying cause of, among others, the gender-biased output problem (Vanmassenhove et al., 2019a), the inability of MT system to handle morphologically richer language correctly (Passban et al., 2018), or the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019). The methods of trying to solve this can be divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work als"
2020.emnlp-main.76,W18-2712,0,0.0299546,"Missing"
2020.emnlp-main.76,I08-2084,0,0.00895525,"16; Shao et al., 2018; Zhang et al., 2019). The methods of trying to solve this can be divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the"
2020.emnlp-main.76,N19-1119,0,0.109348,"Missing"
2020.emnlp-main.76,P16-1162,0,0.864739,"rocessing, pages 1035–1046, c November 16–20, 2020. 2020 Association for Computational Linguistics Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary (Luong et al., 2015; Jean et al., 2015; Li et al., 2016; Pham et al., 2018) or adding extra components (G¨ulc¸ehre et al., 2016; Zhao et al., 2018), which bring in extra training complexity and computing expense. Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model (Luong and Manning, 2016), BPE-based model (Sennrich et al., 2016) and word-piece-based model (Wu et al., 2016). These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models. Although sub-word based NMT models have achieved significant improvements, they still face the token-level frequency imbalance phenomenon, as Table 1 shows. Furthermore, current NMT models generally assign equal training weights to target tokens without considering their frequencies. It is very likely for NMT models to ignore the loss produced by the low-frequency tokens because of their small proportion in the tra"
2020.emnlp-main.76,D18-1510,1,0.904744,"Missing"
2020.emnlp-main.76,D17-1155,0,0.019346,"er- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity. To alleviate this problem, we investigated existing adaptive objectives for other tasks and then pro"
2020.emnlp-main.76,2020.acl-main.278,0,0.035646,"based methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity. To alleviate this problem, we investigated existing adaptive objectives for other tasks and then proposed two heuristic criteria based on the observations. Next,"
2020.emnlp-main.76,C18-1269,0,0.017616,"ng to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity. To alleviate this problem, we investigated existing adaptive objectives for other tasks and then proposed two heuristic cri"
2020.emnlp-main.76,P19-1426,1,0.880268,"Missing"
2020.emnlp-main.76,D18-1036,0,0.0674838,"ance between high-frequency tokens and lowfrequency tokens. NMT models rarely have the opportunity to learn and generate those groundtruth low-frequency tokens in the training process. 1035 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1035–1046, c November 16–20, 2020. 2020 Association for Computational Linguistics Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary (Luong et al., 2015; Jean et al., 2015; Li et al., 2016; Pham et al., 2018) or adding extra components (G¨ulc¸ehre et al., 2016; Zhao et al., 2018), which bring in extra training complexity and computing expense. Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model (Luong and Manning, 2016), BPE-based model (Sennrich et al., 2016) and word-piece-based model (Wu et al., 2016). These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models. Although sub-word based NMT models have achieved significant improvements, they still face the token-level frequency imbalance phenomenon, as Table"
2020.emnlp-main.77,P19-1425,0,0.0625487,"Missing"
2020.emnlp-main.77,P18-1163,1,0.875966,"Missing"
2020.emnlp-main.77,D19-1082,0,0.0189134,"dependency to further improve the diversity and complementariness among Index of Layers 3 4 5 0.6 2 0.2 0.4 1 Recently Transformer-based models (Vaswani et al., 2017; Ott et al., 2018; Wang et al., 2019a) become the de facto methods in Neural Machine Translation, owing to high parallelism and large model capacity. Some researchers devise new modules to improve the Transformer model, including combining the transformer unit with convolution networks (Wu et al., 2019; Zhao et al., 2019; Lioutas and Guo, 2020), improving the self-attention architecture(Fonollosa et al., 2019; Wang et al., 2019b; Hao et al., 2019), and deepening the Transformer architecture by dense connections(Wang et al., 2019a). Since our multi-unit framework makes no limitation about its unit, these models can be easily integrated into our multi-unit framework. There are also some works utilizing the power of multiple modules to capture complex feature representations in NMT. Shazeer et al. (2017) use a vast network and a sparse gated function to select from multiple experts (i.e., MLPs). Ahmed et al. (2017) train a weighted Transformer by replacing the multi-head attention by self-attention branches. Nevertheless, these models ign"
2020.emnlp-main.77,P17-4012,0,0.091145,"Missing"
2020.emnlp-main.77,W04-3250,0,0.142951,"ta, which is also tokenized and split using byte pair encoded (BPE) (Sennrich et al., 2016). We use newstest2017 as our validation set and newstest2018 as our test set, which contains 2001 and 3981 sentences, respectively. Evaluation. For evaluation, we train all the models with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respectively, and we select the model which performs the best on the validation set and report its performance on the test sets. We measure the caseinsensitive/case-sensitive BLEU scores using multibleu.perl 4 with the statistical significance test (Koehn, 2004) 5 for NIST Zh-En and WMT’14 En-De, respectively. For WMT’18 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script 6 . Model and Hyper-parameters. For all our experiments, we basically follow two model settings illustrated in (Vaswani et al., 2017), namely Transformer-Base and Transformer-Big. In Transformer-Base, we use 512 as hidden size, 2048 as filter size and 8 heads in multihead attention. In Transformer-Big, we use 1024 as hidden size, 4096 as filter size, and 16 heads in multihead attention. Besides, since noise types like swapping and reordering are of no"
2020.emnlp-main.77,D18-1317,0,0.220094,"Multihead attentions and position-wise feedforward network, together as a basic unit, plays an essential role in the success of Transformer models. Some researchers (Bapna et al., 2018; Wang et al., 2019a) propose to improve the model capacity by stacking this basic unit many times, i.e., deep Transformers, and achieve promising results. Nevertheless, as an orthogonal direction, investigation over multiple parallel units draws little attention. Compared with single unit models, multiple parallel unit layout is more expressive to capture complex information flow (Tao et al.; Meng et al., 2019; Li et al., 2018, 2019) in two aspects. First, this multiple-unit layout boosts the model by its varied feature space composition and different attentions over inputs. With this diversity, multi-unit models advance in expressiveness. Second, for the multiunit setting, one unit could mitigate the deficiency of other units and compose a more expressive network, in a complementary way. In this paper, we propose the Multi-Unit TransformErs (MUTE), which aim to promote the expressiveness of transformer models by introducing diverse and complementary parallel units. Merely combining multiple identical units in para"
2020.emnlp-main.77,W18-6301,0,0.0162281,"sequential dependency and bias module hardly affects the inference speed. 1054 4 0.300 3 0.275 2 0.225 2 0.200 1 1 2 3 Index of Units (a) MUTE 4 0.325 0.300 0.275 0.250 0.225 0.200 1 4 (b) MUTE + Bias 0.8 Conclusion In this paper, we propose Multi-Unit Transformers for NMT to improve the expressiveness by introducing diverse and complementary units. In addition, we propose two novel techniques, namely bias module and sequential dependency to further improve the diversity and complementariness among Index of Layers 3 4 5 0.6 2 0.2 0.4 1 Recently Transformer-based models (Vaswani et al., 2017; Ott et al., 2018; Wang et al., 2019a) become the de facto methods in Neural Machine Translation, owing to high parallelism and large model capacity. Some researchers devise new modules to improve the Transformer model, including combining the transformer unit with convolution networks (Wu et al., 2019; Zhao et al., 2019; Lioutas and Guo, 2020), improving the self-attention architecture(Fonollosa et al., 2019; Wang et al., 2019b; Hao et al., 2019), and deepening the Transformer architecture by dense connections(Wang et al., 2019a). Since our multi-unit framework makes no limitation about its unit, these models"
2020.emnlp-main.77,P16-1162,0,0.130721,"provement over dev set and test sets, compared with the “Transformer (Base)”. Bold represents the best performance. “†”: significantly better than “Transformer + Relative (Base)” (p < 0.05); “††”: significantly better than “Transformer + Relative (Base)” (p < 0.01). 2016) with 32k merge operations and a shared vocabulary for English and German. We use newstest2013 as our validation set and newstest2014 as our test set, which contain 3000 and 3003 sentences, respectively. For the WMT’18 Zh-En task, we use 18.4M preprocessed data, which is also tokenized and split using byte pair encoded (BPE) (Sennrich et al., 2016). We use newstest2017 as our validation set and newstest2018 as our test set, which contains 2001 and 3981 sentences, respectively. Evaluation. For evaluation, we train all the models with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respectively, and we select the model which performs the best on the validation set and report its performance on the test sets. We measure the caseinsensitive/case-sensitive BLEU scores using multibleu.perl 4 with the statistical significance test (Koehn, 2004) 5 for NIST Zh-En and WMT’14 En-De, respectively. For WMT’18 Zh-En, we use case"
2020.emnlp-main.77,N18-2074,0,0.0268174,"pl script 6 . Model and Hyper-parameters. For all our experiments, we basically follow two model settings illustrated in (Vaswani et al., 2017), namely Transformer-Base and Transformer-Big. In Transformer-Base, we use 512 as hidden size, 2048 as filter size and 8 heads in multihead attention. In Transformer-Big, we use 1024 as hidden size, 4096 as filter size, and 16 heads in multihead attention. Besides, since noise types like swapping and reordering are of no effect on Transformer models with absolute position information, the MUTE models are implemented using relative position information (Shaw et al., 2018). In addition, we only apply multi-unit methods to encoders, provided that the encoder is more crucial to model performance (Wang et al., 2019a). All experiments on MUTE models are conducted with TransformerBase setting. For the basic MUTE model, we use four identity units. As for the Biased MUTE and Sequentially Biased MUTE, we use four units including one identity unit, one swapping unit, one disorder unit and one masking unit. The sample rate pβ is set to 0.85. For more implementation details and experiments on sample rate, please refer to Appendix A and B. 4 https://github.com/moses-smt/mo"
2020.emnlp-main.77,P19-1176,0,0.307349,"rks and attention mechanism (Sutskever et al., 2014; Bahdanau 1 Code is available at https://github.com/Ellio ttYan/Multi Unit Transformer et al., 2014). Following the standard Sequence-toSequence architecture, Transformer models consist of two essential components, namely the encoder and decoder, which rely on stacking several identical layers, i.e., multihead attentions and positionwise feed-forward network. Multihead attentions and position-wise feedforward network, together as a basic unit, plays an essential role in the success of Transformer models. Some researchers (Bapna et al., 2018; Wang et al., 2019a) propose to improve the model capacity by stacking this basic unit many times, i.e., deep Transformers, and achieve promising results. Nevertheless, as an orthogonal direction, investigation over multiple parallel units draws little attention. Compared with single unit models, multiple parallel unit layout is more expressive to capture complex information flow (Tao et al.; Meng et al., 2019; Li et al., 2018, 2019) in two aspects. First, this multiple-unit layout boosts the model by its varied feature space composition and different attentions over inputs. With this diversity, multi-unit mode"
2020.emnlp-main.77,D19-1145,0,0.236302,"rks and attention mechanism (Sutskever et al., 2014; Bahdanau 1 Code is available at https://github.com/Ellio ttYan/Multi Unit Transformer et al., 2014). Following the standard Sequence-toSequence architecture, Transformer models consist of two essential components, namely the encoder and decoder, which rely on stacking several identical layers, i.e., multihead attentions and positionwise feed-forward network. Multihead attentions and position-wise feedforward network, together as a basic unit, plays an essential role in the success of Transformer models. Some researchers (Bapna et al., 2018; Wang et al., 2019a) propose to improve the model capacity by stacking this basic unit many times, i.e., deep Transformers, and achieve promising results. Nevertheless, as an orthogonal direction, investigation over multiple parallel units draws little attention. Compared with single unit models, multiple parallel unit layout is more expressive to capture complex information flow (Tao et al.; Meng et al., 2019; Li et al., 2018, 2019) in two aspects. First, this multiple-unit layout boosts the model by its varied feature space composition and different attentions over inputs. With this diversity, multi-unit mode"
2020.emnlp-main.77,D18-1100,0,0.0522988,"Missing"
2020.findings-emnlp.299,D14-1181,0,0.00244396,"tor. In a max-min game, the SCTKG generator generates essays to make discriminator consider them semantically match with given topics. Discriminator tries to distinguish the generated essays from real essays. In detail, suppose there are a total of m topics, the discriminator produces a sigmoid probability distribution over (m + 1)classes. The score at (m + 1)th index represents the probability that the sample is the generated text. The score at the j th (i ∈ {1, · · · , m}) index represents the probability that it belongs to the real text with the j th topic. Here the discriminator is a CNN (Kim, 2014) text classifier. 3.3 Training We introduce our two stage training method in this section. Stage 1: Similar to a conventional CVAE model, The loss of our SCTKG generator −logp(Y |c) can be expressed as: − L (θ; φ; c; Y )cvae = LKL + Ldecoder = KL (qφ (z|Y, c)kpθ (z|c)) (5) − Eqφ (z|Y,c) (log pD (Y |z, c)) . 2 As shown in Figure 2, in the topic knowledge graph, red circles denote the topic words and blue circles denote their neighboring concepts. Since we have already encoded topic information in the encoder, the graph vector gt in this section mainly focuses on the neighboring concept to assis"
2020.findings-emnlp.299,P02-1040,0,0.123797,"training set. This classifier achieves an accuracy of 0.83 on the test set. During training, the target sentiment labels s is computed by the sentiment classifier automatically. During inference, users can input any sentiment labels to control the sentiment for sentence generation. 4.2 Implementation Details We use the 200-dim pre-trained word embeddings provided by Song et al. (2018) and dimension of sentiment embeddings is 32. The vocabulary size is 50,000 and the batch size is 64. We use a manually tuning method to choose the hyperparameter values and the criterion used to select is BLEU (Papineni et al., 2002a). We use GRU with hidden size 512 for both encoder and decoder and the size of latent variables is 300. We implement the model with 3 The dataset can be download by https://pan. baidu.com/s/17pcfWUuQTbcbniT0tBdwFQ 4 https://github.com/baidu/Senta Tensorflow5 . The number of parameters is 68M and parameters of our model were randomly initialized over a uniform distribution [-0.08,0.08]. We pre-train our model for 80 epochs with the MLE method and adversarial training for 30 epochs. The average runtime for our model is 30 hours on a Tesla P40 GPU machine, which adversarial training takes most"
2020.findings-emnlp.299,N18-2028,0,0.0137321,"ally with three categories, i.e., positive, negative, neutral. This dataset was divided into a training set, validation set, and test set. We use an open-source Chinese sentiment classifier Senta4 to finetune on our manually-label training set. This classifier achieves an accuracy of 0.83 on the test set. During training, the target sentiment labels s is computed by the sentiment classifier automatically. During inference, users can input any sentiment labels to control the sentiment for sentence generation. 4.2 Implementation Details We use the 200-dim pre-trained word embeddings provided by Song et al. (2018) and dimension of sentiment embeddings is 32. The vocabulary size is 50,000 and the batch size is 64. We use a manually tuning method to choose the hyperparameter values and the criterion used to select is BLEU (Papineni et al., 2002a). We use GRU with hidden size 512 for both encoder and decoder and the size of latent variables is 300. We implement the model with 3 The dataset can be download by https://pan. baidu.com/s/17pcfWUuQTbcbniT0tBdwFQ 4 https://github.com/baidu/Senta Tensorflow5 . The number of parameters is 68M and parameters of our model were randomly initialized over a uniform dis"
2020.findings-emnlp.299,speer-havasi-2012-representing,0,0.0352498,"sues, we propose a novel Sentiment-Controllable topic-to-essay generator with a Topic Knowledge Graph enhanced decoder, named SCTKG, which is based on the conditional variational auto-encoder (CVAE) framework. To control the sentiment of the text, we inject the sentiment information in the encoder and decoder of our model to control the sentiment from both sentence level and word level. The sentiment labels are provided by a sentiment classifier during training. To fully utilize the knowledge, the model retrieves a topic knowledge graph from a largescale commonsense knowledge base ConceptNet (Speer and Havasi, 2012). Different from Yang et al. (2019), we preserve the graph structure of the knowledge base and propose a novel Topic Graph Attention (TGA) mechanism. TGA attentively reads the knowledge graphs and makes the full use of the structured, connected semantic information from the graphs for a better generation. In the meantime, to make the generated essays more closely surround the semantics of all input topics, we adopt adversarial training based on a multi-label discriminator. The discriminator provides the reward to the generator based on the coverage of the output on the given topics. Our contri"
2020.findings-emnlp.299,P19-1193,0,0.198648,"o sentences for each generated essay and denote positive sentences in red and negative sentences in blue. Sentences without sentiment label are showed in black. Introduction Topic-to-essay generation (TEG) task aims at generating human-like paragraph-level texts with only several given topics. It has plenty of practical applications, e.g., automatic advertisement generation, intelligent education, or assisting in keyword-based news writing (Lepp¨anen et al., 2017). Because of its great potential in practical use and scientific research, TEG has attracted a lot of interest. (Feng et al., 2018; Yang et al., 2019). However, In TEG, two problems are left to be solved: the neglect of sentiment beneath the text and the insufficient utilization of topic-related knowledge. ∗ This work is done when Lin Qiao was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China A well-performed essay generator should be able to generate multiple vivid and diverse essays when given the topic words. However, previous work tends to generate dull and generic texts. One of the reason is that they neglect the sentiment factor of the text. By modeling and controlling the sentiment of generated sentences, we can"
2020.findings-emnlp.299,W17-3528,0,0.0321639,"Missing"
2020.findings-emnlp.299,D18-1353,0,0.026516,"a commonsense knowledge to enrich the input information and adopt adversarial training to enhancing topic-consistency. However, both of them fail to consider the sentiment factor in the essay generation and fully utilize the external knowledge base. These limitations hinder them from generating high-quality texts. Besides, Chinese poetry generation is similar to our task, which can also be regarded as a topicto-sequence learning task. Li et al. (2018) adopt CVAE and adversarial training to generate diverse poetry. Yang et al. (2017) use CVAE with hybrid decoders to generate Chinese poems. And Yi et al. (2018) use reinforcement learning to directly improve the diversity criteria. However, their models are not directly applicable to TEG task. Because they do not take knowledge into account, their models cannot generate long and meaningful unstructured essays. Controllable Text Generation. Some work has explored style control mechanisms for text generation tasks. For example, Zhou and Wang (2017) use naturally annotated emoji Twitter data for emotional response generation. Wang and Wan (2018) propose adversarial training to control the sentiment of the texts. Chen et al. (2019) propose a semi-supervi"
2020.findings-emnlp.299,D18-1423,0,0.0418389,"Missing"
2020.findings-emnlp.299,P17-1061,0,0.0232594,"the topic words and blue circles denote their neighboring concepts. Since we have already encoded topic information in the encoder, the graph vector gt in this section mainly focuses on the neighboring concept to assist the generation. Topic Label Discriminator Here, θ and φ are the parameters of the prior network and recognition network, respectively. Intuitively, Ldecoder maximizes the sentence generation probability after sampling from the recognition net3339 work, while LKL minimizes the distance between the prior and recognition network. Besides, we use the annealing trick and BOW-loss (Zhao et al., 2017) to alleviate the vanishing latent variable problem in VAE training. Stage 2: After trained the SCTKG generator with equation (5), inspired by SeqGan (Yu et al., 2017), we adopt adversarial training between the generator and the topic label discriminator described in section 3.2. We refer reader to Yu et al. (2017) and Yang et al. (2019) for more details. 4 Experiments 4.1 Datasets We conduct experiments on the ZHIHU corpus (Feng et al., 2018). It consists of Chinese essays3 whose length is between 50 and 100. We select topic words based on frequency and remove rare topic words. The total numb"
2020.wmt-1.24,W14-3346,0,0.0287428,"mputed by, R(θ) = S X X Q(y|x(s) ; θ, α)∆(y, y (s) ), s=1 y∈S(x(s) ) (1) where x(s) and y (s) are two paired sentences. ∆ denotes a risk function and S(x(s) ) ∈ Y is a sampled subset of full search space. Then, the distribution Q is defined over space S(x(s) ), P (y|x(s) ; θ)α . 0 (s α y 0 ∈S(x(s)) P (y |x ; θ) (2) In practice, we use 4 candidates for each source sentence x(s) . Although the paper claimed that sampling generates better candidates, we find that the beam search performs better in our extremely large Transformer model. The risk function we used is the 4-gram sentence-level BLEU (Chen and Cherry, 2014) and we tune the optimal α via grid search within {0.005, 0.05, 0.5, 1, 1.5, 2}. Each model is fine-tuned for a max of 1000 steps. Q(y|x(s) ; θ, α) = P 3.6 Ensemble We split each training data into three shards among Clean, Noisy and Sample data respectively, which yields a total number of 9 shards. For each shard, we train seven varieties (two Deeper transformers, two Wider transformers, two AANs and one DTMT) with different model architecture. Then we apply four finetuning approaches on each model, thus the total number of models is quadrupled (about 200 models). For ensemble, it is difficul"
2020.wmt-1.24,P19-1425,0,0.037441,"Missing"
2020.wmt-1.24,W18-6408,0,0.0161268,"via both large-scale back-translation and knowledge distillation to enhance the models’ performance for all domains. Then, we propose iterative in-domain knowledge transfer, which transfers in-domain knowledge to the huge monolingual corpus (i.e., Chinese), and builds our in-domain synthetic corpus. In the following sections, we elaborate above techniques in detail. 241 3.2.1 Large-scale Back-Translation Back-translation is shown to be very effective to boost the performance of NMT models in both academic research (Hoang et al., 2018; Edunov et al., 2018) and previous years’ WMT competitions (Deng et al., 2018; Sun et al., 2019; Ng et al., 2019; Xia et al., 2019). Following their work, we also train baseline English-to-Chinese models with the parallel data provided by WMT2020. Both the Left-to-Right Transformer (L2R) and the Right-toLeft Transformer (R2L) are used to translate the filtered monolingual English corpus combined with the English side of golden parallel bitext to Chinese. Then the generated Chinese text and the original English text are regarded as the source side and target side, respectively. In practice, it costs us 7 days on 5 NVIDIA V100 GPU machines to generating all back-translat"
2020.wmt-1.24,D18-1045,0,0.32227,"c data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper, we start with an overview of model"
2020.wmt-1.24,W18-2703,0,0.0179191,"and in-domain synthetic data. The out-of-domain synthetic corpus is generated via both large-scale back-translation and knowledge distillation to enhance the models’ performance for all domains. Then, we propose iterative in-domain knowledge transfer, which transfers in-domain knowledge to the huge monolingual corpus (i.e., Chinese), and builds our in-domain synthetic corpus. In the following sections, we elaborate above techniques in detail. 241 3.2.1 Large-scale Back-Translation Back-translation is shown to be very effective to boost the performance of NMT models in both academic research (Hoang et al., 2018; Edunov et al., 2018) and previous years’ WMT competitions (Deng et al., 2018; Sun et al., 2019; Ng et al., 2019; Xia et al., 2019). Following their work, we also train baseline English-to-Chinese models with the parallel data provided by WMT2020. Both the Left-to-Right Transformer (L2R) and the Right-toLeft Transformer (R2L) are used to translate the filtered monolingual English corpus combined with the English side of golden parallel bitext to Chinese. Then the generated Chinese text and the original English text are regarded as the source side and target side, respectively. In practice, it"
2020.wmt-1.24,D16-1139,0,0.190281,"rmers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with a larger filter-size and the average attention based transformer (Zhang et al., 2018). For the RNMT, we use the deep transition based DTMT (Meng and Zhang, 2019) model. We finally ensemble four kinds of models in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algori"
2020.wmt-1.24,D17-1230,0,0.0274829,"domain knowledge transfer. In our experiments, we find that iteratively performing the in-domain knowledge transfer can further provide improvements (see Table 2). For each iteration, we replace the indomain synthetic data and retrain our models, and it costs about 10 days on 8 NVIDIA V100 GPU machines. For the final submission, the knowledge transfer is conducted twice. 3.4 Data Augmentation Aside from synthetic data generation, we also apply two data augmentation methods over our synthetic corpus. Firstly, adding synthetic/natural noises to training data is widely applied in the NLP fields (Li et al., 2017; Belinkov and Bisk, 2017; Cheng et al., 2019) to improve model robustness and enhance model performance. Therefore, we proposed to add token-level synthetic noises. Concretely, we perform random replace, random delete, and random permutation over our data. The probability of enabling each of the three operations is 0.1. We refer to this corrupted corpus as Noisy data. Secondly, as illustrated in (Edunov et al., 2018), sampling generation over back-translation shows its potential in building robust NMT systems. Consequently, we investigate the performance of sampled synthetic data. For back-tr"
2020.wmt-1.24,D19-1559,1,0.791509,"by several transition GRUs (T-GRUs). DTMT enhances the hidden-to-hidden transition with multiple non-linear transformations, as well as maintains a linear transformation path throughout this deep transition by the well-designed linear transformation mechanism to alleviate the vanishing gradient problem. This architecture has demonstrated its superiority over the conventional Transformer model and stacked RNN-based models in NMT (Meng and Zhang, 2019), and also achieves surprising performances on other NLP tasks, such as sequence labeling (Liu et al., 2019) and aspect-based sentiment analysis (Liang et al., 2019). In our experiments, we use the bidirectional deep transition encoder, where each directional deep transition block consists of 1 L-GRU and 4 T-GRU. The decoder contains a query transition block and the decoder transition block, each of which consists of 1 L-GRU and 4 T-GRU. Therefore the DTMT consists of a 5 layer encoder and a 10 layer decoder, with a hidden size of 1,024. We use 8 NVIDIA V100 GPUs to train each model for about three weeks and the batch size is set to 4,096 tokens per GPU. Average Attention Transformer 3 To introduce more diversity in our Transformer models, we use Average"
2020.wmt-1.24,P19-1233,1,0.820814,"a linear transformation enhanced GRU (L-GRU) followed by several transition GRUs (T-GRUs). DTMT enhances the hidden-to-hidden transition with multiple non-linear transformations, as well as maintains a linear transformation path throughout this deep transition by the well-designed linear transformation mechanism to alleviate the vanishing gradient problem. This architecture has demonstrated its superiority over the conventional Transformer model and stacked RNN-based models in NMT (Meng and Zhang, 2019), and also achieves surprising performances on other NLP tasks, such as sequence labeling (Liu et al., 2019) and aspect-based sentiment analysis (Liang et al., 2019). In our experiments, we use the bidirectional deep transition encoder, where each directional deep transition block consists of 1 L-GRU and 4 T-GRU. The decoder contains a query transition block and the decoder transition block, each of which consists of 1 L-GRU and 4 T-GRU. Therefore the DTMT consists of a 5 layer encoder and a 10 layer decoder, with a hidden size of 1,024. We use 8 NVIDIA V100 GPUs to train each model for about three weeks and the batch size is set to 4,096 tokens per GPU. Average Attention Transformer 3 To introduce"
2020.wmt-1.24,2015.iwslt-evaluation.11,0,0.0358315,"T systems. Consequently, we investigate the performance of sampled synthetic data. For back-translated data, we replace beam search with sampling in its generation. For in-domain synthetic data, we replace the golden Chinese with the back sampled pseudo Chinese sentences. We refer to the data with sampling generation as Sample data. As a special case, we refer to the without augmentation data as Clean data. 3.5 In-domain Finetuning We train the model on large-scale out-of-domain data until convergence and then finetune it on smallscale in-domain data, which is widely used for domain adaption (Luong and Manning, 2015; Li et al., 2019). Specifically, we take Chinese− →English test sets of WMT 17 and 18 as in-domain data, and filter out documents that are originally created in 242 English (Sun et al., 2019). We name above finetuning approach as normal finetuning. In all our finetuning experiments, we set the batch size to 4096, and finetune the model for around 400 steps1 on the in-domain data. Furthermore, the well-known problem of exposure bias in sequence-to-sequence generation becomes more serious under domain shift (Wang and Sennrich, 2020). To solve this issue, we further explore some advanced finetun"
2020.wmt-1.24,P19-2049,0,0.161662,"back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and"
2020.wmt-1.24,W19-5333,0,0.019301,"and knowledge distillation to enhance the models’ performance for all domains. Then, we propose iterative in-domain knowledge transfer, which transfers in-domain knowledge to the huge monolingual corpus (i.e., Chinese), and builds our in-domain synthetic corpus. In the following sections, we elaborate above techniques in detail. 241 3.2.1 Large-scale Back-Translation Back-translation is shown to be very effective to boost the performance of NMT models in both academic research (Hoang et al., 2018; Edunov et al., 2018) and previous years’ WMT competitions (Deng et al., 2018; Sun et al., 2019; Ng et al., 2019; Xia et al., 2019). Following their work, we also train baseline English-to-Chinese models with the parallel data provided by WMT2020. Both the Left-to-Right Transformer (L2R) and the Right-toLeft Transformer (R2L) are used to translate the filtered monolingual English corpus combined with the English side of golden parallel bitext to Chinese. Then the generated Chinese text and the original English text are regarded as the source side and target side, respectively. In practice, it costs us 7 days on 5 NVIDIA V100 GPU machines to generating all back-translated data. 3.2.2 Knowledge Distillati"
2020.wmt-1.24,P16-1009,0,0.18507,", we mainly exploit two different architectures in our approaches, namely Transformers and RNMT. For Transformers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with a larger filter-size and the average attention based transformer (Zhang et al., 2018). For the RNMT, we use the deep transition based DTMT (Meng and Zhang, 2019) model. We finally ensemble four kinds of models in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al.,"
2020.wmt-1.24,P16-1162,0,0.554201,", we mainly exploit two different architectures in our approaches, namely Transformers and RNMT. For Transformers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with a larger filter-size and the average attention based transformer (Zhang et al., 2018). For the RNMT, we use the deep transition based DTMT (Meng and Zhang, 2019) model. We finally ensemble four kinds of models in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al.,"
2020.wmt-1.24,P16-1159,0,0.0483208,"knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and results. Finally, we conclude our work in Section 5. 2 Model Architectures In this section,"
2020.wmt-1.24,W19-5341,0,0.381816,"In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and results. Finally, we conclude our work in Section 5. 2 Model Architectures In this section, we first describe the model architectures we use in the Chinese− →English Shared Task, including the Transformer-based (Vaswani et al., 2017) models and RNN-based (Bahdanau et al., 2014; Meng and Zhang, 2019) models. 2.1 Deeper Transformer As shown in previous studies (Wang et al., 2019; Sun et al., 2019), deeper Transformers with prenorm outperform its shallow counterparts on various machine translation benchmarks. In their work, increasing the encoder depth significantly improves the model performance, while they only introduce mild overhead in terms of speed in training and 239 Proceedings of the 5th Conference on Machine Translation (WMT), pages 239–247 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics inference, compared with increasing the decoder side depth. Hence, we train deeper Transformers with a deep encoder aiming for a better encoding representation."
2020.wmt-1.24,2020.acl-main.326,0,0.094302,"ion method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and results. Finally, we conclude our work in Section 5. 2 Model Architectures In this section, we first describe the mode"
2020.wmt-1.24,P19-1176,0,0.0318592,"submitted systems. In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and results. Finally, we conclude our work in Section 5. 2 Model Architectures In this section, we first describe the model architectures we use in the Chinese− →English Shared Task, including the Transformer-based (Vaswani et al., 2017) models and RNN-based (Bahdanau et al., 2014; Meng and Zhang, 2019) models. 2.1 Deeper Transformer As shown in previous studies (Wang et al., 2019; Sun et al., 2019), deeper Transformers with prenorm outperform its shallow counterparts on various machine translation benchmarks. In their work, increasing the encoder depth significantly improves the model performance, while they only introduce mild overhead in terms of speed in training and 239 Proceedings of the 5th Conference on Machine Translation (WMT), pages 239–247 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics inference, compared with increasing the decoder side depth. Hence, we train deeper Transformers with a deep encoder aiming for a better encodi"
2020.wmt-1.24,D19-1430,0,0.0177972,"els in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper,"
2020.wmt-1.24,P18-1166,0,0.124343,"is the highest among all submissions. 1 Introduction Our WeChat AI team participates in the WMT 2020 shared news translation task on Chinese→English. In this year’s translation task, we mainly focus on exploiting several effective model architectures, better data augmentation, training and model ensemble strategies. For model architectures, we mainly exploit two different architectures in our approaches, namely Transformers and RNMT. For Transformers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with a larger filter-size and the average attention based transformer (Zhang et al., 2018). For the RNMT, we use the deep transition based DTMT (Meng and Zhang, 2019) model. We finally ensemble four kinds of models in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the s"
2021.acl-long.228,W19-4828,0,0.0222437,"65 0.2 0.4 0.6 0.8 HSK Width (Normalized) MNLI-m Acc CoLA Mcc 35 20 mally, the mask M is defined as:  1, i∈I Mi = 0, otherwise 91.0 1.0 0.2 0.4 0.6 0.8 1.0 n  where I = round i × of the remained i=1 is the indices activations. Mag Mask masks out the activations with low magnitude. Therefore, this mask is dynamic, i.e., every hTl,i (∀i, l) has its own M. 78 77 0.2 0.4 0.6 0.8 HSK Width (Normalized) 1.0 layers, [SEP] is the most attended token for almost all training samples. Meanwhile, [SEP] frequently appears in the top three positions across all the layers. Similar phenomenon was found in Clark et al. (2019), where [SEP] receives high attention scores from itself and other tokens in the middle layers. Combining this phenomenon and the results in Figure 4 and Figure 5, it can be inferred that the representations of [SEP] is not a desirable source of knowledge for ROSITA and TinyBERT. We conjecture that this is because there exists some trivial patterns in the representations of [SEP], which prevents the student to extract the informative features that are more relevant to the task. 4.3.1 oN W 79 Figure 7: Results of width compression with different masking strategies on CoLA, SST-2, QNLI and MNLI"
2021.acl-long.228,N19-1423,0,0.452991,"chieve the same performance as extensive HSK distillation. Based on the second finding, we further propose an efficient KD paradigm to compress BERT, which does not require loading the teacher during the training of student. For two kinds of student models and computing devices, the proposed KD paradigm gives rise to training speedup of 2.7× ∼3.4×. 86.5 86.0 85.5 ROSITA6 TinyBERT4 85.0 0 10−3 10−2 10−1 Amount of HSK (Norma i(ed) 100 Figure 1: The Acc variation of ROSITA (Liu et al., 2021) and TinyBERT (Jiao et al., 2020) on QNLI with the increase of HSK. Introduction Since the launch of BERT (Devlin et al., 2019), pre-trained language models (PLMs) have been advancing the state-of-the arts (SOTAs) in a wide range of NLP tasks. At the same time, the growing ∗ 87.0 Work was done when Yuanxin Liu was an intern at Pattern Recognition Center, WeChat AI, Tencent Inc, China. † Zheng Lin is the corresponding author. size of PLMs has inspired a wave of research interest in model compression (Han et al., 2016) in the NLP community, which aims to facilitate the deployment of the powerful PLMs to resource-limited scenarios. Knowledge distillation (KD) (Hinton et al., 2015) is an effective technique in model compr"
2021.acl-long.228,P19-1282,0,0.0283087,"e found in MiniLMs (Wang et al., 2020a,b), which only use the teacher’s knowledge to guide the last layer of student. However, they only consider knowledge from the layer dimension, while we investigate the three dimensions of HSK. We explore a variety of strategies to determine feature importance for each single dimension. This is related to a line of studies called the attribution methods, which attempt to attribute a neural network’s prediction to the input features. The attention weights have also been investigated as an attribution method. However, prior work (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Brunner et al., 2020; Hao et al., 2020) finds that attention weights usually fail to correlate well with their contributions to the final prediction. This echoes with our finding that the original Att strategy performs poorly in length compression. However, the attention weights may play different roles in attribution and HSK distillation. Whether the findings in attribution are transferable to HSK distillation is still a problem that needs further investigation. 8 Conclusions and Future Work In this paper, we investigate the compression of HSK in BERT KD. We divide the HSK of BERT into thre"
2021.acl-long.228,D19-1441,0,0.0407769,"model compression (Han et al., 2016) in the NLP community, which aims to facilitate the deployment of the powerful PLMs to resource-limited scenarios. Knowledge distillation (KD) (Hinton et al., 2015) is an effective technique in model compression. In conventional KD, the student model is trained to imitate the teacher’s prediction over classes, i.e., the soft labels. Subsequently, Romero et al. (2015) find that the intermediate representations in the teacher’s hidden layers can also serve as a useful source of knowledge. As an initial attempt to introduce this idea to BERT compression, PKD (Sun et al., 2019) proposed to distill representations of the [CLS] token in BERT’s hidden layers, and later studies (Jiao et al., 2020; Sun et al., 2020; Hou et al., 2020; Liu et al., 2021) extend the distillation of hidden state knowledge (HSK) to all the tokens. In contrast to the previous work that attempts to increase the amount of HSK, in this paper we explore towards the opposite direction to “compress” HSK. We make the observation that although distilling HSK is helpful, the marginal utility diminishes quickly as the amount of HSK increases. To understand this effect, we conduct a series of analysis 292"
2021.acl-long.228,2020.acl-main.195,0,0.0709525,"ited scenarios. Knowledge distillation (KD) (Hinton et al., 2015) is an effective technique in model compression. In conventional KD, the student model is trained to imitate the teacher’s prediction over classes, i.e., the soft labels. Subsequently, Romero et al. (2015) find that the intermediate representations in the teacher’s hidden layers can also serve as a useful source of knowledge. As an initial attempt to introduce this idea to BERT compression, PKD (Sun et al., 2019) proposed to distill representations of the [CLS] token in BERT’s hidden layers, and later studies (Jiao et al., 2020; Sun et al., 2020; Hou et al., 2020; Liu et al., 2021) extend the distillation of hidden state knowledge (HSK) to all the tokens. In contrast to the previous work that attempts to increase the amount of HSK, in this paper we explore towards the opposite direction to “compress” HSK. We make the observation that although distilling HSK is helpful, the marginal utility diminishes quickly as the amount of HSK increases. To understand this effect, we conduct a series of analysis 2928 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference o"
2021.acl-long.228,2020.findings-emnlp.372,0,0.19617,"oved by extracting and distilling the crucial HSK, and 2) using a tiny fraction of HSK can achieve the same performance as extensive HSK distillation. Based on the second finding, we further propose an efficient KD paradigm to compress BERT, which does not require loading the teacher during the training of student. For two kinds of student models and computing devices, the proposed KD paradigm gives rise to training speedup of 2.7× ∼3.4×. 86.5 86.0 85.5 ROSITA6 TinyBERT4 85.0 0 10−3 10−2 10−1 Amount of HSK (Norma i(ed) 100 Figure 1: The Acc variation of ROSITA (Liu et al., 2021) and TinyBERT (Jiao et al., 2020) on QNLI with the increase of HSK. Introduction Since the launch of BERT (Devlin et al., 2019), pre-trained language models (PLMs) have been advancing the state-of-the arts (SOTAs) in a wide range of NLP tasks. At the same time, the growing ∗ 87.0 Work was done when Yuanxin Liu was an intern at Pattern Recognition Center, WeChat AI, Tencent Inc, China. † Zheng Lin is the corresponding author. size of PLMs has inspired a wave of research interest in model compression (Han et al., 2016) in the NLP community, which aims to facilitate the deployment of the powerful PLMs to resource-limited scenari"
2021.acl-long.228,2020.emnlp-main.242,0,0.0437266,"ated Work KD is widely studied in BERT compression. In addition to distilling the teacher’s predictions as in Hinton et al. (2015), researches have shown that the student’s performance can be improved by using the representations from intermediate BERT layers (Sun et al., 2019; Liu et al., 2021; Hou et al., 2020) and the self-attention distributions (Jiao et al., 2020; Sun et al., 2020). Typically, the knowledge is extensively distilled in a layer-wise manner. To fully utilize BERT’s knowledge, some recent work also proposed to combine multiple teacher layers in BERT KD (Passban et al., 2021; Li et al., 2020) or KD on Transformer-based NMT models (Wu et al., 2020). In contrast to these studies that attempt to increase the amount knowledge, we study BERT KD from the compression point of view. Similar idea can be found in MiniLMs (Wang et al., 2020a,b), which only use the teacher’s knowledge to guide the last layer of student. However, they only consider knowledge from the layer dimension, while we investigate the three dimensions of HSK. We explore a variety of strategies to determine feature importance for each single dimension. This is related to a line of studies called the attribution methods,"
2021.acl-long.228,D19-1002,0,0.0191234,"of view. Similar idea can be found in MiniLMs (Wang et al., 2020a,b), which only use the teacher’s knowledge to guide the last layer of student. However, they only consider knowledge from the layer dimension, while we investigate the three dimensions of HSK. We explore a variety of strategies to determine feature importance for each single dimension. This is related to a line of studies called the attribution methods, which attempt to attribute a neural network’s prediction to the input features. The attention weights have also been investigated as an attribution method. However, prior work (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Brunner et al., 2020; Hao et al., 2020) finds that attention weights usually fail to correlate well with their contributions to the final prediction. This echoes with our finding that the original Att strategy performs poorly in length compression. However, the attention weights may play different roles in attribution and HSK distillation. Whether the findings in attribution are transferable to HSK distillation is still a problem that needs further investigation. 8 Conclusions and Future Work In this paper, we investigate the compression of HSK in BERT KD. We divide"
2021.acl-long.268,D15-1166,0,0.0398039,"our approach, with 1.36, 1.50, and 0.63 BLEU improvements, respectively, compared to the Transformer baseline. The human evaluation further verifies that our approaches improve translation adequacy as well as fluency. 1 1 Introduction Neural Machine Translation (NMT) has achieved great success in recent years (Sutskever et al., 2014; ∗ Equal contribution. This work was done when Mengqi Miao was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. † Corresponding author. 1 Code is available at https://github.com/Mlair 77/nmt adequacy Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Meng and Zhang, 2019; Zhang et al., 2019a; Yan et al., 2020b), which generates accurate and fluent translation through modeling the next word conditioned on both the source sentence and partial translation. However, NMT faces the hallucination problem, i.e., translations are fluent but inadequate to the source sentences. One important reason is that the NMT model pays excessive attention to the partial translation to ensure fluency while failing to translate some segments of the source sentence (Weng et al., 2020b), which is actually the overconfidence of the Language M"
2021.acl-long.268,C16-1205,1,0.926239,"tant reason is that the NMT model pays excessive attention to the partial translation to ensure fluency while failing to translate some segments of the source sentence (Weng et al., 2020b), which is actually the overconfidence of the Language Model (LM). In the rest of this paper, the LM mentioned refers to the LM mechanism involved in NMT. Many recent studies attempt to deal with the inadequacy problem of NMT from two main aspects. One is to improve the architecture of NMT, such as adding a coverage vector to track the attention history (Tu et al., 2016), enhancing the crossattention module (Meng et al., 2016, 2018; Weng et al., 2020b), and dividing the source sentence into past and future parts (Zheng et al., 2019). The other aims to propose a heuristic adequacy metric or objective based on the output of NMT. Tu et al. (2017) and Kong et al. (2019) enhance the model’s reconstruction ability and increase the coverage ratio of the source sentences by translations, respectively. Although some researches (Tu et al., 2017; Kong et al., 2019; Weng et al., 2020b) point out that the lack of adequacy is due to the overconfidence of the LM, unfortunately, they do not propose effective solutions to the over"
2021.acl-long.268,D16-1096,0,0.0244082,"se Study. To better illustrate the translation quality of our approach, we show several translation examples in Appendix C. Our approach grasps more segments of the source sentences, which are mistranslated or neglected by the Transformer. 6 Related Work Translation Adequacy of NMT. NMT suffers from the hallucination and inadequacy problem for a long time (Tu et al., 2016; M¨uller et al., 2020; Wang and Sennrich, 2020; Lee et al., 2019). Many studies improve the architecture of NMT to alleviate the inadequacy issue, including tracking translation adequacy by coverage vectors (Tu et al., 2016; Mi et al., 2016), modeling a global representation of source side (Weng et al., 2020a), dividing the source sentence into past and future parts (Zheng et al., 2019), and multi-task learning to improve encoder and cross-attention modules in decoder (Meng et al., 2016, 2018; Weng et al., 2020b). They inductively increase the translation adequacy, while our approaches directly maximize the Margin between the NMT and the LM to prevent the LM from being overconfident. Other studies enhance the translation adequacy by adequacy metrics or additional optimization objectives. Tu et al. (2017) minimize the difference b"
2021.acl-long.268,P10-2041,0,0.0337908,"e LM is still required in inference, slowing down the inference speed largely. The research work descried in this paper has been supported by the National Nature Science Foundation of China (No. 12026606). The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper. Data Selection in NMT. Data selection and data filter methods have been widely used in NMT. To balance data domains or enhance the data quality generated by back-translation (Sennrich et al., 2016b), many approaches have been proposed, such as utilizing language models (Moore and Lewis, 2010; van der Wees et al., 2017; Zhang et al., 2020), translation models (JunczysDowmunt, 2018; Wang et al., 2019a), and curriculum learning (Zhang et al., 2019b; Wang et al., 2019b). Different from the above methods, our MSO dynamically combines language models with translation models for data selection during training, making full use of the models. 7 Conclusion We alleviate the problem of inadequacy translation from the perspective of preventing the LM from being overconfident. Specifically, we firstly propose an indicator of the overconfidence degree of the LM in NMT, i.e., Margin between the"
2021.acl-long.268,2020.amta-research.14,0,0.0755329,"Missing"
2021.acl-long.268,N19-4009,0,0.0172576,".01). “‡”: significantly better than the joint model NMT+LM (p&lt;0.01). * denotes the results come from the cited paper. 5.2 Results on En→Fr and Zh→En The results on WMT14 English-to-French (En→Fr) and WMT19 Chinese-to-English (Zh→En) are shown in Table 2. We also list the results of (Vaswani et al., 2017) and our reimplemented Transformer as the baselines. On En→Fr, our reimplemented result is higher than the result of (Vaswani et al., 2017), since we update 300K steps while Vaswani et al. (2017) only update 100K steps. Many studies obtain similar results to ours (e.g., 41.1 BLEU scores from (Ott et al., 2019)). Compared with the baseline, NMT+LM yields +0.07 and +0.15 BLEU improvements on En→Fr and Zh→En, respectively. The improvement of NMT+LM on En→De in Table 1 (i.e., +0.75) is greater than these two datasets. We conjecture the reason is that the amount of training data of En→De is much smaller than that of En→Fr and Zh→En, thus NMT+LM is more likely to improve the model performance on En→De. Compared with NMT+LM, our MTO achieves further improvements with +0.42 and +1.04 BLEU scores on En→Fr and Zh→En, respectively, which demonstrates the performance improvement is mainly due to our Margin-bas"
2021.acl-long.268,P16-1162,0,0.807517,"training data. Following the same setting in (Vaswani et al., 2017), we use newstest2013 as validation set and newstest2014 as test set, which contain 3000 and 3003 sentences, respectively. For En→Fr, the training dataset contains about 36M sentence pairs, and we use newstest2013 with 3000 sentences as validation set and newstest2014 with 3003 sentences as test set. For Zh→En, we use 20.5M training data and use newstest2018 as validation set and newstest2019 as test set, which contain 3981 and 2000 sentences, respectively. For Zh→En, the number of merge operations in byte pair encoding (BPE) (Sennrich et al., 2016a) is set to 32K for both source and target languages. For En→De and En→Fr, we use a shared vocabulary generated by 32K BPEs. Evaluation. We measure the case-sensitive BLEU scores using multi-bleu.perl 5 for En→De and En→Fr. For Zh→En, case-sensitive BLEU scores are calculated by Moses mteval-v13a.pl script6 . Moreover, we use the paired bootstrap resampling (Koehn, 2004) for significance test. We select the model which performs the best on the validation sets and report its performance on the test sets for evaluation. Model and Hyperparameters. We conduct experiments based on the Transformer"
2021.acl-long.268,P16-1159,0,0.0233663,"ysis We first evaluate the main performance of our approaches (Section 5.1 and 5.2). Then, the human evaluation further confirms the improvements of translation adequacy and fluency (Section 5.3). Finally, we analyze the positive impact of our models on the distribution of Margin and explore how each fragment of our method works (Section 5.4). 5.1 Results on En→De The results on WMT14 English-to-German (En→De) are summarized in Table 1. We list the results from (Vaswani et al., 2017) and several related competitive NMT systems by various methods, such as Minimum Risk Training (MRT) objective (Shen et al., 2016), Simple Fusion of NMT and LM (Stahlberg et al., 2018), optimizing adequacy metrics (Kong et al., 2019; Feng et al., 2019) and improving the Transformer architecture (Yang et al., 2018; Zheng et al., 2019; Yang et al., 2019; Weng et al., 2020b; Yan et al., 2020a). We re7 The LM does not need to be state-of-the-art. The previous study of (Baziotis et al., 2020) has shown that a more powerful LM does not lead to further improvements to NMT. 8 The experimental results show that the model is insensitive to λLM . Therefore we make λLM consistent for all the three tasks. 3460 System En→De ↑ Existing"
2021.acl-long.268,W18-6321,0,0.0592886,"r approaches (Section 5.1 and 5.2). Then, the human evaluation further confirms the improvements of translation adequacy and fluency (Section 5.3). Finally, we analyze the positive impact of our models on the distribution of Margin and explore how each fragment of our method works (Section 5.4). 5.1 Results on En→De The results on WMT14 English-to-German (En→De) are summarized in Table 1. We list the results from (Vaswani et al., 2017) and several related competitive NMT systems by various methods, such as Minimum Risk Training (MRT) objective (Shen et al., 2016), Simple Fusion of NMT and LM (Stahlberg et al., 2018), optimizing adequacy metrics (Kong et al., 2019; Feng et al., 2019) and improving the Transformer architecture (Yang et al., 2018; Zheng et al., 2019; Yang et al., 2019; Weng et al., 2020b; Yan et al., 2020a). We re7 The LM does not need to be state-of-the-art. The previous study of (Baziotis et al., 2020) has shown that a more powerful LM does not lead to further improvements to NMT. 8 The experimental results show that the model is insensitive to λLM . Therefore we make λLM consistent for all the three tasks. 3460 System En→De ↑ Existing NMT systems Transformer (Vaswani et al., 2017) 27.3 M"
2021.acl-long.268,2020.emnlp-main.212,0,0.395537,"77/nmt adequacy Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Meng and Zhang, 2019; Zhang et al., 2019a; Yan et al., 2020b), which generates accurate and fluent translation through modeling the next word conditioned on both the source sentence and partial translation. However, NMT faces the hallucination problem, i.e., translations are fluent but inadequate to the source sentences. One important reason is that the NMT model pays excessive attention to the partial translation to ensure fluency while failing to translate some segments of the source sentence (Weng et al., 2020b), which is actually the overconfidence of the Language Model (LM). In the rest of this paper, the LM mentioned refers to the LM mechanism involved in NMT. Many recent studies attempt to deal with the inadequacy problem of NMT from two main aspects. One is to improve the architecture of NMT, such as adding a coverage vector to track the attention history (Tu et al., 2016), enhancing the crossattention module (Meng et al., 2016, 2018; Weng et al., 2020b), and dividing the source sentence into past and future parts (Zheng et al., 2019). The other aims to propose a heuristic adequacy metric or o"
2021.acl-long.268,P16-1008,0,0.123353,"luent but inadequate to the source sentences. One important reason is that the NMT model pays excessive attention to the partial translation to ensure fluency while failing to translate some segments of the source sentence (Weng et al., 2020b), which is actually the overconfidence of the Language Model (LM). In the rest of this paper, the LM mentioned refers to the LM mechanism involved in NMT. Many recent studies attempt to deal with the inadequacy problem of NMT from two main aspects. One is to improve the architecture of NMT, such as adding a coverage vector to track the attention history (Tu et al., 2016), enhancing the crossattention module (Meng et al., 2016, 2018; Weng et al., 2020b), and dividing the source sentence into past and future parts (Zheng et al., 2019). The other aims to propose a heuristic adequacy metric or objective based on the output of NMT. Tu et al. (2017) and Kong et al. (2019) enhance the model’s reconstruction ability and increase the coverage ratio of the source sentences by translations, respectively. Although some researches (Tu et al., 2017; Kong et al., 2019; Weng et al., 2020b) point out that the lack of adequacy is due to the overconfidence of the LM, unfortunat"
2021.acl-long.268,2020.acl-main.326,0,0.018837,"of MSO and MTO during finetuning. As the training continues, our model gets more competent, and the proportion of sentences judged to be “dirty data” by our model increases rapidly at first and then Case Study. To better illustrate the translation quality of our approach, we show several translation examples in Appendix C. Our approach grasps more segments of the source sentences, which are mistranslated or neglected by the Transformer. 6 Related Work Translation Adequacy of NMT. NMT suffers from the hallucination and inadequacy problem for a long time (Tu et al., 2016; M¨uller et al., 2020; Wang and Sennrich, 2020; Lee et al., 2019). Many studies improve the architecture of NMT to alleviate the inadequacy issue, including tracking translation adequacy by coverage vectors (Tu et al., 2016; Mi et al., 2016), modeling a global representation of source side (Weng et al., 2020a), dividing the source sentence into past and future parts (Zheng et al., 2019), and multi-task learning to improve encoder and cross-attention modules in decoder (Meng et al., 2016, 2018; Weng et al., 2020b). They inductively increase the translation adequacy, while our approaches directly maximize the Margin between the NMT and the"
2021.acl-long.268,D19-1073,0,0.0240946,"paper has been supported by the National Nature Science Foundation of China (No. 12026606). The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper. Data Selection in NMT. Data selection and data filter methods have been widely used in NMT. To balance data domains or enhance the data quality generated by back-translation (Sennrich et al., 2016b), many approaches have been proposed, such as utilizing language models (Moore and Lewis, 2010; van der Wees et al., 2017; Zhang et al., 2020), translation models (JunczysDowmunt, 2018; Wang et al., 2019a), and curriculum learning (Zhang et al., 2019b; Wang et al., 2019b). Different from the above methods, our MSO dynamically combines language models with translation models for data selection during training, making full use of the models. 7 Conclusion We alleviate the problem of inadequacy translation from the perspective of preventing the LM from being overconfident. Specifically, we firstly propose an indicator of the overconfidence degree of the LM in NMT, i.e., Margin between the NMT and the LM. Then we propose Margin-based TokenAcknowledgments References Dzmitry Bahdanau, Kyunghyun Cho,"
2021.acl-long.268,P19-1123,0,0.0346632,"Missing"
2021.acl-long.268,D17-1147,0,0.0406576,"Missing"
2021.acl-long.268,2020.emnlp-main.77,1,0.804875,"Missing"
2021.acl-long.268,D18-1475,1,0.830984,"ction 5.3). Finally, we analyze the positive impact of our models on the distribution of Margin and explore how each fragment of our method works (Section 5.4). 5.1 Results on En→De The results on WMT14 English-to-German (En→De) are summarized in Table 1. We list the results from (Vaswani et al., 2017) and several related competitive NMT systems by various methods, such as Minimum Risk Training (MRT) objective (Shen et al., 2016), Simple Fusion of NMT and LM (Stahlberg et al., 2018), optimizing adequacy metrics (Kong et al., 2019; Feng et al., 2019) and improving the Transformer architecture (Yang et al., 2018; Zheng et al., 2019; Yang et al., 2019; Weng et al., 2020b; Yan et al., 2020a). We re7 The LM does not need to be state-of-the-art. The previous study of (Baziotis et al., 2020) has shown that a more powerful LM does not lead to further improvements to NMT. 8 The experimental results show that the model is insensitive to λLM . Therefore we make λLM consistent for all the three tasks. 3460 System En→De ↑ Existing NMT systems Transformer (Vaswani et al., 2017) 27.3 MRT* (Shen et al., 2016) 27.71 Simple Fusion** (Stahlberg et al., 2018) 27.88 Localness (Yang et al., 2018) 28.11 Context-Aware (Ya"
2021.acl-long.268,2020.acl-main.756,0,0.0242905,"the inference speed largely. The research work descried in this paper has been supported by the National Nature Science Foundation of China (No. 12026606). The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper. Data Selection in NMT. Data selection and data filter methods have been widely used in NMT. To balance data domains or enhance the data quality generated by back-translation (Sennrich et al., 2016b), many approaches have been proposed, such as utilizing language models (Moore and Lewis, 2010; van der Wees et al., 2017; Zhang et al., 2020), translation models (JunczysDowmunt, 2018; Wang et al., 2019a), and curriculum learning (Zhang et al., 2019b; Wang et al., 2019b). Different from the above methods, our MSO dynamically combines language models with translation models for data selection during training, making full use of the models. 7 Conclusion We alleviate the problem of inadequacy translation from the perspective of preventing the LM from being overconfident. Specifically, we firstly propose an indicator of the overconfidence degree of the LM in NMT, i.e., Margin between the NMT and the LM. Then we propose Margin-based Tok"
2021.acl-long.268,P19-1426,1,0.896913,"Missing"
2021.acl-long.268,N19-1189,0,0.0328674,"Missing"
2021.acl-long.271,K16-1002,0,0.021264,"recognition networks can be estimated using the reparameterization trick (Kingma and Welling, 2014). During inference, latent variables obtained via prior networks and predicted question type qt0 are fed to the question decoder, which corresponds to red dashed arrows in Figure 2. The inference process is as follows: 3498 (1) Sample triple-level LV: zt ∼ qφ (zt |p)1 . (2) Sample answer LV: za ∼ pϕ (za |p, zt ). (3) Sample question LV: zq ∼ pϕ (zq |p, zt , za ). (4) Predict question type: qt ∼ pθ (qt|zq , zt , p). (5) Generate question: q ∼ pθ (zq , zt , p, qt). 3 apply KL annealing, word drop (Bowman et al., 2016) and bag-of-word (BOW) loss (Zhao et al., 2017)4 . The KL multiplier λ gradually increases from 0 to 1, and the word drop probability is 0.25. We use Pytorch to implement our model, and the model is trained on Titan Xp GPUs. Experiments In this section, we conduct experiments to evaluate our proposed method. We first introduce some empirical settings, including dataset, hyperparameters, baselines, and evaluation measures. Then we illustrate our results under both automatic and human evaluations. Finally, we give out some cases generated by different models and do further analyses over our meth"
2021.acl-long.271,D17-1070,0,0.0220143,"te Agreement” for all three criteria. mappings in PQ and QA pairs, and relationship modeling for zq and za , GTM can improve the relevance in QA pairs. 3.6 3.7 Question-Answer Coherence Evaluation Automatic metrics in Section “Automatic Metrics” are designed to compare generated questions with ground-truth ones (RUBER also takes the post information into consideration), but ignore answers in the evaluation process. To measure the semantic coherence between generated questions and answers, we apply two methods (Wang et al., 2019): (1) Cosine Similarity: We use the pre-trained Infersent model7 (Conneau et al., 2017) to obtain sentence embeddings and calculate cosine similarity between the embeddings of generated responses 7 The Infersent model is trained to predict the meaning of sentences based on natural language inference, and the cosine similarity computed with it is more consistent with human’s judgements, which performs better than the pre-trained Transformer/BERT model in our experiments. Table 3: Results for human evaluation. Model S2S-Attn CVAE kgCVAE STD HTD RL-CVAE GTM-zt GTM-a GTM-zq /za GTM Cosine Similarity 0.498 0.564 0.578 0.542 0.583 0.607 0.613 0.605 0.618 0.629 Matching Score 5.306 8.0"
2021.acl-long.271,N19-1125,0,0.030195,"Missing"
2021.acl-long.271,W04-3250,0,0.261348,"enerated by GTM are more coherent to answers. Attributing to the design of triple-level latent variable that captures the shared background, one-to-many Model S2S-Attn CVAE kgCVAE STD HTD RL-CVAE GTM-zt GTM-a GTM-zq /za GTM Fluency 0.482 0.462 0.474 0.488 0.526 0.534 0.538 0.532 0.542 0.548 Coherence 0.216 0.484 0.536 0.356 0.504 0.578 0.580 0.570 0.586 0.608 Willingness 0.186 0.428 0.476 0.286 0.414 0.508 0.516 0.512 0.520 0.526 Human Evaluation Results As shown in Table 3, GTM can alleviate the problem of generating dull and deviated questions compared with other models (significance tests (Koehn, 2004), p-value &lt; 0.05). Both our proposed model and the state-of-the-art model RL-CVAE utilize the answer information and the results of them could prove that answers assist the question generation process. Besides, GTM can produce more relevant and intriguing questions, which indicates the effectiveness of modeling the shared background and one-to-many mappings in CQG task. The interannotator agreement is calculated with the Fleiss’ kappa (Fleiss and Cohen, 1973). Fleiss’ kappa for Fluency, Coherence and Willingness is 0.493, 0.446 and 0.512, respectively, indicating “Moderate Agreement” for all t"
2021.acl-long.271,2020.acl-main.20,0,0.138114,"-answer (PQA) triple into post-question (PQ) and question-answer (QA) pairs rather than considering the triple as a whole and modeling the overall coherence. Furthermore, the training process of the matching model only utilizes one-to-one relation of each QA pair and neglects the one-to-many mapping feature. An open-domain PQA often takes place under a background that can be inferred from all utterances in the triple and help enhance the overall coherence. When it comes to the semantic relationship in each triple, the content of a specific question is under the control of its post and answer (Lee et al., 2020). Meanwhile, either a post or an answer could correspond to several meaningful questions. As shown in Table 1, the triple is about a person’s eating activity (the background of the entire conversation). There are one-to-many mappings in both PQ and QA pairs that construct different meaningful combinations, such as P-Q1.1-A1, P-Q1.2-A1, P-Q2.1-A2 and P-Q2.2-A2. An answer connects tightly to both its post and question, and in turn helps decide the expression of a question. On these grounds, we propose a generative triplewise model (GTM) for CQG. Specifically, we firstly introduce a triple-level"
2021.acl-long.271,D19-1317,0,0.13585,"ith users by not only responding but also asking (Li et al., 2017). Besides, raising questions is a proactive way to guide users to go deeper and further into conversations (Yu et al., 2016). Therefore, the ultimate goal of opendomain conversational question generation (CQG) is to enhance the interactiveness and maintain the continuity of a conversation (Wang et al., 2018). Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ∗ Yang Feng is the corresponding author. CQG differs fundamentally from traditional question generation (TQG) (Zhou et al., 2019; Kim et al., 2019; Li et al., 2019) that generates a question given a sentence/paragraph/passage and a specified answer within it. While in CQG, an answer always follows the to-be-generated question, and is unavailable during inference (Wang et al., 2019). At the same time, each utterance in open-domain scenario is casual and can be followed by several appropriate sentences, i.e., one-to-many mapping (Gao et al., 2019; Chen et al., 2019). At first, the input information of CQG was mainly a given post (Wang et al., 2018; Hu et al., 2018), and the generated questions were usually dull or deviated (Q3 and Q4 in Table 1). Based on"
2021.acl-long.271,N16-1014,0,0.380012,"zp and za . (6) GTMa: the variant of GTM that does not take answer into account. That is, answer decoder and za are removed from the loss function and the prior and posterior distributions of zq . Besides, zt here does not capture the semantics from answer. (7) GTMzq /za : GTM variant in which distributions of zq are not conditioned on za , i.e., the fact that the content of question is also controlled by answer is not modelled explicitly by latent variables. In our model, we use an MLP to predict question types during inference, which is different from the conditional training (CT) methods (Li et al., 2016b; Zhou et al., 2018; Shen and Feng, 2020) Inspired by Park et al. (2018), using zt inferred from post with the posterior distribution is better than sampling it from the prior one, i.e., a standard Gaussian distribution. 2 https://drive.google.com/drive/ folder/1wNG30YPHiMc_ZNyE3BH5wa1uVtR8l1pG 3 http://www.reddit.com 4 The total BOW loss is calculated as the sum of all BOW losses between each latent variable and q/a. Please refer to Park et al. (2018) for more details. 5 For those methods with open-source codes, we run the original codes; otherwise, we re-implement them based on the correspo"
2021.acl-long.271,P16-1094,0,0.169949,"zp and za . (6) GTMa: the variant of GTM that does not take answer into account. That is, answer decoder and za are removed from the loss function and the prior and posterior distributions of zq . Besides, zt here does not capture the semantics from answer. (7) GTMzq /za : GTM variant in which distributions of zq are not conditioned on za , i.e., the fact that the content of question is also controlled by answer is not modelled explicitly by latent variables. In our model, we use an MLP to predict question types during inference, which is different from the conditional training (CT) methods (Li et al., 2016b; Zhou et al., 2018; Shen and Feng, 2020) Inspired by Park et al. (2018), using zt inferred from post with the posterior distribution is better than sampling it from the prior one, i.e., a standard Gaussian distribution. 2 https://drive.google.com/drive/ folder/1wNG30YPHiMc_ZNyE3BH5wa1uVtR8l1pG 3 http://www.reddit.com 4 The total BOW loss is calculated as the sum of all BOW losses between each latent variable and q/a. Please refer to Park et al. (2018) for more details. 5 For those methods with open-source codes, we run the original codes; otherwise, we re-implement them based on the correspo"
2021.acl-long.271,D16-1230,0,0.0794378,"Missing"
2021.acl-long.271,2020.emnlp-main.739,0,0.0594831,"Missing"
2021.acl-long.271,D19-1326,0,0.0475725,"Missing"
2021.acl-long.271,P02-1040,0,0.110323,".584 0.622 0.649 0.687 0.650 0.682 0.633 0.663 0.653 0.689 0.660 0.701 0.661 0.710 0.657 0.692 0.669 0.713 0.671 0.720 Table 2: Automatic evaluation results for different models based on four types of metrics. that provide the controllable feature, i.e., question types, in advance for inference. Therefore, we do not consider CT-based models as comparable ones. 3.4 Evaluation Measures To better evaluate our results, we use both quantitative metrics and human judgements in our experiments. Automatic Metrics For automatic evaluation, we mainly choose four kinds of metrics: (1) BLEU Scores: BLEU (Papineni et al., 2002) calculates the n-gram overlap score of generated questions against ground-truth questions. We use BLEU-1 and BLEU-2 here and normalize them to 0 to 1 scale. (2) Embedding Metrics: Average, Greedy and Extrema metrics are embedding-based and measure the semantic similarity between the words in generated questions and ground-truth questions (Serban et al., 2017; Liu et al., 2016). We use word2vec embeddings trained on the Google News Corpus6 in this part. Please refer to Serban et al. (2017) for more details. (3) Dist-1& Dist-2: Following the work of Li et al. (2016a), we apply Distinct to repor"
2021.acl-long.271,N18-1162,0,0.0355925,"Missing"
2021.acl-long.271,P17-2036,0,0.0167679,"a vital position in this field. Serban et al. (2016) proposed the hierarchical recurrent encoder-decoder (HRED) model with a context RNN to integrate historical information from utterance RNNs. To capture utterance-level variations, Serban et al. (2017) raised a new model Variational HRED (VHRED) that augments HRED with CVAEs. After that, VHCR (Park et al., 2018) added a conversation-level latent variable on top of the VHRED, while CSRR (Shen et al., 2019) used three-hierarchy latent variables to model the complex dependency among utterances. In order to detect relative utterances in context, Tian et al. (2017) and Zhang et al. (2018) applied cosine similarity and attention mechanism, respectively. HRAN (Xing et al., 2018) combined the attention results on both word-level and utterance-level. Besides, the future information has also been considered for context modeling. Shen et al. (2018) separated the context into history and future parts, and assumed that each of them conditioned on a latent variable is under a Gaussian distribution. Feng et al. (2020) used future utterances in the discriminator of a GAN, which is similar to Wang et al. (2019). The differences between our method and aforementioned"
2021.acl-long.271,P15-1152,0,0.0332229,"300, 300, and 100. The prior networks and MLPs have one hidden layer with size 300 and tanh non-linearity, while the number of hidden layers in recognition networks for both triple-level and utterance-level variables is 2. We apply dropout ratio of 0.2 during training. The mini-batch size is 64. For optimization, we use Adam (Kingma and Ba, 2015) with a learning rate of 1e-4. In order to alleviate degeneration problem of variational framework (Park et al., 2018), we We compare our methods with four groups of representative models: (1) S2S-Attn: A simple Seq2Seq model with attention mechanism (Shang et al., 2015). (2) CVAE&kgCVAE: The CVAE model integrates an extra BOW loss to generate diverse questions. The kgCVAE is a knowledge-guided CVAE that utilizes some linguistic cues (question types in our experiments) to learn meaningful latent variables (Zhao et al., 2017). (3) STD&HTD: The STD uses soft typed decoder that estimates a type distribution over word types, and the HTD uses hard typed decoder that specifies the type of each word explicitly with Gumbel-softmax (Wang et al., 2018). (4) RL-CVAE: A reinforcement learning method that regards the coherence score (computed by a one-to-one matching netw"
2021.acl-long.271,2020.acl-main.52,1,0.84569,"GTM that does not take answer into account. That is, answer decoder and za are removed from the loss function and the prior and posterior distributions of zq . Besides, zt here does not capture the semantics from answer. (7) GTMzq /za : GTM variant in which distributions of zq are not conditioned on za , i.e., the fact that the content of question is also controlled by answer is not modelled explicitly by latent variables. In our model, we use an MLP to predict question types during inference, which is different from the conditional training (CT) methods (Li et al., 2016b; Zhou et al., 2018; Shen and Feng, 2020) Inspired by Park et al. (2018), using zt inferred from post with the posterior distribution is better than sampling it from the prior one, i.e., a standard Gaussian distribution. 2 https://drive.google.com/drive/ folder/1wNG30YPHiMc_ZNyE3BH5wa1uVtR8l1pG 3 http://www.reddit.com 4 The total BOW loss is calculated as the sum of all BOW losses between each latent variable and q/a. Please refer to Park et al. (2018) for more details. 5 For those methods with open-source codes, we run the original codes; otherwise, we re-implement them based on the corresponding paper. 3.1 Dataset We apply our mode"
2021.acl-long.271,P19-1549,1,0.844722,"Blended Evaluation Routine (Tao et al., 2018) has shown a high correlation with human annotation in open-domain conversation evaluation. There are two versions, one is RubG based on geometric averaging and the other is RubA based on arithmetic averaging. Embedding metrics and BLEU scores are used to measure the similarity between generated and ground-truth questions. RubG/A reflects the se6 https://code.google.com/archive/p/ word2vec/ mantic coherence of PQ pairs (Wang et al., 2019), while Dist-1/2 evaluates the diversity of questions. Human Evaluation Settings Inspired by Wang et al. (2019), Shen et al. (2019), and Wang et al. (2018), we use following three criteria for human evaluation: (1) Fluency measures whether the generated question is reasonable in logic and grammatically correct. (2) Coherence denotes whether the generated question is semantically consistent with the given post. Incoherent questions include dull cases. (3) Willingness measures whether a user is willing to answer the question. This criterion is to justify how likely the generated questions can elicit further interactions. We randomly sample 500 examples from test set, and generate questions using models mentioned above. Then"
2021.acl-long.271,D18-1463,0,0.0162246,"l HRED (VHRED) that augments HRED with CVAEs. After that, VHCR (Park et al., 2018) added a conversation-level latent variable on top of the VHRED, while CSRR (Shen et al., 2019) used three-hierarchy latent variables to model the complex dependency among utterances. In order to detect relative utterances in context, Tian et al. (2017) and Zhang et al. (2018) applied cosine similarity and attention mechanism, respectively. HRAN (Xing et al., 2018) combined the attention results on both word-level and utterance-level. Besides, the future information has also been considered for context modeling. Shen et al. (2018) separated the context into history and future parts, and assumed that each of them conditioned on a latent variable is under a Gaussian distribution. Feng et al. (2020) used future utterances in the discriminator of a GAN, which is similar to Wang et al. (2019). The differences between our method and aforementioned ones in Section 4.1 and 4.2 are: (1) Rather than dividing PQA triples into two parts, i.e., PQ (history and current utterances) and QA (current and future utterances) pairs, we model the entire coherence by utilizing a latent variable to capture the share background in a triple. (2"
2021.acl-long.271,D19-1511,0,0.0687683,"al of opendomain conversational question generation (CQG) is to enhance the interactiveness and maintain the continuity of a conversation (Wang et al., 2018). Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ∗ Yang Feng is the corresponding author. CQG differs fundamentally from traditional question generation (TQG) (Zhou et al., 2019; Kim et al., 2019; Li et al., 2019) that generates a question given a sentence/paragraph/passage and a specified answer within it. While in CQG, an answer always follows the to-be-generated question, and is unavailable during inference (Wang et al., 2019). At the same time, each utterance in open-domain scenario is casual and can be followed by several appropriate sentences, i.e., one-to-many mapping (Gao et al., 2019; Chen et al., 2019). At first, the input information of CQG was mainly a given post (Wang et al., 2018; Hu et al., 2018), and the generated questions were usually dull or deviated (Q3 and Q4 in Table 1). Based on the observation that an answer has strong relevance to its question and post, Wang et al. (2019) tried to integrate answer into the question generation process. They applied a reinforcement learning framework that firstl"
2021.acl-long.271,P18-1204,0,0.389212,"to Q2.2) is decided by its post and answer. Q3 (dull) and Q4 (deviated) are generated given only the post. Introduction Questioning in open-domain dialogue systems is indispensable since a good system should have the ability to well interact with users by not only responding but also asking (Li et al., 2017). Besides, raising questions is a proactive way to guide users to go deeper and further into conversations (Yu et al., 2016). Therefore, the ultimate goal of opendomain conversational question generation (CQG) is to enhance the interactiveness and maintain the continuity of a conversation (Wang et al., 2018). Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ∗ Yang Feng is the corresponding author. CQG differs fundamentally from traditional question generation (TQG) (Zhou et al., 2019; Kim et al., 2019; Li et al., 2019) that generates a question given a sentence/paragraph/passage and a specified answer within it. While in CQG, an answer always follows the to-be-generated question, and is unavailable during inference (Wang et al., 2019). At the same time, each utterance in open-domain scenario is casual and can be followed by several appropriate sentences, i.e., one-to-man"
2021.acl-long.271,W16-3649,0,0.0172703,"mple of CQG task which is talking about a person’s eating activity. There are one-to-many mappings in both PQ and QA pairs. The content of each meaningful and relevant question (Q1.1 to Q2.2) is decided by its post and answer. Q3 (dull) and Q4 (deviated) are generated given only the post. Introduction Questioning in open-domain dialogue systems is indispensable since a good system should have the ability to well interact with users by not only responding but also asking (Li et al., 2017). Besides, raising questions is a proactive way to guide users to go deeper and further into conversations (Yu et al., 2016). Therefore, the ultimate goal of opendomain conversational question generation (CQG) is to enhance the interactiveness and maintain the continuity of a conversation (Wang et al., 2018). Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ∗ Yang Feng is the corresponding author. CQG differs fundamentally from traditional question generation (TQG) (Zhou et al., 2019; Kim et al., 2019; Li et al., 2019) that generates a question given a sentence/paragraph/passage and a specified answer within it. While in CQG, an answer always follows the to-be-generated question, and is un"
2021.acl-long.271,2021.naacl-main.446,0,0.169882,"eally messing me up. Table 5: Two cases comparison among GTM and other baselines. both posts and answers, and could attract people to give an answer to them. However, other baselines may generate dull or deviated responses, even the RL-CVAE model that considers the answer information would only contain the topic words in answers (e.g., the question in case two), but fail to ensure the PQA coherence. eration problem does not exist in our model and latent variables can play their corresponding roles. 4 The researches on open-domain dialogue systems have developed rapidly (Majumder et al., 2020; Zhan et al., 2021; Shen et al., 2021), and our work mainly touches two fields: open-domain conversational question generation (CQG), and context modeling in dialogue systems. We introduce these two fields as follows and point out the main differences between our method and previous ones. 4.1 Figure 3: Total KL divergence (per word) of all latent variables in GTM and GTM-a model (first 30 epochs of validation set). 3.8 Further Analysis of GTM Variational models suffer from the notorious degeneration problem, where the decoders ignore latent variables and reduce to vanilla Seq2Seq models (Zhao et al., 2017; Park"
2021.acl-long.271,C18-1206,0,0.0314446,"Missing"
2021.acl-long.271,P17-1061,0,0.11927,"q, a (not used in inference), and qt, while dashed arrows are for posterior distributions of latent variables. • Our variational hierarchical structure can not only utilize the “future” information (answer), but also capture one-to-many mappings in PQ and QA, which matches the open-domain scenario well. • Experimental results on a large-scale CQG corpus show that our method significantly outperforms the state-of-the-art baselines in both automatic and human evaluations. 2 Proposed Model Given a post as the input, the goal of CQG is to generate the corresponding question. Following the work of Zhao et al. (2017) and Wang et al. (2019), we leverage the question type qt to control the generated question, and take advantage of the answer information a to improve coherence. In training set, each conversation is represented |p| as {p, q, qt, a}, consisting of post p = {pi }i=1 , |q| question q = {qi }i=1 with its question type qt, |a| and answer a = {ai }i=1 . 2.1 Overview The graphical model of GTM for training process is shown in Figure 1. θ, ϕ, and φ are used to denote parameters of generation, prior, and recognition network, respectively. We integrate answer generation to assist question generation wi"
2021.acl-long.271,D19-1622,0,0.0445696,"Missing"
2021.acl-long.394,P16-1004,0,0.18599,"uage (NL) description, which has attracted increasing attention recently due to its potential value in simplifying programming. Instead of modeling the abstract syntax tree (AST) of code snippets directly, most of methods for code generation convert AST into a sequence of tree-construction actions. This allows for using natural language generation (NLG) models, such as the widely-used encoder-decoder Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. * Equal contribution † Corresponding author {wuqq,jssu}@xmu.edu.cn models, and obtains great success (Ling et al., 2016; Dong and Lapata, 2016, 2018; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018, 2019; Hayati et al., 2018; Sun et al., 2019, 2020; Wei et al., 2019; Shin et al., 2019; Xu et al., 2020; Xie et al., 2021). Specifically, an encoder is first used to learn word-level semantic representations of the input NL description. Then, a decoder outputs a sequence of tree-construction actions, with which the corresponding AST is generated through pre-order traversal. Finally, the generated AST is mapped into surface codes via certain deterministic functions. Generally, during the generation of dominant Seq2Tree models based on"
2021.acl-long.394,P18-1068,0,0.0857169,"ments To investigate the effectiveness and generalizability of our model, we carry out experiments on several commonly-used datasets. X λ Lrl (o; θ), |Nmb | Lrl (o; θ) = −Eo∼π [r(o)] (12) 4.2 Baselines To facilitate the descriptions of experimental results, we refer to the enhanced TRANX model as TRANX-RL. In addition to TRANX, we compare our enhanced model with several competitive models: r(o) = (Lmle (ˆ o) − Lmle (o)) ∗ (max(η − p(o), 0)). (11) 5080 • TRANX (w/ pre-train). It is an enhanced version of TRANX with pre-training. We DJANGO Acc. ATIS Acc. GEO Acc. CONALA BLEU / Acc. COARSE2FINE (Dong and Lapata, 2018)† TRANX (Yin and Neubig, 2019)† TREEGEN (Sun et al., 2020) – 77.3 ±0.4 – 87.7 87.6 ±0.1 88.1 ±0.6 88.2 88.8 ±1.0 – – 24.35 ±0.4 / 2.5 ±0.7 – TRANX TRANX (w/ pre-train) TRANX-R2L TRANX-RAND 77.2 ±0.6 77.5 ±0.4 75.9 ±0.8 74.6 ±1.1 87.6 ±0.4 87.8 ±0.7 87.5 ±0.9 86.4 ±1.4 88.8 ±1.0 88.4±1.1 86.4 ±1.0 81.7 ±1.8 24.38 ±0.5 / 2.2 ±0.5 24.57 ±0.5 / 1.4 ±0.3 24.88 ±0.5 / 2.4 ±0.5 19.73 ±1.1 / 1.6 ±0.6 TRANX-RL (w/o pre-train) TRANX-RL 76.3 ±0.7 77.9 ±0.5 87.2 ±0.8 89.1 ±0.5 87.1 ±1.6 89.5 ±1.2 23.38 ±0.8 / 2.1 ±0.2 25.47 ±0.7 / 2.6 ±0.4 Model Table 2: The performance of our model in comparison with var"
2021.acl-long.394,Q19-1042,0,0.0180992,"e above studies that deal with multi-branch nodes in left-to-right order, our model determines the optimal expansion orders of branches for multi-branch nodes. Some researchers have also noticed that the selection of decoding order has an important impact on the performance of neural code generation models. For example, Alvarez-Melis and Jaakkola (2017) introduce a doubly RNN model that combines width and depth recurrences to traverse each node. Dong and Lapata (2018) firstly generate a rough code sketch, and then fill in missing details by considering the input NL description and the sketch. Gu et al. (2019a) present an insertionbased Seq2Seq model that can flexibly generate a sequence in an arbitrary order. In general, these researches still deal with multi-branch AST nodes in a left-to-right manner. Thus, these models are theoretically compatible with our proposed branch selector. (a) The first example. Finally, it should be noted that have been many NLP studies on exploring other decoding methods to improve other NLG tasks (Zhang et al., 2018; Su et al., 2019; Zhang et al., 2019; Welleck et al., 2019; Stern et al., 2019; Gu et al., 2019a,b). However, to the best of our knowledge, our work is"
2021.acl-long.394,2020.emnlp-main.175,0,0.508154,"ltistep expansion order selection and how to determine the optimal expansion order lead to challenges for the model training. To deal with these issues, we introduce reinforcement learning to train the extended Seq2Tree model in an end-to-end way. Concretely, we first pre-train a conventional Seq2Tree model. Then, we employ self-critical training with a reward function that measures loss difference between different branch expansion orders to train the extended Seq2Tree model. 3.2.1 Pre-training It is known that a well-initialized network is very important for applying reinforcement learning (Kang et al., 2020). In this work, we require the model to automatically quantify effects of different branch expansion orders on the quality of the generated action sequences. Therefore, we expect that the model has the basic ability to generate action sequences in random order at the beginning. To do this, instead of using the pre-order traversal based action sequences, we use the randomly-organized action sequences to pre-train the Seq2Tree model. Concretely, for each multi-branch node in an AST, we sample a branch expansion order from a 5079 uniform distribution, and then reorganize the corresponding actions"
2021.acl-long.394,P16-1057,0,0.0266076,"ode, which frequently occurs with ‘gzip’. In the second example, TRANX incorrectly predicts the second child node at the t10 -th timestep, while TRANX-RL firstly predicts it at the timestep t6 . We think this error results from the sequentially generated nodes and the errors in early timesteps would accumulatively harm the predictions of later sibling nodes. By comparison, our model can flexibly generate subtrees with shorter lengths, alleviating error accumulation. 5 Related Work With the prosperity of deep learning, researchers introduce neural networks into code generation. In this aspect, Ling et al. (2016) first explore a Seq2Seq model for code generation. Then, due to the advantage of tree structure, many attempts resort to Seq2Tree models, which represent codes as trees of meaning representations (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018; Sun et al., 2019, 2020). Typically, Yin and Neubig (2018) propose TRANX, which introduces ASTs as intermediate representations of codes and has become the most influential Seq2Tree model. Then, Sun et al. (2019, 2020) respectively explore CNN and Transformer 5082 ever, is not suitable to han"
2021.acl-long.394,P17-1105,0,0.103927,"has attracted increasing attention recently due to its potential value in simplifying programming. Instead of modeling the abstract syntax tree (AST) of code snippets directly, most of methods for code generation convert AST into a sequence of tree-construction actions. This allows for using natural language generation (NLG) models, such as the widely-used encoder-decoder Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. * Equal contribution † Corresponding author {wuqq,jssu}@xmu.edu.cn models, and obtains great success (Ling et al., 2016; Dong and Lapata, 2016, 2018; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018, 2019; Hayati et al., 2018; Sun et al., 2019, 2020; Wei et al., 2019; Shin et al., 2019; Xu et al., 2020; Xie et al., 2021). Specifically, an encoder is first used to learn word-level semantic representations of the input NL description. Then, a decoder outputs a sequence of tree-construction actions, with which the corresponding AST is generated through pre-order traversal. Finally, the generated AST is mapped into surface codes via certain deterministic functions. Generally, during the generation of dominant Seq2Tree models based on pre-order traversal, branches"
2021.acl-long.394,W19-3620,0,0.0633131,"Missing"
2021.acl-long.394,2020.acl-main.538,0,0.155716,"ee (AST) of code snippets directly, most of methods for code generation convert AST into a sequence of tree-construction actions. This allows for using natural language generation (NLG) models, such as the widely-used encoder-decoder Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. * Equal contribution † Corresponding author {wuqq,jssu}@xmu.edu.cn models, and obtains great success (Ling et al., 2016; Dong and Lapata, 2016, 2018; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018, 2019; Hayati et al., 2018; Sun et al., 2019, 2020; Wei et al., 2019; Shin et al., 2019; Xu et al., 2020; Xie et al., 2021). Specifically, an encoder is first used to learn word-level semantic representations of the input NL description. Then, a decoder outputs a sequence of tree-construction actions, with which the corresponding AST is generated through pre-order traversal. Finally, the generated AST is mapped into surface codes via certain deterministic functions. Generally, during the generation of dominant Seq2Tree models based on pre-order traversal, branches of each multi-branch nodes are expanded in a left-to-right order. Figure 1 gives an example of the NL-to-Code conversion conducted by"
2021.acl-long.394,P17-1041,0,0.106221,"attention recently due to its potential value in simplifying programming. Instead of modeling the abstract syntax tree (AST) of code snippets directly, most of methods for code generation convert AST into a sequence of tree-construction actions. This allows for using natural language generation (NLG) models, such as the widely-used encoder-decoder Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. * Equal contribution † Corresponding author {wuqq,jssu}@xmu.edu.cn models, and obtains great success (Ling et al., 2016; Dong and Lapata, 2016, 2018; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018, 2019; Hayati et al., 2018; Sun et al., 2019, 2020; Wei et al., 2019; Shin et al., 2019; Xu et al., 2020; Xie et al., 2021). Specifically, an encoder is first used to learn word-level semantic representations of the input NL description. Then, a decoder outputs a sequence of tree-construction actions, with which the corresponding AST is generated through pre-order traversal. Finally, the generated AST is mapped into surface codes via certain deterministic functions. Generally, during the generation of dominant Seq2Tree models based on pre-order traversal, branches of each multi-branch n"
2021.acl-long.394,D18-2002,0,0.166909,"rders for code generation. • Experimental results and in-depth analyses GenToken Reduce AST ? ? ? ? ExceptHandler type name Name Name id id Exception Code: body (empty) e token constructor except Exception as e: Figure 1: An example of code generation using the conventional Seq2Tree model in pre-order traversal. demonstrate the effectiveness and generality of our model on various datasets. 2 Background As shown in Figure 1, the procedure of code generation can be decomposed into three stages. Based on the learned semantic representations of the input NL utterance, the dominant Seq2Tree model (Yin and Neubig, 2018) first outputs a sequence of abstract syntax description language (ASDL) grammar-based actions. These actions can then be used to construct an AST following the preorder traversal. Finally, the generated AST is mapped into surface code via a user-specified function AST to MR(∗). In the following subsections, we first describe the basic ASDL grammars of Seq2Tree models. Then, we introduce the details of TRANX (Yin and Neubig, 2018), which is selected as our basic model due to its extensive applications and competitive performance (Yin and Neubig, 2019; Shin et al., 2019; Xu et al., 2020). 1 1 P"
2021.acl-long.394,P19-1447,0,0.114297,"NL utterance, the dominant Seq2Tree model (Yin and Neubig, 2018) first outputs a sequence of abstract syntax description language (ASDL) grammar-based actions. These actions can then be used to construct an AST following the preorder traversal. Finally, the generated AST is mapped into surface code via a user-specified function AST to MR(∗). In the following subsections, we first describe the basic ASDL grammars of Seq2Tree models. Then, we introduce the details of TRANX (Yin and Neubig, 2018), which is selected as our basic model due to its extensive applications and competitive performance (Yin and Neubig, 2019; Shin et al., 2019; Xu et al., 2020). 1 1 Please note that our approach is also applicable to other Seq2Tree models. 5077 2.1 ASDL Grammar Formally, an ASDL grammar contains two components: type and constructors. The value of type can be composite or primitive. As shown in the ‘ActionSequence’ and ‘AST z’ parts of Figure 1, a constructor specifies a language component of a particular type using its fields, e.g., ExceptHandler (expr? type, expr? name, stmt∗ body). Each field specifies the type of its child node and contains a cardinality (single, optional ? and sequential ∗) indicating the num"
2021.acl-long.504,P16-5005,0,0.0443917,"Missing"
2021.acl-long.504,P84-1044,0,0.56082,"Missing"
2021.acl-long.504,D18-1232,0,0.0626432,"Missing"
2021.acl-long.504,D16-1139,0,0.438243,"d global-level selections, to pick suitable samples for distillation. We evaluate our approaches on two large-scale machine translation tasks, WMT’14 English-German and WMT’19 Chinese-English. Experimental results show that our approaches yield up to +1.28 and +0.89 BLEU points improvements over the Transformer baseline, respectively. 1 1 Introduction Machine translation has made great progress recently by using sequence-to-sequence models (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Zhang et al., 2019b; Yan et al., 2020). Recently, some knowledge distillation methods (Kim and Rush, 2016; Freitag et al., 2017; Gu ∗ Equal contribution. This work was done when Fusheng Wang was interning at Pattern Recognition Center, Wechat AI, Tencent Inc, China. 1 We release our code on https://github.com/Les lieOverfitting/selective distillation. † et al., 2017; Tan et al., 2019; Wei et al., 2019; Li et al., 2020; Wu et al., 2020) are proposed in the machine translation to help improve model performance by transferring knowledge from a teacher model. These methods can be divided into two categories: word-level and sequence-level, by the granularity of teacher information. In their researches"
2021.acl-long.504,kocmi-bojar-2017-curriculum,0,0.0250539,"Missing"
2021.acl-long.504,W04-3250,0,0.610707,"Missing"
2021.acl-long.504,2020.acl-main.41,0,0.0772118,"Missing"
2021.acl-long.504,2020.wmt-1.24,1,0.729222,"We conduct extensive analyses and find that some of the teacher’s knowledge will hurt the whole effect of knowledge distillation. • We propose two selective strategies: batchlevel selection and global-level selection. The experimental results validate our methods are effective. 2 Related Work Knowledge distillation approach (Hinton et al., 2015) aims to transfer knowledge from teacher model to student model. Recently, many knowledge distillation methods (Kim and Rush, 2016; Hu et al., 2018; Sun et al., 2019; Tang et al., 2019; Jiao et al., 2019; Zhang et al., 2019a, 2020; Chen et al., 2020a; Meng et al., 2020) have been used to get effective student model in the field of natural language processing by using teacher model’s outputs or hidden states as knowledge. As for neural machine translation (NMT), knowledge distillation methods commonly focus on better improving the student model and learning from the teacher model. Kim and Rush (2016) first applied knowledge distillation to NMT and proposed the sequence-level knowledge distillation that lets student model mimic the sequence distribution generated by the teacher model. It was explained as a kind of data augmentation and regularization by Gordon"
2021.acl-long.504,N19-4009,0,0.0397154,"Missing"
2021.acl-long.504,D19-1441,0,0.0535241,"Missing"
2021.acl-long.504,N19-1192,0,0.071906,"Transformer baseline, respectively. 1 1 Introduction Machine translation has made great progress recently by using sequence-to-sequence models (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Zhang et al., 2019b; Yan et al., 2020). Recently, some knowledge distillation methods (Kim and Rush, 2016; Freitag et al., 2017; Gu ∗ Equal contribution. This work was done when Fusheng Wang was interning at Pattern Recognition Center, Wechat AI, Tencent Inc, China. 1 We release our code on https://github.com/Les lieOverfitting/selective distillation. † et al., 2017; Tan et al., 2019; Wei et al., 2019; Li et al., 2020; Wu et al., 2020) are proposed in the machine translation to help improve model performance by transferring knowledge from a teacher model. These methods can be divided into two categories: word-level and sequence-level, by the granularity of teacher information. In their researches, the model learns from teacher models by minimizing gaps between their outputs on every training word/sentence (i.e., corresponding training sample) without distinction. Despite their promising results, previous studies mainly focus on finding what to teach and rarely investigate how these words/s"
2021.acl-long.504,2020.emnlp-main.74,0,0.126671,"1 1 Introduction Machine translation has made great progress recently by using sequence-to-sequence models (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Zhang et al., 2019b; Yan et al., 2020). Recently, some knowledge distillation methods (Kim and Rush, 2016; Freitag et al., 2017; Gu ∗ Equal contribution. This work was done when Fusheng Wang was interning at Pattern Recognition Center, Wechat AI, Tencent Inc, China. 1 We release our code on https://github.com/Les lieOverfitting/selective distillation. † et al., 2017; Tan et al., 2019; Wei et al., 2019; Li et al., 2020; Wu et al., 2020) are proposed in the machine translation to help improve model performance by transferring knowledge from a teacher model. These methods can be divided into two categories: word-level and sequence-level, by the granularity of teacher information. In their researches, the model learns from teacher models by minimizing gaps between their outputs on every training word/sentence (i.e., corresponding training sample) without distinction. Despite their promising results, previous studies mainly focus on finding what to teach and rarely investigate how these words/sentences (i.e., samples), which ser"
2021.acl-long.504,2020.emnlp-main.77,1,0.839154,"Missing"
2021.acl-long.504,N19-1119,0,0.0350845,"Missing"
2021.acl-long.504,P16-1162,0,0.0586626,"ce 9: Loss ← Loss + Lossi 10: Update S with respect to Loss of each word. The storage of queue is much bigger than a mini-batch so that we can evaluate the current batch’s CEs with more words, which reduces the fluctuation of CE distribution caused by the batch-level one. Algorithm 1 details the entire procedure. 6 Experiments We carry out experiments on two large-scale machine translation tasks: WMT’14 English-German (En-De) and WMT’19 Chinese-English (Zh-En). 6.1 Setup Datasets. For WMT’14 En-De task, we use 4.5M preprocessed data, which is tokenized and split using byte pair encoded (BPE) (Sennrich et al., 2016) with 32K merge operations and a shared vocabulary for English and German. We use newstest2013 as the validation set and newstest2014 as the test set, which contain 3000 and 3003 sentences, respectively. For the WMT’19 Zh-En task, we use 20.4M preprocessed data, which is tokenized and split using 47K/32K BPE merge operations for source and target languages. We use newstest2018 as our validation set and newstest2019 as our test set, which contain 3981 and 2000 sentences, respectively. Evaluation. For evaluation, we train all the models with a maximum of 300K steps for WMT EnDe’14 and WMT’19 Zh-"
2021.acl-long.504,2020.emnlp-main.37,0,0.0702232,"Missing"
2021.acl-long.504,P19-1426,1,0.793031,"Missing"
2021.acl-long.504,D19-1086,0,0.0219955,"• Seq-KD (Kim and Rush, 2016). SequenceKD uses teacher generated outputs on training corpus as an extra source. The training loss can be formulated as: Lseq kd = − |V | J X X 1{ˆyj = k} j=1 k=1 × log p(yj = k|ˆ y&lt;j , x; θ), (4) where y ˆ denotes the sequence predicted by teacher model from running beam search, J is the length of target sentence. • Bert-KD (Chen et al., 2020b). This method leverages the pre-trained Bert as teacher model to help NMT model improve machine translation quality. • Other Systems. We also include some existing methods based on Transformer(Base) for comparison, i.e., Zheng et al. (2019); So et al. (2019); Tay et al. (2020). 6.2 Main Results 3 https://github.com/moses-smt/mosesde coder/blob/master/scripts/generic/mteval -v13a.pl Results on WMT’14 English-German. The results on WMT’14 En-De are shown in Table 2. In 6461 SEasy (27.78) SHard (28.42) Distil All (28.14) 15000 12500 10000 7500 5000 0 20000 40000 60000 Training Steps 80000 100000 2500 0 0 Figure 3: The probability for gradients of Lkd and Lce pointing the same direction. this experiment, both the teacher model and student model are Transformer (Base). We also list our implementation of word-level distillation and se"
2021.acl-short.65,P18-1008,0,0.0290905,"assigns larger training weights to tokens with higher BMI, so that easy tokens are updated with coarse granularity while difficult tokens are updated with fine granularity. Experimental results on WMT14 English-to-German and WMT19 Chinese-to-English demonstrate the superiority of our approach compared with the Transformer baseline and previous token-level adaptive training approaches. Further analyses confirm that our method can improve the lexical diversity. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Chen et al., 2018; Meng and Zhang, 2019; Zhang et al., 2019; Yan et al., 2020; Liu et al., 2021) has achieved remarkable success. As a data-driven model, the performance of NMT depends on training corpus. Balanced training data is a crucial factor in building a superior model. ∗ This work was done when Yangyifan Xu was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China † Jinan Xu is the corresponding author of the paper. However, natural languages conform to the Zipf’s law (Zipf, 1949), the frequencies of words exhibit the long tail characteristics, which brings an imbalance in the distribu"
2021.acl-short.65,2020.aacl-main.25,0,0.0828327,"Missing"
2021.acl-short.65,2020.emnlp-main.76,1,0.674679,"Missing"
2021.acl-short.65,D19-1088,0,0.0219702,"l competence increases (Wan et al., 2020). Second, previous studies only use monolingual word frequency information in the target language without considering the source language, which is insufficient for bilingual tasks, e.g., machine translation. The mapping between bilingualism is a more appropriate indicator. As shown in Table 1, word frequency of pleasing and bearings are both 847. Corresponding to Chinese, pleasing has multiple mappings, while bearings is relatively single. The more multivariate the mapping is, the less confidence in predicting the target word given the source context. He et al. (2019) also confirm this view that words with multiple mappings contribute more to the BLEU score. To tackle the above issues, we propose bilingual mutual information (BMI), which has two characteristics: 1) BMI measures the learning difficulty for each target token by considering the strength of association between it and the source sentence; 2) for each target token, BMI can dynamically adjust according to the context. BMI-based adaptive training can dynamically adjust the learning granularity on tokens. Easy tokens are updated with coarse granularity while difficult tokens are updated with 511 Pr"
2021.acl-short.65,W19-6622,0,0.0330324,"Missing"
2021.acl-short.65,2020.emnlp-main.80,0,0.0330367,"ings an imbalance in the distribution of words in training corpora. Some studies (Jiang et al., 2019; Gu et al., 2020) assign different training weights to target tokens according to their frequencies. These approaches alleviate the token imbalance problem and indicate that tokens should be treated differently during training. However, there are two issues in existing approaches. First, these approaches believe that lowfrequency words are not sufficiently trained and thus amplify the weight of them. Nevertheless, low-frequency tokens are not always difficult as the model competence increases (Wan et al., 2020). Second, previous studies only use monolingual word frequency information in the target language without considering the source language, which is insufficient for bilingual tasks, e.g., machine translation. The mapping between bilingualism is a more appropriate indicator. As shown in Table 1, word frequency of pleasing and bearings are both 847. Corresponding to Chinese, pleasing has multiple mappings, while bearings is relatively single. The more multivariate the mapping is, the less confidence in predicting the target word given the source context. He et al. (2019) also confirm this view t"
2021.acl-short.65,2020.emnlp-main.77,1,0.857232,"Missing"
2021.acl-short.65,W04-3250,0,0.357853,"Missing"
2021.acl-short.65,P17-4012,0,0.0902858,"on and report the BLEU scores on newstest2014. ZH-EN. The training data is from WMT19 which consists of 20.5M sentence pairs. The number of merge operations in byte pair encoding (BPE) is set to 32K for both source and target languages. We use newstest2018 as our validation set and newstest2019 as our test set, which contain 4k and 2k sentences, respectively. BMI-based Objective We calculate the token-level weight by scaling BMI and adjusting the lower limit as follows: wj = S · BMI(x, yj ) + B. Experiments 4.2 Systems Transformer. We implement our approach with the open source toolkit THUMT (Zhang et al., 2017) and strictly follow the setting of TransformerBase in (Vaswani et al., 2017). Exponential (Gu et al., 2020). This method adds an additional training weights to lowfrequency target tokens: wj = A · e−T ·Count(yj ) + 1. (4) The two hyperparameters S (scale) and B (base) influence the magnitude of change and the lower limit, respectively. 513 (5) Chi-Square (Gu et al., 2020). The weighting function of this method is similar to the form of chi-square distribution wj = A · Count2 (yj )e−T ·Count(yj ) + 1. (6) B 1.0 BMI 0.9 0.8 0.7 S 0.05 0.10 0.15 0.20 0.25 0.30 0.15 0.20 0.25 0.15 0.20 0.25 0.15"
2021.acl-short.65,P19-1426,1,0.87636,"Missing"
2021.acl-short.65,P16-1162,0,0.0591186,"icult tokens, the model has a higher tolerance because their translation errors may not be absolute. As a result, the loss is small due to the small weight and the difficult tokens are always updated in a fine-grained way. 4 We evaluate our method on the Transformer (Vaswani et al., 2017) and conduct experiments on two widely-studied NMT tasks, WMT14 English-to-German (En-De) and WMT19 Chineseto-English (Zh-En). 4.1 Data Preparation EN-DE. The training data consists of 4.5M sentence pairs from WMT14. Each word in the corpus has been segmented into subword units using byte pair encoding (BPE) (Sennrich et al., 2016) with 32k merge operations. The vocabulary is shared among source and target languages. We select newstest2013 for validation and report the BLEU scores on newstest2014. ZH-EN. The training data is from WMT19 which consists of 20.5M sentence pairs. The number of merge operations in byte pair encoding (BPE) is set to 32K for both source and target languages. We use newstest2018 as our validation set and newstest2019 as our test set, which contain 4k and 2k sentences, respectively. BMI-based Objective We calculate the token-level weight by scaling BMI and adjusting the lower limit as follows: wj"
2021.emnlp-main.186,D16-1091,0,0.159107,"meng@tencent.com, jssu@xmu.edu.cn Abstract and extractive summarization (Barzilay et al., 2002; Nallapati et al., 2012). Dominant sentence ordering models can be Recently, inspired by the great success of deep classified into pairwise ordering models and learning in other NLP tasks, researchers have reset-to-sequence models. However, there is litsorted to neural sentence ordering models, which tle attempt to combine these two types of modcan be classified into: pairwise ordering models els, which inituitively possess complementary advantages. In this paper, we propose a nov(Chen et al., 2016; Agrawal et al., 2016; Li and el sentence ordering framework which introJurafsky, 2017; Moon et al., 2019; Kumar et al., duces two classifiers to make better use of pair2020; Prabhumoye et al., 2020; Zhu et al., 2021) wise orderings for graph-based sentence orderand set-to-sequence models (Gong et al., 2016; Ning (Yin et al., 2019, 2021). Specially, givguyen and Joty, 2017; Logeswaran et al., 2018; en an initial sentence-entity graph, we first Mohiuddin et al., 2018; Cui et al., 2018; Yin et al., introduce a graph-based classifier to predict 2019; Oh et al., 2019; Yin et al., 2020; Cui et al., pairwise orderings b"
2021.emnlp-main.186,N16-1147,0,0.0983536,"Missing"
2021.emnlp-main.186,D18-1465,0,0.174015,"ise ordering models els, which inituitively possess complementary advantages. In this paper, we propose a nov(Chen et al., 2016; Agrawal et al., 2016; Li and el sentence ordering framework which introJurafsky, 2017; Moon et al., 2019; Kumar et al., duces two classifiers to make better use of pair2020; Prabhumoye et al., 2020; Zhu et al., 2021) wise orderings for graph-based sentence orderand set-to-sequence models (Gong et al., 2016; Ning (Yin et al., 2019, 2021). Specially, givguyen and Joty, 2017; Logeswaran et al., 2018; en an initial sentence-entity graph, we first Mohiuddin et al., 2018; Cui et al., 2018; Yin et al., introduce a graph-based classifier to predict 2019; Oh et al., 2019; Yin et al., 2020; Cui et al., pairwise orderings between linked sentences. Then, in an iterative manner, based on the 2020; Yin et al., 2021). Generally, the former pregraph updated by previously predicted highdicts the relative orderings between pairwise senconfident pairwise orderings, another classifier tences, which are then leveraged to produce the is used to predict the remaining uncertain pairfinal ordered sentence sequence. Its advantage lies wise orderings. At last, we adapt a GRN-based in the lightweig"
2021.emnlp-main.186,2020.emnlp-main.511,0,0.200556,"eepLearnXMU/IRSEG. ordered sentences. Overall, these two kinds of models have their 1 Introduction own strengths, which are complementary to each With the rapid development and increasing applica- other. To combine their advantages, Yin et al. tions of natural language processing (NLP), model- (2020) propose FHDecoder that is equipped with ing text coherence has become a significant task, s- three pairwise ordering prediction modules to enince it can provide beneficial information for under- hance the pointer network decoder. Along this line, standing, evaluating and generating multi-sentence Cui et al. (2020) introduce BERT to exploit the texts. As an important subtask, sentence order- deep semantic connection and relative orderings ing aims at recovering unordered sentences back between sentences and achieve SOTA performance to naturally coherent paragraphs. It is required to when equipped with FHDecoder. However, there deal with logic and syntactic consistency, and has still exist two drawbacks: 1) their pairwise ordering increasingly attracted attention due to its wide ap- predictions only depend on involved sentence pairs, plications on several tasks such as text generation without considering"
2021.emnlp-main.186,N19-1423,0,0.0302209,"Missing"
2021.emnlp-main.186,P11-2022,0,0.0285887,"ns ∗ Corresponding author are relatively rough, ignoring distinct difficulties in 2407 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417 c November 7–11, 2021. 2021 Association for Computational Linguistics predicting different sentence pairs. Therefore, we believe that the potential of pairwise orderings in neural sentence ordering models has not been fully exploited. 2 Related Work Early studies mainly focused on exploring humandesigned features for sentence ordering (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005, 2008; Elsner and Charniak, 2011; GuinIn this paper, we propose a novel iterative pairaudeau and Strube, 2013). Recently, neural netwise ordering prediction framework which introwork based sentence ordering models have become duces two classifiers to make better use of pairwise dominant , consisting of the following two kinds of orderings for graph-based sentence ordering (Yin models: et al., 2019, 2021). As an extension of Sentence1) Pairwise models. Generally, they first preEnity Graph Recurrent Network (SE-GRN) (Yin dict the pairwise orderings between sentences and et al., 2019, 2021), our framework enriches the then use"
2021.emnlp-main.186,P19-1102,0,0.0244365,".9M 27min57s 24.0M 46min 25.0M 56min 128.0M Table 6: The runtime on the validation sets and the numbers of parameters for our enhanced models and baseline. Model Coherence SE-GRN (Yin et al., 2021) 46.71 59.47 IRSE-GRN IRSE-GRN+FHDecoder IRSE-GRN+BERT+FHDecoder 47.48 49.84 51.01 60.01 61.81 62.87 Table 5: Coherence probabilities of summaries reordered by different models using weights of 0.8 (left) and 0.5 (right). spect the validity of our proposed framework via multi-document summarization. Concretely, we train different neural sentence ordering models on a large-scale summarization corpus (Fabbri et al., 2019), and then individually use them to reorder the small-scale summarization data of DUC2004 (Task2). Finally, we use coherence probability proposed by (Nayeem and Chali, 2017) to evaluate the coherence of summaries. In this group of experiments, we conduct experiments using different weights: 0.5 and 0.8, as implemented in (Nayeem and Chali, 2017) and (Yin et al., 2020) respectively. The results are reported in Table 5. We can observe that the summaries reordered by IRSE-GRN and its variants achieve higher coherence probabilities than baseline, verifying the effectiveness of our proposed framewo"
2021.emnlp-main.186,D19-1633,0,0.0190762,"we can easily adapt the BART encoder as our sentence encoder. With similar motivation with ours, that is, to combine advantages of above-mentioned two kinds of models, Yin et al. (2020) introduced three pairwise ordering predicting modules (FHDecoder) to enhance the pointer network decoder of ATTOrderNet. Recently, Cui et al. (2020) proposed BERSON that is also equipped with FHDecoder and utilizes BERT to exploit the deep semantic connection and relative ordering between sentences. However, significantly different from them, we borrow the idea from the mask-predict framework (Gu et al., 2018; Ghazvininejad et al., 2019; Deng et al., 2020) to progressively incorporate pairwise ordering information into SE-Graph, which is the basis of our graph-based sentence ordering model. To the best of our knowledge, our work is the first attempt to explore iteratively refined GNN for sentence ordering. 3 Background resented as an undirected sentence-entity graph G = (V , E), where V ={vi }Ii=1 ∪{ˆ vj }Jj=1 and E ={ei,i0 }I,I ei,j }I,J ej,j 0 }J,J i=1,j=1 ∪{ˆ i=1,i0 =1 ∪{¯ j=1,j 0 =1 represent the nodes and edges respectively. Here, nodes include sentence nodes (such as vi ) and entity nodes (such as vˆj ), and each edge"
2021.emnlp-main.186,N12-1093,0,0.0223129,"texts. As an important subtask, sentence order- deep semantic connection and relative orderings ing aims at recovering unordered sentences back between sentences and achieve SOTA performance to naturally coherent paragraphs. It is required to when equipped with FHDecoder. However, there deal with logic and syntactic consistency, and has still exist two drawbacks: 1) their pairwise ordering increasingly attracted attention due to its wide ap- predictions only depend on involved sentence pairs, plications on several tasks such as text generation without considering other sentences in the same (Konstas and Lapata, 2012; Holtzman et al., 2018) set; 2) their one-pass pairwise ordering predictions ∗ Corresponding author are relatively rough, ignoring distinct difficulties in 2407 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417 c November 7–11, 2021. 2021 Association for Computational Linguistics predicting different sentence pairs. Therefore, we believe that the potential of pairwise orderings in neural sentence ordering models has not been fully exploited. 2 Related Work Early studies mainly focused on exploring humandesigned features for sentence orderi"
2021.emnlp-main.186,P03-1069,0,0.2014,"ltzman et al., 2018) set; 2) their one-pass pairwise ordering predictions ∗ Corresponding author are relatively rough, ignoring distinct difficulties in 2407 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417 c November 7–11, 2021. 2021 Association for Computational Linguistics predicting different sentence pairs. Therefore, we believe that the potential of pairwise orderings in neural sentence ordering models has not been fully exploited. 2 Related Work Early studies mainly focused on exploring humandesigned features for sentence ordering (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005, 2008; Elsner and Charniak, 2011; GuinIn this paper, we propose a novel iterative pairaudeau and Strube, 2013). Recently, neural netwise ordering prediction framework which introwork based sentence ordering models have become duces two classifiers to make better use of pairwise dominant , consisting of the following two kinds of orderings for graph-based sentence ordering (Yin models: et al., 2019, 2021). As an extension of Sentence1) Pairwise models. Generally, they first preEnity Graph Recurrent Network (SE-GRN) (Yin dict the pairwise order"
2021.emnlp-main.186,2020.acl-main.703,0,0.0185589,"eness of et al., 2019, 2020), Yin et al. (2019, 2021) repreour framework, we conduct extensive experiments sented input sentences with a unified SE-Graph and on several commonly-used datasets. Experimental then applied GRN to learn sentence representationresults and in-depth analyses show that our model s. Very recently, we notice that Chowdhury et al. enhanced with some proposed technologies (De- (2021) proposes a BART-based sentence ordering vlin et al., 2019; Yin et al., 2020) achieves the model. Please note that our porposed framework state-of-the-art performance. is compatible with BART (Lewis et al., 2020). For 2408 Figure 1: The architecture of SE-GRN model (Yin et al., 2019, 2021). example, we can easily adapt the BART encoder as our sentence encoder. With similar motivation with ours, that is, to combine advantages of above-mentioned two kinds of models, Yin et al. (2020) introduced three pairwise ordering predicting modules (FHDecoder) to enhance the pointer network decoder of ATTOrderNet. Recently, Cui et al. (2020) proposed BERSON that is also equipped with FHDecoder and utilizes BERT to exploit the deep semantic connection and relative ordering between sentences. However, significantly d"
2021.emnlp-main.186,D17-1019,0,0.0162027,"assifiers to make better use of pairwise dominant , consisting of the following two kinds of orderings for graph-based sentence ordering (Yin models: et al., 2019, 2021). As an extension of Sentence1) Pairwise models. Generally, they first preEnity Graph Recurrent Network (SE-GRN) (Yin dict the pairwise orderings between sentences and et al., 2019, 2021), our framework enriches the then use them to produce the final sentence order graph representation with iteratively predicted orvia ranking algorithms (Chen et al., 2016; Agrawal derings between pairwise sentences, which further et al., 2016; Li and Jurafsky, 2017; Kumar et al., benefits the subsequent generation of ordered sen2020; Prabhumoye et al., 2020; Zhu et al., 2021). tences. The basic intuitions behind our work are For example, Chen et al. (2016) first framed sentwo-fold. First, learning contextual sentence reptence ordering as a ranking task conditioned on resentations is helpful to predict pairwise orderpairwise scores. Agrawal et al. (2016) conducted ings. Second, difficulties of predicting ordering the same experiments as (Chen et al., 2016) in the vary with respect to different sentence pairs. Thus, task of image caption storytelling. Sim"
2021.emnlp-main.186,P18-1052,0,0.0865212,"e classified into: pairwise ordering models els, which inituitively possess complementary advantages. In this paper, we propose a nov(Chen et al., 2016; Agrawal et al., 2016; Li and el sentence ordering framework which introJurafsky, 2017; Moon et al., 2019; Kumar et al., duces two classifiers to make better use of pair2020; Prabhumoye et al., 2020; Zhu et al., 2021) wise orderings for graph-based sentence orderand set-to-sequence models (Gong et al., 2016; Ning (Yin et al., 2019, 2021). Specially, givguyen and Joty, 2017; Logeswaran et al., 2018; en an initial sentence-entity graph, we first Mohiuddin et al., 2018; Cui et al., 2018; Yin et al., introduce a graph-based classifier to predict 2019; Oh et al., 2019; Yin et al., 2020; Cui et al., pairwise orderings between linked sentences. Then, in an iterative manner, based on the 2020; Yin et al., 2021). Generally, the former pregraph updated by previously predicted highdicts the relative orderings between pairwise senconfident pairwise orderings, another classifier tences, which are then leveraged to produce the is used to predict the remaining uncertain pairfinal ordered sentence sequence. Its advantage lies wise orderings. At last, we adapt a GRN-base"
2021.emnlp-main.186,D19-1231,0,0.0114904,"., 2002; Nallapati et al., 2012). Dominant sentence ordering models can be Recently, inspired by the great success of deep classified into pairwise ordering models and learning in other NLP tasks, researchers have reset-to-sequence models. However, there is litsorted to neural sentence ordering models, which tle attempt to combine these two types of modcan be classified into: pairwise ordering models els, which inituitively possess complementary advantages. In this paper, we propose a nov(Chen et al., 2016; Agrawal et al., 2016; Li and el sentence ordering framework which introJurafsky, 2017; Moon et al., 2019; Kumar et al., duces two classifiers to make better use of pair2020; Prabhumoye et al., 2020; Zhu et al., 2021) wise orderings for graph-based sentence orderand set-to-sequence models (Gong et al., 2016; Ning (Yin et al., 2019, 2021). Specially, givguyen and Joty, 2017; Logeswaran et al., 2018; en an initial sentence-entity graph, we first Mohiuddin et al., 2018; Cui et al., 2018; Yin et al., introduce a graph-based classifier to predict 2019; Oh et al., 2019; Yin et al., 2020; Cui et al., pairwise orderings between linked sentences. Then, in an iterative manner, based on the 2020; Yin et al."
2021.emnlp-main.186,N16-1098,0,0.0204435,"adapt Equation 1 to incorporate ss-edge weights into the message aggregation of sentence-level nodes: X m(l) i = (l-1) wi,i0 · w(κ(l-1) , κ(l-1) i i0 )κi0 , (6) vi0 ∈Ni w(κ(l-1) , κ(l-1) i i0 ) = σ(Wg [κ(l-1) ; κ(l-1) i i0 ]). Here σ denotes sigmoid function and Wg is learnable parameter matrix. Equation 6 expresses that the sentence-level aggregation should consider not only the semantic representations of the two involved sentences, but also the relative ordering between them. In addition, other Equations are the same as those of conventional GRN, which have been described in Section §3.2. (Mostafazadeh et al., 2016) is about commonsense stories. Both two datasets are composed of 5-sentence stories and randomly split by 8:1:1 for the training/validation/test sets. • NIPS Abstract, AAN Abstract, arXiv Abstract. These three datasets consist of abstracts from research papers, which are collected from NIPS, ACL anthology and arXiv, respectively (Radev et al., 2016; Chen et al., 2016). The partitions for training/validation/test of each dataset are as follows: NIPS Abstract: 2,427/408/377, AAN Abstract: 8,569/962/2,626, arXiv Abstract: 884,912/110,614/110,615 for the training/validation/test sets. Settings. Fo"
2021.emnlp-main.186,2020.emnlp-main.181,0,0.0576172,"Missing"
2021.emnlp-main.186,P13-1010,0,0.0178174,"Missing"
2021.emnlp-main.186,W17-2407,0,0.167596,"erative predictions of pairwise ordering indeed benefit the learning of graph representations. Finally, the result in the last line indicates that removing noisy weights leads to a significant performance drop. It suggests that the utilization of noisy weights is useful for the training of iterative classifier, which makes our model more robust. Ablation Study 5.6 Summary Coherence Evaluation We conduct several experiments to investigate the impacts of our proposed components on ROCstory Following previous studies (Barzilay and Lapata, dataset and arXiv dataset which are the two largest 2005; Nayeem and Chali, 2017), we further in2414 Dataset SE-GRN IRSE-GRN IRSE-GRN+FHDecoder IRSE-GRN+BERT+FHDecoder Runtime #Params Runtime #Params Runtime #Params Runtime #Params NIPS abstract 6s 23.9M 6.2s 24.0M 18s 25.0M 29s 128.0M AAN abstract 31s 23.9M 32.5s 24.0M 1min8s 25.0M 1min20s 128.0M SIND 1min6s 23.9M 1min9s 24.0M 2min3s 25.0M 2min16s 128.0M ROCStory 2min 23.9M 2min5s 24.0M 4min2s 25.0M 4min42s 128.0M arXiv abstract 25min 23.9M 27min57s 24.0M 46min 25.0M 56min 128.0M Table 6: The runtime on the validation sets and the numbers of parameters for our enhanced models and baseline. Model Coherence SE-GRN (Yin et a"
2021.emnlp-main.186,P18-1152,0,0.0172356,"btask, sentence order- deep semantic connection and relative orderings ing aims at recovering unordered sentences back between sentences and achieve SOTA performance to naturally coherent paragraphs. It is required to when equipped with FHDecoder. However, there deal with logic and syntactic consistency, and has still exist two drawbacks: 1) their pairwise ordering increasingly attracted attention due to its wide ap- predictions only depend on involved sentence pairs, plications on several tasks such as text generation without considering other sentences in the same (Konstas and Lapata, 2012; Holtzman et al., 2018) set; 2) their one-pass pairwise ordering predictions ∗ Corresponding author are relatively rough, ignoring distinct difficulties in 2407 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417 c November 7–11, 2021. 2021 Association for Computational Linguistics predicting different sentence pairs. Therefore, we believe that the potential of pairwise orderings in neural sentence ordering models has not been fully exploited. 2 Related Work Early studies mainly focused on exploring humandesigned features for sentence ordering (Lapata, 2003; Barzil"
2021.emnlp-main.186,P17-1121,0,0.0409366,"Missing"
2021.emnlp-main.186,D19-1232,0,0.0324438,"Missing"
2021.emnlp-main.186,2020.acl-main.248,0,0.221993,"spired by the great success of deep classified into pairwise ordering models and learning in other NLP tasks, researchers have reset-to-sequence models. However, there is litsorted to neural sentence ordering models, which tle attempt to combine these two types of modcan be classified into: pairwise ordering models els, which inituitively possess complementary advantages. In this paper, we propose a nov(Chen et al., 2016; Agrawal et al., 2016; Li and el sentence ordering framework which introJurafsky, 2017; Moon et al., 2019; Kumar et al., duces two classifiers to make better use of pair2020; Prabhumoye et al., 2020; Zhu et al., 2021) wise orderings for graph-based sentence orderand set-to-sequence models (Gong et al., 2016; Ning (Yin et al., 2019, 2021). Specially, givguyen and Joty, 2017; Logeswaran et al., 2018; en an initial sentence-entity graph, we first Mohiuddin et al., 2018; Cui et al., 2018; Yin et al., introduce a graph-based classifier to predict 2019; Oh et al., 2019; Yin et al., 2020; Cui et al., pairwise orderings between linked sentences. Then, in an iterative manner, based on the 2020; Yin et al., 2021). Generally, the former pregraph updated by previously predicted highdicts the relativ"
2021.emnlp-main.186,Q19-1002,1,0.882575,"Missing"
2021.emnlp-main.186,2020.acl-main.712,1,0.704554,"Missing"
2021.emnlp-main.186,P18-1030,0,0.0181408,"y. During the process of updating hidden states, the messages for each node are aggregated from its adjacent nodes. Specifically, the sentence-level message m(l) i and (l) entity-level message m ˜ i for a sentence si are defined as follows: m(l) i = X (l-1) w(κ(l-1) , κ(l-1) i i0 )κi0 , vi0 ∈Ni m ˆ (l) i = X (l-1) w(κ ¯ (l-1) , (l-1) i j , r ij )j , (1) ˆi vj ∈N In this section, we give a brief introduction to the SE-GRN (Yin et al., 2019, 2021), which is selected as our baseline due to its competitive performance. As shown in Figure 1, SE-GRN is composed of a Bi-LSTM sentence encoder, GRN (Zhang et al., 2018) paragraph encoder, and a pointer network (Vinyals et al., 2015b) decoder. 3.1 Sentence-Entity Graph where κ(l-1) and (l-1) stand for the neighboring senj i0 tence and entity representations of the i-th sentence ˆi denote node vi at the (l − 1)-th layer, Ni and N the sets of neighboring sentences and entities of vi , and both w(∗) and w(∗) ¯ are gating functions with single-layer networks, involving associated node states and edge label rij (if any). Afterwards, κ(l-1) is updated by concatenating i its original representation κ(0) i , the messages from (l) (l) neighbours (mi and m ˆ i ) and t"
2021.emnlp-main.264,2020.emnlp-main.702,0,0.446094,"ring training instead of the inference stage. 4.3 Systems Mixer. A sequence-level training algorithm for text generations by combining both REINFORCE and cross-entropy (Ranzato et al., 2016). Minimal Risk Training. Minimal Risk Training (MRT) (Shen et al., 2016) introduces evaluation metrics (e.g., BLEU) as loss functions and aims to minimize expected loss on the training data. Target denoising. Meng et al. (2020a) and Meng et al. (2020b) propose to add noisy perturbations into decoder inputs for a more robust translation model against prediction errors. TeaForN. Teacher forcing with n-grams (Goodman et al., 2020) enables the standard teacher forcing with a broader view by a n-grams optimization. Sampling based on training steps. For distinction, we name vanilla scheduled sampling as Sampling based on training steps. We defaultly adopt the sigmoid decay following Zhang et al. (2019). Sampling with sentence oracles. Zhang et al. (2019) refine the sampling candidates of scheduled sampling with sentence oracles, i.e., predictions from beam search. Note that its sampling strategy is based on training steps with the sigmoid decay. Evaluation. For the machine translation task, we set the beam size to 4 and t"
2021.emnlp-main.264,P17-2058,0,0.017942,"both training steps and decoding steps can complement each other. 3.2 Sampling Based on Training Steps As the number of the training step i increases, the model should be exposed to its own predictions more frequently. Thus a decay strategy for sam• Sigmoid Decay5 : f (i) = k i , where e is pling golden tokens f (i) (in Section 2.3) is generk+e k the mathematical constant, and k ≥ 1 is a ally used in existing studies (Bengio et al., 2015; hyperparameter to adjust the decay. Zhang et al., 2019). At a specific training step i, 4 given a target sentence, f (i) is only related to i and Following Goyal et al. (2017), model predictions are the weighted sum of target embeddings over output probabilities. equally conducts the same sampling probability for As model predictions cause a mismatch with golden tokens, all decoding steps. Therefore, f (i) simulates an they can simulate translation errors of the inference scene. 5 inference scene with uniform error rates and still For simplicity, we abbreviate the ‘Inverse Sigmoid decay’ (Bengio et al., 2015) to ‘Sigmoid decay.’ remains a gap with the real inference scene. 3287 Figure 4: Examples of different strategies for 1 − g(t) based on the decoding step t. Th"
2021.emnlp-main.264,W04-3250,0,0.26816,"that its sampling strategy is based on training steps with the sigmoid decay. Evaluation. For the machine translation task, we set the beam size to 4 and the length penalty to 0.6 during inference. We use multibleu.perl to cal- Sampling based on decoding steps. Sampling culate cased sensitive BLEU scores for EN-DE based on decoding steps with exponential decay. and EN-FR, and use mteval-v13a.pl script to calculate cased sensitive BLEU scores for ZH-EN. Sampling based on training and decoding steps. Our sampling based on both training steps and deWe use the paired bootstrap resampling methods (Koehn, 2004) to compute the statistical signifi- coding steps with the ‘Composite’ method. cance of translation results. We report mean and 4.4 Main Results standard-error variation of BLEU scores over three runs. For the text summarization task, we respec- Machine Translation. We list translation qualitively set the beam size to 4/5 and length penalty to ties on three WMT tasks in Table 3. The sentence1.0/1.2 for Gigaword and CNN/DailyMail dataset level training based approaches (e.g., Mixer) bring following previous studies (Song et al., 2019; Qi limited improvements due to the high variance of et al.,"
2021.emnlp-main.264,2021.findings-acl.205,1,0.702906,"ceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3285–3296 c November 7–11, 2021. 2021 Association for Computational Linguistics of the inference scene during training. Scheduled sampling (Bengio et al., 2015) is a representative method, which samples tokens between golden references and model predictions with a scheduled probability. Zhang et al. (2019) further refine the sampling candidates by beam search. Mihaylova and Martins (2019) and Duckworth et al. (2019) extend scheduled sampling to the Transformer with a novel two-pass decoder architecture. Liu et al. (2021) develop a more fine-grained sampling strategy according to the model confidence. Although these sampling-based approaches have been shown effective and training efficient, there still exists an essential issue in their sampling strategies. In the real inference scene, the nature of sequential predictions quickly accumulates errors along with decoding steps, which yields higher error rates for larger decoding steps (Zhou et al., 2019; Zhang et al., 2020a) (Figure 1). However, most sampling-based approaches are merely based on training steps and equally treat all decoding steps2 . Namely, they"
2021.emnlp-main.264,2020.wmt-1.24,1,0.856643,"Missing"
2021.emnlp-main.264,P19-2049,0,0.0653044,"lculate the translation precision (similarly for the error rate) in all experiments. approaches, aiming to simulate the data distribution 3285 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3285–3296 c November 7–11, 2021. 2021 Association for Computational Linguistics of the inference scene during training. Scheduled sampling (Bengio et al., 2015) is a representative method, which samples tokens between golden references and model predictions with a scheduled probability. Zhang et al. (2019) further refine the sampling candidates by beam search. Mihaylova and Martins (2019) and Duckworth et al. (2019) extend scheduled sampling to the Transformer with a novel two-pass decoder architecture. Liu et al. (2021) develop a more fine-grained sampling strategy according to the model confidence. Although these sampling-based approaches have been shown effective and training efficient, there still exists an essential issue in their sampling strategies. In the real inference scene, the nature of sequential predictions quickly accumulates errors along with decoding steps, which yields higher error rates for larger decoding steps (Zhou et al., 2019; Zhang et al., 2020a) (Figu"
2021.emnlp-main.264,2020.findings-emnlp.217,0,0.0203261,"sets in Table 1. 4.2 Implementation Details Training Setup. For the translation task, we follow the default setup of the Transformerbase and Transformerbig models (Vaswani et al., 2017), and provide detailed setups in Appendix A (Table 7). All Transformer models are first trained by teacher forcing with 100k steps, and then trained with different training objects or scheduled sampling approaches for 300k steps. All experiments are conducted on 8 NVIDIA V100 GPUs, where each is allocated with a batch size of approximately 4096 tokens. For the text summarization task, we base on the ProphetNet (Qi et al., 2020) and follow its training setups. We set hyperparameters involved in various scheduled sampling strategies (i.e., f (i) and g(t)) according to the performance on validation sets of each tasks and list k in Table 2. For the linear decay, we set  and b to 0.2 and 1, respectively. Please note that scheduled sampling is only used during training instead of the inference stage. 4.3 Systems Mixer. A sequence-level training algorithm for text generations by combining both REINFORCE and cross-entropy (Ranzato et al., 2016). Minimal Risk Training. Minimal Risk Training (MRT) (Shen et al., 2016) introdu"
2021.emnlp-main.264,2020.tacl-1.18,0,0.0789567,"Missing"
2021.emnlp-main.264,D15-1044,0,0.28662,"glish-French, and WMT 2019 Chinese-English, respectively. When comparing with the stronger vanilla scheduled sampling method, our approaches bring further improvements by 0.58, 0.62, and 0.55 BLEU points on these WMT tasks, respectively. Moreover, our approaches generalize well to the text summarization task and achieve consistently better performance 2 For clarity in this paper, ‘training steps’ refer to the number of parameter updates and ‘decoding steps’ refer to the index of decoded tokens on the decoder side. on two popular benchmarks, i.e., CNN/DailyMail (See et al., 2017) and Gigaword (Rush et al., 2015). The main contributions of this paper can be summarized as follows3 : • To the best of our knowledge, we are the first that propose scheduled sampling methods based on decoding steps from the perspective of simulating the distribution of real translation errors, and provide in-depth analyses on the necessity of our proposals. • We investigate scheduled sampling based on both training steps and decoding steps, which yields further improvements, suggesting that our proposals complement existing studies. • Experiments on three large-scale WMT tasks and two popular text summarization tasks confir"
2021.emnlp-main.264,P17-1099,0,0.0319791,"2014 EnglishGerman, WMT 2014 English-French, and WMT 2019 Chinese-English, respectively. When comparing with the stronger vanilla scheduled sampling method, our approaches bring further improvements by 0.58, 0.62, and 0.55 BLEU points on these WMT tasks, respectively. Moreover, our approaches generalize well to the text summarization task and achieve consistently better performance 2 For clarity in this paper, ‘training steps’ refer to the number of parameter updates and ‘decoding steps’ refer to the index of decoded tokens on the decoder side. on two popular benchmarks, i.e., CNN/DailyMail (See et al., 2017) and Gigaword (Rush et al., 2015). The main contributions of this paper can be summarized as follows3 : • To the best of our knowledge, we are the first that propose scheduled sampling methods based on decoding steps from the perspective of simulating the distribution of real translation errors, and provide in-depth analyses on the necessity of our proposals. • We investigate scheduled sampling based on both training steps and decoding steps, which yields further improvements, suggesting that our proposals complement existing studies. • Experiments on three large-scale WMT tasks and two popula"
2021.emnlp-main.264,P16-1162,0,0.058466,"more predicted tokens to the model with the increase of both i and t (Figure 6 (c)). We will analyze effects of different h(i, t) in Section 5.2. 4 Experiments We validate our proposals on two important sequence generation tasks, i.e., machine translation and text summarization. 4.1 Tasks and Datasets Machine Translation. We use the standard WMT 2014 English-German (EN-DE), WMT 2014 English-French (EN-FR), and WMT 2019 ChineseEnglish (ZH-EN) datasets. We respectively build a shared source-target vocabulary for EN-DE and EN-FR, and unshared vocabularies for ZH-EN. We apply byte-pair encoding (Sennrich et al., 2016) with 32k merge operations for all datasets. Text Summarization. We use two popular sum8 marization datasets: (a) the non-anonymized verWe also tried f (i · (1 − g(t)) in preliminary experiments, but it slightly underperformed the above g(t · (1 − f (i))). sion of the CNN/DailyMail dataset (See et al., 3289 Variable Task Training Steps f (i) (vanilla) Decoding Steps g(t) (ours) Maximum Value Translation Summarization Translation Summarization 300,000 100,000 128 512 Hyperparameter k Linear Exponential Sigmoid -1/150,000 0.99999 20,000 -1/50,000 0.9999 15,000 -1/64 0.99 20 -1/256 0.999 50 Table"
2021.emnlp-main.264,P16-1159,0,0.310399,"lations (Zhou et al., 2019; Zhang et al., 2020a) (shown in Figure 1). Many techniques have been proposed to alleviate the exposure bias problem. To our knowledge, they 1 Introduction mainly fall into two categories. The one is sentenceNeural Machine Translation (NMT) has made level training, which treats the sentence-level metpromising progress in recent years (Sutskever et al., ric (e.g., BLEU) as a reward, and directly maxi2014; Bahdanau et al., 2015; Vaswani et al., 2017). mizes the expected rewards of generated sequences Generally, NMT models are trained to maximize (Ranzato et al., 2016; Shen et al., 2016; Rennie 1 et al., 2017; Pang and He, 2021). Although intuTo calculate the precision for training, we strictly match predicted tokens with ground-truth tokens word by word. itive, they generally suffer from slow and unstable When inference, we relax the strict matching to the fuzzy training due to the high variance of policy gradients matching within a local window of size 3, and truncate or and the credit assignment problem (Sutton, 1984; pad hypotheses to the same length of golden references. We also explore n-gram matching in preliminary experiments and Wiseman and Rush, 2016; Liu et al., 2"
2021.emnlp-main.264,P18-1083,0,0.0610921,"Missing"
2021.emnlp-main.264,D16-1137,0,0.0582174,"Missing"
2021.emnlp-main.264,P19-1426,1,0.760738,"Missing"
2021.emnlp-main.264,Q19-1006,0,0.11075,"ps. We randomly sample 100k training data from WMT 2014 EN-DE and report the average precision of 1k tokens for each decoding step1 . the likelihood of next token given previous golden tokens as inputs, i.e., teacher forcing (Salakhutdinov, 2014). However, at the inference stage, golden tokens are unavailable. The model is exposed to an unseen data distribution generated by itself. This discrepancy between training and inference is named as the exposure bias problem (Ranzato et al., 2016). With the growth of decoding steps, such discrepancy becomes more problematic due to error accumulations (Zhou et al., 2019; Zhang et al., 2020a) (shown in Figure 1). Many techniques have been proposed to alleviate the exposure bias problem. To our knowledge, they 1 Introduction mainly fall into two categories. The one is sentenceNeural Machine Translation (NMT) has made level training, which treats the sentence-level metpromising progress in recent years (Sutskever et al., ric (e.g., BLEU) as a reward, and directly maxi2014; Bahdanau et al., 2015; Vaswani et al., 2017). mizes the expected rewards of generated sequences Generally, NMT models are trained to maximize (Ranzato et al., 2016; Shen et al., 2016; Rennie"
2021.emnlp-main.6,D19-1459,0,0.0282992,"for human evaluation: (1) Coherence measures whether the translation is semantically coherent with the dialogue history; (2) Speaker measures whether the translation preserves the personality of the speaker; (3) Fluency measures whether the translation is fluent and gramDialogue Coherence Following (Lapata and Barzilay, 2005; Xiong et al., 2019), we measure dialogue coherence as sentence similarity, which is determined by the cosine similarity between two sentences s1 and s2 : 10 https://code.google.com/archive/p/word2vec/ Due to no available German dialogue datasets, we choose Taskmaster-1 (Byrne et al., 2019), where the English side of BConTrasT (Farajian et al., 2020) also comes from it. 11 sim(s1 , s2 ) = cos(f (s1 ), f (s2 )), 74 matically correct. First, we randomly sample 200 conversations from the test set of BMELD in Zh⇒En direction. Then, we use the 6 models in Tab. 6 to generate the translated utterances of these sampled conversations. Finally, we assign the translated utterances and their corresponding dialogue history utterances in target language to three postgraduate human annotators, and ask them to make evaluations from the above three criteria. The results in Tab. 6 show that our m"
2021.emnlp-main.6,2021.acl-long.444,1,0.676997,"onality, (4) we design the speaker identification task that judges whether the translated text is consistent with the personality of its original speaker. Together with the main chat translation task, the NCT model is optimized through the joint objectives of all these auxiliary tasks. In this way, the model is enhanced to capture dialogue coherence and speaker personality in conversation, which thus can generate more coherent and speaker-relevant translations. We validate our CSA-NCT framework on the datasets of different language pairs: BConTrasT (Farajian et al., 2020) (En⇔De1 ) and BMELD (Liang et al., 2021a) (En⇔Zh2 ). The experimental results show that our model achieves consistent improvements on four translation tasks in terms of both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), demonstrating its effectiveness and generalizability. Human evaluation further suggests that our model can generate more coherent and speaker-relevant translations compared to the existing related methods. Our contributions are summarized as follows: • We propose a multi-task learning framework with four auxiliary tasks to help the NCT model generate more coherent and speakerrelevant translations. • Ex"
2021.emnlp-main.6,D11-1084,0,0.0312702,"Missing"
2021.emnlp-main.6,2021.acl-long.310,1,0.78471,"Yu )). (5) For a training sample (CYs u , Yu ) with s∈{sx, sy}, we also use the NCT encoder to obtain the representations HYu of the target utterance Yu and HCYs u of the given speaker-specific history context CYs u . P|Yu |L Similar to the NUD task, HYu = |Y1u |t=1 he,t and L s the he,0 of CYu is used as HCYs . Then, to estimate u the probability in Eq. 5, the concatenation of HYu and HCYs is fed into a binary SI classifier, which u is another fully-connected layer on top of the NCT encoder: Speaker Identification (SI). As explored in (Bak and Oh, 2019; Wu et al., 2020; Liang et al., 2021b; Lin et al., 2021), the history utterances of a speaker can reflect a distinctive personality. Fig. 3(d) depicts the SI task in detail, where the NCT model is used to distinguish whether a translated utterance and a given speaker-specific history utterances are spoken by the same speaker. We also construct positive and negative training samples from the chat corpus. A positive sample (CYsxu , Yu ) with the label ` = 1 consists of the target utterance Yu and the speaker sx-specific history context CYsxu , because Yu is the translation of the utterance originally spoken by the speaker sx. A negative sample p(` ="
2021.emnlp-main.6,2020.acl-main.321,0,0.209009,"et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and another in Chinese as shown in Fig. 1), where a chat translator can be applied to help participants communi"
2021.emnlp-main.6,2020.emnlp-main.742,0,0.0181876,"nslations through multi-task learning. Cross-lingual Response Generation (CRG). The CRG task is similar to the MRG as shown in Fig. 3(b), where the NCT model is trained to generate the corresponding utterance Yu in target language which is coherent to the given dialogue history context CXu in source language. We first use the encoder of the NCT model to encode CXu , and then use the NCT decoder to predict Yu . The training objective of this task can be formulated as: 3.3.1 Dialogue Coherence Modeling Many studies (Kuang et al., 2018; Wang et al., 2019b; Xiong et al., 2019; Wang and Wan, 2019; Huang et al., 2020) have indicated that the modeling of global textual coherence can lead to more coherent text generation. Inspired by this, we add two response generation tasks and an utterance discrimination task during the NCT model training. All the three tasks are related to the dialogue coherence of conversations, thus introducing the modeling of dialogue coherence into the NCT model. LCRG = − |Yu | X log(p(Yu,t |CXu , Yu,<t )), t=1 p(Yu,t |CXu , Yu,<t ) = Softmax(Wc hL d,t + bc ), where hL d,t denotes the top-layer decoder hidden state at the t-th decoding step, Wcrg and bcrg are trainable parameters. No"
2021.emnlp-main.6,P18-1118,0,0.0191437,"In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that spe"
2021.emnlp-main.6,W18-6311,0,0.0725571,"“CSA-NCT” represents our proposed approach. Existing Context-Aware NMT Systems. Results on En⇔De. Under the Base setting, our model substantially outperforms the sentencelevel/context-aware baselines by a large margin (e.g., the previous best “Gate-Transformer+FT”), 1.02↑ on En⇒De and 1.12↑ on De⇒En. In term of TER, CSA-NCT also performs better on the two directions, 0.9↓ and 0.7↓ lower than “GateTransformer+FT” (the lower the better), respectively. Under the Big setting, on En⇒De and De⇒En, our model consistently surpasses the baselines and other existing systems again. • Dia-Transformer+FT (Maruf et al., 2018): The original model is RNN-based and an additional encoder is used to incorporate the mixed-language dialogue history. We reimplement it based on Transformer where an additional encoder layer is used to introduce the dialogue history into NMT model. • Doc-Transformer+FT (Ma et al., 2020): A state-of-the-art document-level NMT model based on Transformer sharing the first encoder layer to incorporate the dialogue history. • Gate-Transformer+FT (Zhang et al., 2018): A document-aware Transformer that uses a gate to incorporate the context information. Note that we share the Transformer encoder to"
2021.emnlp-main.6,2020.emnlp-main.175,0,0.0345857,"Missing"
2021.emnlp-main.6,N19-1313,0,0.0122593,"t al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and another in Chinese as shown in Fig. 1), where a chat translator can be"
2021.emnlp-main.6,D18-1325,0,0.0154581,"al Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking"
2021.emnlp-main.6,W04-3250,0,0.035116,"raining objective is finally formulated as Experiments Datasets and Metrics Datasets. As shown in Algorithm 1, the training of our CSA-NCT framework consists of two stages: (1) pre-train the model on a large-scale sentencelevel NMT corpus (WMT208 ); (2) fine-tune on the chat translation corpus (BConTrasT (Farajian et al., 2020) and BMELD (Liang et al., 2021a)). The dataset details (e.g., splits of training, validation or test sets) are described in Appendix A. Metrics. For fair comparison, we use the SacreBLEU9 (Post, 2018) and TER (Snover et al., 2006) with the statistical significance test (Koehn, 2004). For En⇔De, we report case-sensitive score following the WMT20 chat task (Farajian et al., 2020). For Zh⇒En, we report case-insensitive score. For En⇒Zh, the reported SacreBLEU is at the character level. 4.2 En⇒Zh 29.69/55.4 30.52/54.6 attention. All our Transformer models contain L = 6 encoder layers and L = 6 decoder layers and all models are trained using THUMT (Tan et al., 2020) framework. The training step for the first pre-training stage is set to T1 = 200,000 while that of the second fine-tuning stage is set to T2 = 5,000. The batch size for each GPU is set to 4096 tokens. All experime"
2021.emnlp-main.6,C18-1050,0,0.017875,"the NCT model can be enhanced to generate more coherent and speaker-relevant translations through multi-task learning. Cross-lingual Response Generation (CRG). The CRG task is similar to the MRG as shown in Fig. 3(b), where the NCT model is trained to generate the corresponding utterance Yu in target language which is coherent to the given dialogue history context CXu in source language. We first use the encoder of the NCT model to encode CXu , and then use the NCT decoder to predict Yu . The training objective of this task can be formulated as: 3.3.1 Dialogue Coherence Modeling Many studies (Kuang et al., 2018; Wang et al., 2019b; Xiong et al., 2019; Wang and Wan, 2019; Huang et al., 2020) have indicated that the modeling of global textual coherence can lead to more coherent text generation. Inspired by this, we add two response generation tasks and an utterance discrimination task during the NCT model training. All the three tasks are related to the dialogue coherence of conversations, thus introducing the modeling of dialogue coherence into the NCT model. LCRG = − |Yu | X log(p(Yu,t |CXu , Yu,<t )), t=1 p(Yu,t |CXu , Yu,<t ) = Softmax(Wc hL d,t + bc ), where hL d,t denotes the top-layer decoder h"
2021.emnlp-main.6,P19-1116,0,0.0223492,"Missing"
2021.emnlp-main.6,D15-1130,0,0.0255851,"oposed approach. 1 Figure 1: A dialogue example (En⇔Zh) when translating the utterance Xu . CRG: cross-lingual response generation. MRG: monolingual response generation. tion task becomes more important and has a wider range of applications. In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., th"
2021.emnlp-main.6,P02-1040,0,0.1097,"ther with the main chat translation task, the NCT model is optimized through the joint objectives of all these auxiliary tasks. In this way, the model is enhanced to capture dialogue coherence and speaker personality in conversation, which thus can generate more coherent and speaker-relevant translations. We validate our CSA-NCT framework on the datasets of different language pairs: BConTrasT (Farajian et al., 2020) (En⇔De1 ) and BMELD (Liang et al., 2021a) (En⇔Zh2 ). The experimental results show that our model achieves consistent improvements on four translation tasks in terms of both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), demonstrating its effectiveness and generalizability. Human evaluation further suggests that our model can generate more coherent and speaker-relevant translations compared to the existing related methods. Our contributions are summarized as follows: • We propose a multi-task learning framework with four auxiliary tasks to help the NCT model generate more coherent and speakerrelevant translations. • Extensive experiments on datasets of different language pairs demonstrate that our model with multi-task learning achieves the state-ofthe-art performances on the ch"
2021.emnlp-main.6,P18-1117,0,0.0188357,"ever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and another in Chinese as"
2021.emnlp-main.6,P19-1050,0,0.0330435,"Missing"
2021.emnlp-main.6,I17-3009,0,0.0152874,"gure 1: A dialogue example (En⇔Zh) when translating the utterance Xu . CRG: cross-lingual response generation. MRG: monolingual response generation. tion task becomes more important and has a wider range of applications. In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherenc"
2021.emnlp-main.6,W18-6319,0,0.016912,"raining, with the main chat translation task and four auxiliary tasks, the total training objective is finally formulated as Experiments Datasets and Metrics Datasets. As shown in Algorithm 1, the training of our CSA-NCT framework consists of two stages: (1) pre-train the model on a large-scale sentencelevel NMT corpus (WMT208 ); (2) fine-tune on the chat translation corpus (BConTrasT (Farajian et al., 2020) and BMELD (Liang et al., 2021a)). The dataset details (e.g., splits of training, validation or test sets) are described in Appendix A. Metrics. For fair comparison, we use the SacreBLEU9 (Post, 2018) and TER (Snover et al., 2006) with the statistical significance test (Koehn, 2004). For En⇔De, we report case-sensitive score following the WMT20 chat task (Farajian et al., 2020). For Zh⇒En, we report case-insensitive score. For En⇒Zh, the reported SacreBLEU is at the character level. 4.2 En⇒Zh 29.69/55.4 30.52/54.6 attention. All our Transformer models contain L = 6 encoder layers and L = 6 decoder layers and all models are trained using THUMT (Tan et al., 2020) framework. The training step for the first pre-training stage is set to T1 = 200,000 while that of the second fine-tuning stage is"
2021.emnlp-main.6,2020.wmt-1.60,0,0.0508315,"Missing"
2021.emnlp-main.6,2020.wmt-1.74,0,0.0176454,"ations. Acknowledgements Related Work The research work descried in this paper has been supported by the National Key R&D Program of China (2020AAA0108001) and the National Nature Science Foundation of China (No. 61976015, 61976016, 61876198 and 61370130). The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper. Chat NMT. Little prior work is available due to the lack of human-annotated publicly available data (Farajian et al., 2020). Therefore, some existing studies (Wang et al., 2016; Maruf et al., 2018; Zhang and Zhou, 2019; Rikters et al., 2020) mainly pay attention to designing methods to automatically construct the subtitle corpus, which may contain noisy bilingual utterances. Recently, Farajian et al. (2020) organize the WMT20 chat translation task and first provide a chat corpus post-edited by humans. More recently, based on document-level parallel corpus, Wang et al. (2021) propose to jointly identify omissions and typos within dialogue along with translating utterances by using the context. As a concurrent work, Liang et al. (2021a) provide a clean bilingual dialogue dataset and design a variational framework for NCT. Different"
2021.emnlp-main.6,D19-1085,0,0.0969453,"al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and another in Chinese as shown in Fig. 1), where a ch"
2021.emnlp-main.6,P16-1162,0,0.146835,"Missing"
2021.emnlp-main.6,2006.amta-papers.25,0,0.320291,"ion task, the NCT model is optimized through the joint objectives of all these auxiliary tasks. In this way, the model is enhanced to capture dialogue coherence and speaker personality in conversation, which thus can generate more coherent and speaker-relevant translations. We validate our CSA-NCT framework on the datasets of different language pairs: BConTrasT (Farajian et al., 2020) (En⇔De1 ) and BMELD (Liang et al., 2021a) (En⇔Zh2 ). The experimental results show that our model achieves consistent improvements on four translation tasks in terms of both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), demonstrating its effectiveness and generalizability. Human evaluation further suggests that our model can generate more coherent and speaker-relevant translations compared to the existing related methods. Our contributions are summarized as follows: • We propose a multi-task learning framework with four auxiliary tasks to help the NCT model generate more coherent and speakerrelevant translations. • Extensive experiments on datasets of different language pairs demonstrate that our model with multi-task learning achieves the state-ofthe-art performances on the chat translation task and signif"
2021.emnlp-main.6,D17-1301,0,0.0245697,"gure 1: A dialogue example (En⇔Zh) when translating the utterance Xu . CRG: cross-lingual response generation. MRG: monolingual response generation. tion task becomes more important and has a wider range of applications. In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherenc"
2021.emnlp-main.6,L16-1436,0,0.0257929,"at our model yields more coherent and speaker-relevant translations. Acknowledgements Related Work The research work descried in this paper has been supported by the National Key R&D Program of China (2020AAA0108001) and the National Nature Science Foundation of China (No. 61976015, 61976016, 61876198 and 61370130). The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper. Chat NMT. Little prior work is available due to the lack of human-annotated publicly available data (Farajian et al., 2020). Therefore, some existing studies (Wang et al., 2016; Maruf et al., 2018; Zhang and Zhou, 2019; Rikters et al., 2020) mainly pay attention to designing methods to automatically construct the subtitle corpus, which may contain noisy bilingual utterances. Recently, Farajian et al. (2020) organize the WMT20 chat translation task and first provide a chat corpus post-edited by humans. More recently, based on document-level parallel corpus, Wang et al. (2021) propose to jointly identify omissions and typos within dialogue along with translating utterances by using the context. As a concurrent work, Liang et al. (2021a) provide a clean bilingual dialo"
2021.emnlp-main.6,2020.amta-research.11,0,0.0387902,"ls (e.g., splits of training, validation or test sets) are described in Appendix A. Metrics. For fair comparison, we use the SacreBLEU9 (Post, 2018) and TER (Snover et al., 2006) with the statistical significance test (Koehn, 2004). For En⇔De, we report case-sensitive score following the WMT20 chat task (Farajian et al., 2020). For Zh⇒En, we report case-insensitive score. For En⇒Zh, the reported SacreBLEU is at the character level. 4.2 En⇒Zh 29.69/55.4 30.52/54.6 attention. All our Transformer models contain L = 6 encoder layers and L = 6 decoder layers and all models are trained using THUMT (Tan et al., 2020) framework. The training step for the first pre-training stage is set to T1 = 200,000 while that of the second fine-tuning stage is set to T2 = 5,000. The batch size for each GPU is set to 4096 tokens. All experiments in the first stage are conducted utilizing 8 NVIDIA Tesla V100 GPUs, while we use 4 GPUs for the second stage, i.e., fine-tuning. That gives us about 8*4096 and 4*4096 tokens per update for all experiments in the first-stage and second-stage, respectively. All models are optimized using Adam (Kingma and Ba, 2014) with β1 = 0.9 and β2 = 0.998, and learning rate is set to 1.0 for a"
2021.emnlp-main.6,2021.naacl-industry.14,0,0.0299036,"d suggestions to improve this paper. Chat NMT. Little prior work is available due to the lack of human-annotated publicly available data (Farajian et al., 2020). Therefore, some existing studies (Wang et al., 2016; Maruf et al., 2018; Zhang and Zhou, 2019; Rikters et al., 2020) mainly pay attention to designing methods to automatically construct the subtitle corpus, which may contain noisy bilingual utterances. Recently, Farajian et al. (2020) organize the WMT20 chat translation task and first provide a chat corpus post-edited by humans. More recently, based on document-level parallel corpus, Wang et al. (2021) propose to jointly identify omissions and typos within dialogue along with translating utterances by using the context. As a concurrent work, Liang et al. (2021a) provide a clean bilingual dialogue dataset and design a variational framework for NCT. Different from them, we focus on introducing the modeling of dialogue coherence and speaker personality into the NCT model with multi-task learning to promote the translation quality. References JinYeong Bak and Alice Oh. 2019. Variational hierarchical user-based conversation model. In Proceedings of EMNLP-IJCNLP, pages 1941–1950. Calvin Bao, Yow-"
2021.emnlp-main.6,W17-4811,0,0.0197872,"a wider range of applications. In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation invol"
2021.emnlp-main.6,W18-6312,0,0.023112,"ranslating the utterance Xu . CRG: cross-lingual response generation. MRG: monolingual response generation. tion task becomes more important and has a wider range of applications. In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter fo"
2021.emnlp-main.6,D19-1511,0,0.0743925,"al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and another in Chinese as shown in Fig. 1), where a ch"
2021.emnlp-main.6,Q18-1029,0,0.0199723,"MT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and an"
2021.emnlp-main.6,2020.acl-main.7,0,0.0390104,"1|CYsxu , Yu )) − log(p(` = 0|CYsyu , Yu )). (5) For a training sample (CYs u , Yu ) with s∈{sx, sy}, we also use the NCT encoder to obtain the representations HYu of the target utterance Yu and HCYs u of the given speaker-specific history context CYs u . P|Yu |L Similar to the NUD task, HYu = |Y1u |t=1 he,t and L s the he,0 of CYu is used as HCYs . Then, to estimate u the probability in Eq. 5, the concatenation of HYu and HCYs is fed into a binary SI classifier, which u is another fully-connected layer on top of the NCT encoder: Speaker Identification (SI). As explored in (Bak and Oh, 2019; Wu et al., 2020; Liang et al., 2021b; Lin et al., 2021), the history utterances of a speaker can reflect a distinctive personality. Fig. 3(d) depicts the SI task in detail, where the NCT model is used to distinguish whether a translated utterance and a given speaker-specific history utterances are spoken by the same speaker. We also construct positive and negative training samples from the chat corpus. A positive sample (CYsxu , Yu ) with the label ` = 1 consists of the target utterance Yu and the speaker sx-specific history context CYsxu , because Yu is the translation of the utterance originally spoken by"
2021.emnlp-main.6,D19-1081,0,0.0254488,"Missing"
2021.emnlp-main.6,P19-1193,0,0.0575932,"Missing"
2021.emnlp-main.6,D18-1049,0,0.0126714,"setting, on En⇒De and De⇒En, our model consistently surpasses the baselines and other existing systems again. • Dia-Transformer+FT (Maruf et al., 2018): The original model is RNN-based and an additional encoder is used to incorporate the mixed-language dialogue history. We reimplement it based on Transformer where an additional encoder layer is used to introduce the dialogue history into NMT model. • Doc-Transformer+FT (Ma et al., 2020): A state-of-the-art document-level NMT model based on Transformer sharing the first encoder layer to incorporate the dialogue history. • Gate-Transformer+FT (Zhang et al., 2018): A document-aware Transformer that uses a gate to incorporate the context information. Note that we share the Transformer encoder to obtain the context representation instead of utilizing the additional context encoder, which performs better in our experiments. 4.5 Results on En⇔Zh. We also conduct experiments on the BMELD dataset. Concretely, on En⇒Zh and Zh⇒En, our model also presents notable improvements over all comparison models by at least 2.43↑ and 0.77↑ BLEU gains under the Base setting, and by 1.73↑ and 1.43↑ BLEU gains under the Big setting, respectively. These results demonstrate t"
2021.emnlp-main.6,2020.emnlp-main.279,0,0.0525763,"Missing"
2021.findings-acl.105,P16-1154,0,0.0383867,"∈ RL is obtained by the dot-product attention:   exp hkti · qt  . Skti = P (4) exp h j j · qt k ∈Kt k t t the knowledge kts Finally, with the highest attention score is selected for further response generation. If the gold knowledge kt exists, we could train this task via the Cross Entropy (CE) loss: LKS = LCE (S, kt ) = − log Skt , 2.5 (5) Decoder Following (Dinan et al., 2019; Kim et al., 2020a), our Transformer-based decoder  takes the  representation concatenation Hrc = Hxt ; Hkts ∈ RNrc ,d of current utterance xt and the selected knowledge kts as input, and uses the copy mechanism (Gu et al., 2016) to generate responses. The process of generating a word can be formulated as follows:  snt = TD Hrc , yt&lt;n ∈ Rd pv = softmax (Wo snt ) ∈ R|V| pcp , ˜snt = MultiHeadcp (snt , Hrc , Hrc ) ,  pgen = sigmoid Wgen˜snt ∈ R1  p (V) = pgen ·pv + 1−pgen ·pcp ∈ R|V| where TD denotes the Transformer decoder, snt is the hidden vector for the n-th word in the response yt at t-th turn, pcp is the attention weight of the first head in the additional multi-head self-attention layer for the copy mechanism, which is short for MultiHeadcp (Vaswani et al., 2017), V is the vocabulary, and p (V) is the final ge"
2021.findings-acl.105,2020.sigdial-1.35,0,0.0770602,"Missing"
2021.findings-acl.105,D18-1248,0,0.0198208,"et al., 2020b) that finetunes GPT-2 (Radford et al., 2019) with the unsupervised pretrained knowledge selection module on unlabeled dialogues. We are different in two aspects: (1) Our DDSL leverages knowledge distillation to alleviate the label noise at the pretraining stage; (2) We adopt the sample weighting idea at the finetuning stage. And we will leverage GPT-2 for future study. Our work is inspired by Distant Supervision (DS), an effective method to generate labeled data with external knowledge base (KB) for information extraction (Mintz et al., 2009; Min et al., 2013; Zeng et al., 2015; Wang et al., 2018). Following this idea, Gopalakrishnan et al. (2019) use the oracle knowledge from DS to construct the TopicalChat dataset. Similarly, Qin et al. (2019b) obtain the weakly labeled data to train a KB retriever in the task-oriented dialogue system. Ren et al. (2019) propose a distantly supervised learning schema at segment level to effectively learn the topic transition vector. Although inspired by the similar idea, we are devoted to knowledge selection in the unsupervised setting, which is a different application of DS. Moreover, rather than just using distant supervision, we design our DDSL wit"
2021.findings-acl.105,P19-1369,0,0.0828184,"contradictory response in case 2 and does not provide new information in case 3. (4) Whereas, our UKSDGPF firstly selects the appropriate knowledge as human does, and then generate fluent and informative responses by alleviate the mismatch knowledge selection problem with the help of the pretraining-finetuning strategy. This indicates the importance of leveraging the selected knowledge properly for future study. 5 Related Work External knowledge has been wildly explored to enhance dialogue understanding and/or improve dialogue generation (Zhu et al., 2017; Liu et al., 2018; Chen et al., 2019; Wu et al., 2019; Chen et al., 2020a; Tuan et al., 2020; Sun et al., 2020; Zhang et al., 2020; Yu et al., 2020; Ni et al., 2021). To make use of knowledge, knowledge access is very important. Therefore, knowledge selection which selecting appropriate knowledge given the dialog context gains much attention (Zhang et al., 2019; Meng et al., 2019; Ren et al., 2019; Dinan 1237 Context Human Case 1 SKT UKSDGPF Are you a fan of elvis presley? KS Regarded as one of the most significant cultural icons of the 20th century, he is often referred to as the “king of rock and ...” DG You mean the king of rock and roll. Act"
2021.findings-acl.105,N19-1325,0,0.115059,"SDG et al., 2019; Tang and Hu, 2019; Qin et al., 2019a; Li et al., 2019b; Zheng and Zhou, 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020; Lin et al., 2020). However, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge dialogue generation in reality (Zheng et al., 2020). It is natural to extract the external knowledge via information retrieval technology. Several works regard the retrieved knowledge sentences as the preidentified document (Ghazvininejad et al., 2018; Michel Galley, 2018; Yang et al., 2019b; Gopalakrishnan et al., 2019). However, the retrieved document contains redundant and irrelevant information which are harmful for dialogue generation (Zhao et al., 2020b). Hence, knowledge selection which chooses an appropriate sentence from the pre-retrieved knowledge pool gains much attention and it plays the role of knowledge access for generation. Dinan et al. (2019) first propose knowledge selection for dialogue generation which are two sequential subtasks and the generation is based on the selected knowledge. Several works follow their setting and achieve improvements with latent vari"
2021.findings-acl.105,W19-5917,0,0.0526971,"Missing"
2021.findings-acl.105,D15-1203,0,0.0353767,"recent work (Zhao et al., 2020b) that finetunes GPT-2 (Radford et al., 2019) with the unsupervised pretrained knowledge selection module on unlabeled dialogues. We are different in two aspects: (1) Our DDSL leverages knowledge distillation to alleviate the label noise at the pretraining stage; (2) We adopt the sample weighting idea at the finetuning stage. And we will leverage GPT-2 for future study. Our work is inspired by Distant Supervision (DS), an effective method to generate labeled data with external knowledge base (KB) for information extraction (Mintz et al., 2009; Min et al., 2013; Zeng et al., 2015; Wang et al., 2018). Following this idea, Gopalakrishnan et al. (2019) use the oracle knowledge from DS to construct the TopicalChat dataset. Similarly, Qin et al. (2019b) obtain the weakly labeled data to train a KB retriever in the task-oriented dialogue system. Ren et al. (2019) propose a distantly supervised learning schema at segment level to effectively learn the topic transition vector. Although inspired by the similar idea, we are devoted to knowledge selection in the unsupervised setting, which is a different application of DS. Moreover, rather than just using distant supervision, we"
2021.findings-acl.105,2020.coling-main.392,1,0.736258,"ase 3. (4) Whereas, our UKSDGPF firstly selects the appropriate knowledge as human does, and then generate fluent and informative responses by alleviate the mismatch knowledge selection problem with the help of the pretraining-finetuning strategy. This indicates the importance of leveraging the selected knowledge properly for future study. 5 Related Work External knowledge has been wildly explored to enhance dialogue understanding and/or improve dialogue generation (Zhu et al., 2017; Liu et al., 2018; Chen et al., 2019; Wu et al., 2019; Chen et al., 2020a; Tuan et al., 2020; Sun et al., 2020; Zhang et al., 2020; Yu et al., 2020; Ni et al., 2021). To make use of knowledge, knowledge access is very important. Therefore, knowledge selection which selecting appropriate knowledge given the dialog context gains much attention (Zhang et al., 2019; Meng et al., 2019; Ren et al., 2019; Dinan 1237 Context Human Case 1 SKT UKSDGPF Are you a fan of elvis presley? KS Regarded as one of the most significant cultural icons of the 20th century, he is often referred to as the “king of rock and ...” DG You mean the king of rock and roll. Actually yes I am. best of all time. Don’t you agree? KS Elvis Aaron Presley (Ja"
2021.findings-acl.105,P18-1205,0,0.0307008,"ecoder. Experiments on two knowledge-grounded dialogue datasets show that our approach manages to select knowledge more accurately in the unsupervised setting and generates more informative responses, even outperforming many strong supervised baselines.1 1 Introduction To avoid general and dull dialogue generation (Li et al., 2016), knowledge-grounded dialogue which equips dialogue systems with external knowledge has become a popular research topic. Thanks to the hand-collected knowledge-grounded dialogue datasets which align each dialogue (even each utterance) with a pre-identified document (Zhang et al., 2018; Zhou et al., 2018; Moghe et al., 2018; Zhou et al., 2020), many researchers focus on injecting the given knowledge to generate informative responses and achieve promising results (Yavuz 1 The codes and model checkpoints will be available at https://github.com/ErenChan/UKSDG et al., 2019; Tang and Hu, 2019; Qin et al., 2019a; Li et al., 2019b; Zheng and Zhou, 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020; Lin et al., 2020). However, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge d"
2021.findings-acl.105,2020.emnlp-main.272,0,0.23476,"wever, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge dialogue generation in reality (Zheng et al., 2020). It is natural to extract the external knowledge via information retrieval technology. Several works regard the retrieved knowledge sentences as the preidentified document (Ghazvininejad et al., 2018; Michel Galley, 2018; Yang et al., 2019b; Gopalakrishnan et al., 2019). However, the retrieved document contains redundant and irrelevant information which are harmful for dialogue generation (Zhao et al., 2020b). Hence, knowledge selection which chooses an appropriate sentence from the pre-retrieved knowledge pool gains much attention and it plays the role of knowledge access for generation. Dinan et al. (2019) first propose knowledge selection for dialogue generation which are two sequential subtasks and the generation is based on the selected knowledge. Several works follow their setting and achieve improvements with latent variable models (Kim et al., 2020a; Chen et al., 2020b) or more complex selection mechanism (Niu et al., 2019; Meng et al., 2020; Zheng et al., 2020; Meng et al., 2021). Altho"
2021.findings-acl.105,2020.findings-emnlp.11,0,0.437795,"l., 2018; Zhou et al., 2020), many researchers focus on injecting the given knowledge to generate informative responses and achieve promising results (Yavuz 1 The codes and model checkpoints will be available at https://github.com/ErenChan/UKSDG et al., 2019; Tang and Hu, 2019; Qin et al., 2019a; Li et al., 2019b; Zheng and Zhou, 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020; Lin et al., 2020). However, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge dialogue generation in reality (Zheng et al., 2020). It is natural to extract the external knowledge via information retrieval technology. Several works regard the retrieved knowledge sentences as the preidentified document (Ghazvininejad et al., 2018; Michel Galley, 2018; Yang et al., 2019b; Gopalakrishnan et al., 2019). However, the retrieved document contains redundant and irrelevant information which are harmful for dialogue generation (Zhao et al., 2020b). Hence, knowledge selection which chooses an appropriate sentence from the pre-retrieved knowledge pool gains much attention and it plays the role of knowledge access for generation. Din"
2021.findings-acl.105,2020.acl-main.635,0,0.126015,"sets show that our approach manages to select knowledge more accurately in the unsupervised setting and generates more informative responses, even outperforming many strong supervised baselines.1 1 Introduction To avoid general and dull dialogue generation (Li et al., 2016), knowledge-grounded dialogue which equips dialogue systems with external knowledge has become a popular research topic. Thanks to the hand-collected knowledge-grounded dialogue datasets which align each dialogue (even each utterance) with a pre-identified document (Zhang et al., 2018; Zhou et al., 2018; Moghe et al., 2018; Zhou et al., 2020), many researchers focus on injecting the given knowledge to generate informative responses and achieve promising results (Yavuz 1 The codes and model checkpoints will be available at https://github.com/ErenChan/UKSDG et al., 2019; Tang and Hu, 2019; Qin et al., 2019a; Li et al., 2019b; Zheng and Zhou, 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020; Lin et al., 2020). However, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge dialogue generation in reality (Zheng et al., 2020). It is n"
2021.findings-acl.105,D18-1076,0,0.0647169,"on two knowledge-grounded dialogue datasets show that our approach manages to select knowledge more accurately in the unsupervised setting and generates more informative responses, even outperforming many strong supervised baselines.1 1 Introduction To avoid general and dull dialogue generation (Li et al., 2016), knowledge-grounded dialogue which equips dialogue systems with external knowledge has become a popular research topic. Thanks to the hand-collected knowledge-grounded dialogue datasets which align each dialogue (even each utterance) with a pre-identified document (Zhang et al., 2018; Zhou et al., 2018; Moghe et al., 2018; Zhou et al., 2020), many researchers focus on injecting the given knowledge to generate informative responses and achieve promising results (Yavuz 1 The codes and model checkpoints will be available at https://github.com/ErenChan/UKSDG et al., 2019; Tang and Hu, 2019; Qin et al., 2019a; Li et al., 2019b; Zheng and Zhou, 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020; Lin et al., 2020). However, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge dialogue generation"
2021.findings-acl.20,P19-1258,1,0.884804,"Missing"
2021.findings-acl.20,D14-1162,0,0.0896297,"lf-attention (Vaswani et al., 2017) is then performed on the vertices, which generates a relation score sij between node features ui and uj : Feature Representation Similar to (Anderson et al., 2018), we extract the image features by using a pretrained Faster RCNN (Ren et al., 2015b). We select µ object proposals for each image, where each object proposal is represented by a 2048-dimension feature vector. The obtained visual region features are denoted as µ v = vi=0 ∈ Rµ×dv . To extract the question features, each word is embedded into a 300-dimensional vector initialed with the Glove vector (Pennington et al., 2014). The word embeddings are taken as inputs by an LSTM encoder (Hochreiter and Schmidhuber, 1997), which produces the initial question representation q ∈ Rλ×dq . Each history sentence features are obtained as same as the question features. We concatenate the last state hlast ∈ Rdh of each turn history features to get the initial history represenGraph Attention sij = (Ui ui )T · Vj uj √ , du (1) where Ui and Vj are trainable parameters. We apply a softmax function over the correlation score sij to obtain weight αij : exp(sij + cu,lab(i,j) ) , j∈N (i) exp(sij + cu,lab(i,j) ) αij = P (2) where c{·}"
2021.findings-acl.205,P18-1167,0,0.0668752,"Missing"
2021.findings-acl.205,2020.emnlp-main.475,0,0.0488938,"Missing"
2021.findings-acl.205,2020.emnlp-main.702,0,0.274689,"d finally achieves its peak when tgolden = 0.9. Thus we finally set tgolden to 0.9. 4.3 Systems Mixer. A sequence-level training algorithm for text generations by combining both REINFORCE and cross-entropy (Ranzato et al., 2016). Minimal Risk Training. Minimal Risk Training (MRT) (Shen et al., 2016) introduces evaluation metrics (e.g., BLEU) as loss functions and aims to minimize expected loss on the training data. 2331 Model Transformerbase (Vaswani et al., 2017) Transformerbase (Vaswani et al., 2017) † + Mixer (Ranzato et al., 2016) † + Minimal Risk Training (Shen et al., 2016) † + TeaForN (Goodman et al., 2020) + TeaForN (Goodman et al., 2020) † + Self-paced learning (Wan et al., 2020) † + Vanilla scheduled sampling (Bengio et al., 2015) † + Target denoising (Meng et al., 2020) † + Sampling with sentence oracles (Zhang et al., 2019) + Sampling with sentence oracles (Zhang et al., 2019) † + Confidence-aware scheduled sampling (ours) † + Confidence-aware scheduled sampling with target denoising (ours) † Transformerbig (Vaswani et al., 2017) Transformerbig (Vaswani et al., 2017) † + Mixer (Ranzato et al., 2016) † + Minimal Risk Training (Shen et al., 2016) † + TeaForN (Goodman et al., 2020) + TeaForN ("
2021.findings-acl.205,P17-2058,0,0.241768,"er to adjust the sharpness of the decay. We draw visible examples for different decay strategies in Figure 2. 3 3.1 Model Confidence Estimation We explore two approaches to estimate model confidence at each token position. Decay Strategies on Training Steps • Inverse Sigmoid Decay: f (i) = we elaborate the fine-grained schedule strategy based on model confidence. Finally, we explore to sample more noisy tokens instead of predicted tokens for high-confidence positions. Approaches In this section, we firstly describe how to estimate model confidence at each token position. Secondly, 3 Following Goyal et al. (2017), model predictions are the weighted sum of target embeddings over output probabilities. As model predictions cause a mismatch with golden tokens, they can simulate translation errors of the inference scene. Predicted Translation Probability (PTP). Current NMT models are well-calibrated with regularization techniques in the training setting (Ott et al., 2018; M¨uller et al., 2019; Wang et al., 2020). Namely the predicted translation probability can directly serve as the model confidence. At the tth target token position, we calculate the model confidence conf (t) as follow: conf (t) = P (yt |y"
2021.findings-acl.205,2020.emnlp-main.176,0,0.0552264,"Missing"
2021.findings-acl.205,W04-3250,0,0.347725,"each is allocated with a batch size of approximately 4096 tokens. We use Adam optimizer (Kingma and Ba, 2014) with 4000 warmup steps. During training and the Monte Carlo Dropout process, we set dropout (Srivastava et al., 2014) rate to 0.1 for the Transformerbase and 0.3 for the Transformerbig . Evaluation. We set the beam size to 4 and the length penalty to 0.6 during inference. We use multibleu.perl to calculate case-sensitive BLEU scores for WMT14 EN-DE and EN-FR, and use mteval-v13a.pl to calculate case-sensitive BLEU scores for WMT19 ZH-EN. We use the paired bootstrap resampling methods (Koehn, 2004) to compute the statistical significance of test results. 4.2 Methods Transformerbase + PTP + Expectation + Variance Experiments Hyperparameter Experiments In this section, we elaborate hyperparameters settings involved in our approaches according to the performance on the validation set of WMT14 ENDE, and share these settings for all WMT tasks. Different Confidence Estimations. In this section, we analyze effects of different estimations for model confidence described in Section 3.1. As shown in Table 2, we observe that Monte Carlo dropout sampling based approaches (i.e., expectation and vari"
2021.findings-acl.205,P16-1162,0,0.238733,"Missing"
2021.findings-acl.205,P16-1159,0,0.0534505,"cing (Goodfellow et al., 2016). However, at the inference stage, golden tokens are unavailable. The model is exposed to an unseen data distribution generated by itself. This discrepancy between training and inference is named as the exposure bias problem (Ranzato et al., 2016). Many techniques have been proposed to alleviate the exposure bias problem. To our knowledge, they mainly fall into two categories. The one is sentence-level training, which treats the sentencelevel metric (e.g., BLEU) as a reward, and directly maximizes the expected rewards of generated sequences (Ranzato et al., 2016; Shen et al., 2016; Rennie et al., 2017). Although intuitive, they generally suffer from slow and unstable training due to the high variance of policy gradients and the credit assignment problem (Sutton, 1984; Liu et al., 2018; Wang et al., 2018). Another category is samplingbased approaches, aiming to simulate the data distribution of reference during training. Scheduled sampling (Bengio et al., 2015) is a representative method, which samples tokens between golden references and model predictions with a scheduled probability. Zhang et al. (2019) further refine the sampling space of scheduled sampling with pred"
2021.findings-acl.205,2020.wmt-1.24,1,0.755962,"ompetence, based on which we design fine-grained schedule strategies. Namely, we sample predicted tokens as target inputs for high-confidence positions and still ground-truth tokens for low-confidence positions. In this way, the NMT model is exactly exposed to corresponding tokens according to its real-time competence rather than coarse-grained predefined patterns. Additionally, we observe that most predicted tokens are the same as ground-truth tokens due to teacher forcing1 , degenerating scheduled sampling to the original teacher forcing mode. Therefore, we further expose more noisy tokens (Meng et al., 2020) (e.g., wordy and incorrect word order) instead of predicted ones for high-confidence token positions. Experimentally, we evaluate our approach on the Transformer (Vaswani et al., 2017) and conduct experiments on large-scale WMT 2014 EnglishGerman (EN-DE), WMT 2014 English-French (EN-FR), and WMT 2019 Chinese-English (ZHEN). The main contributions of this paper can be summarized as follows2 : • To the best of our knowledge, we are the first to propose confidence-aware scheduled sampling for NMT, which exactly samples corresponding tokens according to the real-time model competence rather than"
2021.findings-acl.205,P19-2049,0,0.537351,"though intuitive, they generally suffer from slow and unstable training due to the high variance of policy gradients and the credit assignment problem (Sutton, 1984; Liu et al., 2018; Wang et al., 2018). Another category is samplingbased approaches, aiming to simulate the data distribution of reference during training. Scheduled sampling (Bengio et al., 2015) is a representative method, which samples tokens between golden references and model predictions with a scheduled probability. Zhang et al. (2019) further refine the sampling space of scheduled sampling with predictions from beam search. Mihaylova and Martins (2019) and Duckworth et al. (2019) extend scheduled sampling to the Transformer with a novel twopass decoding architecture. Although these sampling-based approaches have been shown effective, most of them schedule the sampling probability based on training steps. We argue this schedule strategy has two following limitations: 1) It is far from exactly reflecting the real-time model competence; 2) It is only based on training steps and equally treat all token positions, which is too coarse-grained to guide the 2327 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2327–"
2021.findings-acl.205,2020.emnlp-main.80,0,0.0335257,"ausing no additional computation costs. Monte Carlo Dropout Sampling. The model confidence can be quantified by Bayesian neural networks (Buntine and Weigend, 1991; Neal, 2012), which place distributions over the weights of neural networks. We adopt widely used Monte Carlo dropout sampling (Gal and Ghahramani, 2016; Wang et al., 2019b) to approximate Bayesian inference. Given a batch of training data and current NMT model parameterized by θ, we repeatedly conduct forward propagation K times4 . On the k-th propagation, part of neurons θˆ(k) in network 2329 4 We empirically set K to 5 following Wan et al. (2020). θ are randomly deactivated. Eventually, we obtain K sets of model parameters {θˆ(k) }K k=1 and corresponding translation probabilities. We use the expectation or variance of translation probabilities to estimate the model confidence (Wang et al., 2019b). Intuitively, the higher expectation or, the lower variance of translation probabilities reflects higher model confidence. Formally at the t-th token position, we estimate the model confidence conf (t) that calculated by the expectation of translation probabilities: h iK conf (t) = E P (yt |y&lt;t , X, θˆ(k) ) k=1 (4) We also use the variance of"
2021.findings-acl.205,P19-1176,0,0.011971,"s follow: conf (t) = P (yt |y&lt;t , X, θ) (3) Since we base our approach on the Transformer with two-pass decoding (Mihaylova and Martins, 2019; Duckworth et al., 2019), above predicted translation probability can be directly obtained in the first-pass decoding (shown in Figure 1), causing no additional computation costs. Monte Carlo Dropout Sampling. The model confidence can be quantified by Bayesian neural networks (Buntine and Weigend, 1991; Neal, 2012), which place distributions over the weights of neural networks. We adopt widely used Monte Carlo dropout sampling (Gal and Ghahramani, 2016; Wang et al., 2019b) to approximate Bayesian inference. Given a batch of training data and current NMT model parameterized by θ, we repeatedly conduct forward propagation K times4 . On the k-th propagation, part of neurons θˆ(k) in network 2329 4 We empirically set K to 5 following Wan et al. (2020). θ are randomly deactivated. Eventually, we obtain K sets of model parameters {θˆ(k) }K k=1 and corresponding translation probabilities. We use the expectation or variance of translation probabilities to estimate the model confidence (Wang et al., 2019b). Intuitively, the higher expectation or, the lower variance of"
2021.findings-acl.205,D19-1073,0,0.0193893,"s follow: conf (t) = P (yt |y&lt;t , X, θ) (3) Since we base our approach on the Transformer with two-pass decoding (Mihaylova and Martins, 2019; Duckworth et al., 2019), above predicted translation probability can be directly obtained in the first-pass decoding (shown in Figure 1), causing no additional computation costs. Monte Carlo Dropout Sampling. The model confidence can be quantified by Bayesian neural networks (Buntine and Weigend, 1991; Neal, 2012), which place distributions over the weights of neural networks. We adopt widely used Monte Carlo dropout sampling (Gal and Ghahramani, 2016; Wang et al., 2019b) to approximate Bayesian inference. Given a batch of training data and current NMT model parameterized by θ, we repeatedly conduct forward propagation K times4 . On the k-th propagation, part of neurons θˆ(k) in network 2329 4 We empirically set K to 5 following Wan et al. (2020). θ are randomly deactivated. Eventually, we obtain K sets of model parameters {θˆ(k) }K k=1 and corresponding translation probabilities. We use the expectation or variance of translation probabilities to estimate the model confidence (Wang et al., 2019b). Intuitively, the higher expectation or, the lower variance of"
2021.findings-acl.205,2020.acl-main.278,0,0.0764053,"isy tokens instead of predicted tokens for high-confidence positions. Approaches In this section, we firstly describe how to estimate model confidence at each token position. Secondly, 3 Following Goyal et al. (2017), model predictions are the weighted sum of target embeddings over output probabilities. As model predictions cause a mismatch with golden tokens, they can simulate translation errors of the inference scene. Predicted Translation Probability (PTP). Current NMT models are well-calibrated with regularization techniques in the training setting (Ott et al., 2018; M¨uller et al., 2019; Wang et al., 2020). Namely the predicted translation probability can directly serve as the model confidence. At the tth target token position, we calculate the model confidence conf (t) as follow: conf (t) = P (yt |y&lt;t , X, θ) (3) Since we base our approach on the Transformer with two-pass decoding (Mihaylova and Martins, 2019; Duckworth et al., 2019), above predicted translation probability can be directly obtained in the first-pass decoding (shown in Figure 1), causing no additional computation costs. Monte Carlo Dropout Sampling. The model confidence can be quantified by Bayesian neural networks (Buntine and"
2021.findings-acl.205,P18-1083,0,0.167393,"s the exposure bias problem (Ranzato et al., 2016). Many techniques have been proposed to alleviate the exposure bias problem. To our knowledge, they mainly fall into two categories. The one is sentence-level training, which treats the sentencelevel metric (e.g., BLEU) as a reward, and directly maximizes the expected rewards of generated sequences (Ranzato et al., 2016; Shen et al., 2016; Rennie et al., 2017). Although intuitive, they generally suffer from slow and unstable training due to the high variance of policy gradients and the credit assignment problem (Sutton, 1984; Liu et al., 2018; Wang et al., 2018). Another category is samplingbased approaches, aiming to simulate the data distribution of reference during training. Scheduled sampling (Bengio et al., 2015) is a representative method, which samples tokens between golden references and model predictions with a scheduled probability. Zhang et al. (2019) further refine the sampling space of scheduled sampling with predictions from beam search. Mihaylova and Martins (2019) and Duckworth et al. (2019) extend scheduled sampling to the Transformer with a novel twopass decoding architecture. Although these sampling-based approaches have been shown"
2021.findings-acl.205,2020.emnlp-main.216,0,0.0777456,"Missing"
2021.findings-acl.205,P17-4012,0,0.0861965,"0 4 We conduct experiments on three large-scale WMT 2014 English-German (EN-DE), WMT 2014 English-French (EN-FR), and WMT 2019 ChineseEnglish (ZH-EN) translation tasks. We respectively build a shared source-target vocabulary for the ENDE and EN-FR datasets, and unshared vocabularies for the ZH-EN dataset. We apply byte-pair encoding (Sennrich et al., 2016) with 32k merge operations for all datasets. More datasets statistics are listed in Table 1. 4.1 Implementation Details Training Setup. We train the Transformerbase and Transformerbig models (Vaswani et al., 2017) with the open-source THUMT (Zhang et al., 2017). All Transformer models are first trained by teacher forcing with 100k steps, and then trained with different training objects or scheduled sampling approaches for 300k steps. All experiments are conducted on 8 NVIDIA Tesla V100 GPUs, where each is allocated with a batch size of approximately 4096 tokens. We use Adam optimizer (Kingma and Ba, 2014) with 4000 warmup steps. During training and the Monte Carlo Dropout process, we set dropout (Srivastava et al., 2014) rate to 0.1 for the Transformerbase and 0.3 for the Transformerbig . Evaluation. We set the beam size to 4 and the length penalty"
2021.findings-acl.205,P19-1426,1,0.904866,"Missing"
2021.findings-acl.205,Q19-1006,0,0.060016,"the performance bottleneck for Encoder20 Transformerbase becomes more evident (dashed blue line). Despite this, our approaches (solid blue line) still keep improving performance with the growth of decoder layers on the stronger Encoder20 Transformerbase . In summary, our confidence-aware schedule strategy brings a meaningful increase in the difficulty of decoders, and the bottleneck at the decoder side is alleviated to a certain extend. 5.4 Effects on Different Sequence Lengths Due to error accumulations, the exposure bias problem becomes more problematic with the growth of sequence lengths (Zhou et al., 2019; Zhang et al., 2020). Thus it is intuitive to verify the effectiveness of our approach over different sequence lengths. Considering the validation set of WMT14 EN-DE (3k) is too small to cover scenarios with various sentence lengths, we randomly select 10k training data with lengths from 10 to 100. As shown in Figure 5, our approach consistently outperform the Transformerbase model at different sequence lengths. Moreover, the improvements of our approach over the Transformerbase is gradually increasing with sentence lengths. Specifically, we observe more than 1.0 BLEU improvements when senten"
2021.findings-acl.205,2020.acl-main.620,0,0.0308177,"19b). Intuitively, the higher expectation or, the lower variance of translation probabilities reflects higher model confidence. Formally at the t-th token position, we estimate the model confidence conf (t) that calculated by the expectation of translation probabilities: h iK conf (t) = E P (yt |y&lt;t , X, θˆ(k) ) k=1 (4) We also use the variance of translation probabilities to estimate the model confidence conf (t) as an alternative: conf (t) = 1 − Var [P (yt |y&lt;t , X, θ)]K k=1 (5) where Var[·] denotes the variance of a distribution that calculated following the setting in (Wang et al., 2019b; Zhou et al., 2020). We will further analyze the effect of different confidence estimations in Section 4.2. 3.2 Confidence-Aware Scheduled Sampling The confidence score conf (t) quantifies whether the current NMT model is confident or hesitant on predicting the t-th target token. We take conf (t) as exact and real-time information to conduct a fine-grained schedule strategy in each training iteration. Specifically, a lower conf (t) indicates that the current model θ still struggles with the teacher forcing mode for the t-th target token, namely underfitting for the conditional probability P (yt |y&lt;t , X, θ). Thu"
2021.findings-acl.205,D15-1105,0,0.0236066,"Missing"
2021.findings-acl.38,2020.acl-main.728,0,0.110046,"on (Xu et al., 2015; Anderson et al., 2016, 2018; Cornia et al., 2020) and visual question answering (Ren et al., 2015a; Gao et al., 2015; Lu et al., 2016; Anderson et al., 2018). In the real world, our conversations (Chen et al., 2020b, 2019) usually have multiple turns. As an extension of conventional single-turn visual question answering, Das et al. (2017) introduce a multi-turn visual question answering task named visual dialogue, which aims to explore the ability of an AI agent to hold a meaningful multi-turn dialogue with humans in natural language about visual content. Visual dialogue (Agarwal et al., 2020; Wang et al., 2020; Qi et al., 2020; Murahari et al., 2020) requires agents to give a response on the basis of understanding both visual and textual content. One of the key challenges in visual dialogue is how to solve multimodal co-reference (Das et al., 2017; Kottur et al., 2018). Therefore, some fusion-based models (Das et al., 2017) are proposed to fuse spatial image features and textual features in order to obtain a joint representation. Then attention-based models (Lu et al., 2017; Wu et al., 2018; Kottur et al., 2018) are proposed to dynamically attend to spatial image features in orde"
2021.findings-acl.38,2020.emnlp-main.275,1,0.373377,"le of visual dialogue. The color in text background corresponds to the same color box in the image, which indicates the same entity. Our model firstly associates textual entities with objects explicitly and then gives contextually and visually coherent answers to contextual questions. Introduction Recently, there is increasing interest in visionlanguage tasks, such as image caption (Xu et al., 2015; Anderson et al., 2016, 2018; Cornia et al., 2020) and visual question answering (Ren et al., 2015a; Gao et al., 2015; Lu et al., 2016; Anderson et al., 2018). In the real world, our conversations (Chen et al., 2020b, 2019) usually have multiple turns. As an extension of conventional single-turn visual question answering, Das et al. (2017) introduce a multi-turn visual question answering task named visual dialogue, which aims to explore the ability of an AI agent to hold a meaningful multi-turn dialogue with humans in natural language about visual content. Visual dialogue (Agarwal et al., 2020; Wang et al., 2020; Qi et al., 2020; Murahari et al., 2020) requires agents to give a response on the basis of understanding both visual and textual content. One of the key challenges in visual dialogue is how to s"
2021.findings-acl.38,P19-1258,1,0.88914,"Missing"
2021.findings-acl.38,P19-1648,0,0.260117,"f understanding both visual and textual content. One of the key challenges in visual dialogue is how to solve multimodal co-reference (Das et al., 2017; Kottur et al., 2018). Therefore, some fusion-based models (Das et al., 2017) are proposed to fuse spatial image features and textual features in order to obtain a joint representation. Then attention-based models (Lu et al., 2017; Wu et al., 2018; Kottur et al., 2018) are proposed to dynamically attend to spatial image features in order to find related visual content. Furthermore, models based on object-level image features (Niu et al., 2019; Gan et al., 2019; Chen et al., 2020a; Jiang et al., 2020a; Nguyen 436 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 436–446 August 1–6, 2021. ©2021 Association for Computational Linguistics et al., 2020; Jiang et al., 2020b) are proposed to effectively leverage the visual content for multimodal co-reference. However, as implicit exploration of multimodal co-reference, these methods implicitly attend to spatial or object-level image features, which is trained with the whole model and is inevitably distracted by unnecessary visual content. Intuitively, specific mapping of obj"
2021.findings-acl.38,P19-1002,1,0.786372,"notes the multi-head self-attention layer (Vaswani et al., 2017), then  vg(n) = FFN vˆgni , (6) i where n = 1, . . . , Nv and FFN(·) denotes the position wise feed-forward networks (Vaswani et al., 2017). After Nv layers computation, we obtain the final visual grounding features vgi by: vgi v) = vg(N , i (7) Actually, there are some questions that do not contain any entities in the visual dialogue, such as “anything else ?”. For such questions, we use the features of the whole image instead, i.e. vgi = v. 2.4 Multimodal Incremental Transformer Inspired by the idea of incremental transformer (Li et al., 2019) which is originally designed for the single-modal dialogue task, we make an extension and propose a multimodal incremental transformer, which is composed of a Multimodal Incremental Transformer Encoder (MITE) and a Gated CrossAttention Decoder (GCAD). The MITE uses an incremental encoding scheme to encode multi-turn 3 For simplicity, we omit the descriptions of layer normalization and residual connection. 2.4.1 MITE To effectively encode multi-turn utterances grounded in visual content, we design the Multimodal Incremental Transformer Encoder (MITE). As shown in Figure 3 (b), at the i-th roun"
2021.findings-acl.38,2020.emnlp-main.269,0,0.122558,"nderson et al., 2016, 2018; Cornia et al., 2020) and visual question answering (Ren et al., 2015a; Gao et al., 2015; Lu et al., 2016; Anderson et al., 2018). In the real world, our conversations (Chen et al., 2020b, 2019) usually have multiple turns. As an extension of conventional single-turn visual question answering, Das et al. (2017) introduce a multi-turn visual question answering task named visual dialogue, which aims to explore the ability of an AI agent to hold a meaningful multi-turn dialogue with humans in natural language about visual content. Visual dialogue (Agarwal et al., 2020; Wang et al., 2020; Qi et al., 2020; Murahari et al., 2020) requires agents to give a response on the basis of understanding both visual and textual content. One of the key challenges in visual dialogue is how to solve multimodal co-reference (Das et al., 2017; Kottur et al., 2018). Therefore, some fusion-based models (Das et al., 2017) are proposed to fuse spatial image features and textual features in order to obtain a joint representation. Then attention-based models (Lu et al., 2017; Wu et al., 2018; Kottur et al., 2018) are proposed to dynamically attend to spatial image features in order to find related v"
2021.findings-emnlp.158,2020.acl-main.728,0,0.0194252,"ng and is optimized for the VD setting especially. Experimental results on the VisDial v1.0 dataset show that our approach achieves state-of-theart performance on both image-guessing task and question diversity. Human study further proves that our model generates more visually related, informative and coherent questions. 1 Introduction Visual Dialog (VD), which expects AI agents to conduct visually related dialog, has attracted growing interests due to its research significance and application prospects. Most of the work (Lu et al., 2017; Niu et al., 2019; Gan et al., 2019; Chen et al., 2020; Agarwal et al., 2020; Nguyen et al., 2020; Chen et al., 2021) pays attention to modeling an Answerer agent. However, it is also important to ∗ Equal contribution. Work was done when Zheng and Xu were interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. † Xiaojie Wang is the corresponding author. model a VD Questioner agent that can constantly ask visually related and informative questions. Previous researches (Das et al., 2017b; Murahari et al., 2019a; Zhou et al., 2019) have explored building open-domain VD Questioner under a QBot-A-Bot image-guessing game setting, namely GuessWhich (Das et al"
2021.findings-emnlp.158,2021.findings-acl.20,1,0.719069,"cially. Experimental results on the VisDial v1.0 dataset show that our approach achieves state-of-theart performance on both image-guessing task and question diversity. Human study further proves that our model generates more visually related, informative and coherent questions. 1 Introduction Visual Dialog (VD), which expects AI agents to conduct visually related dialog, has attracted growing interests due to its research significance and application prospects. Most of the work (Lu et al., 2017; Niu et al., 2019; Gan et al., 2019; Chen et al., 2020; Agarwal et al., 2020; Nguyen et al., 2020; Chen et al., 2021) pays attention to modeling an Answerer agent. However, it is also important to ∗ Equal contribution. Work was done when Zheng and Xu were interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. † Xiaojie Wang is the corresponding author. model a VD Questioner agent that can constantly ask visually related and informative questions. Previous researches (Das et al., 2017b; Murahari et al., 2019a; Zhou et al., 2019) have explored building open-domain VD Questioner under a QBot-A-Bot image-guessing game setting, namely GuessWhich (Das et al., 2017b). Given an undisclosed image, Gu"
2021.findings-emnlp.158,P19-1648,0,0.0192871,"Augmented Guesser (AugG) that is strong and is optimized for the VD setting especially. Experimental results on the VisDial v1.0 dataset show that our approach achieves state-of-theart performance on both image-guessing task and question diversity. Human study further proves that our model generates more visually related, informative and coherent questions. 1 Introduction Visual Dialog (VD), which expects AI agents to conduct visually related dialog, has attracted growing interests due to its research significance and application prospects. Most of the work (Lu et al., 2017; Niu et al., 2019; Gan et al., 2019; Chen et al., 2020; Agarwal et al., 2020; Nguyen et al., 2020; Chen et al., 2021) pays attention to modeling an Answerer agent. However, it is also important to ∗ Equal contribution. Work was done when Zheng and Xu were interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. † Xiaojie Wang is the corresponding author. model a VD Questioner agent that can constantly ask visually related and informative questions. Previous researches (Das et al., 2017b; Murahari et al., 2019a; Zhou et al., 2019) have explored building open-domain VD Questioner under a QBot-A-Bot image-guessing g"
2021.findings-emnlp.158,N16-1014,0,0.0347327,"of Q-Bot with the following metrics: 1) lastly the candidate images (images in validation Unique questions (Murahari et al., 2019a): mean split) are sorted according to their similarity to the number of unique questions in the 10-round dialog; prediction and compute the rank of the target im- 2) Mutual overlap (Deshpande et al., 2018): mean age. The evaluation metrics are: 1) MRR (Radev BLEU-4 (Papineni, 2002) overlap with the other et al., 2002): mean reciprocal rank of target image; 9 questions in the 10-round dialog; 3) Dist-n and 2) R@k (Das et al., 2017b): the existence of target Ent-n (Li et al., 2016; Zhang et al., 2018): number image in the top-k images; 3) Mean (Das et al., and entropy of distinct n-grams in the generated 2017b): mean rank of target image; 4) PMR (Das questions divided by the total number of tokens. et al., 2017b): percentile mean rank. As shown in Tab. 3, row 6 indicates that our We illustrate the results in Tab. 2. As shown in method achieves the new SOTA performance on row 10, our method achieves the best performance question diversity. Specifically, our RL-ReeQ on all metrics and becomes the new state of the achieves approximately 2 points improvement on art, with a"
2021.findings-emnlp.158,D19-1152,0,0.322691,"e to its research significance and application prospects. Most of the work (Lu et al., 2017; Niu et al., 2019; Gan et al., 2019; Chen et al., 2020; Agarwal et al., 2020; Nguyen et al., 2020; Chen et al., 2021) pays attention to modeling an Answerer agent. However, it is also important to ∗ Equal contribution. Work was done when Zheng and Xu were interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. † Xiaojie Wang is the corresponding author. model a VD Questioner agent that can constantly ask visually related and informative questions. Previous researches (Das et al., 2017b; Murahari et al., 2019a; Zhou et al., 2019) have explored building open-domain VD Questioner under a QBot-A-Bot image-guessing game setting, namely GuessWhich (Das et al., 2017b). Given an undisclosed image, GuessWhich can be regarded to have two stages: 1) Dialog generation stage: Q-Bot (Questioner, who only knows a caption of the image at first) successively asks questions to collect information about the image, and A-Bot (Answerer, who can see the image) answers the questions. 2) Guess stage: Q-Bot guesses the target image based on the generated dialog. Corresponding to the two stages, Q-Bot has two roles, i.e,"
2021.findings-emnlp.158,P02-1040,0,0.111109,"ance on image-guessing task. In concrete, QGen and A-Bot firstly generate 10-round dialog, then Question Diversity. We evaluate the question diGuesser makes a prediction about the unseen image, versity of Q-Bot with the following metrics: 1) lastly the candidate images (images in validation Unique questions (Murahari et al., 2019a): mean split) are sorted according to their similarity to the number of unique questions in the 10-round dialog; prediction and compute the rank of the target im- 2) Mutual overlap (Deshpande et al., 2018): mean age. The evaluation metrics are: 1) MRR (Radev BLEU-4 (Papineni, 2002) overlap with the other et al., 2002): mean reciprocal rank of target image; 9 questions in the 10-round dialog; 3) Dist-n and 2) R@k (Das et al., 2017b): the existence of target Ent-n (Li et al., 2016; Zhang et al., 2018): number image in the top-k images; 3) Mean (Das et al., and entropy of distinct n-grams in the generated 2017b): mean rank of target image; 4) PMR (Das questions divided by the total number of tokens. et al., 2017b): percentile mean rank. As shown in Tab. 3, row 6 indicates that our We illustrate the results in Tab. 2. As shown in method achieves the new SOTA performance on"
2021.findings-emnlp.158,radev-etal-2002-evaluating,0,0.122657,"AugG− means only stochastic negative samples are used in training. Noticeably, our result on Unique questions is approaching the upper bound, i.e., 10. Besides, our method also achieves better language diversity according to Mutual overlap, Dist-1, Dist-2, Ent-1 and Ent-2 (row 3 and row 6). A-Bot Performance. We evaluate the A-Bot performance in a retrieval setting, following Das et al. (2017a). Additional 100 candidate answers for each instance are provided and the model is evaluated by retrieval metrics: 1) NDCG (Järvelin and Kekäläinen, 2002): normalized discounted cumulative gain; 2) MRR (Radev et al., 2002): mean reciprocal rank of the ground truth answer; 3) R@k (Das et al., 2017a): the existence of the ground truth answer in the top-k answers; 4) Mean (Das et al., 2017a): mean rank of the ground truth answer. Tab. 4 shows the comparing results of A-Bot performance. Our model achieves higher NDCG, MRR, R@1, R@5 and R@10. # 1 2 3 4 5 6 DasQ+r1 DasQ+r2 DasQ+r3 ReeQ+r1 ReeQ+r2 ReeQ+r3 MRR↑ 25.65 32.19 32.77 32.27 32.78 33.65 R@1↑ 16.30 19.09 19.47 18.56 19.38 19.91 R@5↑ 40.50 46.27 46.75 46.75 47.00 48.5 R@10↑ 55.43 60.76 62.89 61.01 62.65 62.94 Mean↓ 28.57 21.64 20.45 19.58 19.46 18.05 PMR↑ 98.76"
2021.findings-emnlp.158,N19-1265,0,0.0204564,"al. (2017b) propose the task and generate questions in a sequence-tosequence fashion. Murahari et al. (2019a) propose to reduce repetition by penalizing the similarity in successive dialog hidden states. Zhou et al. (2019) retrieve the most-likely image, encode the image into a multi-modal context vector and use it to decode questions. These methods follow a sequenceto-sequence fashion while ReeQ explicitly uses related-entities as guidance to generate questions following a learned strategy. Our work is also relevant to the works (Zhang et al., 2017; Zhao and Tresp, 2018; Strub et al., 2017; Shekhar et al., 2019; Shukla et al., 2019; Xu et al., 2020) that focus on VD Questioner for GuessWhat?! (de Vries et al., 2017), where the goal is to locate a target object in the image and the answers can only be “yes/no/not available”. Compared to them, building a Questioner in a more open-domain VD setting is of more difficulty. Moreover, Q-Bot in GuessWhich has no access to visual information, making it harder to ask visually related questions. 7 Conclusion In this paper, we propose Related entity enhanced Questioner (ReeQ) and Augmented Guesser (AugG) to enhance Visual Dialog Questioner in both SL and RL. Re"
2021.findings-emnlp.158,P19-1646,0,0.0197375,"he task and generate questions in a sequence-tosequence fashion. Murahari et al. (2019a) propose to reduce repetition by penalizing the similarity in successive dialog hidden states. Zhou et al. (2019) retrieve the most-likely image, encode the image into a multi-modal context vector and use it to decode questions. These methods follow a sequenceto-sequence fashion while ReeQ explicitly uses related-entities as guidance to generate questions following a learned strategy. Our work is also relevant to the works (Zhang et al., 2017; Zhao and Tresp, 2018; Strub et al., 2017; Shekhar et al., 2019; Shukla et al., 2019; Xu et al., 2020) that focus on VD Questioner for GuessWhat?! (de Vries et al., 2017), where the goal is to locate a target object in the image and the answers can only be “yes/no/not available”. Compared to them, building a Questioner in a more open-domain VD setting is of more difficulty. Moreover, Q-Bot in GuessWhich has no access to visual information, making it harder to ask visually related questions. 7 Conclusion In this paper, we propose Related entity enhanced Questioner (ReeQ) and Augmented Guesser (AugG) to enhance Visual Dialog Questioner in both SL and RL. ReeQ generates question"
2021.findings-emnlp.158,D19-1014,0,0.0362213,"Missing"
2021.findings-emnlp.212,D14-1179,0,0.00928461,"Missing"
2021.findings-emnlp.212,2020.acl-main.747,0,0.036555,"ere Slang is the language sets for training. Proportional Sampling. Another method is sampling by proportion (Neubig and Hu, 2018). This method improves the model’s performance on high resource languages and reduces the performance of the model on low resource languages. Specifically, we calculate its sampling weight ψi for each language pair i as i |DTrain | , k k∈Slang |DTrain | ψi = P (4) where DTrain is the training corpora of language i. Temperature-based Sampling. It samples the language pairs according to the corpora size exponentiated by a temperature term τ (Arivazhagan et al., 2019; Conneau et al., 2020) as 1/τ ψi = P pk 1/τ k∈Slang pk i |DTrain | . k k∈Slang |DTrain | ∗ θ = argmin θ MultiDDS-S. MultiDDS-S (Wang et al., 2020) is a dynamic sampling method performing differentiable data sampling. It takes turns to optimize the sampling weights of different languages and the multilingual machine translation model, showing more significant potential than static sampling methods. This method optimizes the sample weight ψ to minimize the development loss as follows ψ ∗ = argmin L(θ∗ ; DDev ), (6) ψi L(θ; DTrain ), (7) i=1 where DDev and DTrain denote the development corpora and the training corpora"
2021.findings-emnlp.212,2020.coling-tutorials.3,0,0.150973,"e (Barrault et al., 2020). This would lead has been learned; and 2) HRLs-evaluated Comto low learning competencies for distant languages petence, evaluating whether an LRL is ready to be learned according to HRLs’ Self-evaluated compared to closely related languages. Therefore, Competence. Based on the above competenmultilingual machine translation is inherently imcies, we utilize the proposed CCL-M algobalanced, and dealing with this imbalance is critirithm to gradually add new languages into the cal to advancing multilingual machine translation training set in a curriculum learning manner. (Dabre et al., 2020). Furthermore, we propose a novel competenceTo address the above problem, existing balancaware dynamic balancing sampling strategy for better selecting training samples in muling methods can be divided into two categories, tilingual training. Experimental results show i.e., static and dynamic. 1) Among static balancing that our approach has achieved a steady and methods, temperature-based sampling (Arivazhasignificant performance gain compared to the gan et al., 2019) is the most common one, comprevious state-of-the-art approach on the TED pensating for the gap between different training talks"
2021.findings-emnlp.212,N19-4009,0,0.0116756,"(Johnson et al., 2017). 4.2 Implementation Details Baseline. We select three static heuristic strategies: uniform sampling, proportional sampling, and temperature-based sampling (τ = 5), and the bitext models for the baseline. In addition, we compare our approach with the previous state-of-the-art sampling method, MultiDDS-S (Wang et al., 2020). All baseline methods use the same model and the same set of hyper-parameters as our approach. 4 https://github.com/google/ sentencepiece Model. We validate our approach upon the multilingual Transformer (Vaswani et al., 2017) implemented by fairseq5 (Ott et al., 2019). The number of layers is 6 and the number of attention heads is 4, with the embedding dimension dmodel of 512 and the feed-forward dimension dff of 1024 as (Wang et al., 2020). For training stability, we adopt PreLN (Xiong et al., 2020) for the layer-norm (Ba et al., 2016) module. For M2O tasks, we use a shared encoder with a vocabulary of 64k. Similarly, for O2M tasks, we use a shared decoder with a vocabulary of 64k. Training Setup. We use the Adam optimizer (Kingma and Ba, 2014) with β1 = 0.9, β2 = 0.98 to optimize the model. Further, the same learning rate schedule as Vaswani et al. (2017"
2021.findings-emnlp.212,W18-6301,0,0.0194124,"l., 2016) module. For M2O tasks, we use a shared encoder with a vocabulary of 64k. Similarly, for O2M tasks, we use a shared decoder with a vocabulary of 64k. Training Setup. We use the Adam optimizer (Kingma and Ba, 2014) with β1 = 0.9, β2 = 0.98 to optimize the model. Further, the same learning rate schedule as Vaswani et al. (2017) is used, i.e., linearly increase the learning rate for 4000 steps to 2e-4 and decay proportionally to the inverse square root of the step number. We accumulate the batch size to 9,600 and adopt half-precision training implemented by apex6 for faster convergence (Ott et al., 2018). For regularization, we also use a dropout (Srivastava et al., 2014) p = 0.3 and a label smoothing (Szegedy et al., 2016) ls = 0.1. As for our approach, we sample 256 candidates from each languages’ development corpora every 100 steps to calculate the Self-evaluated Competence c for each language and HRLs-evaluated Competence cˆ for each LRL. Evaluation. In practice, we perform a grid search for the best threshold t in {0.5, 0.6, 0.7, 0.8, 0.9, 1.0}, and select the checkpoints with the lowest weighted loss7 on the development sets to conduct the evaluation. The corresponding early stopping p"
2021.findings-emnlp.212,N19-1119,0,0.0412201,"Missing"
2021.findings-emnlp.212,W18-6319,0,0.0149429,"ple 256 candidates from each languages’ development corpora every 100 steps to calculate the Self-evaluated Competence c for each language and HRLs-evaluated Competence cˆ for each LRL. Evaluation. In practice, we perform a grid search for the best threshold t in {0.5, 0.6, 0.7, 0.8, 0.9, 1.0}, and select the checkpoints with the lowest weighted loss7 on the development sets to conduct the evaluation. The corresponding early stopping patience is set to 10. For target sentence generation, we set the beam size to 5 and a length penalty of 1.0. Following Wang et al. (2020), we use the SacreBLEU (Post, 2018) to evaluate the model performance. In the end, we compare our result with MultiDDS-S using paired bootstrap resampling (Koehn, 2004) for significant test. 4.3 Results Main Results. The main results are listed in Table 1. As we can see, both methods significantly 5 https://github.com/pytorch/fairseq https://github.com/NVIDIA/apex 7 This loss is calculated by averaging the loss of each samples in development corpora of all languages, which is equivalent to taking the proportional weighted average of the loss for each language. 2486 6 M2O Related Diverse Method O2M Related Diverse Bitext Models"
2021.findings-emnlp.212,2020.acl-main.148,0,0.0392629,"Missing"
2021.naacl-main.126,D17-1134,0,0.0316569,"Missing"
2021.naacl-main.126,D16-1130,0,0.0155853,"he H-LSTM suffers from Implicit discourse relation recognition (IDRR) the long-distance forgetting problem, which may aims to identify logical relations between two ad- fail to model the long-distance and non-continuous dependency across multiple sentences (like green jacent sentences in discourse without the guidance lines in Figure 1). of connectives (e.g., because, but), which is one of the major challenges in discourse parsing. With To overcome these limitations, we propose a the rise of deep learning, lots of sentence-modeling novel Context Tracking Network (CT-Net), which based methods (Liu and Li, 2016; Rönnqvist et al., can track essential context for each sentence from 2017; Bai and Zhao, 2018; Xu et al., 2019; Shi the intricate discourse, without being affected by and Demberg, 2019) have emerged in the field of the spatial distance. The CT-Net computes contexIDRR. These methods typically focus on modeling tual representation through two main steps. Firstly, the local semantics of these two sentences, without it converts the paragraph into the paragraph assoconsidering wider discourse context. ciation graph (PAG) (Figure 1), which contains Contextual information plays an important role th"
2021.naacl-main.126,P19-1411,0,0.0228543,"Missing"
2021.naacl-main.126,D14-1162,0,0.0879083,"ained updating mechanism, which is executed T rounds. At the t-th round, we denote the state of the i-th sentence node as git , and the state of the j-th token node of the i-th sentence as hti,j . The states transition from the (t-1)-th to the t-th round consists of three computation processes: token-tosentence updating, sentence-to-sentence updating and sentence-to-token updating. The first two processes are responsible for updating sentence nodes, while the last one is for updating token nodes. Node Initialization. When t = 0, we initialize token nodes with the concatenation of char, GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings. And the dimension is reduced: glove elmo h0i,j = xi,j = W [xchar i,j ; xi,j ; xi,j ] + b (1) where W , b are parameters. The sentence node gi0 is initialized as the average of its token nodes. Token-to-Sentence Updating. This process updates the sentence state git with the token states of last round ht−1 i,j . We employ Sentence-state LSTM (SLSTM) (Zhang et al., 2018) to achieve this. SLSTM is a novel graph RNN that converts a sentence into a graph with one global sentence node and several local word nodes, just like the sub-graph in the PAG (inside"
2021.naacl-main.126,N18-1202,0,0.0292719,"executed T rounds. At the t-th round, we denote the state of the i-th sentence node as git , and the state of the j-th token node of the i-th sentence as hti,j . The states transition from the (t-1)-th to the t-th round consists of three computation processes: token-tosentence updating, sentence-to-sentence updating and sentence-to-token updating. The first two processes are responsible for updating sentence nodes, while the last one is for updating token nodes. Node Initialization. When t = 0, we initialize token nodes with the concatenation of char, GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings. And the dimension is reduced: glove elmo h0i,j = xi,j = W [xchar i,j ; xi,j ; xi,j ] + b (1) where W , b are parameters. The sentence node gi0 is initialized as the average of its token nodes. Token-to-Sentence Updating. This process updates the sentence state git with the token states of last round ht−1 i,j . We employ Sentence-state LSTM (SLSTM) (Zhang et al., 2018) to achieve this. SLSTM is a novel graph RNN that converts a sentence into a graph with one global sentence node and several local word nodes, just like the sub-graph in the PAG (inside the dotted ellipse in Figure 2)"
2021.naacl-main.126,P09-1077,0,0.0256947,"Missing"
2021.naacl-main.126,prasad-etal-2008-penn,0,0.00847306,"connective prediction (CP). These three tasks share the same encoder but use three different MLPs. The objective function is as follows: L=−α r∈R k∈Ni 1594 CX idrr j yidrr j log ybidrr j=1 −γ Ccp X −β CX edrr j j yedrr log ybedrr j=1 j j ycp log ybcp j=1 (6) where α, β, γ are adjustable hyper-parameters. yidrr , yedrr and ycp are ground-truth labels of IDRR, EDRR and CP respectively, while ybidrr , ybedrr and ybcp are corresponding predictions. Cidrr , Cedrr and Ccp represent the number of classes of IDRR, EDRR, and CP respectively. 3 3.1 Experiment Dataset We conduct experiments on PDTB 2.0 (Prasad et al., 2008), which contains 16, 224 implicit instances and 18, 459 explicit instances. We perform one-vs-others binary classification and 4-way classification on 4 top-level discourse relations: comparison (Comp.), contingency (Cont.), expansion (Exp.), and temporal (Temp.). Following Pitler et al. (2009), we use sections 2-20 for training, sections 21-22 for test and sections 0-1 for validation. The metric is F1 score, and for 4-way classification, we calculate the macro-average F1 score. 3.2 Implementation Details Details of the PAG. We set the number of sentences to build PAGs as 6, and use zero paddi"
2021.naacl-main.126,P17-1093,0,0.0416331,"Missing"
2021.naacl-main.126,P17-2040,0,0.0341838,"Missing"
2021.naacl-main.126,D19-1586,0,0.491369,"Missing"
2021.naacl-main.126,P19-1058,0,0.013328,"may aims to identify logical relations between two ad- fail to model the long-distance and non-continuous dependency across multiple sentences (like green jacent sentences in discourse without the guidance lines in Figure 1). of connectives (e.g., because, but), which is one of the major challenges in discourse parsing. With To overcome these limitations, we propose a the rise of deep learning, lots of sentence-modeling novel Context Tracking Network (CT-Net), which based methods (Liu and Li, 2016; Rönnqvist et al., can track essential context for each sentence from 2017; Bai and Zhao, 2018; Xu et al., 2019; Shi the intricate discourse, without being affected by and Demberg, 2019) have emerged in the field of the spatial distance. The CT-Net computes contexIDRR. These methods typically focus on modeling tual representation through two main steps. Firstly, the local semantics of these two sentences, without it converts the paragraph into the paragraph assoconsidering wider discourse context. ciation graph (PAG) (Figure 1), which contains Contextual information plays an important role three types of edges between sentences, namely (1) in understanding sentences. Take the paragraph adjacency edge ("
2021.naacl-main.126,P18-1030,0,0.0153513,"sible for updating sentence nodes, while the last one is for updating token nodes. Node Initialization. When t = 0, we initialize token nodes with the concatenation of char, GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings. And the dimension is reduced: glove elmo h0i,j = xi,j = W [xchar i,j ; xi,j ; xi,j ] + b (1) where W , b are parameters. The sentence node gi0 is initialized as the average of its token nodes. Token-to-Sentence Updating. This process updates the sentence state git with the token states of last round ht−1 i,j . We employ Sentence-state LSTM (SLSTM) (Zhang et al., 2018) to achieve this. SLSTM is a novel graph RNN that converts a sentence into a graph with one global sentence node and several local word nodes, just like the sub-graph in the PAG (inside the dotted ellipse in Figure 2). At the t-th round, the hidden state of i-th sentence git is computed as follows: t−1 t−1 t−1 git = SLSTMh→g (ht−1 i,0 , hi,1 ..., hi,|Si |, gi ) (2) where SLSTMh→g represents the process of updating the sentence state with token states by SLSTM, and its detailed equations are shown in Appendix A. |Si |is the number of tokens in Si . Sentence-to-Sentence Updating. After merging t"
C12-2080,P04-1015,0,0.052779,"rd segmentation, dictionary maximum matching. Proceedings of COLING 2012: Posters, pages 819–828, COLING 2012, Mumbai, December 2012. 819 1 Introduction Word segmentation is a basic and important task for information processing of Chinese. Most effective approaches (Xue and Shen, 2003; Ng and Low, 2004) to CWS treat it as a character tagging task, in which the model used to make tagging decisions can be trained by discriminative methods, such as Maximum Entropy (ME) (Ratnaparkhi and Adwait, 1996), Conditional Random Fields (Lafferty et al., 2001), perceptron training algorithm (Collins, 2002; Collins and Roark, 2004), etc. These methods have achieved good results, but rely on large scale high quality annotated corpora, which are rare in resource-poor languages and domains. Besides, directly adapting a classifier trained on one domain to another domain leads to poorer performance 1 . Given a dictionary, dictionary maximum matching (DMM) is an alternative in the case of no available annotated corpora, but its performance is not satisfying due to the poor ability on OOV words recognition. Dict Dictionary Matching Feature Extraction Training with ME Classifier Raw Text Figure 1: The pipeline of our method. In"
C12-2080,P11-2095,0,0.0356264,"Missing"
C12-2080,P09-1059,1,0.854383,"dictionary matching based feature instances extraction, we give a brief introduction of gap tagging classification strategy for segmentation. Formally, a Chinese sentence can be represented as a character sequence: C1:n = C1 C2 · · · Cn , where Ci (i = 1, · · · , n) is a character. We explicitly add the gap Gi (i = 1, · · · , n − 1) of character Ci and Ci+1 to the sentence C1:n , denoted as C1:n |G1:n−1 = C1 G1 C2 G2 · · · Gn−1 Cn . Then the segmented results with gaps represented as follows: 1 C1:e1 |G1:e1 −1 , Ge1 , Ce1 +1:e2 |Ge1 +1:e2 −1 , Ge2 , · · · , Gem−1 , Cem−1 +1:em |Gem−1 +1:em −1 Jiang et al. (2009) describe a similar situation. We also tried to directly adapt a classifier trained with news corpus to Chinese medicine patent corpus, which led to dramatic decrease in accuracy. 820 where Ci: j |Gi: j−1 denotes the character-gap sequence with gaps Gi: j−1 . As is shown above, there are two kinds of gaps, one occurs inside a word, such as G1 · · · Ge1 −1 ; the other occurs between two words, such as Ge1 . Word Segmentation can be treated as a gap tagging problem. 2.1 From Character Tagging to Gap Tagging Ng and Low (2004) give a boundary tag to each character denoting its relative position in"
C12-2080,D12-1038,1,0.780811,"Missing"
C12-2080,N09-1036,0,0.0434464,"Missing"
C12-2080,P09-1058,0,0.0458646,"Missing"
C12-2080,P11-1141,0,0.0231981,"Missing"
C12-2080,J09-4006,0,0.0361854,"Missing"
C12-2080,P09-1012,0,0.0726165,"Missing"
C12-2080,W04-3236,0,0.0199792,":e2 |Ge1 +1:e2 −1 , Ge2 , · · · , Gem−1 , Cem−1 +1:em |Gem−1 +1:em −1 Jiang et al. (2009) describe a similar situation. We also tried to directly adapt a classifier trained with news corpus to Chinese medicine patent corpus, which led to dramatic decrease in accuracy. 820 where Ci: j |Gi: j−1 denotes the character-gap sequence with gaps Gi: j−1 . As is shown above, there are two kinds of gaps, one occurs inside a word, such as G1 · · · Ge1 −1 ; the other occurs between two words, such as Ge1 . Word Segmentation can be treated as a gap tagging problem. 2.1 From Character Tagging to Gap Tagging Ng and Low (2004) give a boundary tag to each character denoting its relative position in a word. There are four boundary tags: &quot;b&quot; for a character that begins a word, &quot;m&quot; for a character that occurs in the middle of a word, &quot;e&quot; for a character that ends a word, and &quot;s&quot; for a character that occurs as a single-character word. Following Ng and Low (2004), the feature templates and the corresponding instances are listed in Table 1. Feature Template Ci (i = −2, · · · , 2) Ci Ci+1 (i = −2, · · · , 1) C−1 C1 Pu(C0 ) T (C−2 )T (C−1 )T (C0 )T (C1 )T (C2 ) Instances C−2 = , C−1 = , C0 = , C1 = , C2 = C−2 C−1 = , C−1 C0"
C12-2080,W96-0213,0,0.579617,"Missing"
C12-2080,P11-1139,0,0.0233332,"Missing"
C12-2080,P12-1027,0,0.0602497,"Missing"
C12-2080,J11-3001,0,0.0503322,"Missing"
C12-2080,C10-1132,0,0.0490416,"Missing"
C12-2080,W03-1728,0,0.08618,"Missing"
C12-2080,P07-1106,0,0.0647758,"Missing"
C12-2080,P08-1101,0,0.0425805,"Missing"
C12-2080,D10-1082,0,0.0260372,"Missing"
C14-1104,J93-2003,0,0.0392884,"eled independently in order to break the synchronous constraint, resorting to dependency structures with dependency edges as atomic manipulating units. Large-scale experiments on Chinese to English translation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most"
C14-1104,W09-2307,0,0.0218502,"translation model that achieves the state-of-the-art performance. 2 Dependency Edge-based Transfer Model 2.1 Edges in Dependency Trees Given a sentence, its dependency tree is a directed acyclic graph with words in the sentence as nodes. An example dependency tree is shown in Figure 1 (a). An edge in the tree represents a dependency relationship between a pair of words, a head and a dependent. When a nominal dependent acts as a subject and modifies a verbal head, they usually have a fixed relative position. In Figure 1 (a), “`aob¯am˘a” modifies “f¯ab`u”. The grammatical relation label nsubj (Chang et al., 2009) between them denotes that a noun phrase acts as the subject of a clause. “`aob¯am˘a” is on the left of “f¯ab`u”. Based on the above observations, we take the edge as the elementary structure of a dependency tree and regard a dependency tree to be a set of edges. Definition 1. An source side edge is a 4-tuple e = ⟨H, D, P, R⟩, where H is the head, D is the dependent, P denotes the relative position between H and D, left or right, R is the grammatical relation label . In Figure 1 (b), the upper sides of transfer rules are source side edges extracted from the dependency tree. 1104 àobāmǎ jīntiān"
C14-1104,P05-1033,0,0.294445,"Large-scale experiments on Chinese to English translation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspo"
C14-1104,P05-1066,0,0.140302,"Missing"
C14-1104,W04-1513,0,0.0163552,"hibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspondence between two languages. In contrast, the analysis-transfer-generation methodology i"
C14-1104,D09-1023,0,0.0382526,"Missing"
C14-1104,J14-2005,0,0.0313161,"Missing"
C14-1104,N04-1014,0,0.0296779,"omic manipulating units. Large-scale experiments on Chinese to English translation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the struc"
C14-1104,2006.amta-papers.8,0,0.028994,"nslation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspondence between two languages. In contrast, the analysis"
C14-1104,N03-1017,0,0.0760436,"traint, resorting to dependency structures with dependency edges as atomic manipulating units. Large-scale experiments on Chinese to English translation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some"
C14-1104,W02-1610,0,0.105786,"Missing"
C14-1104,C04-1090,0,0.0308064,"ur model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspondence between two languages. In contrast, the analysis-transfer-g"
C14-1104,P06-1077,1,0.84498,"periments on Chinese to English translation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspondence between two"
C14-1104,W02-1018,0,0.0298914,"k the synchronous constraint, resorting to dependency structures with dependency edges as atomic manipulating units. Large-scale experiments on Chinese to English translation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin"
C14-1104,D13-1108,1,0.743092,"Missing"
C14-1104,P08-1023,1,0.818582,"se to English translation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspondence between two languages. In co"
C14-1104,P02-1038,0,0.10659,"for the loss of word information. The single node translations of the generalized words are also extracted. The unaligned words of the target side is handled by extending nsp(head) and tsp(dependent) on both left and right directions. We do this process similar with the method of Och and Ney (2004). We might obtain m(m ≥ 1) extended rules from an acceptable edge. The frequency of each rule is divided by m. We take the extracted rule set as observed data and make use of relative frequency estimator to obtain the translation probabilities P (t|s) and P (s|t). 4 Decoding and Generation We follow Och and Ney (2002), using a general log-linear model to score the sentence generated by each concatenation of the target edges. Let c be concatenations that concatenate the target edges to generate the target sentence e. The probability of e is defined as： P (c) ∝ ∏ ϕi (c)λi (1) i where ϕi (c) are features defined on concatenations and λi are feature weights. In our experiments of this paper, thirteen features are used as follows: • Transfer rules translation probabilities P (t|s) and P (s|t), and lexical translation probabilities Plex (t|s) and Plex (s|t); • Bilingual phrases probabilities Pbp (t|s) and Pbp (s"
C14-1104,J03-1002,0,0.00640586,"tion limit to our model. We take open source phrase-based system Moses (with default configuration)1 as our baseline system. 5.1 Experimental Setting Our training corpus consists of 1.25M sentence pairs from LDC data, including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. To obtain the dependency trees of the source side, we parse the source sentences with Stanford Parser (Klein and Manning, 2003) into projective dependency structures with nodes annotated by POS tags and edges by dependency labels. To obtain the word alignments, we run GIZA++ (Och and Ney, 2003) on the corpus in both directions and apply “grow-diag-and” refinement (Koehn et al., 2003). We extract the phrases covering no more than 10 nodes of the fixed structures. We use SRILM (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We use NIST MT Evaluation test set 2002 as our development set, 2003-2005 NIST datasets as testsets. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric2 . We make use of the minimum error rate training algorithm (Och, 2003) in order to maximize the BLE"
C14-1104,J04-4002,0,0.115754,"“f¯ab`u” modifying any noun, respectively. The generalized rules are also called 1107 *:VV nsubj Generalize head àobāmǎ * fābù t cen adja nsubj non obama ĸ àobāmǎ issue non fābù nsubj ent ac -adj Generalize dependent obama ķ *:NN t issue en adjac non* Ĺ Figure 4: Generalization of transfer rule. un-lexicalized rules for the loss of word information. The single node translations of the generalized words are also extracted. The unaligned words of the target side is handled by extending nsp(head) and tsp(dependent) on both left and right directions. We do this process similar with the method of Och and Ney (2004). We might obtain m(m ≥ 1) extended rules from an acceptable edge. The frequency of each rule is divided by m. We take the extracted rule set as observed data and make use of relative frequency estimator to obtain the translation probabilities P (t|s) and P (s|t). 4 Decoding and Generation We follow Och and Ney (2002), using a general log-linear model to score the sentence generated by each concatenation of the target edges. Let c be concatenations that concatenate the target edges to generate the target sentence e. The probability of e is defined as： P (c) ∝ ∏ ϕi (c)λi (1) i where ϕi (c) are"
C14-1104,P03-1021,0,0.00786932,"ents, we run GIZA++ (Och and Ney, 2003) on the corpus in both directions and apply “grow-diag-and” refinement (Koehn et al., 2003). We extract the phrases covering no more than 10 nodes of the fixed structures. We use SRILM (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus. We use NIST MT Evaluation test set 2002 as our development set, 2003-2005 NIST datasets as testsets. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric2 . We make use of the minimum error rate training algorithm (Och, 2003) in order to maximize the BLEU score of the development set. The statistical significance test is performed by sign-test (Collins et al., 2005). 5.2 Influence of Maximum Distortion Limit Figure 6 gives the performance of our system with different maximum distortion limits in terms of uncased BLEU of three NIST test sets. The performance of different distortion limit are consistent on both development set and three test sets. Maximum distortion limit 2 gets the best performances. A low distortion limit may cause the target sentence been translated more close to the sequence of the source, espec"
C14-1104,P05-1034,0,0.665237,"performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspondence between two languages. In contrast, the analysis-transfer-generation methodology in rule-based transla"
C14-1104,2001.mtsummit-papers.53,0,0.114607,"Missing"
C14-1104,P08-1066,0,0.175114,"ificantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspondence between two languages. In contrast, the analysis-transfer-generation methodology in rule-based translation solves the mac"
C14-1104,D11-1020,1,0.945069,"ming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which directly model the structural correspondence between two languages. In contrast, the analysis-transfer-generation methodology in rule-based translation solves the machine translation p"
C14-1104,W07-0706,1,0.820474,"el and some of those previous works acquired transfer rules automatically from word-aligned corpus (Richardson et al., 2001; Carbonell et al., 2002; Lavoie et al., 2002; Lin, 2004). Gimpel and Smith (2009) and Gimpel and Smith (2014) used quasi-synchronous dependency grammar for MT and they are similar to our idea of doing transfer of dependency syntax in a non-synchronous setting. They do the translation as monolingual lattice parsing. As dependency-based system, Lin (2004) used path as the transfer unit and regarded the translation problem with minimal path covering. Quirk et al. (2005) and Xiong et al. (2007) used treelets to model the source dependency tree using synchronous grammars. Quirk et al. (2005) projected the source dependency structure into target side by word alignment and faced the problem of non-isomorphism between languages. Xiong et al. (2007) directly modeled the treelet to the corresponding target string to alleviate the problem. Xie et al. (2011) directly specified the ordering information in head-dependents rules that represent the source side as head-dependents relations and the target side as string. Differently, our model uses a much simpler elementary structure, edge, which"
C14-1104,P01-1067,0,0.164083,"th dependency edges as atomic manipulating units. Large-scale experiments on Chinese to English translation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models. 1 Introduction Researches in statistical machine translation have been flourishing in recent years. Statistical translation methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002; Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004; Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004; Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher generalization capability by leveraging the hierarchical structures in natural languages, and achieve the state-of-the-art performance in these years. Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation procedures which"
C14-1104,carbonell-etal-2002-automatic,0,\N,Missing
C16-1205,D14-1179,0,0.0570165,"Missing"
C16-1205,N16-1102,0,0.0151591,"ce representation than just history of attention, and is therefore a more powerful model for machine translation. 2181 We also provide some actual translation examples (see Appendix) to show that our I NTERACTIVE ATTENTION can get better performance then baselines, especially on solving under-translation problem. We think the interactive mechanism of NMTIA is helpful for the decoder to automatically distinguish which parts have been translated and which parts are under-translated. 5 Related Work Our work is related to recent works that focus on improving attention models (Luong et al., 2015a; Cohn et al., 2016; Feng et al., 2016). Luong et al. (2015a) proposed to use global and local attention models to improve translation performance. They use a global one to attend to all source words and a local one to look at a subset of source words at a time. Cohn et al. (2016) extended the attention-based NMT to include structural biases from word-based alignment models, which achieved improvements across several language pairs. Feng et al. (2016) added implicit distortion and fertility models to attentionbased NMT to achieve translation improvements. These works are different with our I NTERACTIVE ATTENTION"
C16-1205,P05-1066,0,0.21663,"Missing"
C16-1205,P15-1001,0,0.0579261,"can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comparable results with"
C16-1205,N03-1017,0,0.0445318,"T baselines: Groundhog and RNNsearch? (our implementation of improved attention model as described in Section 2.2), and our I NTERACTIVE ATTENTION model (NMTIA ). The “*” indicates that the results are significantly (p&lt;0.01) better than those of all the baseline systems. 4.3 Comparison Systems We compare our NMTIA with four systems: • Moses (Koehn et al., 2007): an open source phrase-based translation system5 with default configuration. The word alignments are obtained with GIZA++ (Och and Ney, 2003) on the training corpora in both directions, using the “grow-diag-final-and” balance strategy (Koehn et al., 2003). The 4-gram language model with modified Kneser-Ney smoothing is trained on the target portion of training data with the SRILM toolkit (Stolcke and others, 2002), • Groundhog: an open source NMT system6 implemented with the conventional attention model (Bahdanau et al., 2015). • RNNsearch? : our in-house implementation of NMT system with the improved conventional attention model as described in Section 2.2. • Coverage Model: state-of-the-art variants of attention-based NMT model (Tu et al., 2016) which improve the attention mechanism through modeling a soft coverage on the source representati"
C16-1205,D15-1166,0,0.701706,"NTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comp"
C16-1205,P15-1002,0,0.384963,"NTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comp"
C16-1205,J03-1002,0,0.0114395,"Missing"
C16-1205,P16-1159,0,0.0609123,"rt variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comparable results with far less parameters and training instances (Jean et al., 2015). This superiority in efficiency comes mainly from"
C16-1205,P16-1008,1,0.724987,"paper, we propose a new attention mechanism, called I NTERACTIVE ATTENTION, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. I NTERACTIVE ATTENTION can keep track of the interaction history and therefore improve the translation performance. Experiments on NIST Chinese-English translation task show that I NTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an in"
C16-1205,D16-1027,1,0.848213,"attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comparable results with far less parameters and training instances (Jean et al., 20"
C16-1205,Q16-1027,0,0.0182714,"ntion-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comparable results with far less parameters and training instances (Jean et al., 2015). This superiority in efficiency comes mainly from the mechanism of dy"
C16-1205,P07-2045,0,\N,Missing
D12-1038,W06-1615,0,0.0614391,"r: S+ (y|Ms→t , Mt→s , Φ, x) = (1 − λ) × S(y|Ms→t , Φ, x) (2) + λ × S(x|Mt→s , Φ, y) The weight parameter λ is tuned on the developing set. To integrating the predict-self reestimation into the iterative transformation training, a reversed transformation model is introduced and the enhanced scoring function above is used when the function T RANS A NNOTATE invokes the function D ECODE . 4 Related Works Researches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al., 2006; Daum´e III, 2007), and adaptation between different annotation guidelines (Jiang et al., 2009; Zhu et al., 2011). There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008). This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the util"
D12-1038,P09-2014,0,0.0169834,"ed when the function T RANS A NNOTATE invokes the function D ECODE . 4 Related Works Researches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al., 2006; Daum´e III, 2007), and adaptation between different annotation guidelines (Jiang et al., 2009; Zhu et al., 2011). There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008). This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the utilization rate of human-annotated knowledge. The iterative training procedure proposed in this work shares some similarity with the co-training algorithm in parsing (Sarkar, 2001), where the training procedure lets two different models learn from each other during parsing the raw text. The key idea of co-training is utilize"
D12-1038,P04-1015,0,0.22417,"adopt the 4 boundary tags of Ng and Low (2004), b, m, e and s, where b, m and e mean the beginning, the middle and the end of a word, and s indicates a single-character word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern s or bm∗ e, indicating single-character words or multi-character words, respectively. We choose the perceptron algorithm (Collins, 2002) to train the character classifier. It is an online training algorithm and has been successfully used in many NLP tasks, including POS tagging (Collins, 2002), parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Jiang et al., 2008; Zhang and Clark, 2010). The training procedure learns a discriminative model mapping from the inputs x ∈ X to the outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. We use the function GEN(x) to enumerate the candidate results of an input x, and the function Φ to map a training example (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd . Given the character sequence x, the decoder finds the output F (x) that maximizes the score function: F (x) = argmax S(y|~ α,"
D12-1038,W02-1001,0,0.854722,"+ Φ(xi , yi ) − Φ(xi , zi ) 8: Output: Parameters α ~ solved in a character classification approach by extending the boundary tags to include POS information. For word segmentation we adopt the 4 boundary tags of Ng and Low (2004), b, m, e and s, where b, m and e mean the beginning, the middle and the end of a word, and s indicates a single-character word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern s or bm∗ e, indicating single-character words or multi-character words, respectively. We choose the perceptron algorithm (Collins, 2002) to train the character classifier. It is an online training algorithm and has been successfully used in many NLP tasks, including POS tagging (Collins, 2002), parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Jiang et al., 2008; Zhang and Clark, 2010). The training procedure learns a discriminative model mapping from the inputs x ∈ X to the outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. We use the function GEN(x) to enumerate the candidate results of an input x, and the function Φ to map a"
D12-1038,P07-1033,0,0.114075,"Missing"
D12-1038,P11-2095,0,0.119802,"rio of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental setting as the annotation adaptation work (Jiang et al., 2009) for convenience of comparison. The two corpora are segmented following different segmentation guidelines and differ largel"
D12-1038,J07-3004,0,0.0432111,"ced scoring function above is used when the function T RANS A NNOTATE invokes the function D ECODE . 4 Related Works Researches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al., 2006; Daum´e III, 2007), and adaptation between different annotation guidelines (Jiang et al., 2009; Zhu et al., 2011). There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008). This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the utilization rate of human-annotated knowledge. The iterative training procedure proposed in this work shares some similarity with the co-training algorithm in parsing (Sarkar, 2001), where the training procedure lets two different models learn from each other during parsing the raw text. The key idea"
D12-1038,P08-1102,1,0.9146,", m and e mean the beginning, the middle and the end of a word, and s indicates a single-character word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern s or bm∗ e, indicating single-character words or multi-character words, respectively. We choose the perceptron algorithm (Collins, 2002) to train the character classifier. It is an online training algorithm and has been successfully used in many NLP tasks, including POS tagging (Collins, 2002), parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Jiang et al., 2008; Zhang and Clark, 2010). The training procedure learns a discriminative model mapping from the inputs x ∈ X to the outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. We use the function GEN(x) to enumerate the candidate results of an input x, and the function Φ to map a training example (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd . Given the character sequence x, the decoder finds the output F (x) that maximizes the score function: F (x) = argmax S(y|~ α, Φ, x) 2 Classification-Based Chinese Word Segmentation y∈GEN(x)"
D12-1038,P09-1059,1,0.943817,"iang and Fandong Meng and Qun Liu and Yajuan Lu¨ Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences {jiangwenbin, mengfandong, liuqun, lvyajuan}@ict.ac.cn Abstract nese word segmentation. However, the need of cascaded classification decisions makes it less practical for tasks of high computational complexity such as parsing, and less efficient to incorporate more than two annotated corpora. In this paper we first describe the technology of automatic annotation transformation, which is based on the annotation adaptation algorithm (Jiang et al., 2009). It can automatically transform a human-annotated corpus from one annotation guideline to another. We then propose two optimization strategies, iterative training and predict-self reestimation, to further improve the accuracy of annotation guideline transformation. Experiments on Chinese word segmentation show that, the iterative training strategy together with predictself reestimation brings significant improvement over the simple annotation transformation baseline, and leads to classifiers with significantly higher accuracy and several times faster processing than annotation adaptation does"
D12-1038,N09-1036,0,0.0850306,"ed dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental setting as the annotation adaptation work (Jiang et al., 2009) for convenience of comparison. The two corpora are segmented following"
D12-1038,P09-1058,0,0.228495,". The predictself methodology is implicit in many unsupervised learning approaches, it has been successfully used by (Daum´e III, 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the"
D12-1038,P11-1141,0,0.189156,"Trans. Baseline Table 3: Data partitioning for CTB and PD. to train the transformation models. The predictself methodology is implicit in many unsupervised learning approaches, it has been successfully used by (Daum´e III, 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from"
D12-1038,P09-1012,0,0.107899,"pt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental setting as the annotation adaptation work (Jiang et al., 2009) for convenience of comparison. The two corpora are segmented following different segmentation gu"
D12-1038,P07-2055,0,0.0621717,"ain the transformation models. The predictself methodology is implicit in many unsupervised learning approaches, it has been successfully used by (Daum´e III, 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et"
D12-1038,W04-3236,0,0.841665,"ation algorithm and the two optimization methods. After the introduction of related works in section 4, we give the experimental results on Chinese word segmentation in section 5. Algorithm 1 Perceptron training algorithm. 1: Input: Training examples (xi , yi ) 2: α ~ ←0 3: for t ← 1 .. T do 4: for i ← 1 .. N do 5: zi ← argmaxz∈GEN(xi ) Φ(xi , z) · α ~ 6: if zi 6= yi then 7: α ~ ←α ~ + Φ(xi , yi ) − Φ(xi , zi ) 8: Output: Parameters α ~ solved in a character classification approach by extending the boundary tags to include POS information. For word segmentation we adopt the 4 boundary tags of Ng and Low (2004), b, m, e and s, where b, m and e mean the beginning, the middle and the end of a word, and s indicates a single-character word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern s or bm∗ e, indicating single-character words or multi-character words, respectively. We choose the perceptron algorithm (Collins, 2002) to train the character classifier. It is an online training algorithm and has been successfully used in many NLP tasks, including POS tagging (Collins, 2002), parsing (Collins and Roark, 2004) and word segmentation"
D12-1038,N01-1023,0,0.154489,"Missing"
D12-1038,P11-1139,0,0.3586,"many unsupervised learning approaches, it has been successfully used by (Daum´e III, 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental setting as t"
D12-1038,C10-1132,0,0.212957,"logy is implicit in many unsupervised learning approaches, it has been successfully used by (Daum´e III, 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental"
D12-1038,W03-1728,0,0.219269,"utputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. We use the function GEN(x) to enumerate the candidate results of an input x, and the function Φ to map a training example (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd . Given the character sequence x, the decoder finds the output F (x) that maximizes the score function: F (x) = argmax S(y|~ α, Φ, x) 2 Classification-Based Chinese Word Segmentation y∈GEN(x) = argmax Φ(x, y) · α ~ (1) y∈GEN(x) Chinese word segmentation can be formalized as the problem of sequence labeling (Xue and Shen, 2003), where each character in the sentence is given a boundary tag denoting its position in a word. Following Ng and Low (2004), joint word segmentation and part-of-speech (POS) tagging can also be 413 Where α ~ ∈ Rd is the parameter vector (that is, the discriminative model) and Φ(x, y) · α ~ is the inner product of Φ(x, y) and α ~. Algorithm 1 shows the perceptron algorithm for tuning the parameter α ~ . The “averaged parameters” Type Unigram Bigram Property Feature Templates C−2 C−1 C0 C1 C2 C−2 C−1 C−1 C0 C0 C1 C1 C2 C−1 C1 P u(C0 ) T (C−2 )T (C−1 )T (C0 )T (C1 )T (C2 ) Type Baseline Guiding T"
D12-1038,P07-1106,0,0.674995,"b, m, e and s, where b, m and e mean the beginning, the middle and the end of a word, and s indicates a single-character word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern s or bm∗ e, indicating single-character words or multi-character words, respectively. We choose the perceptron algorithm (Collins, 2002) to train the character classifier. It is an online training algorithm and has been successfully used in many NLP tasks, including POS tagging (Collins, 2002), parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Jiang et al., 2008; Zhang and Clark, 2010). The training procedure learns a discriminative model mapping from the inputs x ∈ X to the outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. We use the function GEN(x) to enumerate the candidate results of an input x, and the function Φ to map a training example (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd . Given the character sequence x, the decoder finds the output F (x) that maximizes the score function: F (x) = argmax S(y|~ α, Φ, x) 2 Classification-Based Chinese Word Se"
D12-1038,D10-1082,0,0.847117,"eginning, the middle and the end of a word, and s indicates a single-character word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern s or bm∗ e, indicating single-character words or multi-character words, respectively. We choose the perceptron algorithm (Collins, 2002) to train the character classifier. It is an online training algorithm and has been successfully used in many NLP tasks, including POS tagging (Collins, 2002), parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Jiang et al., 2008; Zhang and Clark, 2010). The training procedure learns a discriminative model mapping from the inputs x ∈ X to the outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. We use the function GEN(x) to enumerate the candidate results of an input x, and the function Φ to map a training example (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd . Given the character sequence x, the decoder finds the output F (x) that maximizes the score function: F (x) = argmax S(y|~ α, Φ, x) 2 Classification-Based Chinese Word Segmentation y∈GEN(x) = argmax Φ(x, y) · α ~ ("
D12-1038,I08-4017,0,0.0624271,"2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental setting as the annotation adaptation work (Jiang et al., 2009) for convenience of comparison. The two cor"
D12-1038,P11-2126,0,0.146239,"on the developing set. To integrating the predict-self reestimation into the iterative transformation training, a reversed transformation model is introduced and the enhanced scoring function above is used when the function T RANS A NNOTATE invokes the function D ECODE . 4 Related Works Researches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al., 2006; Daum´e III, 2007), and adaptation between different annotation guidelines (Jiang et al., 2009; Zhu et al., 2011). There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008). This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the utilization rate of human-annotated knowledge. The iterative training procedure proposed in this work shares some simi"
D12-1038,P04-1059,0,\N,Missing
D13-1108,P08-1009,0,0.0222484,"els (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) 1074 proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and the constituency-to-string model. Most works make effort to introduce linguistic knowledge into the phrase-based model and hierarchical phrase"
D13-1108,P05-1033,0,0.864996,"P2 , VP3 indicate the phrases which can not be captured by dependency syntactic phrases. (c) is the corresponding bilingual sentences. The subscripts of phrasal nodes are used for distinguishing the nodes with same phrasal categories. approach yields encouraging results by exploiting two types of trees. Large-scale experiments (Section 5) on Chinese-English translation show that our model significantly outperforms the state-ofthe-art single constituency-to-string model by averaged +2.45 BLEU points, dependency-to-string model by averaged +0.91 BLEU points, and hierarchical phrase-based model (Chiang, 2005) by averaged +1.12 BLEU points, on three Chinese-English NIST test sets. 2 Grammar We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. A headdependents relation consists of a head and all its dependents in dependency trees, and it can represent long distance dependencies. Incorporating phrasal nodes of constituency trees into head-dependents relations further enhances the compatibility with phrases of our rules. Figure 1 shows an example of phrases whic"
D13-1108,J07-2003,0,0.179871,"rt the input phrasal nodes labeled dependency tree into a target string among all possible derivations. Given the source constituency tree and dependency tree, we first generate phrasal nodes labeled dependency tree T as described in Section 3.1, then the decoder transverses each node in T by postorder. For each node n, it enumerates all instances of CHDR rooted at n, and checks the rule set for matched translation rules. A larger translation is generated by substituting the variables in the target side of a translation rule with the translations of the corresponding dependents. Cube pruning (Chiang, 2007; Huang and Chiang, 2007) is used to find the k-best items with integrated language model for each node. To balance the performance and speed of the decoder, we limit the search space by reducing the 1072 number of translation rules used for each node. There are two ways to limit the rule table size: by a fixed limit (rule-limit) of how many rules are retrieved for each input node, and by a threshold (rulethreshold) to specify that the rule with a score lower than β times of the best score should be discarded. On the other hand, instead of keeping the full list of candidates for a given node,"
D13-1108,P05-1066,0,0.290508,"Missing"
D13-1108,P05-1067,0,0.529981,"ndencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that represent the source side as head-dependents relations which ar"
D13-1108,P12-1100,1,0.661115,"1). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) 1074 proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and the constituency-to-string model. Most works make effort to introduce linguistic knowledge into the phrase-based model and hierarchical phrasebased model with constituency trees. Only the work proposed by Mi and Liu, (2010) utilized constituency and dependency trees, while their work applied two types of trees on two sides. Instead, our model simultaneously utilizes constituency and dependency trees on the source side to direct the translation, which is concerned w"
D13-1108,W02-1039,0,0.349783,"l phrasal nodes from the constituency tree C to the dependency tree T , which can be easily accomplished by phrases mapping according to the common covered source sequences. As dependency trees can capture some phrasal information by dependency syntactic 1069 phrases, in order to complement the information that dependency trees can not capture, we only label the phrasal nodes that cover dependency non-syntactic phrases. Then, we annotate alignment information to the phrasal nodes labeled dependency tree T , as shown in Figure 4. For description convenience, we make use of the notion of spans (Fox, 2002; Lin, 2004). Given a node n in the source phrasal nodes labeled T with word alignment information, the spans of n induced by the word alignment are consecutive sequences of words in the target sentence. As shown in Figure 4, we annotate each node n of phrasal nodes labeled T with two attributes: node span and subtree span; besides, we annotate phrasal span to the parts covered by phrasal nodes in each subtree rooted at n. The three types of spans are defined as follows: Definition 1 Given a node n, its node span nsp(n) is the consecutive target word sequence aligned with the node n. æ Take t"
D13-1108,N04-1035,0,0.61655,"4: An annotated dependency tree. Each node is annotated with two spans, the former is node span and the latter subtree span. The fragments covered by phrasal nodes are annotated with phrasal spans. The nodes denoted by the solid line box are not nsp consistent. 3 Rule Extraction In this section, we describe how to extract rules from a set of 4-tuples hC, T, S, Ai, where C is a source constituency tree, T is a source dependency tree, S is a target side sentence, and A is an word alignment relation between T /C and S. We extract CHDR rules from each 4-tuple hC, T, S, Ai based on GHKM algorithm (Galley et al., 2004) with three steps: 1. Label the dependency tree with phrasal nodes from the constituency tree, and annotate alignment information to the phrasal nodes labeled dependency tree (Section 3.1). 2. Identify acceptable CHDR fragments from the annotated dependency tree for rule induction (Section 3.2). 3. Induce a set of lexicalized and generalized CHDR rules from the acceptable fragments (Section 3.3). 3.1 Annotation Given a 4-tuple hC, T, S, Ai, we first label phrasal nodes from the constituency tree C to the dependency tree T , which can be easily accomplished by phrases mapping according to the c"
D13-1108,N04-1014,0,0.441996,"del achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents"
D13-1108,P07-1019,0,0.118738,"hrasal nodes labeled dependency tree into a target string among all possible derivations. Given the source constituency tree and dependency tree, we first generate phrasal nodes labeled dependency tree T as described in Section 3.1, then the decoder transverses each node in T by postorder. For each node n, it enumerates all instances of CHDR rooted at n, and checks the rule set for matched translation rules. A larger translation is generated by substituting the variables in the target side of a translation rule with the translations of the corresponding dependents. Cube pruning (Chiang, 2007; Huang and Chiang, 2007) is used to find the k-best items with integrated language model for each node. To balance the performance and speed of the decoder, we limit the search space by reducing the 1072 number of translation rules used for each node. There are two ways to limit the rule table size: by a fixed limit (rule-limit) of how many rules are retrieved for each input node, and by a threshold (rulethreshold) to specify that the rule with a score lower than β times of the best score should be discarded. On the other hand, instead of keeping the full list of candidates for a given node, we keep a topscoring subs"
D13-1108,2006.amta-papers.8,0,0.492272,"r the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that"
D13-1108,N03-1017,0,0.262119,"ording to the notion in the phrase-based model (Koehn et /NR), tsp( al., 2003). For example, nsp( /NN) and psp(NP1 ) are consistent while nsp( /JJ) and nsp( /NN) are not consistent. The annotation can be achieved by a single postorder transversal of the phrasal nodes labeled dependency tree. For simplicity, we call the annotated phrasal nodes labeled dependency tree annotated dependency tree. The extraction of bilingual phrases (including the translation of head node, dependency syntactic phrases and the fragment covered by a phrasal node) can be readily achieved by the algorithm described in Koehn et al., (2003). In the following, we focus on CHDR rules extraction.  ? 3.2 )P æ Intel 1 Before present the method of acceptable fragments identification, we give a brief description of CHDR fragments. A CHDR fragment is an annotated fragment that consists of a source head-dependents relation with/without constituency phrasal nodes, a target string and the word alignment information between the source and target side. We identify the acceptable CHDR fragments that are suitable for rule induction from the annotated dependency tree. We divide the acceptable CHDR fragments into two categories depending on w"
D13-1108,C04-1090,0,0.866617,"e) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that represent the source side as head-depend"
D13-1108,P06-1077,1,0.835734,"y improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our mode"
D13-1108,P09-1063,1,0.87343,"Missing"
D13-1108,P11-1128,1,0.890153,"Missing"
D13-1108,P08-1114,0,0.0207519,"2g Ñy 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) 1074 proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and"
D13-1108,P10-1145,1,0.777695,"al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) 1074 proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and the constituency-to-string model. Most works make effort to introduce linguistic knowledge into the phrase-based model and hierarchical phrasebased model with constituency trees. Only the work proposed by Mi and Liu, (2010) utilized constit"
D13-1108,P08-1023,1,0.872168,"which can be captured by our CHDR-phrasal rules and translated into the correct result “reemergence” with bilingual phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 2g Ñy 2g Ñy 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu"
D13-1108,P02-1038,0,0.524086,"le the unaligned words of the target side by extending the node spans of the lexicalized head and leaf nodes, and the subtree spans of the lexicalized dependents, on both left and right directions. This procedure is similar with the method of Och and Ney, (2004). During this process, we might obtain m(m ≥ 1) CHDR rules from an acceptable fragment. Each of these rules is assigned with a fractional count 1/m. We take the extracted rule set as observed data and make use of relative frequency estimator to obtain the translation probabilities P (t|s) and P (s|t). 4 Decoding and the Model Following Och and Ney, (2002), we adopt a general loglinear model. Let d be a derivation that convert a source phrasal nodes labeled dependency tree into a target string e. The probability of d is defined as: Y P (d) ∝ φi (d)λi (1) i where φi are features defined on derivations and λi are feature weights. In our experiments of this paper, the features are used as follows: • CHDR rules translation probabilities P (t|s) and P (s|t), and CHDR rules lexical translation probabilities Plex (t|s) and Plex (s|t); • bilingual phrases translation probabilities Pbp (t|s) and Pbp (s|t), and bilingual phrases lexical translation proba"
D13-1108,J03-1002,0,0.00904566,"). Finally, we give detail analysis (Section 5.4). 5.1 Data Preparation Our training data consists of 1.25M sentence pairs extracted from LDC 1 data. We choose NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test sets 2003 (MT03), 2004 (MT04) and 2005 (MT05) as our test sets. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric 2 . We parse the source sentences to constituency trees (without binarization) and projective dependency trees with Stanford Parser (Klein and Manning, 2002). The word alignments are obtained by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and using the “grow-diag-finaland” balance strategy (Koehn et al., 2003). We get bilingual phrases from word-aligned data with algorithm described in Koehn et al. (2003) by running Moses Toolkit 3 . We apply SRI Language Modeling Toolkit (Stolcke and others, 2002) to train a 4-gram 1 Including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 2 ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 3 http://www.statmt.org/moses/ System Moses-chart cons2str dep2str consdep2str Rule # 116.4M 25.4M+32.5M 19.6M+32.5M 23"
D13-1108,J04-4002,0,0.206297,"rules in Xie et al., (2011). CHDR-normal rules are equivalent with the head-dependents relation rules and the CHDRphrasal rules are the extension of these rules. For convenience of description, we use the subscript to distinguish the phrasal nodes with the same category, such as VP2 and VP3 . In actual operation, we use VP instead of VP2 and VP3 . We handle the unaligned words of the target side by extending the node spans of the lexicalized head and leaf nodes, and the subtree spans of the lexicalized dependents, on both left and right directions. This procedure is similar with the method of Och and Ney, (2004). During this process, we might obtain m(m ≥ 1) CHDR rules from an acceptable fragment. Each of these rules is assigned with a fractional count 1/m. We take the extracted rule set as observed data and make use of relative frequency estimator to obtain the translation probabilities P (t|s) and P (s|t). 4 Decoding and the Model Following Och and Ney, (2002), we adopt a general loglinear model. Let d be a derivation that convert a source phrasal nodes labeled dependency tree into a target string e. The probability of d is defined as: Y P (d) ∝ φi (d)λi (1) i where φi are features defined on deriv"
D13-1108,P03-1021,0,0.347804,"Missing"
D13-1108,P05-1034,0,0.638161,"BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that represent the source side as head-dependents relations which are incorporated with"
D13-1108,P08-1066,0,0.730547,"oy single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that represent the source side as head-dependents relations which are incorporated with constituency phrasal nodes, and the tar"
D13-1108,D11-1020,1,0.723184,"Technology, Chinese Academy of Sciences §University of Chinese Academy of Sciences {mengfandong,xiejun,songlinfeng,lvyajuan}@ict.ac.cn ‡Centre for Next Generation Localisation Faculty of Engineering and Computing, Dublin City University qliu@computing.dcu.ie Abstract quences of words combine to form constituents, and constituency-based models show better compatibility with phrases. However, dependency trees describe the grammatical relation between words of the sentence, and represent long distance dependencies in a concise manner. Dependency-based models, such as dependency-to-string model (Xie et al., 2011), exhibit better capability of long distance reorderings. We present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees. We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. Our rules hold the property of long distance reorderings and the compatibility with phrases. Large-scale experimental results show that our model achieves significantly imp"
D13-1108,W07-0706,1,0.921503,"els, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that represent the source side as head-dependents relations which are incorporated with constituency phrasal"
D13-1108,P01-1067,0,0.200551,"results show that our model achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and"
D13-1108,C12-1186,0,0.0116478,"translated into the correct result “reemergence” with bilingual phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 2g Ñy 2g Ñy 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) 1074 proposed a constituency-to-dependency tran"
D13-1108,2007.mtsummit-papers.71,0,0.0319027,"(means verb phrase), which can be captured by our CHDR-phrasal rules and translated into the correct result “reemergence” with bilingual phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 2g Ñy 2g Ñy 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based"
D13-1108,J08-3004,0,\N,Missing
D14-1060,E14-1035,0,0.0217039,"acketing model to reward translation hypothe546 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546–556, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms are important for domain adaptation of mach"
D14-1060,E12-1073,0,0.0152036,"tes separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair. This section presents the three models of term translation. They are the term translation disambiguation model, term translation consistency model and term bracketing model respectively. 4 4.1 Models Term Translation Disambiguation Model The most straightforward way to disambiguate term translations in different domain"
D14-1060,W14-3358,0,0.0581314,"acketing model to reward translation hypothe546 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546–556, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms are important for domain adaptation of mach"
D14-1060,J07-2003,0,0.423966,"should be translated consistently. Term translation consistency has not been investigated in these studies. Our term bracketing model is also related to Xiong et al. (2009)’s syntax-driven bracketing model for phrase-based translation, which predicts whether a phrase is bracketable or not using rich syntactic constraints. The difference is that we construct the model with automatically created bilingual term bank and do not depend on any syntactic knowledge. ses where bracketable multi-word terms are translated as a whole unit. We integrate the three models into hierarchical phrase-based SMT (Chiang, 2007). Large-scale experiment results show that they are all able to achieve significant improvements of up to 0.89 BLEU points over the baseline. When simultaneously integrating the three models into SMT, we can gain a further improvement, which outperforms the baseline by up to 1.16 BLEU points. In the remainder of this paper, we begin with a brief overview of related work in Section 2, and bilingual term extraction in Section 3. We then elaborate the proposed three models for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Secti"
D14-1060,W07-2415,0,0.0148819,"erefore, we use these two methods to extract monolingual multiword terms and then combine the extracted terms. Currently, there are mainly two strategies to conduct bilingual term extraction from parallel corpora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair. This section presents the three models of term translation"
D14-1060,P11-2031,0,0.150467,"pirically, we set the maximum length of a term to 6 words5 . For both the C-value/NC-value and LLRbased extraction methods, we set the context window size to 5 words, which is a widely-used setting in previous work. And we set C-value/NCvalue score threshold to 0 and LLR score threshold to 10 according to the training corpora. We used the case-insensitive 4-gram BLEU6 as our evaluation metric. In order to alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for all our experiments and presented the average BLEU scores on the three runs following the suggestion by Clark et al. (2011). We used an in-house hierarchical phrase-based decoder to verify our proposed models. Although the decoder translates a document in a sentenceby-sentence fashion, it incorporates documentinformed information for sentence translation via the proposed term translation models trained on documents. Experiments In this section, we conducted experiments to answer the following three questions. 1. Are our term translation disambiguation, consistency and bracketing models able to improve translation quality in BLEU? 2. Does the combination of the three models provide further improvements? 3. To what"
D14-1060,itagaki-aikawa-2008-post,0,0.0268507,"ranslated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms are important for domain adaptation of machine translation. These studies do not focus on the three issues of term translation as discussed in Section 1. Furthermore, domain and document-informed information is not used to assist term translation. Itagaki et al. (2007) propose a statistical method to calculate translation consistency for terms with explicit dom"
D14-1060,2007.mtsummit-papers.36,0,0.137099,"es is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms are important for domain adaptation of machine translation. These studies do not focus on the three issues of term translation as discussed in Section 1. Furthermore, domain and document-informed information is not used to assist term translation. Itagaki et al. (2007) propose a statistical method to calculate translation consistency for terms with explicit domain information. Partially inspired by their study, we introduce a term translation consistency metric with document-informed information. Furthermore, we integrate the proposed term translation consistency model into an actual SMT system, which has not been done by Itagaki et al. (2007). Ture et al. (2012) use IR-inspired tf-idf scores to encourage consistent translation choice. Guillou (2013) investigates what kind of words should be translated consistently. Term translation consistency has not been"
D14-1060,N03-1017,0,0.0461755,"t of 4.28M sentence pairs extracted from LDC1 data with document boundaries explicitly provided. The bilingual training data contain 67,752 documents, 124.8M Chinese words and 140.3M English words. We chose NIST MT05 as the MERT (Och, 2003) tuning set, NIST MT06 as the development test set, and NIST MT08 as the final test set. The numbers of documents/sentences in NIST MT05, MT06 and MT08 are 100/1082, 79/1664 and 109/1357 respectively. The word alignments were obtained by running GIZA++ (Och and Ney, 2003) on the corpora in both directions and using the “grow-diag-finaland” balance strategy (Koehn et al., 2003). We adopted SRI Language Modeling Toolkit (Stolcke and others, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the English Gigaword corpus. For the topic model, we used the open source 2 http://sourceforge.net/projects/gibbslda/ http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://nlp.stanford.edu/software/tagger.shtml 5 We determine the maximum length of a term by testing {5, 6, 7, 8} in our preliminary experiments. We find that length 6 produces a slightly better performance than other values. 6 ftp://jaguar.ncsl.nist.gov/mt/"
D14-1060,P12-2023,0,0.0198425,"dels for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Section 5. Finally, we conclude and provide directions for future work in Section 6. 2 Related Work In this section, we briefly introduce related work and highlight the differences between our work and previous studies. As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores the topic model (Blei et al., 2003) for machine translation (Zhao and Xing, 2006; Su et al., 2012; Xiao et al., 2012; Eidelman et al., 2012). Zhao and Xing (2006) employ three models that enable word alignment process to leverage topical contents of document-pairs with topic model. Su et al. (2012) establish the relationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these studies, the topic model is not used to ad"
D14-1060,E09-1057,0,0.0196172,"o extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair. This section presents the three models of term translation. They are the term translation disambiguation model, term translation consistency model and term bracketing model respectively. 4 4.1 Models Term Translation Disambiguation Model The most straightforward way to disambiguate term translati"
D14-1060,W07-2456,0,0.0231945,"pora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair. This section presents the three models of term translation. They are the term translation disambiguation model, term translation consistency model and term bracketing model respectively. 4 4.1 Models Term Translation Disambiguation Model The most straightforward way to disam"
D14-1060,D11-1084,0,0.0602182,"istribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these studies, the topic model is not used to address the issues of term translation mentioned in Section 1. Our work is also related to document-level SMT in that we use document-informed information for term translation. Tiedemann (2010) propose cache-based language and translation models, which are built on recently translated sentences. Gong et al. (2011) extend this by further introducing two additional caches. They employ a static cache to store bilingual phrases extracted 3 Bilingual Term Extraction Bilingual term extraction is to extract terms from two languages with the purpose of creating or ex547 tending a bilingual term bank, which in turn can be used to improve other tasks such as information retrieval and machine translation. In this paper, we want to automatically build a bilingual term bank so that we can model term translation to improve translation quality of SMT. Our interest is to extract multi-word terms. tracts more flexible"
D14-1060,J03-1002,0,0.00837229,"hat extent do the proposed models affect the translations of test sets? 5.1 Setup Our training data consist of 4.28M sentence pairs extracted from LDC1 data with document boundaries explicitly provided. The bilingual training data contain 67,752 documents, 124.8M Chinese words and 140.3M English words. We chose NIST MT05 as the MERT (Och, 2003) tuning set, NIST MT06 as the development test set, and NIST MT08 as the final test set. The numbers of documents/sentences in NIST MT05, MT06 and MT08 are 100/1082, 79/1664 and 109/1357 respectively. The word alignments were obtained by running GIZA++ (Och and Ney, 2003) on the corpora in both directions and using the “grow-diag-finaland” balance strategy (Koehn et al., 2003). We adopted SRI Language Modeling Toolkit (Stolcke and others, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the English Gigaword corpus. For the topic model, we used the open source 2 http://sourceforge.net/projects/gibbslda/ http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://nlp.stanford.edu/software/tagger.shtml 5 We determine the maximum length of a term by testing {5, 6, 7, 8} in our preliminary experiments. We fi"
D14-1060,P03-1021,0,0.0697129,"n probability Pd (tei |tfi , D ) given the docu0 ment D is formulated as follows: 0 Pd (tei |tfi , D ) = K X 0 p(tei |tfi , z = k) ∗ p(z = k|D ) (3) k=1 Whenever a source term tfi is translated into tei , we check whether the pair of tfi and its translation tei can be found in our bilingual term bank. If it can be found, we calculate the conditional translation probability from tfi to tei given the document 0 D according to Eq. (3). The term translation disambiguation model is integrated into the log-linear model of SMT as a feature. Its weight is tuned via minimum error rate training (MERT) (Och, 2003). Through the feature, we can enable the decoder to favor translation hypotheses that contain target term translations appropriate for the domain represented by the topic distribution of the corresponding document. 4.2 Qk = m=1 n=1 Nm M X X qmn ∗ p(k|m) (5) m=1 n=1 where M is the number of documents in which the source term tf occurs, Nm is the number of unique corresponding term translations of tf in the mth document, qmn is the frequency of the nth translation of tf in the mth document, p(k|m) is the conditional probability of the mth document over topic k, and Qk is the normalization factor"
D14-1060,W13-3302,0,0.016021,"tion 1. Furthermore, domain and document-informed information is not used to assist term translation. Itagaki et al. (2007) propose a statistical method to calculate translation consistency for terms with explicit domain information. Partially inspired by their study, we introduce a term translation consistency metric with document-informed information. Furthermore, we integrate the proposed term translation consistency model into an actual SMT system, which has not been done by Itagaki et al. (2007). Ture et al. (2012) use IR-inspired tf-idf scores to encourage consistent translation choice. Guillou (2013) investigates what kind of words should be translated consistently. Term translation consistency has not been investigated in these studies. Our term bracketing model is also related to Xiong et al. (2009)’s syntax-driven bracketing model for phrase-based translation, which predicts whether a phrase is bracketable or not using rich syntactic constraints. The difference is that we construct the model with automatically created bilingual term bank and do not depend on any syntactic knowledge. ses where bracketable multi-word terms are translated as a whole unit. We integrate the three models int"
D14-1060,D12-1108,0,0.0215582,"(Vasconcellos et al., 2001). In this paper, we study domain-specific and context-sensitive term translation for SMT. • Term Bracketing Model: We use the bracketing model to reward translation hypothe546 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546–556, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicat"
D14-1060,W06-2403,0,0.012483,"cts more flexible terms, these two methods are complementary to each other. Therefore, we use these two methods to extract monolingual multiword terms and then combine the extracted terms. Currently, there are mainly two strategies to conduct bilingual term extraction from parallel corpora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as"
D14-1060,P09-1036,1,0.931411,"with explicit domain information. Partially inspired by their study, we introduce a term translation consistency metric with document-informed information. Furthermore, we integrate the proposed term translation consistency model into an actual SMT system, which has not been done by Itagaki et al. (2007). Ture et al. (2012) use IR-inspired tf-idf scores to encourage consistent translation choice. Guillou (2013) investigates what kind of words should be translated consistently. Term translation consistency has not been investigated in these studies. Our term bracketing model is also related to Xiong et al. (2009)’s syntax-driven bracketing model for phrase-based translation, which predicts whether a phrase is bracketable or not using rich syntactic constraints. The difference is that we construct the model with automatically created bilingual term bank and do not depend on any syntactic knowledge. ses where bracketable multi-word terms are translated as a whole unit. We integrate the three models into hierarchical phrase-based SMT (Chiang, 2007). Large-scale experiment results show that they are all able to achieve significant improvements of up to 0.89 BLEU points over the baseline. When simultaneous"
D14-1060,W09-2907,1,0.898735,"Missing"
D14-1060,P12-1048,1,0.841057,"then elaborate the proposed three models for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Section 5. Finally, we conclude and provide directions for future work in Section 6. 2 Related Work In this section, we briefly introduce related work and highlight the differences between our work and previous studies. As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores the topic model (Blei et al., 2003) for machine translation (Zhao and Xing, 2006; Su et al., 2012; Xiao et al., 2012; Eidelman et al., 2012). Zhao and Xing (2006) employ three models that enable word alignment process to leverage topical contents of document-pairs with topic model. Su et al. (2012) establish the relationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these"
D14-1060,D13-1163,1,0.848608,"we study domain-specific and context-sensitive term translation for SMT. • Term Bracketing Model: We use the bracketing model to reward translation hypothe546 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546–556, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a"
D14-1060,W10-2602,0,0.0853383,"elationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these studies, the topic model is not used to address the issues of term translation mentioned in Section 1. Our work is also related to document-level SMT in that we use document-informed information for term translation. Tiedemann (2010) propose cache-based language and translation models, which are built on recently translated sentences. Gong et al. (2011) extend this by further introducing two additional caches. They employ a static cache to store bilingual phrases extracted 3 Bilingual Term Extraction Bilingual term extraction is to extract terms from two languages with the purpose of creating or ex547 tending a bilingual term bank, which in turn can be used to improve other tasks such as information retrieval and machine translation. In this paper, we want to automatically build a bilingual term bank so that we can model"
D14-1060,P06-2124,0,0.02353,"ion in Section 3. We then elaborate the proposed three models for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Section 5. Finally, we conclude and provide directions for future work in Section 6. 2 Related Work In this section, we briefly introduce related work and highlight the differences between our work and previous studies. As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores the topic model (Blei et al., 2003) for machine translation (Zhao and Xing, 2006; Su et al., 2012; Xiao et al., 2012; Eidelman et al., 2012). Zhao and Xing (2006) employ three models that enable word alignment process to leverage topical contents of document-pairs with topic model. Su et al. (2012) establish the relationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during tran"
D14-1060,N12-1046,0,0.0216783,"ation. These studies do not focus on the three issues of term translation as discussed in Section 1. Furthermore, domain and document-informed information is not used to assist term translation. Itagaki et al. (2007) propose a statistical method to calculate translation consistency for terms with explicit domain information. Partially inspired by their study, we introduce a term translation consistency metric with document-informed information. Furthermore, we integrate the proposed term translation consistency model into an actual SMT system, which has not been done by Itagaki et al. (2007). Ture et al. (2012) use IR-inspired tf-idf scores to encourage consistent translation choice. Guillou (2013) investigates what kind of words should be translated consistently. Term translation consistency has not been investigated in these studies. Our term bracketing model is also related to Xiong et al. (2009)’s syntax-driven bracketing model for phrase-based translation, which predicts whether a phrase is bracketable or not using rich syntactic constraints. The difference is that we construct the model with automatically created bilingual term bank and do not depend on any syntactic knowledge. ses where brack"
D14-1060,I08-2084,0,0.0314953,"ild a bilingual term bank so that we can model term translation to improve translation quality of SMT. Our interest is to extract multi-word terms. tracts more flexible terms, these two methods are complementary to each other. Therefore, we use these two methods to extract monolingual multiword terms and then combine the extracted terms. Currently, there are mainly two strategies to conduct bilingual term extraction from parallel corpora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases"
D14-1060,D12-1097,0,0.0241244,"01). In this paper, we study domain-specific and context-sensitive term translation for SMT. • Term Bracketing Model: We use the bracketing model to reward translation hypothe546 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546–556, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingua"
D14-1060,P12-1079,1,0.927846,"e proposed three models for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Section 5. Finally, we conclude and provide directions for future work in Section 6. 2 Related Work In this section, we briefly introduce related work and highlight the differences between our work and previous studies. As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores the topic model (Blei et al., 2003) for machine translation (Zhao and Xing, 2006; Su et al., 2012; Xiao et al., 2012; Eidelman et al., 2012). Zhao and Xing (2006) employ three models that enable word alignment process to leverage topical contents of document-pairs with topic model. Su et al. (2012) establish the relationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these studies, the topic"
D19-1097,D17-1146,0,0.0153586,"ories. In addition, the global information in S-LSTM is modeled by aggregating the local features with gating mechanisms, which may lose sight of sequential information of the whole sentence. Therefore, We apply external BiLSTMs to supply global sequential features, which is shown highly necessary for both tasks in our experiments. 7 6 Related Work Memory Network Memory network is a general machine learning framework introduced by Weston et al. (2014), which have been shown effective in question answering (Weston et al., 2014; Sukhbaatar et al., 2015), machine translation (Wang et al., 2016a; Feng et al., 2017), aspect level sentiment classification (Tang et al., 2016), etc. For spoken language understanding, Chen et al. (2016) introduce memory mechanisms to encode historical utterances. In this paper, we propose two memories to explicitly capture the seConclusion We propose a novel Collaborative Memory Network (CM-Net) for jointly modeling slot filling and intent detection. The CM-Net is able to explicitly capture the semantic correlations among words, slots and intents in a collaborative manner, and incrementally enrich the information flows with local context and global sequential information. Ex"
D19-1097,N18-2118,0,0.212802,"es 1051–1060, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics # 1 2 3 4 Utterance play Roy Orbison tunes now add this Roy Orbison song onto Women of Comedy book a spot for seven at a bar with chicken french book french food for me and angeline at a restaurant Slot tag artist artist served dish cuisine Intent PlayMusic AddToPlaylist BookRestaurant BookRestaurant Table 1: Examples in SNIPS with annotations of intent label for the utterance and slot tags for partial words. unidirectionally with the guidance from intent representations via gating mechanisms (Goo et al., 2018; Li et al., 2018), while the predictions of intents lack the guidance from slots. Moreover, the capsule network with dynamic routing algorithms (Zhang et al., 2018a) is proposed to perform interactions in both directions. However, there are still two limitations in this model. The one is that the information flows from words to slots, slots to intents and intents to words in a pipeline manner, which is to some extent limited in capturing complicated correlations among words, slots and intents. The other is that the local context information which has been shown highly useful for the slot fill"
D19-1097,W19-5906,0,0.023433,"interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China † Jinan Xu is the corresponding author of the paper. 1 Code is available at: https://github.com/Adaxry/CMNet. for a given utterance, which are referred as intent detection and slot filling, respectively. Past years have witnessed rapid developments in diverse deep learning models (Haffner et al., 2003; Sarikaya et al., 2011) for SLU. To take full advantage of supervised signals of slots and intents, and share knowledge between them, most of existing works apply joint models that mainly based on CNNs (Xu and Sarikaya, 2013; Gupta et al., 2019), RNNs (Guo et al., 2014a; Liu and Lane, 2016), and asynchronous bi-model (Wang et al., 2018). Generally, these joint models encode words convolutionally or sequentially, and then aggregate hidden states into a utterance-level representation for the intent prediction, without interactions between representations of slots and intents. Intuitively, slots and intents from similar fields tend to occur simultaneously, which can be observed from Figure 1 and Table 1. Therefore, it is beneficial to generate the representations of slots and intents with the guidance from each other. Some works explore"
D19-1097,W03-0426,0,0.137692,"procedures, the hidden state is updated with abundant information from different perspectives, namely word embeddings, local contexts, slots and intents representations. The local calculation layer in each CMblock has been shown highly useful for both tasks, and especially for the slot filling task, which will be validated in our experiments in Section 5.2. Global Recurrence Bi-directional RNNs, especially the BiLSTMs (Hochreiter and Schmidhuber, 1997) are regarded to encode both past and future information of a sentence, which have become a dominant method in various sequence modeling tasks (Hammerton, 2003; Sundermeyer et al., 2012). The inherent nature of BiLSTMs is able to supplement global sequential information, which is insufficiently modeled in the previous local calculation layer. Thus we apply an additional BiLSTMs layer upon the local calculation layer in each CM-block. By taking the slot- and intent-specific local context representations as inputs, we can obtain more specific global sequential representations. Formally, it takes the hidden state hl−1 inherited from the t local calculation layer as input, and conduct recurrent steps as follows: → − ← − hlt = [hlt ; ht l ] → − → −l → −"
D19-1097,H90-1021,0,0.123954,"Missing"
D19-1097,P82-1020,0,0.842662,"Missing"
D19-1097,D16-1223,0,0.0299513,"lot and intent feat int tures Ht are utilized to provide guidances for the next local calculation layer. Bi-LSTM Global Recurrence Output states Local Calculation Hidden states ··· ht-1 ht ht+1 ··· Embeddings ··· xt-1 xt xt+1 ··· Slot features ··· ht-1slot htslot ht+1slot ··· Intent features ··· ht-1int htint ht+1int ··· Deliberate Attention Slot Memory Intent Memory Figure 3: The internal structure of our CM-Block, which is composed of deliberate attention, local calculation and global recurrent respectively. Local Calculation Local context information is highly useful for sequence modeling (Kurata et al., 2016; Wang et al., 2016b). Zhang et al. (2018b) propose the S-LSTM to encode both local and sentence-level information simultaneously, and it has been shown more powerful for text representation when compared with the conventional BiLSTMs. We extend the Sand intentLSTM with slot-specific features Hslot t retrieved from memories. specific features Hslot t Specifically, at each input position t, we take the local window context ξt , word embedding xt , slot feature hslot and intent feature hint as inputs to t t conduct combinatorial calculation simultaneously. Formally, in the lth layer, the hidden"
D19-1097,N16-1030,0,0.0277893,"nference Layer. 3.2 The most probable intent label yˆint is predicted by softmax normalization over the intent label set: P (e y = j|v Intent Memory (4) i=1 ye∈S int int x1 3.1 As to the prediction of intent, the word-level hidden states H are firstly summarized into a utterance-level representation vint via mean pooling (or max pooling or self-attention, etc.): vint = slots Embedding Layer where A is the transition matrix that Ai,j indicates the score of a transition from i to j, and P is the score matrix output by RNNs. Pi,j indicates the score of the j th tag of the ith word in a sentence (Lample et al., 2016). When testing, the Viterbi algorithm (Forney, 1973) is used to search the sequence of slot tags with maximum score: e slot ∈Yx y yN (1) i=1 ˆ slot = arg max F (H, y e slot ) y ··· ··· Here Yx is the set of all possible sequences of tags, and F (·) is the score function calculated by: N X yt Inference Layer slot slot ··· (6) Embedding Layers Pre-trained Word Embedding The pre-trained word embeddings has been indicated as a de-facto standard of neural network architectures for various NLP tasks. We adapt the cased, 300d Glove2 (Pennington et al., 2014) to initialize word embeddings, and keep th"
D19-1097,D18-1417,0,0.165367,"ng Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics # 1 2 3 4 Utterance play Roy Orbison tunes now add this Roy Orbison song onto Women of Comedy book a spot for seven at a bar with chicken french book french food for me and angeline at a restaurant Slot tag artist artist served dish cuisine Intent PlayMusic AddToPlaylist BookRestaurant BookRestaurant Table 1: Examples in SNIPS with annotations of intent label for the utterance and slot tags for partial words. unidirectionally with the guidance from intent representations via gating mechanisms (Goo et al., 2018; Li et al., 2018), while the predictions of intents lack the guidance from slots. Moreover, the capsule network with dynamic routing algorithms (Zhang et al., 2018a) is proposed to perform interactions in both directions. However, there are still two limitations in this model. The one is that the information flows from words to slots, slots to intents and intents to words in a pipeline manner, which is to some extent limited in capturing complicated correlations among words, slots and intents. The other is that the local context information which has been shown highly useful for the slot filling (Mesnil et al."
D19-1097,D14-1162,0,0.0905085,"of the j th tag of the ith word in a sentence (Lample et al., 2016). When testing, the Viterbi algorithm (Forney, 1973) is used to search the sequence of slot tags with maximum score: e slot ∈Yx y yN (1) i=1 ˆ slot = arg max F (H, y e slot ) y ··· ··· Here Yx is the set of all possible sequences of tags, and F (·) is the score function calculated by: N X yt Inference Layer slot slot ··· (6) Embedding Layers Pre-trained Word Embedding The pre-trained word embeddings has been indicated as a de-facto standard of neural network architectures for various NLP tasks. We adapt the cased, 300d Glove2 (Pennington et al., 2014) to initialize word embeddings, and keep them frozen. Character-aware Word Embedding It has been demonstrated that character level information (e.g. capitalization and prefix) (Collobert et al., 2011) is crucial for sequence labeling. We use one layer of CNN followed by max pooling to generate character-aware word embeddings. 3.3 CM-block The CM-block is the core module of our CM-Net, which is designed with three computational com2 j=1 i=1 1053 https://nlp.stanford.edu/projects/glove/ ponents: Deliberate Attention, Local Calculation and Global Recurrence respectively. Deliberate Attention To f"
D19-1097,N18-1202,0,0.122653,"Missing"
D19-1097,W09-1119,0,0.0153174,"Zhang et al., 2018a). CAIS We collect utterances from the Chinese Artificial Intelligence Speakers (CAIS), and annotate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material. Since the utterances are collected from speaker systems in the real world, intent labels are partial to the PlayMusic option. We adopt the BIOES tagging scheme for slots instead of the BIO2 used in the ATIS, since previous studies have highlighted meaningful improvements with this scheme (Ratinov and Roth, 2009) in the sequence labeling field. Metrics Slot filling is typically treated as a sequence labeling problem, and thus we take the conlleval 4 as the token-level F1 metric. The intent detection is evaluated with the classification accuracy. Specially, several utterances in the ATIS are tagged with more than one labels. Following previous works (Tur et al., 2010; Zhang and Wang, 2016), we count an utterrance as a correct classification if any ground truth label is predicted. 4 https://www.clips.uantwerpen.be/conll2000/chunking/ conlleval.txt 4.2 Implementation Details All trainable parameters in o"
D19-1097,D16-1021,0,0.0230266,"eled by aggregating the local features with gating mechanisms, which may lose sight of sequential information of the whole sentence. Therefore, We apply external BiLSTMs to supply global sequential features, which is shown highly necessary for both tasks in our experiments. 7 6 Related Work Memory Network Memory network is a general machine learning framework introduced by Weston et al. (2014), which have been shown effective in question answering (Weston et al., 2014; Sukhbaatar et al., 2015), machine translation (Wang et al., 2016a; Feng et al., 2017), aspect level sentiment classification (Tang et al., 2016), etc. For spoken language understanding, Chen et al. (2016) introduce memory mechanisms to encode historical utterances. In this paper, we propose two memories to explicitly capture the seConclusion We propose a novel Collaborative Memory Network (CM-Net) for jointly modeling slot filling and intent detection. The CM-Net is able to explicitly capture the semantic correlations among words, slots and intents in a collaborative manner, and incrementally enrich the information flows with local context and global sequential information. Experiments on two standard benchmarks and our CAIS corpus de"
D19-1097,D16-1027,0,0.180193,"nt tures Ht are utilized to provide guidances for the next local calculation layer. Bi-LSTM Global Recurrence Output states Local Calculation Hidden states ··· ht-1 ht ht+1 ··· Embeddings ··· xt-1 xt xt+1 ··· Slot features ··· ht-1slot htslot ht+1slot ··· Intent features ··· ht-1int htint ht+1int ··· Deliberate Attention Slot Memory Intent Memory Figure 3: The internal structure of our CM-Block, which is composed of deliberate attention, local calculation and global recurrent respectively. Local Calculation Local context information is highly useful for sequence modeling (Kurata et al., 2016; Wang et al., 2016b). Zhang et al. (2018b) propose the S-LSTM to encode both local and sentence-level information simultaneously, and it has been shown more powerful for text representation when compared with the conventional BiLSTMs. We extend the Sand intentLSTM with slot-specific features Hslot t retrieved from memories. specific features Hslot t Specifically, at each input position t, we take the local window context ξt , word embedding xt , slot feature hslot and intent feature hint as inputs to t t conduct combinatorial calculation simultaneously. Formally, in the lth layer, the hidden state ht is updated"
D19-1097,C16-1229,0,0.0929106,"nt tures Ht are utilized to provide guidances for the next local calculation layer. Bi-LSTM Global Recurrence Output states Local Calculation Hidden states ··· ht-1 ht ht+1 ··· Embeddings ··· xt-1 xt xt+1 ··· Slot features ··· ht-1slot htslot ht+1slot ··· Intent features ··· ht-1int htint ht+1int ··· Deliberate Attention Slot Memory Intent Memory Figure 3: The internal structure of our CM-Block, which is composed of deliberate attention, local calculation and global recurrent respectively. Local Calculation Local context information is highly useful for sequence modeling (Kurata et al., 2016; Wang et al., 2016b). Zhang et al. (2018b) propose the S-LSTM to encode both local and sentence-level information simultaneously, and it has been shown more powerful for text representation when compared with the conventional BiLSTMs. We extend the Sand intentLSTM with slot-specific features Hslot t retrieved from memories. specific features Hslot t Specifically, at each input position t, we take the local window context ξt , word embedding xt , slot feature hslot and intent feature hint as inputs to t t conduct combinatorial calculation simultaneously. Formally, in the lth layer, the hidden state ht is updated"
D19-1097,N18-2050,0,0.354129,"sponding author of the paper. 1 Code is available at: https://github.com/Adaxry/CMNet. for a given utterance, which are referred as intent detection and slot filling, respectively. Past years have witnessed rapid developments in diverse deep learning models (Haffner et al., 2003; Sarikaya et al., 2011) for SLU. To take full advantage of supervised signals of slots and intents, and share knowledge between them, most of existing works apply joint models that mainly based on CNNs (Xu and Sarikaya, 2013; Gupta et al., 2019), RNNs (Guo et al., 2014a; Liu and Lane, 2016), and asynchronous bi-model (Wang et al., 2018). Generally, these joint models encode words convolutionally or sequentially, and then aggregate hidden states into a utterance-level representation for the intent prediction, without interactions between representations of slots and intents. Intuitively, slots and intents from similar fields tend to occur simultaneously, which can be observed from Figure 1 and Table 1. Therefore, it is beneficial to generate the representations of slots and intents with the guidance from each other. Some works explore enhancing the slot filling task 1051 Proceedings of the 2019 Conference on Empirical Methods"
D19-1097,P18-1030,0,0.113748,"rbison song onto Women of Comedy book a spot for seven at a bar with chicken french book french food for me and angeline at a restaurant Slot tag artist artist served dish cuisine Intent PlayMusic AddToPlaylist BookRestaurant BookRestaurant Table 1: Examples in SNIPS with annotations of intent label for the utterance and slot tags for partial words. unidirectionally with the guidance from intent representations via gating mechanisms (Goo et al., 2018; Li et al., 2018), while the predictions of intents lack the guidance from slots. Moreover, the capsule network with dynamic routing algorithms (Zhang et al., 2018a) is proposed to perform interactions in both directions. However, there are still two limitations in this model. The one is that the information flows from words to slots, slots to intents and intents to words in a pipeline manner, which is to some extent limited in capturing complicated correlations among words, slots and intents. The other is that the local context information which has been shown highly useful for the slot filling (Mesnil et al., 2014), is not explicitly modeled. In this paper, we try to address these issues, and thus propose a novel Collaborative Memory Network, named CM"
D19-1164,2012.eamt-1.60,0,0.059146,"X, Y ) = 1 |D| j × J X I X {log P (yij |, y j<i , xj , D <j ; Θ) j=1 i=1 + PCCs(Capsenc (xj ), Capsdec (y j ))} (11) where Θ are parameters of the model, D <j are historical sentences of the to-be-translated source sentence, xj is the to-be-translated sentence and y j<i denotes the generated target hypothesis. 4 Experiments 4.1 Settings Datasets and Evaluation Metrics We carry out experiments on English-German translation tasks in three different domains: talks, news, and speeches. The corpora statistics are shown in Table 1. • TED. This corpus is a Machine Translation part of the IWSLT 2017 (Cettolo et al., 2012) evaluation compaigns1 , each TED talk is considered to be a document. we take the tst2016-207 as the test set, and other as our development set. • News. We take the sentence-aligned document-delimited News Commentary v11 corpus2 as our training set. The WMT’16 news-test2015 and news-test2016 are used for development and testing respectively. • Europarl. The corpus are extracted from the Europarl v7 (Koehn, 2005) according to the method mentioned in Maruf and Haffari (2018). The training, development and test sets are obtained through randomly splitting the corpus. We download all of above ext"
D19-1164,2010.iwslt-papers.10,0,0.0797008,"Missing"
D19-1164,D12-1108,0,0.0219715,"ords as inputs of the QCN and 4 duplicated query. Blue color denotes positive correlation and red means negative. From top to bottom are four heat maps in 0th to 3th iterations. value. See Figure 7, we can find that PCCs varies as iteration changes. 5 Related Work Document-level Machine Translation Document-level machine translation became a hot research direction in the later stage of statistical machine translation era. Hardmeier and Federico (2010) represented the links between word pairs in the context using a word dependency model for SMT to improve the translation of anaphoric pronouns. Hardmeier et al. (2012, 2013) first proposed a new document-level SMT paradigm that translates whole documents as units. However, in this period, most of the work has not achieved too many compelling results or has been only focused on a part of difficulties. With the coming of the era of Neural Machine Translation, many works began to focus on Document-level NMT tasks. Xiong et al. (2019) trained a reward teacher to refine the translation quality from a document perspective. Tiedemann and Scherrer (2017) simply concatenated sentences in one document as models’ input or output. Jean et al. (2017) used additional co"
D19-1164,P13-4033,0,0.11176,"Missing"
D19-1164,2005.mtsummit-papers.11,0,0.131344,"tasks in three different domains: talks, news, and speeches. The corpora statistics are shown in Table 1. • TED. This corpus is a Machine Translation part of the IWSLT 2017 (Cettolo et al., 2012) evaluation compaigns1 , each TED talk is considered to be a document. we take the tst2016-207 as the test set, and other as our development set. • News. We take the sentence-aligned document-delimited News Commentary v11 corpus2 as our training set. The WMT’16 news-test2015 and news-test2016 are used for development and testing respectively. • Europarl. The corpus are extracted from the Europarl v7 (Koehn, 2005) according to the method mentioned in Maruf and Haffari (2018). The training, development and test sets are obtained through randomly splitting the corpus. We download all of above extracted corpora3 from Maruf et al. (2019). The tokenization and 1 https://wit3.fbk.eu/ http://www.casmacat.eu/corpus/news-commentary.html 3 https://github.com/sameenmaruf/selectiveattn/tree/master/data 2 truecase pre-processing are implemented on all datasets using the scripts of the Moses Toolkit4 (Koehn et al., 2007). We also apply segmentation into BPE subword units5 (Sennrich et al., 2016) with 30K merge opera"
D19-1164,P07-2045,0,0.00823295,"ed for development and testing respectively. • Europarl. The corpus are extracted from the Europarl v7 (Koehn, 2005) according to the method mentioned in Maruf and Haffari (2018). The training, development and test sets are obtained through randomly splitting the corpus. We download all of above extracted corpora3 from Maruf et al. (2019). The tokenization and 1 https://wit3.fbk.eu/ http://www.casmacat.eu/corpus/news-commentary.html 3 https://github.com/sameenmaruf/selectiveattn/tree/master/data 2 truecase pre-processing are implemented on all datasets using the scripts of the Moses Toolkit4 (Koehn et al., 2007). We also apply segmentation into BPE subword units5 (Sennrich et al., 2016) with 30K merge operations. We use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate the translation quality. Models and Baselines We use the Transformer architecture as our context-agnostic baseline and adopt three contextaware baselines (Zhang et al., 2018a; Miculicich et al., 2018; Maruf and Haffari, 2018). We performed the same configuration on our models according to the settings of the Maruf and Haffari (2018). Specifically, for the Transformer, we set the hidden size and"
D19-1164,W07-0734,0,0.0738311,"training, development and test sets are obtained through randomly splitting the corpus. We download all of above extracted corpora3 from Maruf et al. (2019). The tokenization and 1 https://wit3.fbk.eu/ http://www.casmacat.eu/corpus/news-commentary.html 3 https://github.com/sameenmaruf/selectiveattn/tree/master/data 2 truecase pre-processing are implemented on all datasets using the scripts of the Moses Toolkit4 (Koehn et al., 2007). We also apply segmentation into BPE subword units5 (Sennrich et al., 2016) with 30K merge operations. We use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate the translation quality. Models and Baselines We use the Transformer architecture as our context-agnostic baseline and adopt three contextaware baselines (Zhang et al., 2018a; Miculicich et al., 2018; Maruf and Haffari, 2018). We performed the same configuration on our models according to the settings of the Maruf and Haffari (2018). Specifically, for the Transformer, we set the hidden size and point-wise FFN size as 512 and 2048 respectively. We use 4 layers and 8 attention heads in both encoder and decoder. All dropout rates are set to 0.1 for context-agnostic model and 0.2 for"
D19-1164,P18-1118,0,0.829638,"slation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentenc"
D19-1164,N19-1313,0,0.76905,"have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical attentions retrieved by the current generation. That is, it utilizes a wordle"
D19-1164,W13-3303,0,0.0224921,"1 Introduction The encoder-decoder based Neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation ("
D19-1164,D18-1325,0,0.678009,"skever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the"
D19-1164,P02-1040,0,0.104751,"d in Maruf and Haffari (2018). The training, development and test sets are obtained through randomly splitting the corpus. We download all of above extracted corpora3 from Maruf et al. (2019). The tokenization and 1 https://wit3.fbk.eu/ http://www.casmacat.eu/corpus/news-commentary.html 3 https://github.com/sameenmaruf/selectiveattn/tree/master/data 2 truecase pre-processing are implemented on all datasets using the scripts of the Moses Toolkit4 (Koehn et al., 2007). We also apply segmentation into BPE subword units5 (Sennrich et al., 2016) with 30K merge operations. We use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate the translation quality. Models and Baselines We use the Transformer architecture as our context-agnostic baseline and adopt three contextaware baselines (Zhang et al., 2018a; Miculicich et al., 2018; Maruf and Haffari, 2018). We performed the same configuration on our models according to the settings of the Maruf and Haffari (2018). Specifically, for the Transformer, we set the hidden size and point-wise FFN size as 512 and 2048 respectively. We use 4 layers and 8 attention heads in both encoder and decoder. All dropout rates are set to 0.1 fo"
D19-1164,P16-1162,0,0.0824309,"xtracted from the Europarl v7 (Koehn, 2005) according to the method mentioned in Maruf and Haffari (2018). The training, development and test sets are obtained through randomly splitting the corpus. We download all of above extracted corpora3 from Maruf et al. (2019). The tokenization and 1 https://wit3.fbk.eu/ http://www.casmacat.eu/corpus/news-commentary.html 3 https://github.com/sameenmaruf/selectiveattn/tree/master/data 2 truecase pre-processing are implemented on all datasets using the scripts of the Moses Toolkit4 (Koehn et al., 2007). We also apply segmentation into BPE subword units5 (Sennrich et al., 2016) with 30K merge operations. We use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate the translation quality. Models and Baselines We use the Transformer architecture as our context-agnostic baseline and adopt three contextaware baselines (Zhang et al., 2018a; Miculicich et al., 2018; Maruf and Haffari, 2018). We performed the same configuration on our models according to the settings of the Maruf and Haffari (2018). Specifically, for the Transformer, we set the hidden size and point-wise FFN size as 512 and 2048 respectively. We use 4 layers and 8 atte"
D19-1164,W17-4814,0,0.0550498,"ecoder based Neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al."
D19-1164,W17-4811,0,0.135797,"ween word pairs in the context using a word dependency model for SMT to improve the translation of anaphoric pronouns. Hardmeier et al. (2012, 2013) first proposed a new document-level SMT paradigm that translates whole documents as units. However, in this period, most of the work has not achieved too many compelling results or has been only focused on a part of difficulties. With the coming of the era of Neural Machine Translation, many works began to focus on Document-level NMT tasks. Xiong et al. (2019) trained a reward teacher to refine the translation quality from a document perspective. Tiedemann and Scherrer (2017) simply concatenated sentences in one document as models’ input or output. Jean et al. (2017) used additional context encoder to capture larger-context information. Kuang et al. (2017); Tu et al. (2018) used a cache to memorize most relevant words or features in previous sentences or translations. Recently, several studies integrated additional modules into the Transformer-based NMTs for modeling contextual information. (Voita et al., 2018; Zhang et al., 2018a), Maruf and Haffari (2018) proposed a document-level NMT using a 1534 memory-networks, and Wang et al. (2017) and Miculicich et al. (20"
D19-1164,Q18-1029,0,0.259966,"ang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical attentions retrieved by the current generation. That is,"
D19-1164,P18-1117,0,0.312421,"et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical attentions retrieved"
D19-1164,D17-1301,0,0.328004,"ni et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical attentions retrieved by the current gen"
D19-1164,D18-1486,0,0.0519226,"necessary to distinguish the role of each context word and model their relationship especially when one context word could take on multiple roles (Zhang et al., 2018b). However, this is difficult to realize for the HAN model as its final representation for the context is produced with an isolated relevance with the query word which ignores relations with other context words. To address the problem, we introduce Capsule Networks into document-level translation which have proven good at modelling the parts-wholes relations between low-level capsules and highlevel capsules (Hinton et al., 2011; Xiao et al., 2018; Sabour et al., 2017; Hinton et al., 2018; Gu and Feng, 2019). With capsule networks, the words in a context source sentence is taken as lowlevel capsules and the information of different perspectives is treated as high-level capsules. Then in the dynamic routing process of capsule networks, all the low-level capsules trade off against each other and consider over all the high-level capsules and drop themselves at a proper proportion to the high-level capsules. In this way, the relation among low-level capsules and that between low1527 Proceedings of the 2019 Conference on Empirical Methods i"
D19-1164,D18-1350,0,0.031748,"slty divided documentlevel translation tasks into two types: offline and online. Capsule Networks Hinton et al. (2011) proposed the capsule conception to use vector for describing the pose of an object. The dynamic routing algorithm was proposed by Sabour et al. (2017) to build the partwhole relationship through the iterative routing procedure. Hinton et al. (2018) designed a new routing style based on the EM algorithm. Some researchers investigated to apply the capsule network for various tasks. Wang et al. (2018) investigated a novel capsule network with dynamic routing for linear time NMT. Yang et al. (2018) explored capsule networks for text classification with strategies to stabilize the dynamic routing process. Gu and Feng (2019) introduces capsule networks into Transformer to model the relations between different heads in multi-head attention. We specifically investigated dynamic routing algorithms for the document-level NMT. 6 Conclusion We have proposed a novel Query-guided Capsule Network with an improved dynamic routing algorithm for enhancing context modeling for the document-level Neural Machine Translation Model. Experiments on English-German in different domains showed our model signi"
D19-1164,D18-1049,0,0.652933,"anau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical"
D19-1164,P19-1426,1,0.873378,"Missing"
D19-1164,C18-1110,1,0.888176,"anau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical"
P15-1003,P14-1062,0,0.207783,"Missing"
P15-1003,J07-2003,0,0.064903,"Missing"
P15-1003,D14-1179,0,0.0842528,"Missing"
P15-1003,P05-1066,0,0.106045,"Missing"
P15-1003,P14-1129,0,0.0614801,"Missing"
P15-1003,N04-1035,0,0.0736712,"Missing"
P15-1003,P07-1019,0,0.0144132,"Missing"
P15-1003,D13-1176,0,0.131209,"Missing"
P15-1003,N03-1017,0,0.00644014,"Missing"
P15-1003,P07-2045,0,0.0132544,"Missing"
P15-1003,D13-1108,1,0.848069,"Missing"
P15-1003,P02-1038,0,0.428481,"Missing"
P15-1003,J03-1002,0,0.00890853,"Missing"
P15-1003,P03-1021,0,0.072223,"Missing"
P15-1003,P08-1066,0,0.0292075,"Missing"
P15-1003,D11-1020,1,0.715388,"Missing"
P15-1003,D13-1106,0,\N,Missing
P18-1163,D18-1549,0,0.0245783,"k that augments the training data with monolingual corpora (Sennrich et al., 2016a; Cheng et al., 2016; He et al., 2016a; Zhang and Zong, 2016). They all leverage complex models such as inverse NMT models to generate translation equivalents for monolingual corpora. Then they augment the parallel corpora with these pseudo corpora to improve NMT models. Some authors have recently endeavored to achieve zero-shot NMT through transferring knowledge from bilingual corpora of other language pairs (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2017) or monolingual corpora (Lample et al., 2018; Artetxe et al., 2018). Our work significantly differs from these work. We do not resort to any complicated models to generate perturbed data and do not depend on extra monolingual or bilingual corpora. The way we exploit is more convenient and easy to implement. We focus more on improving the robustness of NMT models. 6 Conclusion We have proposed adversarial stability training to improve the robustness of NMT models. The basic idea is to train both the encoder and decoder robust to input perturbations by enabling them to behave similarly for the original input and its perturbed counterpart. We propose two approac"
P18-1163,P17-1176,1,0.843537,"ugmentation has the capability to improve the robustness of NMT models. In NMT, there is a number of work that augments the training data with monolingual corpora (Sennrich et al., 2016a; Cheng et al., 2016; He et al., 2016a; Zhang and Zong, 2016). They all leverage complex models such as inverse NMT models to generate translation equivalents for monolingual corpora. Then they augment the parallel corpora with these pseudo corpora to improve NMT models. Some authors have recently endeavored to achieve zero-shot NMT through transferring knowledge from bilingual corpora of other language pairs (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2017) or monolingual corpora (Lample et al., 2018; Artetxe et al., 2018). Our work significantly differs from these work. We do not resort to any complicated models to generate perturbed data and do not depend on extra monolingual or bilingual corpora. The way we exploit is more convenient and easy to implement. We focus more on improving the robustness of NMT models. 6 Conclusion We have proposed adversarial stability training to improve the robustness of NMT models. The basic idea is to train both the encoder and decoder robust to input perturbations by en"
P18-1163,D14-1181,0,0.00580016,"f model parameters using Adam SGD (Kingma and Ba, 2015). Its learning rate is initially set to 0.05 and varies according to the formula in Vaswani et al. (2017). Our adversarial stability training initializes the model based on the parameters trained by maximum likelihood estimation (MLE). We denote adversarial stability training based on lexical-level perturbations and feature-level perturbations respectively as ASTlexical and ASTfeature . We only sample one perturbed neighbour x0 ∈ N (x) for training efficiency. For the discriminator used in Linv , we adopt the CNN discriminator proposed by Kim (2014) to address the variable-length problem of the sequence generated by the encoder. In the CNN discriminator, the filter windows are set to 3, 4, 5 and rectified linear units are applied after convolution operations. We tune the hyperparameters on the validation set through a grid search. We find that both the optimal values of α and β are set to 1.0. The standard variance in Gaussian noise used in the formula (6) is set to 0.01. The number of words that are replaced in the sentence x during lexical-level perturbations is taken as max(0.2|x|, 1) in which |x |is the length of x. The default beam"
P18-1163,P16-1185,1,0.558429,"robustness of networks (Goodfellow et al., 2015; Miyato et al., 2016; Zheng et al., 2016). Belinkov and Bisk (2018) introduce adversarial examples to training data for character-based NMT models. In contrast to theirs, adversarial stability training aims to stabilize both the encoder and decoder in NMT models. We adopt adversarial learning to learn the perturbation-invariant encoder. Data Augmentation Data augmentation has the capability to improve the robustness of NMT models. In NMT, there is a number of work that augments the training data with monolingual corpora (Sennrich et al., 2016a; Cheng et al., 2016; He et al., 2016a; Zhang and Zong, 2016). They all leverage complex models such as inverse NMT models to generate translation equivalents for monolingual corpora. Then they augment the parallel corpora with these pseudo corpora to improve NMT models. Some authors have recently endeavored to achieve zero-shot NMT through transferring knowledge from bilingual corpora of other language pairs (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2017) or monolingual corpora (Lample et al., 2018; Artetxe et al., 2018). Our work significantly differs from these work. We do not resort to any complic"
P18-1163,D17-1230,0,0.0323464,"bout 100K iterations, which indicates that discriminator outputs probability 0.5 for both positive and negative samples and it cannot distinguish them. Thus the behaviors of the encoder for x and its perturbed neighbour x0 perform nearly consistently. 5 Related Work Our work is inspired by two lines of research: (1) adversarial learning and (2) data augmentation. Adversarial Learning Generative Adversarial Network (GAN) (Goodfellow et al., 2014) and its related derivative have been widely applied in computer vision (Radford et al., 2015; Salimans et al., 2016) and natural language processing (Li et al., 2017; Yang et al., 2018). Previous work has constructed adversarial examples to attack trained networks and make networks resist them, which has proved to improve the robustness of networks (Goodfellow et al., 2015; Miyato et al., 2016; Zheng et al., 2016). Belinkov and Bisk (2018) introduce adversarial examples to training data for character-based NMT models. In contrast to theirs, adversarial stability training aims to stabilize both the encoder and decoder in NMT models. We adopt adversarial learning to learn the perturbation-invariant encoder. Data Augmentation Data augmentation has the capabi"
P18-1163,D15-1166,0,0.12942,"Missing"
P18-1163,P02-1040,0,0.102639,"of x and y, we also construct a mini-batch consisting of the perturbed neighbour x0 and y. We propagate the information to calculate these three loss functions according to arrows. Then, gradients are collected to update three sets of model parameters. Except for the gradients of Linv with respect to θenc are multiplying by −1, other gradients are normally backpropagated. Note that we update θinv and θenc simultaneously for training efficiency. 4 4.1 Experiments Setup We evaluated our adversarial stability training on translation tasks of several language pairs, and reported the 4-gram BLEU (Papineni et al., 2002) score as calculated by the multi-bleu.perl script. Chinese-English We used the LDC corpus consisting of 1.25M sentence pairs with 27.9M Chinese words and 34.5M English words respectively. We selected the best model using the NIST 2006 set as the validation set (hyper-parameter optimization and model selection). The NIST 2002, 2003, 2004, 2005, and 2008 datasets are used as test sets. English-German We used the WMT 14 corpus containing 4.5M sentence pairs with 118M English words and 111M German words. The validation set is newstest2013, and the test set is newstest2014. English-French We used"
P18-1163,P16-1009,0,0.199849,"on set is newstest2013, and the test set is newstest2014. English-French We used the IWSLT corpus which contains 0.22M sentence pairs with 4.03M English words and 4.12M French words. The IWLST corpus is very dissimilar from the NIST and WMT corpora. As they are collected from TED talks and inclined to spoken language, we want to verify our approaches on the nonnormative text. The IWSLT 14 test set is taken as the validation set and 15 test set is used as the test set. For English-German and English-French, we tokenize both English, German and French words using tokenize.perl script. We follow Sennrich et al. (2016b) to split words into subword units. The numbers of merge operations in byte pair encoding (BPE) are set to 30K, 40K and 30K respectively for Chinese-English, English-German, and English-French. We report the case-sensitive tokenized BLEU score for English-German and English-French and the caseinsensitive tokenized BLEU score for ChineseEnglish. Our baseline system is an in-house NMT system. Following Bahdanau et al. (2015), we implement an RNN-based NMT in which both the encoder and decoder are two-layer RNNs with residual connections between layers (He et al., 2016b). The gating mechanism o"
P18-1163,P16-1162,0,0.426482,"on set is newstest2013, and the test set is newstest2014. English-French We used the IWSLT corpus which contains 0.22M sentence pairs with 4.03M English words and 4.12M French words. The IWLST corpus is very dissimilar from the NIST and WMT corpora. As they are collected from TED talks and inclined to spoken language, we want to verify our approaches on the nonnormative text. The IWSLT 14 test set is taken as the validation set and 15 test set is used as the test set. For English-German and English-French, we tokenize both English, German and French words using tokenize.perl script. We follow Sennrich et al. (2016b) to split words into subword units. The numbers of merge operations in byte pair encoding (BPE) are set to 30K, 40K and 30K respectively for Chinese-English, English-German, and English-French. We report the case-sensitive tokenized BLEU score for English-German and English-French and the caseinsensitive tokenized BLEU score for ChineseEnglish. Our baseline system is an in-house NMT system. Following Bahdanau et al. (2015), we implement an RNN-based NMT in which both the encoder and decoder are two-layer RNNs with residual connections between layers (He et al., 2016b). The gating mechanism o"
P18-1163,P16-1159,1,0.736098,"on the validation set through a grid search. We find that both the optimal values of α and β are set to 1.0. The standard variance in Gaussian noise used in the formula (6) is set to 0.01. The number of words that are replaced in the sentence x during lexical-level perturbations is taken as max(0.2|x|, 1) in which |x |is the length of x. The default beam size for decoding is 10. 4.2 4.2.1 Translation Results NIST Chinese-English Translation Table 2 shows the results on Chinese-English translation. Our strong baseline system significantly outperforms previously reported results on 1760 System Shen et al. (2016) Wang et al. (2017) Zhang et al. (2018) this work Training MRT MLE MLE MLE ASTlexical ASTfeature MT06 37.34 37.29 38.38 41.38 43.57 44.44 MT02 40.36 – – 43.52 44.82 46.10 MT03 40.93 39.35 40.02 41.50 42.95 44.07 MT04 41.37 41.15 42.32 43.64 45.05 45.61 MT05 38.81 38.07 38.84 41.58 43.45 44.06 MT08 29.23 – – 31.60 34.85 34.94 Table 2: Case-insensitive BLEU scores on Chinese-English translation. System Shen et al. (2016) Luong et al. (2015) Kalchbrenner et al. (2017) Wang et al. (2017) Wu et al. (2016) Gehring et al. (2017) Vaswani et al. (2017) Architecture Gated RNN with 1 layer LSTM with 4 la"
P18-1163,P17-1013,0,0.0910566,"set through a grid search. We find that both the optimal values of α and β are set to 1.0. The standard variance in Gaussian noise used in the formula (6) is set to 0.01. The number of words that are replaced in the sentence x during lexical-level perturbations is taken as max(0.2|x|, 1) in which |x |is the length of x. The default beam size for decoding is 10. 4.2 4.2.1 Translation Results NIST Chinese-English Translation Table 2 shows the results on Chinese-English translation. Our strong baseline system significantly outperforms previously reported results on 1760 System Shen et al. (2016) Wang et al. (2017) Zhang et al. (2018) this work Training MRT MLE MLE MLE ASTlexical ASTfeature MT06 37.34 37.29 38.38 41.38 43.57 44.44 MT02 40.36 – – 43.52 44.82 46.10 MT03 40.93 39.35 40.02 41.50 42.95 44.07 MT04 41.37 41.15 42.32 43.64 45.05 45.61 MT05 38.81 38.07 38.84 41.58 43.45 44.06 MT08 29.23 – – 31.60 34.85 34.94 Table 2: Case-insensitive BLEU scores on Chinese-English translation. System Shen et al. (2016) Luong et al. (2015) Kalchbrenner et al. (2017) Wang et al. (2017) Wu et al. (2016) Gehring et al. (2017) Vaswani et al. (2017) Architecture Gated RNN with 1 layer LSTM with 4 layers ByteNet with 3"
P18-1163,N18-1122,0,0.0232541,"ons, which indicates that discriminator outputs probability 0.5 for both positive and negative samples and it cannot distinguish them. Thus the behaviors of the encoder for x and its perturbed neighbour x0 perform nearly consistently. 5 Related Work Our work is inspired by two lines of research: (1) adversarial learning and (2) data augmentation. Adversarial Learning Generative Adversarial Network (GAN) (Goodfellow et al., 2014) and its related derivative have been widely applied in computer vision (Radford et al., 2015; Salimans et al., 2016) and natural language processing (Li et al., 2017; Yang et al., 2018). Previous work has constructed adversarial examples to attack trained networks and make networks resist them, which has proved to improve the robustness of networks (Goodfellow et al., 2015; Miyato et al., 2016; Zheng et al., 2016). Belinkov and Bisk (2018) introduce adversarial examples to training data for character-based NMT models. In contrast to theirs, adversarial stability training aims to stabilize both the encoder and decoder in NMT models. We adopt adversarial learning to learn the perturbation-invariant encoder. Data Augmentation Data augmentation has the capability to improve the"
P18-1163,D16-1160,0,0.0309436,"al., 2015; Miyato et al., 2016; Zheng et al., 2016). Belinkov and Bisk (2018) introduce adversarial examples to training data for character-based NMT models. In contrast to theirs, adversarial stability training aims to stabilize both the encoder and decoder in NMT models. We adopt adversarial learning to learn the perturbation-invariant encoder. Data Augmentation Data augmentation has the capability to improve the robustness of NMT models. In NMT, there is a number of work that augments the training data with monolingual corpora (Sennrich et al., 2016a; Cheng et al., 2016; He et al., 2016a; Zhang and Zong, 2016). They all leverage complex models such as inverse NMT models to generate translation equivalents for monolingual corpora. Then they augment the parallel corpora with these pseudo corpora to improve NMT models. Some authors have recently endeavored to achieve zero-shot NMT through transferring knowledge from bilingual corpora of other language pairs (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2017) or monolingual corpora (Lample et al., 2018; Artetxe et al., 2018). Our work significantly differs from these work. We do not resort to any complicated models to generate perturbed data an"
P19-1002,P17-4012,0,0.0491261,"Missing"
P19-1002,N16-1014,0,0.0876363,"coder) to improve context coherence and knowledge correctness. Our empirical study on a real-world Document Grounded Dataset proves that responses generated by our model significantly outperform competitive baselines on both context coherence and knowledge relevance. 1 Introduction Past few years have witnessed the rapid development of dialogue systems. Based on the sequenceto-sequence framework (Sutskever et al., 2014), most models are trained in an end-to-end manner with large corpora of human-to-human dialogues and have obtained impressive success (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016; Serban et al., 2016). While there is still a long way for reaching the ultimate goal of dialogue systems, which is to be able to talk like humans. And one of the essential intelligence to achieve this goal is the ability to make use of knowledge. ∗ Fandong Meng is the corresponding author of the paper. This work was done when Zekang Li was interning at Pattern Recognition Center, WeChat AI, Tencent. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 12–21 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics"
P19-1002,P18-1138,1,0.924905,"Huazhong University of Science and Technology ‡ Pattern Recognition Center, WeChat AI, Tencent Inc, China ♦ Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences ♠ School of Computer Science and Engineering, Northeastern University, China zekangli97@gmail.com, {chengniu,fandongmeng,jiezhou}@tencent.com fengyang@ict.ac.cn, qianli@stumail.neu.edu.cn Abstract There are several works on dialogue systems exploiting knowledge. The Mem2Seq (Madotto et al., 2018) incorporates structured knowledge into the end-to-end task-oriented dialogue. Liu et al. (2018) introduces factmatching and knowledge-diffusion to generate meaningful, diverse and natural responses using structured knowledge triplets. Ghazvininejad et al. (2018), Parthasarathi and Pineau (2018), Yavuz et al. (2018), Dinan et al. (2018) and Lo and Chen (2019) apply unstructured text facts in open-domain dialogue systems. These works mainly focus on integrating factoid knowledge into dialogue systems, while factoid knowledge requires a lot of work to build up, and is only limited to expressing precise facts. Documents as a knowledge source provide a wide spectrum of knowledge, including b"
P19-1002,D15-1166,0,0.0293207,"using Self-Attentive Encoder, u ˆ (2) is the output words after the second-pass decoder. Training In contrast to the original Deliberation Network (Xia et al., 2017), where they propose a complex joint learning framework using Monte Carlo Method, we minimize the following loss as Xiong et al. (2018) do: Lmle = Lmle1 + Lmle2 (k+1) (29) log P (ˆ u(2)i ) Experiments 3.2 Baselines We compare our proposed model with the following state-of-the-art baselines: Models not using document knowledge: Seq2Seq: A simple encoder-decoder model (Shang et al., 2015; Vinyals and Le, 2015) with global attention (Luong et al., 2015). We concatenate utterances context to a long sentence as input. HRED: A hierarchical encoder-decoder model (Serban et al., 2016), which is composed of a word-level LSTM for each sentence and a sentence-level LSTM connecting utterances. Transformer: The state-of-the-art NMT model based on multi-head attention (Vaswani et al., 2017). We concatenate utterances context to a long sentence as its input. Models using document knowledge: Seq2Seq (+knowledge) and HRED (+knowledge) are based on Seq2Seq and HRED respectively. They both concatenate document knowledge representation and last decoding outp"
P19-1002,P17-1171,0,0.0317822,"arathi and Pineau (2018), Yavuz et al. (2018), Dinan et al. (2018) and Lo and Chen (2019) apply unstructured text facts in open-domain dialogue systems. These works mainly focus on integrating factoid knowledge into dialogue systems, while factoid knowledge requires a lot of work to build up, and is only limited to expressing precise facts. Documents as a knowledge source provide a wide spectrum of knowledge, including but not limited to factoid, event updates, subjective opinion, etc. Recently, intensive research has been applied on using documents as knowledge sources for QuestionAnswering (Chen et al., 2017; Huang et al., 2018; Yu et al., 2018; Rajpurkar et al., 2018; Reddy et al., 2018). The Document Grounded Conversation is a task to generate natural dialogue responses when chatting about the content of a specific document. This task requires to integrate document knowledge with the multi-turn dialogue history. Different from previous knowledge grounded dialogue systems, Document Grounded Conversations utilize documents as the knowledge source, and hence are able to employ a wide spectrum of knowledge. And the Document Grounded Conversations is also different from document QA since the context"
P19-1002,P18-1136,0,0.0323799,"Yang Feng♦ , Qian Li♠ , Jie Zhou‡ † Dian Group, School of Electronic Information and Communications Huazhong University of Science and Technology ‡ Pattern Recognition Center, WeChat AI, Tencent Inc, China ♦ Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences ♠ School of Computer Science and Engineering, Northeastern University, China zekangli97@gmail.com, {chengniu,fandongmeng,jiezhou}@tencent.com fengyang@ict.ac.cn, qianli@stumail.neu.edu.cn Abstract There are several works on dialogue systems exploiting knowledge. The Mem2Seq (Madotto et al., 2018) incorporates structured knowledge into the end-to-end task-oriented dialogue. Liu et al. (2018) introduces factmatching and knowledge-diffusion to generate meaningful, diverse and natural responses using structured knowledge triplets. Ghazvininejad et al. (2018), Parthasarathi and Pineau (2018), Yavuz et al. (2018), Dinan et al. (2018) and Lo and Chen (2019) apply unstructured text facts in open-domain dialogue systems. These works mainly focus on integrating factoid knowledge into dialogue systems, while factoid knowledge requires a lot of work to build up, and is only limited to expressing"
P19-1002,P02-1040,0,0.106854,"liberation decoder. Knowledge-Attention Transformer (KAT): As shown in Figure 2 (d), the encoder of this model is a simplified version of Incremental Transformer Encoder (ITE), which doesn’t have context-attention sub-layer. We concatenate utterances context to a long sentence as its input. The decoder of the model is a simplified Context-Knowledge-Attention Decoder (CKAD). It doesn’t have context-attention sub-layer either. This setup is to test how effective the context has been exploited in the full model. 3.3 3.4 Evaluation Metrics Automatic Evaluation: We adopt perplexity (PPL) and BLEU (Papineni et al., 2002) to automatically evaluate the response generation performance. Models are evaluated using perplexity of the gold response as described in (Dinan et al., 2018). Lower perplexity indicates better performance. BLEU measures n-gram overlap between a generated response and a gold response. However, since there is only one reference for each response and there may exist multiple feasible responses, BLEU scores are extremely low. We compute BLEU score by the multi-bleu.perl3 Manual Evaluation: Manual evaluations are essential for dialogue generation. We randomly sampled 30 conversations containing 6"
P19-1002,D18-1073,0,0.251661,"chnology, Chinese Academy of Sciences ♠ School of Computer Science and Engineering, Northeastern University, China zekangli97@gmail.com, {chengniu,fandongmeng,jiezhou}@tencent.com fengyang@ict.ac.cn, qianli@stumail.neu.edu.cn Abstract There are several works on dialogue systems exploiting knowledge. The Mem2Seq (Madotto et al., 2018) incorporates structured knowledge into the end-to-end task-oriented dialogue. Liu et al. (2018) introduces factmatching and knowledge-diffusion to generate meaningful, diverse and natural responses using structured knowledge triplets. Ghazvininejad et al. (2018), Parthasarathi and Pineau (2018), Yavuz et al. (2018), Dinan et al. (2018) and Lo and Chen (2019) apply unstructured text facts in open-domain dialogue systems. These works mainly focus on integrating factoid knowledge into dialogue systems, while factoid knowledge requires a lot of work to build up, and is only limited to expressing precise facts. Documents as a knowledge source provide a wide spectrum of knowledge, including but not limited to factoid, event updates, subjective opinion, etc. Recently, intensive research has been applied on using documents as knowledge sources for QuestionAnswering (Chen et al., 2017; Huang"
P19-1002,P18-2124,0,0.0369028,"al. (2018) and Lo and Chen (2019) apply unstructured text facts in open-domain dialogue systems. These works mainly focus on integrating factoid knowledge into dialogue systems, while factoid knowledge requires a lot of work to build up, and is only limited to expressing precise facts. Documents as a knowledge source provide a wide spectrum of knowledge, including but not limited to factoid, event updates, subjective opinion, etc. Recently, intensive research has been applied on using documents as knowledge sources for QuestionAnswering (Chen et al., 2017; Huang et al., 2018; Yu et al., 2018; Rajpurkar et al., 2018; Reddy et al., 2018). The Document Grounded Conversation is a task to generate natural dialogue responses when chatting about the content of a specific document. This task requires to integrate document knowledge with the multi-turn dialogue history. Different from previous knowledge grounded dialogue systems, Document Grounded Conversations utilize documents as the knowledge source, and hence are able to employ a wide spectrum of knowledge. And the Document Grounded Conversations is also different from document QA since the contextual consistent conversation response should be generated. To"
P19-1002,P15-1152,0,0.0751858,"Missing"
P19-1002,D18-1049,0,0.0387442,"Missing"
P19-1002,D18-1076,0,0.190263,"ess of the responses. The first-pass decoder focuses on contextual coherence, while the second-pass decoder refines the result of the firstpass decoder by consulting the relevant document knowledge, and hence increases the knowledge relevance and correctness. This is motivated by human cognition process. In real-world human conversations, people usually first make a draft on how to respond the previous utterance, and then consummate the answer or even raise questions by consulting background knowledge. We test the effectiveness of our proposed model on Document Grounded Conversations Dataset (Zhou et al., 2018). Experiment results show that our model is capable of generating responses of more context coherence and knowledge relevance. Sometimes document knowledge is even well used to guide the following conversations. Both automatic and manual evaluations show that our model substantially outperforms the competitive baselines. Our contributions are as follows: Incremental Transformer Encoder Document k-2 Self-Attentive Encoder Document k-1 Incremental Transformer Utterance k-1 Self-Attentive Encoder Document k Utterance k Incremental Transformer Self-Attentive Encoder Self-Attentive Encoder First-pa"
P19-1233,C18-1139,0,0.482507,"training and development data 4 . More notably, our GCDT surpasses the models that exploit additional task-specific resources or annotated corpora (Luo et al., 2015; Yang et al., 2017b; Chiu and Nichols, 2016). Additionally, we conduct experiments by leveraging the well-known BERT as an external resource for relatively fair comparison with models 4 We achieve F1 score of 92.18 when training on both training and development data without extra resources. 2435 Models (Rei, 2017) (Liu et al., 2017) (Peters et al., 2017)† (Peters et al., 2018) (Clark et al., 2018) (2018) BERTBASE (2018) BERTLARGE (Akbik et al., 2018)† GCDT + BERTLARGE F1 86.26 91.71 ± 0.10 91.93 ± 0.19 92.20 92.61 92.40 92.80 93.09 93.47 ± 0.03 # 0 1 2 3 F1 93.88 95.96 ± 0.08 96.37 ± 0.05 96.72 ± 0.05 97.00 97.30 ± 0.03 Table 4: F1 scores on the CoNLL2000 Chunking task by leveraging language model. We establish new stateof-the-art result on this task. that utilize external language models trained on massive corpora. Especially, Rei (2017) and Liu et al. (2017) build task-specific language models only on supervised data. Table 3 and Table 4 show that our GCDT outperforms previous state-of-theart results substantially at 93.47 (+0.38) on NE"
P19-1233,W17-4710,0,0.159173,"bulary. Formally, the label of word xt is predicted as the probabilistic equation (Eq. 13) st = DTde (ht , yt−1 ; θDTde ) (11) lt = st Wl + bl (12) P (yt = j|x) = sof tmax(lt )[j] (13) As we can see from the above procedures and Figure 1, our GCDT firstly encodes the global contextual representation along the sequential axis by Deep Transition RNN Deep transition RNNs extend conventional RNNs by increasing the transition depth of consecutive hidden states. Previous studies have shown the superiority of this architecture on both language modeling (Pascanu et al., 2014) and machine translation (Barone et al., 2017; Meng and Zhang, 2019). Particularly, Meng and Zhang (2019) propose to maintain a linear transformation path throughout the deep transition procedure with a linear gate to enhance the transition structure. Following Meng and Zhang (2019), the deep transition block in our hierarchical model is composed of two key components, namely Linear Transformation enhanced GRU (L-GRU) and Transition GRU (T-GRU). At each time step, LGRU first encodes each token with an additional linear transformation of the input embedding, then the hidden state of L-GRU is passed into a chain of 2433 T-GRU connected mer"
P19-1233,C02-1025,0,0.189985,"14; Xin et al., 2018), IntNet (Xin et al., 2018). The shallow connections between consecutive hidden states in those models inspire us to deepen the transition path for richer representation. More recently, there has been a growing body of work exploring to leverage language model trained on massive corpora in both character level (Peters et al., 2017, 2018; Akbik et al., 2018) and token level (Devlin et al., 2018). Inspired by the effectiveness of language model embeddings, we conduct auxiliary experiments by leveraging the well-known BERT as an additional feature. Exploit Global Information Chieu and Ng (2002) explore the usage of global feature in the whole document by the co-occurrence of each token, which is fed into a maximum entropy classifier. With the widespread application of distributed word representations (Mikolov et al., 2013) and neural networks (Collobert et al., 2011; Huang et al., 2015) in sequence labeling tasks, the global information is encoded into hidden states of BiRNNs. Specially, Yang et al. (2017a) leverage global sentence patterns for NER reranking. Inspired by the global sentence-level representation in S-LSTM (Zhang et al., 2018), we propose a more concise approach to ca"
P19-1233,Q16-1026,0,0.760702,"ndamental and challenging problems of Natural Language Processing (NLP). Recently, neural models have become the de-facto standard for high-performance systems. Among various neural networks for sequence labeling, bi-directional RNNs (BiRNNs), especially BiLSTMs (Hochreiter and Schmidhuber, 1997) have become a dominant method on ∗ This work was done when Yijin Liu was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China † Jinan Xu is the corresponding author of the paper. 1 Code is available at: https://github.com/Adaxry/GCDT. multiple benchmark datasets (Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Peters et al., 2017). However, there are several natural limitations of the BiLSTMs architecture. For example, at each time step, the BiLSTMs consume an incoming word and construct a new summary of the past subsequence. This procedure should be highly nonlinear, to allow the hidden states to rapidly adapt to the mutable input while still preserving a useful summary of the past (Pascanu et al., 2014). While in BiLSTMs, even stacked BiLSTMs, the transition depth between consecutive hidden states are inherently shallow. Moreover, global contextual information, which has bee"
P19-1233,D18-1217,0,0.0548064,"Missing"
P19-1233,D17-1206,0,0.0543637,"Missing"
P19-1233,P82-1020,0,0.801678,"Missing"
P19-1233,N16-1030,0,0.867902,"g problems of Natural Language Processing (NLP). Recently, neural models have become the de-facto standard for high-performance systems. Among various neural networks for sequence labeling, bi-directional RNNs (BiRNNs), especially BiLSTMs (Hochreiter and Schmidhuber, 1997) have become a dominant method on ∗ This work was done when Yijin Liu was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China † Jinan Xu is the corresponding author of the paper. 1 Code is available at: https://github.com/Adaxry/GCDT. multiple benchmark datasets (Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Peters et al., 2017). However, there are several natural limitations of the BiLSTMs architecture. For example, at each time step, the BiLSTMs consume an incoming word and construct a new summary of the past subsequence. This procedure should be highly nonlinear, to allow the hidden states to rapidly adapt to the mutable input while still preserving a useful summary of the past (Pascanu et al., 2014). While in BiLSTMs, even stacked BiLSTMs, the transition depth between consecutive hidden states are inherently shallow. Moreover, global contextual information, which has been shown highly useful"
P19-1233,D15-1104,0,0.221855,"tly modeled at each position, as the nature of recurrent architecture makes RNN partial to the most recent input token. While our context-aware representation is incorporated with local word embeddings directly, which assists in capturing useful representations through combinatorial computing between diverse local word embeddings and the global contextual embedding. We further investigate the effects on positions where the global embedding is used. (Section 5.1) 2434 4 4.1 Models (Collobert et al., 2011)* (Huang et al., 2015)* (Passos et al., 2014)* (Lample et al., 2016) (Yang et al., 2016)* (Luo et al., 2015)* (Ma and Hovy, 2016) (Yang et al., 2017b)*† (Zhang et al., 2018) (Yang et al., 2017a) (Chiu and Nichols, 2016)*† (Xin et al., 2018) GCDT GCDT + BERTLARGE Experiments Datasets and Metric NER The CoNLL03 NER task (Sang and De Meulder, 2003) is tagged with four linguistic entity types (PER, LOC, ORG, MISC). Standard data includes train, development and test sets. Chunking The CoNLL2000 Chunking task (Sang and Buchholz, 2000) defines 11 syntactic chunk types (NP, VP, PP, etc.). Standard data includes train and test sets. Metric We adopt the BIOES tagging scheme for both tasks instead of the stand"
P19-1233,P16-1101,0,0.212471,"Missing"
P19-1233,W14-1609,0,0.0885275,"Missing"
P19-1233,P17-1161,0,0.39535,"Language Processing (NLP). Recently, neural models have become the de-facto standard for high-performance systems. Among various neural networks for sequence labeling, bi-directional RNNs (BiRNNs), especially BiLSTMs (Hochreiter and Schmidhuber, 1997) have become a dominant method on ∗ This work was done when Yijin Liu was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China † Jinan Xu is the corresponding author of the paper. 1 Code is available at: https://github.com/Adaxry/GCDT. multiple benchmark datasets (Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Peters et al., 2017). However, there are several natural limitations of the BiLSTMs architecture. For example, at each time step, the BiLSTMs consume an incoming word and construct a new summary of the past subsequence. This procedure should be highly nonlinear, to allow the hidden states to rapidly adapt to the mutable input while still preserving a useful summary of the past (Pascanu et al., 2014). While in BiLSTMs, even stacked BiLSTMs, the transition depth between consecutive hidden states are inherently shallow. Moreover, global contextual information, which has been shown highly useful for model sequence (Z"
P19-1233,N18-1202,0,0.170779,"Missing"
P19-1233,W09-1119,0,0.228702,"and Nichols, 2016)*† (Xin et al., 2018) GCDT GCDT + BERTLARGE Experiments Datasets and Metric NER The CoNLL03 NER task (Sang and De Meulder, 2003) is tagged with four linguistic entity types (PER, LOC, ORG, MISC). Standard data includes train, development and test sets. Chunking The CoNLL2000 Chunking task (Sang and Buchholz, 2000) defines 11 syntactic chunk types (NP, VP, PP, etc.). Standard data includes train and test sets. Metric We adopt the BIOES tagging scheme for both tasks instead of the standard BIO2, since previous studies have highlighted meaningful improvements with this scheme (Ratinov and Roth, 2009). We take the official conlleval 3 as the token-level F1 metric. Since the data size if relatively small, we train each final model for 5 times with different parameter initialization and report the mean and standard deviation F1 value. 4.2 Table 1: F1 scores on CoNLL03. † refers to models trained on both training and development set. * refers to adopting external task-specific resources. Models (Collobert et al., 2011)* (Huang et al., 2015)* (Yang et al., 2017b) (Zhai et al., 2017) (Hashimoto et al., 2017) (Søgaard and Goldberg, 2016) (Xin et al., 2018) GCDT GCDT + BERTLARGE Implementation De"
P19-1233,P17-1194,0,0.0389681,"Missing"
P19-1233,W00-0726,0,0.826473,"Missing"
P19-1233,W03-0419,0,0.299564,"Missing"
P19-1233,P16-2038,0,0.0556517,"Missing"
P19-1233,D18-1279,0,0.311212,"Missing"
P19-1233,yang-etal-2017-neural-reranking,0,0.217434,"ure of recurrent architecture makes RNN partial to the most recent input token. While our context-aware representation is incorporated with local word embeddings directly, which assists in capturing useful representations through combinatorial computing between diverse local word embeddings and the global contextual embedding. We further investigate the effects on positions where the global embedding is used. (Section 5.1) 2434 4 4.1 Models (Collobert et al., 2011)* (Huang et al., 2015)* (Passos et al., 2014)* (Lample et al., 2016) (Yang et al., 2016)* (Luo et al., 2015)* (Ma and Hovy, 2016) (Yang et al., 2017b)*† (Zhang et al., 2018) (Yang et al., 2017a) (Chiu and Nichols, 2016)*† (Xin et al., 2018) GCDT GCDT + BERTLARGE Experiments Datasets and Metric NER The CoNLL03 NER task (Sang and De Meulder, 2003) is tagged with four linguistic entity types (PER, LOC, ORG, MISC). Standard data includes train, development and test sets. Chunking The CoNLL2000 Chunking task (Sang and Buchholz, 2000) defines 11 syntactic chunk types (NP, VP, PP, etc.). Standard data includes train and test sets. Metric We adopt the BIOES tagging scheme for both tasks instead of the standard BIO2, since previous studies have hi"
P19-1233,P18-1030,0,0.187121,"). However, there are several natural limitations of the BiLSTMs architecture. For example, at each time step, the BiLSTMs consume an incoming word and construct a new summary of the past subsequence. This procedure should be highly nonlinear, to allow the hidden states to rapidly adapt to the mutable input while still preserving a useful summary of the past (Pascanu et al., 2014). While in BiLSTMs, even stacked BiLSTMs, the transition depth between consecutive hidden states are inherently shallow. Moreover, global contextual information, which has been shown highly useful for model sequence (Zhang et al., 2018), is insufficiently captured at each token position in BiLSTMs. Subsequently, inadequate representations flow into the final prediction layer, which leads to the restricted performance of BiLSTMs. In this paper, we present a global context enhanced deep transition architecture to eliminate the mentioned limitations of BiLSTMs. In particular, we base our network on the deep transition (DT) RNN (Pascanu et al., 2014), which increases the transition depth between consecutive hidden states for richer representations. Furthermore, we assign each token an additional representation, which is a summat"
P19-1288,W17-4123,0,0.128847,"ty. Recently, the Transformer model (Vaswani et al., 2017) further enhances the translation performance on multiple language pairs, while suffering from the slow decoding procedure, which reJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author stricts its application scenarios. The slow decoding problem of the Transformer model is caused by its autoregressive nature, which means that the target sentence is generated word by word according to the source sentence representations and the target translation history. Non-autoregressive Transformer model (Gu et al., 2017a) is proposed to accelerate the decoding process, which can simultaneously generate target words by discarding the autoregressive mechanism. Since the generation of target words is independent, NAT models utilize alternative information such as encoder inputs (Gu et al., 2017a), translation results from other systems (Lee et al., 2018; Guo et al., 2018) and latent variables (Kaiser et al., 2018) as decoder inputs. Without considering the target translation history, NAT models are weak to exploit the target words collocation knowledge and tend to generate repeated target words at adjacent time"
P19-1288,D17-1210,0,0.0371484,"Missing"
P19-1288,P84-1044,0,0.302576,"Missing"
P19-1288,D16-1139,0,0.285399,"of predicted token fed to the fusion layer. 4 Related Work Gu et al. (2017a) introduced the nonautoregressive Transformer model to accelerate the translation. Lee et al. (2018) proposed a nonautoregressive sequence model based on iterative refinement, where the outputs of the decoder are fed back as inputs in the next iteration. Guo et al. (2018) proposed to enhance the decoder inputs with phrase-table lookup and embedding mapping. Kaiser et al. (2018) used a sequence of autoregressively generated discrete latent variables as inputs of the decoder. Knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016) is a method for training a smaller and faster student network to perform better by learning from a teacher network, which is crucial in NAT models. Gu et al. (2017a) applied Sequence-level knowledge distillation to eliminate the multimodality in the training corpus. Li et al. (2018) further proposed to improve non-autoregressive models through distilling knowledge from intermediary hidden states and attention weights of autoregressive models. Apart from non-autoregressive translation, there are works toward speeding up the translation from other perspectives. Wang et al. (2018) proposed the s"
P19-1288,P02-1040,0,0.103733,"th Annual Meeting of the Association for Computational Linguistics, pages 3013–3024 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics In this paper, we present two approaches to retrieve the target sequential information for NAT models to enhance their translation ability and meanwhile preserve the fast-decoding property. Firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for NAT (Reinforce-NAT) to reduce the variance and stabilize the training procedure. We leverage the sequence-level objectives (e.g., BLEU (Papineni et al., 2002), GLEU (Wu et al., 2017), TER (Snover et al., 2006)) instead of the cross-entropy objective to encourage NAT model to generate high quality sentences rather than the correct token for each position. Secondly, we propose an innovative Transformer decoder named FS-decoder to fuse the target sequential information into the top layer of the decoder. The bottom layers of the FS-decoder run in parallel to keep the decoding speed and the top layer of the FS-decoder can exploit target sequential information to guide the target words generation procedure. We conduct experiments on three machine transla"
P19-1288,P16-1162,0,0.0899481,"et al., 2017). Importance sampling estimates the properties of a particular distribution through sampling on a different proposal distribution. Complementary sum sampling reducdes the variance through suming over the important subset and estimating the rest via sampling. 5 5.1 Experiments Settings Dataset. We conduct experiments on three translation tasks3 : IWSLT16 En→De (196k pairs), WMT14 En↔De (4.5M pairs) and WMT16 En↔Ro (610k pairs). We use the preprocessed datasets released by Lee et al. (2018), where all sentences are tokenized and segmented into subword units using the BPE algorithm (Sennrich et al., 2016). For all tasks, source and target languages share the vocabulary with size 40k. For WMT14 En-De, we employ newstest-2013 and newstest-2014 as development and test sets. For WMT16 En-Ro, we take newsdev-2016 and newstest-2016 as development and test sets. For IWSLT16 En-De, we use the test2013 for validation. Baselines. We take the Transformer model (Vaswani et al., 2017) as the autoregressive baseline. The non-autoregressive model based on iterative refinement (Lee et al., 2018) is the nonautoregressive baseline, and we set the number of iterations to 2. Pre-train. To evaluate the sequence-le"
P19-1288,D18-1510,1,0.827268,"Missing"
P19-1288,P16-1159,0,0.121143,"search is fed into the decoder to guide the generation of the next word. The prominent feature of the autoregressive model is that it requires the target side historical information in the decoding procedure. Therefore target words are generated in the one-by-one style. Due to the autoregressive property, the decoding speed is limited, which restricts the application of the autoregressive model. 2.2 Reinforcement learning techniques (Sutton et al., 2000; Ng et al., 1999; Sutton, 1984) have been widely applied to improve the performance of the autoregressive NMT with sequence-level objectives (Shen et al., 2016; Ranzato et al., 2015; Bahdanau et al., 2016). As sequence-level objectives are usually non-differentiable, the loss function is defined as the negative expected reward: Lθ = − θ = arg max{L(θ)} θ L(θ) = M X T X m=1 t=1 m log(p(ytm |y<t , X m , θ)), (2) X p(Y|X, θ) · r(Y), (3) Y=y1:T where Y = y1:T denotes possible sequences generated by the model, and r(Y) is the corresponding reward such as BLEU, GLEU and TER for generating sequence Y. Enumerating all the possible target sequences is impossible due to the exponential search space, and REINFORCE (Williams, 1992) gives an elegant way to estim"
P19-1288,2006.amta-papers.25,0,0.0258753,"l Linguistics, pages 3013–3024 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics In this paper, we present two approaches to retrieve the target sequential information for NAT models to enhance their translation ability and meanwhile preserve the fast-decoding property. Firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for NAT (Reinforce-NAT) to reduce the variance and stabilize the training procedure. We leverage the sequence-level objectives (e.g., BLEU (Papineni et al., 2002), GLEU (Wu et al., 2017), TER (Snover et al., 2006)) instead of the cross-entropy objective to encourage NAT model to generate high quality sentences rather than the correct token for each position. Secondly, we propose an innovative Transformer decoder named FS-decoder to fuse the target sequential information into the top layer of the decoder. The bottom layers of the FS-decoder run in parallel to keep the decoding speed and the top layer of the FS-decoder can exploit target sequential information to guide the target words generation procedure. We conduct experiments on three machine translation tasks (IWSLT16 En→De, WMT14 En↔De, WMT16 En→Ro"
P19-1288,D18-1149,0,0.507576,"ding problem of the Transformer model is caused by its autoregressive nature, which means that the target sentence is generated word by word according to the source sentence representations and the target translation history. Non-autoregressive Transformer model (Gu et al., 2017a) is proposed to accelerate the decoding process, which can simultaneously generate target words by discarding the autoregressive mechanism. Since the generation of target words is independent, NAT models utilize alternative information such as encoder inputs (Gu et al., 2017a), translation results from other systems (Lee et al., 2018; Guo et al., 2018) and latent variables (Kaiser et al., 2018) as decoder inputs. Without considering the target translation history, NAT models are weak to exploit the target words collocation knowledge and tend to generate repeated target words at adjacent time steps (Wang et al., 2019). Over-translation and undertranslation problems are aggravated and often occur due to the above reasons. Table 1 shows an inferior translation example generated by a NAT model. Compared to the autoregressive Transformer, NAT models achieve significant speedup while suffering from a large gap in translation qu"
P19-1288,P18-2053,0,0.0697674,"neural machine translation without damaging the translation quality. Sequence-level training techniques have been widely explored in autoregressive neural machine translation, where most works (Ranzato et al., 2015; Shen et al., 2016; Wu et al., 2016; He et al., 2016; Wu et al., 2017; Yang et al., 2017) relied on reinforcement learning (Williams, 1992; Sutton et al., 2000) to build the gradient estimator. Recently, techniques for sequence-level training with continuous objectives have been explored, including deterministic policy gradient algorithms (Gu et al., 2017b), bag-of-words objective (Ma et al., 2018) and probabilistic n-gram matching (Shao et al., 2018). However, to the best of our knowledge, sequence-level training has not been applied to non-autoregressive models yet. The methods of variance reduction through focusing on the important parts of the distribution include importance sampling (Bengio et al., 2003; Glynn and Iglehart, 1989) and complementary sum sampling (Botev et al., 2017). Importance sampling estimates the properties of a particular distribution through sampling on a different proposal distribution. Complementary sum sampling reducdes the variance through suming over the i"
P19-1288,D18-1044,0,0.0803608,"et al., 2015; Kim and Rush, 2016) is a method for training a smaller and faster student network to perform better by learning from a teacher network, which is crucial in NAT models. Gu et al. (2017a) applied Sequence-level knowledge distillation to eliminate the multimodality in the training corpus. Li et al. (2018) further proposed to improve non-autoregressive models through distilling knowledge from intermediary hidden states and attention weights of autoregressive models. Apart from non-autoregressive translation, there are works toward speeding up the translation from other perspectives. Wang et al. (2018) proposed the semi-autoregressive Transformer that generates a group of words in parallel at each time step. Press and Smith (2018) proposed the eager translation model that does not use the attention mechanism and has low latency. Zhang et al. (2018a) proposed the average attention network to accelerate decoding, which achieves significant speedup over the uncached Transformer. Zhang et al. (2018b) proposed cube pruning to speedup the beam search for neural machine translation without damaging the translation quality. Sequence-level training techniques have been widely explored in autoregress"
P19-1288,D18-1397,0,0.0355153,"e Y from the probability distribution and estimate the gradient with the gradient of log-probability weighted by the reward r(Y): ∇θ Lθ = t=1 where θ is a set of model parameters and y<t = {y1 , · · · , yt−1 } is the translation history. Given the training set D = {XM , YM } with M sentence pairs, the training objective is to maximize the loglikelihood of the training data as: Sequence-Level Training for Autoregressive NMT T X (4) ∇θ log(p(yt |y<t , X, θ)) · r(Y)]. − E[ Y t=1 Current reinforcement learning (RL) methods are designed for autoregressive models. Moreover, previous investigations (Wu et al., 2018; Weaver and Tao, 2013) show that the RL-based training procedure is unstable due to its high variance of gradient estimation. 3014 2.3 Non-Autoregressive Neural Machine Translation Non-autoregressive neural machine translation (Gu et al., 2017a) is proposed to accelerate the decoding process, which can simultaneously generate target words by discarding the autoregressive mechanism. The translation probability from X to Y is modeled as follows: P (Y |X, θ) = T Y p(yt |X, θ). (5) t=1 Given the training set D = {XM , YM } with M sentence pairs, the training objective is to maximize the log-likel"
P19-1288,1983.tc-1.13,0,0.575939,"Missing"
P19-1288,N18-1122,0,0.0997661,"Missing"
P19-1288,P18-1166,0,0.0194701,"liminate the multimodality in the training corpus. Li et al. (2018) further proposed to improve non-autoregressive models through distilling knowledge from intermediary hidden states and attention weights of autoregressive models. Apart from non-autoregressive translation, there are works toward speeding up the translation from other perspectives. Wang et al. (2018) proposed the semi-autoregressive Transformer that generates a group of words in parallel at each time step. Press and Smith (2018) proposed the eager translation model that does not use the attention mechanism and has low latency. Zhang et al. (2018a) proposed the average attention network to accelerate decoding, which achieves significant speedup over the uncached Transformer. Zhang et al. (2018b) proposed cube pruning to speedup the beam search for neural machine translation without damaging the translation quality. Sequence-level training techniques have been widely explored in autoregressive neural machine translation, where most works (Ranzato et al., 2015; Shen et al., 2016; Wu et al., 2016; He et al., 2016; Wu et al., 2017; Yang et al., 2017) relied on reinforcement learning (Williams, 1992; Sutton et al., 2000) to build the gradi"
P19-1288,D18-1460,1,0.730675,"liminate the multimodality in the training corpus. Li et al. (2018) further proposed to improve non-autoregressive models through distilling knowledge from intermediary hidden states and attention weights of autoregressive models. Apart from non-autoregressive translation, there are works toward speeding up the translation from other perspectives. Wang et al. (2018) proposed the semi-autoregressive Transformer that generates a group of words in parallel at each time step. Press and Smith (2018) proposed the eager translation model that does not use the attention mechanism and has low latency. Zhang et al. (2018a) proposed the average attention network to accelerate decoding, which achieves significant speedup over the uncached Transformer. Zhang et al. (2018b) proposed cube pruning to speedup the beam search for neural machine translation without damaging the translation quality. Sequence-level training techniques have been widely explored in autoregressive neural machine translation, where most works (Ranzato et al., 2015; Shen et al., 2016; Wu et al., 2016; He et al., 2016; Wu et al., 2017; Yang et al., 2017) relied on reinforcement learning (Williams, 1992; Sutton et al., 2000) to build the gradi"
P19-1426,W04-1013,0,0.0135768,"d select an oracle word oracle to simulate the context word. The oracle yj−1 word should be a word similar to the ground truth or a synonym. Using different strategies will prooracle . One option is duce a different oracle word yj−1 that word-level greedy search could be employed to output the oracle word of each step, which is called Word-level Oracle (called WO). Besides, we can further optimize the oracle by enlarging the search space with beam search and then reranking the candidate translations with a sentencelevel metric, e.g. BLEU (Papineni et al., 2002), GLEU (Wu et al., 2016), ROUGE (Lin, 2004), etc, the selected translation is called oracle sentence, the words in the translation are Sentence-level Oracle (denoted as SO). Word-Level Oracle For the {j−1}-th decoding step, the direct way to select the word-level oracle is to pick the word with the highest probability from the word distribution Pj−1 drawn by Equation (9), which is shown in Figure 2. The predicted score in oj−1 is the value before the softmax operation. In practice, we can acquire more robust word-level oracles by introducing the Gumbel-Max technique (Gumbel, 1954; Maddison et al., 2014), which provides a simple and eff"
P19-1426,P02-1040,0,0.104092,"o the ground truth word yj−1 predict yj , thus, we could select an oracle word oracle to simulate the context word. The oracle yj−1 word should be a word similar to the ground truth or a synonym. Using different strategies will prooracle . One option is duce a different oracle word yj−1 that word-level greedy search could be employed to output the oracle word of each step, which is called Word-level Oracle (called WO). Besides, we can further optimize the oracle by enlarging the search space with beam search and then reranking the candidate translations with a sentencelevel metric, e.g. BLEU (Papineni et al., 2002), GLEU (Wu et al., 2016), ROUGE (Lin, 2004), etc, the selected translation is called oracle sentence, the words in the translation are Sentence-level Oracle (denoted as SO). Word-Level Oracle For the {j−1}-th decoding step, the direct way to select the word-level oracle is to pick the word with the highest probability from the word distribution Pj−1 drawn by Equation (9), which is shown in Figure 2. The predicted score in oj−1 is the value before the softmax operation. In practice, we can acquire more robust word-level oracles by introducing the Gumbel-Max technique (Gumbel, 1954; Maddison et"
P19-1426,P16-1162,0,0.357348,"Missing"
P19-1426,D18-1510,1,0.715141,"Missing"
P19-1426,P16-1159,0,0.340677,"ntence-level oracle to relieve the overcorrection problem and neither the noise perturbations on the predicted distribution. Another direction of attempts is the sentencelevel training with the thinking that the sentencelevel metric, e.g., BLEU, brings a certain degree of flexibility for generation and hence is more robust to mitigate the exposure bias problem. To avoid the problem of exposure bias, Ranzato et al. (2015) presented a novel algorithm Mixed Incremental Cross-Entropy Reinforce (MIXER) for sequence-level training, which directly optimized the sentence-level BLEU used at inference. Shen et al. (2016) introduced the Minimum Risk Training (MRT) into the end-to-end NMT model, which optimized model parameters by minimizing directly the expected loss with respect to arbitrary evaluation metrics, e.g., sentence-level BLEU. Shao et al. (2018) proposed to eliminate the exposure bias through a probabilistic n-gram matching objective, which trains NMT NMT under the greedy decoding strategy. 5 Experiments 5.1 For Zh→En, the training dataset consists of 1.25M sentence pairs extracted from LDC corpora1 . We choose the NIST 2002 (MT02) dataset as the validation set, which has 878 sentences, and the NIS"
P19-1426,P16-1008,0,0.105283,"odel parameters with batch size setting to 80. Moreover, the learning rate is adjusted by adadelta optimizer (Zeiler, 2012) with ρ=0.95 and =1e-6. Dropout is applied on the output layer with dropout rate being 0.5. For Transformer model, we train base model with 1 We carry out experiments on the NIST Chinese→English (Zh→En) and the WMT’14 English→German (En→De) translation tasks. Settings These sentence pairs are mainly extracted from LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 2 http://www.statmt.org/wmt14/ translation-task.html 4338 Systems Tu et al. (2016) Shen et al. (2016) Zhang et al. (2017) this work Architecture MT03 MT04 MT05 Existing end-to-end NMT systems Coverage 33.69 38.05 35.01 MRT 37.41 39.87 37.45 Distortion 37.93 40.40 36.81 Our end-to-end NMT systems RNNsearch 37.93 40.53 36.65 + SS-NMT 38.82 41.68 37.28 + MIXER 38.70 40.81 37.59 + OR-NMT 40.40‡†? 42.63‡†? 38.87‡†? Transformer 46.89 47.88 47.40 + word oracle 47.42 48.34 47.89 + sentence oracle 48.31∗ 49.40∗ 48.72∗ MT06 Average 34.83 36.80 35.77 35.40 37.88 37.73 35.80 37.98 38.38 38.44‡ 46.66 47.34 48.45∗ 37.73 38.94 38.87 40.09 47.21 47.75 48.72 Table 1: Case-insensitive BLEU s"
P19-1426,1983.tc-1.13,0,0.438724,"Missing"
P19-1426,P17-1140,1,0.844345,"nhance the overcorrection recovery capacity. For the sentencelevel oracle selection, we set the beam size to be 3, set τ =0.5 in Equation (11) and µ=12 for the decay function in Equation (15). OR-NMT is the abbreviation of NMT with Overcorrection Recovery. 3 https://github.com/pytorch/fairseq Results on Zh→En Translation Results on the RNNsearch As shown in Table 1, Tu et al. (2016) propose to model coverage in RNN-based NMT to improve the adequacy of translations. Shen et al. (2016) propose minimum risk training (MRT) for NMT to directly optimize model parameters with respect to BLEU scores. Zhang et al. (2017) model distortion to enhance the attention model. Compared with them, our baseline system RNNsearch 1) outperforms previous shallow RNN-based NMT system equipped with the coverage model (Tu et al., 2016); and 2) achieves competitive performance with the MRT (Shen et al., 2016) and the Distortion (Zhang et al., 2017) on the same datasets. We hope that the strong shallow baseline system used in this work makes the evaluation convincing. We also compare with the other two related methods that aim at solving the exposure bias problem, including the scheduled sampling (Bengio et al., 2015) (SS-NMT)"
P19-1426,P05-1066,0,\N,Missing
S12-1108,H05-1079,0,0.0376177,"y treat the input expressions simply as surface strings, they may operate on syntactic or semantic representations of the input expressions, or on representations combining information from different 1 http://edits.fbk.eu/ 715 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 715–720, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics levels. Logic-based approach is to map the language expressions to logical meaning representations, and then rely on logical entailment checks, possibly by invoking theorem provers (Rinaldi et al., 2003; Bos & Markert, 2005; Tatu & Moldovan, 2005, 2007). An alternative to use logical meaning representations is to start by mapping each word of the input language expressions to a vector that shows how strongly the word co-occurs with particular other words in corpora (Lin, 1998b), possibly also taking into account syntactic information, for example requiring that the co-occurring words participate in particular syntactic dependencies (Pad´o & Lapata, 2007). Several textual entailment recognizing methods operate directly on the input surface strings. For example, they compute the string edit distance (Levenshtein,"
S12-1108,P05-1033,0,0.0209569,"ramework of our system, where a machine translation model is employed to translate foreign language into English, since original EDITS could only deal with the text in the same language pairs. 716 In the following of this section, we will describe the translation module and configuration of EDITS in details. Figure 1: The framework of our system. 2.1 Machine Translation Recently, machine translation has attracted intensive attention and has been well studied in natural language community. Effective models, such as Phrase-Based model (Koehn et al., 2003), Hierarchical Phrase-Based model (HPB) (Chiang, 2005), and Syntax-Based (Liu et al., 2006) model have been proposed to improve the translation quality. However, since current translation models require parallel corpus to extract translation rules, while parallel corpus on some language pairs such as Italian-English and Spanish-English are hard to obtain, therefore, we could use Google Translation Toolkit (GTT) to generate translation. Specifically, WMT 2 released some bilingual corpus for training, thus we use some portion to train a French-English translation engine using hierarchical phrase-based model. We also exploit system combination techn"
S12-1108,W07-1401,0,0.0260318,"nt (TE) recognition under a new dimension (cross-linguality), and within a new challenging application scenario (content synchronization) Readers can refer to M. Negri et al. 2012.s., for more detailed introduction. 1 Textual entailment, on the other hand, recognize, generate, or extract pairs of natural language expressions, and infer that if one element is true, whether the other element is also true. Several methods are proposed by previous researchers. There have been some workshops on textual entailment in recent years. The recognizing textual entailment challenges (Bar-Haim et al. 2006; Giampiccolo, Magnini, Dagan, & Dolan, 2007; Giampiccolo, Dang, Magnini, Dagan, & Dolan, 2008), currently in the 7th year, provide additional significant thrust. Consequently, there are a large number of published articles, proposed methods, and resources related to textual entailment. A special issue on textual entailment was also recently published, and its editorial provides a brief overview of textual entailment methods (Dagan, Dolan, Magnini, & Roth, 2009). Textual entailment recognizers judge whether or not two given language expressions constitute a correct textual entailment pair. Different methods may operate at different leve"
S12-1108,P10-4008,0,0.330664,"Missing"
S12-1108,W07-1407,0,0.133475,"the input language expressions to a vector that shows how strongly the word co-occurs with particular other words in corpora (Lin, 1998b), possibly also taking into account syntactic information, for example requiring that the co-occurring words participate in particular syntactic dependencies (Pad´o & Lapata, 2007). Several textual entailment recognizing methods operate directly on the input surface strings. For example, they compute the string edit distance (Levenshtein, 1966) of the two input strings, the number of their common words, or combinations of several string similarity measures (Malakasiotis & Androutsopoulos, 2007). Dependency grammar parsers (Melcuk, 1987; Kubler, McDonald, & Nivre, 2009) are popular in textual entailment research. However, cross-lingual textual entailment brings some problems on past algorithms. On the other hand, many methods can’t be applied to it directly. In this paper, we propose a translation based method for cross-lingual textual entailment, which has been described in Mehdad et al. 2010. First, we translate one part of the text, which termed as “t1” and written in one language, into English, which termed as “t2”. Then, we use EDITS, an open source package, to recognize entailm"
S12-1108,N10-1045,0,0.238488,", they compute the string edit distance (Levenshtein, 1966) of the two input strings, the number of their common words, or combinations of several string similarity measures (Malakasiotis & Androutsopoulos, 2007). Dependency grammar parsers (Melcuk, 1987; Kubler, McDonald, & Nivre, 2009) are popular in textual entailment research. However, cross-lingual textual entailment brings some problems on past algorithms. On the other hand, many methods can’t be applied to it directly. In this paper, we propose a translation based method for cross-lingual textual entailment, which has been described in Mehdad et al. 2010. First, we translate one part of the text, which termed as “t1” and written in one language, into English, which termed as “t2”. Then, we use EDITS, an open source package, to recognize entailment relations between two parts. Large-scale experiments are conducted on four language pairs, French-English, Spanish-English, Italian-English and GermanEnglish. Although our method achieves promising results reported by organizers, it is still far from perfect compared to other participants. The remainder of this paper is organized as follows. We describe our system framework in section 2. We report e"
S12-1108,S12-1053,0,0.0420748,"idirectional prediction, thus we exchange the hypothesis and test to detect entailment in another direction. Experimental results show that our method achieves promising results but not perfect results compared to other participants. 1 Introduction In Cross-Lingual Textual Entailment task (CLTE) of 2012, the organizers hold a task for CrossLingual Textual Entailment. The Cross-Lingual Textual Entailment task addresses textual entailment (TE) recognition under a new dimension (cross-linguality), and within a new challenging application scenario (content synchronization) Readers can refer to M. Negri et al. 2012.s., for more detailed introduction. 1 Textual entailment, on the other hand, recognize, generate, or extract pairs of natural language expressions, and infer that if one element is true, whether the other element is also true. Several methods are proposed by previous researchers. There have been some workshops on textual entailment in recent years. The recognizing textual entailment challenges (Bar-Haim et al. 2006; Giampiccolo, Magnini, Dagan, & Dolan, 2007; Giampiccolo, Dang, Magnini, Dagan, & Dolan, 2008), currently in the 7th year, provide additional significant thrust. Consequently, ther"
S12-1108,D11-1062,0,0.213452,"Missing"
S12-1108,J07-2002,0,0.0279302,"Missing"
S12-1108,N03-1017,0,0.00756406,"Missing"
S12-1108,W03-1604,0,0.0226145,". For example, they may treat the input expressions simply as surface strings, they may operate on syntactic or semantic representations of the input expressions, or on representations combining information from different 1 http://edits.fbk.eu/ 715 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 715–720, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics levels. Logic-based approach is to map the language expressions to logical meaning representations, and then rely on logical entailment checks, possibly by invoking theorem provers (Rinaldi et al., 2003; Bos & Markert, 2005; Tatu & Moldovan, 2005, 2007). An alternative to use logical meaning representations is to start by mapping each word of the input language expressions to a vector that shows how strongly the word co-occurs with particular other words in corpora (Lin, 1998b), possibly also taking into account syntactic information, for example requiring that the co-occurring words participate in particular syntactic dependencies (Pad´o & Lapata, 2007). Several textual entailment recognizing methods operate directly on the input surface strings. For example, they compute the string edit di"
S12-1108,W07-1404,0,0.0509293,"Missing"
S12-1108,P06-1077,1,0.857523,"achine translation model is employed to translate foreign language into English, since original EDITS could only deal with the text in the same language pairs. 716 In the following of this section, we will describe the translation module and configuration of EDITS in details. Figure 1: The framework of our system. 2.1 Machine Translation Recently, machine translation has attracted intensive attention and has been well studied in natural language community. Effective models, such as Phrase-Based model (Koehn et al., 2003), Hierarchical Phrase-Based model (HPB) (Chiang, 2005), and Syntax-Based (Liu et al., 2006) model have been proposed to improve the translation quality. However, since current translation models require parallel corpus to extract translation rules, while parallel corpus on some language pairs such as Italian-English and Spanish-English are hard to obtain, therefore, we could use Google Translation Toolkit (GTT) to generate translation. Specifically, WMT 2 released some bilingual corpus for training, thus we use some portion to train a French-English translation engine using hierarchical phrase-based model. We also exploit system combination technique (A Rosti et al., 2007) to improv"
S12-1108,H05-1047,0,\N,Missing
S12-1108,P07-1040,0,\N,Missing
S12-1108,P11-1134,0,\N,Missing
W11-1911,P05-1022,0,0.0603318,"Missing"
W11-1911,P08-1067,0,0.0477932,"Missing"
W11-1911,P04-1018,0,0.370236,"andidates among all constituents from both given parse tree and packed forest. The packed forest is a compact representation of all parse trees for a given sentence. Readers can refer to (Mi et al., 2008) for detailed definitions. Once the mentions are identified, the left step is to group mentions referring to same object into similar entity. This problem can be viewed as binary classification problem of determining whether each mention pairs corefer. We use a Maximum Entropy classifier to predict the possibility that two mentions refer to the similar entity. And mainly following the work of Luo et al. (2004), we use a beam search algorithm based on Bell Tree to obtain the global optimal classification. As this is the first time we participate competition of coreference resolution, we mainly concentrate on developing fault tolerant capability of our system while omitting feature engineering and other helpful technologies. 2 Mention Detection The first step of the coreference resolution tries to recognize occurrences of mentions in documents. Note that we recognize mention boundaries only on development and test set while generating training 76 Proceedings of the 15th Conference on Computational Na"
W11-1911,P08-1023,1,0.901686,"Missing"
W11-1911,P10-1142,0,0.0187597,"we use the L-BFGS parameter estimation algorithm with gaussian prior smoothing (Chen and Rosenfeld, 1999). We set the gaussian prior to 2 and train the model in 100 iterations. • [A][B][C][D], [A][B][CD] 3.1 Creation of Entities This stage aims to create the mentions detected in the first stage into entities, according to the prediction of classifier. One simple method is to use a greedy algorithm, by comparing each mention to its previous mentions and refer to the one that has the highest probability. In principle, this algorithm is too greedy and sometimes results in unreasonable partition (Ng, 2010). To address this problem, we follow the literature (Luo et al., 2004) and propose to use beam search to find global optimal partition. Intuitively, creation of entities can be casted as partition problem. And the number of partitions equals the Bell Number (Bell, which has a ∑ 1934), kn “closed” formula B(n) = 1e ∞ . k=0 k! Clearly, this number is very huge when n is large, enumeration of all partitions is impossible, so we instead designing a beam search algorithm to find the best partition. Formally, the task is to optimize the following objective, yˆ = arg max ϕ∈P ∑ P rob(e) (1) e∈ϕ ∑ pos("
W11-1911,W11-1901,0,0.0939802,"Missing"
W11-1911,J01-4004,0,0.0947592,"ence, intuitively, we 77 3 Determining Coreference This stage is to determine which mentions belong to the same entity. We train a Maximum Entropy classifier (Le, 2004) to decide whether two mentions are coreferent. We use the method proposed by Soon, et al.’s to generate the training instances, where a positive instance is formed between current mention Mj and its closest preceding antecedent Mi , and a negative instance is created by paring Mj with each of the intervening mentions, Mi+1 , Mi+2 ,...,Mj−1 . We use the following features to train our classifier. Features in Soon et al.’s work (Soon et al., 2001) Lexical features IS PREFIX: whether the string of one mention is prefix of the other; IS SUFFIX: whether the string of one mention is suffix of the other; ACRONYM: whether one mention is the acronym of the other; Distance features SENT DIST: distance between the sentences containing the two mentions; MEN DIST: number of mentions between two mentions; Grammatical features IJ PRONOUN: whether both mentions are pronoun; I NESTED: whether mention i is nested in another mention; J NESTED: whether mention j is nested in another mention; Syntax features HEAD: whether the heads of two mentions have t"
W11-1911,D08-1022,0,\N,Missing
