2020.acl-main.338,P19-1140,0,0.0276113,"present a simpliﬁed graph neural network model, called graph convolutional networks (GCN), which has been exported to several tasks such as scene recognition (Yuan et al., 2019), 3668 semi-supervised node classiﬁcation (Zhang et al., 2019b), text-to-SQL parsing (Bogin et al., 2019) and relation extraction (Sahu et al., 2019). On this basis, some other improved Graph-based Neural Networks are proposed. Morris et al. (2019) propose a generalization of Graph-based Neural Networks, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. Cao et al. (2019) propose a novel Multi-channel Graph Neural Network model to learn alignment-oriented knowledge graph embeddings by robustly encoding two knowledge graphs via multiple channels. More recently, there exist several studies also adopting graph-based neural networks to ASC. For instance, Hou et al. (2019) and Zhang et al. (2019a) build GCN over the dependency tree of a sentence to exploit syntactical information and word dependencies for learning better aspect-related sentence representation for ASC. Different from all the above studies, this paper proposes a novel Cooperative Graph Attention Netw"
2020.acl-main.338,D17-1047,0,0.0234547,"sting studies mainly focus on utilizing various approaches (e.g., attention mechanism and memory network) to align each aspect and the sentence for learning aspect-related sentence representation. Wang et al. (2016) propose an attention-based LSTM in order to explore the potential correlation of aspects and sentiment polarities in ASC. Wang et al. (2018) propose a hierarchical attention network to incorporate both words and clauses information for ASC. He et al. (2018a) propose an attention-based approach to incorporate the aspect-related syntactic information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information for performing ASC. Wang et al. (2019a) and Wang et al. (2019b) leverage reinforcement learning grounded approaches to select aspect-relevant words for ASC. Recently, a few studies have recognized the information deﬁciency problem in ASC and attempted to using external information to improve the performance of ASC. He et al. (2018b) and Chen and Qian (2019) incorporate the knowledge from document-le"
2020.acl-main.338,P19-1052,0,0.0183307,"information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information for performing ASC. Wang et al. (2019a) and Wang et al. (2019b) leverage reinforcement learning grounded approaches to select aspect-relevant words for ASC. Recently, a few studies have recognized the information deﬁciency problem in ASC and attempted to using external information to improve the performance of ASC. He et al. (2018b) and Chen and Qian (2019) incorporate the knowledge from document-level sentiment classiﬁcation to improve the performance of ASC. Ma et al. (2018) propose an extension of LSTM to integrate the commonsense knowledge into the recurrent encoder for improving the performance of ASC. In addition, it is worthwhile to note that Hazarika et al. (2018) also investigate the inter-aspect sentiment dependency for ASC, but is limited to capture this information inside a single sentence. In summary, all the above studies ignore the document-level sentiment preference information, which can be leveraged to effectively mitigate the"
2020.acl-main.338,N19-1423,0,0.00741175,"es each new vertex vector h tex vi by considering neighboring vertices’ vectors {hj }Ij=1 with the following formulas: ˆ i = tanh( h I  αij W hj + b) j=1 exp(f (w [W hi ; W hj ])) αij = I t=1 exp(f (w  [W h (1) i ; W ht ])) where αij is the attention weight (i.e., the edge weight) between vertex vi and vertex vj . f (·) is a LeakyReLU activation function. [; ] denotes vector concatenation. W ∈ Rd×d and w ∈ R2d are the trainable parameters. In the following, we will illustrate the ﬁve main components of our CoGAN approach respectively. 3.2 Encoding Block As a text encoding mechanism, BERT (Devlin et al., 2019) can be ﬁne-tuned to create state-of-the-art models for a range of NLP tasks, e.g., text classiﬁcation and natural language inference. In our approach, we use BERT-base2 (uncased) model to encode both the aspect and the sentence as follows. • Aspect Encoding. Since an aspect ak consists of an entity eentity and an attribute eattribute (Pontiki et al., 2015), we process the entity-attribute pair (eentity , eattribute ) into the input pair format of BERT as: [CLS] eentity [SEP] eattribute [SEP] Then, we feed the entity-attribute pair into BERT and regard the mark “[CLS]” representation as the as"
2020.acl-main.338,C18-1096,0,0.0832994,"networks. Aspect Sentiment Classiﬁcation. The ASC task aims to predict the sentiment polarity for each aspect discussed inside a sentence. Existing studies mainly focus on utilizing various approaches (e.g., attention mechanism and memory network) to align each aspect and the sentence for learning aspect-related sentence representation. Wang et al. (2016) propose an attention-based LSTM in order to explore the potential correlation of aspects and sentiment polarities in ASC. Wang et al. (2018) propose a hierarchical attention network to incorporate both words and clauses information for ASC. He et al. (2018a) propose an attention-based approach to incorporate the aspect-related syntactic information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information for performing ASC. Wang et al. (2019a) and Wang et al. (2019b) leverage reinforcement learning grounded approaches to select aspect-relevant words for ASC. Recently, a few studies have recognized the information deﬁciency problem in ASC and attempted to u"
2020.acl-main.338,P18-2092,0,0.0685403,"networks. Aspect Sentiment Classiﬁcation. The ASC task aims to predict the sentiment polarity for each aspect discussed inside a sentence. Existing studies mainly focus on utilizing various approaches (e.g., attention mechanism and memory network) to align each aspect and the sentence for learning aspect-related sentence representation. Wang et al. (2016) propose an attention-based LSTM in order to explore the potential correlation of aspects and sentiment polarities in ASC. Wang et al. (2018) propose a hierarchical attention network to incorporate both words and clauses information for ASC. He et al. (2018a) propose an attention-based approach to incorporate the aspect-related syntactic information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information for performing ASC. Wang et al. (2019a) and Wang et al. (2019b) leverage reinforcement learning grounded approaches to select aspect-relevant words for ASC. Recently, a few studies have recognized the information deﬁciency problem in ASC and attempted to u"
2020.acl-main.338,P19-1048,0,0.0578973,"n task to a sentence pair classiﬁcation task. In our implementation, we regard the pair of sentence and its aspect as the input pair of BERT-base model (Devlin et al., 2018) for performing ASC. 8) CADMN. This approach employs attention model to attend on relevant aspects for enhancing the aspect representation. This is a state-of-the-art approach proposed by Song et al. (2019). 9) IMN. This approach is a multi-task learning approach, which employs a novel message passing mechanism to better exploit the correlation among the tasks related to ASC. This is a state-of-the-art approach proposed by He et al. (2019). 10) BERT-QA. This approach is an extension of the above BERT baseline proposed by Sun et al. (2019). In this study, we adopt BERTpair-QA-M in our implementation. This is another state-of-the-art approach for ASC. 11) Sentiue. This is the best-performed system in SemEval-2015 Task 12 (Saias, 2015), which achieves the best accuracy scores in both the laptop15 and restaurant15 domains. 12) XRCE. This is the best-performed system in SemEval-2016 Task 5 (Pontiki et al., 2016), which achieves the best accuracy score in the restaurant16 domain. 13) IIT-TUDA. This is also the best-performed system i"
2020.acl-main.338,S16-1174,0,0.0691356,"Missing"
2020.acl-main.338,P10-1043,1,0.697296,"ITY, polarity = positive - Category = FOOD#PRICES, polarity = positive S3: The lava cake dessert was incredible and I recommend it. - Category = FOOD#QUALITY, polarity = positive Figure 1: Two documents from SemEval 2016 (Pontiki et al. (2016)) datasets, where aspect category is deﬁned as the entity E and attribute A pair (i.e., E#A). Red lines denote the intra-aspect sentiment consistency and blue lines denote the inter-aspect sentiment tendency. Introduction Aspect Sentiment Classiﬁcation (ASC), a ﬁnegrained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010), aims to identify the sentiment polarity (e.g., positive, negative or neutral) for each aspect discussed inside a sentence. For example, the sentence “The restaurant has quite low price but the food tastes not good” would be assigned with a positive polarity for the aspect price and with a negative ∗ Corresponding Author: Jingjing Wang. polarity for the aspect food. Over the past decade, the ASC task has been drawing more and more interests (Tang et al., 2016b; Wang et al., 2018) due to its wide applications, such as e-commerce customer service (Jing et al., 2015), public opinion mining (Wang"
2020.acl-main.338,S16-1002,0,0.132215,"Missing"
2020.acl-main.338,S15-2082,0,0.248872,"Missing"
2020.acl-main.338,P19-1423,0,0.043128,"e document-level sentiment preference information, which can be leveraged to effectively mitigate the information deﬁciency problem in ASC. Graph-based Neural Networks. In recent years, graph-based neural networks have received more and more attentions. As a pioneer, Kipf and Welling (2017) present a simpliﬁed graph neural network model, called graph convolutional networks (GCN), which has been exported to several tasks such as scene recognition (Yuan et al., 2019), 3668 semi-supervised node classiﬁcation (Zhang et al., 2019b), text-to-SQL parsing (Bogin et al., 2019) and relation extraction (Sahu et al., 2019). On this basis, some other improved Graph-based Neural Networks are proposed. Morris et al. (2019) propose a generalization of Graph-based Neural Networks, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. Cao et al. (2019) propose a novel Multi-channel Graph Neural Network model to learn alignment-oriented knowledge graph embeddings by robustly encoding two knowledge graphs via multiple channels. More recently, there exist several studies also adopting graph-based neural networks to ASC. For instance, Hou et al. (2019) and Zh"
2020.acl-main.338,S15-2130,0,0.15077,"entation. This is a state-of-the-art approach proposed by Song et al. (2019). 9) IMN. This approach is a multi-task learning approach, which employs a novel message passing mechanism to better exploit the correlation among the tasks related to ASC. This is a state-of-the-art approach proposed by He et al. (2019). 10) BERT-QA. This approach is an extension of the above BERT baseline proposed by Sun et al. (2019). In this study, we adopt BERTpair-QA-M in our implementation. This is another state-of-the-art approach for ASC. 11) Sentiue. This is the best-performed system in SemEval-2015 Task 12 (Saias, 2015), which achieves the best accuracy scores in both the laptop15 and restaurant15 domains. 12) XRCE. This is the best-performed system in SemEval-2016 Task 5 (Pontiki et al., 2016), which achieves the best accuracy score in the restaurant16 domain. 13) IIT-TUDA. This is also the best-performed system in SemEval-2016 Task 5 (Pontiki et al., 2016), while achieving the best accuracy score in the laptop16 domain. 15) CoGAN w/o Intra-Aspect Consistency. Our approach only modeling Inter-Aspect Tendency. 16) CoGAN w/o Inter-Aspect Tendency. Our approach only modeling Intra-Aspect Consistency. 17) CoGAN"
2020.acl-main.338,N19-1035,0,0.360347,"S] Excellent food … [SEP] What do you … Encoding Block  ൌ ܹݎ  ܾ ݒො Layer L ܧ sentence vector ݒ ܵ ݒොଶ ݒොଵ Layer 2 … ܧሾௌாሿ … ܽ [SEP] … Shared BERT ܧଵ ୀଵ Layer 1 ܧሾௌሿ quality … ܧሾௌாሿ … ܧଵ ܵାଵ ௧௧௨௧ [SEP] ൌ ሺሺݒ  ߙ ሺ ߙ ܹ ݒ ሻሻ  ܾሻ ܵ Aspect Input: FOOD#QUALITY ௧௧௬ ூᇲ ሺ௧ሻ ݒො ூ ܵଷ ሺ௧ሻ ݒො ܵଶ ൌ ሺ ߙ ܹఈ ݒ  ܾఈ ሻ ୀଵ Inter-Aspect Tendency Modeling Block Interaction Block Figure 2: The overall framework of our proposed Cooperative Graph Attention Networks (CoGAN). • Sentence Encoding. We borrow the approach proposed by Sun et al. (2019) to generate the aspectrelated sentence representation, which has achieved promising performance for the ASC task. Following Sun et al. (2019), we ﬁrst process the sentence si and its corresponding aspect ak into the input pair format of BERT as: [CLS] si [SEP] question(ak ) [SEP] where question(·) denotes the construction of auxiliary question sentence for aspect ak proposed by Sun et al. (2019). For example, the auxiliary sentence for aspect FOOD#PRICE is constructed as “what do you think of the food and price?”. Then, we similarly feed the above pair into BERT (shared with aspect encoding)"
2020.acl-main.338,C16-1311,0,0.576477,"n Aspect Sentiment Classiﬁcation (ASC), a ﬁnegrained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010), aims to identify the sentiment polarity (e.g., positive, negative or neutral) for each aspect discussed inside a sentence. For example, the sentence “The restaurant has quite low price but the food tastes not good” would be assigned with a positive polarity for the aspect price and with a negative ∗ Corresponding Author: Jingjing Wang. polarity for the aspect food. Over the past decade, the ASC task has been drawing more and more interests (Tang et al., 2016b; Wang et al., 2018) due to its wide applications, such as e-commerce customer service (Jing et al., 2015), public opinion mining (Wang et al., 2019c) and Question Answering (Wang et al., 2019a). In the literature, given the ASC datasets (Pontiki et al. (2016)) where aspects (i.e., entity and attribute) are manually annotated comprehensively sentence by sentence, previous studies model the aspect sentiment independently sentence by sentence, which suffer from the problem of ignoring the document-level sentiment preference information. In this study, we argue that such documentlevel sentiment"
2020.acl-main.338,D16-1021,0,0.477432,"n Aspect Sentiment Classiﬁcation (ASC), a ﬁnegrained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010), aims to identify the sentiment polarity (e.g., positive, negative or neutral) for each aspect discussed inside a sentence. For example, the sentence “The restaurant has quite low price but the food tastes not good” would be assigned with a positive polarity for the aspect price and with a negative ∗ Corresponding Author: Jingjing Wang. polarity for the aspect food. Over the past decade, the ASC task has been drawing more and more interests (Tang et al., 2016b; Wang et al., 2018) due to its wide applications, such as e-commerce customer service (Jing et al., 2015), public opinion mining (Wang et al., 2019c) and Question Answering (Wang et al., 2019a). In the literature, given the ASC datasets (Pontiki et al. (2016)) where aspects (i.e., entity and attribute) are manually annotated comprehensively sentence by sentence, previous studies model the aspect sentiment independently sentence by sentence, which suffer from the problem of ignoring the document-level sentiment preference information. In this study, we argue that such documentlevel sentiment"
2020.acl-main.338,P19-1345,1,0.819933,"010), aims to identify the sentiment polarity (e.g., positive, negative or neutral) for each aspect discussed inside a sentence. For example, the sentence “The restaurant has quite low price but the food tastes not good” would be assigned with a positive polarity for the aspect price and with a negative ∗ Corresponding Author: Jingjing Wang. polarity for the aspect food. Over the past decade, the ASC task has been drawing more and more interests (Tang et al., 2016b; Wang et al., 2018) due to its wide applications, such as e-commerce customer service (Jing et al., 2015), public opinion mining (Wang et al., 2019c) and Question Answering (Wang et al., 2019a). In the literature, given the ASC datasets (Pontiki et al. (2016)) where aspects (i.e., entity and attribute) are manually annotated comprehensively sentence by sentence, previous studies model the aspect sentiment independently sentence by sentence, which suffer from the problem of ignoring the document-level sentiment preference information. In this study, we argue that such documentlevel sentiment preference information is crucial to 3667 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3667–3677 c"
2020.acl-main.338,D19-1560,1,0.883168,"Missing"
2020.acl-main.338,D16-1058,0,0.0694742,"hree top-performed systems from SemEval-2015 Task 12 and SemEval2016 Task 5 (Pontiki et al., 2015, 2016). 2 Related Work In this section, we ﬁrst review the Aspect Sentiment Classiﬁcation (ASC) task, and then introduce the related studies on graph-based neural networks. Aspect Sentiment Classiﬁcation. The ASC task aims to predict the sentiment polarity for each aspect discussed inside a sentence. Existing studies mainly focus on utilizing various approaches (e.g., attention mechanism and memory network) to align each aspect and the sentence for learning aspect-related sentence representation. Wang et al. (2016) propose an attention-based LSTM in order to explore the potential correlation of aspects and sentiment polarities in ASC. Wang et al. (2018) propose a hierarchical attention network to incorporate both words and clauses information for ASC. He et al. (2018a) propose an attention-based approach to incorporate the aspect-related syntactic information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information"
2020.acl-main.338,D19-1464,0,0.0482997,"e this information inside a single sentence. In summary, all the above studies ignore the document-level sentiment preference information, which can be leveraged to effectively mitigate the information deﬁciency problem in ASC. Graph-based Neural Networks. In recent years, graph-based neural networks have received more and more attentions. As a pioneer, Kipf and Welling (2017) present a simpliﬁed graph neural network model, called graph convolutional networks (GCN), which has been exported to several tasks such as scene recognition (Yuan et al., 2019), 3668 semi-supervised node classiﬁcation (Zhang et al., 2019b), text-to-SQL parsing (Bogin et al., 2019) and relation extraction (Sahu et al., 2019). On this basis, some other improved Graph-based Neural Networks are proposed. Morris et al. (2019) propose a generalization of Graph-based Neural Networks, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. Cao et al. (2019) propose a novel Multi-channel Graph Neural Network model to learn alignment-oriented knowledge graph embeddings by robustly encoding two knowledge graphs via multiple channels. More recently, there exist several studies"
2020.ccl-1.22,J93-2003,0,0.141654,"Missing"
2020.ccl-1.22,P17-2025,0,0.060887,"Missing"
2020.ccl-1.22,N16-1014,0,0.0667337,"Missing"
2020.ccl-1.22,D17-1230,0,0.066836,"Missing"
2020.ccl-1.22,I17-1099,0,0.0289316,"Missing"
2020.ccl-1.22,C16-1316,0,0.0325835,"Missing"
2020.ccl-1.22,P02-1040,0,0.10603,"Missing"
2020.ccl-1.22,W12-2018,0,0.0501177,"Missing"
2020.ccl-1.22,D17-1235,0,0.0498423,"Missing"
2020.ccl-1.22,N15-1020,0,0.0668821,"Missing"
2020.ccl-1.22,D15-1199,0,0.0392959,"Missing"
2020.ccl-1.22,D19-1124,0,0.0419545,"Missing"
2020.ccl-1.22,D19-1186,0,0.0332404,"Missing"
2020.ccl-1.22,P18-1102,0,0.0318686,"Missing"
2020.coling-main.17,C10-1021,1,0.898412,"ause pair extraction (ECPE), and graph neural networks (GNNs). 2.1 Emotion Cause Extraction and Emotion-Cause Pair Extraction The task of emotion cause extraction (ECE) which extracts the causes of given emotion keywords has been intensively studied for years. Most of the previous works focused on contextual information extraction from the context of the given emotion keyword either with manual rules or with machine learning methods. Lee et al. (2010) constructed an emotion cause corpus from Sinica Corpus and then built a rule-based system to extract linguistic features. Based on this corpus, Chen et al. (2010) proposed a multi-label approach with linguistic patterns that can capture linguistic cues in contexts with manual rules. Other rule-based feature extraction methods (Neviarouskaya and Aono, 2013; Li and Xu, 2014; Gao et al., 2015a; Gao et al., 2015b; Yada et al., 2017; Yu et al., 2019) were also proposed to extract contextual features. Other than rule-based methods, Gui et al. (2016) constructed a Chinese event-driven ECE corpus with SINA city news and proposed a convolution kernel-based multi-kernel Support Vector Machine (SVM) to 199 extract contextual features from given syntactical trees."
2020.coling-main.17,D18-1066,1,0.850628,"(2016) constructed a Chinese event-driven ECE corpus with SINA city news and proposed a convolution kernel-based multi-kernel Support Vector Machine (SVM) to 199 extract contextual features from given syntactical trees. Afterward, deep learning has attracted attention from the ECE research community. Gui et al. (2017) converted the ECE task to a Question Answering (QA) task and proposed a Convolutional Multiple-Slot Deep Memory Network to store relevant contextual information. Other neural models (Li et al., 2018; Xu et al., 2019) were also proposed to extract contextual information. Besides, Chen et al. (2018) presented a neural network-based joint approach for emotion extraction and emotion cause extraction to capture mutual benefits across these two emotion analysis tasks. Different from ECE in which emotion keywords are provided before the extraction of their causes, the task of emotion-cause pair extraction (ECPE) was first proposed in Xia & Ding (2019), in which emotions and their corresponding causes are extracted at the same time. For this new task, they proposed a two-step approach, which firstly extracted emotion clauses and cause clauses individually using an interactive multi-task learni"
2020.coling-main.17,N19-1423,0,0.0162455,"Missing"
2020.coling-main.17,D19-1015,0,0.0229599,"ly. In addition, z is a normalization factor which is the node degree. σ is a non-linear activation function and ReLU (Nair and Hinton, 2010) is used in this paper. 202 2 for node cp using the After that, the second transformation is applied to obtain the representation gi,j i,j features output from the first transformation: X X 1 2 1 2 1 2 2 gi,k WD1 + gi,t WD2 + gi,j WSL ) (8) gi,j = σ( k∈D1 t∈D2 2 2 2 where WD1 ∈ Rdout ×dout , WD2 ∈ Rdout ×dout , and WSL ∈ Rdout ×dout are weight matrices for the p normalized nodes linked to node ci,j . Compared to the feature transformation process used in Ghosal et al. (2019), we distinguish contextual information propagation through D1 edges and D2 edges and use different weight matrices to deal with the two propagations separately. Moreover, using the two transformations plus D2 edges, contextual information can be propagated between any two nodes with the greatest distance in a pair graph. E.g., in Pair Graph 3 in Fig. 2, the information on cp3,1 and cp3,5 can be transmit to each other through the two transformations which use two D2 edges (i.e., cp3,1 ↔ cp3,3 ↔ cp3,5 ). 3.4 Classification Emotion-Cause Pair Extraction: Since two clauses in an emotion-cause pai"
2020.coling-main.17,D16-1170,0,0.122265,"th manual rules or with machine learning methods. Lee et al. (2010) constructed an emotion cause corpus from Sinica Corpus and then built a rule-based system to extract linguistic features. Based on this corpus, Chen et al. (2010) proposed a multi-label approach with linguistic patterns that can capture linguistic cues in contexts with manual rules. Other rule-based feature extraction methods (Neviarouskaya and Aono, 2013; Li and Xu, 2014; Gao et al., 2015a; Gao et al., 2015b; Yada et al., 2017; Yu et al., 2019) were also proposed to extract contextual features. Other than rule-based methods, Gui et al. (2016) constructed a Chinese event-driven ECE corpus with SINA city news and proposed a convolution kernel-based multi-kernel Support Vector Machine (SVM) to 199 extract contextual features from given syntactical trees. Afterward, deep learning has attracted attention from the ECE research community. Gui et al. (2017) converted the ECE task to a Question Answering (QA) task and proposed a Convolutional Multiple-Slot Deep Memory Network to store relevant contextual information. Other neural models (Li et al., 2018; Xu et al., 2019) were also proposed to extract contextual information. Besides, Chen e"
2020.coling-main.17,D17-1167,0,0.122012,"ic cues in contexts with manual rules. Other rule-based feature extraction methods (Neviarouskaya and Aono, 2013; Li and Xu, 2014; Gao et al., 2015a; Gao et al., 2015b; Yada et al., 2017; Yu et al., 2019) were also proposed to extract contextual features. Other than rule-based methods, Gui et al. (2016) constructed a Chinese event-driven ECE corpus with SINA city news and proposed a convolution kernel-based multi-kernel Support Vector Machine (SVM) to 199 extract contextual features from given syntactical trees. Afterward, deep learning has attracted attention from the ECE research community. Gui et al. (2017) converted the ECE task to a Question Answering (QA) task and proposed a Convolutional Multiple-Slot Deep Memory Network to store relevant contextual information. Other neural models (Li et al., 2018; Xu et al., 2019) were also proposed to extract contextual information. Besides, Chen et al. (2018) presented a neural network-based joint approach for emotion extraction and emotion cause extraction to capture mutual benefits across these two emotion analysis tasks. Different from ECE in which emotion keywords are provided before the extraction of their causes, the task of emotion-cause pair extr"
2020.coling-main.17,W10-0206,1,0.859452,"te the effectiveness of our PairGCN model. 2 Related Works In this section, we will briefly summarise related research on emotion cause extraction (ECE), emotioncause pair extraction (ECPE), and graph neural networks (GNNs). 2.1 Emotion Cause Extraction and Emotion-Cause Pair Extraction The task of emotion cause extraction (ECE) which extracts the causes of given emotion keywords has been intensively studied for years. Most of the previous works focused on contextual information extraction from the context of the given emotion keyword either with manual rules or with machine learning methods. Lee et al. (2010) constructed an emotion cause corpus from Sinica Corpus and then built a rule-based system to extract linguistic features. Based on this corpus, Chen et al. (2010) proposed a multi-label approach with linguistic patterns that can capture linguistic cues in contexts with manual rules. Other rule-based feature extraction methods (Neviarouskaya and Aono, 2013; Li and Xu, 2014; Gao et al., 2015a; Gao et al., 2015b; Yada et al., 2017; Yu et al., 2019) were also proposed to extract contextual features. Other than rule-based methods, Gui et al. (2016) constructed a Chinese event-driven ECE corpus wit"
2020.coling-main.17,D18-1506,0,0.0619436,"19) were also proposed to extract contextual features. Other than rule-based methods, Gui et al. (2016) constructed a Chinese event-driven ECE corpus with SINA city news and proposed a convolution kernel-based multi-kernel Support Vector Machine (SVM) to 199 extract contextual features from given syntactical trees. Afterward, deep learning has attracted attention from the ECE research community. Gui et al. (2017) converted the ECE task to a Question Answering (QA) task and proposed a Convolutional Multiple-Slot Deep Memory Network to store relevant contextual information. Other neural models (Li et al., 2018; Xu et al., 2019) were also proposed to extract contextual information. Besides, Chen et al. (2018) presented a neural network-based joint approach for emotion extraction and emotion cause extraction to capture mutual benefits across these two emotion analysis tasks. Different from ECE in which emotion keywords are provided before the extraction of their causes, the task of emotion-cause pair extraction (ECPE) was first proposed in Xia & Ding (2019), in which emotions and their corresponding causes are extracted at the same time. For this new task, they proposed a two-step approach, which fir"
2020.coling-main.17,I13-1121,0,0.416003,"he causes of given emotion keywords has been intensively studied for years. Most of the previous works focused on contextual information extraction from the context of the given emotion keyword either with manual rules or with machine learning methods. Lee et al. (2010) constructed an emotion cause corpus from Sinica Corpus and then built a rule-based system to extract linguistic features. Based on this corpus, Chen et al. (2010) proposed a multi-label approach with linguistic patterns that can capture linguistic cues in contexts with manual rules. Other rule-based feature extraction methods (Neviarouskaya and Aono, 2013; Li and Xu, 2014; Gao et al., 2015a; Gao et al., 2015b; Yada et al., 2017; Yu et al., 2019) were also proposed to extract contextual features. Other than rule-based methods, Gui et al. (2016) constructed a Chinese event-driven ECE corpus with SINA city news and proposed a convolution kernel-based multi-kernel Support Vector Machine (SVM) to 199 extract contextual features from given syntactical trees. Afterward, deep learning has attracted attention from the ECE research community. Gui et al. (2017) converted the ECE task to a Question Answering (QA) task and proposed a Convolutional Multiple"
2020.coling-main.17,D19-1569,0,0.0276015,"odelling contextual information does not consider dependency relations among local neighborhood candidate pairs. 2.2 Graph Neural Networks The Graph Convolutional Network (GCN) was first proposed in Kipf & Welling (2017) for node classification, which operated directly on a graph. After that, Graph Neural Networks have been widely applied to various NLP tasks, such as relation extraction, aspect-level sentiment analysis, and text classification. Zhang et al. (2018) used GCNs to capture long-range relations among dependency trees and further applied a novel pruning strategy to the input trees. Sun et al. (2019) proposed a GCN for aspect-level sentiment analysis, which propagated both contextual and dependency information from opinion words to aspect words. In addition, Yao et al. (2019) built a text graph based on word co-occurrence and document-word relations and then learned a Text Graph Convolutional Network for text classification. Ghosal et al.(2019) used two layers of GCNs to capture speaker information for emotion recognition in conversations. In this paper, we attempt to use GCNs to model dependency relations in a local neighborhood so as to capture pair-level contextual information for ECPE"
2020.coling-main.17,P19-1096,0,0.193758,"xtraction (ECPE), which was first proposed in Xia & Ding (2019), aims to extract emotion expressions and their corresponding causes in a document simultaneously. Different from emotion cause extraction (ECE) (Lee et al., 2010; Gui et al., 2016) which extracts the causes for given emotion expressions, EPCE is a much more challenging task. There has been a surging interest in developing neural models either for emotion cause extraction or for emotion extraction, while ECPE, as a special causal relation extraction task, is newly proposed and remains largely unexplored. Previous research on ECPE (Xia and Ding, 2019) focused on designing pipeline systems in which emotion clauses and cause clauses are extracted separately, and the two sets of clauses are paired to generate candidate emotion-cause pairs, and then emotion-cause pairs are selected from these candidate pairs with a filter. Hence, prediction errors unavoidably accumulate through the pipeline framework. Therefore, in this work, we aim to design an end-to-end framework in which any two clauses in a document are paired (i.e., one is a candidate emotion clause, and the other is a candidate cause clause) so as to generate candidate emotion-cause pai"
2020.coling-main.17,D18-1244,0,0.0228356,"aired with each cause clause and these candidate pairs were filtered by a logistic regression model. Overall, in the previous works on ECE and ECPE, modelling contextual information does not consider dependency relations among local neighborhood candidate pairs. 2.2 Graph Neural Networks The Graph Convolutional Network (GCN) was first proposed in Kipf & Welling (2017) for node classification, which operated directly on a graph. After that, Graph Neural Networks have been widely applied to various NLP tasks, such as relation extraction, aspect-level sentiment analysis, and text classification. Zhang et al. (2018) used GCNs to capture long-range relations among dependency trees and further applied a novel pruning strategy to the input trees. Sun et al. (2019) proposed a GCN for aspect-level sentiment analysis, which propagated both contextual and dependency information from opinion words to aspect words. In addition, Yao et al. (2019) built a text graph based on word co-occurrence and document-word relations and then learned a Text Graph Convolutional Network for text classification. Ghosal et al.(2019) used two layers of GCNs to capture speaker information for emotion recognition in conversations. In"
2020.coling-main.221,P18-1236,0,0.0859762,"al studies illustrate the importance of proposed sentiment forecasting task, and justify the effectiveness of our NSF model over several strong baselines. 1 Introduction Developing intelligent chatbots is of great appealing to both the industry and the academics. However it is challenging to build up such an intelligent chatbot which involves a series of high-level natural language processing techniques, such as sentiment analysis of utterances in dialog. Previous studies on sentiment classification focus on determining polarity (positive or negative) in a single document (Pang and Lee, 2008; Amplayo et al., 2018). In comparison, only few studies focus on determining polarity of utterances in dialog (Herzig et al., 2016; Majumder et al., 2018). However, all of these studies focus on determining the polarity of existing utterances. It may be more important to predict the polarity of next utterance yet to come. Given the example in Figure 1, although B expresses a positive sentiment in second utterance, A still shows a negative sentiment in his response. In this case, if B know that A would be very upset after his first utterance, he may revise his utterance to let A feel more comfortable. Hence, predict"
2020.coling-main.221,C18-1063,0,0.0192703,"Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sentimental text on the customer’s perception of the service agent. On the basic, Herzig et al. (2016) used SVM to classify sentiment in customer support dialogs by integrating both text based turn and dialog features. Recently, Cerisara et al. (2018) proposed a multi-task hierarchical recurrent network to classify sentiment and dialog act jointly. Majumder et al. (2018) proposed a recurrent neural networks to track of the individual states throughout the dialog and employed this information for sentiment classification. Different from previous studies which focus on detecting the polarity of existing utterances, we propose a novel and important task to forecast polarity of next utterance yet to come. 3 Neural Sentiment Forecasting Modeling As illustrated in Figure 1, given a existed utterance sequence {u1 , u2 , ..., un−1 } in a dialog d,"
2020.coling-main.221,D16-1171,0,0.0190581,"to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sentimental text on the customer’s perception of the service agent. On the basic, Herzig et al. (2016) used SVM to classify sentiment in customer support dialogs by integrating both text based turn and dialog features. Recently,"
2020.coling-main.221,D18-1280,0,0.0169534,"el to learn the representation of ui (1 ≤ i ≤ 3) (Section 3.1), and then employs the representation of ui to forecasting sentiment of next utterance un . • LSTMseq is a sequence based sentiment forecasting model, it employs a LSTM model to learn dialog representation d from the existed utterance sequence {u1 , u2 , u3 } (Eq. 2), and then employ the dialog representation d to forecast sentiment of un . • ICON takes one utterance with previous k utterances as input, and uses a GRU model for modeling inter-personal dependency in previous utterances and stores all history with one memory network (Hazarika et al., 2018). 1 There exist six categories of emotion in the dataset: joy, anger, disgust, fear, sadness, and surprise. Besides, many utterances do not express any emotion (i.e., neutral). 2453 Table 1: Comparison with baselines. Pos F1. Neg F1. Avg F1. LSTM1 0.545 0.285 0.415 2 LSTM 0.506 0.310 0.408 LSTM3 0.558 0.273 0.415 seq LSTM 0.563 0.342 0.453 ICON 0.529 0.293 0.411 DialogRNN 0.540 0.358 0.449 NSF 0.586 0.387 0.486 • DialogRNN employs recurrent neural networks to keep track of the individual states of utterances and uses this information for sentiment classification in dialog (Majumder et al., 201"
2020.coling-main.221,W16-3609,0,0.137295,"our NSF model over several strong baselines. 1 Introduction Developing intelligent chatbots is of great appealing to both the industry and the academics. However it is challenging to build up such an intelligent chatbot which involves a series of high-level natural language processing techniques, such as sentiment analysis of utterances in dialog. Previous studies on sentiment classification focus on determining polarity (positive or negative) in a single document (Pang and Lee, 2008; Amplayo et al., 2018). In comparison, only few studies focus on determining polarity of utterances in dialog (Herzig et al., 2016; Majumder et al., 2018). However, all of these studies focus on determining the polarity of existing utterances. It may be more important to predict the polarity of next utterance yet to come. Given the example in Figure 1, although B expresses a positive sentiment in second utterance, A still shows a negative sentiment in his response. In this case, if B know that A would be very upset after his first utterance, he may revise his utterance to let A feel more comfortable. Hence, predicting the polarity of the next utterance can help a speaker to improve their utterances, which is important in"
2020.coling-main.221,D18-1136,0,0.0213367,": Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sentimental text on the customer’s perception of the service agent. On the basic, Herzig et al. (2016) used SVM to classify sentiment in customer suppor"
2020.coling-main.221,D14-1181,0,0.00759751,"inguistics, pages 2448–2458 Barcelona, Spain (Online), December 8-13, 2020 A: John, I&apos;ve asked you not to smoke in here! I don&apos;t want to see you smoking in my office again. B: I&apos;m sorry, Ms. Fairbanks. I won&apos;t let it happen again. A: Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated tha"
2020.coling-main.221,I17-1099,0,0.0645772,"Missing"
2020.coling-main.221,D13-1170,0,0.0067668,"Spain (Online), December 8-13, 2020 A: John, I&apos;ve asked you not to smoke in here! I don&apos;t want to see you smoking in my office again. B: I&apos;m sorry, Ms. Fairbanks. I won&apos;t let it happen again. A: Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversation"
2020.coling-main.221,P15-1150,0,0.0123949,"oke in here! I don&apos;t want to see you smoking in my office again. B: I&apos;m sorry, Ms. Fairbanks. I won&apos;t let it happen again. A: Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sentimental text on th"
2020.coling-main.221,C16-1311,0,0.0168024,"banks. I won&apos;t let it happen again. A: Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sentimental text on the customer’s perception of the service agent. On the basic, Herzig et al. (2016) used S"
2020.coling-main.221,D16-1169,1,0.852207,"asked you not to smoke in here! I don&apos;t want to see you smoking in my office again. B: I&apos;m sorry, Ms. Fairbanks. I won&apos;t let it happen again. A: Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sen"
2020.coling-main.94,P15-1077,0,0.0302088,"rivastava and Sutton (2017) propose the neural topic models (NTM) to mine the topic information inside texts. Gui et al. (2019a) propose a reinforcement learning based neural topic model to alleviate the limitations of traditional topic coherence measures. Wang et al. (2020) also propose a topicaware multi-task learning model to learn topic-enriched utterance representations in customer service, which is inspirational to our topic-enriched auxiliary learning framework. Unlike the above studies modeling topics under the assumption that the topic-word distribution is a multinomial distribution, Das et al. (2015) model topics with multivariate gaussian distribution over the word embedding space to deal with the new word issue, which is inspirational to our proposed modality-agnostic topic model. However, all the prior topic models rely on word vocabulary and thus are specially-designed for text modality, which cannot be directly adopted to capture the topic information inside images. Different from all the above studies, this paper proposes a new modality-agnostic topic model to mine the global topics from either the discrete textual signals or the continuous visual signals. On this basis, a MTAL appr"
2020.coling-main.94,N19-1423,0,0.0535922,"text and image along the timeline (see Figure 1) posted by a user, the primary task aims at modeling both the text sequence and the corresponding image sequence to perform depression prediction for this user. Figure 2 shows the illustration of the primary task. First of all, given n pairs 1080 Convolutions … ෝ ൌ ࣆ  ܃ ࣕ  ڄሺ܃ሻ) ा Pooling ࣆሺ܃ሻ ܐ (ሺ܃ሻ) Inference Network ࣂ … ܃ Input ۱ ܃ Generative Network Figure 3: The framework of the proposed modality-agnostic topic model. of text and image, each text and each image are encoded with a shared (i.e., parameter sharing) BERT (Devlin et al., 2019) model and a shared VGG (Simonyan and Zisserman, 2015) model respectively. Text Encoder. As a pre-trained text encoding mechanism, BERT can be ﬁne-tuned to create state-ofthe-art models for a range of NLP tasks, e.g., text classiﬁcation and natural language inference. In our approach, we use BERT-Base (uncased) model as the shared text encoder. Given the t-th text xtext = t text , ..., xtext } of each user, we adopt BERT to encode this text and use the mark “[CLS]” repre, x {xtext t1 t2 tn ˆ t ∈ R768 to compute the text vector htext ˆ h ). Here, ˆt + b sentation h ∈ Rd of xtext as htext = tanh"
2020.coling-main.94,W16-0307,0,0.0284064,"and image modalities) like Gui et al. (2019b) are much less and limited to neglect the topic information inside multiple modalities. In the following, we will ﬁrst review the depression detection task and then introduce the related studies on neural topic models. Depression Detection. The ubiquity of social media poses a great opportunity to perform depression detection. Prior studies mainly focus on identifying depressed persons by analyzing the generated textual information in social media. Speciﬁcally, Choudhury et al. (2013) focus on the differences in word usage for depression detection. Gkotsis et al. (2016) focus on the depth of syntax-parsing trees for depression detection. In recent years, researchers begin to use multimodal information (e.g., the text, speech and image) for depression detection. Speciﬁcally, Yin et al. (2019) propose a hierarchical RNN network to extract the features from the vision, speech and text for depression detection. Gui et al. (2019b) propose a reinforced GRU network to capture both the textual and visual information for depression detection. In addition, it is worthwhile to mention that, Resnik et al. (2015) and Shen et al. (2017) also investigate 1079 Text and Imag"
2020.coling-main.94,C18-1201,0,0.14871,"n shown in Eq.(4). For clarity, the losses for two auxiliary tasks, i.e., the textual topic modeling and visual topic modeling, are denoted as Ltext and Limage respectively. Finally, the joint loss L is deﬁned as follows: L = Lprimary + λ(Ltext + Limage ) (8) where λ is a weight and ﬁne-tuned to be 0.25 for balancing the losses for primary and auxiliary tasks. 1083 Approach H-LSTM (Wang et al., 2018) BERT + LSTM BERT + LSTM + Textual Topic Modeling VGG + LSTM VGG + LSTM + Visual Topic Modeling EF-LSTM (Zadeh et al., 2018) CoMemory (Xu et al., 2018) CoATT (Zhang et al., 2018) Hybrid Attention (Gu et al., 2018) CoMMA (Gui et al., 2019b) Primary Task MTAL Modality Text Image Text + Image Precision (P) 77.2 79.2 81.1 62.3 66.8 79.9 80.6 79.6 80.6 78.3 81.4 84.2 Recall (R) 77.1 79.2 81.2 61.7 66.7 79.9 80.4 80.3 80.6 79.4 80.9 84.2 F1 77.1 79.2 81.1 62.0 66.7 79.9 80.5 79.9 80.6 78.8 81.1 84.2 Acc. 77.1 79.2 82.3 61.7 66.7 79.9 80.4 80.4 80.6 79.2 80.9 84.2 Table 2: Performance comparison of various kinds of approaches with the single-modality (text or image) and the multimodality (text and image) for depression detection. 4 Experimentation To validate the effectiveness of our approach, we evaluate the"
2020.coling-main.94,D19-1350,0,0.291954,"demonstrates that the proposed MTAL approach can signiﬁcantly outperform several state-of-the-art baselines, including the representative textual depression detection approaches and the state-of-the-art multimodal-based approaches. 2 Related Work Depression detection is an interdisciplinary research task and has been drawing ever-more attention in NLP with a focus on extracting various types of features from text modality (Choudhury et al., 2013; Nambisan et al., 2015). Compared with the studies on the text modality, the studies on multimodality (e.g., both the text and image modalities) like Gui et al. (2019b) are much less and limited to neglect the topic information inside multiple modalities. In the following, we will ﬁrst review the depression detection task and then introduce the related studies on neural topic models. Depression Detection. The ubiquity of social media poses a great opportunity to perform depression detection. Prior studies mainly focus on identifying depressed persons by analyzing the generated textual information in social media. Speciﬁcally, Choudhury et al. (2013) focus on the differences in word usage for depression detection. Gkotsis et al. (2016) focus on the depth of"
2020.coling-main.94,W15-1212,0,0.0289221,"n the differences in word usage for depression detection. Gkotsis et al. (2016) focus on the depth of syntax-parsing trees for depression detection. In recent years, researchers begin to use multimodal information (e.g., the text, speech and image) for depression detection. Speciﬁcally, Yin et al. (2019) propose a hierarchical RNN network to extract the features from the vision, speech and text for depression detection. Gui et al. (2019b) propose a reinforced GRU network to capture both the textual and visual information for depression detection. In addition, it is worthwhile to mention that, Resnik et al. (2015) and Shen et al. (2017) also investigate 1079 Text and Image Encoding … ܂௧భ ܂௧ሾಽೄሿ Primary Task ܂௧ሾೄಶುሿ ܂௧ trm …Shared BERT trm trm … trm … ۳௧భ ۳௧ሾೄಶುሿ ۳௧ … [CLS] ܐ௧௫௧ ଶ ܐଷ LSTM LSTM LSTM … ܐ௧௫௧ ௧ … ۳௧ሾಽೄሿ ܐଶ ܐଵ௧௫௧ … trm trm ܐଵ LSTM Depressed? Yes or No … ܞଵ  ܃௧௫௧ ܐ [SEP] ܞଶ ܞଷ ܞ୬ ܘఏ =ሺ܅   ܊ ሻ Modality Fusion The t-th text ܚ 2ʹ4ൈ ʹ24 2 1ͳʹ ൈ ͳͳʹ ܙ௧௫௧ Shared VGG Modality-Agnostic Topic Model 5 ൈ ͷ6 2ͺ ൈ ʹ8 ܐ௧ ConvNet 1Ͷ ൈ ͳ4 Pool  ܐଵ 7ൈ  FC D=512 The t-th image D=4096  ܐଶ Textual Topic Modelling ܙ Modali"
2020.emnlp-main.291,P17-1067,0,0.0252471,"multi-label emotion detection. The detailed evaluation demonstrates the effectiveness of our approach. 1 Visual Modality Acoustic Modality Emotions Emotion detection is to predict emotion categories, such as angry, happy, and surprise, expressed by an utterance of a speaker and has largely encompassed a variety of applications, such as online chatting (Galik and Rank, 2012; Zhang et al., c), news analysis (Li et al., 2015; Zhu et al., 2019) and dialogue systems (Ghosal et al., 2019; Zhang et al., d). Over the last few years, there has been a substantial body of research on emotion detection (Abdul-Mageed and Ungar, 2017; Zhou et al., 2019; Zhang et al., a), where a considerable amount of work has focused on multi-label emotion detection (Li et al., 2015; Yu et al., 2018; Ying et al., 2019). Basically, emotion detection is a multi-label classiﬁcation problem since one utterance naturally tends to involve more than one emotion category. However, classifying instances with multiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video seg"
2020.emnlp-main.291,P19-2045,0,0.0493231,"Missing"
2020.emnlp-main.291,D19-1566,0,0.0224143,"motion detection rely on special knowledge of emotion, such as context information (Li et al., 2015), cross-domain transferring (Yu et al., 2018) and external resource (Ying et al., 2019). In fact, when there is no special knowledge (Kim et al., 2018), it can be normally handled by multi-label text classiﬁcation approaches. In the multi-modal community, related studies normally focus on single-label emotion task and the studies for multi-label emotion task are much less and limited to be transformed to multiple binary classiﬁcation (Zadeh et al., 2018b; Wang et al., 2019; Akhtar et al., 2019; Chauhan et al., 2019). In the following, we give an overview of multi-label emotion/text classiﬁcation and multi-modal emotion detection. Multi-label Emotion/Text Classiﬁcation. Recent studies normally cast multi-label emotion detection task as a classiﬁcation problem and leverage the special knowledge as auxiliary information (Yu et al., 2018; Ying et al., 2019). These approaches may not be easily extended to those tasks without external knowledge. At this time, the multi-label text classiﬁcation approaches can be quickly applied to emotion detection. There have been a large number of representative studies for t"
2020.emnlp-main.291,S18-1019,0,0.0211402,"g, we give an overview of multi-label emotion/text classiﬁcation and multi-modal emotion detection. Multi-label Emotion/Text Classiﬁcation. Recent studies normally cast multi-label emotion detection task as a classiﬁcation problem and leverage the special knowledge as auxiliary information (Yu et al., 2018; Ying et al., 2019). These approaches may not be easily extended to those tasks without external knowledge. At this time, the multi-label text classiﬁcation approaches can be quickly applied to emotion detection. There have been a large number of representative studies for that. Kant et al. (2018) leverage the pre-trained BERT to perform multi-label emotion task and Kim et al. (2018) propose an attention-based classiﬁer that predicts multiple emotions of a given sentence. More recently, Yang et al. (2018) propose a sequence generation model and Yang et al. (2019) leverage a reinforced approach to ﬁnd a better sequence than a baseline sequence, but it still relies on the pretrained seq2seq model with a pre-deﬁned order of ground-truth. Different from above studies, we focus on multilabel emotion detection in a multi-modal scenario by considering the modality dependence besides the label"
2020.emnlp-main.291,P15-1101,1,0.814811,"Missing"
2020.emnlp-main.291,D14-1162,0,0.0827516,"Missing"
2020.emnlp-main.291,N19-1321,0,0.0449293,"Missing"
2020.emnlp-main.291,P19-1656,0,0.174944,"ultilabel emotion detection in a multi-modal scenario by considering the modality dependence besides the label dependence. To the best of our knowledge, this is the ﬁrst attempt to perform multi-label emotion detection in a multi-modal scenario. Multi-modal Emotion Detection. Recent studies on multi-modal emotion detection largely depend on multi-modal fusion framework to perform binary classiﬁcation within each emotion category. Recently, Wang et al. (2019) introduce a recurrent attended variation embedding network for multimodal language analysis with non-verbal shifted word representation. Tsai et al. (2019) employ the Transformer-based architecture to capture the long-range interactions inside and across different modalities. However, they still cast the multi-label emotion detection as multiple binary classiﬁcation problems. Different from above studies, we focus on multimodal emotion detection in a multi-label scenario by considering the label dependence besides the modality dependence. To the best of our knowledge, this is the ﬁrst attempt to perform multimodal emotion detection in a multi-label scenario. 3585 3 Output Probabilities Data Pre-processing Softmax We extract low-level handcrafted"
2020.emnlp-main.291,P18-1208,0,0.0263778,"Missing"
2020.emnlp-main.291,D19-1444,0,0.0275047,"parameters of β1 and β2 , 0.9 and 0.999 respectively. The beam size K is set to be 5 at both training and inference stages. To motivate future research, the code will be released via github 3 . Evaluation Metrics and Signiﬁcance Test. In our study, we employ three evaluation metrics to measure the performances of different approaches to multi-modal multi-label emotion detection, i.e., multi-label Accuracy (Acc), Hamming Loss (HL) and micro F1 measure (F1 ). These metrics have been popularly used in some multi-label classiﬁcation problems (Li et al., 2015; Yang et al., 2019; Aly et al., 2019; Wu et al., 2019). Note that smaller Hamming Loss corresponds to better classiﬁcation quality, while larger Accuracy 2 3 For a thorough comparison, we implement various baseline approaches in three groups: Multi-label Classiﬁcation Approaches. In this group, the baselines use different approaches to deal with the multi-label issue without considering the modality dependence issue. Speciﬁcally, in these approaches, the multi-modal inputs are early fused (simply concatenated) as a new input. (1) BR5 (Shen et al., 2004), which transforms the multi-label task into multiple single-label binary classiﬁcation problem"
2020.emnlp-main.291,D19-1044,0,0.0370485,"ultiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video segment. Introduction ∗ ĊĊ the hardest thing that we face is ĊĊ main challenge is how to model the label dependence in the classiﬁcation approach. For example, in the utterance as shown in Figure 1, both the Sad and Disgust emotions are more likely to exist, rather than the conﬂicting emotions of Sad and Happy. Recent studies, such as (Yang et al., 2019) and (Xiao et al., 2019), have begun to address this challenge. However, almost all existing studies in multilabel emotion detection focus on one modality (e.g., textual modality). Only very recently, the research community has become increasingly aware of the need on multi-modal emotion detection (Zadeh et al., 2018b) due to its wide potential applications, e.g., with the massively growing importance of analyzing conversations in speech (Gu et al., 2019) and video (Majumder et al., 2019). In this study, we aim to tackle multi-modal multi-label emotion detection. Compared with single modality, multimodal multi-label"
2020.emnlp-main.291,P19-1518,0,0.255733,"sifying instances with multiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video segment. Introduction ∗ ĊĊ the hardest thing that we face is ĊĊ main challenge is how to model the label dependence in the classiﬁcation approach. For example, in the utterance as shown in Figure 1, both the Sad and Disgust emotions are more likely to exist, rather than the conﬂicting emotions of Sad and Happy. Recent studies, such as (Yang et al., 2019) and (Xiao et al., 2019), have begun to address this challenge. However, almost all existing studies in multilabel emotion detection focus on one modality (e.g., textual modality). Only very recently, the research community has become increasingly aware of the need on multi-modal emotion detection (Zadeh et al., 2018b) due to its wide potential applications, e.g., with the massively growing importance of analyzing conversations in speech (Gu et al., 2019) and video (Majumder et al., 2019). In this study, we aim to tackle multi-modal multi-label emotion detection. Compared with single modality,"
2020.emnlp-main.291,C18-1330,0,0.116609,"ask as a classiﬁcation problem and leverage the special knowledge as auxiliary information (Yu et al., 2018; Ying et al., 2019). These approaches may not be easily extended to those tasks without external knowledge. At this time, the multi-label text classiﬁcation approaches can be quickly applied to emotion detection. There have been a large number of representative studies for that. Kant et al. (2018) leverage the pre-trained BERT to perform multi-label emotion task and Kim et al. (2018) propose an attention-based classiﬁer that predicts multiple emotions of a given sentence. More recently, Yang et al. (2018) propose a sequence generation model and Yang et al. (2019) leverage a reinforced approach to ﬁnd a better sequence than a baseline sequence, but it still relies on the pretrained seq2seq model with a pre-deﬁned order of ground-truth. Different from above studies, we focus on multilabel emotion detection in a multi-modal scenario by considering the modality dependence besides the label dependence. To the best of our knowledge, this is the ﬁrst attempt to perform multi-label emotion detection in a multi-modal scenario. Multi-modal Emotion Detection. Recent studies on multi-modal emotion detecti"
2020.emnlp-main.291,D19-1552,1,0.831171,". The detailed evaluation demonstrates the effectiveness of our approach. 1 Visual Modality Acoustic Modality Emotions Emotion detection is to predict emotion categories, such as angry, happy, and surprise, expressed by an utterance of a speaker and has largely encompassed a variety of applications, such as online chatting (Galik and Rank, 2012; Zhang et al., c), news analysis (Li et al., 2015; Zhu et al., 2019) and dialogue systems (Ghosal et al., 2019; Zhang et al., d). Over the last few years, there has been a substantial body of research on emotion detection (Abdul-Mageed and Ungar, 2017; Zhou et al., 2019; Zhang et al., a), where a considerable amount of work has focused on multi-label emotion detection (Li et al., 2015; Yu et al., 2018; Ying et al., 2019). Basically, emotion detection is a multi-label classiﬁcation problem since one utterance naturally tends to involve more than one emotion category. However, classifying instances with multiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video segment. Introduction"
2020.emnlp-main.291,P19-1045,1,0.763654,"and different modalities (modality dependence). Particularly, we propose a multi-modal sequence-to-set approach to effectively model both kinds of dependence in multi-modal multi-label emotion detection. The detailed evaluation demonstrates the effectiveness of our approach. 1 Visual Modality Acoustic Modality Emotions Emotion detection is to predict emotion categories, such as angry, happy, and surprise, expressed by an utterance of a speaker and has largely encompassed a variety of applications, such as online chatting (Galik and Rank, 2012; Zhang et al., c), news analysis (Li et al., 2015; Zhu et al., 2019) and dialogue systems (Ghosal et al., 2019; Zhang et al., d). Over the last few years, there has been a substantial body of research on emotion detection (Abdul-Mageed and Ungar, 2017; Zhou et al., 2019; Zhang et al., a), where a considerable amount of work has focused on multi-label emotion detection (Li et al., 2015; Yu et al., 2018; Ying et al., 2019). Basically, emotion detection is a multi-label classiﬁcation problem since one utterance naturally tends to involve more than one emotion category. However, classifying instances with multiple possible categories is sometimes much more difﬁcul"
2020.emnlp-main.291,D19-5541,0,0.142045,"otion categories, such as angry, happy, and surprise, expressed by an utterance of a speaker and has largely encompassed a variety of applications, such as online chatting (Galik and Rank, 2012; Zhang et al., c), news analysis (Li et al., 2015; Zhu et al., 2019) and dialogue systems (Ghosal et al., 2019; Zhang et al., d). Over the last few years, there has been a substantial body of research on emotion detection (Abdul-Mageed and Ungar, 2017; Zhou et al., 2019; Zhang et al., a), where a considerable amount of work has focused on multi-label emotion detection (Li et al., 2015; Yu et al., 2018; Ying et al., 2019). Basically, emotion detection is a multi-label classiﬁcation problem since one utterance naturally tends to involve more than one emotion category. However, classifying instances with multiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video segment. Introduction ∗ ĊĊ the hardest thing that we face is ĊĊ main challenge is how to model the label dependence in the classiﬁcation approach. For example, in the utterance"
2020.emnlp-main.291,D18-1137,0,0.0783276,"is to predict emotion categories, such as angry, happy, and surprise, expressed by an utterance of a speaker and has largely encompassed a variety of applications, such as online chatting (Galik and Rank, 2012; Zhang et al., c), news analysis (Li et al., 2015; Zhu et al., 2019) and dialogue systems (Ghosal et al., 2019; Zhang et al., d). Over the last few years, there has been a substantial body of research on emotion detection (Abdul-Mageed and Ungar, 2017; Zhou et al., 2019; Zhang et al., a), where a considerable amount of work has focused on multi-label emotion detection (Li et al., 2015; Yu et al., 2018; Ying et al., 2019). Basically, emotion detection is a multi-label classiﬁcation problem since one utterance naturally tends to involve more than one emotion category. However, classifying instances with multiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video segment. Introduction ∗ ĊĊ the hardest thing that we face is ĊĊ main challenge is how to model the label dependence in the classiﬁcation approach. For examp"
2020.findings-emnlp.179,K18-1048,0,0.0258478,"echanism to locate the most relevant historical customer utterances and seller utterances, and then produce their context representations. • We use a gated strategy to generate the final response by comprehensively considering the different importance of current dialogue and historical dialogues under a hybrid network. • Empirical results show that our proposed approach outperforms state-of-the-art competitors significantly on a real-world multi-turn customer service dialogue dataset with both automatic and manual evaluation. 2 Related Work Previous research on multi-turn dialogue generation (Chaudhuri et al., 2018; Zhou et al., 2018; Olabiyi et al., 2018) has drawn a huge amount of attention from academia and industry, which has broader usage scenario than single-turn dialogue generation (Zhang et al., 2018; Li et al., 2017). Serban et al. (2016); Chen et al. (2018); Wu et al. (2016) proposed a hierarchical encoderdecoder framework to model all the context utterances which can better grasp the overall information of the dialogues. However, these models are difficult to generalize, and their results are unsatisfied since responses maybe vary a lot for the same question towards different occasions and sp"
2020.findings-emnlp.179,D17-1230,0,0.0279764,"ering the different importance of current dialogue and historical dialogues under a hybrid network. • Empirical results show that our proposed approach outperforms state-of-the-art competitors significantly on a real-world multi-turn customer service dialogue dataset with both automatic and manual evaluation. 2 Related Work Previous research on multi-turn dialogue generation (Chaudhuri et al., 2018; Zhou et al., 2018; Olabiyi et al., 2018) has drawn a huge amount of attention from academia and industry, which has broader usage scenario than single-turn dialogue generation (Zhang et al., 2018; Li et al., 2017). Serban et al. (2016); Chen et al. (2018); Wu et al. (2016) proposed a hierarchical encoderdecoder framework to model all the context utterances which can better grasp the overall information of the dialogues. However, these models are difficult to generalize, and their results are unsatisfied since responses maybe vary a lot for the same question towards different occasions and speakers. Recent studies have noticed the problem and try to alleviate it by incorporating helpful external information into response generation, e.g., speakers’ emotional information. (Zhang et al., 2019a,b; Wang et"
2020.findings-emnlp.179,P02-1040,0,0.114129,"Missing"
2020.findings-emnlp.179,P19-1362,0,0.0962756,"et al., 2018; Li et al., 2017). Serban et al. (2016); Chen et al. (2018); Wu et al. (2016) proposed a hierarchical encoderdecoder framework to model all the context utterances which can better grasp the overall information of the dialogues. However, these models are difficult to generalize, and their results are unsatisfied since responses maybe vary a lot for the same question towards different occasions and speakers. Recent studies have noticed the problem and try to alleviate it by incorporating helpful external information into response generation, e.g., speakers’ emotional information. (Zhang et al., 2019a,b; Wang et al., 2020). Zhao et al. (2019) proposed a review response generation model in the E-commerce platform, which used the reinforcement learning and copy mechanism to fuse external product information, thereby generating informative and diverse responses. Zheng et al. (2019) proposed a dialogue generation model considering personality traits such as age, name, and gender. Meng et al. (2019) proposed RefNet, which used background descriptions about the target dialogue and used a copy mechanism to copy tokens or semantic units. However, all these models are difficult to generalize in re"
2020.findings-emnlp.179,P18-1103,0,0.0115415,"ost relevant historical customer utterances and seller utterances, and then produce their context representations. • We use a gated strategy to generate the final response by comprehensively considering the different importance of current dialogue and historical dialogues under a hybrid network. • Empirical results show that our proposed approach outperforms state-of-the-art competitors significantly on a real-world multi-turn customer service dialogue dataset with both automatic and manual evaluation. 2 Related Work Previous research on multi-turn dialogue generation (Chaudhuri et al., 2018; Zhou et al., 2018; Olabiyi et al., 2018) has drawn a huge amount of attention from academia and industry, which has broader usage scenario than single-turn dialogue generation (Zhang et al., 2018; Li et al., 2017). Serban et al. (2016); Chen et al. (2018); Wu et al. (2016) proposed a hierarchical encoderdecoder framework to model all the context utterances which can better grasp the overall information of the dialogues. However, these models are difficult to generalize, and their results are unsatisfied since responses maybe vary a lot for the same question towards different occasions and speakers. Recent stud"
2021.acl-short.70,2020.emnlp-main.457,0,0.023036,"ng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, which can deal with multi-modal scenarios and offers an alternative solution to robustly enhan"
2021.acl-short.70,D18-1529,0,0.0242889,"Missing"
2021.acl-short.70,I13-1181,0,0.0120815,"xperimental results demonstrate that our approach performs signiﬁcantly better than the single-modal state-of-the-art and the multi-modal approaches with early fused features of CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies l"
2021.acl-short.70,2020.coling-main.183,0,0.021417,"Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, which can deal with multi-modal scenarios and offers an alternative solution to robustly enhancing neural CWS models. 3 Data Collection and Annotation We collect the multi-modal data for CWS from a Chinese news reporting platform “Xuexi”2 . We mainly focus on the audios equipped with machine transcription text. In total, we crawl 120 short videos and segment them into about 2000 sentences. To avoid the contextual inﬂuence and augment the robust of designed computing model, we randomly 2 https://www.xuexi.cn/ Size 250 50.5"
2021.acl-short.70,P14-1028,0,0.0182098,"CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020"
2021.acl-short.70,N19-1276,0,0.01271,"r CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, which can deal with multi-modal scenarios and offers an alternative solution to robustly enhancing neural CWS models. 3 Data Collection and Annotation We collect the multi-modal data for CWS from a Chinese news reporting platform “Xuexi”2 . We mainly focus on the audios equipped with machine transcription text. In total, we crawl 120 short videos and segment them into about 2000 sentences. To avoid the contextual inﬂuence and augment the robust of designed computing model, we randomly 2 https://www.xue"
2021.acl-short.70,C04-1081,0,0.235078,"aracters Total Words Total Time(s) Finally, we leverage the CRF to perform sequence labeling on the basis of the above character representation. We evaluate our approach on the newly annotated small-scale dataset with different size of training sets. The experimental results demonstrate that our approach performs signiﬁcantly better than the single-modal state-of-the-art and the multi-modal approaches with early fused features of CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020;"
2021.acl-short.70,2020.findings-emnlp.260,0,0.0593949,"Missing"
2021.acl-short.70,D11-1090,0,0.0337871,"aining sets. The experimental results demonstrate that our approach performs signiﬁcantly better than the single-modal state-of-the-art and the multi-modal approaches with early fused features of CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models,"
2021.acl-short.70,2020.acl-main.735,0,0.048718,"Missing"
2021.acl-short.70,2020.acl-main.734,0,0.0661243,"ard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, which can deal with multi-modal scenarios and offers an alternative solution to robustly enhancing neural CWS models. 3 Data Collect"
2021.acl-short.70,P19-1656,0,0.0280817,"timedependent uni-modal interaction, time-dependent multi-modal interaction and CRF labeling. Figure 2 shows the overall architecture of our TMIN. 4.1 Time-dependent Uni-modal Interaction To better capture the temporal correspondences between different modalities (Zhang et al., 2019; Ju et al., 2020), we ﬁrst align two modalities by extracting the exact time stamp of each phoneme and character using Montreal Forced Aligner (McAuliffe et al., 2017). For machines to understand human utterance, they must be ﬁrst able to understand the intramodal dynamics (Zadeh et al., 2018; Wang et al., 2019b; Tsai et al., 2019) in each modality, such as the word order and grammar in text, breathe and 551 Multi-attention Gating B h I  αK ĊĊ CRF Layer  α2 CRF Labeling  α1 ^ha O ĊĊ Multi-modal Hybrid Representation h1 ^hx LSTHMG Gating LSTM h1a h2 Gating LSTM h1 x ĊĊ Gating LSTM LSTM LSTM hn ĊĊ Time-depedent Multi-modal Interaction LSTM ĊĊ Transformerbased Unit Transformerbased Unit Transformerbased Unit Transformerbased Unit ᗵ 享 i=1 Transformerbased Unit ĊĊ ĊĊ Transformerbased Unit Time-dependent Uni-modal Text Interaction ભ ĊĊ i=2 Audio i=n ĊĊ Figure 2: The overview of our proposed TMIN. tone in audio. Textual Mod"
2021.acl-short.70,I17-1017,0,0.0125628,"ture engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, which can deal with multi-modal scenarios and offers an alternative solution to robustly enhancing neural CWS models. 3 Data Collection and Annotation We collect the multi-modal data for CWS from a Chinese news reporting platform “Xuexi”2 . We mainly focus on the audios equipped with machine transcription text. In total, we crawl 120 short videos and segment them into about 2000 sentences. To avoid the contextual inﬂuence and augment the robust of designed computing model, we"
2021.acl-short.70,O03-4002,0,0.261908,"Sentences Avg. Length (Character) Avg. Length (Word) Avg. Length (Time)(s) Max Length (Character) Max Length (Word) Max Length (Time)(s) Total Characters Total Words Total Time(s) Finally, we leverage the CRF to perform sequence labeling on the basis of the above character representation. We evaluate our approach on the newly annotated small-scale dataset with different size of training sets. The experimental results demonstrate that our approach performs signiﬁcantly better than the single-modal state-of-the-art and the multi-modal approaches with early fused features of CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al.,"
2021.acl-short.70,P17-1078,0,0.0125572,"k, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, whi"
2021.acl-short.70,2020.emnlp-main.291,1,0.714341,"be represented as: X = (x1 , x2 , · · · , xn ) ∈ Rn×d1 . Acoustic Modality. We use a famous audio processing tool, i.e., OpenSMILE (Eyben et al., 2010), to extract the MFCC, LP-coefﬁcients, pure FFT spectrum, etc. from dual channels (Jayram et al., 2002; Sakran et al., 2017), and leverage multiple Transformer layers (Vaswani et al., 2017) to perform intra-modal interactions. Then, each character-level audio feature can be represented as: A = (a1 , a2 , · · · , an ) ∈ Rn×d2 . 4.2 Time-dependent Multi-modal Interaction To better capture the cross-modal semantic correspondences (Wu et al., 2020; Zhang et al., 2020), we design a long- and short-term hybrid memory gating (LSTHMG) block, which is a extension of standard LSTM. We ﬁrst obtain the current memory of each character-level representation for both modalities. ˆ x , cx = LSTMx (xi , hx , cx ) h i i i i−1 i−1 a a a ˆ , c = LSTM (ai , ha , ca ) h i i i i−1 i−1 (1) (2) where LSTM denotes the standard LSTM (Graves et al., 2013). After current updating, we employ multiattention to control the different contributions of each hidden state. ˆ i + MA(h ˆ x, h ˆ a) hi = h i i ˆi + = h L  l=0 (softmax( (3) Ql (K l ) √ d )V l ) (4) where MA denotes the multi"
2021.acl-short.70,D13-1031,0,0.0153164,"emonstrate that our approach performs signiﬁcantly better than the single-modal state-of-the-art and the multi-modal approaches with early fused features of CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external info"
2021.acl-short.70,D13-1061,0,0.0394832,"Missing"
2021.acl-short.70,D17-1079,0,0.0120733,"quence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with"
2021.acl-short.70,P19-2029,0,0.0283995,"Missing"
2021.emnlp-main.360,2020.coling-main.24,0,0.481817,"extraction besides the corresponding sentiment classification in a multi-modal scenario. Note that we also propose a multi-modal joint learning approach to improve the performance of both MATE and MASC. Text-based Joint Aspect Terms Extraction and Sentiment Classification. Some studies (Zhang et al., 2020a) have attempted to solve both sub-tasks in a more integrated way, by jointly extracting aspect terms and predicting their sentiment polarities. The most recent and representative are a span-based extract-then-classify approach (Hu et al., 2019) and a directed GCN with syntactic information (Chen et al., 2020). However, all the above studies can not model the visual guidance for both sub-tasks. Different from them, we propose a multi-modal joint framework to handle both MATE and MASC. 3 Joint Multi-modal Aspect-Sentiment Analysis 3.1 Cross-modal Relation Detection Unlike traditional approaches, which take visual information into consideration completely and ignore whether image can bring benefits to text, we incorporate the image-text relation into the model and only retain the auxiliary visual information towards the text. Therefore, we build a relation module by pre-training to properly exploit v"
2021.emnlp-main.360,P19-1052,0,0.0263826,"ent Classification (MASC). Different from text-based aspect sentiment classification (Sundararaman et al., 2020; Ji et al., 2020; Liang et al., 2020b,a), it is challenging to effectively fuse the textual and visual information. As a pioneer, Xu et al. (2019) collect a benchmark Chinese dataset from a digital product review platform for multi-modal aspect-level sentiment analysis and propose a multi-interactive memory network to iteratively fuse the textual and visual representations. In the past five years, text-based aspect-level sentiment analysis has drawn much attention (Luo et al., 2019; Chen and Qian, 2019; Zhang and Qian, Recently, Yu and Jiang (2019) annotate two 2020; Zheng et al., 2020; Tulkens and van Cranen- datasets in Twitter for multi-modal target-oriented burgh, 2020; Akhtar et al., 2020). While, multi- (aka aspect-level) sentiment classification and levermodal target-oriented sentiment analysis has be- age BERT as backbone to effectively combine both come more and more vital because of its urgent textual and visual modalities. In the same period, 4396 Yu et al. (2020a) propose a target-sensitive attention and fusion network to address both text-based and multi-modal target-oriented s"
2021.emnlp-main.360,2020.emnlp-main.164,0,0.118715,"(a) RT @ funnytwittingg : [OBAMA]Neg TO [ISRAEL]Neu ? [OBAMA]Neg TO [UKRAINE] ? [OBAMA]Neg TO [USA]Neu ? (b) Figure 1: Two examples for joint multi-modal aspectsentiment analysis. aspect terms from a free text with its accompanying image (Wu et al., 2020a). Second, MASC aims to classify the sentiment polarity of a multi-modal post towards a given aspect in textual modality (Yu and Jiang, 2019). To better satisfy the practical applications, the aspect term-polarity co-extraction, which solves ATE and ASC simultaneously, receives much attention recently in a textual scenario (Wan et al., 2020; Chen and Qian, 2020b; Ying et al., 2020). However, to our best knowledge, in the multi-modal scenario, the joint MATE and MASC, i.e., joint multi-modal aspect-sentiment analysis (JMASA), have never been investigated so far. For this joint multi-modal task, we believe that there exist the following challenges at least. 1 Introduction On the one hand, visual modality may provide no clues for one of sub-tasks. For example, in FigMulti-modal aspect-level (aka target-oriented) senure 1(a), since the image shows most of the content timent analysis (MALSA) is an important and finedescribed in the text, and we can’t inf"
2021.emnlp-main.360,2020.acl-main.340,0,0.174693,"(a) RT @ funnytwittingg : [OBAMA]Neg TO [ISRAEL]Neu ? [OBAMA]Neg TO [UKRAINE] ? [OBAMA]Neg TO [USA]Neu ? (b) Figure 1: Two examples for joint multi-modal aspectsentiment analysis. aspect terms from a free text with its accompanying image (Wu et al., 2020a). Second, MASC aims to classify the sentiment polarity of a multi-modal post towards a given aspect in textual modality (Yu and Jiang, 2019). To better satisfy the practical applications, the aspect term-polarity co-extraction, which solves ATE and ASC simultaneously, receives much attention recently in a textual scenario (Wan et al., 2020; Chen and Qian, 2020b; Ying et al., 2020). However, to our best knowledge, in the multi-modal scenario, the joint MATE and MASC, i.e., joint multi-modal aspect-sentiment analysis (JMASA), have never been investigated so far. For this joint multi-modal task, we believe that there exist the following challenges at least. 1 Introduction On the one hand, visual modality may provide no clues for one of sub-tasks. For example, in FigMulti-modal aspect-level (aka target-oriented) senure 1(a), since the image shows most of the content timent analysis (MALSA) is an important and finedescribed in the text, and we can’t inf"
2021.emnlp-main.360,N19-1423,0,0.135672,"k and a softn=1 be the set of data samples. Given a word sequence X = {x1 , x2 , · · · , xk } max activation function as follows: with length k and an image I, the joint task is to extract a aspect terms list A = {a1 , a2 , · · · , am } pr = softmax(W2 tanh(W1 H)) (5) and classify the aspect sentiment list S = {s1 , s2 , · · · , sm } simultaneously, where m denotes where W1 ∈ R4∗dm ×dm and W2 ∈ Rdm ×2 are the number of aspects. Note that the word embed- two trainable parameter matrices. H means the dings are obtained by pre-processing via BERT concatenation of Ho , Hx , Ho→x and Hx→o . Since (Devlin et al., 2019) due to its excellent ability the relation score can also be binary: 0 or 1, we of textual representation, meanwhile the image re- calculated by equation similarly to equation 5,but gion embeddings are obtained by pre-processing score pr < 0.5 = 0, p < 0.5. Then we try both via ResNet (He et al., 2016) due to its excellent soft and hard relations to guide our multi-modal ability of visual representation. joint tasks. 4397 BERT Embedding Crossmodal Attenion ... Multi-aspect Extraction H ak ... T3  T2  T1  H a3 H o Tk Trm Trm ... Trm Trm Trm ... Trm E1 E2 ... Ek H u1  H s2  u1,1 H s1  u1,"
2021.emnlp-main.360,P19-1051,0,0.30161,"Different from them, we propose to jointly perform aspect terms extraction besides the corresponding sentiment classification in a multi-modal scenario. Note that we also propose a multi-modal joint learning approach to improve the performance of both MATE and MASC. Text-based Joint Aspect Terms Extraction and Sentiment Classification. Some studies (Zhang et al., 2020a) have attempted to solve both sub-tasks in a more integrated way, by jointly extracting aspect terms and predicting their sentiment polarities. The most recent and representative are a span-based extract-then-classify approach (Hu et al., 2019) and a directed GCN with syntactic information (Chen et al., 2020). However, all the above studies can not model the visual guidance for both sub-tasks. Different from them, we propose a multi-modal joint framework to handle both MATE and MASC. 3 Joint Multi-modal Aspect-Sentiment Analysis 3.1 Cross-modal Relation Detection Unlike traditional approaches, which take visual information into consideration completely and ignore whether image can bring benefits to text, we incorporate the image-text relation into the model and only retain the auxiliary visual information towards the text. Therefore"
2021.emnlp-main.360,2020.emnlp-main.570,0,0.0372153,"2020a,b) However, all the above studies completely ignore the sentiment polarity analysis dependent on the detected target, which has great facilitates in practical applications, such as e-commerce. Different from them, we propose to jointly perform the corresponding sentiment classification besides aspect terms extraction in a multi-modal scenario. Note that we propose a multi-modal joint learning approach to improve the performance of both MATE and MASC. Multi-modal Aspect Sentiment Classification (MASC). Different from text-based aspect sentiment classification (Sundararaman et al., 2020; Ji et al., 2020; Liang et al., 2020b,a), it is challenging to effectively fuse the textual and visual information. As a pioneer, Xu et al. (2019) collect a benchmark Chinese dataset from a digital product review platform for multi-modal aspect-level sentiment analysis and propose a multi-interactive memory network to iteratively fuse the textual and visual representations. In the past five years, text-based aspect-level sentiment analysis has drawn much attention (Luo et al., 2019; Chen and Qian, 2019; Zhang and Qian, Recently, Yu and Jiang (2019) annotate two 2020; Zheng et al., 2020; Tulkens and van Cranen"
2021.emnlp-main.360,D19-1468,0,0.0233119,"to the industry recently (Akhtar et al., 2019; Zadeh et al., 2020; Sun et al., 2021a; Tang et al., 2019; Zhang et al., 2020b, 2021a). In the following, we mainly overview the limited studies of multi-modal aspect terms extraction and multi-modal aspect sentiment classification on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the gap between text and image. Several related studies with focus on named entity recognition propose to leverage the whole image information by ResNet encoding to augment each word representation, such as (Moon et al., 2018; Zhang et al., 2018) upon RNN, (Yu et al., 2020b) upon Transformer and (Zhang et al., 2021b) on GNN. Besides, several related studies propose to leveraging the fine-grained visual information by object detection, such as (Wu et al., 2020a,b) However, all the above studies completely ignore the sentiment polarity analysis dependent on the"
2021.emnlp-main.360,P19-1056,0,0.0191113,"odal Aspect Sentiment Classification (MASC). Different from text-based aspect sentiment classification (Sundararaman et al., 2020; Ji et al., 2020; Liang et al., 2020b,a), it is challenging to effectively fuse the textual and visual information. As a pioneer, Xu et al. (2019) collect a benchmark Chinese dataset from a digital product review platform for multi-modal aspect-level sentiment analysis and propose a multi-interactive memory network to iteratively fuse the textual and visual representations. In the past five years, text-based aspect-level sentiment analysis has drawn much attention (Luo et al., 2019; Chen and Qian, 2019; Zhang and Qian, Recently, Yu and Jiang (2019) annotate two 2020; Zheng et al., 2020; Tulkens and van Cranen- datasets in Twitter for multi-modal target-oriented burgh, 2020; Akhtar et al., 2020). While, multi- (aka aspect-level) sentiment classification and levermodal target-oriented sentiment analysis has be- age BERT as backbone to effectively combine both come more and more vital because of its urgent textual and visual modalities. In the same period, 4396 Yu et al. (2020a) propose a target-sensitive attention and fusion network to address both text-based and multi-mo"
2021.emnlp-main.360,P19-1344,0,0.0248744,"hes. 2 Related Work need to be applied to the industry recently (Akhtar et al., 2019; Zadeh et al., 2020; Sun et al., 2021a; Tang et al., 2019; Zhang et al., 2020b, 2021a). In the following, we mainly overview the limited studies of multi-modal aspect terms extraction and multi-modal aspect sentiment classification on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the gap between text and image. Several related studies with focus on named entity recognition propose to leverage the whole image information by ResNet encoding to augment each word representation, such as (Moon et al., 2018; Zhang et al., 2018) upon RNN, (Yu et al., 2020b) upon Transformer and (Zhang et al., 2021b) on GNN. Besides, several related studies propose to leveraging the fine-grained visual information by object detection, such as (Wu et al., 2020a,b) However, all the above studies completely ignor"
2021.emnlp-main.360,N18-1078,0,0.0282201,"fication on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the gap between text and image. Several related studies with focus on named entity recognition propose to leverage the whole image information by ResNet encoding to augment each word representation, such as (Moon et al., 2018; Zhang et al., 2018) upon RNN, (Yu et al., 2020b) upon Transformer and (Zhang et al., 2021b) on GNN. Besides, several related studies propose to leveraging the fine-grained visual information by object detection, such as (Wu et al., 2020a,b) However, all the above studies completely ignore the sentiment polarity analysis dependent on the detected target, which has great facilitates in practical applications, such as e-commerce. Different from them, we propose to jointly perform the corresponding sentiment classification besides aspect terms extraction in a multi-modal scenario. Note that we p"
2021.emnlp-main.360,2020.aacl-main.33,0,0.0301429,"ection, such as (Wu et al., 2020a,b) However, all the above studies completely ignore the sentiment polarity analysis dependent on the detected target, which has great facilitates in practical applications, such as e-commerce. Different from them, we propose to jointly perform the corresponding sentiment classification besides aspect terms extraction in a multi-modal scenario. Note that we propose a multi-modal joint learning approach to improve the performance of both MATE and MASC. Multi-modal Aspect Sentiment Classification (MASC). Different from text-based aspect sentiment classification (Sundararaman et al., 2020; Ji et al., 2020; Liang et al., 2020b,a), it is challenging to effectively fuse the textual and visual information. As a pioneer, Xu et al. (2019) collect a benchmark Chinese dataset from a digital product review platform for multi-modal aspect-level sentiment analysis and propose a multi-interactive memory network to iteratively fuse the textual and visual representations. In the past five years, text-based aspect-level sentiment analysis has drawn much attention (Luo et al., 2019; Chen and Qian, 2019; Zhang and Qian, Recently, Yu and Jiang (2019) annotate two 2020; Zheng et al., 2020; Tulke"
2021.emnlp-main.360,P19-1053,0,0.0572799,"Missing"
2021.emnlp-main.360,2020.acl-main.290,0,0.0610892,"Missing"
2021.emnlp-main.360,2020.coling-main.13,0,0.0357248,", all the above studies completely ignore the sentiment polarity analysis dependent on the detected target, which has great facilitates in practical applications, such as e-commerce. Different from them, we propose to jointly perform the corresponding sentiment classification besides aspect terms extraction in a multi-modal scenario. Note that we propose a multi-modal joint learning approach to improve the performance of both MATE and MASC. Multi-modal Aspect Sentiment Classification (MASC). Different from text-based aspect sentiment classification (Sundararaman et al., 2020; Ji et al., 2020; Liang et al., 2020b,a), it is challenging to effectively fuse the textual and visual information. As a pioneer, Xu et al. (2019) collect a benchmark Chinese dataset from a digital product review platform for multi-modal aspect-level sentiment analysis and propose a multi-interactive memory network to iteratively fuse the textual and visual representations. In the past five years, text-based aspect-level sentiment analysis has drawn much attention (Luo et al., 2019; Chen and Qian, 2019; Zhang and Qian, Recently, Yu and Jiang (2019) annotate two 2020; Zheng et al., 2020; Tulkens and van Cranen- datasets in Twitte"
2021.emnlp-main.360,P19-1272,0,0.0608412,"Missing"
2021.emnlp-main.360,2020.emnlp-main.286,0,0.0998868,"Missing"
2021.emnlp-main.360,2020.acl-main.306,0,0.0900057,"Missing"
2021.emnlp-main.360,2020.emnlp-main.141,0,0.0335747,"s-modal relation detection to control whether the image adds to the text meaning. Second, we leverage the joint hierarchical framework to separately attend to the effective visual information for each sub-task instead of collapsed tagging framework. Finally, we can obtain all potential aspect term-polarity pairs. Extensive experiments and analysis on two multi-modal datasets in Twitter show that our approach performs significantly better than text-based joint approaches and collapsed multi-modal joint approaches. 2 Related Work need to be applied to the industry recently (Akhtar et al., 2019; Zadeh et al., 2020; Sun et al., 2021a; Tang et al., 2019; Zhang et al., 2020b, 2021a). In the following, we mainly overview the limited studies of multi-modal aspect terms extraction and multi-modal aspect sentiment classification on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the"
2021.emnlp-main.360,2020.findings-emnlp.72,0,0.202055,"ds to the text meaning. Second, we leverage the joint hierarchical framework to separately attend to the effective visual information for each sub-task instead of collapsed tagging framework. Finally, we can obtain all potential aspect term-polarity pairs. Extensive experiments and analysis on two multi-modal datasets in Twitter show that our approach performs significantly better than text-based joint approaches and collapsed multi-modal joint approaches. 2 Related Work need to be applied to the industry recently (Akhtar et al., 2019; Zadeh et al., 2020; Sun et al., 2021a; Tang et al., 2019; Zhang et al., 2020b, 2021a). In the following, we mainly overview the limited studies of multi-modal aspect terms extraction and multi-modal aspect sentiment classification on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the gap between text and image. Several related studies with f"
2021.emnlp-main.360,2020.emnlp-main.291,1,0.842073,"ds to the text meaning. Second, we leverage the joint hierarchical framework to separately attend to the effective visual information for each sub-task instead of collapsed tagging framework. Finally, we can obtain all potential aspect term-polarity pairs. Extensive experiments and analysis on two multi-modal datasets in Twitter show that our approach performs significantly better than text-based joint approaches and collapsed multi-modal joint approaches. 2 Related Work need to be applied to the industry recently (Akhtar et al., 2019; Zadeh et al., 2020; Sun et al., 2021a; Tang et al., 2019; Zhang et al., 2020b, 2021a). In the following, we mainly overview the limited studies of multi-modal aspect terms extraction and multi-modal aspect sentiment classification on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the gap between text and image. Several related studies with f"
C08-1137,C04-1073,0,0.0585709,"n et al. (2003); a lexicalized reordering model was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatically extracted the rewriting patterns. The rewriting patterns are employed on the input source sentence to make the word order more accordant to target language. Collins et al. (2005) described an approach to reorder German in German-to-English translation. The method concentrates on the German clauses and six types of transforming rules are applied to the parsed source sentence. However, all the rules are manually built. Li et al. (2"
C08-1137,W06-1609,0,0.0665215,"ld be divided into two categories: one is integrated into the decoder and the other is employed as a preprocessing module. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Many reordering methods belong to the former category. Distortion model was first employed by Koehn et al. (2003); a lexicalized reordering model was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatically extracted the rewri"
C08-1137,D07-1077,0,0.435576,"osed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatically extracted the rewriting patterns. The rewriting patterns are employed on the input source sentence to make the word order more accordant to target language. Collins et al. (2005) described an approach to reorder German in German-to-English translation. The method concentrates on the German clauses and six types of transforming rules are applied to the parsed source sentence. However, all the rules are manually built. Li et al. (2007) used a parser to get the syntactic t"
C08-1137,P07-1091,0,0.0130776,"l was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatically extracted the rewriting patterns. The rewriting patterns are employed on the input source sentence to make the word order more accordant to target language. Collins et al. (2005) described an approach to reorder German in German-to-English translation. The method concentrates on the German clauses and six types of transforming rules are applied to the parsed source sentence. However, all the rules are manually built. Li et al. (2007) used a parser to get the syntactic t"
C08-1137,P03-1054,0,0.00678349,"Missing"
C08-1137,P05-1033,0,0.020392,"sting reordering approaches could be divided into two categories: one is integrated into the decoder and the other is employed as a preprocessing module. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Many reordering methods belong to the former category. Distortion model was first employed by Koehn et al. (2003); a lexicalized reordering model was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatic"
C08-1137,P06-1066,0,0.107323,"pproaches could be divided into two categories: one is integrated into the decoder and the other is employed as a preprocessing module. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Many reordering methods belong to the former category. Distortion model was first employed by Koehn et al. (2003); a lexicalized reordering model was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatically extracted the rewri"
C08-1137,J04-4002,0,0.0732557,"Missing"
C08-1137,N03-1017,0,0.00581239,"model. However, reordering is always a key issue in the decoding process. A number of models have been developed to deal with the problem of reordering. The existing reordering approaches could be divided into two categories: one is integrated into the decoder and the other is employed as a preprocessing module. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Many reordering methods belong to the former category. Distortion model was first employed by Koehn et al. (2003); a lexicalized reordering model was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord"
C08-1137,W07-0401,0,0.0694085,"Missing"
C08-1137,P02-1038,0,\N,Missing
C08-1137,P05-1066,0,\N,Missing
C10-1021,P09-1095,0,0.070286,"Missing"
C10-1021,P08-2045,0,0.0216045,"Missing"
C10-1021,C08-1111,0,0.10928,". Both manually generalized patterns and automatically generalized patterns are designed to extract general cause expressions or specific constructions for emotion causes. Experiments show that our system achieves a performance much higher than a baseline model. 1 Introduction Text-based emotion processing has been a center of attention in the NLP field in the past few years. Most previous researches have focused on detecting the surface information of emotions, especially emotion classes, e.g., “happiness” and “anger” (Mihalcea and Liu 2006, Strapparava and Mihalcea 2008, Abbasi et al, 2008, Tokuhisa et al. 2008). Although most emotion theories recognize the important role of causes in emotion analysis (Descartes, 1649; James, 1884; Plutchik 1980, Wierzbicka 1999), very few studies explore the interactions between emotion and causes. Emotion-cause interaction is the eventive relation which potentially yields the most crucial information in terms of information extraction. For instance, knowing the existence of an emotion is often insufficient to predict future events or decide on the best reaction. However, if the emotion cause is known in addition to the type of emotion, prediction of future events o"
C10-1021,Y09-1011,1,0.821787,"e first attempt to explore the correlation between emotions and causes, and annotate a Chinese emotion cause corpus. The emotion cause corpus focuses on five primary emotions, namely “happiness”, “sadness”, “fear”, “anger”, and “surprise”. The emotions are explicitly expressed by emotion keywords, e.g., gao1xing4 “happy”, shang1xin1 “sad”, etc. The corpus is created as follows. 1. 6,058 entries of Chinese sentences are extracted from the Academia Sinica Balanced Corpus of Mandarin Chinese (Sinica Corpus) with the pattern-match method as well as the list of 91 Chinese primary emotion keywords (Chen et al., 2009). Each entry contains the focus sentence with the emotion keyword “&lt;FocusSentence&gt;” plus the sentence before “&lt;PrefixSentence&gt;” and after “&lt;SuffixSentence&gt;” it. For each entry, the emotion keywords are indexed since more than one emotion may be presented in an entry; 2. Some preprocessing, such as balancing the number of entry among emotions, is done to remove some entries. Finally, 5,629 entries remain; 3. Each emotion keyword is annotated with its corresponding causes if existing. An emotion keyword can sometimes be associ181 3.2 The Analysis of Emotion Causes To have a deeper understanding"
C10-1021,W03-1210,0,0.0123079,"Missing"
C10-1021,W10-0206,1,0.745634,"xtraction, as emotion cause detection is a case of cause detection, some typical patterns used in existing cause detection systems, e.g., “because” and “thus”, can be adopted. In addition, various linguistic cues are examined which potentially indicate emotion causes, such as causative verbs and epistemic markers (Lee at al. 2010a). Then some linguistic patterns of emotion causes are manu179 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 179–187, Beijing, August 2010 ally generalized by examining the linguistic context of the empirical data (Lee et al., 2010b). It is expected that these manually generalized patterns often yield a low-coverage problem. Thus, we extracted features which enable us to automatically capture more emotion-specific constructions. Experiments show that such an integrated system with various linguistic features performs promisingly well. We believe that the present study should provide the foundation for future research on emotion analysis, such as the detection of implicit emotion or cause. The paper is organized as follows. Section 2 discusses the related work on cause-effect detection. Section 3 briefly describes the em"
C10-1021,lee-etal-2010-emotion,1,0.749997,"xtraction, as emotion cause detection is a case of cause detection, some typical patterns used in existing cause detection systems, e.g., “because” and “thus”, can be adopted. In addition, various linguistic cues are examined which potentially indicate emotion causes, such as causative verbs and epistemic markers (Lee at al. 2010a). Then some linguistic patterns of emotion causes are manu179 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 179–187, Beijing, August 2010 ally generalized by examining the linguistic context of the empirical data (Lee et al., 2010b). It is expected that these manually generalized patterns often yield a low-coverage problem. Thus, we extracted features which enable us to automatically capture more emotion-specific constructions. Experiments show that such an integrated system with various linguistic features performs promisingly well. We believe that the present study should provide the foundation for future research on emotion analysis, such as the detection of implicit emotion or cause. The paper is organized as follows. Section 2 discusses the related work on cause-effect detection. Section 3 briefly describes the em"
C10-1021,P02-1047,0,0.0220873,"Missing"
C10-1072,P07-1056,0,0.455354,"Missing"
C10-1072,C04-1200,0,0.209598,"selection method is presented to automatically generate the labeled training data for polarity shifting detection of sentences. The remainder of this paper is organized as follows. Section 2 introduces the related work of sentiment classification. Section 3 presents our approach in details. Experimental results are presented and analyzed in Section 4. Finally, 636 Section 5 draws the conclusion and outlines the future work. 2 Related Work Generally, sentiment classification can be performed at four different levels: word level (Wiebe, 2000), phrase level (Wilson et al., 2009), sentence level (Kim and Hovy, 2004; Liu et al., 2005), and document level (Turney, 2002; Pang et al., 2002; Pang and Lee, 2004; Riloff et al., 2006). This paper focuses on document-level sentiment classification. In the literature, there are mainly two kinds of approaches on document-level sentiment classification: term-counting approaches (lexicon-based) and machine learning approaches (corpus-based). Term-counting approaches usually involve deriving a sentiment measure by calculating the total number of negative and positive terms (Turney, 2002; Kim and Hovy, 2004; Kennedy and Inkpen, 2006). Machine learning approaches recas"
C10-1072,P08-2065,1,0.901692,"Missing"
C10-1072,W02-1011,0,0.0411334,"shifting detection of sentences. Then, by using the obtained binary classifier, each document in the original polarity classification training data is split into two partitions, polarity-shifted and polarity-unshifted, which are used to train two base classifiers respectively for further classifier combination. The experimental results across four different domains demonstrate the effectiveness of our approach. 1 Introduction Sentiment classification is a special task of text classification whose objective is to classify a text according to the sentimental polarities of opinions it contains (Pang et al., 2002), e.g., favorable or unfavorable, positive or negative. This task has received considerable interests in the computational linguistic community due to its potential applications. In the literature, machine learning approaches have dominated the research in sentiment classification and achieved the state-of-the-art performance (e.g., Kennedy and Inkpen, 2006; ‡ Natural Language Processing Lab School of Computer Science and Technology Soochow University gdzhou@suda.edu.cn Pang et al., 2002). In a typical machine learning approach, a document (text) is modeled as a bag-of-words, i.e. a set of con"
C10-1072,P02-1053,0,0.02264,"e sentimental orientation of the whole text depends on the sum of the sentimental polarities of content words. Although this assumption is reasonable and has led to initial success, it is linguistically unsound since many function words and constructions can shift the sentimental polarities of a text. For example, in the sentence ‘The chair is not comfortable’, the polarity of the word ‘comfortable’ is positive while the polarity of the whole sentence is reversed because of the negation word ‘not’. Therefore, the overall sentiment of a document is not necessarily the sum of the content parts (Turney, 2002). This phenomenon is one main reason why machine learning approaches fail under some circumstances. As a typical case of polarity shifting, negation has been paid close attention and widely studied in the literature (Na et al., 2004; Wilson et al., 2009; Kennedy and Inkpen, 2006). Generally, there are two steps to incorporate negation information into a system: negation detection and negation classification. For negation detection, some negation trigger words, such as ‘no’, ‘not’, and ‘never’, are usually applied to recognize negation phrases or sentences. As for negation classification, one w"
C10-1072,P09-1027,0,0.204745,"Missing"
C10-1072,J09-3003,0,0.511671,"uctions can shift the sentimental polarities of a text. For example, in the sentence ‘The chair is not comfortable’, the polarity of the word ‘comfortable’ is positive while the polarity of the whole sentence is reversed because of the negation word ‘not’. Therefore, the overall sentiment of a document is not necessarily the sum of the content parts (Turney, 2002). This phenomenon is one main reason why machine learning approaches fail under some circumstances. As a typical case of polarity shifting, negation has been paid close attention and widely studied in the literature (Na et al., 2004; Wilson et al., 2009; Kennedy and Inkpen, 2006). Generally, there are two steps to incorporate negation information into a system: negation detection and negation classification. For negation detection, some negation trigger words, such as ‘no’, ‘not’, and ‘never’, are usually applied to recognize negation phrases or sentences. As for negation classification, one way to import negation information is to directly reverse the polarity of the words which contain negation trigger words as far as term-counting approaches are considered (Kennedy and Inkpen, 2006). An alternative way is to add some negation features (e."
C10-1072,P04-1035,0,\N,Missing
C10-1072,W06-1652,0,\N,Missing
C10-1072,P09-1078,1,\N,Missing
C10-1072,P09-1079,0,\N,Missing
C10-1072,I08-1039,0,\N,Missing
C12-2067,W06-1655,0,0.0242448,"ce labelling algorithms (Tseng et al., 2005), e.g., conditional random fields (CRF), which perform well in both invocabulary (IV) recall and out-of-vocabulary (OOV) recall. Based on the character labelling approach, many related studies make efforts to improve the performance by various means, such as using more tags and features (Tang et al., 2009; Zhao et al., 2006), employing word-based tagging without tagging (Zhang and Clark, 2007), employing some joint models that combines a generative model and a discriminative model (Wang et al., 2010; Wang et al., 2011) or Markov and semi-Markov CRF (Andrew, 2006), and integrating unsupervised segmentation features (Zhao and Kit, 2011). Although there are various studies CWS individually, there are few studies of active learning on CWS. One related work is about active learning on Japanese word segmentation via Support Vector Machines (SVM) (Sassano, 2002). However, both the two challenging problems mentioned above are unsolved. Specifically, that study annotates the whole sentence as a basic unit, which means much more annotation effort than our model. Furthermore, our corpus scale is much larger than the one in Sassano (2002). This makes SVM impracti"
C12-2067,P07-1007,0,0.0489837,"Missing"
C12-2067,N06-1016,0,0.0567906,"Missing"
C12-2067,I05-3017,0,0.10462,"Missing"
C12-2067,J98-4002,0,0.0700262,"Missing"
C12-2067,P07-2018,1,0.755901,"selected for oracle labelling; Uncertainty-Diversity sampling: In each iteration, all the instances in the unlabeled data U are ranked according to their uncertainty-diversity values and top instances are selected for oracle labeling. 4 4.1 Experimentation Experimental Setting The SIGHAN Bakeoff 2 dataset consists of four different corpora: PKU, MSR, CityU, and AS. But we only report the performance on three of the corpora except AS due to its significant large scale in causing the out-of-memory error. The basic segmenter in the active learning process is trained with a 2-tag labelling model (Huang et al., 2007; Huang and Xue, 2012) and implemented with a public tool for CRF implementation, i.e. CRF++ (Kudo, 2005). For the feature template, we adopt the one by Li and Huang (2009). In all experiments, we use the standard F1 score as our main performance measurement. Besides, the out-of-vocabulary (OOV) recall is used to evaluate the OOV issue. 4.2 Experimental Results In this experiment, we compare the random selection strategy and the two sampling strategies as illustrated in Section 3.3: uncertainty sampling and uncertainty-diversity sampling. To fairly compare the performances of different samplin"
C12-2067,P08-1102,0,0.0582361,"Missing"
C12-2067,Y09-2034,1,0.89236,"sity values and top instances are selected for oracle labeling. 4 4.1 Experimentation Experimental Setting The SIGHAN Bakeoff 2 dataset consists of four different corpora: PKU, MSR, CityU, and AS. But we only report the performance on three of the corpora except AS due to its significant large scale in causing the out-of-memory error. The basic segmenter in the active learning process is trained with a 2-tag labelling model (Huang et al., 2007; Huang and Xue, 2012) and implemented with a public tool for CRF implementation, i.e. CRF++ (Kudo, 2005). For the feature template, we adopt the one by Li and Huang (2009). In all experiments, we use the standard F1 score as our main performance measurement. Besides, the out-of-vocabulary (OOV) recall is used to evaluate the OOV issue. 4.2 Experimental Results In this experiment, we compare the random selection strategy and the two sampling strategies as illustrated in Section 3.3: uncertainty sampling and uncertainty-diversity sampling. To fairly compare the performances of different sampling strategies, we make sure that the number of annotated boundaries in either uncertainty sampling or uncertainty-diversity sampling is the same as random selection. Figure"
C12-2067,D12-1013,1,0.8313,"Missing"
C12-2067,W04-3236,0,0.0922903,"Missing"
C12-2067,P02-1064,0,0.529185,"ive learning on CWS. First, the state-of-the-art methods treat CWS as a sequence labelling task (Jiang et al., 2008; Ng and Low, 2004; Tseng et al., 2005; Zhang et al., 2006), i.e. labelling characters with tags from a pre-defined tag set, representing the position of a character in a word. Different from traditional classification tasks, each character is tagged sequentially according to its corresponding context. Under this circumstance, a character cannot be determined as a single unit to query in active learning. One possible solution is to select one sentence as a unit for annotation, as Sassano (2002) does for Japanese word segmentation. However, such solution is expensive for annotation and since one sentence might contain some words which can be easily segmented correctly by existing models with high confidence, annotating them becomes a waste of time and manual effort. Second, the number of the characters in a CWS corpus is normally extremely huge. For example, among the four corpora in SIGHAN Bakeoff 2 (Emerson, 2005), even the smallest corpus contains more than 1,800,000 characters while others are much larger in the order of tens of millions of characters. Compared to other tasks lik"
C12-2067,D08-1112,0,0.0916365,"ount to avoid duplicate annotation. For example, in the example E-A above, both the words ‘索拉纳 ’ and ‘波 兰 ’ are unknown words for the initial segmenter learned by the initial labelled set L with the boundaries of I A1 , I A 2 , I A9 , I B1 , I B 2 , and I B 9 , among the top uncertain instances. Obviously, some boundaries share the same segmentation information, e.g., I A1 and I B1 . Therefore, labelling both of them is a waste. One straightforward way to handle such duplicate annotation is to compute the similarity between every two instances and then pick those with the highest diversities (Settles and Craven, 2008). This method, however, requires O(N2) in computational complexity where N is the number of all boundaries. When N is huge (e.g. N&gt;1,800,000 in our experiments), the high computational burden is simply unacceptable. Fortunately, we find that the similarity between two boundaries is highly related to their surrounding character N-grams (in particular bigrams) and we can better evaluate the diversity with the help of the surrounding character bigrams. This is done in this paper by recording the frequencies of all surrounding bigrams in a set Scc , where f ci ci +1 ∈ S cc indicates the frequency"
C12-2067,P04-1075,1,0.646546,"Missing"
C12-2067,P98-2206,0,0.0867883,"For the second challenge, we propose 684 a diversity measurement among the instances to avoid duplicate annotation, so as to further reduce the annotation efforts. 2 Related Work Research on CWS has a long history and various methods have been proposed in the literature. Basically, these methods are mainly focus on two categories: unsupervised and supervised. Unsupervised methods aim to build a segmentation system without any lexicon or labelled data. They often start from an empirical definition of a word and then use some statistical measures, e.g. mutual information (Sproat and Shih, 1990; Sun et al., 1998), to learn words from a large unlabelled data resource. Although these unsupervised methods can capture many strong words, their performance is often not high enough for the practical use. Supervised methods, such as HMM tagging (Xue, 2003), character-based classification (Wang et al., 2008) and morpheme-based lexical chunking (Fu et al., 2008), attempt to acquire a model based on a dictionary or a labelled data set. Among them, character-based classification has drawn most attention recently and been further implemented with sequence labelling algorithms (Tseng et al., 2005), e.g., conditiona"
C12-2067,I05-3027,0,0.152497,"ttles and Craven, 2008). Although active learning has been widely employed to many NLP tasks, such as word sense disambiguation (Chan and Ng, 2007; Chen et al., 2006; Fujii et al., 1998), text categorization (Lewis and Gale, 1994; Liere and Tadepalli, 1997; McCallum and Nigam, 1998; Li et al., 2012), and named entity recognition (Shen et al., 2004), there are few studies of active learning on CWS, probably due to the strong challenges inherent in performing active learning on CWS. First, the state-of-the-art methods treat CWS as a sequence labelling task (Jiang et al., 2008; Ng and Low, 2004; Tseng et al., 2005; Zhang et al., 2006), i.e. labelling characters with tags from a pre-defined tag set, representing the position of a character in a word. Different from traditional classification tasks, each character is tagged sequentially according to its corresponding context. Under this circumstance, a character cannot be determined as a single unit to query in active learning. One possible solution is to select one sentence as a unit for annotation, as Sassano (2002) does for Japanese word segmentation. However, such solution is expensive for annotation and since one sentence might contain some words wh"
C12-2067,C10-1132,0,0.014142,"rawn most attention recently and been further implemented with sequence labelling algorithms (Tseng et al., 2005), e.g., conditional random fields (CRF), which perform well in both invocabulary (IV) recall and out-of-vocabulary (OOV) recall. Based on the character labelling approach, many related studies make efforts to improve the performance by various means, such as using more tags and features (Tang et al., 2009; Zhao et al., 2006), employing word-based tagging without tagging (Zhang and Clark, 2007), employing some joint models that combines a generative model and a discriminative model (Wang et al., 2010; Wang et al., 2011) or Markov and semi-Markov CRF (Andrew, 2006), and integrating unsupervised segmentation features (Zhao and Kit, 2011). Although there are various studies CWS individually, there are few studies of active learning on CWS. One related work is about active learning on Japanese word segmentation via Support Vector Machines (SVM) (Sassano, 2002). However, both the two challenging problems mentioned above are unsolved. Specifically, that study annotates the whole sentence as a basic unit, which means much more annotation effort than our model. Furthermore, our corpus scale is mu"
C12-2067,O03-4002,0,0.155392,"posed in the literature. Basically, these methods are mainly focus on two categories: unsupervised and supervised. Unsupervised methods aim to build a segmentation system without any lexicon or labelled data. They often start from an empirical definition of a word and then use some statistical measures, e.g. mutual information (Sproat and Shih, 1990; Sun et al., 1998), to learn words from a large unlabelled data resource. Although these unsupervised methods can capture many strong words, their performance is often not high enough for the practical use. Supervised methods, such as HMM tagging (Xue, 2003), character-based classification (Wang et al., 2008) and morpheme-based lexical chunking (Fu et al., 2008), attempt to acquire a model based on a dictionary or a labelled data set. Among them, character-based classification has drawn most attention recently and been further implemented with sequence labelling algorithms (Tseng et al., 2005), e.g., conditional random fields (CRF), which perform well in both invocabulary (IV) recall and out-of-vocabulary (OOV) recall. Based on the character labelling approach, many related studies make efforts to improve the performance by various means, such as"
C12-2067,W03-1730,0,0.0549875,"Missing"
C12-2067,P06-2123,0,0.046661,"Missing"
C12-2067,P07-1106,0,0.0245741,"to acquire a model based on a dictionary or a labelled data set. Among them, character-based classification has drawn most attention recently and been further implemented with sequence labelling algorithms (Tseng et al., 2005), e.g., conditional random fields (CRF), which perform well in both invocabulary (IV) recall and out-of-vocabulary (OOV) recall. Based on the character labelling approach, many related studies make efforts to improve the performance by various means, such as using more tags and features (Tang et al., 2009; Zhao et al., 2006), employing word-based tagging without tagging (Zhang and Clark, 2007), employing some joint models that combines a generative model and a discriminative model (Wang et al., 2010; Wang et al., 2011) or Markov and semi-Markov CRF (Andrew, 2006), and integrating unsupervised segmentation features (Zhao and Kit, 2011). Although there are various studies CWS individually, there are few studies of active learning on CWS. One related work is about active learning on Japanese word segmentation via Support Vector Machines (SVM) (Sassano, 2002). However, both the two challenging problems mentioned above are unsolved. Specifically, that study annotates the whole sentence"
C12-2067,I08-4017,0,0.204369,"Missing"
C12-2067,Y06-1012,0,0.0603917,"8) and morpheme-based lexical chunking (Fu et al., 2008), attempt to acquire a model based on a dictionary or a labelled data set. Among them, character-based classification has drawn most attention recently and been further implemented with sequence labelling algorithms (Tseng et al., 2005), e.g., conditional random fields (CRF), which perform well in both invocabulary (IV) recall and out-of-vocabulary (OOV) recall. Based on the character labelling approach, many related studies make efforts to improve the performance by various means, such as using more tags and features (Tang et al., 2009; Zhao et al., 2006), employing word-based tagging without tagging (Zhang and Clark, 2007), employing some joint models that combines a generative model and a discriminative model (Wang et al., 2010; Wang et al., 2011) or Markov and semi-Markov CRF (Andrew, 2006), and integrating unsupervised segmentation features (Zhao and Kit, 2011). Although there are various studies CWS individually, there are few studies of active learning on CWS. One related work is about active learning on Japanese word segmentation via Support Vector Machines (SVM) (Sassano, 2002). However, both the two challenging problems mentioned abov"
C12-2067,C08-1059,0,\N,Missing
C12-2067,I08-4015,0,\N,Missing
C14-1050,P13-1024,0,0.0234788,"searches, our study is forced on skill inference instead of traditional tag suggestion. Basically, the social connections in skill inference are much different from those in social tagging. In our study, we use co-major, co-title and other academic and business relationships to build the social connections. Meanwhile, there are also few researches concern to propose a joint model to leverage both personal and skill connections. 2.3 Factor Graph Model Among various approaches investigated in social networks in the last several years (Leskovec et al., 2010; Lu et al., 2010; Lampos et al., 2013; Guo et al., 2013), Factor Graph Model (FGM) becomes an effective way to represent and optimize the relationship in social networks (Dong et al., 2012; Yang et al., 2012b) via a graph structure. Tang et al. (2011a) and Zhuang et al. (2012) formalized the problem of social relationship learning as a semi-supervised framework, and proposed Partially-labeled Pairwise Factor Graph Model (PLP-FGM) for inferring the types of social ties. Tang et al. (2013) further proposed a factor graph based distributed learning method to construct a conformity influence model and formalize the effects of social conformity in a pro"
C14-1050,P13-1098,0,0.0205867,"fferent from above researches, our study is forced on skill inference instead of traditional tag suggestion. Basically, the social connections in skill inference are much different from those in social tagging. In our study, we use co-major, co-title and other academic and business relationships to build the social connections. Meanwhile, there are also few researches concern to propose a joint model to leverage both personal and skill connections. 2.3 Factor Graph Model Among various approaches investigated in social networks in the last several years (Leskovec et al., 2010; Lu et al., 2010; Lampos et al., 2013; Guo et al., 2013), Factor Graph Model (FGM) becomes an effective way to represent and optimize the relationship in social networks (Dong et al., 2012; Yang et al., 2012b) via a graph structure. Tang et al. (2011a) and Zhuang et al. (2012) formalized the problem of social relationship learning as a semi-supervised framework, and proposed Partially-labeled Pairwise Factor Graph Model (PLP-FGM) for inferring the types of social ties. Tang et al. (2013) further proposed a factor graph based distributed learning method to construct a conformity influence model and formalize the effects of social"
C14-1050,C12-2064,0,0.0132631,"heir information in an unconstrained manner (Ohkura et al., 2006; Si et al., 2010). Ohkura et al. (2006) created a multi-tagger to determine whether a particular tag from a candidate tag list should be attached to a weblog. Lappas et al. (2011) proposed a social endorsement-based approach to generate social tags from Twitter.com and Flickr.com where various kinds of information in recommendations and comments are used. Liu et al. (2012) propose a probabilistic model to connect the semantic relations between words and tags of microblog, and takes the social network structure as regularization. Li et al., (2012) propose to model context-aware relations of tags for suggestion by regarding resource content as context of tags. 521 Different from above researches, our study is forced on skill inference instead of traditional tag suggestion. Basically, the social connections in skill inference are much different from those in social tagging. In our study, we use co-major, co-title and other academic and business relationships to build the social connections. Meanwhile, there are also few researches concern to propose a joint model to leverage both personal and skill connections. 2.3 Factor Graph Model Amo"
C14-1050,D11-1146,0,0.0313099,"Missing"
C14-1050,C12-2074,0,0.012552,"ese skills is fully leveraged in the inference. 2.2 Social Tag Suggestion Social tag suggestion aims to extract proper tags from social media and can thus help people organize their information in an unconstrained manner (Ohkura et al., 2006; Si et al., 2010). Ohkura et al. (2006) created a multi-tagger to determine whether a particular tag from a candidate tag list should be attached to a weblog. Lappas et al. (2011) proposed a social endorsement-based approach to generate social tags from Twitter.com and Flickr.com where various kinds of information in recommendations and comments are used. Liu et al. (2012) propose a probabilistic model to connect the semantic relations between words and tags of microblog, and takes the social network structure as regularization. Li et al., (2012) propose to model context-aware relations of tags for suggestion by regarding resource content as context of tags. 521 Different from above researches, our study is forced on skill inference instead of traditional tag suggestion. Basically, the social connections in skill inference are much different from those in social tagging. In our study, we use co-major, co-title and other academic and business relationships to bu"
C14-1050,C10-1114,0,0.0165272,"relationship between persons). Expert finding is in nature different from skill inference. Our study predicts various skills attachable to a person collectively with both personal and skill connections among people. One distinguishing characteristics of our study is that several skills from a person are simultaneously modeled and the relationship among these skills is fully leveraged in the inference. 2.2 Social Tag Suggestion Social tag suggestion aims to extract proper tags from social media and can thus help people organize their information in an unconstrained manner (Ohkura et al., 2006; Si et al., 2010). Ohkura et al. (2006) created a multi-tagger to determine whether a particular tag from a candidate tag list should be attached to a weblog. Lappas et al. (2011) proposed a social endorsement-based approach to generate social tags from Twitter.com and Flickr.com where various kinds of information in recommendations and comments are used. Liu et al. (2012) propose a probabilistic model to connect the semantic relations between words and tags of microblog, and takes the social network structure as regularization. Li et al., (2012) propose to model context-aware relations of tags for suggestion"
C14-1050,P02-1053,0,0.00901377,"Missing"
C16-1153,P13-2037,0,0.0144394,"or-based document representation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open domains. Most of the previous studies focused on monolingual text, while our proposed bilingual attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from monolingual and bilingual contexts with attention mechanisms. 2.2 Research on Code-switching and Bilingual Text Code-switched documents have received considerable attention in the NLP community (Adel et al., 2013; Garrette et al., 2015). Several studies have focused on code-switching identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting code-switched points (Solorio and Liu, 2008), identifying code-switched tokens (Lignos and Marcus, 2013), adding code-switched support to language models (Li and Fung, 2012), and POS tagging for code-switching text (Jamatia et al., 2015). There is relatively little work focus on predicting emotion in code-switching text. Wang et al. (2015) proposed a machine translation based approach to predict emotion in"
C16-1153,D08-1092,0,0.024572,"g monolingual and bilingual information in both lexical and document level with neural network model and attention mechanism, while previous research only focused on lexicallevel bilingual information. In addition, we do not use any external resource, such as bilingual and sentiment dictionary, to train our model. More remotely connected, multilingual natural language processing has attracted increasing attention in the computational linguistic community due to its broad real-world applications. Relevant studies have been reported in various natural language processing tasks, such as parsing (Burkett and Klein, 2008), information retrieval (Gao et al., 2009), text classification (Amini et al., 2010), and so on. There are a number of studies on predicting sentiment polarity through multilingual text. Wan (2009) incorporated unlabeled data in the target language into classifier with co-training to improve classification performance. Wei and Pal (2010) regarded cross-lingual sentiment classification as a domain adaptation task and applied structural correspondence learning (SCL) to tackle this problem. Their approach achieves a better performance than the co-training algorithm. More recently, Meng et al. (20"
C16-1153,C14-1008,0,0.00907215,"-specific lexicon. For emotion classification, Liu et al. (2013) used a co-training framework to infer the news from readers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and emotion analysis (Vo and Zhang, 2015; Zhang et al., 2015). dos Santos and Gatti (2014) proposed a character-based deep convolutional neural network to predict sentiment of short text. Tang et al. (2015) proposed a neural network model to learn vector-based document representation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open domains. Most of the previous studies focused on monolingual text, while our proposed bilingual attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from monolingual and bilingual context"
C16-1153,P09-1121,0,0.0223953,"exical and document level with neural network model and attention mechanism, while previous research only focused on lexicallevel bilingual information. In addition, we do not use any external resource, such as bilingual and sentiment dictionary, to train our model. More remotely connected, multilingual natural language processing has attracted increasing attention in the computational linguistic community due to its broad real-world applications. Relevant studies have been reported in various natural language processing tasks, such as parsing (Burkett and Klein, 2008), information retrieval (Gao et al., 2009), text classification (Amini et al., 2010), and so on. There are a number of studies on predicting sentiment polarity through multilingual text. Wan (2009) incorporated unlabeled data in the target language into classifier with co-training to improve classification performance. Wei and Pal (2010) regarded cross-lingual sentiment classification as a domain adaptation task and applied structural correspondence learning (SCL) to tackle this problem. Their approach achieves a better performance than the co-training algorithm. More recently, Meng et al. (2012) employed the parallel corpus for cross"
C16-1153,N15-1109,0,0.0140355,"epresentation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open domains. Most of the previous studies focused on monolingual text, while our proposed bilingual attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from monolingual and bilingual contexts with attention mechanisms. 2.2 Research on Code-switching and Bilingual Text Code-switched documents have received considerable attention in the NLP community (Adel et al., 2013; Garrette et al., 2015). Several studies have focused on code-switching identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting code-switched points (Solorio and Liu, 2008), identifying code-switched tokens (Lignos and Marcus, 2013), adding code-switched support to language models (Li and Fung, 2012), and POS tagging for code-switching text (Jamatia et al., 2015). There is relatively little work focus on predicting emotion in code-switching text. Wang et al. (2015) proposed a machine translation based approach to predict emotion in codeswitching text with"
C16-1153,R15-1033,0,0.0212779,"Missing"
C16-1153,lee-etal-2014-annotating,1,0.895524,"Missing"
C16-1153,C12-1102,0,0.00831582,"tecture to capture informative words from monolingual and bilingual contexts with attention mechanisms. 2.2 Research on Code-switching and Bilingual Text Code-switched documents have received considerable attention in the NLP community (Adel et al., 2013; Garrette et al., 2015). Several studies have focused on code-switching identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting code-switched points (Solorio and Liu, 2008), identifying code-switched tokens (Lignos and Marcus, 2013), adding code-switched support to language models (Li and Fung, 2012), and POS tagging for code-switching text (Jamatia et al., 2015). There is relatively little work focus on predicting emotion in code-switching text. Wang et al. (2015) proposed a machine translation based approach to predict emotion in codeswitching text with various external resources. Our approach departs from the previous work that we model the task by considering monolingual and bilingual information in both lexical and document level with neural network model and attention mechanism, while previous research only focused on lexicallevel bilingual information. In addition, we do not use an"
C16-1153,P15-1101,1,0.208181,"focused on analyzing emotions in monolingual text. Some of these studies emotion lexicon building, for example, Rao et al. (2012) automatically built a word-emotion mapping dictionary for social emotion detection, Yang et al. (2014) proposed a novel emotion-aware topic model to build a domain-specific lexicon. For emotion classification, Liu et al. (2013) used a co-training framework to infer the news from readers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and emotion analysis (Vo and Zhang, 2015; Zhang et al., 2015). dos Santos and Gatti (2014) proposed a character-based deep convolutional neural network to predict sentiment of short text. Tang et al. (2015) proposed a neural network model to learn vector-based document representation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open dom"
C16-1153,P13-1018,0,0.0560854,"proposed model. Visualization of the attention layers illustrates that the model selects informative words qualitatively. 1 Introduction Microblogs such as Twitter and Facebook have gained tremendous popularity in the past decade, they often contain extremely current, even breaking, information about world events. However, the writing style of microblogs tends to be quite colloquial and nonstandard, unlike the style found in more traditional, edited genres (Li et al., 2015; Vo and Zhang, 2015). In addition, authors from multi-lingual communities tend to write code-switching posts frequently (Ling et al., 2013; Wang et al., 2015). These pose challenges for automatic emotion prediction tasks. There has been some previous research focusing on both emotion analysis (Pang et al., 2002; Lee et al., 2014) and code-switching text analysis (Solorio and Liu, 2008; Ling et al., 2013; Jamatia et al., 2015). However, little research has focused on predicting emotion in code-switching text. Different from monolingual emotion prediction, the emotion in code-switching posts can be expressed in either monolingual or bilingual forms. In this study, we focus on Chinese and English mixed code-switching text from Chin"
C16-1153,P13-2091,1,0.280309,"ows the effectiveness of our proposed BAN model with both monolingual and bilingual information. 2 2.1 Related Works Emotion Analysis Over the last decade, there has been much work exploring various aspects of emotion analysis (Wiebe et al., 2005). While most focused on analyzing emotions in monolingual text. Some of these studies emotion lexicon building, for example, Rao et al. (2012) automatically built a word-emotion mapping dictionary for social emotion detection, Yang et al. (2014) proposed a novel emotion-aware topic model to build a domain-specific lexicon. For emotion classification, Liu et al. (2013) used a co-training framework to infer the news from readers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and emotion analysis (Vo and Zhang, 2015; Zhang et al., 2015). dos Santos and Gatti (2014) proposed a character-based deep convolut"
C16-1153,P12-1060,0,0.0429676,"nd Klein, 2008), information retrieval (Gao et al., 2009), text classification (Amini et al., 2010), and so on. There are a number of studies on predicting sentiment polarity through multilingual text. Wan (2009) incorporated unlabeled data in the target language into classifier with co-training to improve classification performance. Wei and Pal (2010) regarded cross-lingual sentiment classification as a domain adaptation task and applied structural correspondence learning (SCL) to tackle this problem. Their approach achieves a better performance than the co-training algorithm. More recently, Meng et al. (2012) employed the parallel corpus for cross-lingual sentiment classification. They explored the case when no labeled data is available in the parallel corpus. However, such multi-lingual models do not explicitly consider code-switching, since their data sets are always parallel corpus. As the two languages are mixed in the code-switching text without parallel, code-switching corpus is more difficult to process. 3 Bilingual Attention Network Given a post X with T words (X =< w1 , w2 , ..., wT >), where each word wt is represented with a Kdimensional embedding (Mikolov et al., 2013), our goal is to"
C16-1153,W02-1011,0,0.0389416,"Missing"
C16-1153,D08-1102,0,0.0135616,"attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from monolingual and bilingual contexts with attention mechanisms. 2.2 Research on Code-switching and Bilingual Text Code-switched documents have received considerable attention in the NLP community (Adel et al., 2013; Garrette et al., 2015). Several studies have focused on code-switching identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting code-switched points (Solorio and Liu, 2008), identifying code-switched tokens (Lignos and Marcus, 2013), adding code-switched support to language models (Li and Fung, 2012), and POS tagging for code-switching text (Jamatia et al., 2015). There is relatively little work focus on predicting emotion in code-switching text. Wang et al. (2015) proposed a machine translation based approach to predict emotion in codeswitching text with various external resources. Our approach departs from the previous work that we model the task by considering monolingual and bilingual information in both lexical and document level with neural network model a"
C16-1153,D15-1167,0,0.00344898,"ers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and emotion analysis (Vo and Zhang, 2015; Zhang et al., 2015). dos Santos and Gatti (2014) proposed a character-based deep convolutional neural network to predict sentiment of short text. Tang et al. (2015) proposed a neural network model to learn vector-based document representation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open domains. Most of the previous studies focused on monolingual text, while our proposed bilingual attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from monolingual and bilingual contexts with attention mechanisms. 2.2 Research on Code-switching and Bilingual Text Code-switched documents have received"
C16-1153,P09-1027,0,0.0105089,"on, we do not use any external resource, such as bilingual and sentiment dictionary, to train our model. More remotely connected, multilingual natural language processing has attracted increasing attention in the computational linguistic community due to its broad real-world applications. Relevant studies have been reported in various natural language processing tasks, such as parsing (Burkett and Klein, 2008), information retrieval (Gao et al., 2009), text classification (Amini et al., 2010), and so on. There are a number of studies on predicting sentiment polarity through multilingual text. Wan (2009) incorporated unlabeled data in the target language into classifier with co-training to improve classification performance. Wei and Pal (2010) regarded cross-lingual sentiment classification as a domain adaptation task and applied structural correspondence learning (SCL) to tackle this problem. Their approach achieves a better performance than the co-training algorithm. More recently, Meng et al. (2012) employed the parallel corpus for cross-lingual sentiment classification. They explored the case when no labeled data is available in the parallel corpus. However, such multi-lingual models do n"
C16-1153,P15-2125,1,0.914845,"documents have received considerable attention in the NLP community (Adel et al., 2013; Garrette et al., 2015). Several studies have focused on code-switching identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting code-switched points (Solorio and Liu, 2008), identifying code-switched tokens (Lignos and Marcus, 2013), adding code-switched support to language models (Li and Fung, 2012), and POS tagging for code-switching text (Jamatia et al., 2015). There is relatively little work focus on predicting emotion in code-switching text. Wang et al. (2015) proposed a machine translation based approach to predict emotion in codeswitching text with various external resources. Our approach departs from the previous work that we model the task by considering monolingual and bilingual information in both lexical and document level with neural network model and attention mechanism, while previous research only focused on lexicallevel bilingual information. In addition, we do not use any external resource, such as bilingual and sentiment dictionary, to train our model. More remotely connected, multilingual natural language processing has attracted inc"
C16-1153,P10-2048,0,0.013418,"ilingual natural language processing has attracted increasing attention in the computational linguistic community due to its broad real-world applications. Relevant studies have been reported in various natural language processing tasks, such as parsing (Burkett and Klein, 2008), information retrieval (Gao et al., 2009), text classification (Amini et al., 2010), and so on. There are a number of studies on predicting sentiment polarity through multilingual text. Wan (2009) incorporated unlabeled data in the target language into classifier with co-training to improve classification performance. Wei and Pal (2010) regarded cross-lingual sentiment classification as a domain adaptation task and applied structural correspondence learning (SCL) to tackle this problem. Their approach achieves a better performance than the co-training algorithm. More recently, Meng et al. (2012) employed the parallel corpus for cross-lingual sentiment classification. They explored the case when no labeled data is available in the parallel corpus. However, such multi-lingual models do not explicitly consider code-switching, since their data sets are always parallel corpus. As the two languages are mixed in the code-switching"
C16-1153,P14-2069,0,0.0236212,"hen a sequence of tokens is relevant rather than simply filtering for sequences of tokens, taken out of context. Evaluation shows the effectiveness of our proposed BAN model with both monolingual and bilingual information. 2 2.1 Related Works Emotion Analysis Over the last decade, there has been much work exploring various aspects of emotion analysis (Wiebe et al., 2005). While most focused on analyzing emotions in monolingual text. Some of these studies emotion lexicon building, for example, Rao et al. (2012) automatically built a word-emotion mapping dictionary for social emotion detection, Yang et al. (2014) proposed a novel emotion-aware topic model to build a domain-specific lexicon. For emotion classification, Liu et al. (2013) used a co-training framework to infer the news from readers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and em"
C16-1153,N16-1174,0,0.0377432,"iations to the LSTM model, and we choose one for which the hidden state ht for each time-step t is given by: it = σ(W (i) xt + U (i) hi−1 + b(i) ) (1) ft = σ(W (f ) xt + U (f ) ht−1 + b(f ) ) (2) ot = σ(W (o) xt + U (o) ht−1 + b(o) ) (3) ut = tanh(W (u) + U (u) ht−1 + b(u) ) (4) ct = it ut + ft ct−1 (5) ht = ot tanh(ct ) (6) where σ denotes the sigmoid function. After the LSTM process, we obtain an annotation ht for a given word wt . 3.2 Attention Mechanism Not all words contribute equally to the representation of the meaning. Hence, we introduce an attention mechanism (Bahdanau et al., 2014; Yang et al., 2016) to extract the words that are important to the meaning of the post, and aggregate the representation of those informative words to form a vector. Since emotion can be expressed in either one or two languages in code-switching text, we build the vectors from monolingual and bilingual contexts respectively. For the monolingual case, we build two vectors v (cn) and v (en) to capture informative information from the Chinese and English contexts separately. For the bilingual case, we construct a vector v (bi) to capture the salient words from the mixed text. Bilingual Attention. We use an attentio"
C16-1153,D15-1073,1,0.85283,"c model to build a domain-specific lexicon. For emotion classification, Liu et al. (2013) used a co-training framework to infer the news from readers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and emotion analysis (Vo and Zhang, 2015; Zhang et al., 2015). dos Santos and Gatti (2014) proposed a character-based deep convolutional neural network to predict sentiment of short text. Tang et al. (2015) proposed a neural network model to learn vector-based document representation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open domains. Most of the previous studies focused on monolingual text, while our proposed bilingual attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from mono"
C16-1197,D11-1120,0,0.0891532,"Missing"
C16-1197,D13-1114,0,0.456469,"n is a fundamental task with regard to infer user’s gender from the user-generated data. Recently, this task is getting increasingly more attention in some prevailing research fields, such as social network analysis and natural language processing. Applications developed from gender classification have enormous commercial value in personalization, marketing and judicial investigation (Mukherjee and Liu, 2010; Burger et al., 2001; Volkova et al., 2013). In social media, conventional methods handle gender classification as a supervised learning problem over the past decade (Corney et al., 2002; Ciot et al., 2013). In supervised learning approaches, both user-generated textual and user social link features are verified to be effective for gender classification. For instance, in Figure 1, it is easy to infer User c to be a female through analyzing her saying “I&apos;m gonna be a mom!! ” Meanwhile, it is also possible to infer User c is more likely to be a female through analyzing her social link since she follows a cosmetic-selling User “Dior”. Although supervised methods have achieved remarkable success for gender classification, their good performances always depend on a large amount of labeled data, which"
C16-1197,D12-1135,0,0.0191123,"ction 3 introduces data collection and analysis. Section 4 describes our TSFG approach to gender classification. Section 5 presents the experimental results. Finally, Section 6 gives the conclusion and future work. 2 Related Work In the last decade, gender classification has been studied in two main aspects: supervised learning and semi-supervised learning. As for supervised learning, gender classification has been extensively studied in several textual styles, such as Blog (Nowson and Oberlander, 2006; Peersman et al., 2011; Gianfortoni et al., 2011), E-mail (Mohanmad et al., 2011), YouTube (Filippova, 2012) and Micro-blog (Rao et al., 2010; Liu et al., 2013). These studies mainly focus on employing various kinds of textual features such as character, word, POS features and their n-gram features to train the classifier. More recently, some studies focus on some specific application scenarios on supervised gender classification, such as multi-lingual gender classification (Ciot et al., 2013; Alowibdi et al., 2013) and interactive gender classification (Li et al., 2015). As for semi-supervised learning, gender classification has been studied with much less previous studies. Ikeda et al. (2008) prop"
C16-1197,W11-2606,0,0.378867,"llows. Section 2 overviews related work on gender classification. Section 3 introduces data collection and analysis. Section 4 describes our TSFG approach to gender classification. Section 5 presents the experimental results. Finally, Section 6 gives the conclusion and future work. 2 Related Work In the last decade, gender classification has been studied in two main aspects: supervised learning and semi-supervised learning. As for supervised learning, gender classification has been extensively studied in several textual styles, such as Blog (Nowson and Oberlander, 2006; Peersman et al., 2011; Gianfortoni et al., 2011), E-mail (Mohanmad et al., 2011), YouTube (Filippova, 2012) and Micro-blog (Rao et al., 2010; Liu et al., 2013). These studies mainly focus on employing various kinds of textual features such as character, word, POS features and their n-gram features to train the classifier. More recently, some studies focus on some specific application scenarios on supervised gender classification, such as multi-lingual gender classification (Ciot et al., 2013; Alowibdi et al., 2013) and interactive gender classification (Li et al., 2015). As for semi-supervised learning, gender classification has been studie"
C16-1197,D10-1021,0,0.379759,"el both the textual and the “same-interest” link information. Empirical studies demonstrate the effectiveness of the proposed approach to semi-supervised gender classification. 1 Introduction Gender classification is a fundamental task with regard to infer user’s gender from the user-generated data. Recently, this task is getting increasingly more attention in some prevailing research fields, such as social network analysis and natural language processing. Applications developed from gender classification have enormous commercial value in personalization, marketing and judicial investigation (Mukherjee and Liu, 2010; Burger et al., 2001; Volkova et al., 2013). In social media, conventional methods handle gender classification as a supervised learning problem over the past decade (Corney et al., 2002; Ciot et al., 2013). In supervised learning approaches, both user-generated textual and user social link features are verified to be effective for gender classification. For instance, in Figure 1, it is easy to infer User c to be a female through analyzing her saying “I&apos;m gonna be a mom!! ” Meanwhile, it is also possible to infer User c is more likely to be a female through analyzing her social link since she"
C16-1197,W11-1709,0,0.0283685,"Missing"
C16-1197,P15-1169,0,0.0878251,"Missing"
C16-1197,P11-1077,0,0.0453406,"Missing"
C16-1197,D13-1187,0,0.0660849,"nk information. Empirical studies demonstrate the effectiveness of the proposed approach to semi-supervised gender classification. 1 Introduction Gender classification is a fundamental task with regard to infer user’s gender from the user-generated data. Recently, this task is getting increasingly more attention in some prevailing research fields, such as social network analysis and natural language processing. Applications developed from gender classification have enormous commercial value in personalization, marketing and judicial investigation (Mukherjee and Liu, 2010; Burger et al., 2001; Volkova et al., 2013). In social media, conventional methods handle gender classification as a supervised learning problem over the past decade (Corney et al., 2002; Ciot et al., 2013). In supervised learning approaches, both user-generated textual and user social link features are verified to be effective for gender classification. For instance, in Figure 1, it is easy to infer User c to be a female through analyzing her saying “I&apos;m gonna be a mom!! ” Meanwhile, it is also possible to infer User c is more likely to be a female through analyzing her social link since she follows a cosmetic-selling User “Dior”. Alt"
C16-1199,D15-1141,0,0.0338907,"es demonstrate that our approach performs much better than many strong baseline approaches. Note that the motivation of employing LSTM as our single-perspective learning approach is that LSTM equips with a special gating mechanism that controls access to memory cells and it is powerful and effective at capturing long-term dependencies (Bengio et al., 1994). This advantage is helpful for modeling text and thus this approach has been successfully applied to a variety of NLP tasks, such as machine translation (Bahdanau et al., 2015), sentiment analysis (Tang et al., 2015), and sequence labeling (Chen et al., 2015). The remainder of this paper is organized as follows. Section 2 overviews related work on user classification. Section 3 introduces data collection. Section 4 proposes our multi-perspective ensemble LSTM approach with multiple textual perspectives for user classification. Section 5 evaluates our approach with a benchmark dataset. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, many previous studies have been devoted to the research on user classification with multiple attributes, such as user gender and user age. User gender classification has bee"
C16-1199,D13-1114,0,0.0509071,"ection 6 gives the conclusion and future work. 2 Related Work Over the last decade, many previous studies have been devoted to the research on user classification with multiple attributes, such as user gender and user age. User gender classification has been extensively studied in several domains, such as Blog (Peersman et al., 2011; Gianfortoni et al., 2011), E-mail (Mohanmad et al., 2011), YouTube (Filippova, 2012) and Micro-blog (Liu et al., 2013). More recently, some studies focus on some specific application scenarios on gender classification, such as multi-lingual gender classification (Ciot et al., 2013; Alowibdi et al., 2013), inferring gender by crowd (Nguyen et al., 2014) and interactive gender classification (Li et al., 2015). User age classification has been studied in two main domains, i.e., blog (Burger and Hender son, 2006) and social media (Machinnon and Warren, 2006). In the blog domain, Schler et al. (2006) focus on textual features extracted from the blog text, such as word context features and POS stylistic features. Burger and Henderson (2006) explore some social features, such as location, time, and friend features, related to blogger age. Other studies, such as Rosenthal and"
C16-1199,D12-1135,0,0.228565,"n. Section 4 proposes our multi-perspective ensemble LSTM approach with multiple textual perspectives for user classification. Section 5 evaluates our approach with a benchmark dataset. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, many previous studies have been devoted to the research on user classification with multiple attributes, such as user gender and user age. User gender classification has been extensively studied in several domains, such as Blog (Peersman et al., 2011; Gianfortoni et al., 2011), E-mail (Mohanmad et al., 2011), YouTube (Filippova, 2012) and Micro-blog (Liu et al., 2013). More recently, some studies focus on some specific application scenarios on gender classification, such as multi-lingual gender classification (Ciot et al., 2013; Alowibdi et al., 2013), inferring gender by crowd (Nguyen et al., 2014) and interactive gender classification (Li et al., 2015). User age classification has been studied in two main domains, i.e., blog (Burger and Hender son, 2006) and social media (Machinnon and Warren, 2006). In the blog domain, Schler et al. (2006) focus on textual features extracted from the blog text, such as word context feat"
C16-1199,W11-2606,0,0.248406,"ted work on user classification. Section 3 introduces data collection. Section 4 proposes our multi-perspective ensemble LSTM approach with multiple textual perspectives for user classification. Section 5 evaluates our approach with a benchmark dataset. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, many previous studies have been devoted to the research on user classification with multiple attributes, such as user gender and user age. User gender classification has been extensively studied in several domains, such as Blog (Peersman et al., 2011; Gianfortoni et al., 2011), E-mail (Mohanmad et al., 2011), YouTube (Filippova, 2012) and Micro-blog (Liu et al., 2013). More recently, some studies focus on some specific application scenarios on gender classification, such as multi-lingual gender classification (Ciot et al., 2013; Alowibdi et al., 2013), inferring gender by crowd (Nguyen et al., 2014) and interactive gender classification (Li et al., 2015). User age classification has been studied in two main domains, i.e., blog (Burger and Hender son, 2006) and social media (Machinnon and Warren, 2006). In the blog domain, Schler et al. (2006) focus on textual featu"
C16-1199,P09-1078,1,0.803869,"see that our ensemble LSTM approach performs best and it is also performs better than other strong ensemble strategies with multiple textual perspectives, such as Voting LSTM and Weighted_Sum LSTM. Significance test shows that our ensemble LSTM approach significantly outperforms other approaches when multiple textual perspectives are used (p-value<0.05). 5.3 Effectiveness Analysis and Case Study In order to further illustrate the superiority of our approach, we give a case study as following. Table 8 shows the selected features sorted by the feature selection method of information gain (IG) (Li et al., 2009) when the task of gender classification is considered. We extract the features from the original message text and the retweeted message text separately. This table shows the top-10 IG features from the original message text and their ranks in the retweeted message text. N denotes the sequence number of the feature in the selected features. Ff denotes the feature frequency in all samples of female. Fm denotes the feature frequency in all samples of male. For instance, the sequence number of emoticon “rabbit” in original message is the first, the feature frequency in all samples of female is 587"
C16-1199,W11-1709,0,0.0673424,"Missing"
C16-1199,D10-1021,0,0.115059,"Missing"
C16-1199,C14-1184,0,0.203088,"Missing"
C16-1199,P11-1077,0,0.0182223,"t et al., 2013; Alowibdi et al., 2013), inferring gender by crowd (Nguyen et al., 2014) and interactive gender classification (Li et al., 2015). User age classification has been studied in two main domains, i.e., blog (Burger and Hender son, 2006) and social media (Machinnon and Warren, 2006). In the blog domain, Schler et al. (2006) focus on textual features extracted from the blog text, such as word context features and POS stylistic features. Burger and Henderson (2006) explore some social features, such as location, time, and friend features, related to blogger age. Other studies, such as Rosenthal and McKeown (2011) and Goswami et al. (2009) explore both the textual and social features in automatic age classification. In the social media domain, Mackinnon and Warren (2006) explore some kind of social features, i.e., the relationship between users to predict a user’s age and country of residence in a social network. Peersman et al. (2011) apply a text categorization approach to age classification with textual features only, i.e., word unigrams and bigrams. More recently, Marquardt et al. (2014) propose a multi-label classification approach to predict both the gender and age of authors from texts. Specific"
C16-1199,D14-1121,0,0.320716,"Missing"
C16-1199,D15-1167,0,0.0176011,"fuse all textual knowledge. Empirical studies demonstrate that our approach performs much better than many strong baseline approaches. Note that the motivation of employing LSTM as our single-perspective learning approach is that LSTM equips with a special gating mechanism that controls access to memory cells and it is powerful and effective at capturing long-term dependencies (Bengio et al., 1994). This advantage is helpful for modeling text and thus this approach has been successfully applied to a variety of NLP tasks, such as machine translation (Bahdanau et al., 2015), sentiment analysis (Tang et al., 2015), and sequence labeling (Chen et al., 2015). The remainder of this paper is organized as follows. Section 2 overviews related work on user classification. Section 3 introduces data collection. Section 4 proposes our multi-perspective ensemble LSTM approach with multiple textual perspectives for user classification. Section 5 evaluates our approach with a benchmark dataset. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, many previous studies have been devoted to the research on user classification with multiple attributes, such as user gender and u"
C16-1249,H05-1073,0,0.640009,"sponse text samples. The remainder of this paper is organized as follows. Section 2 overviews related work on emotion classification. Section 3 introduces the baseline approach to semi-supervised reader emotion classification with single-view label propagation. Section 4 presents our two-view label propagation approach to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unlabeled source-text data Ur Unlab"
C16-1249,C10-1021,1,0.714715,"ta Lr Labeled response-text data Us Unlabeled source-text data Ur Unlabeled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition probability matrix among the source text data The transition probability matrix among the response text data The transition probability matrix among the source and response-text data Mr M s ,r Table 1: Symbol definition LP Figure 2: The framework of single-view label propagation approach to semi-supervised learning on reader emotion classification 2005; Chen et al., 2010; Purver and Battersby, 2012; Hasegawa et al., 2013; Qadir and Riloff, 2014), mainly from the supervised learning paradigm. Compared with above studies on writer emotion classification, studies on reader emotion classification are much limited. Lin et al. (2007) first describe the task of reader emotion classification on news articles with some standard machine learning approaches. Lin et al. (2008) further exploit more features to improve the performance. More recently, Liu et al. (2013) propose a co-training approach to semi-supervised learning on reader emotion classification by considering"
C16-1249,P09-2038,0,0.0160824,"to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unlabeled source-text data Ur Unlabeled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition probability matrix among the source text data The transition probability matrix among the response text data The transition probability matrix among the source"
C16-1249,P13-1095,0,0.0255958,"ource-text data Ur Unlabeled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition probability matrix among the source text data The transition probability matrix among the response text data The transition probability matrix among the source and response-text data Mr M s ,r Table 1: Symbol definition LP Figure 2: The framework of single-view label propagation approach to semi-supervised learning on reader emotion classification 2005; Chen et al., 2010; Purver and Battersby, 2012; Hasegawa et al., 2013; Qadir and Riloff, 2014), mainly from the supervised learning paradigm. Compared with above studies on writer emotion classification, studies on reader emotion classification are much limited. Lin et al. (2007) first describe the task of reader emotion classification on news articles with some standard machine learning approaches. Lin et al. (2008) further exploit more features to improve the performance. More recently, Liu et al. (2013) propose a co-training approach to semi-supervised learning on reader emotion classification by considering the message text and the comment text as two views"
C16-1249,P13-2091,1,0.759792,"classification due to its importance in more and more real-life applications, such as content recommendation and online advertisement. Conventional approaches to reader emotion classification conceptualize the task as a supervised learning problem and rely on a large-scale human-annotated data for model learning. Although such supervised approaches deliver reasonably good performance, the reliance on labeled data, which is normally difficult and highly expensive to obtain, presents a major obstacle to the widespread application of reader emotion classification. To alleviate the problem above, Liu et al. (2013) originally propose a semi-supervised learning approach to reader emotion classification to improve the performance by enlarging the labeled data with automatically inferred annotations of unlabeled instances. Their basic idea mainly lies on unique characteristics in reader emotion analysis, different from the case in writer emotion analysis. That is, apart from the source text (e.g., news text), another type of text, the response text (e.g., comment text) written by the reader as a response to the source text, is available to help determine the reader emotion of the source text. For example,"
C16-1249,W02-1011,0,0.0223043,"ability between the source and response text samples. The remainder of this paper is organized as follows. Section 2 overviews related work on emotion classification. Section 3 introduces the baseline approach to semi-supervised reader emotion classification with single-view label propagation. Section 4 presents our two-view label propagation approach to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unl"
C16-1249,E12-1049,0,0.0178318,"nse-text data Us Unlabeled source-text data Ur Unlabeled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition probability matrix among the source text data The transition probability matrix among the response text data The transition probability matrix among the source and response-text data Mr M s ,r Table 1: Symbol definition LP Figure 2: The framework of single-view label propagation approach to semi-supervised learning on reader emotion classification 2005; Chen et al., 2010; Purver and Battersby, 2012; Hasegawa et al., 2013; Qadir and Riloff, 2014), mainly from the supervised learning paradigm. Compared with above studies on writer emotion classification, studies on reader emotion classification are much limited. Lin et al. (2007) first describe the task of reader emotion classification on news articles with some standard machine learning approaches. Lin et al. (2008) further exploit more features to improve the performance. More recently, Liu et al. (2013) propose a co-training approach to semi-supervised learning on reader emotion classification by considering the message text and the co"
C16-1249,D14-1127,0,0.0137017,"beled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition probability matrix among the source text data The transition probability matrix among the response text data The transition probability matrix among the source and response-text data Mr M s ,r Table 1: Symbol definition LP Figure 2: The framework of single-view label propagation approach to semi-supervised learning on reader emotion classification 2005; Chen et al., 2010; Purver and Battersby, 2012; Hasegawa et al., 2013; Qadir and Riloff, 2014), mainly from the supervised learning paradigm. Compared with above studies on writer emotion classification, studies on reader emotion classification are much limited. Lin et al. (2007) first describe the task of reader emotion classification on news articles with some standard machine learning approaches. Lin et al. (2008) further exploit more features to improve the performance. More recently, Liu et al. (2013) propose a co-training approach to semi-supervised learning on reader emotion classification by considering the message text and the comment text as two views. However, their success"
C16-1249,D09-1150,0,0.111693,"on approach to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unlabeled source-text data Ur Unlabeled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition probability matrix among the source text data The transition probability matrix among the response text data The transition proba"
C16-1249,P14-2070,0,0.0266015,"ith single-view label propagation. Section 4 presents our two-view label propagation approach to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unlabeled source-text data Ur Unlabeled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition probability matrix among the source text data The tr"
C16-1249,P02-1053,0,0.0181175,"source and response text samples. The remainder of this paper is organized as follows. Section 2 overviews related work on emotion classification. Section 3 introduces the baseline approach to semi-supervised reader emotion classification with single-view label propagation. Section 4 presents our two-view label propagation approach to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unlabeled source-"
C16-1249,E12-1031,0,0.0194119,"ed reader emotion classification with single-view label propagation. Section 4 presents our two-view label propagation approach to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unlabeled source-text data Ur Unlabeled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition probability matrix among"
C16-1249,J09-3003,0,0.019718,"s. The remainder of this paper is organized as follows. Section 2 overviews related work on emotion classification. Section 3 introduces the baseline approach to semi-supervised reader emotion classification with single-view label propagation. Section 4 presents our two-view label propagation approach to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unlabeled source-text data Ur Unlabeled response-text dat"
C16-1249,C10-1136,0,0.0258978,"to semi-supervised reader emotion classification with single-view label propagation. Section 4 presents our two-view label propagation approach to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unlabeled source-text data Ur Unlabeled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition pr"
C16-1310,C10-1021,1,0.82193,"is proposed to refine the classification results. Empirical studies show the effectiveness of the proposed approach to corpus fusion for emotion classification. 1 Introduction Emotion classification aims to recognize human emotions, such as joy, anger or surprise in a given text. Emotion classification has a variety of applications including online chatting (Galik and Rank, 2012), news classification (Liu et al., 2013) and stock marketing (Bollen et al., 2011). In recent years, emotion classification in social media has been greatly popular in the Natural Language Processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012; Li et al., 2015). Because of the popularity of social media today, the analysis of short text on social media becomes more important (Kiritchenko et al., 2014; Wen and Wan, 2014; Wang et al., 2015). Users express their feelings and emotions on various social media platforms. Existing emotion classification approaches are based on corpus classification methods where humanannotated emotion corpora are leveraged to train a machine learning-based emotion classification models. Recently, several different emotion corpora have been proposed by different researchers, suc"
C16-1310,P04-1059,0,0.0359125,"re LDA model to build a domainspecific lexicon. Xu et al. (2010) use language resource, such as synonym dictionary, semantic dictionary, and labeled and unlabeled corpus to construct the similarity matrix between words and seed words. They build an emotion lexicon with five emotion classes using graph-based rules. As a special expression of words, emoticons play an important role in emotion analysis due to the explosion in social media. Tang et al. (2013) annotate data from microblog posts with the help of emoticons. There are several studies to address corpus adaptation problem in NLP field. Gao et al. (2004) do a pioneer work by describing a transformation-based converter to transfer a certain word segmentation result to another annotation guideline. Jiang et al. (2009) investigate the automatic integration of word segmentation knowledge in different annotated corpora. Similar approaches are applied to constituency parsing (Zhu et al., 2011) and word segmentation (Sun and Wan, 2012) Unlike all above studies, we propose a corpus fusion approach to emotion classification in order to address the corpus fusion problem to combine two corpora with different emotion taxonomies and annotation guidelines."
C16-1310,P09-1059,0,0.0257711,"corpus to construct the similarity matrix between words and seed words. They build an emotion lexicon with five emotion classes using graph-based rules. As a special expression of words, emoticons play an important role in emotion analysis due to the explosion in social media. Tang et al. (2013) annotate data from microblog posts with the help of emoticons. There are several studies to address corpus adaptation problem in NLP field. Gao et al. (2004) do a pioneer work by describing a transformation-based converter to transfer a certain word segmentation result to another annotation guideline. Jiang et al. (2009) investigate the automatic integration of word segmentation knowledge in different annotated corpora. Similar approaches are applied to constituency parsing (Zhu et al., 2011) and word segmentation (Sun and Wan, 2012) Unlike all above studies, we propose a corpus fusion approach to emotion classification in order to address the corpus fusion problem to combine two corpora with different emotion taxonomies and annotation guidelines. To the best of our knowledge, this is the first attempt to address this task in emotion analysis. 3 Corpus Two emotion corpora we used are respectively constructed"
C16-1310,P15-1101,1,0.814442,"ts. Empirical studies show the effectiveness of the proposed approach to corpus fusion for emotion classification. 1 Introduction Emotion classification aims to recognize human emotions, such as joy, anger or surprise in a given text. Emotion classification has a variety of applications including online chatting (Galik and Rank, 2012), news classification (Liu et al., 2013) and stock marketing (Bollen et al., 2011). In recent years, emotion classification in social media has been greatly popular in the Natural Language Processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012; Li et al., 2015). Because of the popularity of social media today, the analysis of short text on social media becomes more important (Kiritchenko et al., 2014; Wen and Wan, 2014; Wang et al., 2015). Users express their feelings and emotions on various social media platforms. Existing emotion classification approaches are based on corpus classification methods where humanannotated emotion corpora are leveraged to train a machine learning-based emotion classification models. Recently, several different emotion corpora have been proposed by different researchers, such as Yao et al. (2014) and Huang et al. (2015)"
C16-1310,P13-2091,1,0.849112,"taxonomies. The objective of this approach is to utilize the annotated data from one corpus to help the emotion classification on another corpus. An Integer Linear Programming (ILP) optimization is proposed to refine the classification results. Empirical studies show the effectiveness of the proposed approach to corpus fusion for emotion classification. 1 Introduction Emotion classification aims to recognize human emotions, such as joy, anger or surprise in a given text. Emotion classification has a variety of applications including online chatting (Galik and Rank, 2012), news classification (Liu et al., 2013) and stock marketing (Bollen et al., 2011). In recent years, emotion classification in social media has been greatly popular in the Natural Language Processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012; Li et al., 2015). Because of the popularity of social media today, the analysis of short text on social media becomes more important (Kiritchenko et al., 2014; Wen and Wan, 2014; Wang et al., 2015). Users express their feelings and emotions on various social media platforms. Existing emotion classification approaches are based on corpus classification methods where humanann"
C16-1310,W10-0204,0,0.0385772,"ruct an emotion from TENCENT microblog including both simple and complex emotion annotation. According to the text granularity, emotion analysis works can be generally divided into three levels: document-level, sentence-level, and word-level. Gilad (2005) uses SVM to model a document-level 3288 emotion classifier with blog articles. Yang et al. (2007) identify the emotion types of blog articles based on SVM and CRF with sentiment lexicon. Lin et al. (2007) use the articles on Yahoo! News to analysis the news readers’ emotion. Sentence-level emotion analysis is mainly based on emotion lexicon. Mohammad and Turney (2010) study the effect of word level emotion lexicons for sentence level emotion analysis. They use word level emotion lexicons based on Word Net and NRC-10 to predict the emotion in sentences with Logistic Regression and SVM. Das and Bandyopadhyay (2010) categorize the emotions on Bengali blog. They first identify the emotion of words in a sentence, then judge the emotion of this sentence according to the words’ emotion. Aman and Szpakowicz (2007) implement a knowledge-based sentence level emotion recognition method. Word-level emotion analysis aims to construct emotion lexicon, which plays an imp"
C16-1310,pak-paroubek-2010-twitter,0,0.0477381,"on for emotion classification. Section 5 illustrates the experiments to evaluate the proposed approach. Section 6 gives the conclusion and future work. 2 Related Work In last decade, mainstream approaches for emotion analysis are corpus-based machine learning methods. Several studies construct emotion corpus from social media platform such as blog, microblog and news portal. Gilad (2005) collects blog texts from LiveJournal to construct an emotion corpus with 815,494 blog articles. Quan and Ren (2009) build an emotion corpus from blogs with eight types of emotions on three granularity levels. Pak and Paroubek (2010) establish an emotion corpus by capturing tweets on Twitter. Yao et al. (2014) build an emotion corpus with seven emotion types from SINA microblog. Huang et al. (2015) construct an emotion from TENCENT microblog including both simple and complex emotion annotation. According to the text granularity, emotion analysis works can be generally divided into three levels: document-level, sentence-level, and word-level. Gilad (2005) uses SVM to model a document-level 3288 emotion classifier with blog articles. Yang et al. (2007) identify the emotion types of blog articles based on SVM and CRF with se"
C16-1310,E12-1049,0,0.0226259,"ine the classification results. Empirical studies show the effectiveness of the proposed approach to corpus fusion for emotion classification. 1 Introduction Emotion classification aims to recognize human emotions, such as joy, anger or surprise in a given text. Emotion classification has a variety of applications including online chatting (Galik and Rank, 2012), news classification (Liu et al., 2013) and stock marketing (Bollen et al., 2011). In recent years, emotion classification in social media has been greatly popular in the Natural Language Processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012; Li et al., 2015). Because of the popularity of social media today, the analysis of short text on social media becomes more important (Kiritchenko et al., 2014; Wen and Wan, 2014; Wang et al., 2015). Users express their feelings and emotions on various social media platforms. Existing emotion classification approaches are based on corpus classification methods where humanannotated emotion corpora are leveraged to train a machine learning-based emotion classification models. Recently, several different emotion corpora have been proposed by different researchers, such as Yao et al. (2014) and H"
C16-1310,D09-1150,0,0.0296881,"ated studies. Section 3 introduces two corpora used in this paper. Section 4 proposes the approach to corpus fusion for emotion classification. Section 5 illustrates the experiments to evaluate the proposed approach. Section 6 gives the conclusion and future work. 2 Related Work In last decade, mainstream approaches for emotion analysis are corpus-based machine learning methods. Several studies construct emotion corpus from social media platform such as blog, microblog and news portal. Gilad (2005) collects blog texts from LiveJournal to construct an emotion corpus with 815,494 blog articles. Quan and Ren (2009) build an emotion corpus from blogs with eight types of emotions on three granularity levels. Pak and Paroubek (2010) establish an emotion corpus by capturing tweets on Twitter. Yao et al. (2014) build an emotion corpus with seven emotion types from SINA microblog. Huang et al. (2015) construct an emotion from TENCENT microblog including both simple and complex emotion annotation. According to the text granularity, emotion analysis works can be generally divided into three levels: document-level, sentence-level, and word-level. Gilad (2005) uses SVM to model a document-level 3288 emotion class"
C16-1310,W04-2401,0,0.00977023,"p(rYi = eY2 ) ...p(rYi = eY7 )}, PHi = {p(rHi = eH1 ), p(rHi = eH2 ) ...p(rHi = eH7 )} (2) where p(rYi = eY1 ) denotes the probability of ti belonging to happiness under the emotion taxonomy of YAO (2014), and p(rHi = eH1 ) denotes the probability of ti belonging to joy under the emotion taxonomy of HUANG (2015). The rest can be done in the same manner. 2 http://t.qq.com/ 3290 4.2 Global Optimization with ILP ILP optimization aims to refine the label result given the probability result. We design objective function and constraints to exploit the similarity between two emotion taxonomies. Like Roth and Yih (2004), we firstly define following assignment costs: cYi = −log(p(rYi = eYi )) + log(1 − p(rYi = eYi )), cHi = −log(p(rHi = eHi )) + log(1 − p(rHi = eHi )), (3) 1 ≤i ≤ 7 where cYi is the cost of ti belonging to the ith emotion class under the taxonomy of YAO (2014), and cHi is the cost of ti belonging to the ith emotion class under the taxonomy of HUANG (2015). For each sample ti in testing set there can be two cost vectors CY and CH , and two label vectors LY and LH used on storing the refined labels of ti : CY = [c Y1 c Y2 ... c Y7 ]T , LY = [y1 y2 ... y7 ], CH = [c H1 c H2 ... c H7 ]T LH = [z1 z"
C16-1310,P12-1025,0,0.0164327,"e in emotion analysis due to the explosion in social media. Tang et al. (2013) annotate data from microblog posts with the help of emoticons. There are several studies to address corpus adaptation problem in NLP field. Gao et al. (2004) do a pioneer work by describing a transformation-based converter to transfer a certain word segmentation result to another annotation guideline. Jiang et al. (2009) investigate the automatic integration of word segmentation knowledge in different annotated corpora. Similar approaches are applied to constituency parsing (Zhu et al., 2011) and word segmentation (Sun and Wan, 2012) Unlike all above studies, we propose a corpus fusion approach to emotion classification in order to address the corpus fusion problem to combine two corpora with different emotion taxonomies and annotation guidelines. To the best of our knowledge, this is the first attempt to address this task in emotion analysis. 3 Corpus Two emotion corpora we used are respectively constructed by Yao et al. (2014) and Huang et al. (2015). We simply denote the two corpora as YAO (2014) and HUANG (2015) in the rest of this paper for convenience. YAO (2014) is constructed from SINA microblog1 . It categorizes"
C16-1310,P15-2125,1,0.837377,"otions, such as joy, anger or surprise in a given text. Emotion classification has a variety of applications including online chatting (Galik and Rank, 2012), news classification (Liu et al., 2013) and stock marketing (Bollen et al., 2011). In recent years, emotion classification in social media has been greatly popular in the Natural Language Processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012; Li et al., 2015). Because of the popularity of social media today, the analysis of short text on social media becomes more important (Kiritchenko et al., 2014; Wen and Wan, 2014; Wang et al., 2015). Users express their feelings and emotions on various social media platforms. Existing emotion classification approaches are based on corpus classification methods where humanannotated emotion corpora are leveraged to train a machine learning-based emotion classification models. Recently, several different emotion corpora have been proposed by different researchers, such as Yao et al. (2014) and Huang et al. (2015). However, the size of each existing labeled corpus might be rather limited due to the high cost of data annotation, which results low performance in traditional supervised emotion"
C16-1310,C10-1136,0,0.0331506,"on Word Net and NRC-10 to predict the emotion in sentences with Logistic Regression and SVM. Das and Bandyopadhyay (2010) categorize the emotions on Bengali blog. They first identify the emotion of words in a sentence, then judge the emotion of this sentence according to the words’ emotion. Aman and Szpakowicz (2007) implement a knowledge-based sentence level emotion recognition method. Word-level emotion analysis aims to construct emotion lexicon, which plays an important auxiliary role in emotion analysis. Yang et al. (2014) propose Emotion-aware LDA model to build a domainspecific lexicon. Xu et al. (2010) use language resource, such as synonym dictionary, semantic dictionary, and labeled and unlabeled corpus to construct the similarity matrix between words and seed words. They build an emotion lexicon with five emotion classes using graph-based rules. As a special expression of words, emoticons play an important role in emotion analysis due to the explosion in social media. Tang et al. (2013) annotate data from microblog posts with the help of emoticons. There are several studies to address corpus adaptation problem in NLP field. Gao et al. (2004) do a pioneer work by describing a transformati"
C16-1310,P14-2069,0,0.025824,"icons for sentence level emotion analysis. They use word level emotion lexicons based on Word Net and NRC-10 to predict the emotion in sentences with Logistic Regression and SVM. Das and Bandyopadhyay (2010) categorize the emotions on Bengali blog. They first identify the emotion of words in a sentence, then judge the emotion of this sentence according to the words’ emotion. Aman and Szpakowicz (2007) implement a knowledge-based sentence level emotion recognition method. Word-level emotion analysis aims to construct emotion lexicon, which plays an important auxiliary role in emotion analysis. Yang et al. (2014) propose Emotion-aware LDA model to build a domainspecific lexicon. Xu et al. (2010) use language resource, such as synonym dictionary, semantic dictionary, and labeled and unlabeled corpus to construct the similarity matrix between words and seed words. They build an emotion lexicon with five emotion classes using graph-based rules. As a special expression of words, emoticons play an important role in emotion analysis due to the explosion in social media. Tang et al. (2013) annotate data from microblog posts with the help of emoticons. There are several studies to address corpus adaptation pr"
C16-1310,P11-2126,0,0.0253018,"of words, emoticons play an important role in emotion analysis due to the explosion in social media. Tang et al. (2013) annotate data from microblog posts with the help of emoticons. There are several studies to address corpus adaptation problem in NLP field. Gao et al. (2004) do a pioneer work by describing a transformation-based converter to transfer a certain word segmentation result to another annotation guideline. Jiang et al. (2009) investigate the automatic integration of word segmentation knowledge in different annotated corpora. Similar approaches are applied to constituency parsing (Zhu et al., 2011) and word segmentation (Sun and Wan, 2012) Unlike all above studies, we propose a corpus fusion approach to emotion classification in order to address the corpus fusion problem to combine two corpora with different emotion taxonomies and annotation guidelines. To the best of our knowledge, this is the first attempt to address this task in emotion analysis. 3 Corpus Two emotion corpora we used are respectively constructed by Yao et al. (2014) and Huang et al. (2015). We simply denote the two corpora as YAO (2014) and HUANG (2015) in the rest of this paper for convenience. YAO (2014) is construc"
C18-1119,P07-1056,0,0.0228053,"cting the user-pivotal word network is beneficial for learning a better user embedding for cross-media user profiling. 4.3 Performance Comparison For thorough comparison, several approaches are implemented for cross-media user profiling: • Baseline-SVM(BOW): a SVM classifier (or SVR regressor) trained with only labeled data in the source media and the representation model is BOW. This approach is proposed by Li et al. (2015). • Baseline-SCL(BOW): a famous textual domain adaptation approach named SCL which bridges the knowledge between the source and target domains using some pivotal features (Blitzer et al., 2007) and the representation model is BOW. We consider the source media as the source domain and the target media as the target domain. The number of the pivotal features is set to be 200. • Baseline-SDA(BOW): another famous textual domain adaptation approach with stacked denoising auto-encoder to extract meaningful representation (Glorot et al., 2011) and the representation model is BOW. Similar to SCL, we consider the source media as the source domain and the target media as the target domain. • Baseline-LSTM(Word Embeddings): a LSTM classification (or regression) model using Word Embeddings as i"
C18-1119,D13-1114,0,0.123047,"Wang et al., 2017; Li et al., 2016; Li et al., 2015) and age (Marquardt et al., 2014) identification. Recently, along with the boom of social media, user profiling has been drawing more and more attention in various social applications, such as personality analysis (Li et al., 2016; Sarawgi et al., 2011; O’Connor et al., 2010), intelligent marking (Preotiuc-Pietro et al., 2015) and online advertising (Zhang et al., 2016; Volkova et al., 2013). In the literature, conventional approaches normally recast user profiling as a supervised learning problem (Marquardt et al., 2014; Zhang et al., 2016; Ciot et al., 2013; Corney et al., 2002) by exploring various user generated textual features and user social connection features. However, the performance largely depends on a large amount of labeled data and the performance normally degrades dramatically when the model is tested on a different social media. Even worse, many real scenarios may involve several social media with some of them lacking sufficient labeled data to train a well-performed model in each social media. For example, Facebook.com is a social media site where many users publicize their age attribute in their homepages, making the collection"
C18-1119,W11-2606,0,0.0258669,"Missing"
C18-1119,C16-1197,1,0.748289,"ormation. Then, we learn user embedding by jointly learning the heterogeneous network composed of above two networks. Finally, we train a classification (or regression) model with the obtained user embeddings as input to perform user profiling. Empirical studies demonstrate the effectiveness of the proposed approach to two cross-media user profiling tasks, i.e., cross-media gender classification and cross-media age regression. 1 Introduction User profiling is a task which leverages user generated content (UGC) to automatically identify the data about a user, such as gender (Wang et al., 2017; Li et al., 2016; Li et al., 2015) and age (Marquardt et al., 2014) identification. Recently, along with the boom of social media, user profiling has been drawing more and more attention in various social applications, such as personality analysis (Li et al., 2016; Sarawgi et al., 2011; O’Connor et al., 2010), intelligent marking (Preotiuc-Pietro et al., 2015) and online advertising (Zhang et al., 2016; Volkova et al., 2013). In the literature, conventional approaches normally recast user profiling as a supervised learning problem (Marquardt et al., 2014; Zhang et al., 2016; Ciot et al., 2013; Corney et al.,"
C18-1119,D10-1021,0,0.0372759,"der identification and age identification) in several research communities, such as natural language processing and social network analysis. For the gender identification task, Mohammad and Yang (2013) show that there are marked differences across genders in how they use emotion words in work-place email. Ciot et al. (2013) conduct the first assessment of latent attribute inference in various languages beyond English, focusing on gender inference of Twitter users. Li et al. (2015) aim to identify the genders of two interactive users on the basis of micro-blog text. Some other studies, such as Mukherjee and Liu (2010), Peersman et al. (2011) and Gianfortoni et al. (2011) focus on exploring more effective features to improve the performance. Wang et al. (2017) propose a joint learning approach in order to leverage the relationship features among relevant user attributes. For the age identification task, most studies are devoted to explore efficient features in blog and social media. Schler et al. (2006) focus on textual features extracted from the blog text, such as word context features and POS stylistic features. Peersman et al. (2011) apply a text categorization approach to age classification with textua"
C18-1119,P15-1169,0,0.033224,"edia user profiling tasks, i.e., cross-media gender classification and cross-media age regression. 1 Introduction User profiling is a task which leverages user generated content (UGC) to automatically identify the data about a user, such as gender (Wang et al., 2017; Li et al., 2016; Li et al., 2015) and age (Marquardt et al., 2014) identification. Recently, along with the boom of social media, user profiling has been drawing more and more attention in various social applications, such as personality analysis (Li et al., 2016; Sarawgi et al., 2011; O’Connor et al., 2010), intelligent marking (Preotiuc-Pietro et al., 2015) and online advertising (Zhang et al., 2016; Volkova et al., 2013). In the literature, conventional approaches normally recast user profiling as a supervised learning problem (Marquardt et al., 2014; Zhang et al., 2016; Ciot et al., 2013; Corney et al., 2002) by exploring various user generated textual features and user social connection features. However, the performance largely depends on a large amount of labeled data and the performance normally degrades dramatically when the model is tested on a different social media. Even worse, many real scenarios may involve several social media with"
C18-1119,D14-1121,0,0.0138118,"man et al. (2011) apply a text categorization approach to age classification with textual features extracted from the text in social media. More recently, Marquardt et al. (2014) propose a multi-label classification approach to predict both the gender and age of authors from texts adopting some sentiment and emotion features. Differently, in this paper, we cast age identification task as a regression problem instead of a classification problem. Different from all above studies, we focus on the cross-media user profiling task. To the best of our knowledge, there are only two related papers by (Sap et al., 2014; Pardo et al., 2016) which mentions the cross-media user profiling issue. In their paper, only textual information is used in cross-media user profiling. Unlike their study, this paper firstly employs both textual and social information in cross-media user profiling. 3 3.1 Our Approach Framework Overview Our approach consists of two main phases, i.e., the representation learning phase and the classification (or regression) phase, which has been illustrated in Figure 4. In the representation phase, we first construct two different types of cross-media networks, i.e., crossmedia user-word netwo"
C18-1119,W11-0310,0,0.0238514,"monstrate the effectiveness of the proposed approach to two cross-media user profiling tasks, i.e., cross-media gender classification and cross-media age regression. 1 Introduction User profiling is a task which leverages user generated content (UGC) to automatically identify the data about a user, such as gender (Wang et al., 2017; Li et al., 2016; Li et al., 2015) and age (Marquardt et al., 2014) identification. Recently, along with the boom of social media, user profiling has been drawing more and more attention in various social applications, such as personality analysis (Li et al., 2016; Sarawgi et al., 2011; O’Connor et al., 2010), intelligent marking (Preotiuc-Pietro et al., 2015) and online advertising (Zhang et al., 2016; Volkova et al., 2013). In the literature, conventional approaches normally recast user profiling as a supervised learning problem (Marquardt et al., 2014; Zhang et al., 2016; Ciot et al., 2013; Corney et al., 2002) by exploring various user generated textual features and user social connection features. However, the performance largely depends on a large amount of labeled data and the performance normally degrades dramatically when the model is tested on a different social m"
C18-1119,D13-1187,0,0.140753,"ss-media age regression. 1 Introduction User profiling is a task which leverages user generated content (UGC) to automatically identify the data about a user, such as gender (Wang et al., 2017; Li et al., 2016; Li et al., 2015) and age (Marquardt et al., 2014) identification. Recently, along with the boom of social media, user profiling has been drawing more and more attention in various social applications, such as personality analysis (Li et al., 2016; Sarawgi et al., 2011; O’Connor et al., 2010), intelligent marking (Preotiuc-Pietro et al., 2015) and online advertising (Zhang et al., 2016; Volkova et al., 2013). In the literature, conventional approaches normally recast user profiling as a supervised learning problem (Marquardt et al., 2014; Zhang et al., 2016; Ciot et al., 2013; Corney et al., 2002) by exploring various user generated textual features and user social connection features. However, the performance largely depends on a large amount of labeled data and the performance normally degrades dramatically when the model is tested on a different social media. Even worse, many real scenarios may involve several social media with some of them lacking sufficient labeled data to train a well-perfo"
C18-1119,C16-1199,1,0.835775,"assification and cross-media age regression. 1 Introduction User profiling is a task which leverages user generated content (UGC) to automatically identify the data about a user, such as gender (Wang et al., 2017; Li et al., 2016; Li et al., 2015) and age (Marquardt et al., 2014) identification. Recently, along with the boom of social media, user profiling has been drawing more and more attention in various social applications, such as personality analysis (Li et al., 2016; Sarawgi et al., 2011; O’Connor et al., 2010), intelligent marking (Preotiuc-Pietro et al., 2015) and online advertising (Zhang et al., 2016; Volkova et al., 2013). In the literature, conventional approaches normally recast user profiling as a supervised learning problem (Marquardt et al., 2014; Zhang et al., 2016; Ciot et al., 2013; Corney et al., 2002) by exploring various user generated textual features and user social connection features. However, the performance largely depends on a large amount of labeled data and the performance normally degrades dramatically when the model is tested on a different social media. Even worse, many real scenarios may involve several social media with some of them lacking sufficient labeled dat"
C18-1215,D15-1075,0,0.185054,"c resources (Yih et al., 2013), tree edit distance (Yao and Durme, 2013) and named entities (Severyn and Moschitti, 2013). 1 https://www.taobao.com/ 2541 2 In deep learning methods, some neural network algorithms are employed to train learning models. Briefly, these methods could be categorized into three categories, i.e., siamense networks, attentive networks and compare-aggregate networks. In siamense networks, related studies use classic neural networks, such as LSTM and CNN, to get the representations separately and then concatenate them to classify. (Feng et al., 2015; Yang et al., 2015; Bowman et al., 2015). In attentive networks, instead of using the final time step of LSTM to represent a sentence, related studies use the attention strategy to get the weight of overall time steps and then use the weight to represent the sentence. (Tan et al., 2016; Hermann et al., 2015, Yin et al., 2015). In compare-aggregate networks, related studies use different matching strategy to get relationships within words. (He and Lin, 2016; Wang et al., 2017; Wang and Jiang, 2016; Trischler et al., 2016; Parikn et al., 2016.). However, all above approaches are similar to our One vs. One Matching model which deals wi"
C18-1215,N16-1108,0,0.0177049,"studies use classic neural networks, such as LSTM and CNN, to get the representations separately and then concatenate them to classify. (Feng et al., 2015; Yang et al., 2015; Bowman et al., 2015). In attentive networks, instead of using the final time step of LSTM to represent a sentence, related studies use the attention strategy to get the weight of overall time steps and then use the weight to represent the sentence. (Tan et al., 2016; Hermann et al., 2015, Yin et al., 2015). In compare-aggregate networks, related studies use different matching strategy to get relationships within words. (He and Lin, 2016; Wang et al., 2017; Wang and Jiang, 2016; Trischler et al., 2016; Parikn et al., 2016.). However, all above approaches are similar to our One vs. One Matching model which deals with the matching measurement between one sentence (or one piece of text) and another sentence (or another piece of text). In contrast, our approach is a One vs. Many Matching model which deals with the matching measurement between one sentence (or one piece of text) and multiple sentences (or multiple pieces of text). 3 Data Collection and Annotation We collect 4,060 question-answer pairs from “Asking All” in Taobao,"
C18-1215,D16-1244,0,0.0724571,"Missing"
C18-1215,D13-1044,0,0.0493654,"Missing"
C18-1215,P16-1044,0,0.189583,"mine whether an answer is answering a given question. For instance, in Figure 1, the question “Where is dear john filmed at?” in E1 has two candidate answers “The movie was filmed in 2009 in Charleston.” and “The file was released on May 25, 2010 on DVD.” The first answer is determined with the “Matching” category since it answers the question while the second answer is “Non-matching” since it could not answer the question. The past five years have witnessed a huge exploding interest in the research on QA matching, due to its widely applications, such as question answering (Yang et al., 2015; Tan et al., 2016; Wang et al., 2017) and reading comprehension (Trischler et al., 2016; Dhingra et al., 2017). However, all existing QA matching studies only focus on formal text. In real applications, there exists many scenarios where the QA text is informal. For instance, E2 is a question-answer pair extE1: Two QA pairs in formal text Q1: Where is dear john filmed at? Label: Matching Q2:Where is dear john filmed at? Label: Non-matching E2: Three QA pairs in informal text Q: Will the response time slow after updating os? What about the battery? What about the screen? A1: The movie was filmed in 2009 in Charl"
C18-1215,P16-1041,0,0.0278233,"ce, in Figure 1, the question “Where is dear john filmed at?” in E1 has two candidate answers “The movie was filmed in 2009 in Charleston.” and “The file was released on May 25, 2010 on DVD.” The first answer is determined with the “Matching” category since it answers the question while the second answer is “Non-matching” since it could not answer the question. The past five years have witnessed a huge exploding interest in the research on QA matching, due to its widely applications, such as question answering (Yang et al., 2015; Tan et al., 2016; Wang et al., 2017) and reading comprehension (Trischler et al., 2016; Dhingra et al., 2017). However, all existing QA matching studies only focus on formal text. In real applications, there exists many scenarios where the QA text is informal. For instance, E2 is a question-answer pair extE1: Two QA pairs in formal text Q1: Where is dear john filmed at? Label: Matching Q2:Where is dear john filmed at? Label: Non-matching E2: Three QA pairs in informal text Q: Will the response time slow after updating os? What about the battery? What about the screen? A1: The movie was filmed in 2009 in Charleston. A2: The film was released on May 25, 2010 on DVD. A: The respon"
C18-1215,C10-1131,0,0.083737,"Missing"
C18-1215,P16-1122,0,0.0269575,"Missing"
C18-1215,D15-1237,0,0.283339,"is a task to determine whether an answer is answering a given question. For instance, in Figure 1, the question “Where is dear john filmed at?” in E1 has two candidate answers “The movie was filmed in 2009 in Charleston.” and “The file was released on May 25, 2010 on DVD.” The first answer is determined with the “Matching” category since it answers the question while the second answer is “Non-matching” since it could not answer the question. The past five years have witnessed a huge exploding interest in the research on QA matching, due to its widely applications, such as question answering (Yang et al., 2015; Tan et al., 2016; Wang et al., 2017) and reading comprehension (Trischler et al., 2016; Dhingra et al., 2017). However, all existing QA matching studies only focus on formal text. In real applications, there exists many scenarios where the QA text is informal. For instance, E2 is a question-answer pair extE1: Two QA pairs in formal text Q1: Where is dear john filmed at? Label: Matching Q2:Where is dear john filmed at? Label: Non-matching E2: Three QA pairs in informal text Q: Will the response time slow after updating os? What about the battery? What about the screen? A1: The movie was filme"
C18-1215,N13-1106,0,0.0393979,"Missing"
C18-1215,P13-1171,0,0.02644,"erent from the above corpora, the question-answer pairs in our corpus are informal text. 2.2 Matching methods Generally speaking, QA matching methods could be split into two categories: shallow learning methods and deep learning methods. In shallow learning methods, some shallow learning algorithms, such as CRF, SVM and MaxEnt, are employed to train the learning models (Wang et al., 2010). Besides the learning algorithms, the related studies on shallow learning methods mainly focus on feature engineering, using linguistic tools and using external resources, such as lexical semantic resources (Yih et al., 2013), tree edit distance (Yao and Durme, 2013) and named entities (Severyn and Moschitti, 2013). 1 https://www.taobao.com/ 2541 2 In deep learning methods, some neural network algorithms are employed to train learning models. Briefly, these methods could be categorized into three categories, i.e., siamense networks, attentive networks and compare-aggregate networks. In siamense networks, related studies use classic neural networks, such as LSTM and CNN, to get the representations separately and then concatenate them to classify. (Feng et al., 2015; Yang et al., 2015; Bowman et al., 2015). In atten"
D12-1013,P11-1013,0,0.0126695,"lts. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 140 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classification suffers from the problem that their balanced selection strategy in co-training would generate many errors in late iterations due to the imbalanced nature of the"
D12-1013,P09-1083,0,0.0167307,"nwhile, automatically label most informative majority-class samples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. 1 1 Introduction Sentiment classification is the task of identifying the sentiment polarity (e.g., positive or negative) of ‡ a natural language text towards a given topic (Pang et al., 2002; Turney, 2002) and has become the core component of many important applications in opinion analysis (Cui et al., 2006; Li et al., 2009; Lloret et al., 2009; Zhang and Ye, 2008). Most of previous studies in sentiment classification focus on learning models from a large number of labeled data. However, in many real-world applications, manual annotation is expensive and time-consuming. In these situations, active learning approaches could be helpful by actively selecting most informative samples for manual annotation. Compared to traditional active learning for sentiment classification, active learning for imbalanced sentiment classification faces some unique challenges. As a specific type of sentiment classification, imbalance"
D12-1013,P08-2065,1,0.839243,"e experimental results. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 140 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classification suffers from the problem that their balanced selection strategy in co-training would generate many errors in late iterations due to the imbala"
D12-1013,P10-1043,1,0.760833,"e learning approach for imbalanced sentiment classification. Section 4 reports the experimental results. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 140 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classification suffers from the problem that their balanced selection strat"
D12-1013,P07-1056,0,0.296743,". Section 4 reports the experimental results. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 140 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classification suffers from the problem that their balanced selection strategy in co-training would generate many errors in late iteration"
D12-1013,N09-3013,0,0.012429,"ally label most informative majority-class samples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. 1 1 Introduction Sentiment classification is the task of identifying the sentiment polarity (e.g., positive or negative) of ‡ a natural language text towards a given topic (Pang et al., 2002; Turney, 2002) and has become the core component of many important applications in opinion analysis (Cui et al., 2006; Li et al., 2009; Lloret et al., 2009; Zhang and Ye, 2008). Most of previous studies in sentiment classification focus on learning models from a large number of labeled data. However, in many real-world applications, manual annotation is expensive and time-consuming. In these situations, active learning approaches could be helpful by actively selecting most informative samples for manual annotation. Compared to traditional active learning for sentiment classification, active learning for imbalanced sentiment classification faces some unique challenges. As a specific type of sentiment classification, imbalanced sentiment classific"
D12-1013,W02-1011,0,0.0234829,"rmative minority-class samples for manual annotation by leveraging a certainty measurement and an uncertainty measurement, and in the meanwhile, automatically label most informative majority-class samples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. 1 1 Introduction Sentiment classification is the task of identifying the sentiment polarity (e.g., positive or negative) of ‡ a natural language text towards a given topic (Pang et al., 2002; Turney, 2002) and has become the core component of many important applications in opinion analysis (Cui et al., 2006; Li et al., 2009; Lloret et al., 2009; Zhang and Ye, 2008). Most of previous studies in sentiment classification focus on learning models from a large number of labeled data. However, in many real-world applications, manual annotation is expensive and time-consuming. In these situations, active learning approaches could be helpful by actively selecting most informative samples for manual annotation. Compared to traditional active learning for sentiment classification, active l"
D12-1013,P02-1053,0,0.00907258,"ass samples for manual annotation by leveraging a certainty measurement and an uncertainty measurement, and in the meanwhile, automatically label most informative majority-class samples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. 1 1 Introduction Sentiment classification is the task of identifying the sentiment polarity (e.g., positive or negative) of ‡ a natural language text towards a given topic (Pang et al., 2002; Turney, 2002) and has become the core component of many important applications in opinion analysis (Cui et al., 2006; Li et al., 2009; Lloret et al., 2009; Zhang and Ye, 2008). Most of previous studies in sentiment classification focus on learning models from a large number of labeled data. However, in many real-world applications, manual annotation is expensive and time-consuming. In these situations, active learning approaches could be helpful by actively selecting most informative samples for manual annotation. Compared to traditional active learning for sentiment classification, active learning for imb"
D12-1013,P09-1027,0,0.0155781,"s our active learning approach for imbalanced sentiment classification. Section 4 reports the experimental results. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 140 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classification suffers from the problem that their balanc"
D12-1013,D07-1082,0,0.265593,"les which have the largest disagreement among several committee classifiers. Besides query by committee (QBC) as the first of such type (Freund et al., 1997), co-testing learns a committee of member classifiers from different views and selects those contention points (i.e., unlabeled examples on which the views predict different labels) for manual annotation (Muslea et al., 2006). However, most previous studies focus on the scenario of balanced class distribution and only a few recent studies address the active learning issue on imbalanced classification problems including Yang and Ma (2010), Zhu and Hovy (2007), Ertekin et al. (2007a) and Ertekin et al. (2007b)2. Unfortunately, they straightly adopt the uncertainty sampling as the active selection strategy to address active learning in imbalanced classification, which completely ignores the class imbalance problem in the selected samples. Attenberg and Provost (2010) highlights the importance of selecting samples by considering the proportion of the classes. Their simulation experiment on text categorization confirms that selecting class-balanced samples is more important than traditional active selection strategies like uncertainty. However, the pr"
D13-1067,P10-1015,0,0.0802864,"Missing"
D13-1067,W04-3247,0,0.0724279,"Missing"
D13-1067,I11-1054,0,0.0451692,"Missing"
D13-1067,W04-1013,0,0.0134177,"Missing"
D13-1067,J98-3005,0,0.158407,"some motivational analysis. In Section 5, we explain our proposed model and describe algorithms for parameter estimation and prediction. In Section 6, we present our experimental results. We sum up our work and discuss future directions in Section 7. 2 Related Work In this section, we will introduce the related work on the traditional topic-based summarization, social-based summarization and factor graph model respectively. 716 2.1 Topic-based Summarization Generally, traditional topic-based summarization can be categorized into two categories: extractive (Radev et al., 2004) and abstractive (Radev and McKeown, 1998) summarization. The former selects a subset of sentences from original document(s) to form a summary; the latter reorganizes some sentences to form a summary where several complex technologies, such as information fusion, sentence compression and reformulation are necessarily employed (Wan and Yang, 2008; Celikyilmaz and Hakkani-Tur, 2011; Wang and Zhou, 2012). This study focuses on extractive summarization. Radev et al. (2004) proposed a centroid-based method to rank the sentences in a document set, using various kinds of features, such as the cluster centroid, position and TF-IDF features. R"
D13-1067,P11-1077,0,0.0136695,", supervised learning for summarization is relatively rare. A typical work is Shen et al., (2007) which present a Conditional Random Fields (CRF) based framework to treat the summarization task as a sequence labeling problem. However, different from all existing studies, our work is the first attempt to consider both textual information and social relationship information for supervised summarization. 2.2 Social-based Summarization As web 2.0 has empowered people to actively interact with each other, studies focusing on social media have attracted much attention recently (Meeder et al., 2011; Rosenthal and McKeown, 2011; Yang et al., 2011a). Social-based summarization is exactly a special case of summarization where the social connection is employed to help obtaining the summarization. Although topicbased summarization has been extensively studied, studies on social-based summarization are relative new and rare. Hu et al., (2011) proposed an unsupervised PageRank-based social summarization approach by incorporating both document context and user context in the sentence evaluation process. Meng et al., (2012) proposed a unified optimization framework to produce opinion summaries of tweets through integrating"
D13-1067,D12-1024,0,0.0173319,") summarization. The former selects a subset of sentences from original document(s) to form a summary; the latter reorganizes some sentences to form a summary where several complex technologies, such as information fusion, sentence compression and reformulation are necessarily employed (Wan and Yang, 2008; Celikyilmaz and Hakkani-Tur, 2011; Wang and Zhou, 2012). This study focuses on extractive summarization. Radev et al. (2004) proposed a centroid-based method to rank the sentences in a document set, using various kinds of features, such as the cluster centroid, position and TF-IDF features. Ryang and Abekawa (2012) proposed a reinforcement learning approach on text summarization, which models the summarization within a reinforcement learning-based framework. Compared to unsupervised approaches, supervised learning for summarization is relatively rare. A typical work is Shen et al., (2007) which present a Conditional Random Fields (CRF) based framework to treat the summarization task as a sequence labeling problem. However, different from all existing studies, our work is the first attempt to consider both textual information and social relationship information for supervised summarization. 2.2 Social-ba"
D13-1067,P11-1155,0,0.133274,"lly. To the best of our knowledge, this is the first research that explores automatic summarization of personal profiles in social media. A straightforward approach is to consider personal profile summarization as a traditional document summarization problem, which treating each personal profile independently and generate a summary for each personal profile individually. For example, the well-known extraction and ranking approaches (e.g. PageRank, HITS) extract a certain amount of important sentences from a document according to some ranking measurements to form a summary (Wan and Yang, 2008; Wan, 2011). However, such straightforward approaches are not sufficient to benefit from the carrier of personal profiles. As the centroid of social networking, people are usually connected to others with similar 715 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 715–725, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics background in social media (e.g. co-major, cocorporation). Therefore, it is reasonable to leverage social connection to improve the performance of profile summarizing. For example if there are co-"
D13-1067,P11-1050,0,\N,Missing
D18-1066,H05-1073,0,0.162878,"d have similar data sizes in our experimental data, it seems much easier to detect ‘sad’ instances than to detect ‘angry’ instances. This is maybe because ‘angry’ is caused by more various events and it is more difficult to capture and utilize those cause events. Thus, it is necessary for the emotion classification to have an encoder which can extract the eventbased information of emotion cause from texts. 4 In recent years, intensive studies have explored supervised machine learning approaches using various types of features for different-level emotion classification, such as document level (Alm et al. 2005; Li et al. 2014; Huang et al. 2016), sentence level or short text level (Tokushisa et al. 2008; Bhowmick et al. 2009; Xu et al. 2012; Wen and Wan, 2014; Li et al. 2015; Felbo et al., 2017), and so on. Moreover, since both emotion and sentiment belong to affective feeling, some studies have explored the join learning of sentiment classification and emotion classification (Gao et al., 2013; Wang et al., 2015). In the other hand, most of previous emotion cause detection studies is clause-based, which examine whether a clause around a given emotion keyword is a cause or not. Moreover, these studi"
D18-1066,C08-1111,0,0.0967937,"Missing"
D18-1066,C10-1021,1,0.82214,"et al. 2014; Huang et al. 2016), sentence level or short text level (Tokushisa et al. 2008; Bhowmick et al. 2009; Xu et al. 2012; Wen and Wan, 2014; Li et al. 2015; Felbo et al., 2017), and so on. Moreover, since both emotion and sentiment belong to affective feeling, some studies have explored the join learning of sentiment classification and emotion classification (Gao et al., 2013; Wang et al., 2015). In the other hand, most of previous emotion cause detection studies is clause-based, which examine whether a clause around a given emotion keyword is a cause or not. Moreover, these studies (Chen et al., 2010; Xu et al., 2017; Ghazi et al., 2015; Gui et al., 2017; Cheng et al., 2017) focus on how to extract two kinds of features for supervised model learning: explicit expression patterns (e.g. “to cause”, “for”), and implicit features which can reflect the causal relation. Encoder Sequence Prec Rec F1 ConvMSCauseCL 34.3 77.5 47.5 Memnet ATT all 55.4 60.9 58.0 LSTM all 61.3 58.3 55.6 JMECause all 53.1 66.7 59.1 Table 4: The performances of emotion cause detection models. (all: EmoKW, CauseCL plus Context) 5 Table 4 shows the performances of different emotion cause detection models, where “Sequence”"
D18-1066,D17-1169,0,0.179012,"and the attention layer focuses on the learning of word importance (weights). Because of the feature sparse problem in our small-scaled experimental data, the attention network often cannot effectively extract features to represent an event (see §3.2). Thus, in our joint encoder, we use the attention network to extract affective features (e.g. “ ” in Fig. 1) and the LSTM network to extract event-based features (e.g. “I found that only I was at home again” in Fig. 1). 1 647 The Joint Approach https://code.google.com/p/word2vec/ The attention network: we implement the attention network used in Felbo et al. (2017), which includes two layers: a BiLSTM layer which extracts a sequence feature for each input word, and an attention layer which represents the input sequence using weighted words. The LSTM network: the network uses a BiLSTM layer to capture a sequence feature for each input word, and then uses the average of those features as the representation of the input sequence. In the linear decoder, there are two classification networks (CNet EClass and CNet ECause) for EClass and ECause separately. Each classification network uses a linear layer to build a probabilistic classification model. provides a"
D18-1066,D17-1167,0,0.138932,"t text level (Tokushisa et al. 2008; Bhowmick et al. 2009; Xu et al. 2012; Wen and Wan, 2014; Li et al. 2015; Felbo et al., 2017), and so on. Moreover, since both emotion and sentiment belong to affective feeling, some studies have explored the join learning of sentiment classification and emotion classification (Gao et al., 2013; Wang et al., 2015). In the other hand, most of previous emotion cause detection studies is clause-based, which examine whether a clause around a given emotion keyword is a cause or not. Moreover, these studies (Chen et al., 2010; Xu et al., 2017; Ghazi et al., 2015; Gui et al., 2017; Cheng et al., 2017) focus on how to extract two kinds of features for supervised model learning: explicit expression patterns (e.g. “to cause”, “for”), and implicit features which can reflect the causal relation. Encoder Sequence Prec Rec F1 ConvMSCauseCL 34.3 77.5 47.5 Memnet ATT all 55.4 60.9 58.0 LSTM all 61.3 58.3 55.6 JMECause all 53.1 66.7 59.1 Table 4: The performances of emotion cause detection models. (all: EmoKW, CauseCL plus Context) 5 Table 4 shows the performances of different emotion cause detection models, where “Sequence” lists the sequences of input words used by each model."
D18-1066,P15-1101,1,0.817241,"sed by more various events and it is more difficult to capture and utilize those cause events. Thus, it is necessary for the emotion classification to have an encoder which can extract the eventbased information of emotion cause from texts. 4 In recent years, intensive studies have explored supervised machine learning approaches using various types of features for different-level emotion classification, such as document level (Alm et al. 2005; Li et al. 2014; Huang et al. 2016), sentence level or short text level (Tokushisa et al. 2008; Bhowmick et al. 2009; Xu et al. 2012; Wen and Wan, 2014; Li et al. 2015; Felbo et al., 2017), and so on. Moreover, since both emotion and sentiment belong to affective feeling, some studies have explored the join learning of sentiment classification and emotion classification (Gao et al., 2013; Wang et al., 2015). In the other hand, most of previous emotion cause detection studies is clause-based, which examine whether a clause around a given emotion keyword is a cause or not. Moreover, these studies (Chen et al., 2010; Xu et al., 2017; Ghazi et al., 2015; Gui et al., 2017; Cheng et al., 2017) focus on how to extract two kinds of features for supervised model lea"
D18-1401,P07-1056,0,0.0552003,"g by incorporating sentiment polarities of text in loss functions. Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a"
D18-1401,D15-1007,0,0.0135061,"darker. Is the sun cream really effective? Answer 2: No, just depending on my own experience. Figure 1: Two examples of QA text pairs from “customer questions & answers” section in Amazon. Introduction Sentiment analysis, a.k.a. opinion mining, is a task which aims to identify the user sentiment orientation of a product/brand/service by monitoring the online textual data, e.g., reviews and social media messages. It has attracted huge attention in both academic and industrial communities due to its widespread applications, such like recommendation (Zhang et al., 2014) and social media mining (Chambers et al., 2015). As the fundamental component in sentiment analysis, sentiment classification mainly classifies the sentiment polarity as positive or negative, and has been well-studied from both sentence-level (Kim and Hovy, 2004) and document-level (Xu et al., 2016). ∗ Corresponding author Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With th"
D18-1401,P17-1055,0,0.0244919,"ed by concatenating the forward and backward hidden states. For simplicity, we note contextual representation of SQi as HQi , and contextual representation of SAj as HAj respectively: HQi = [hi,1 , hi,2 , ..., hi,n , ..., hi,Ni ] (1) HAj = [hj,1 , hj,2 , ..., hj,m , ..., hj,Mj ] (2) where D[i,j] ∈ RNi ×Mj denotes the bidirectional matching matrix for the [SQi , SAj ] unit. Each element in D[i,j] is the score that measures how well the word in SQi semantically matches the word in SAj and vice versa. Given the bidirectional matching matrix D[i,j] , we use attention mechanism (Yang et al., 2016; Cui et al., 2017) to mine the sentiment matching information between question and answer from two directions, which could be seen as an Answerto-Question attention and a Question-to-Answer attention as follows. • Answer-to-Question Attention: We employ row-wise operations to compute the attention r weight vector α[i,j] as follows: r &gt; U[i,j] = tanh(Wr · D[i,j] ) (4) r r α[i,j] = softmax(wr&gt; · U[i,j] ) (5) r where α[i,j] ∈ RNi is the Answer-to-Question attention weight vector regarding the importance degrees of all words in Q-sentence SQi , Wr ∈ 0 0 Rd ×Mj and wr ∈ Rd are weight matrices. After computing the An"
D18-1401,P11-2104,0,0.019958,"ional sentiment classification has been carried out in different text levels, such like word-level, documentlevel and aspect-level. Word-level sentiment classification has been studied in a long period in the research community of sentiment analysis. Some early studies have devoted their efforts to predicting the sentiment polarity of a word with different learning models and resources. Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the search hits. Hassan and Radev (2010) and Hassan et al. (2011) applied a Markov random walk model to determine the word polarities with a large word relatedness graph, and the synonyms and hypernyms in WordNet (Miller, 1995). More recently, some studies aim to learn better word embedding of a word rather than its polarity. Tang et al. (2014) developed three neural networks to learn word em3655 Beauty Shoe Electronic Positive 3,676 4,025 3,807 Negative 981 819 1,017 Conflict 318 412 528 Neutral 5,025 4,744 4,648 Total 10,000 10,000 10,000 Table 1: Category distribution of the annotated data in three domains. bedding by incorporating sentiment polarities o"
D18-1401,P10-1041,0,0.015063,"eral, the research on traditional sentiment classification has been carried out in different text levels, such like word-level, documentlevel and aspect-level. Word-level sentiment classification has been studied in a long period in the research community of sentiment analysis. Some early studies have devoted their efforts to predicting the sentiment polarity of a word with different learning models and resources. Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the search hits. Hassan and Radev (2010) and Hassan et al. (2011) applied a Markov random walk model to determine the word polarities with a large word relatedness graph, and the synonyms and hypernyms in WordNet (Miller, 1995). More recently, some studies aim to learn better word embedding of a word rather than its polarity. Tang et al. (2014) developed three neural networks to learn word em3655 Beauty Shoe Electronic Positive 3,676 4,025 3,807 Negative 981 819 1,017 Conflict 318 412 528 Neutral 5,025 4,744 4,648 Total 10,000 10,000 10,000 Table 1: Category distribution of the annotated data in three domains. bedding by incorporati"
D18-1401,P11-1013,0,0.0608791,"Missing"
D18-1401,C04-1200,0,0.274096,"is, a.k.a. opinion mining, is a task which aims to identify the user sentiment orientation of a product/brand/service by monitoring the online textual data, e.g., reviews and social media messages. It has attracted huge attention in both academic and industrial communities due to its widespread applications, such like recommendation (Zhang et al., 2014) and social media mining (Chambers et al., 2015). As the fundamental component in sentiment analysis, sentiment classification mainly classifies the sentiment polarity as positive or negative, and has been well-studied from both sentence-level (Kim and Hovy, 2004) and document-level (Xu et al., 2016). ∗ Corresponding author Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics c"
D18-1401,D15-1180,0,0.0466625,"Missing"
D18-1401,P10-1043,1,0.831516,"Missing"
D18-1401,P15-2005,1,0.840444,"e annotated data in three domains. bedding by incorporating sentiment polarities of text in loss functions. Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classificatio"
D18-1401,D17-1048,0,0.011926,"et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task. Recently, Wang et al. (2016) proposed an attention-based LSTM neural network to aspect-level sentiment classification by exploring the connection between an aspect and the content of a sentence. Tang et al. (2016) proposed a deep memory network with multiple attention-based computational layers to improve the performance. Wang et al."
D18-1401,P14-5010,0,0.00460503,"Missing"
D18-1401,D15-1298,0,0.0617943,"Missing"
D18-1401,W02-1011,0,0.0273117,"[Q-sentence, A-sentence] unit for exploring sentiment information. Finally, the self-matching attention layer in the model can capture the importance of these [Q-sentence, A-sentence] matching vectors obtained from QA bidirectional matching layer, which could effectively refine the evidence for inferring the sentiment polarity of a QA text pair. Experimental results show that the proposed approach significantly outperforms several strong baselines for QA-style sentiment classification. 2 Related Work Sentiment classification has become a hot research field in NLP since the pioneering work by Pang et al. (2002). In general, the research on traditional sentiment classification has been carried out in different text levels, such like word-level, documentlevel and aspect-level. Word-level sentiment classification has been studied in a long period in the research community of sentiment analysis. Some early studies have devoted their efforts to predicting the sentiment polarity of a word with different learning models and resources. Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the searc"
D18-1401,P13-4009,0,0.17484,"Missing"
D18-1401,W06-1652,0,0.0451964,"l 5,025 4,744 4,648 Total 10,000 10,000 10,000 Table 1: Category distribution of the annotated data in three domains. bedding by incorporating sentiment polarities of text in loss functions. Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel"
D18-1401,P15-1098,0,0.01816,"iment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task. Recently, Wang et al. (2016) proposed an a"
D18-1401,C14-1053,0,0.0241938,"6). ∗ Corresponding author Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics comparing with traditional reviews (Wachsmuth et al., 2014; Zhou et al., 2015a). Comparing to the traditional reviews, the QA style reviews can be more informative and convincing. More importantly, because answer providers are randomly picked from the users who already purchased the target item, this new form of review can be more reliable and trustful. Regarding QA-style sentiment analysis, one straightforward method is to directly employ an existing sentiment classification approach that works well on traditional reviews, such as RNN (Nguyen and Shirai, 2015) and LSTM (Chen et al., 2016). However, because of the significant differences between QA-s"
D18-1401,D16-1058,0,0.308112,"assification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task. Recently, Wang et al. (2016) proposed an attention-based LSTM neural network to aspect-level sentiment classification by exploring the connection between an aspect and the content of a sentence. Tang et al. (2016) proposed a deep memory network with multiple attention-based computational layers to improve the performance. Wang et al. (2018) proposed a hierarchical attention network to explore both word-level and clause-level sentiment information towards a target aspect. Unlike all the prior studies, this paper focuses on a very different kind of text representation, i.e., QA-style text level, for sentiment classificatio"
D18-1401,P15-1102,0,0.0469393,"Missing"
D18-1401,D16-1172,0,0.173147,"ch aims to identify the user sentiment orientation of a product/brand/service by monitoring the online textual data, e.g., reviews and social media messages. It has attracted huge attention in both academic and industrial communities due to its widespread applications, such like recommendation (Zhang et al., 2014) and social media mining (Chambers et al., 2015). As the fundamental component in sentiment analysis, sentiment classification mainly classifies the sentiment polarity as positive or negative, and has been well-studied from both sentence-level (Kim and Hovy, 2004) and document-level (Xu et al., 2016). ∗ Corresponding author Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics comparing with traditional reviews (Wa"
D18-1401,N16-1174,0,0.313624,"f each word is formed by concatenating the forward and backward hidden states. For simplicity, we note contextual representation of SQi as HQi , and contextual representation of SAj as HAj respectively: HQi = [hi,1 , hi,2 , ..., hi,n , ..., hi,Ni ] (1) HAj = [hj,1 , hj,2 , ..., hj,m , ..., hj,Mj ] (2) where D[i,j] ∈ RNi ×Mj denotes the bidirectional matching matrix for the [SQi , SAj ] unit. Each element in D[i,j] is the score that measures how well the word in SQi semantically matches the word in SAj and vice versa. Given the bidirectional matching matrix D[i,j] , we use attention mechanism (Yang et al., 2016; Cui et al., 2017) to mine the sentiment matching information between question and answer from two directions, which could be seen as an Answerto-Question attention and a Question-to-Answer attention as follows. • Answer-to-Question Attention: We employ row-wise operations to compute the attention r weight vector α[i,j] as follows: r &gt; U[i,j] = tanh(Wr · D[i,j] ) (4) r r α[i,j] = softmax(wr&gt; · U[i,j] ) (5) r where α[i,j] ∈ RNi is the Answer-to-Question attention weight vector regarding the importance degrees of all words in Q-sentence SQi , Wr ∈ 0 0 Rd ×Mj and wr ∈ Rd are weight matrices. Aft"
D18-1401,D16-1021,0,0.0277727,"Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task. Recently, Wang et al. (2016) proposed an attention-based LSTM neural network to aspect-level sentiment classification by exploring the connection between an aspect and the content of a sentence. Tang et al. (2016) proposed a deep memory network with multiple attention-based computational layers to improve the performance. Wang et al. (2018) proposed a hierarchical attention network to explore both word-level and clause-level sentiment information towards a target aspect. Unlike all the prior studies, this paper focuses on a very different kind of text representation, i.e., QA-style text level, for sentiment classification. To the best of our knowledge, this is the first attempt to perform sentiment classification on this text level. 3 Data Collection and Annotation We collect QA text pairs from “Asking"
D18-1401,P14-1146,0,0.0370401,"their efforts to predicting the sentiment polarity of a word with different learning models and resources. Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the search hits. Hassan and Radev (2010) and Hassan et al. (2011) applied a Markov random walk model to determine the word polarities with a large word relatedness graph, and the synonyms and hypernyms in WordNet (Miller, 1995). More recently, some studies aim to learn better word embedding of a word rather than its polarity. Tang et al. (2014) developed three neural networks to learn word em3655 Beauty Shoe Electronic Positive 3,676 4,025 3,807 Negative 981 819 1,017 Conflict 318 412 528 Neutral 5,025 4,744 4,648 Total 10,000 10,000 10,000 Table 1: Category distribution of the annotated data in three domains. bedding by incorporating sentiment polarities of text in loss functions. Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On o"
D18-1401,P15-1042,0,0.129521,"or Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics comparing with traditional reviews (Wachsmuth et al., 2014; Zhou et al., 2015a). Comparing to the traditional reviews, the QA style reviews can be more informative and convincing. More importantly, because answer providers are randomly picked from the users who already purchased the target item, this new form of review can be more reliable and trustful. Regarding QA-style sentiment analysis, one straightforward method is to directly employ an existing sentiment classification approach that works well on traditional reviews, such as RNN (Nguyen and Shirai, 2015) and LSTM (Chen et al., 2016). However, because of the significant differences between QA-style and classical"
D19-1552,P17-1067,0,0.497484,"tal results show the usefulness of personal attributes, and the effectiveness of our proposed NPD approach in capturing such personal attributes with significant gains over the state-of-the-art models. 1 Introduction The advent of social media and its prosperity enable the creation of massive online user-generated content including opinions and product reviews. Analyzing such user-generated contents allows to detect the users’ emotional states, which are useful for various downstream applications. In the literature, there are a large number of works on emotion detection (Roberts et al., 2012; Abdul-Mageed and Ungar, 2017; Gupta et al., 2017), both discrete and neural models have been ∗ corresponding auther used to predict the emotions of posts in social media. For example, Roberts et al. (2012) used a series of binary SVM classifiers to detect the emotion of a post, while Gupta et al. (2017) used sentiment based semantic embedding and a LSTM model to learn the representation of a post for emotion detection. Different from previous researches, which consider each post individually, we think that posts in social media are much correlated by the authors’ backgrounds. Motivated by the principle of homophily (Laza"
D19-1552,D16-1053,0,0.0134642,"ere,1) SVM is a widely used baseline to predict the emotion of a post in social media (Yang et al., 2007).2) Abdul17 is a standard LSTM model which consist of a LSTM layer and a fully connected layer, and it is modified from the model in Abdul-Mageed and Ungar (2017). The LSTM model yields the stateof-the-art performance on emotion detection in recent researches.3)Vaswani17 is an improved LSTM model with a self-attention mechanism. The self-attention mechanism is used to capture the structural information and has been successfully applied in various natural language processing tasks recently (Cheng et al., 2016; Vaswani et al., 2017) From Table 2, we find that all of the neural models outperform SVM significantly. This indicates that neural models are much more effective than discrete models in emotion detection. In addition, our proposed NPD model outperforms both the standard LSTM model (Abdul17) and the improved LSTM model with self-attention (Vaswani17) significantly. This shows the effectiveness of our proposed NPD model with both adversarial discriminators and attention mechanisms. This also shows the usefulness of personal attributes for emotion detection. Moreover, we find that the performan"
D19-1552,P15-1073,0,0.0289832,"ollobert et al., 2011; Goldberg, 2016). However, few works use neural network models for emotion detection. Abdul-Mageed and Ungar (2017) used a gated recurrent neural network model for emotion detection with a largescale dataset. Zhang et al. (2018) used an auxiliary and attention based LSTM to detect emotion on a cross-lingual dataset. Lexicon and social information are very important for emotion detection, and there are many researches focus on this topic. For example, Strapparava and Mihalcea (2008) used WordNetAffect to compute the sentimental score of a post. More recently, In addition, Hovy (2015) used both the age and gender information of the authors to improve the performance of sentiment analysis. Vosoughi et al. (2016) explored the relationship among locations, date time, authors and sentiments. Different from previous works which consider each post individually, we think that the posts in social media can be connected through the authors’ backgrounds and should be better addressed. On the basis, we propose a neural personal discrimination model to determine the personal background attributes from each post through adversarial discriminators, and aggregate the representation of in"
D19-1552,roberts-etal-2012-empatweet,0,0.18827,"r addressed. Experimental results show the usefulness of personal attributes, and the effectiveness of our proposed NPD approach in capturing such personal attributes with significant gains over the state-of-the-art models. 1 Introduction The advent of social media and its prosperity enable the creation of massive online user-generated content including opinions and product reviews. Analyzing such user-generated contents allows to detect the users’ emotional states, which are useful for various downstream applications. In the literature, there are a large number of works on emotion detection (Roberts et al., 2012; Abdul-Mageed and Ungar, 2017; Gupta et al., 2017), both discrete and neural models have been ∗ corresponding auther used to predict the emotions of posts in social media. For example, Roberts et al. (2012) used a series of binary SVM classifiers to detect the emotion of a post, while Gupta et al. (2017) used sentiment based semantic embedding and a LSTM model to learn the representation of a post for emotion detection. Different from previous researches, which consider each post individually, we think that posts in social media are much correlated by the authors’ backgrounds. Motivated by th"
D19-1552,W15-2904,0,0.0729155,"Missing"
D19-1552,C14-1050,1,0.814011,"eat this in such weather!) However, the personal attributes are not easy to obtain in most social media websites. On one hand, most websites may not contain useful personal information. On the other hand, people are normally not willing to attach their personal information in social media. Besides, integrating personal attributes into emotion detection is challenging, since it is hard to capture attributes-aware words, such as “little brother” and “bashi”(comfortable), to connect the posts with similar backgrounds. Although there are some related works on either personal attribute extraction (Wang et al., 2014) or emotion detection with personal attributes (Li et al., 2016), none of them address both challenges at the same time. In this paper, we propose a Neural Personal Discrimination (NPD) model with both adversarial discriminators and attention mechanisms to tackle above challenges. Here, the Adversarial discriminators (Goodfellow et al., 2014) are used to determine the personal attributes, e.g., gender or location, of a post, providing the inherent correlationship between emotions and personal backgrounds, while the Attention mechanisms (Wang et al., 2016) are utilized to aggregate the represen"
D19-1552,C16-1153,1,0.929679,"either personal attribute extraction (Wang et al., 2014) or emotion detection with personal attributes (Li et al., 2016), none of them address both challenges at the same time. In this paper, we propose a Neural Personal Discrimination (NPD) model with both adversarial discriminators and attention mechanisms to tackle above challenges. Here, the Adversarial discriminators (Goodfellow et al., 2014) are used to determine the personal attributes, e.g., gender or location, of a post, providing the inherent correlationship between emotions and personal backgrounds, while the Attention mechanisms (Wang et al., 2016) are utilized to aggregate the representation of informative attributes-aware words into a vector for emotion prediction, providing insights into which words contribute to a personal background. Experimental results show the usefulness of personal attributes in emotion detection, and the effectiveness of our proposed NPD model with both adversarial discriminators and attention mechanisms over the state-of-the-art discrete and neural models. 2 Related Work Earlier works on emotion detection are based on discrete models. For example, Yang et al. (2007) built a support vector machine (SVM) model"
D19-1560,D16-1011,0,0.0517837,"ground-truth sentiment rating for aspect xaspect . δ The model will assign a reward score to each seis a L2 regularization. quence according to the designed scores function, and then estimates b(τ h ) as the average of those 3 Experimentation rewards. Similarly, the policy gradient w.r.t. θl of 3.1 Experimental Settings low-level policy is given by, ∇θl J(θ ) = Eτ l ∼πl [ l ki  Rl ∇θl log π l (ai,j |sli,j ; θl )] j=1 (8) Data. We conduct our experiments on three public datasets on DASC, i.e., TripUser (Li et al., 2018), TripAdvisor (Wang et al., 2010) and BeerAdvocate (McAuley et al., 2012; Lei et al., 2016). In the experiment, we adopt Discourse Segmentation 5585 TripUser Development Test Acc.↑ MSE↓ Acc.↑ MSE↓ SVM 46.35† 1.025† LSTM 53.23 0.787 52.74 0.794 MAMC 55.49† 0.583† HARN 58.15† 0.528† HUARN 60.70† 0.514† C-HAN 58.49 0.602 57.38 0.543 HS-LSTM 59.75 0.566 59.01 0.524 RL-Word-Selection 60.15 0.475 59.55 0.519 RL-Clause-Selection 61.32 0.433 60.54 0.461 HRL 62.97 0.336 62.84 0.351 Approaches TripAdvisor Development Test Acc.↑ MSE↓ Acc.↑ MSE↓ 34.30‡ 1.982‡ 35.26‡ 1.963‡ 43.85‡ 1.525‡ 44.02‡ 1.470‡ 46.21‡ 1.091‡ 46.56‡ 1.083‡ 48.21‡ 0.923‡ 47.61 0.914 47.08 0.955 48.45 0.947 46.84 1.013 48.55"
D19-1560,D15-1167,0,0.042799,".8; λ1 , λ2 and λ3 are 0.25, 0.25 and 0.5 respectively. λ1 , λ2 are 0.6 and 0.4. Additionally, the batch size is set to be 64, regularization weight is set to be 10−5 and the dropout rate is 0.2. Evaluation Metrics. The performance is evaluated using Accuracy (Acc.) and MSE as Yin et al. (2017). Moreover, t-test is used to evaluate the signiﬁcance of the performance difference between two approaches (Yang and Liu, 1999). Baselines. We compare HRL with the following baselines: 1) SVM (Yin et al., 2017). This approach only adopts unigram, bigram as features to train an SVM classiﬁer. 2) LSTM (Tang et al., 2015). This is a neural network approach to document-level sentiment classiﬁcation which employs gated LSTM to learn text representation. 3) MAMC (Yin et al., 2017). This approach employs hierarchical iterative attention to learn aspect-speciﬁc representation. This is a state-of3 http://alt.qcri.org/tools/discourse-parser/ the-art approach to DASC. 4) HARN (Li et al., 2018). This approach adopts hierarchical attention to incorporate overall rating and aspect information so as to learn aspect-speciﬁc representation. This is another state-of-the-art approach to DASC. 5) HUARN (Li et al., 2018). This"
D19-1560,C18-1079,0,0.214585,"t classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010). This task aims to predict the sentiment rating for each given aspect mentioned in a document-level review. For instance, Figure 1 shows a review document with four given aspects of a hotel (i.e., location, room, value, service). The goal of DASC is to predict the rating score towards each aspect by analyzing the whole document. In the last decade, this task has been drawing more and more interests of researchers in the Natural Language Processing community (Titov and McDonald, 2008; Yin et al., 2017; Li et al., 2018). In previous studies, neu∗ Corresponding author - room: # # # $ $ (3) - service: # # # # $ (4) ral models have shown to be effective for performance improvement on DASC. Despite the advantages, these complex neural network approaches often offer little transparency w.r.t. their inner working mechanisms and suffer from the lack of interpretability. However, clearly understanding where and how such a model makes such a decision is rather important for developing real-world applications (Liu et al., 2018; Marcus, 2018). As human beings, if asked to evaluate the sentiment rating for a speciﬁc asp"
D19-1560,D16-1021,0,0.142468,"Missing"
D19-1560,D16-1127,0,0.103263,"Missing"
D19-1560,P10-1043,1,0.876096,"oach to DASC over the state-of-the-art baselines. 1 is a little uncomfortable .]Clause3 [I’m often nitpicking for room decoration.]]Clause4 [ Besides, the price is very expensive ] Clause5 [ although the staff service is professional .]]Clause6 Rating of Each Aspect - location: # # # # # (5) - value: # $ $ $ $ (1) Figure 1: An example of a review document, where clauses and words with different colors refer to different aspects. Introduction Document-level Aspect Sentiment Classiﬁcation (DASC) is a ﬁne-grained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010). This task aims to predict the sentiment rating for each given aspect mentioned in a document-level review. For instance, Figure 1 shows a review document with four given aspects of a hotel (i.e., location, room, value, service). The goal of DASC is to predict the rating score towards each aspect by analyzing the whole document. In the last decade, this task has been drawing more and more interests of researchers in the Natural Language Processing community (Titov and McDonald, 2008; Yin et al., 2017; Li et al., 2018). In previous studies, neu∗ Corresponding author - room: # # # $ $ (3) - ser"
D19-1560,P08-1036,0,0.0746941,"assiﬁcation (DASC) is a ﬁne-grained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010). This task aims to predict the sentiment rating for each given aspect mentioned in a document-level review. For instance, Figure 1 shows a review document with four given aspects of a hotel (i.e., location, room, value, service). The goal of DASC is to predict the rating score towards each aspect by analyzing the whole document. In the last decade, this task has been drawing more and more interests of researchers in the Natural Language Processing community (Titov and McDonald, 2008; Yin et al., 2017; Li et al., 2018). In previous studies, neu∗ Corresponding author - room: # # # $ $ (3) - service: # # # # $ (4) ral models have shown to be effective for performance improvement on DASC. Despite the advantages, these complex neural network approaches often offer little transparency w.r.t. their inner working mechanisms and suffer from the lack of interpretability. However, clearly understanding where and how such a model makes such a decision is rather important for developing real-world applications (Liu et al., 2018; Marcus, 2018). As human beings, if asked to evaluate th"
D19-1560,D16-1058,0,0.252027,"Missing"
D19-1560,D17-1217,0,0.61734,"e-grained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010). This task aims to predict the sentiment rating for each given aspect mentioned in a document-level review. For instance, Figure 1 shows a review document with four given aspects of a hotel (i.e., location, room, value, service). The goal of DASC is to predict the rating score towards each aspect by analyzing the whole document. In the last decade, this task has been drawing more and more interests of researchers in the Natural Language Processing community (Titov and McDonald, 2008; Yin et al., 2017; Li et al., 2018). In previous studies, neu∗ Corresponding author - room: # # # $ $ (3) - service: # # # # $ (4) ral models have shown to be effective for performance improvement on DASC. Despite the advantages, these complex neural network approaches often offer little transparency w.r.t. their inner working mechanisms and suffer from the lack of interpretability. However, clearly understanding where and how such a model makes such a decision is rather important for developing real-world applications (Liu et al., 2018; Marcus, 2018). As human beings, if asked to evaluate the sentiment rating"
D19-1560,C18-1074,0,0.0316687,"ect sentiment-relevant words and discard those irrelevant and noisy words. For instance, for aspect location, words “this”, “is” in Clause1 are noisy words and should be discarded since they make no contribution to implying the sentiment rating. One possible way to alleviate this problem is to also leverage the soft-attention mechanism as proposed in Li et al. (2018). However, this soft-attention mechanism may induce additional noise and lack interpretability because it tends to assign higher weights to some domain-speciﬁc words rather than real sentiment-relevant words (Mudinas et al., 2012; Zou et al., 2018). For instance, this soft-attention mechanism tends to regard the name of a hotel “Hilton” with a good reputation in Clause3 as a positive word which could mislead the model into assigning a higher rating to aspect room. Therefore, a well-behaved approach should highlight sentiment-relevant words and discard noisy words for a speciﬁc aspect during model training. In this paper, we propose a Hierarchical Reinforcement Learning (HRL) approach with a highlevel policy and a low-level policy to address the above two challenges in DASC. First, a highlevel policy is leveraged to select aspect-relevan"
lee-etal-2010-emotion,W09-3001,1,\N,Missing
lee-etal-2010-emotion,C08-1111,0,\N,Missing
lee-etal-2010-emotion,S07-1072,0,\N,Missing
lee-etal-2014-annotating,W09-3001,1,\N,Missing
lee-etal-2014-annotating,S07-1000,0,\N,Missing
lee-etal-2014-annotating,C08-1111,0,\N,Missing
P08-2065,P07-1056,0,0.888588,"d corpora is difficult and time-consuming. Given the limited multi-domain training data, an interesting task arises, how to best make full use of all training data to improve sentiment classification performance. We name Related Work Sentiment classification has become a hot topic since the publication work that discusses classification of movie reviews by Pang et al. (2002). This was followed by a great many studies into sentiment classification focusing on many domains besides that of movie. Research into sentiment classification over multiple domains remains sparse. It is worth noting that Blitzer et al. (2007) deal with the domain adaptation problem for sentiment classification where labeled data from one domain is used to train a classifier for classifying data from a different domain. Our work focuses on the problem of how to make multiple domains ‘help each other’ when all contain some labeled samples. These two problems are both important for real applications of sentiment classification. 3 3.1 Our Approaches Problem Statement In a standard supervised classification problem, we seek a predictor f (also called a classifier) that 257 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pa"
P08-2065,W02-1011,0,0.0208768,"ta from multiple domains. To achieve this, we propose two approaches of fusion, feature-level and classifier-level, to use training data from multiple domains simultaneously. Experimental studies show that multi-domain sentiment classification using the classifier-level approach performs much better than single domain classification (using the training data individually). 1 2 Introduction Sentiment classification is a special task of text categorization that aims to classify documents according to their opinion of, or sentiment toward a given subject (e.g., if an opinion is supported or not) (Pang et al., 2002). This task has created a considerable interest due to its wide applications. Sentiment classification is a very domainspecific problem; training a classifier using the data from one domain may fail when testing against data from another. As a result, real application systems usually require some labeled data from multiple domains, guaranteeing an acceptable performance for different domains. However, each domain has a very limited amount of training data due to the fact that creating largescale high-quality labeled corpora is difficult and time-consuming. Given the limited multi-domain traini"
P09-1078,P07-1056,0,0.0295975,"e carried out on both topic-based and sentiment text classification datasets. In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG. In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents 1 . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset2 (Pang and Lee, 2004) and one dataset from product reviews of domain DVD3 (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews. In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories. Classification Algorithm: Many classification algorithms are available for text classification, such as Naïve Bayes, Maximum Entropy, k-NN, and SVM. Among these methods, SVM is shown to perform better than other methods (Yang and Pedersen, 1997; Pang et al., 1 2 3 http://people.csail.mit.edu/~jrennie/20Newsgroups/ http://www.cs.cornell.edu/People/pabo/movie-review-data/ http://www.seas.upenn.edu/~mdredze/dataset"
P09-1078,P06-2079,0,0.0544922,"Missing"
P09-1078,W02-1011,0,0.0203942,"Missing"
P09-1078,P04-1035,0,0.00597525,"Experimental Studies 4.1 Experimental Setup Data Set: The experiments are carried out on both topic-based and sentiment text classification datasets. In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG. In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents 1 . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset2 (Pang and Lee, 2004) and one dataset from product reviews of domain DVD3 (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews. In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories. Classification Algorithm: Many classification algorithms are available for text classification, such as Naïve Bayes, Maximum Entropy, k-NN, and SVM. Among these methods, SVM is shown to perform better than other methods (Yang and Pedersen, 1997; Pang et al., 1 2 3 http://people.csail.mit.edu/~jrennie/20Newsgroups/ http://www.cs.cornell.ed"
P09-1078,W06-1652,0,0.262065,"Missing"
P10-1043,P07-1056,0,0.914534,"rmally domain-independent and serves as highly relevant clues to sentiment classification. The latter type of statement called impersonal view, e.g. ‘it is too small’, contains Y ’s “objective” (i.e. or at least criteria-based) evaluation of the target object. This kind of information tends to contain much domain-specific classification knowledge. Although such information is sometimes not as explicit as personal views in classifying the sentiment of a text, speaker’s sentiment is usually implied by the evaluation result. It is well-known that sentiment classification is very domain-specific (Blitzer et al., 2007), so it is critical to eliminate its dependence on a large-scale labeled data for its wide applications. Since the unlabeled data is ample and easy to collect, a successful semi-supervised sentiment classification system would significantly minimize the involvement of labor and time. Therefore, given the two different views mentioned above, one promising application is to adopt them in co-training algorithms, which has been proven to be an effective semi-supervised learning strategy of incorporating unlabeled data to further improve the classification performance (Zhu, 2005). In addition, we w"
P10-1043,P09-1027,0,0.158629,"7) present a domain adaptation approach for sentiment classification. Semi-supervised methods combine unlabeled data with labeled training data (often small-scaled) to improve the models. Compared to the supervised and unsupervised methods, semi-supervised methods for sentiment classification are relatively new and have much less related studies. Dasgupta and Ng (2009) integrate various methods in semi-supervised sentiment classification including spectral clustering, active learning, transductive learning, and ensemble learning. They achieve a very impressive improvement across five domains. Wan (2009) applies a co-training method to semi-supervised learning with labeled English corpus and unlabeled Chinese corpus for Chinese sentiment classification. 3 Unsupervised Mining of Personal and Impersonal Views As mentioned in Section 1, the objective of sentiment classification is to classify a specific binary relation: X ’s evaluation on Y, where X is an object set including different kinds of persons and Y is another object set including the target objects to be evaluated. First of all, we focus on an analysis on sentences in product reviews regarding the two views: personal and impersonal vie"
P10-1043,J09-3003,0,0.0225,"Section 3 presents our unsupervised approach for mining personal and impersonal views. Section 4 and Section 5 propose our supervised and semi-supervised methods on sentiment classification respectively. Experimental results are presented and analyzed in Section 6. Section 7 discusses on the differences between personal/impersonal and subjective/objective. Finally, Section 8 draws our conclusions and outlines the future work. 2 Related Work Recently, a variety of studies have been reported on sentiment classification at different levels: word level (Esuli and Sebastiani, 2005), phrase level (Wilson et al., 2009), sentence level (Kim and Hovy, 2004; Liu et al., 2005), and document level (Turney, 2002; Pang et al., 2002). This paper focuses on the document-level sentiment classification. Generally, document-level sentiment classification methods can be categorized into three types: unsupervised, supervised, and semi-supervised. Unsupervised methods involve deriving a sentiment classifier without any labeled documents. Most of previous work use a set of labeled sentiment words called seed words to perform unsupervised classification. Turney (2002) determines the sentiment orientation of a document by ca"
P10-1043,C08-1135,0,0.0450621,"on methods can be categorized into three types: unsupervised, supervised, and semi-supervised. Unsupervised methods involve deriving a sentiment classifier without any labeled documents. Most of previous work use a set of labeled sentiment words called seed words to perform unsupervised classification. Turney (2002) determines the sentiment orientation of a document by calculating point-wise mutual information between the words in the document and the seed words of ‘excellent’ and ‘poor’. Kennedy and Inkpen (2006) use a term-counting method with a set of seed words to determine the sentiment. Zagibalov and Carroll (2008) first propose a seed word selection approach and then apply the same term-counting method for Chinese sentiment classifications. These unsupervised approaches are believed to be domain-independent for sentiment classification. Supervised methods consider sentiment classification as a standard classification problem in which labeled data in a domain are used to train a domain-specific classifier. Pang et al. (2002) are the first to apply supervised machine learning methods to sentiment classification. Subsequently, many other studies make efforts to improve the performance of machine learning-"
P10-1043,C04-1200,0,0.118119,"pproach for mining personal and impersonal views. Section 4 and Section 5 propose our supervised and semi-supervised methods on sentiment classification respectively. Experimental results are presented and analyzed in Section 6. Section 7 discusses on the differences between personal/impersonal and subjective/objective. Finally, Section 8 draws our conclusions and outlines the future work. 2 Related Work Recently, a variety of studies have been reported on sentiment classification at different levels: word level (Esuli and Sebastiani, 2005), phrase level (Wilson et al., 2009), sentence level (Kim and Hovy, 2004; Liu et al., 2005), and document level (Turney, 2002; Pang et al., 2002). This paper focuses on the document-level sentiment classification. Generally, document-level sentiment classification methods can be categorized into three types: unsupervised, supervised, and semi-supervised. Unsupervised methods involve deriving a sentiment classifier without any labeled documents. Most of previous work use a set of labeled sentiment words called seed words to perform unsupervised classification. Turney (2002) determines the sentiment orientation of a document by calculating point-wise mutual informat"
P10-1043,P07-1055,0,0.0367316,"nt classification. Supervised methods consider sentiment classification as a standard classification problem in which labeled data in a domain are used to train a domain-specific classifier. Pang et al. (2002) are the first to apply supervised machine learning methods to sentiment classification. Subsequently, many other studies make efforts to improve the performance of machine learning-based classifiers by various means, such as using subjectivity summarization (Pang and Lee, 2004), seeking new superior textual features (Riloff et al., 2006), and employing document subcomponent information (McDonald et al., 2007). As far as the challenge of domain-dependency is concerned, Blitzer et al. (2007) present a domain adaptation approach for sentiment classification. Semi-supervised methods combine unlabeled data with labeled training data (often small-scaled) to improve the models. Compared to the supervised and unsupervised methods, semi-supervised methods for sentiment classification are relatively new and have much less related studies. Dasgupta and Ng (2009) integrate various methods in semi-supervised sentiment classification including spectral clustering, active learning, transductive learning, and ens"
P10-1043,W02-1011,0,0.0239994,"statements towards a target object for evaluation. To obtain them, an unsupervised mining approach is proposed. On this basis, an ensemble method and a co-training algorithm are explored to employ the two views in supervised and semi-supervised sentiment classification respectively. Experimental results across eight domains demonstrate the effectiveness of our proposed approach. 1 Introduction As a special task of text classification, sentiment classification aims to classify a text according to the expressed sentimental polarities of opinions such as ‘thumb up’ or ‘thumb down’ on the movies (Pang et al., 2002). This task has recently received considerable interests in the Natural Language Processing (NLP) community due to its wide applications. In general, the objective of sentiment classification can be represented as a kind of binary relation R, defined as an ordered triple (X, Y, G), where X is an object set including different kinds of people (e.g. writers, reviewers, or users), Y is another object set including the target objects (e.g. products, events, or even some people), and G is a subset of the Cartesian product X × Y . The concerned relation in sentiment classification is X ’s evaluation"
P10-1043,W06-1652,0,0.105249,"supervised approaches are believed to be domain-independent for sentiment classification. Supervised methods consider sentiment classification as a standard classification problem in which labeled data in a domain are used to train a domain-specific classifier. Pang et al. (2002) are the first to apply supervised machine learning methods to sentiment classification. Subsequently, many other studies make efforts to improve the performance of machine learning-based classifiers by various means, such as using subjectivity summarization (Pang and Lee, 2004), seeking new superior textual features (Riloff et al., 2006), and employing document subcomponent information (McDonald et al., 2007). As far as the challenge of domain-dependency is concerned, Blitzer et al. (2007) present a domain adaptation approach for sentiment classification. Semi-supervised methods combine unlabeled data with labeled training data (often small-scaled) to improve the models. Compared to the supervised and unsupervised methods, semi-supervised methods for sentiment classification are relatively new and have much less related studies. Dasgupta and Ng (2009) integrate various methods in semi-supervised sentiment classification inclu"
P10-1043,P02-1053,0,0.0110004,"and Section 5 propose our supervised and semi-supervised methods on sentiment classification respectively. Experimental results are presented and analyzed in Section 6. Section 7 discusses on the differences between personal/impersonal and subjective/objective. Finally, Section 8 draws our conclusions and outlines the future work. 2 Related Work Recently, a variety of studies have been reported on sentiment classification at different levels: word level (Esuli and Sebastiani, 2005), phrase level (Wilson et al., 2009), sentence level (Kim and Hovy, 2004; Liu et al., 2005), and document level (Turney, 2002; Pang et al., 2002). This paper focuses on the document-level sentiment classification. Generally, document-level sentiment classification methods can be categorized into three types: unsupervised, supervised, and semi-supervised. Unsupervised methods involve deriving a sentiment classifier without any labeled documents. Most of previous work use a set of labeled sentiment words called seed words to perform unsupervised classification. Turney (2002) determines the sentiment orientation of a document by calculating point-wise mutual information between the words in the document and the seed wo"
P10-1043,P04-1035,0,\N,Missing
P10-1043,P09-1079,0,\N,Missing
P13-2091,I08-1041,0,0.0192107,"ve and negative) with a high probability and does not apply to fine-grained emotion categories (e.g., happy, angry, and sad). This motivates our joint modeling in terms of the coarse-grained emotion categories. Specifically, we consider the news text and the comment text as two different views of expressing either the news reader’s or comment writer’s emotions. Given the two views, a co-training algorithm is proposed to perform semi-supervised emotion classification so that the information in the unlabeled data can be exploited to improve the classification performance. ing (Alm et al., 2005; Aman and Szpakowicz, 2008; Chen et al., 2010; Purver and Battersby, 2012; Moshfeghi et al., 2011), and so far, we have not seen any studies on semi-supervised learning on fine-grained emotion classification. 2 3 2.1 Related Work Comment Writer’s Emotion Classification Comment writer’s emotion classification has been a hot research topic in NLP during the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment cl"
P13-2091,C10-1021,1,0.605042,". I still can not forget last year. (2) My father-in-law got to experience this quake... what a suffering. Comment Writer’s emotion: sad Comment Reader’s emotion: Unknown Introduction Emotion classification aims to predict the emotion categories (e.g., happy, angry, or sad) of a given text (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). With the rapid growth of computer mediated communication applications, such as social websites and miro-blogs, the research on emotion classification has been attracting more and more attentions recently from the natural language processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012). In general, a single text may possess two kinds of emotions, writer’s emotion and reader’s emotion, where the former concerns the emotion expressed by the writer when writing the text and the latter concerns the emotion expressed by a reader after reading the text. For example, consider two short texts drawn from a news and corresponding comments, as shown in Figure 1. On * * Corresponding author Figure 1: An example of writer’s and reader’s emotions on a news and its comments Accordingly, emotion classification can be grouped into two categories: reader’s emotio"
P13-2091,W02-1011,0,0.0151406,"the two views, a co-training algorithm is proposed to perform semi-supervised emotion classification so that the information in the unlabeled data can be exploited to improve the classification performance. ing (Alm et al., 2005; Aman and Szpakowicz, 2008; Chen et al., 2010; Purver and Battersby, 2012; Moshfeghi et al., 2011), and so far, we have not seen any studies on semi-supervised learning on fine-grained emotion classification. 2 3 2.1 Related Work Comment Writer’s Emotion Classification Comment writer’s emotion classification has been a hot research topic in NLP during the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et"
P13-2091,E12-1049,0,0.120126,"orget last year. (2) My father-in-law got to experience this quake... what a suffering. Comment Writer’s emotion: sad Comment Reader’s emotion: Unknown Introduction Emotion classification aims to predict the emotion categories (e.g., happy, angry, or sad) of a given text (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). With the rapid growth of computer mediated communication applications, such as social websites and miro-blogs, the research on emotion classification has been attracting more and more attentions recently from the natural language processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012). In general, a single text may possess two kinds of emotions, writer’s emotion and reader’s emotion, where the former concerns the emotion expressed by the writer when writing the text and the latter concerns the emotion expressed by a reader after reading the text. For example, consider two short texts drawn from a news and corresponding comments, as shown in Figure 1. On * * Corresponding author Figure 1: An example of writer’s and reader’s emotions on a news and its comments Accordingly, emotion classification can be grouped into two categories: reader’s emotion and writer’s emotion classi"
P13-2091,D09-1150,0,0.697347,"emotion while the emotion of a reader after reading the comments is not clear (Some may feel sorry but others might feel careless). News: Today's Japan earthquake could be 2011 quake aftershock. …… News Writer’s emotion: None News Reader’s emotion: sad, worried Comments: (1) I hope everything is ok, so sad. I still can not forget last year. (2) My father-in-law got to experience this quake... what a suffering. Comment Writer’s emotion: sad Comment Reader’s emotion: Unknown Introduction Emotion classification aims to predict the emotion categories (e.g., happy, angry, or sad) of a given text (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). With the rapid growth of computer mediated communication applications, such as social websites and miro-blogs, the research on emotion classification has been attracting more and more attentions recently from the natural language processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012). In general, a single text may possess two kinds of emotions, writer’s emotion and reader’s emotion, where the former concerns the emotion expressed by the writer when writing the text and the latter concerns the emotion expressed by a reader after reading the te"
P13-2091,P09-2038,0,0.311983,"motion of a reader after reading the comments is not clear (Some may feel sorry but others might feel careless). News: Today's Japan earthquake could be 2011 quake aftershock. …… News Writer’s emotion: None News Reader’s emotion: sad, worried Comments: (1) I hope everything is ok, so sad. I still can not forget last year. (2) My father-in-law got to experience this quake... what a suffering. Comment Writer’s emotion: sad Comment Reader’s emotion: Unknown Introduction Emotion classification aims to predict the emotion categories (e.g., happy, angry, or sad) of a given text (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). With the rapid growth of computer mediated communication applications, such as social websites and miro-blogs, the research on emotion classification has been attracting more and more attentions recently from the natural language processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012). In general, a single text may possess two kinds of emotions, writer’s emotion and reader’s emotion, where the former concerns the emotion expressed by the writer when writing the text and the latter concerns the emotion expressed by a reader after reading the text. For example, consider two"
P13-2091,W06-1652,0,0.014076,"in NLP during the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grained emotion classification aims to classify a text into multiple emotion categories, such as happy, angry, and sad. One main group of related studies on this task is about emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). Besides, all the related studies focus on supervised learn2.2 News Reader’s Emotion Classification While comment writer’s emotion cl"
P13-2091,P09-1079,0,0.0353899,"st decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grained emotion classification aims to classify a text into multiple emotion categories, such as happy, angry, and sad. One main group of related studies on this task is about emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). Besides, all the related studies focus on supervised learn2.2 News Reader’s Emotion Classification While comment writer’s emotion classification has been e"
P13-2091,P02-1053,0,0.00981,"-training algorithm is proposed to perform semi-supervised emotion classification so that the information in the unlabeled data can be exploited to improve the classification performance. ing (Alm et al., 2005; Aman and Szpakowicz, 2008; Chen et al., 2010; Purver and Battersby, 2012; Moshfeghi et al., 2011), and so far, we have not seen any studies on semi-supervised learning on fine-grained emotion classification. 2 3 2.1 Related Work Comment Writer’s Emotion Classification Comment writer’s emotion classification has been a hot research topic in NLP during the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li"
P13-2091,P10-1043,1,0.556285,"2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grained emotion classification aims to classify a text into multiple emotion categories, such as happy, angry, and sad. One main group of related studies on this task is about emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). Besides, all the related studies focus on supervised learn2.2 News Reader’s Emotion Classification While comment writer’s emotion classification has been extensively studie"
P13-2091,E12-1031,0,0.100839,"slike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grained emotion classification aims to classify a text into multiple emotion categories, such as happy, angry, and sad. One main group of related studies on this task is about emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). Besides, all the related studies focus on supervised learn2.2 News Reader’s Emotion Classification While comment writer’s emotion classification has been extensively studied, there are only a few studies on news reader’s emotion classification from the NLP and related communities. Lin et al. (2007) first describe the task of reader’s emotion classification on the news articles and then employ some standard machine learning approaches to train a classifier for determining the reader’s em"
P13-2091,J09-3003,0,0.0290799,"to perform semi-supervised emotion classification so that the information in the unlabeled data can be exploited to improve the classification performance. ing (Alm et al., 2005; Aman and Szpakowicz, 2008; Chen et al., 2010; Purver and Battersby, 2012; Moshfeghi et al., 2011), and so far, we have not seen any studies on semi-supervised learning on fine-grained emotion classification. 2 3 2.1 Related Work Comment Writer’s Emotion Classification Comment writer’s emotion classification has been a hot research topic in NLP during the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grai"
P13-2091,C10-1136,0,0.440288,"uch as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grained emotion classification aims to classify a text into multiple emotion categories, such as happy, angry, and sad. One main group of related studies on this task is about emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). Besides, all the related studies focus on supervised learn2.2 News Reader’s Emotion Classification While comment writer’s emotion classification has been extensively studied, there are only a few studies on news reader’s emotion classification from the NLP and related communities. Lin et al. (2007) first describe the task of reader’s emotion classification on the news articles and then employ some standard machine learning approaches to train a classifier for dete"
P13-2091,H05-1073,0,\N,Missing
P13-2093,H05-1044,0,0.111715,"Missing"
P13-2093,I08-1039,0,0.0832963,"Missing"
P13-2093,C04-1200,0,0.270078,"Missing"
P13-2093,Y09-1033,1,0.888859,"Missing"
P13-2093,C10-1072,1,0.885018,"ing as training data, and the remaining one fold serving as test data. All of the following results are reported in terms of an average of 5-fold cross validation. 4.2 Evaluated Systems We evaluate four machine learning systems that are proposed to address polarity shift in document-level polarity classification: 1) Baseline: standard machine learning methods based on the BOW model, without handling polarity shift; 2) Das-2001: the method proposed by Das and Chen (2001), where “NOT” is attached to the words in the scope of negation as a preprocessing step; 3) Li-2010: the approach proposed by Li et al. (2010). The details of the algorithm is introduced in related work; 4) DTDP: our approach proposed in Section 3. The WordNet dictionary is used for sample reversion. The empirical value of the parameter a and t are used in the evaluation. 4.3 Comparison of the Evaluated Systems In table 1, we report the classification accuracy of four evaluated systems using unigram features. We consider two widely-used classification algorithms: SVM and Naïve Bayes. For SVM, the LibSVM toolkit3 is used with a linear kernel and the default penalty parameter. For Naïve Bayes, the OpenPR-NB toolkit4 is used. 2 http://"
P13-2093,W02-1011,0,0.0228982,"Missing"
P13-2093,P02-1053,0,0.00767782,"Missing"
P14-2136,W06-0901,0,0.473053,"trigger type determination in event extraction. To make the training data in different languages help each other, we propose a uniform text representation with bilingual features to represent the samples and handle the difficulty of locating the triggers in the translated text from both monolingual and bilingual perspectives. Empirical studies demonstrate the effectiveness of the proposed approach to bilingual classification on trigger type determination.  1 Introduction Event extraction is an increasingly hot and challenging research topic in the natural language processing (NLP) community (Ahn, 2006; Saun et al. 2006; Zhao et al. 2008). It aims to automatically extract certain types of events with the arguments to present the texts under a structured form. In event extraction, there are four primary subtasks, named trigger identification, trigger type determination, argument identification, and argument role determination (Chen and NG, 2012). As an important technology in information extraction, event extraction could be applied to many fields such as information retrieval, summarization, text mining, and question answering. Recently, the dominative approach to event extraction is based"
P14-2136,C12-1033,0,0.0128044,"udies demonstrate the effectiveness of the proposed approach to bilingual classification on trigger type determination.  1 Introduction Event extraction is an increasingly hot and challenging research topic in the natural language processing (NLP) community (Ahn, 2006; Saun et al. 2006; Zhao et al. 2008). It aims to automatically extract certain types of events with the arguments to present the texts under a structured form. In event extraction, there are four primary subtasks, named trigger identification, trigger type determination, argument identification, and argument role determination (Chen and NG, 2012). As an important technology in information extraction, event extraction could be applied to many fields such as information retrieval, summarization, text mining, and question answering. Recently, the dominative approach to event extraction is based on supervised learning where a set of labeled samples are exploited to train a model to extract the events. However, the availa * Corresponding author ble labeled data are rather sparse due to various kinds of event categories. For example, the event taxonomy in ACE 2005 1 (Automatic Content Extraction) includes 8 types of events, with 33 subtype"
P14-2136,N09-2053,0,0.0196321,"ction. Hong et al. (2011) leverage cross-entity information to improve traditional event extraction, regarding entity type consistency as a key feature. More recently, Li et al. (2013) propose a joint framework based on structured prediction which extracts triggers and arguments together. In Chinese, relevant studies in event extraction are in a relatively primary stage with focus on more special characteristics and challenges. Tan et al. (2008) employ local feature selection and explicit discrimination of positive and negative features to ensure the performance of trigger type determination. Chen and Ji (2009) apply lexical, syntactic and semantic features in trigger labeling and argument labeling to improve the performance. More recently, Li et al. (2012) and Li et al. (2013) introduce two inference mechanisms to infer unknown triggers and recover trigger mentions respectively with morphological structures. In comparison with above studies, we focus on bilingual event extraction. Although bilingual classification has been paid lots of attention in other fields (Wan 2008; Haghighi et al., 2008; Ismail et al., 2010; Lu et al., 2011；Li et al., 2013), there is few related work in event extraction. The"
P14-2136,P08-1088,0,0.0919865,"Missing"
P14-2136,P11-1113,1,0.872285,"approaches have been explored recently. Bethard and Martin (2006) formulate the event identification as a classification problem in a wordchunking paradigm, introducing a variety of linguistically motivated features. Ahn (2006) proposes a trigger-based method. It first identifies the trigger in an event, and then uses a multiclassifier to implement trigger type determination. Ji and Grishman (2008) employ an approach to propagate consistent event arguments across sentences and documents. Liao and Grishman (2010) apply document level information to improve the performance of event extraction. Hong et al. (2011) leverage cross-entity information to improve traditional event extraction, regarding entity type consistency as a key feature. More recently, Li et al. (2013) propose a joint framework based on structured prediction which extracts triggers and arguments together. In Chinese, relevant studies in event extraction are in a relatively primary stage with focus on more special characteristics and challenges. Tan et al. (2008) employ local feature selection and explicit discrimination of positive and negative features to ensure the performance of trigger type determination. Chen and Ji (2009) apply"
P14-2136,P10-1081,0,0.164099,"nt extraction has been mainly studied in both English and Chinese. In English, various supervised learning approaches have been explored recently. Bethard and Martin (2006) formulate the event identification as a classification problem in a wordchunking paradigm, introducing a variety of linguistically motivated features. Ahn (2006) proposes a trigger-based method. It first identifies the trigger in an event, and then uses a multiclassifier to implement trigger type determination. Ji and Grishman (2008) employ an approach to propagate consistent event arguments across sentences and documents. Liao and Grishman (2010) apply document level information to improve the performance of event extraction. Hong et al. (2011) leverage cross-entity information to improve traditional event extraction, regarding entity type consistency as a key feature. More recently, Li et al. (2013) propose a joint framework based on structured prediction which extracts triggers and arguments together. In Chinese, relevant studies in event extraction are in a relatively primary stage with focus on more special characteristics and challenges. Tan et al. (2008) employ local feature selection and explicit discrimination of positive and"
P14-2136,P11-1033,0,0.0212729,"features to ensure the performance of trigger type determination. Chen and Ji (2009) apply lexical, syntactic and semantic features in trigger labeling and argument labeling to improve the performance. More recently, Li et al. (2012) and Li et al. (2013) introduce two inference mechanisms to infer unknown triggers and recover trigger mentions respectively with morphological structures. In comparison with above studies, we focus on bilingual event extraction. Although bilingual classification has been paid lots of attention in other fields (Wan 2008; Haghighi et al., 2008; Ismail et al., 2010; Lu et al., 2011；Li et al., 2013), there is few related work in event extraction. The only one related work we find is Ji (2009) which proposes an inductive learning approach to exploit cross-lingual predicate clusters to improve the event extraction task with the main goal to get the event taggers from extra resources, i.e., an English and Chinese parallel corpus. Differently, our goal is to make the lawww.google.com 843 beled data from two languages help each other without any other extra resources, which is original in the study of event extraction. 3 The Proposed Approach Trigger type determination aims t"
P14-2136,W99-0604,0,0.360088,"Missing"
P14-2136,D08-1058,0,0.469981,"e determination. Accordingly, our goal is to design a classifier which is trained with labeled data from two different languages and is capable of classifying the test data from both languages. Generally, this task possesses two main challenges. The first challenge is text representation, namely, how to eliminate the language gap between the two languages. To tackle this, we first employ Google Translate2, a state-of-the-art machine translation system, to gain the translation of an event instance, similar to what has been widely done by previous studies in bilingual classification tasks e.g., Wan (2008); Then, we uniformly represent each text with bilingual word features. That is, we augment each original feature vector into a novel one which contains the translated features. The second challenge is the translation for some specific features. It is well-known that some specific features, such as the triggers and their context features, are extremely important for determining the event types. For example, in E3, both trigger “left” and named entity “Saddam” are important features to tell the event type, i.e., &quot;Transport/Movement&quot;. When it is translated to Chinese, it is also required to know"
P14-2136,C10-2055,0,0.0598022,"Missing"
P14-2136,W09-1704,0,0.254569,"ng et al. (2011) leverage cross-entity information to improve traditional event extraction, regarding entity type consistency as a key feature. More recently, Li et al. (2013) propose a joint framework based on structured prediction which extracts triggers and arguments together. In Chinese, relevant studies in event extraction are in a relatively primary stage with focus on more special characteristics and challenges. Tan et al. (2008) employ local feature selection and explicit discrimination of positive and negative features to ensure the performance of trigger type determination. Chen and Ji (2009) apply lexical, syntactic and semantic features in trigger labeling and argument labeling to improve the performance. More recently, Li et al. (2012) and Li et al. (2013) introduce two inference mechanisms to infer unknown triggers and recover trigger mentions respectively with morphological structures. In comparison with above studies, we focus on bilingual event extraction. Although bilingual classification has been paid lots of attention in other fields (Wan 2008; Haghighi et al., 2008; Ismail et al., 2010; Lu et al., 2011；Li et al., 2013), there is few related work in event extraction. The"
P14-2136,P08-1030,0,0.150833,"al studies. In Section 5, we conclude our work and give some future work. 2 Related Work In the NLP community, event extraction has been mainly studied in both English and Chinese. In English, various supervised learning approaches have been explored recently. Bethard and Martin (2006) formulate the event identification as a classification problem in a wordchunking paradigm, introducing a variety of linguistically motivated features. Ahn (2006) proposes a trigger-based method. It first identifies the trigger in an event, and then uses a multiclassifier to implement trigger type determination. Ji and Grishman (2008) employ an approach to propagate consistent event arguments across sentences and documents. Liao and Grishman (2010) apply document level information to improve the performance of event extraction. Hong et al. (2011) leverage cross-entity information to improve traditional event extraction, regarding entity type consistency as a key feature. More recently, Li et al. (2013) propose a joint framework based on structured prediction which extracts triggers and arguments together. In Chinese, relevant studies in event extraction are in a relatively primary stage with focus on more special character"
P14-2136,N03-1017,0,0.0126015,"Missing"
P14-2136,C12-1099,1,0.885174,"Missing"
P14-2136,P13-1008,0,0.0914419,"roducing a variety of linguistically motivated features. Ahn (2006) proposes a trigger-based method. It first identifies the trigger in an event, and then uses a multiclassifier to implement trigger type determination. Ji and Grishman (2008) employ an approach to propagate consistent event arguments across sentences and documents. Liao and Grishman (2010) apply document level information to improve the performance of event extraction. Hong et al. (2011) leverage cross-entity information to improve traditional event extraction, regarding entity type consistency as a key feature. More recently, Li et al. (2013) propose a joint framework based on structured prediction which extracts triggers and arguments together. In Chinese, relevant studies in event extraction are in a relatively primary stage with focus on more special characteristics and challenges. Tan et al. (2008) employ local feature selection and explicit discrimination of positive and negative features to ensure the performance of trigger type determination. Chen and Ji (2009) apply lexical, syntactic and semantic features in trigger labeling and argument labeling to improve the performance. More recently, Li et al. (2012) and Li et al. (2"
P14-2136,W06-1618,0,\N,Missing
P15-1101,H05-1073,0,0.059114,"context dependence in the corpus. Section 4 proposes our DFG approach to sentence-level emotion classification. Section 5 evaluates the proposed approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, there has been an explosion of work exploring various aspects of emotion analysis, such as emotion resource creation (Wiebe et al., 2005; Quan and Ren, 2009; Xu et al., 2010), writer’s emotion vs. reader’s emotion analysis (Lin et al., 2008; Liu et al., 2013), emotion cause event analysis (Chen et al., 2010), document-level emotion classification (Alm et al., 2005; Li et al., 2014) and sentence-level or short text-level emotion classification (Tokushisa et al., 2008; Bhowmick et al., 2009; Xu et al., 2012). This work focuses on sentence-level emotion classification. Among the studies on sentence-level emotion classification, Tokushisa et al. (2008) propose a data-oriented method for inferring the emotion of an utterance sentence in a dialog system. They leverage a huge collection of emotion-provoking event instances from the Web to deal with the data sparseness problem in sentence-level emotion classification. Bhowmick et al. (2009) and Bhowmick et al."
P15-1101,C10-1021,1,0.385285,"analysis. Section 3 presents our observations on label and context dependence in the corpus. Section 4 proposes our DFG approach to sentence-level emotion classification. Section 5 evaluates the proposed approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, there has been an explosion of work exploring various aspects of emotion analysis, such as emotion resource creation (Wiebe et al., 2005; Quan and Ren, 2009; Xu et al., 2010), writer’s emotion vs. reader’s emotion analysis (Lin et al., 2008; Liu et al., 2013), emotion cause event analysis (Chen et al., 2010), document-level emotion classification (Alm et al., 2005; Li et al., 2014) and sentence-level or short text-level emotion classification (Tokushisa et al., 2008; Bhowmick et al., 2009; Xu et al., 2012). This work focuses on sentence-level emotion classification. Among the studies on sentence-level emotion classification, Tokushisa et al. (2008) propose a data-oriented method for inferring the emotion of an utterance sentence in a dialog system. They leverage a huge collection of emotion-provoking event instances from the Web to deal with the data sparseness problem in sentence-level emotion c"
P15-1101,P13-2091,1,0.778375,"she turn over to me and her little soft hand fall onto my face.</S1&gt; <S2&gt;Praise the Lord, that is all I want.</S2&gt; <S3&gt;Feeling the warm of her hand and the attachment she hold to me, I couldn’t afford to move even a little, fearing I may lost her hand.</S3&gt;)……) ------------------------------------------------------------------- Sentence-level Emotion Classification  Input: Introduction  Output: Predicting emotion categories, such as anger, joy, and anxiety, expressed by a piece of text encompasses a variety of applications, such as online chatting (Galik et al., 2012), news classification (Liu et al., 2013) and stock marketing (Bollen et al., 2011). Over the past decade, there has been a substantial body of research on emotion classification, where a considerable amount of work has focused on document-level emotion classification. Recently, the research community has become increasingly aware of the need on sentence-level emotion classification due to its wide potential applications, e.g. the massively growing importance of analyzing short text in social media (Kiritchenko et al., 2014; Wen and Wan, 2014). In general, sentence-level emotion classification exhibits two challenges. 1 S1, S2, S3 S1"
P15-1101,D09-1150,0,0.054791,"t of work has focused on document-level emotion classification. Recently, the research community has become increasingly aware of the need on sentence-level emotion classification due to its wide potential applications, e.g. the massively growing importance of analyzing short text in social media (Kiritchenko et al., 2014; Wen and Wan, 2014). In general, sentence-level emotion classification exhibits two challenges. 1 S1, S2, S3 S1 : joy, love S2: joy S3: joy, love, anxiety Figure 1: An example of a paragraph and the sentences therein with their emotion categories from the corpus collected by Quan and Ren (2009) On one hand, like document-level emotion classification, sentence-level emotion classification is naturally a multi-label classification problem. That is, each sentence might involve more than one emotion category. For example, as shown in Figure 1, in one paragraph, two sentences, i.e., S1 and S3, have two and three emotion categories respectively. Automatically classifying instances with multiple possible categories is * Corresponding author 1045 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lang"
P15-1101,C08-1111,0,0.0139364,"Missing"
P15-1101,C10-1136,0,0.0192099,"sification into a unified framework. The remainder of this paper is organized as follows. Section 2 overviews related work on emotion analysis. Section 3 presents our observations on label and context dependence in the corpus. Section 4 proposes our DFG approach to sentence-level emotion classification. Section 5 evaluates the proposed approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, there has been an explosion of work exploring various aspects of emotion analysis, such as emotion resource creation (Wiebe et al., 2005; Quan and Ren, 2009; Xu et al., 2010), writer’s emotion vs. reader’s emotion analysis (Lin et al., 2008; Liu et al., 2013), emotion cause event analysis (Chen et al., 2010), document-level emotion classification (Alm et al., 2005; Li et al., 2014) and sentence-level or short text-level emotion classification (Tokushisa et al., 2008; Bhowmick et al., 2009; Xu et al., 2012). This work focuses on sentence-level emotion classification. Among the studies on sentence-level emotion classification, Tokushisa et al. (2008) propose a data-oriented method for inferring the emotion of an utterance sentence in a dialog system. They leverage a"
P15-2005,W02-1011,0,0.0350279,"ation to guarantee a suitable size of the data for training the meta-classifier. Evaluation on four domains shows that such a semi-stacking strategy performs consistently better than its member algorithms. 1 Introduction The past decade has witnessed a huge exploding interest in sentiment analysis from the natural language processing and data mining communities due to its inherent challenges and wide applications (Pang et al., 2008; Liu, 2012). One fundamental task in sentiment analysis is sentiment classification, which aims to determine the sentimental orientation a piece of text expresses (Pang et al., 2002). For instance, the sentence &quot;I absolutely love this product.&quot; is supposed to be determined as a positive expression in sentimental orientation.  While early studies focus on supervised learning, where only labeled data are required to train the classification model (Pang et al., 2002), recent studies devote more and more to reduce the heavy dependence on the large amount of labeled data by exploiting semi-supervised learning approaches, such as co-training (Wan, 2009; Li et al., 2011), label propagation (Sindhwani and Melville, 2008), and deep learning (Zhou et al., 2013), to sentiment class"
P15-2005,W06-1652,0,0.629157,"the learning algorithm aims to learn a classifier from a small scale of labeled samples, named initial labeled data, with a large number of unlabeled samples. In the sequel, we refer the labeled data as L  {( xi , yi )}inL1 where xi R d is the d dimensional input vector, and yi is its output label. The unlabeled data in the target domain is denoted as U  {( xk )}nkU1 . Suppose l semi is a semi-supervised Related Work Early studies on sentiment classification mainly focus on supervised learning methods with algorithm designing and feature engineering (Pang et al., 2002; Cui et al., 2006; Riloff et al., 2006; Li et al., 2009). Recently, most studies on sentiment classification aim to improve the performance by exploiting unlabeled data in two main aspects: semi-supervised learning (Dasgupta and Ng, 2009; Wan, 2009; Li et al., 2010) and cross-domain learning (Blitzer et al. 2007; He et al. 2011; Li et al., 2013). Specifically, existing approaches to semi-supervised sentiment classification could be categorized into two main groups: bootstrappingstyle and graph-based. As for bootstrapping-style approaches, Wan (2009) considers two different languages as two views and applies co-training to conduct"
P15-2005,P07-1056,0,0.374058,"and yi is its output label. The unlabeled data in the target domain is denoted as U  {( xk )}nkU1 . Suppose l semi is a semi-supervised Related Work Early studies on sentiment classification mainly focus on supervised learning methods with algorithm designing and feature engineering (Pang et al., 2002; Cui et al., 2006; Riloff et al., 2006; Li et al., 2009). Recently, most studies on sentiment classification aim to improve the performance by exploiting unlabeled data in two main aspects: semi-supervised learning (Dasgupta and Ng, 2009; Wan, 2009; Li et al., 2010) and cross-domain learning (Blitzer et al. 2007; He et al. 2011; Li et al., 2013). Specifically, existing approaches to semi-supervised sentiment classification could be categorized into two main groups: bootstrappingstyle and graph-based. As for bootstrapping-style approaches, Wan (2009) considers two different languages as two views and applies co-training to conduct semi-supervised sentiment classification. Similarly, Li et al. (2010) propose two views, named personal and impersonal views, and apply co-training to use unlabeled data in a monolingual corpus. More recently, Gao et al. (2014) propose a feature subspace-based self-training"
P15-2005,P09-1027,0,0.242552,"analysis is sentiment classification, which aims to determine the sentimental orientation a piece of text expresses (Pang et al., 2002). For instance, the sentence &quot;I absolutely love this product.&quot; is supposed to be determined as a positive expression in sentimental orientation.  While early studies focus on supervised learning, where only labeled data are required to train the classification model (Pang et al., 2002), recent studies devote more and more to reduce the heavy dependence on the large amount of labeled data by exploiting semi-supervised learning approaches, such as co-training (Wan, 2009; Li et al., 2011), label propagation (Sindhwani and Melville, 2008), and deep learning (Zhou et al., 2013), to sentiment classification. Empirical evaluation on various domains demonstrates the effectiveness of the unlabeled data in enhancing the performance  * Corresponding author 27 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 27–31, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics seen as an extension of the famous sta"
P15-2005,P09-1079,0,0.0173928,"ata as L  {( xi , yi )}inL1 where xi R d is the d dimensional input vector, and yi is its output label. The unlabeled data in the target domain is denoted as U  {( xk )}nkU1 . Suppose l semi is a semi-supervised Related Work Early studies on sentiment classification mainly focus on supervised learning methods with algorithm designing and feature engineering (Pang et al., 2002; Cui et al., 2006; Riloff et al., 2006; Li et al., 2009). Recently, most studies on sentiment classification aim to improve the performance by exploiting unlabeled data in two main aspects: semi-supervised learning (Dasgupta and Ng, 2009; Wan, 2009; Li et al., 2010) and cross-domain learning (Blitzer et al. 2007; He et al. 2011; Li et al., 2013). Specifically, existing approaches to semi-supervised sentiment classification could be categorized into two main groups: bootstrappingstyle and graph-based. As for bootstrapping-style approaches, Wan (2009) considers two different languages as two views and applies co-training to conduct semi-supervised sentiment classification. Similarly, Li et al. (2010) propose two views, named personal and impersonal views, and apply co-training to use unlabeled data in a monolingual corpus. More"
P15-2005,P11-1013,0,0.351351,"label. The unlabeled data in the target domain is denoted as U  {( xk )}nkU1 . Suppose l semi is a semi-supervised Related Work Early studies on sentiment classification mainly focus on supervised learning methods with algorithm designing and feature engineering (Pang et al., 2002; Cui et al., 2006; Riloff et al., 2006; Li et al., 2009). Recently, most studies on sentiment classification aim to improve the performance by exploiting unlabeled data in two main aspects: semi-supervised learning (Dasgupta and Ng, 2009; Wan, 2009; Li et al., 2010) and cross-domain learning (Blitzer et al. 2007; He et al. 2011; Li et al., 2013). Specifically, existing approaches to semi-supervised sentiment classification could be categorized into two main groups: bootstrappingstyle and graph-based. As for bootstrapping-style approaches, Wan (2009) considers two different languages as two views and applies co-training to conduct semi-supervised sentiment classification. Similarly, Li et al. (2010) propose two views, named personal and impersonal views, and apply co-training to use unlabeled data in a monolingual corpus. More recently, Gao et al. (2014) propose a feature subspace-based self-training to semi-supervis"
P15-2005,P10-1043,1,0.861229,"e xi R d is the d dimensional input vector, and yi is its output label. The unlabeled data in the target domain is denoted as U  {( xk )}nkU1 . Suppose l semi is a semi-supervised Related Work Early studies on sentiment classification mainly focus on supervised learning methods with algorithm designing and feature engineering (Pang et al., 2002; Cui et al., 2006; Riloff et al., 2006; Li et al., 2009). Recently, most studies on sentiment classification aim to improve the performance by exploiting unlabeled data in two main aspects: semi-supervised learning (Dasgupta and Ng, 2009; Wan, 2009; Li et al., 2010) and cross-domain learning (Blitzer et al. 2007; He et al. 2011; Li et al., 2013). Specifically, existing approaches to semi-supervised sentiment classification could be categorized into two main groups: bootstrappingstyle and graph-based. As for bootstrapping-style approaches, Wan (2009) considers two different languages as two views and applies co-training to conduct semi-supervised sentiment classification. Similarly, Li et al. (2010) propose two views, named personal and impersonal views, and apply co-training to use unlabeled data in a monolingual corpus. More recently, Gao et al. (2014)"
P15-2005,P09-1078,1,0.73468,"hm aims to learn a classifier from a small scale of labeled samples, named initial labeled data, with a large number of unlabeled samples. In the sequel, we refer the labeled data as L  {( xi , yi )}inL1 where xi R d is the d dimensional input vector, and yi is its output label. The unlabeled data in the target domain is denoted as U  {( xk )}nkU1 . Suppose l semi is a semi-supervised Related Work Early studies on sentiment classification mainly focus on supervised learning methods with algorithm designing and feature engineering (Pang et al., 2002; Cui et al., 2006; Riloff et al., 2006; Li et al., 2009). Recently, most studies on sentiment classification aim to improve the performance by exploiting unlabeled data in two main aspects: semi-supervised learning (Dasgupta and Ng, 2009; Wan, 2009; Li et al., 2010) and cross-domain learning (Blitzer et al. 2007; He et al. 2011; Li et al., 2013). Specifically, existing approaches to semi-supervised sentiment classification could be categorized into two main groups: bootstrappingstyle and graph-based. As for bootstrapping-style approaches, Wan (2009) considers two different languages as two views and applies co-training to conduct semi-supervised se"
P15-2125,P13-2037,0,0.0572364,"n social media has become of great value to market predictions and analysis (Liu et al., 2013; Lee et al., 2014). Previous researches on emotion analysis have mainly focused on emotion expressions in monolingual texts (Chen et al., 2010; Lee et al., 2013a). However, in informal settings such as micro-blogs, emotions are often expressed by a mixture of different natural languages. Such a mixture of language is called codeswitching. Specifically, code-switching text is defined as text that contains more than one language (code). It is a common phenomenon in multilingual communities (Auer, 1999; Adel et al., 2013). For instance, [E1-E3] are three examples of codeswitching emotional posts containing both Chi*Corresponding author 763 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 763–768, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics dentify the code-switching posts. After removing posts containing noise and advertisements, we extract 4,195 code-switching posts from the dataset for emotion annotation. Five basic emotions are annotate"
P15-2125,J90-2002,0,0.86105,"arch. We systematically explore both the bilingual and sentimental information to detect emotions in code-switching posts. Moreover, we use a term-document bipartite graph to incorporate these two kinds of information, and propose a Label Propagation (LP) based approach to learn and predict emotion in code-switching texts. In the following subsections, we will discuss these issues one by one. 4.1 The candidate target sentences made up of a sequence of the optional target words are ranked by the language model. The output will be generated only if it reaches the maximum probability as follows (Brown et al., 1990; Zhao et al., 2009): c = argmax 4.2 For using bilingual information, a word-by-word statistical machine translation strategy is adopted to translate words from English into Chinese. For better clarity, a word-based decoding, which adopts a log-linear framework as in (Och and Ney, 2002) with translation model and language model being the only features, is used: P (1) h1 (c, e) = log(pγ (c|e)) (2) p(wc ) (4) Sentimental Information Sentimental information is very useful in emotion detection (Gao et al., 2013). In this paper, we extract polarity from both Chinese and English texts to ensure text"
P15-2125,C10-1021,1,0.941793,"ard approach to handle this issue is to translate texts from one language into another. Since Chinese is the dominant language in our data set, a word-by-word statistical machine translation strategy (Zhao et al., 2009) is adopted to translate English words into Chinese. Additionally, as text from micro-blogs is informal, With the rapid development of Web 2.0, emotion analysis in social media has become of great value to market predictions and analysis (Liu et al., 2013; Lee et al., 2014). Previous researches on emotion analysis have mainly focused on emotion expressions in monolingual texts (Chen et al., 2010; Lee et al., 2013a). However, in informal settings such as micro-blogs, emotions are often expressed by a mixture of different natural languages. Such a mixture of language is called codeswitching. Specifically, code-switching text is defined as text that contains more than one language (code). It is a common phenomenon in multilingual communities (Auer, 1999; Adel et al., 2013). For instance, [E1-E3] are three examples of codeswitching emotional posts containing both Chi*Corresponding author 763 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7"
P15-2125,lee-etal-2014-annotating,1,0.859796,"bridging the gap between different languages becomes essential for emotion detection in code-switching texts. A straightforward approach to handle this issue is to translate texts from one language into another. Since Chinese is the dominant language in our data set, a word-by-word statistical machine translation strategy (Zhao et al., 2009) is adopted to translate English words into Chinese. Additionally, as text from micro-blogs is informal, With the rapid development of Web 2.0, emotion analysis in social media has become of great value to market predictions and analysis (Liu et al., 2013; Lee et al., 2014). Previous researches on emotion analysis have mainly focused on emotion expressions in monolingual texts (Chen et al., 2010; Lee et al., 2013a). However, in informal settings such as micro-blogs, emotions are often expressed by a mixture of different natural languages. Such a mixture of language is called codeswitching. Specifically, code-switching text is defined as text that contains more than one language (code). It is a common phenomenon in multilingual communities (Auer, 1999; Adel et al., 2013). For instance, [E1-E3] are three examples of codeswitching emotional posts containing both Ch"
P15-2125,C12-1102,0,0.0621802,"; Volkova et al., 2012; Lee et al., 2014). Moreover, emotion classification is one of the most important tasks in emotion analysis, while emotion classification aims to classify text into multiple emotion categories (Chen et al., 2010; Liu et al., 2013). Despite a growing body of research on emotion analysis, little has been done on the analysis of emotion in code-switching due to the complexities of processing two languages at the same time. Besides, although several research studies have focused on analyzing bilingual (Wan, 2009; Lu et al., 2011; Tang et al., 2014) and code-switching texts (Li and Fung, 2012; Ling et al., 2013; Lignos and Marcus, 2013), none of them has studied the multilingual code-switching issues in emotion detection. This research area is especially crucial when public emotions are mostly expressed in the free-form text on the Internet. 3 Figure 1: Distribution of Emotions and Languages The joint distribution between emotions and caused languages is illustrated in Figure 1. The Y-axis of the figure presents the conditional probability of a post expressing the emotion ei given that lj is the caused language, p(ei |lj ). It is suggested in Figure 2 that: 1) happiness occurs mor"
P15-2125,P13-1018,0,0.0517095,"012; Lee et al., 2014). Moreover, emotion classification is one of the most important tasks in emotion analysis, while emotion classification aims to classify text into multiple emotion categories (Chen et al., 2010; Liu et al., 2013). Despite a growing body of research on emotion analysis, little has been done on the analysis of emotion in code-switching due to the complexities of processing two languages at the same time. Besides, although several research studies have focused on analyzing bilingual (Wan, 2009; Lu et al., 2011; Tang et al., 2014) and code-switching texts (Li and Fung, 2012; Ling et al., 2013; Lignos and Marcus, 2013), none of them has studied the multilingual code-switching issues in emotion detection. This research area is especially crucial when public emotions are mostly expressed in the free-form text on the Internet. 3 Figure 1: Distribution of Emotions and Languages The joint distribution between emotions and caused languages is illustrated in Figure 1. The Y-axis of the figure presents the conditional probability of a post expressing the emotion ei given that lj is the caused language, p(ei |lj ). It is suggested in Figure 2 that: 1) happiness occurs more frequently than o"
P15-2125,P13-2091,1,0.940989,"selves.) [E2] • ˜é{Ò´/Oœv kk5 §Ø O â´1n ö0""shit! (A quote, to my great disgust, is ”There’s no staking claims in a relationship based on who got there first - the one who isn.t loved is the true third party.” Shit!) [E3] ù o ‡ y - : { "" "" "" ¶ ‚holdØ Ø4 Bœœœ (The so-called ”highlighting”...we can’t hold it anymore.) Introduction It is more difficult to detect emotions in codeswitching texts than in monolingual ones since emotions in code-switching posts can be expressed through one or two languages. Hence, traditional automatic emotion detection methods which simply consider monolingual texts (Liu et al., 2013; Lee et al., 2013a) would not be readily applicable. The key issue of emotion detection in codeswitching texts is to deal with the emotions expressed through different languages. Thus bridging the gap between different languages becomes essential for emotion detection in code-switching texts. A straightforward approach to handle this issue is to translate texts from one language into another. Since Chinese is the dominant language in our data set, a word-by-word statistical machine translation strategy (Zhao et al., 2009) is adopted to translate English words into Chinese. Additionally, as te"
P15-2125,P11-1033,0,0.0210093,"s task is about emotion resource construction (Xu et al., 2010; Volkova et al., 2012; Lee et al., 2014). Moreover, emotion classification is one of the most important tasks in emotion analysis, while emotion classification aims to classify text into multiple emotion categories (Chen et al., 2010; Liu et al., 2013). Despite a growing body of research on emotion analysis, little has been done on the analysis of emotion in code-switching due to the complexities of processing two languages at the same time. Besides, although several research studies have focused on analyzing bilingual (Wan, 2009; Lu et al., 2011; Tang et al., 2014) and code-switching texts (Li and Fung, 2012; Ling et al., 2013; Lignos and Marcus, 2013), none of them has studied the multilingual code-switching issues in emotion detection. This research area is especially crucial when public emotions are mostly expressed in the free-form text on the Internet. 3 Figure 1: Distribution of Emotions and Languages The joint distribution between emotions and caused languages is illustrated in Figure 1. The Y-axis of the figure presents the conditional probability of a post expressing the emotion ei given that lj is the caused language, p(ei"
P15-2125,P02-1038,0,0.116703,"dict emotion in code-switching texts. In the following subsections, we will discuss these issues one by one. 4.1 The candidate target sentences made up of a sequence of the optional target words are ranked by the language model. The output will be generated only if it reaches the maximum probability as follows (Brown et al., 1990; Zhao et al., 2009): c = argmax 4.2 For using bilingual information, a word-by-word statistical machine translation strategy is adopted to translate words from English into Chinese. For better clarity, a word-based decoding, which adopts a log-linear framework as in (Och and Ney, 2002) with translation model and language model being the only features, is used: P (1) h1 (c, e) = log(pγ (c|e)) (2) p(wc ) (4) Sentimental Information Sentimental information is very useful in emotion detection (Gao et al., 2013). In this paper, we extract polarity from both Chinese and English texts to ensure text of similar polarity will be connected. In this paper, both Chinese5 and English6 sentimental lexicons are employed to identify candidate opinion expressions by searching the occurrences of negative and positive expressions in text, and predict the polarity of both Chinese and English t"
P15-2125,D09-1150,0,0.258839,"Missing"
P15-2125,P02-1053,0,0.0201887,"l being the only features, is used: P (1) h1 (c, e) = log(pγ (c|e)) (2) p(wc ) (4) Sentimental Information Sentimental information is very useful in emotion detection (Gao et al., 2013). In this paper, we extract polarity from both Chinese and English texts to ensure text of similar polarity will be connected. In this paper, both Chinese5 and English6 sentimental lexicons are employed to identify candidate opinion expressions by searching the occurrences of negative and positive expressions in text, and predict the polarity of both Chinese and English texts through the word-counting approach (Turney, 2002). Bilingual Information exp [ 2i=1 λi hi (c, e)] P (c|e) = P P2 c exp [ i=1 λi hi (c, e)] Y 4.3 where LP-based Emotion Detection For the knowledge of bilingual and sentimental information to be well incorporated, we use a termdocument bipartite graph to incorporate the information, and propose a label propagation based approach to learn and predict emotion in codeswitching texts. The input of the LP algorithm is a graph describing the relationship between each sample pair in the labeled and test data (Sindhwani and Melville, 2008; Li et al., 2013). In a bipartite graph, the nodes consist of tw"
P15-2125,E12-1031,0,0.0275893,"pose a label propagation (Zhu and Ghahramani, 2002) based approach to learn and predict in the graph. Specially, the label information between Chinese and English texts would be propagated through the bipartite graph by word-document relations, bilingual information, and sentiment information. Evaluation of the data set indicates the importance of the task and the effectiveness of our proposed approach. 2 Related Work Emotion analysis has been a hot research topic in NLP in the last decade. One main group of related studies on this task is about emotion resource construction (Xu et al., 2010; Volkova et al., 2012; Lee et al., 2014). Moreover, emotion classification is one of the most important tasks in emotion analysis, while emotion classification aims to classify text into multiple emotion categories (Chen et al., 2010; Liu et al., 2013). Despite a growing body of research on emotion analysis, little has been done on the analysis of emotion in code-switching due to the complexities of processing two languages at the same time. Besides, although several research studies have focused on analyzing bilingual (Wan, 2009; Lu et al., 2011; Tang et al., 2014) and code-switching texts (Li and Fung, 2012; Lin"
P15-2125,C10-1136,0,0.115476,"formation and propose a label propagation (Zhu and Ghahramani, 2002) based approach to learn and predict in the graph. Specially, the label information between Chinese and English texts would be propagated through the bipartite graph by word-document relations, bilingual information, and sentiment information. Evaluation of the data set indicates the importance of the task and the effectiveness of our proposed approach. 2 Related Work Emotion analysis has been a hot research topic in NLP in the last decade. One main group of related studies on this task is about emotion resource construction (Xu et al., 2010; Volkova et al., 2012; Lee et al., 2014). Moreover, emotion classification is one of the most important tasks in emotion analysis, while emotion classification aims to classify text into multiple emotion categories (Chen et al., 2010; Liu et al., 2013). Despite a growing body of research on emotion analysis, little has been done on the analysis of emotion in code-switching due to the complexities of processing two languages at the same time. Besides, although several research studies have focused on analyzing bilingual (Wan, 2009; Lu et al., 2011; Tang et al., 2014) and code-switching texts ("
P15-2125,P09-1027,0,0.0798541,"dies on this task is about emotion resource construction (Xu et al., 2010; Volkova et al., 2012; Lee et al., 2014). Moreover, emotion classification is one of the most important tasks in emotion analysis, while emotion classification aims to classify text into multiple emotion categories (Chen et al., 2010; Liu et al., 2013). Despite a growing body of research on emotion analysis, little has been done on the analysis of emotion in code-switching due to the complexities of processing two languages at the same time. Besides, although several research studies have focused on analyzing bilingual (Wan, 2009; Lu et al., 2011; Tang et al., 2014) and code-switching texts (Li and Fung, 2012; Ling et al., 2013; Lignos and Marcus, 2013), none of them has studied the multilingual code-switching issues in emotion detection. This research area is especially crucial when public emotions are mostly expressed in the free-form text on the Internet. 3 Figure 1: Distribution of Emotions and Languages The joint distribution between emotions and caused languages is illustrated in Figure 1. The Y-axis of the figure presents the conditional probability of a post expressing the emotion ei given that lj is the cause"
P15-2125,P09-1007,1,0.915067,"omatic emotion detection methods which simply consider monolingual texts (Liu et al., 2013; Lee et al., 2013a) would not be readily applicable. The key issue of emotion detection in codeswitching texts is to deal with the emotions expressed through different languages. Thus bridging the gap between different languages becomes essential for emotion detection in code-switching texts. A straightforward approach to handle this issue is to translate texts from one language into another. Since Chinese is the dominant language in our data set, a word-by-word statistical machine translation strategy (Zhao et al., 2009) is adopted to translate English words into Chinese. Additionally, as text from micro-blogs is informal, With the rapid development of Web 2.0, emotion analysis in social media has become of great value to market predictions and analysis (Liu et al., 2013; Lee et al., 2014). Previous researches on emotion analysis have mainly focused on emotion expressions in monolingual texts (Chen et al., 2010; Lee et al., 2013a). However, in informal settings such as micro-blogs, emotions are often expressed by a mixture of different natural languages. Such a mixture of language is called codeswitching. Spe"
P15-2125,D09-1102,1,0.884445,"Missing"
P19-1045,D18-1024,0,0.017936,"gan to apply adversarial learning to various NLP tasks. Zhang et al. (2016) and Zhao et al. (2017) constructed adversarial networks with CNNs and LSTMs to train text generation models. Wu et al. (2017) proposed two types of adversarial models which consist of CNNs and RNNs, respectively. They discussed the advantages and disadvantages of two implementations on two relation extraction datasets. Masumura et al. (2018) proposed an adversarial training approach for multi-task multilingual learning, which jointly conducts task discrimination among languages and language discrimination among tasks. Chen and Cardie (2018) applied adversarial learning to multilingual word representation learning which maps word embedValence Score (SV) Discriminating Score (P) Arousal Score (SA) Regressor for Valence (RV) Discriminator (D) Regressor for Arousal (RA) Feature Extractor A (Ext) Attention for Valence (Att V) Shared Attention (Att S) Attention for Arousal (Att A) Input (X) Figure 2: The framework of the Valence-Arousal Adversarial Attention Network which conducts adversarial learning between a pair of emotion dimensions. The frameworks of Valence-Dominance AAN and ArousalDominance AAN can be inferred in the same mann"
P19-1045,E17-2092,0,0.0970912,"ion. The dimensional emotion score ranges from 1.0 to 5.0. In this example, the word very in blue only suggests one emotion dimension (i.e, a high Arousal score). The word scared and disaster in red suggest two emotion dimensions. Specifically, scared suggests a low Valence score and a low Dominance score, while Disaster denotes a low Valence score and a high Arousal score. suitable for fine-grained emotion analysis and has gained an increasing attention recently due to the availability of several emotion regression corpora in the last few years (Preotiuc-Pietro et al., 2016; Yu et al., 2016; Hahn and Buechel, 2017). In principle, these emotion regression corpora apply the widely-admitted Valence-Arousal model or Valence-Arousal-Dominance model (Barrett, 2006) to describe emotions with a continuous real number space in two or three dimensions. Moreover, while different emotion classification corpora often apply different classification systems, they describe emotions with a limited number of discrete pre-defined emotion categories. In the literature, most of the existing studies in emotion regression focus on a single emotion dimension by training multiple independent models for different emotion dimensi"
P19-1045,P17-1001,0,0.0453615,"Missing"
P19-1045,P17-1067,0,0.0923803,"Missing"
P19-1045,D18-1064,0,0.111373,"scores can be inferred from the colored words in this figure. Although the degree adverb, such as very, only suggests a high Arousal score, an emotional word often suggests more than one dimensional emotion score. This hints a possibility that the relationship between two emotion dimensions can be leveraged, which is overlooked by existing singledimensional emotion regression studies. In this paper, we try to model the multidimensional learning task as a multi-task learning task through adversarial learning. Recently, studies in multi-task learning via adversarial learning (Liu et al., 2017; Masumura et al., 2018), which tried to conduct adversarial learning (Goodfellow et al., 2014) between multiple tasks to learn taskspecific features for achieving better performance for each task, has achieved a great success. We apply adversarial learning to model the task not only due to its capability of multi-task learning, but also due to its inherent collocability with attention mechanism. In the literature, adversarial learning has the difficulty in learning latent representations from discrete structures (e.g., sequence of word embeddings). Thus, most of existing studies in NLP apply adversarial learning wit"
P19-1045,P15-2030,0,0.0310476,"-CNN in all cases. Furthermore, AAN outperforms its two counterparts (i.e., Attention Network and Joint Learning), justifying the effectiveness of the proposed adversarial learning approach. However, the overall r-values on EMOBANK are relatively low. This indicates the inherent difficulty of emotion regression on EMOBANK. As a reference, the average oracle rvalue between human annotators of EMOBANK is about 0.6 (Hahn and Buechel, 2017). 4.2 Baselines In this study, the following baselines for emotion regression are implemented for fair comparison: • Deep CNN: A CNN-based approach proposed by Bitvai and Cohn (2015). This approach applies multiple parallel CNNs to extract multiple n-gram features in a text, and 476 Domain News Domain Fictions Domain Blogs Domain Essays Domain Letters Domain Travel Guides Domain Approach Deep CNN Regional CNN-LSTM Context LSTM-CNN Attention Network Joint Learning AAN Deep CNN Regional CNN-LSTM Context LSTM-CNN Attention Network Joint Learning AAN Deep CNN Regional CNN-LSTM Context LSTM-CNN Attention Network Joint Learning AAN Deep CNN Regional CNN-LSTM Context LSTM-CNN Attention Network Joint Learning AAN Deep CNN Regional CNN-LSTM Context LSTM-CNN Attention Network Joint"
P19-1045,E17-2093,0,0.428358,"ores. Specifically, our proposed AAN has two features: • Second, unlike existing single-dimensional emotion regression studies which separately train models for different emotion dimensions, AAN can leverage shared information between emotion dimensions (e.g., word scare contributes to both Valence and Dominance in the example shown in Figure 1) to better rate different emotion dimension scores, and thus achieve better regression results. We apply AAN to the task of multi-dimensional emotion regression on a large-scale emotion regression corpus, namely EMOBANK, contributed by Hahn and Buechel (2017). Empirical evaluation on EMOBANK Reader’s and Writer’s multidimensional emotion regression tasks shows that AAN achieves significant improvements in rvalues over several strong baselines. Furthermore, it also shows that adversarial training between two attention layers is more effective than simply applying attention mechanism individually to each emotion dimension, or simply training two regressors jointly for a pair of emotion dimensions. 2 Related Work 2.1 Emotion Regression Compared with emotion classification, emotion regression had a late start due to the severe lack of large-scale anno"
P19-1045,W16-0404,0,0.253732,"Missing"
P19-1045,D18-1107,0,0.0487028,"Missing"
P19-1045,P16-2037,0,0.451261,"emotion regression corpora apply the widely-admitted Valence-Arousal model or Valence-Arousal-Dominance model (Barrett, 2006) to describe emotions with a continuous real number space in two or three dimensions. Moreover, while different emotion classification corpora often apply different classification systems, they describe emotions with a limited number of discrete pre-defined emotion categories. In the literature, most of the existing studies in emotion regression focus on a single emotion dimension by training multiple independent models for different emotion dimensions (Yu et al., 2015; Wang et al., 2016a). Hence in this paper, we seek to solve multi-dimensional emotion regression via a joint approach. Recently, attention mechanism Introduction Emotion analysis aims to recognize human emotion expression in a given text (Mishne et al., 2005; Abdul-Mageed and Ungar, 2017). Typically, studies in emotion analysis can be divided into either emotion classification (Yang et al., 2007; Tripathi et al., 2017) or emotion regression (Yu et al., 2015; Wang et al., 2016a). While emotion classification aims to label an input text with a single or multiple emotion categories, emotion regression aims to rate"
P19-1045,D16-1058,0,0.537606,"emotion regression corpora apply the widely-admitted Valence-Arousal model or Valence-Arousal-Dominance model (Barrett, 2006) to describe emotions with a continuous real number space in two or three dimensions. Moreover, while different emotion classification corpora often apply different classification systems, they describe emotions with a limited number of discrete pre-defined emotion categories. In the literature, most of the existing studies in emotion regression focus on a single emotion dimension by training multiple independent models for different emotion dimensions (Yu et al., 2015; Wang et al., 2016a). Hence in this paper, we seek to solve multi-dimensional emotion regression via a joint approach. Recently, attention mechanism Introduction Emotion analysis aims to recognize human emotion expression in a given text (Mishne et al., 2005; Abdul-Mageed and Ungar, 2017). Typically, studies in emotion analysis can be divided into either emotion classification (Yang et al., 2007; Tripathi et al., 2017) or emotion regression (Yu et al., 2015; Wang et al., 2016a). While emotion classification aims to label an input text with a single or multiple emotion categories, emotion regression aims to rate"
P19-1045,D17-1187,0,0.435229,"ores. Specifically, our proposed AAN has two features: • Second, unlike existing single-dimensional emotion regression studies which separately train models for different emotion dimensions, AAN can leverage shared information between emotion dimensions (e.g., word scare contributes to both Valence and Dominance in the example shown in Figure 1) to better rate different emotion dimension scores, and thus achieve better regression results. We apply AAN to the task of multi-dimensional emotion regression on a large-scale emotion regression corpus, namely EMOBANK, contributed by Hahn and Buechel (2017). Empirical evaluation on EMOBANK Reader’s and Writer’s multidimensional emotion regression tasks shows that AAN achieves significant improvements in rvalues over several strong baselines. Furthermore, it also shows that adversarial training between two attention layers is more effective than simply applying attention mechanism individually to each emotion dimension, or simply training two regressors jointly for a pair of emotion dimensions. 2 Related Work 2.1 Emotion Regression Compared with emotion classification, emotion regression had a late start due to the severe lack of large-scale anno"
P19-1045,N16-1066,0,0.44401,"l emotion regression. The dimensional emotion score ranges from 1.0 to 5.0. In this example, the word very in blue only suggests one emotion dimension (i.e, a high Arousal score). The word scared and disaster in red suggest two emotion dimensions. Specifically, scared suggests a low Valence score and a low Dominance score, while Disaster denotes a low Valence score and a high Arousal score. suitable for fine-grained emotion analysis and has gained an increasing attention recently due to the availability of several emotion regression corpora in the last few years (Preotiuc-Pietro et al., 2016; Yu et al., 2016; Hahn and Buechel, 2017). In principle, these emotion regression corpora apply the widely-admitted Valence-Arousal model or Valence-Arousal-Dominance model (Barrett, 2006) to describe emotions with a continuous real number space in two or three dimensions. Moreover, while different emotion classification corpora often apply different classification systems, they describe emotions with a limited number of discrete pre-defined emotion categories. In the literature, most of the existing studies in emotion regression focus on a single emotion dimension by training multiple independent models for"
P19-1045,P15-2129,0,0.431875,"principle, these emotion regression corpora apply the widely-admitted Valence-Arousal model or Valence-Arousal-Dominance model (Barrett, 2006) to describe emotions with a continuous real number space in two or three dimensions. Moreover, while different emotion classification corpora often apply different classification systems, they describe emotions with a limited number of discrete pre-defined emotion categories. In the literature, most of the existing studies in emotion regression focus on a single emotion dimension by training multiple independent models for different emotion dimensions (Yu et al., 2015; Wang et al., 2016a). Hence in this paper, we seek to solve multi-dimensional emotion regression via a joint approach. Recently, attention mechanism Introduction Emotion analysis aims to recognize human emotion expression in a given text (Mishne et al., 2005; Abdul-Mageed and Ungar, 2017). Typically, studies in emotion analysis can be divided into either emotion classification (Yang et al., 2007; Tripathi et al., 2017) or emotion regression (Yu et al., 2015; Wang et al., 2016a). While emotion classification aims to label an input text with a single or multiple emotion categories, emotion regr"
P19-1345,D17-1047,0,0.161136,"e-ofthe-art approaches to ASC as baselines. Since the input of all these approaches should be a single sequence, we concatenate question and answer text to generate a single sequence. Besides, we employ some QA matching approaches to ASC-QA and implement several basic versions of RBAN as baselines. Note that, for fair comparison, all the above baselines adopt the same pre-trained word embeddings as RBAN. The baselines are listed as follows in detail: 1) LSTM (Wang et al., 2016). This approach only adopts a standard LSTM network to model the text without considering aspect information. 2) RAM (Chen et al., 2017). This is a state-of-theart deep memory network approach to ASC. 3) GCAE (Xue and Li, 2018). This is a state-ofthe-art approach to ASC which combines CNN and gating mechanisms to learn text representation. 4) S-LSTM (Wang and Lu, 2018). This is a state-of-the-art approach to ASC which considers structural dependencies between targets and opinion terms. 5) BIDAF (Seo et al., 2016). This is a QA matching approach to reading comprehension. We substitute its decoding layer with softmax decoder to perform ASC-QA. 6) HMN (Shen et al., 2018a). This is a QA matching approach to coarse-grained sentimen"
P19-1345,P14-2009,0,0.0298397,"QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Customer Question-Answering (QA)”, has become increasingly popular and a large-scale of such QA style reviews (as shown in Figure 1) could be found in several famous e-commerce platforms (e.g., Amazon and Taobao). Compared to traditional non-interactive cust"
P19-1345,P11-1016,0,0.0880512,"ent Classiﬁcation Towards QA - Input: QA text pair with given aspects - Output: [battery life]: Positive [operating speed]: Negative Figure 1: An example for illustrating the proposed task of Aspect Sentiment Classiﬁcation towards QuestionAnswering (ASC-QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Cust"
P19-1345,D16-1011,0,0.0706496,"Missing"
P19-1345,C18-1079,0,0.0249668,"Missing"
P19-1345,D13-1171,0,0.358181,"Missing"
P19-1345,S15-2082,0,0.190919,"Missing"
P19-1345,S14-2004,0,0.52776,"oposed task of Aspect Sentiment Classiﬁcation towards QuestionAnswering (ASC-QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Customer Question-Answering (QA)”, has become increasingly popular and a large-scale of such QA style reviews (as shown in Figure 1) could be found in several famous e-commerce platfor"
P19-1345,P13-4009,0,0.0428117,"Missing"
P19-1345,D18-1401,1,0.853282,"Missing"
P19-1345,D16-1021,0,0.411895,"wards QA - Input: QA text pair with given aspects - Output: [battery life]: Positive [operating speed]: Negative Figure 1: An example for illustrating the proposed task of Aspect Sentiment Classiﬁcation towards QuestionAnswering (ASC-QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Customer Question-Answe"
P19-1345,P08-1036,0,0.0510394,"entence-level text classiﬁcation which aims to incorporate aspect information into a model. Recently, Wang et al. (2016); Ma et al. (2017) propose an attention based LSTM to ASC by exploring the connection between an aspect and the content of a sentence. Tang et al. (2016b), Chen et al. (2017) and Wang et al. (2018b) employ memory networks to model the context and aspect. Wang and Lu (2018) propose a segmentation attention to capture structural dependency between target and opinion terms. Document-level ASC aims to predict sentiment ratings for aspects inside a long text. Traditional studies (Titov and McDonald, 2008; Wang et al., 2010; Pontiki et al., 2016) solve document-level ASC as a sub-problem by utilizing heuristic based methods or topic models. Recently, Lei et al. (2016) focus on extracting rationales for aspects in a document. Li et al. (2018) propose an useraware attention approach to document-level ASC. Yin et al. (2017) model document-level ASC as a machine comprehension problem, of which the input is also a parallel unit, i.e., question and answer. However, their question texts are pseudo and artiﬁcially constructed. This disaccords with the fact that real-world question texts also possibly"
P19-1345,P18-1088,0,0.34373,"text pair with given aspects - Output: [battery life]: Positive [operating speed]: Negative Figure 1: An example for illustrating the proposed task of Aspect Sentiment Classiﬁcation towards QuestionAnswering (ASC-QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Customer Question-Answering (QA)”, has beco"
P19-1345,D16-1058,0,0.427641,"determine the polarity towards each aspect category discussed in a QA text pair. 4.2 Baselines For comparison, we implement several state-ofthe-art approaches to ASC as baselines. Since the input of all these approaches should be a single sequence, we concatenate question and answer text to generate a single sequence. Besides, we employ some QA matching approaches to ASC-QA and implement several basic versions of RBAN as baselines. Note that, for fair comparison, all the above baselines adopt the same pre-trained word embeddings as RBAN. The baselines are listed as follows in detail: 1) LSTM (Wang et al., 2016). This approach only adopts a standard LSTM network to model the text without considering aspect information. 2) RAM (Chen et al., 2017). This is a state-of-theart deep memory network approach to ASC. 3) GCAE (Xue and Li, 2018). This is a state-ofthe-art approach to ASC which combines CNN and gating mechanisms to learn text representation. 4) S-LSTM (Wang and Lu, 2018). This is a state-of-the-art approach to ASC which considers structural dependencies between targets and opinion terms. 5) BIDAF (Seo et al., 2016). This is a QA matching approach to reading comprehension. We substitute its decod"
P19-1345,P18-1234,0,0.0124307,"e a single sequence, we concatenate question and answer text to generate a single sequence. Besides, we employ some QA matching approaches to ASC-QA and implement several basic versions of RBAN as baselines. Note that, for fair comparison, all the above baselines adopt the same pre-trained word embeddings as RBAN. The baselines are listed as follows in detail: 1) LSTM (Wang et al., 2016). This approach only adopts a standard LSTM network to model the text without considering aspect information. 2) RAM (Chen et al., 2017). This is a state-of-theart deep memory network approach to ASC. 3) GCAE (Xue and Li, 2018). This is a state-ofthe-art approach to ASC which combines CNN and gating mechanisms to learn text representation. 4) S-LSTM (Wang and Lu, 2018). This is a state-of-the-art approach to ASC which considers structural dependencies between targets and opinion terms. 5) BIDAF (Seo et al., 2016). This is a QA matching approach to reading comprehension. We substitute its decoding layer with softmax decoder to perform ASC-QA. 6) HMN (Shen et al., 2018a). This is a QA matching approach to coarse-grained sentiment classiﬁcation towards QA style reviews. 7) MAMC (Yin et al., 2017). This is a QA matching"
P19-1345,D17-1217,0,0.0355952,"Missing"
W12-6309,C12-2067,1,0.824288,"t-processing rules to improve the performance. As a result, our system obtains a competitive F-score in comparison with other participating systems. 1 Our System 2.1 Overview Input Pre-processing Replace digits, Chinese numbers, punctuations, English characters. CRF-based segmentation Introduction Because Chinese context is written without natural delimiters, word segmentation becomes an essential initial step in many tasks on Chinese language processing. Though recognizing words seems easy for human beings, automatic Chinese Word Segmentation by computers is not a trivial problem (Xue, 2003; Li et al., 2012). The state-of-the-art Chinese Word Segmentation systems have achieved a quite high precision on traditional media text. However, the performance of segmentation is not so satisfying for MicroBlog corpora. MicroBlog messages are often short, and they make heavy use of colloquial language. Furthermore, they require situational context for interpretation. Thus, we first analyze and annotate some MicroBlog messages, and then propose a novel pre-processing and postprocessing approach on the CRF-based segmentation system for the MicroBlog corpora. The experimental results show that our system perfo"
W12-6309,Y09-2034,1,0.893056,"Missing"
W12-6309,O03-4002,0,0.0539183,"ing and post-processing rules to improve the performance. As a result, our system obtains a competitive F-score in comparison with other participating systems. 1 Our System 2.1 Overview Input Pre-processing Replace digits, Chinese numbers, punctuations, English characters. CRF-based segmentation Introduction Because Chinese context is written without natural delimiters, word segmentation becomes an essential initial step in many tasks on Chinese language processing. Though recognizing words seems easy for human beings, automatic Chinese Word Segmentation by computers is not a trivial problem (Xue, 2003; Li et al., 2012). The state-of-the-art Chinese Word Segmentation systems have achieved a quite high precision on traditional media text. However, the performance of segmentation is not so satisfying for MicroBlog corpora. MicroBlog messages are often short, and they make heavy use of colloquial language. Furthermore, they require situational context for interpretation. Thus, we first analyze and annotate some MicroBlog messages, and then propose a novel pre-processing and postprocessing approach on the CRF-based segmentation system for the MicroBlog corpora. The experimental results show tha"
W12-6309,W03-1728,0,0.0467913,"Missing"
W12-6309,I08-4017,0,0.0374016,"Missing"
W12-6309,Y06-1012,0,\N,Missing
Y09-1033,P07-1056,0,0.274462,"opose the classification algorithm to do the classification on the text with two-bags-of-words modeling. The remainder of this paper is organized as follows. Section 2 introduces the related work on CVS applications in sentiment classification. Section 3 presents our approach in detail. Experimental results are presented and analyzed in Section 4. Finally, Section 5 draws our conclusions and outlines the future work. 2 Related Work During recent several years, various of issues have been studied for sentiment classification, such as feature extraction (Riloff et al., 2006), domain adaptation (Blitzer et al., 2007) and multi-domain learning (Li and Zong, 2008). For a detailed survey of this research field, see Pang and Lee (2008). However, most studies directly borrow machine learning approach from traditional topic-based text classification and very few work are focus on incorporating linguistic knowledge that sentiment text particularly contains, e.g., valence shifting phenomena and comparative sentences (Jindal and Liu, 2006). Pang et al. (2002) first employ machine learning approach to sentiment classification and find that machine learning methods definitely outperform human-produced baselines. In"
Y09-1033,P08-2065,1,0.809329,"Missing"
Y09-1033,W02-1011,0,0.0211866,"-bag-of-words modeling) across five different domains. Keywords: Sentiment classification, opinion mining, linear Classifier. 1 Introduction Sentiment classification is a task to classify text according to sentimental polarities of opinions they contain (e.g., favorable or unfavorable). This task has received considerable interests in computational linguistic community due to its wide applications. In the latest studies of this task, machine learning techniques become the state-of-the-art approach and have achieved much better results than some rule-based approaches (Kennedy and Inkpen, 2006; Pang et al., 2002) . In machine learning approach, a document (text) is usually modeled as a bag-of-words, a set of words without any word order or syntactic relation information. Therefore, the whole sentimental orientation is highly influenced by the sentiment polarity of each word. Notice that although each word takes a fixed sentiment polarity itself, its polarity contributed to the whole sentence or document might be completely the opposite. Negation and contrast transition are exactly the two kinds of linguistic phenomena which are able to reverse the sentiment polarity. For example, see a sentence contai"
Y09-1033,P04-1035,0,0.208111,"ed in terms of linear classifiers, the corresponding ideas for the first and third strategies are general for any other classification algorithms. Overall speaking, only the third one really utilizes both the reversed-sentiment and non-reversed sentiment information for learning. Also, it shares the similar computational complexity as traditional machine learning approaches based on one-bag-of-words modeling. 4 Experimental Studies 4.1 Experimental Setup Data Set: There are some famous public data sets available for sentiment classification studies. Among them, Cornell movie-review dataset 1 (Pang and Lee, 2004) and product reviews 2 (Blitzer et al., 2007) are most popularly used. Both of them are 2-category (positive and negative) tasks and each consists of 2,000 reviews in a domain. The results in some previous work are sometimes not consistent due to the application of different domains of reviews when negation is considered (Pang et al., 2002 and Na et al., 2004). Thus we follow the way of Blitzer et al. (2007) to collect more data involving data in our experiments. Specifically, we totally collect 5 domains of reviews from Amazon.cn, namely Book, Camera, HD (Hard Disk), Health and Kitchen. Each"
Y09-1033,W06-1652,0,0.0366118,"Missing"
Y09-1033,P02-1053,0,0.00962774,"olarity contributed to the whole sentence or document might be completely the opposite. Negation and contrast transition are exactly the two kinds of linguistic phenomena which are able to reverse the sentiment polarity. For example, see a sentence containing negation ""this movie is not good"" and another sentence containing contrast transition ""this mouse is good looking, but it works terribly"". The sentiment polarity of the word good in these two sentences is positive but the whole sentences are negative. Therefore, we can see that the whole sentiment is not necessarily the sum of the parts (Turney, 2002). This phenomenon is one main reason why machine learning often fails to classify some testing samples (Dredze et al., 2008). Fortunately, a language usually has some special words which indicate the possible polarity shift of a word or even a sentence. These words are called contextual valence shifters (CVSs) which can cause the valence of a lexical item to shift from one pole to the other or, less forcefully, even to modify the valence towards a more neutral position (Polanyi and Zaenen, 2006). Generally speaking, CVSs are classified into two categories: sentence-based and Copyright 2009 by"
Y09-2034,W08-0336,0,0.0137294,"eywords: Chinese word segmentation, conditional random field, word boundary decision. 1 Introduction Chinese word segmentation (CWS) is the task of segmenting text of character string into word list as original Chinese text contains no explicit boundaries between every two words. This task is an indispensible preprocessing requirement for many applications in Chinese language technology. A realistic CWS system necessarily performs well on both segmentation accuracy and speed. Segmentation accuracy is essential for many applications. For instance, in machine translation for Chinese to English (Chang et al., 2008), segmentation errors would cause translation mistakes directly. Translation systems without a wonderful CWS model are impossible to offer good results. State-of-the-arts approach called character tagging (Xue, 2003) has shown to be excellent in segmentation accuracy. This approach mainly aims to detect the character position in a certain word, e.g., beginning, middle or end of a word. It achieves much better performances than traditional word-based (or dictionary) approach, e.g., n-gram word maximum probability (Sun et al., 2006), because of its apparent advantages on detecting out-ofvocabula"
Y09-2034,P07-2018,1,0.904627,"ocessing step in the applications, long segmentation time would make the applications’ whole running time unacceptable by users. Therefore, it is meaningful to simplify the complexity of CWS approaches so as to reducing segmentation training and testing time. In terms of this view, character tagging approach (often using 4-tags (Xue, 2003; Ng and Low, 2004) or even 6-tags (Zhao et al., 2006)) is not so satisfactory in its training and testing time. Especially, given a very huge training data, this approach might not get a training model due to its large time and memory space demand. Recently, Huang et al. (2007) propose an interesting approach, named word boundary decision (WBD), which turns from words towards word boundaries. WBD tries to detect the Copyright 2009 by Shoushan Li and Chu-Ren Huang 23rd Pacific Asia Conference on Language, Information and Computation, pages 726–732 726 nature of boundary between two characters, which can be either a word boundary or not, i.e. a boundary between two words or a mere character boundary. This approach performs better than traditional word-based (or dictionary) approach but still worse than character tagging approach (Huang et al., 2008). However, this app"
Y09-2034,O08-1009,1,0.882268,"demand. Recently, Huang et al. (2007) propose an interesting approach, named word boundary decision (WBD), which turns from words towards word boundaries. WBD tries to detect the Copyright 2009 by Shoushan Li and Chu-Ren Huang 23rd Pacific Asia Conference on Language, Information and Computation, pages 726–732 726 nature of boundary between two characters, which can be either a word boundary or not, i.e. a boundary between two words or a mere character boundary. This approach performs better than traditional word-based (or dictionary) approach but still worse than character tagging approach (Huang et al., 2008). However, this approach takes a big advantage over character tagging approach in its training and testing time. In this paper, we deeply analyze the relationship between character tagging approach and WBD approach and propose a new implementation of WBD approach with conditional random field (CRF) learning approach. This implementation will make WBD approach achieve competitive performance compared to character tagging approach with 4-tags which represents the state-of-the-art approach in CWS studies but need much less training time and memory space. In the remaining part of the paper, we rev"
Y09-2034,P08-1102,0,0.202328,"o the classification. For example, when classifying the character ‘新’, we use the character features and also use the previous tag (the tag of the character ‘的’) in the classification features. 3 WBD Implementation with Character Tagging using CRF The segmentation task is to classify each character with a tag of &apos;1&apos; or &apos;0&apos;, which represents a word boundary appears after this character or not. There are several classification algorithms which can be applied to do the segmentation, such as maximum entropy (Xue, 2003), conditional random field (CRF) (Tseng et al., 2005) and perceptron algorithm (Jiang et al., 2008). We use CRF learning method as it gives state-of-the-arts performance for word segmentation and can also easily incorporate different types of features (Tseng et al., 2005). CRF is a statistical sequence modeling framework which aims to compute the following probability of a label sequence for a particular of character string: 1 pλ (Y |W ) = exp(∑∑ λk f k ( yt −1 ,W , t )) Z (W ) t∈T k where Y = { yt } is the label sequence for a character string. Here, yt ∈ {1, 0} which represents that whether there is a word boundary after the current character or not. W is the sequence of unsegmented chara"
Y09-2034,W06-0115,0,0.025879,"ion 1 Features C0 , C1 C−1C0 , C0 C1 , C1C2 T−1C0T0 , C−1T−1C0T0 , T−1C0T0C1 This tool is available at: http://crfpp.sourceforge.net/ 729 Function The single character features The character bi-gram features The character adding tag transition features 4 Experimental Studies In this section, we would empirically compare the two implementations: WBD with metaprobability classification (Huang et al., 2007) and WBD with character tagging with CRF. Furthermore, we would compare the WBD with character tagging implement with traditional 4tag character tagging approach. We use SIGHAN Bakeoff 2 data (Levow, 2006) for experimental studies. The data consists of four different sources: PKU, MSR, CityU, and AS. Their detailed information is given in Table 3. In all experiments, we mainly use F-measure (F1) as the performance measurement. F1 is defined as F1 = 2 PR / ( P + R ) where P is precision and R is recall. Another evaluation measurement is out-of-vocabulary (OOV) recall, which is used to evaluate the ability of OOV word recognition. Table 3: Corpus Information Corpus Abbrv. Beijing University Microsoft Research City University of Hong Kong Academia Sinica PKU MSR CityU AS Training Size (Words/Types"
Y09-2034,W04-3236,0,0.0573578,"vantages on detecting out-ofvocabulary (OOV) words. On the other side, the segmentation speed is also very important in some applications, such as information retrieval and online machine translation systems. Since CWS system is almost always used as a preprocessing step in the applications, long segmentation time would make the applications’ whole running time unacceptable by users. Therefore, it is meaningful to simplify the complexity of CWS approaches so as to reducing segmentation training and testing time. In terms of this view, character tagging approach (often using 4-tags (Xue, 2003; Ng and Low, 2004) or even 6-tags (Zhao et al., 2006)) is not so satisfactory in its training and testing time. Especially, given a very huge training data, this approach might not get a training model due to its large time and memory space demand. Recently, Huang et al. (2007) propose an interesting approach, named word boundary decision (WBD), which turns from words towards word boundaries. WBD tries to detect the Copyright 2009 by Shoushan Li and Chu-Ren Huang 23rd Pacific Asia Conference on Language, Information and Computation, pages 726–732 726 nature of boundary between two characters, which can be eithe"
Y09-2034,I05-3027,0,0.0546458,"rrent character but also its previous tag to do the classification. For example, when classifying the character ‘新’, we use the character features and also use the previous tag (the tag of the character ‘的’) in the classification features. 3 WBD Implementation with Character Tagging using CRF The segmentation task is to classify each character with a tag of &apos;1&apos; or &apos;0&apos;, which represents a word boundary appears after this character or not. There are several classification algorithms which can be applied to do the segmentation, such as maximum entropy (Xue, 2003), conditional random field (CRF) (Tseng et al., 2005) and perceptron algorithm (Jiang et al., 2008). We use CRF learning method as it gives state-of-the-arts performance for word segmentation and can also easily incorporate different types of features (Tseng et al., 2005). CRF is a statistical sequence modeling framework which aims to compute the following probability of a label sequence for a particular of character string: 1 pλ (Y |W ) = exp(∑∑ λk f k ( yt −1 ,W , t )) Z (W ) t∈T k where Y = { yt } is the label sequence for a character string. Here, yt ∈ {1, 0} which represents that whether there is a word boundary after the current character"
Y09-2034,O03-4002,0,0.712794,"ontains no explicit boundaries between every two words. This task is an indispensible preprocessing requirement for many applications in Chinese language technology. A realistic CWS system necessarily performs well on both segmentation accuracy and speed. Segmentation accuracy is essential for many applications. For instance, in machine translation for Chinese to English (Chang et al., 2008), segmentation errors would cause translation mistakes directly. Translation systems without a wonderful CWS model are impossible to offer good results. State-of-the-arts approach called character tagging (Xue, 2003) has shown to be excellent in segmentation accuracy. This approach mainly aims to detect the character position in a certain word, e.g., beginning, middle or end of a word. It achieves much better performances than traditional word-based (or dictionary) approach, e.g., n-gram word maximum probability (Sun et al., 2006), because of its apparent advantages on detecting out-ofvocabulary (OOV) words. On the other side, the segmentation speed is also very important in some applications, such as information retrieval and online machine translation systems. Since CWS system is almost always used as a"
Y09-2034,Y06-1012,0,0.704104,"lary (OOV) words. On the other side, the segmentation speed is also very important in some applications, such as information retrieval and online machine translation systems. Since CWS system is almost always used as a preprocessing step in the applications, long segmentation time would make the applications’ whole running time unacceptable by users. Therefore, it is meaningful to simplify the complexity of CWS approaches so as to reducing segmentation training and testing time. In terms of this view, character tagging approach (often using 4-tags (Xue, 2003; Ng and Low, 2004) or even 6-tags (Zhao et al., 2006)) is not so satisfactory in its training and testing time. Especially, given a very huge training data, this approach might not get a training model due to its large time and memory space demand. Recently, Huang et al. (2007) propose an interesting approach, named word boundary decision (WBD), which turns from words towards word boundaries. WBD tries to detect the Copyright 2009 by Shoushan Li and Chu-Ren Huang 23rd Pacific Asia Conference on Language, Information and Computation, pages 726–732 726 nature of boundary between two characters, which can be either a word boundary or not, i.e. a bo"
