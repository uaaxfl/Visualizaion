2020.emnlp-main.345,N19-1423,0,0.00766245,"ties (assuming such probabilities lie within [0, 1]). Note that, RuleNN’s philosophy is distinct from other explainable AI approaches (Ribeiro et al., 2016; Serrano and Smith, 2019). We show that it is possible to learn human-interpretable models by designing neural networks keeping explainability in mind. As opposed to learning an explainable model (e.g., RuleNN), one may also choose to explain a black-box model. Such efforts are usually restricted to explaining outcomes and only provide a shallow understanding of the overall model, if at all (Guidotti et al., 2018). While recent embeddings (Devlin et al., 2019) may lead to improved accuracy, these remain poorly understood (Moradshahi et al., 2019). One avenue of future work is to learn explainable rules that domain experts can interact with on top of such embeddings. Another is to learn rules and dictionaries jointly, which may also aid sentiment analysis (Wilson et al., 2005). 4219 Acknowledgments We would like to thank the reviewers for helpful feedback. We would also like to acknowledge the help of a lot of people not present in the author list which was instrumental in making this research possible: Shivakumar Vaithyanathan (for thought provokin"
2020.emnlp-main.345,C02-1150,0,0.297263,"of predicates from P and returns it as an LE if (Line 4): 1) its associated weight (product of corresponding numbers in αi , ∀i = 1, . . . m) is non-zero, and 2) it evaluates to true on some instance in D. When learning k CGMs, we invoke Algorithm 1 once per CGM and union the LEs. Algorithm 1’s complexity is exponential in m but it is efficient for short LEs which makes sense since longer LEs are hard to interpret. In practice, post-hoc retrieval results in a few hundred LEs (Section 5 discusses how to navigate such a set of LEs). 5 Experiments Datasets: We experiment with two datasets: TREC (Li and Roth, 2002) comprising questions, and the real-world Contracts data (proprietary) comprising sentences from legal contracts among enterprises. Contracts calls for out-of-domain generalization since its training set involves contracts with IBM as first party while the test set includes more diverse companies. Table 2 provides broad-level statistics. Sentences in Contracts may be labeled with 0, 1 or more labels (multi-label classification), so we treat each label as a binary class labeling task. Table 3 4216 |P| 0.09 0.07 0.06 0.05 0.06 0.10 0.08 0.05 0.04 101 48 80 79 39 117 77 95 71 Predicate-based Skew"
2020.emnlp-main.345,N16-3020,0,0.175651,"for Sentence Classification Prithviraj Sen IBM Research San Jose, CA, USA senp@us.ibm.com Marina Danilevsky IBM Research San Jose, CA, USA mdanile@us.ibm.com Yunyao Li Siddhartha Brahma Matthias Boehm IBM Research Google Research Graz University of Technology San Jose, CA, USA Mountain View, CA, USA Graz, Austria yunyaoli@us.ibm.com sidbrahma@google.com m.boehm@tugraz.at Laura Chiticariu IBM Watson San Jose, CA, USA chiti@us.ibm.com Rajasekar Krishnamurthy IBM Watson San Jose, CA, USA rajase@us.ibm.com Abstract approach explains predictions from a black-box model by using a surrogate models (Ribeiro et al., 2016). Another extracts explanations from neural network layer activations, especially when said activations appeal to human intuition such as attention (Bahdanau et al., 2015) which may be interpreted as importance weights assigned to (latent) features derived by the model. While such approaches are useful, they raise questions such as whether the purported explanation provided by the surrogate correctly reflects the process employed by the black-box model to arrive at the prediction (sometimes called inexact explanation (Chu et al., 2018)). Similarly, attention only provides noisy explanations (S"
2020.emnlp-main.345,J05-1004,0,0.0912359,"21, c November 16–20, 2020. 2020 Association for Computational Linguistics Dictionaries A1 transmit.01 Argm A0 Sentences z } |{ z }| { }| { z } |{ z Notices may be transmitted electronically , by registered mail. |{z } register.02 NLU facts predicates RuleNN LEs Verify LEs Figure 2: Pictorial depiction of our approach. communication(s) ← Contains(s, a) ∧ a.A1 = notice ∧ (a.verb = inform ∨ a.verb = transmit) Figure 1: Legal contract sentence and an LE for label communication (syntax simplified for brevity). Figure 1 shows such a sentence along with linguistic abstractions in PropBank notation (Palmer et al., 2005) extracted using shallow semantic parsing. It consists of two actions, transmit.01 and register.02, with arguments A0, A1 and Argm. Figure 1 also shows an FOL rule that assigns label communication by evaluating linguistic clues. We refer to such rules as linguistic expressions (LE). More precisely, the LE in the figure assigns communication if: action belongs to the sentence, surface form of the action belongs to a dictionary containing “inform” and “transmit”, and its A1 argument matches a dictionary containing “notice”. Not only does the LE’s conditions evaluate to true on the sentence examp"
2020.emnlp-main.345,D14-1052,0,0.0747777,"Missing"
2020.emnlp-main.345,D14-1162,0,0.0832405,"yond the available training data. By evaluating on two real-world sentence classification datasets and comparing against a host of baselines, we show that LEs learned by RuleNN lead to large gains in terms of area under the precision-recall curve (AUC-PR). Averaging across labels, RuleNN’s AUC-PR is 6.8×, 7.6×, 1.5× that of ILP, StarAI, other neuro-symbolic AI approaches, respectively. We also compare against black-box methods that are far less explainable. In particular, we show that RuleNN’s LEs are comparable to bi-directional LSTMs (Hochreiter and Schmidhuber, 1997) with GloVE embeddings (Pennington et al., 2014). A user study with 4 domain experts confirms that RuleNN’s LEs are interpretable, capture domain semantics and are conducive to human-machine model co-creation. We make the following contributions: • Propose LEs for explainable NLP constructed using predicates from NLU. • Propose RuleNN, a modular NN for learning multiple LEs. Given predicates, RuleNN can learn rules for any application, not just NLP. • Compare with ILP, StarAI, neuro-symbolic AI and LSTMs on real sentence classification data. • Evaluate explainability of LEs via a user study. • Illustrate human-machine co-creation by showing"
2020.emnlp-main.345,P19-1282,0,0.115355,"). Another extracts explanations from neural network layer activations, especially when said activations appeal to human intuition such as attention (Bahdanau et al., 2015) which may be interpreted as importance weights assigned to (latent) features derived by the model. While such approaches are useful, they raise questions such as whether the purported explanation provided by the surrogate correctly reflects the process employed by the black-box model to arrive at the prediction (sometimes called inexact explanation (Chu et al., 2018)). Similarly, attention only provides noisy explanations (Serrano and Smith, 2019). Such approaches leave room for improvement because explainability is treated as an after-thought whereas our goal is to treat it as a first-class citizen. In other words, is it possible to devise a neural network that directly learns a model expressed in a clear, human-readable dialect? Interpretability of predictive models is becoming increasingly important with growing adoption in the real-world. We present RuleNN, a neural network architecture for learning transparent models for sentence classification. The models are in the form of rules expressed in first-order logic, a dialect with wel"
2020.emnlp-main.345,H05-1044,0,0.120764,"learning an explainable model (e.g., RuleNN), one may also choose to explain a black-box model. Such efforts are usually restricted to explaining outcomes and only provide a shallow understanding of the overall model, if at all (Guidotti et al., 2018). While recent embeddings (Devlin et al., 2019) may lead to improved accuracy, these remain poorly understood (Moradshahi et al., 2019). One avenue of future work is to learn explainable rules that domain experts can interact with on top of such embeddings. Another is to learn rules and dictionaries jointly, which may also aid sentiment analysis (Wilson et al., 2005). 4219 Acknowledgments We would like to thank the reviewers for helpful feedback. We would also like to acknowledge the help of a lot of people not present in the author list which was instrumental in making this research possible: Shivakumar Vaithyanathan (for thought provoking discussions), Yiwei Yang, Walter S. Lasecki, Eser Kandogan (for help building the UI which made the user study possible), Diman Ghazi, Poornima Chozhiyath Raman, Ramiya Venkatachalam, Vinitha Yaski and Sneha Srinivasan (for help with the user study). This work was done while Siddhartha Brahma and Matthias Boehm were at"
2020.emnlp-main.517,C18-1058,1,0.749278,"entity normalization and variant generation. Learning the implicit structured representations of entity names without context and external knowledge is particularly challenging. In this paper, we present a novel learning framework that combines active learning and weak supervision to solve this problem. Our experimental evaluation show that this framework enables the learning of high-quality models from merely a dozen or so labeled examples. 1 Introduction Entity normalization and variant generation are fundamental for a variety of other tasks such as semantic search and relation extraction (Bhutani et al., 2018; Arasu and Kaushik, 2009). Given an entity name E, the goal of entity normalization is to convert E to a canonical form (e.g., “Jordan, Michael” → “Michael Jordan”), while the goal of entity variant generation is to convert E to a set of different textual representations that refer to the same entity as E (e.g., “Michael Jordan” → {“Jordan, Michael”, “MJ”, “M. Jordan”, . . .}). Typically, entity normalization and variant generation are done by first performing entity linking (Moro et al., 2014; Zhao et al., 2019; Li et al., 2017), i.e., matching entity names appearing in some context (e.g., f"
2020.emnlp-main.517,D11-1071,0,0.0230822,", 2018; Ju et al., 2018; Finkel and Manning, 2009; Dinarelli and Rosset, 2012) identify nested entities within some context using fully supervised methods that require large amounts of labeled data, whereas our goal is to learn from very few labels (e.g., &lt; 15) in a contextless fashion. Active learning (Settles, 2009) and weak supervision have been widely adopted for solving many entitycentric problems, such as entity resolution (Kasai et al., 2019; Qian et al., 2019, 2017; Gurajada et al., 2019), NER (Lison et al., 2020; Shen et al., 2018; He and Sun, 2017; Nadeau, 2007), and entity linking (Chen and Ji, 2011). While the power of the combination of the two techniques has been demonstrated in other domains (e.g., computer vision (Brust et al., 2020)), to the best of our knowlFigure 1: BERT-CRF based model edge, the two approaches are usually applied in isolation in prior entity-related work. Recently, data programming approaches (e.g., (Ratner et al., 2017; Safranchik et al., 2020)) use labeling functions/rules to generate weak labels to train machine learning models in low-resource scenarios. Data programming approaches like Snorkel usually assume that labeling functions are manually provided by us"
2020.emnlp-main.517,P19-1586,1,0.811661,"inkel and Manning, 2009), NER focuses on identifying the outermost flat entities and completely ignores their internal structured representations. (Katiyar and Cardie, 2018; Ju et al., 2018; Finkel and Manning, 2009; Dinarelli and Rosset, 2012) identify nested entities within some context using fully supervised methods that require large amounts of labeled data, whereas our goal is to learn from very few labels (e.g., &lt; 15) in a contextless fashion. Active learning (Settles, 2009) and weak supervision have been widely adopted for solving many entitycentric problems, such as entity resolution (Kasai et al., 2019; Qian et al., 2019, 2017; Gurajada et al., 2019), NER (Lison et al., 2020; Shen et al., 2018; He and Sun, 2017; Nadeau, 2007), and entity linking (Chen and Ji, 2011). While the power of the combination of the two techniques has been demonstrated in other domains (e.g., computer vision (Brust et al., 2020)), to the best of our knowlFigure 1: BERT-CRF based model edge, the two approaches are usually applied in isolation in prior entity-related work. Recently, data programming approaches (e.g., (Ratner et al., 2017; Safranchik et al., 2020)) use labeling functions/rules to generate weak labels t"
2020.emnlp-main.517,N18-1079,0,0.138318,"the tasks. Unfortunately, in some scenarios, such as search (Thompson and Dozier, 1997), entity names are not surrounded by context. Furthermore, for specialized domainspecific applications, there may not be a knowledge base to govern the names of the relevant entities. Thus, entity linking is not always applicable. In this paper, we take the view that entity normalization and variant generation can be done without contextual information or external KBs if we understand the internal structures of entity names. As observed in (Campos et al., 2015; Bhutani et al., 2018; Arasu and Kaushik, 2009; Katiyar and Cardie, 2018; Finkel and Manning, 2009), entity names often have implicit structures that can be exploited to solve entity normalization and variant generation. Table 1 shows how we can manipulate such structured representations of entity names to generate different variations without help from context or external knowledge. Declarative frameworks are proposed in (Arasu and Kaushik, 2009; Campos et al., 2015) to allow developers to manually specify rules that parse entity names into a structured representation. To avoid such low-level manual effort, (Katiyar and Cardie, 2018; Finkel and Manning, 2009) use"
2020.emnlp-main.517,W97-0315,0,0.550507,"rt E to a set of different textual representations that refer to the same entity as E (e.g., “Michael Jordan” → {“Jordan, Michael”, “MJ”, “M. Jordan”, . . .}). Typically, entity normalization and variant generation are done by first performing entity linking (Moro et al., 2014; Zhao et al., 2019; Li et al., 2017), i.e., matching entity names appearing in some context (e.g., free text) to named entities in curated knowledge bases (KBs), then use the canonical form or variations (of the linked entities) residing in the KBs to complete the tasks. Unfortunately, in some scenarios, such as search (Thompson and Dozier, 1997), entity names are not surrounded by context. Furthermore, for specialized domainspecific applications, there may not be a knowledge base to govern the names of the relevant entities. Thus, entity linking is not always applicable. In this paper, we take the view that entity normalization and variant generation can be done without contextual information or external KBs if we understand the internal structures of entity names. As observed in (Campos et al., 2015; Bhutani et al., 2018; Arasu and Kaushik, 2009; Katiyar and Cardie, 2018; Finkel and Manning, 2009), entity names often have implicit s"
2020.findings-emnlp.279,D18-1548,0,0.0577109,"Missing"
2020.findings-emnlp.279,taule-etal-2008-ancora,0,0.0515269,"guments are labeled at a very granular level, and multiple arguments in these languages may correspond to a single argument in the source language. For example, multiple arguments in Czech frequently map to only one corresponding argument Languages with Similar Linguistic Annotations: To further study the effectiveness of CLAR, we analyze the cross-lingual transfer between the languages known to have similar linguistic annotations. We expect to observe better cross-lingual transfer between such language pairs. Specifically, we examine Spanish (ES) and Catalan (CA) from the same AnCora corpus (Taulé et al., 2008). We consider ES as the source language because it has more training samples than CA. In Table 6 we show the paired arguments detected by CLAR along with the euclidean distance between them. It can be seen that the euclidean distance for all paired arguments are close to 1, confirming that CLAR can effectively match semantically similar arguments across languages. The experimental results are summarized in Table 5. As expected, CLAR surpasses all prior re3119 Training Method P R F1 CA Baseline 78.47 75.44 76.92 Polyglot CLAR Polyglot CLAR 79.10 78.72 77.59 78.35 75.90 77.91 76.68 76.54 77.47 7"
2020.findings-emnlp.279,D18-1034,0,0.0283746,"rporates syntactic information from a parser (Kiperwasser and Goldberg, 2016). Further, (Li et al., 2018) proposes a more general framework to integrate syntax into SRL tasks. All these methods have been shown to perform well on rich resource languages. Several recent attempts have been made to transfer knowledge from rich source languages to low resource languages for SRL tasks (Mulcaire et al., 2018, 2019) such that the knowledge transfer helps the model to learn better feature representations for low resource languages. To some extent, in other NLP tasks such as named identity recognition (Xie et al., 2018), and syntactic dependency parsing (Ammar et al., 2016) this knowledge transfer seems to be helping low resource languages. Our experimental results further strengthen this claim and confirm that languages share knowledge at the semantic level as well. An alternative line of work transfers crosslingual knowledge to generate semantic labels for low resource languages by exploiting the monolingual SRL model and Multilingual parallel data (Akbik et al., 2016; Akbik and Li, 2016) with an assumption that the sentences in parallel corpora are semantically equivalent. Similarly, (Prazák and Konopík,"
2020.findings-emnlp.279,P16-2033,0,0.0282318,"t CLAR consistently improves SRL performance on multiple languages over monolingual and polyglot baselines for low resource languages. 1 Figure 1: Example of predicate-argument structure from the CoNLL 2009 training data for I) Chinese, II) German, and III) English. Introduction Semantic Role Labeling (SRL) is the task of labeling each predicate and its corresponding arguments in a given sentence. SRL provides a more stable meaning representation across syntactically different sentences and has been seen to help a wide range of NLP applications such as question answering (Maqsud et al., 2014; Yih et al., 2016) and machine translation (Shi et al., 2016). b ∗ Work done while at IBM Research Recent end-to-end deep neural networks for SRL, though performing well for languages with large training data (Marcheggiani et al., 2017; Tan et al., 2018; He et al., 2018), are much less effective for low resources languages due to very limited annotated data for these languages. Methods such as polyglot training (Mulcaire et al., 2018) seek to make these models perform better on low resource languages by combining supervision from multiple languages. The key idea in polyglot training is to combine the training d"
2020.findings-emnlp.279,W09-1209,0,0.0715659,"Missing"
2020.findings-emnlp.279,P15-1109,0,0.0168926,"study (Appendix D), it may introduce additional noise. We plan to explore this direction in the future. 5 Related Work Models for SRL largely fall into two categories: syntax-agnostic and syntax-aware. For a long time, syntax was considered a prerequisite for better SRL performance (Punyakanok et al., 2008; Gildea and Jurafsky, 2002). In the absence of syntactic information, these methods struggle to capture the discriminatory features and thus perform poorly. Recently, end-to-end deep neural models have been shown to extract useful discriminatory features even without syntactic information (Zhou and Xu, 2015; Marcheggiani et al., 2017; Tan et al., 2018; He et al., 2018) and achieve state-of-the-art performance. However, some works (Roth and Lapata, 2016; He et al., 2017; Strubell et al., 2018) argue that given a high-quality syntax parser, it is 3120 possible to further improve the SRL performance. Along this line, (Marcheggiani and Titov, 2017) proposed a SRL model based on graph convolutional networks which incorporates syntactic information from a parser (Kiperwasser and Goldberg, 2016). Further, (Li et al., 2018) proposes a more general framework to integrate syntax into SRL tasks. All these"
2020.findings-emnlp.38,P17-2082,0,0.0418849,"Missing"
2020.findings-emnlp.38,C16-1058,1,0.911728,"by requiring majority agreement and no uncertainty. These difficult cases are then decided by experts with the necessary knowledge. Second, consider the challenge that there can be an overwhelming number of options. The Filter phase reduces the complexity of the task, focusing attention on likely options. This assumes that our filtering process removes unlikely options without removing the correct ones, which we verify experimentally in Section 5.1. Comparison Approaches In our experiments, we compare with three other data annotation methods. Automatic uses the output of a statistical model (Akbik and Li, 2016), with no human input. Review-Select uses a two phase process. First, five workers review the system prediction. If any worker marks the prediction as incorrect, another set of workers choose an answer and we assign the most common choice. Review-Expert uses the same review process as the previous approach, but an expert chooses the answer rather than the crowd. 3 For argument tasks, there is one more option “none of the above”, to cover situations where the automatic system assigns an argument to an incorrect predicate. 4 Experimental Setup We consider experiments on two sets of data, both fr"
2020.findings-emnlp.38,W15-1601,0,0.0340743,"Missing"
2020.findings-emnlp.38,N19-1224,0,0.0216722,"Missing"
2020.findings-emnlp.38,E14-4044,0,0.056425,"Missing"
2020.findings-emnlp.38,P18-1191,0,0.0426626,"Missing"
2020.findings-emnlp.38,P13-2130,0,0.0482145,"Missing"
2020.findings-emnlp.38,2020.lrec-1.30,0,0.0659117,"Missing"
2020.findings-emnlp.38,P17-1044,0,0.0355788,"Missing"
2020.findings-emnlp.38,D15-1076,0,0.063773,"Missing"
2020.findings-emnlp.38,W11-0404,0,0.055554,"Missing"
2020.findings-emnlp.38,N13-1062,0,0.0671724,"Missing"
2020.findings-emnlp.38,C12-2053,0,0.0434751,"Missing"
2020.findings-emnlp.38,N16-1104,0,0.0354739,"Missing"
2020.findings-emnlp.38,J05-1004,0,0.0652782,"Missing"
2020.findings-emnlp.38,P15-2067,0,0.0508123,"Missing"
2020.findings-emnlp.38,D16-1264,0,0.0459107,"Missing"
2020.findings-emnlp.38,2020.acl-main.626,0,0.0349925,"Missing"
2020.findings-emnlp.38,W11-0409,0,0.0507009,"Missing"
2020.findings-emnlp.38,D13-1170,0,0.00661981,"Missing"
2020.findings-emnlp.38,W13-0215,0,0.0779562,"Missing"
2020.findings-emnlp.38,D17-1205,1,0.881211,"Missing"
2020.findings-emnlp.38,P12-1087,0,0.0411489,"Missing"
2020.nli-1.1,P17-2057,0,0.0183438,"inference could be improved if the question representation reflected all prior inferences. 7 Modern KB-QA systems use neural network models for semantic matching. These use an encode-compare approach (Luo et al., 2018; Yih et al., 2015; Yu et al., 2017), wherein continuous representations of question and query candidates are compared to pick a candidate which is executed to find answers. These methods require question-answer pairs as training data and focus on a single knowledge source. Combining multiple knowledge sources in KB-QA has been studied before, but predominantly for textual data. (Das et al., 2017b) uses memory networks and universal schema to support inference on the union of KB and text. (Sun et al., 2018) enriches KB subgraphs with entity links from text documents and formulates KB-QA as a node classification task. The key limitations for these methods are that a) they cannot handle highly compositional questions and b) they ignore the relational structure between the entities in the text. Our proposed system additionally uses an extracted KB that explicitly models the relations between entities and can compose complex queries from simple queries. We formulate complex query construc"
2020.nli-1.1,C16-1226,0,0.0366507,"Missing"
2020.nli-1.1,P15-1034,0,0.068232,"Missing"
2020.nli-1.1,D18-1242,0,0.229429,"or the question. Since there can be multiple ways to answer a complex question, we derive several full query derivations. We rank them based on the semantic similarity scores of their partial queries, query structure and entity linking scores. We execute the best derivation over the multiple KBs. Fig. 3 shows the architecture of our proposed system, M ULTIQUE. 3 Partial Query Candidate Generation We first describe how we find candidates for partial queries given an input question. We use a staged generation method with staged states and actions. Compared to previous methods (Yih et al., 2015; Luo et al., 2018) which assume a question has one main relation, our strategy can handle complex questions which have multiple main relations (and hence partial queries). We include a new action At that denotes the end of the search for a partial query and transition to a state St . State St refers back to the composition tree to determine the join condition between the current partial query and the next query. If they share an answer node, candidate generation for the subsequent query can resume independently. Otherwise, it waits for the answers to the current query. We generate (entity, mention) pairs for a"
2020.nli-1.1,C16-1236,0,0.0962343,"wledge source K= {Kc , Ko } is denoted as K=(V, E, R), where V is the set of entities and E is a set of triples (s, r, o). A triple denotes a relation r ∈ R between subject s ∈ V and object o ∈ V. The relation set R is a collection of ontological relations Ro from Kc and textual relations Rt from Ko . A higher order relation is expressed using multiple triples connected using a special CVT node. tem, M ULTIQUE, which constructs query patterns to answer complex questions from simple queries each targeting a specific KB. We build upon recent work on semantic parsing using neural network models (Bao et al., 2016; Yih et al., 2015) to learn the simple queries for complex questions. These methods follow an enumerate-encode-compare approach, where candidate queries are first collected and encoded as semantic vectors, which are then compared to the vector representation of the question. The candidate with the highest semantic similarity is then executed over the KB. We propose two key modifications to adapt these models to leverage information from multiple KBs and support complex questions. First, to enable collective inference over ontological and textual relations from the KBs, we align the different"
2020.nli-1.1,D15-1166,0,0.0515495,"i using an embedding matrix Ew and use an LSTM to encode the sequence to a latent vector q w . Similarly, we encode the dependency tree into a latent vector q dep . Attention mechanism. Simple questions contain expressions for matching one main relation path. A complex question, however, has expressions for matching multiple relation paths, which could interfere with each other. For instance, words ‘college’ and ‘attend’ can distract the matching of the phrase ‘author of’ to the relation book.author. We mitigate this issue by improving the question representation using an attention mechanism (Luong et al., 2015). The idea is to learn to emphasize parts of the question that are relevant to a context derived using the partial query vector g. Formally, given all hidden vectors ht at time step t ∈ {1, 2, . . . , n} of the token-level representation of the question, we derive a context vector c as the weighted sum of all the hidden states: n X c= αt ht Encoding main relation path. The main relation path can have different forms, a textual relation from Ko or an ontological relation from Kc . In order to collectively infer over them in the same space, we first align the textual relations to ontological rel"
2020.nli-1.1,D17-1252,0,0.240619,"sing features such as semantic similarity scores, entity linking scores, number of constraints in the query, number of variables, number of relations and number of answer entities. Given the best scoring derivation, we translate it to a KB query and evaluate it to return answers to the quesImplicit Supervision Obtaining questions with fully-annotated queries is expensive, especially when queries are complex. In contrast, obtaining answers is easier. In such a setting, the quality of a query candidate is often measured indirectly by computing the F1 score of its answers to the labeled answers (Peng et al., 2017a). However, for complex questions, answers to the partial queries may have little or no overlap with the labeled answers. We, therefore, adopt an alternative scoring strategy where we estimate the quality of a partial query as the best F1 score of all its full query derivations. Formally, we compute a (k) score V (Gi ) for a partial query as: (k) Query Composition (k) V (Gi ) = max F1 (Dt+1 ) i≤t≤n−1 where Dt denotes the derivation at level t and n denotes the number of partial queries. 5 tion. Such an approach has been shown to be successful in answering complex questions over a single knowl"
2020.nli-1.1,D14-1162,0,0.0841218,"Missing"
2020.nli-1.1,D18-1455,0,0.0191474,"s use neural network models for semantic matching. These use an encode-compare approach (Luo et al., 2018; Yih et al., 2015; Yu et al., 2017), wherein continuous representations of question and query candidates are compared to pick a candidate which is executed to find answers. These methods require question-answer pairs as training data and focus on a single knowledge source. Combining multiple knowledge sources in KB-QA has been studied before, but predominantly for textual data. (Das et al., 2017b) uses memory networks and universal schema to support inference on the union of KB and text. (Sun et al., 2018) enriches KB subgraphs with entity links from text documents and formulates KB-QA as a node classification task. The key limitations for these methods are that a) they cannot handle highly compositional questions and b) they ignore the relational structure between the entities in the text. Our proposed system additionally uses an extracted KB that explicitly models the relations between entities and can compose complex queries from simple queries. We formulate complex query construction as a search problem. This is broadly related to structured output prediction (Peng et al., 2017b) and path f"
2020.nli-1.1,N18-1059,0,0.0749367,"ents, we set the threshold to 5 to avoid sparse and noisy signals in the alignment. 4.3 5 In this work, we focus on constructing complex queries using a sequence of simple partial queries, each with one main relation path. Since the original question does not have to be chunked into simple questions, constructing composition trees for such questions is fairly simple. Heuristically, a composition tree can simply be derived by estimating the number of main relations (verb phrases) in the question and the dependency between them (subordinating or coordinating). We use a more sophisticated model (Talmor and Berant, 2018) to derive the composition tree. The post-order traversal of the tree yields the order in which partial queries should be executed. Given a computation tree, we adopt a beam search and evaluate best k candidates for a partial query at each level. This helps maintain tractability in the large space of possible complex query derivations. The semantic matching model only independently scores the partial queries and not complete derivations. We, thus, need to find the best derivation that captures the meaning of the complex input question. To determine the best derivation, we aggregate the scores"
2020.nli-1.1,D17-1060,0,0.0292226,"s KB subgraphs with entity links from text documents and formulates KB-QA as a node classification task. The key limitations for these methods are that a) they cannot handle highly compositional questions and b) they ignore the relational structure between the entities in the text. Our proposed system additionally uses an extracted KB that explicitly models the relations between entities and can compose complex queries from simple queries. We formulate complex query construction as a search problem. This is broadly related to structured output prediction (Peng et al., 2017b) and path finding (Xiong et al., 2017; Das et al., 2017a) methods which learn to navigate the search space using supervision from question-answer pairs. These methods are effective for answering simple questions because the search space is small and the rewards to guide the search can be estimated reliably. We extend the ideas of learning from implicit supervision (Liang et al., 2016) and integrate it with partial query evaluation and priors to Related Work KB-QA methods can be broadly classified into: 8 preserve the supervision signals. 8 Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krish"
2020.nli-1.1,P16-1220,0,0.0630966,"Missing"
2020.nli-1.1,P15-1128,0,0.571162,"Kc , Ko } is denoted as K=(V, E, R), where V is the set of entities and E is a set of triples (s, r, o). A triple denotes a relation r ∈ R between subject s ∈ V and object o ∈ V. The relation set R is a collection of ontological relations Ro from Kc and textual relations Rt from Ko . A higher order relation is expressed using multiple triples connected using a special CVT node. tem, M ULTIQUE, which constructs query patterns to answer complex questions from simple queries each targeting a specific KB. We build upon recent work on semantic parsing using neural network models (Bao et al., 2016; Yih et al., 2015) to learn the simple queries for complex questions. These methods follow an enumerate-encode-compare approach, where candidate queries are first collected and encoded as semantic vectors, which are then compared to the vector representation of the question. The candidate with the highest semantic similarity is then executed over the KB. We propose two key modifications to adapt these models to leverage information from multiple KBs and support complex questions. First, to enable collective inference over ontological and textual relations from the KBs, we align the different relation forms and"
2020.nli-1.1,P16-2033,0,0.0278409,"Missing"
2020.nlpcovid19-acl.1,2020.acl-main.207,1,0.909878,"text with entity mentions predicted from several techniques, including weak supervision using the NLM’s Unified Medical Language System (UMLS) Metathesaurus (Bodenreider, 2004). Text classification Some efforts focus on extracting sentences or passages of interest. For example, Liang and Xie (2020) uses BERT (Devlin et al., 2019) to extract sentences from CORD-19 that contain C OVID -19-related radiological findings. Pretrained model weights BioBERT and SciBERT have been popular pretrained LMs for C OVID 19-related tasks. DeepSet has released a BERTbase model pretrained on CORD-19.21 SPECTER (Cohan et al., 2020) paper embeddings computed using paper titles and abstracts are being released with each CORD-19 update. SeVeN relation embeddings (Espinosa-Anke and Schockaert, 2018) between word pairs have also been made available for CORD-19.22 Knowledge graphs The Covid Graph project23 releases a C OVID -19 knowledge graph built from mining several public data sources, including 19 https://github.com/allenai/cord19 There are many Search and QA systems to survey. We have chosen to highlight the systems that were made publiclyavailable within a few weeks of the CORD-19 initial release. 21 https://huggingfac"
2020.nlpcovid19-acl.1,N19-1423,0,0.0147555,"l entities can be useful. NER and linking can be performed using NLP toolkits like ScispaCy (Neumann et al., 2019) or language models like BioBERT-base (Lee et al., 2019) and SciBERTbase (Beltagy et al., 2019) finetuned on biomedical NER datasets. Wang et al. (2020) augments CORD-19 full text with entity mentions predicted from several techniques, including weak supervision using the NLM’s Unified Medical Language System (UMLS) Metathesaurus (Bodenreider, 2004). Text classification Some efforts focus on extracting sentences or passages of interest. For example, Liang and Xie (2020) uses BERT (Devlin et al., 2019) to extract sentences from CORD-19 that contain C OVID -19-related radiological findings. Pretrained model weights BioBERT and SciBERT have been popular pretrained LMs for C OVID 19-related tasks. DeepSet has released a BERTbase model pretrained on CORD-19.21 SPECTER (Cohan et al., 2020) paper embeddings computed using paper titles and abstracts are being released with each CORD-19 update. SeVeN relation embeddings (Espinosa-Anke and Schockaert, 2018) between word pairs have also been made available for CORD-19.22 Knowledge graphs The Covid Graph project23 releases a C OVID -19 knowledge graph"
2020.nlpcovid19-acl.1,C18-1225,0,0.0454275,"Missing"
2020.nlpcovid19-acl.1,2020.acl-main.447,1,0.934558,"ed paper entry as canonical. If any metadata in the canonical entry are missing, values from other members of the cluster are promoted to fill in the blanks. Cluster filtering Some entries harvested from sources are not papers, and instead correspond to materials like tables of contents, indices, or informational documents. These entries are identified in an ad hoc manner and removed from the dataset. 2.3 Processing full text Most papers are associated with one or more PDFs.12 To extract full text and bibliographies from each PDF, we use the PDF parsing pipeline created for the S2ORC dataset (Lo et al., 2020).13 In (Lo et al., 2020), we introduce the S2ORC JSON format for representing scientific paper full text, 11 This is a conservative clustering policy in which any metadata conflict prohibits clustering. An alternative policy would be to cluster if any identifier matches, under which a, b, and c would form one cluster with identifiers (x, y, [z, z 0 ]). 12 PMC papers can have multiple associated PDFs per paper, separating the main text from supplementary materials. 13 One major difference in full text parsing for CORD-19 is that we do not use ScienceParse,14 as we always derive this metadata fr"
2020.nlpcovid19-acl.1,D18-1211,0,0.0563002,"Missing"
2020.nlpcovid19-acl.1,W19-5034,0,0.0403962,"D-19 have already been developed. Most combine elements of text-based information retrieval and extraction, as illustrated in Figure 3. We have compiled a list of these efforts on the CORD19 public GitHub repository19 and highlight some systems in Table 2.20 4.3 Text mining and NLP research The following is a summary of resources released by the NLP community on top of CORD-19 to support other research activities. Information extraction To support extractive systems, NER and entity linking of biomedical entities can be useful. NER and linking can be performed using NLP toolkits like ScispaCy (Neumann et al., 2019) or language models like BioBERT-base (Lee et al., 2019) and SciBERTbase (Beltagy et al., 2019) finetuned on biomedical NER datasets. Wang et al. (2020) augments CORD-19 full text with entity mentions predicted from several techniques, including weak supervision using the NLM’s Unified Medical Language System (UMLS) Metathesaurus (Bodenreider, 2004). Text classification Some efforts focus on extracting sentences or passages of interest. For example, Liang and Xie (2020) uses BERT (Devlin et al., 2019) to extract sentences from CORD-19 that contain C OVID -19-related radiological findings. Pret"
2020.nlpcovid19-acl.1,D19-1410,0,0.0687887,"Missing"
2020.nlpcovid19-acl.1,P19-1436,0,0.0588806,"Missing"
2020.nlpcovid19-acl.1,P18-4015,1,0.803645,"Missing"
2020.nlpcovid19-acl.1,D19-1371,1,\N,Missing
2020.nlpcovid19-acl.1,2020.nlpcovid19-acl.2,0,\N,Missing
2021.acl-long.64,K19-1049,0,0.0117416,"labels Li (see Figure 3). Inference. Given mention mi and candidate set Ci , similar to training, we generate features for each mention-candidate pair (mi , eij ) in the feature generation step. We then pass them through the learned LNN network to obtain final scores for each candidate entity in Ci as shown in Figure 3. 5 Baselines. We compare our approach to (1) BLINK (Wu et al., 2020), the current state-of-theart on both short-text and long-text EL, (2) three BERT-based models - (a) BERT: both mention and candidate entity embeddings are obtained via BERTbase pre-trained encoder, similar to (Gillick et al., 2019), (b) BERTWiki: mention embeddings are obtained from BERTbase , while candidate entity is from pretrained Wiki2Vec (Yamada et al., 2020), (c) Box: BERTWiki embeddings finetuned with Query2Box embeddings (see Section 4.1). In addition to the aforementioned black-box neural models, we also compare our approach to (3) two logistic regression models that use the same feature set as LNN-EL: LogisticRegression without BLINK and LogisticRegressionBLIN K with BLINK. Furthermore, we use the following variants of our approach: (4) RuleEL: a baseline rule-based EL approach with manually defined weights a"
2021.acl-long.64,E06-1002,0,0.365303,"cian Popa2 , Prithviraj Sen2 , Yunyao Li2 , Alexander Gray2 1 2 3 MIT IBM Research University of Oregon hjian42@mit.edu, {sairam.gurajada, alexander.gray}@ibm.com, luqh@cs.uoregon.edu, sumit.neelam@in.ibm.com, {lpopa,senp,yunyaoli}@us.ibm.com Abstract Entity Linking (EL) is the task of disambiguating textual mentions by linking them to canonical entities provided by a knowledge graph (KG) such as DBpedia, YAGO (Suchanek et al., 2007) or Wikidata (Vrandeˇci´c and Krötzsch, 2014). A large body of existing work deals with EL in the context of longer text (i.e., comprising of multiple sentences) (Bunescu and Pasca, 2006). The general approach is: 1) extract features measuring some degree of similarity between the textual mention and any one of several candidate entities (Mihalcea and Csomai, 2007; Cucerzan, 2007; Ratinov et al., 2011), followed by 2) the disambiguation step, either heuristics-based (non-learning) (Hoffart et al., 2011; Sakor et al., 2019; Ferragina and Scaiella, 2012) or learning-based (Mihalcea and Csomai, 2007; Cucerzan, 2007; Ratinov et al., 2011; Hoffart et al., 2012; Ganea and Hofmann, 2017), to link the mention to an actual entity. A particular type of entity linking, focused on short t"
2021.acl-long.64,D07-1074,0,0.097485,"com, {lpopa,senp,yunyaoli}@us.ibm.com Abstract Entity Linking (EL) is the task of disambiguating textual mentions by linking them to canonical entities provided by a knowledge graph (KG) such as DBpedia, YAGO (Suchanek et al., 2007) or Wikidata (Vrandeˇci´c and Krötzsch, 2014). A large body of existing work deals with EL in the context of longer text (i.e., comprising of multiple sentences) (Bunescu and Pasca, 2006). The general approach is: 1) extract features measuring some degree of similarity between the textual mention and any one of several candidate entities (Mihalcea and Csomai, 2007; Cucerzan, 2007; Ratinov et al., 2011), followed by 2) the disambiguation step, either heuristics-based (non-learning) (Hoffart et al., 2011; Sakor et al., 2019; Ferragina and Scaiella, 2012) or learning-based (Mihalcea and Csomai, 2007; Cucerzan, 2007; Ratinov et al., 2011; Hoffart et al., 2012; Ganea and Hofmann, 2017), to link the mention to an actual entity. A particular type of entity linking, focused on short text (i.e., a single sentence or question), has attracted recent attention due to its relevance for downstream applications such as question answering (e.g., (Kapanipathi et al., 2021)) and conver"
2021.acl-long.64,2020.aacl-main.46,1,0.736288,"ally, we formulate the problem as a ranking of the candidates in Ci so that the “correct&quot; entity for mi is ranked highest. Following existing approaches(e.g. (Sakor et al., 2019; Wu et al., 2020), we use off-the-shelf lookup tools such as DBpedia lookup2 to retrieve top-100 candidates for each mention. While this service is specific to DBpedia, we assume that similar services exist or can be implemented on top of other KGs. 3.2 Logical Neural Networks Fueled by the rise in complexity of deep learning, recently there has been a push towards learning interpretable models (Guidotti et al., 2018; Danilevsky et al., 2020). While linear classifiers, decision lists/trees may also be considered interpretable, rules expressed in first-order logic (FOL) form a much more powerful, closed language that offer semantics clear enough for human interpretation and a larger range of operators facilitating the expression of richer models. To learn these rules, neuro-symbolic AI typically substitutes conjunctions (disjunctions) with differentiable t-norms (t-conorms) (Esteva and Godo, 2001). However, since these norms do not have any learnable parameters (more details in Appendix A.1), their behavior cannot be adjusted, thus"
2021.acl-long.64,D11-1072,0,0.413428,"nking them to canonical entities provided by a knowledge graph (KG) such as DBpedia, YAGO (Suchanek et al., 2007) or Wikidata (Vrandeˇci´c and Krötzsch, 2014). A large body of existing work deals with EL in the context of longer text (i.e., comprising of multiple sentences) (Bunescu and Pasca, 2006). The general approach is: 1) extract features measuring some degree of similarity between the textual mention and any one of several candidate entities (Mihalcea and Csomai, 2007; Cucerzan, 2007; Ratinov et al., 2011), followed by 2) the disambiguation step, either heuristics-based (non-learning) (Hoffart et al., 2011; Sakor et al., 2019; Ferragina and Scaiella, 2012) or learning-based (Mihalcea and Csomai, 2007; Cucerzan, 2007; Ratinov et al., 2011; Hoffart et al., 2012; Ganea and Hofmann, 2017), to link the mention to an actual entity. A particular type of entity linking, focused on short text (i.e., a single sentence or question), has attracted recent attention due to its relevance for downstream applications such as question answering (e.g., (Kapanipathi et al., 2021)) and conversational systems. Short-text EL is particularly challenging because the limited context surrounding mentions results in great"
2021.acl-long.64,N19-1423,0,0.0281573,"ccurring mentions from text to connected entities in the KG is an instance of collective entity linking. This example provides some intuition as to how priors, local features (string similarity) and collective entity linking can be exploited to overcome the limited context in short-text EL. While the use of priors, local features and nonlocal features (for collective linking) has been proposed before (Ratinov et al., 2011), our goal in this paper is to provide an extensible framework that can combine any number of such features and more, including contextual embeddings such as BERT encodings (Devlin et al., 2019) and Query2box embeddings (Ren et al., 2020), and even the results of previously developed neural EL models (e.g., BLINK (Wu et al., 2020)). Additionally, such a framework must not only allow for easy inclusion of new sources of evidence but also for interpretability of the resulting model (Guidotti et al., 2018). An approach that combines disparate features should, at the very least, be able to state, post-training, which features are detrimental and which features aid EL performance and under what conditions, in order to enable actionable insights in the next iteration of model improvement."
2021.acl-long.64,2020.emnlp-main.522,0,0.14368,"ffective in zero-shot settings. BLINK is quite effective on short text (as observed in our findings); in our approach, we use BLINK both as a baseline and as a component that is combined in larger rules. For short-text EL, some prior works (Sakor et al., 2019; Ferragina and Scaiella, 2012; Mendes et al., 2011) address the joint problem of mention detection and linking, with primary focus on identifying mention spans, while linking is done via heuristic methods without learning. (Sakor et al., 2019) also jointly extracts relation spans which aide in overall linking performance. The recent ELQ (Li et al., 2020) extends BLINK to jointly learn mention detection and linking. In contrast, we focus solely on linking and take a different strategy based on combining logic rules with learning. This facilitates a principled way combining multiple types of EL features with interpretability and learning using promising gradient-based techniques. Rule-based Learning. FOL rules and learning have been successfully applied in some NLP tasks and also other domains. Of these, the task that is closest to ours is entity resolution (ER), which is the task of linking two entities across two structured datasets. In this"
2021.acl-long.64,P19-1335,0,0.011986,"e dataset on which it was trained, but also on other datasets from the same domain without further training. 2 Related Work Entity Linking Models. Entity Linking is a wellstudied problem in NLP, especially for long text. Approaches such as (Bunescu and Pasca, 2006; Ratinov et al., 2011; Sil et al., 2012; Hoffart et al., 2011; Shen et al., 2015) use a myriad of classical ML and deep learning models to combine priors, local and global features. These techniques, in general, can be applied to short text, but the lack of sufficient context may render them ineffective. The recently proposed BLINK (Logeswaran et al., 2019; 776 Wu et al., 2020) uses powerful transformer-based encoder architectures trained on massive amounts of data (such as Wikipedia, Wikia) to achieve SotA performance on entity disambiguation tasks, and is shown to be especially effective in zero-shot settings. BLINK is quite effective on short text (as observed in our findings); in our approach, we use BLINK both as a baseline and as a component that is combined in larger rules. For short-text EL, some prior works (Sakor et al., 2019; Ferragina and Scaiella, 2012; Mendes et al., 2011) address the joint problem of mention detection and linking"
2021.acl-long.64,D17-1277,0,0.0346463,"Missing"
2021.acl-long.64,D12-1011,0,0.061757,"Missing"
2021.acl-long.64,D14-1162,0,0.089076,"Missing"
2021.acl-long.64,P11-1138,0,0.671271,",yunyaoli}@us.ibm.com Abstract Entity Linking (EL) is the task of disambiguating textual mentions by linking them to canonical entities provided by a knowledge graph (KG) such as DBpedia, YAGO (Suchanek et al., 2007) or Wikidata (Vrandeˇci´c and Krötzsch, 2014). A large body of existing work deals with EL in the context of longer text (i.e., comprising of multiple sentences) (Bunescu and Pasca, 2006). The general approach is: 1) extract features measuring some degree of similarity between the textual mention and any one of several candidate entities (Mihalcea and Csomai, 2007; Cucerzan, 2007; Ratinov et al., 2011), followed by 2) the disambiguation step, either heuristics-based (non-learning) (Hoffart et al., 2011; Sakor et al., 2019; Ferragina and Scaiella, 2012) or learning-based (Mihalcea and Csomai, 2007; Cucerzan, 2007; Ratinov et al., 2011; Hoffart et al., 2012; Ganea and Hofmann, 2017), to link the mention to an actual entity. A particular type of entity linking, focused on short text (i.e., a single sentence or question), has attracted recent attention due to its relevance for downstream applications such as question answering (e.g., (Kapanipathi et al., 2021)) and conversational systems. Short"
2021.acl-long.64,N19-1243,0,0.0409557,"Missing"
2021.acl-long.64,2020.emnlp-main.519,0,0.130241,"as to how priors, local features (string similarity) and collective entity linking can be exploited to overcome the limited context in short-text EL. While the use of priors, local features and nonlocal features (for collective linking) has been proposed before (Ratinov et al., 2011), our goal in this paper is to provide an extensible framework that can combine any number of such features and more, including contextual embeddings such as BERT encodings (Devlin et al., 2019) and Query2box embeddings (Ren et al., 2020), and even the results of previously developed neural EL models (e.g., BLINK (Wu et al., 2020)). Additionally, such a framework must not only allow for easy inclusion of new sources of evidence but also for interpretability of the resulting model (Guidotti et al., 2018). An approach that combines disparate features should, at the very least, be able to state, post-training, which features are detrimental and which features aid EL performance and under what conditions, in order to enable actionable insights in the next iteration of model improvement. Our Approach. We propose to use rules in firstorder logic (FOL), an interpretable fragment of logic, as a glue to combine EL features into"
2021.acl-long.64,2020.emnlp-demos.4,0,0.0774561,"T and the inherent structure of the target KG, we incorporate an embeddingbased similarity by training a mini entity linking model without any aforementioned prior information. We first tag the input text T with a special token [MENT] to indicate the position of mention mi , and then encode T with BERT, i.e., mi = BERT(mi , T ). Each candidate eij is encoded with 6 spacy.io/usage/vectors-similarity Neighborhood Projection N (CCameron ) CTitanic BoxTitanic Figure 4: Candidates for linking the ‘Titanic’ mention appear in the intersection of the two boxes. a pre-trained graph embedding Wiki2Vec (Yamada et al., 2020), i.e., eij = Wiki2Vec(eij ). The candidates are ranked in order of the cosine similarity to mi , i.e., Simcos (mi , eij ). The mini EL model is optimized with margin ranking loss so that the correct candidate is ranked higher. BERT with Box Embeddings. While features such as Context (see Table 1) can exploit other mentions appearing within the same piece of text, they only do so via textual similarity. A more powerful method is to jointly disambiguate the mentions in text to the actual entities in the KG, thus exploiting the structural context in the KG. Intuitively, the simultaneous linking"
2021.findings-acl.339,2020.findings-emnlp.89,1,0.729463,"d OntoNotes roles but also AMR specific relations such as polarity or mode. As shown in Figure 1, AMR provides a representation that is fairly close to the KB representation. A special amr-unknown node, indicates the missing concept that represents the answer to the given question. In the example of Figure 1, amr-unknown is a person, who is the subject of act-01. Furthermore, AMR helps identify intermediate variables 3885 that behave as secondary unknowns. In this case, a movie produced by Benicio del Toro in Spain. NSQA utilizes a stack-Transformer transitionbased model (Naseem et al., 2019; Astudillo et al., 2020) for AMR parsing. An advantage of transition-based systems is that they provide explicit question text to AMR node alignments. This allows encoding closely integrated text and AMR input to multiple modules (Entity Linking and Relation Linking) that can benefit from this joint input. 2.2 AMR to KG Logic The core contribution of this work is our next step where the AMR of the question is transformed to a query graph aligned with the underlying knowledge graph. We formalize the two graphs as follows: AMR graph G is a rooted edge-labeled directed acyclic graph hVG , EG i. The edge set EG consists"
2021.findings-acl.339,W13-2322,0,0.0612546,"Missing"
2021.findings-acl.339,D13-1160,0,0.0607099,"f the football world cup 2018? Table 4: Question types supported by NSQA , with examples from QALD Falcon NMD+BLINK Dataset QALD-9 QALD-9 P 0.81 0.82 R 0.83 0.90 F1 0.82 0.85 Falcon NMD+BLINK LC-QuAD 1.0 LC-QuAD 1.0 0.56 0.87 0.69 0.86 0.62 0.86 Table 5: Performance of Entity Linking modules compared to SOTA Falcon on our dev sets we intend to explore more uses of such reasoners for KBQA in the future. 4 Related Work Early work in KBQA focused mainly on designing parsing algorithms and (synchronous) grammars to semantically parse input questions into KB queries (Zettlemoyer and Collins, 2007; Berant et al., 2013), with a few exceptions from the information extraction perspective that directly rely on relation detection (Yao and Van Durme, 2014; Bast and Haussmann, 2015). All the above approaches train statistical machine learning models based on human-crafted features and the performance is usually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to reduce the human efforts"
2021.findings-acl.339,P13-2131,0,0.0801353,"Missing"
2021.findings-acl.339,dorr-etal-1998-thematic,0,0.326306,"Missing"
2021.findings-acl.339,kingsbury-palmer-2002-treebank,0,0.0480491,"(AMR) graph; (ii) transforms the AMR graph to a set of candidate KB-aligned logical queries, via a novel but simple graph transformation approach; (iii) uses a Logical Neural Network (LNN) (Riegel et al., 2020) to reason over KB facts and produce answers to KB-aligned logical queries. We describe each of these modules in the following sections. 2.1 AMR Parsing NSQA utilizes AMR parsing to reduce the complexity and noise of natural language questions. An AMR parse is a rooted, directed, acyclic graph. AMR nodes represent concepts, which may include normalized surface symbols, Propbank frames (Kingsbury and Palmer, 2002) as well as other AMR-specific constructs to handle named entities, quantities, dates and other phenomena. Edges in an AMR graph represent the relations between concepts such as standard OntoNotes roles but also AMR specific relations such as polarity or mode. As shown in Figure 1, AMR provides a representation that is fairly close to the KB representation. A special amr-unknown node, indicates the missing concept that represents the answer to the given question. In the example of Figure 1, amr-unknown is a person, who is the subject of act-01. Furthermore, AMR helps identify intermediate vari"
2021.findings-acl.339,2020.findings-emnlp.288,1,0.824748,"Missing"
2021.findings-acl.339,P19-1451,1,0.842549,"cepts such as standard OntoNotes roles but also AMR specific relations such as polarity or mode. As shown in Figure 1, AMR provides a representation that is fairly close to the KB representation. A special amr-unknown node, indicates the missing concept that represents the answer to the given question. In the example of Figure 1, amr-unknown is a person, who is the subject of act-01. Furthermore, AMR helps identify intermediate variables 3885 that behave as secondary unknowns. In this case, a movie produced by Benicio del Toro in Spain. NSQA utilizes a stack-Transformer transitionbased model (Naseem et al., 2019; Astudillo et al., 2020) for AMR parsing. An advantage of transition-based systems is that they provide explicit question text to AMR node alignments. This allows encoding closely integrated text and AMR input to multiple modules (Entity Linking and Relation Linking) that can benefit from this joint input. 2.2 AMR to KG Logic The core contribution of this work is our next step where the AMR of the question is transformed to a query graph aligned with the underlying knowledge graph. We formalize the two graphs as follows: AMR graph G is a rooted edge-labeled directed acyclic graph hVG , EG i."
2021.findings-acl.339,N19-1243,0,0.0615569,"Missing"
2021.findings-acl.339,D07-1071,0,0.057132,"l start [sic] the final match of the football world cup 2018? Table 4: Question types supported by NSQA , with examples from QALD Falcon NMD+BLINK Dataset QALD-9 QALD-9 P 0.81 0.82 R 0.83 0.90 F1 0.82 0.85 Falcon NMD+BLINK LC-QuAD 1.0 LC-QuAD 1.0 0.56 0.87 0.69 0.86 0.62 0.86 Table 5: Performance of Entity Linking modules compared to SOTA Falcon on our dev sets we intend to explore more uses of such reasoners for KBQA in the future. 4 Related Work Early work in KBQA focused mainly on designing parsing algorithms and (synchronous) grammars to semantically parse input questions into KB queries (Zettlemoyer and Collins, 2007; Berant et al., 2013), with a few exceptions from the information extraction perspective that directly rely on relation detection (Yao and Van Durme, 2014; Bast and Haussmann, 2015). All the above approaches train statistical machine learning models based on human-crafted features and the performance is usually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to re"
2021.findings-acl.339,N19-1301,0,0.0169546,"proaches train statistical machine learning models based on human-crafted features and the performance is usually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to reduce the human efforts and feature engineering. This includes two most commonly adopted directions: (1) embedding-based approaches to make the pipeline end-to-end differentiable (Bordes et al., 2015; Xu et al., 2019); (2) hard-decision approaches that generate a sequence of actions that forms the subgraph (Xu et al., 2018; Bhutani et al., 2019). On domains with complex questions, like QALD and LC-QuAD, end-to-end approaches with harddecisions have also been developed. Some have primarily focused on generating SPARQL sketches (Maheshwari et al., 2019; Chen et al., 2020) where they evaluate these sketches (2-hop) by providing gold entities and ignoring the evaluation of selecting target variables or other aggregation functions like sorting and counting. (Zheng and Zhang, 2019) generates the question subgrap"
2021.findings-acl.339,D18-1110,1,0.837164,"ually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to reduce the human efforts and feature engineering. This includes two most commonly adopted directions: (1) embedding-based approaches to make the pipeline end-to-end differentiable (Bordes et al., 2015; Xu et al., 2019); (2) hard-decision approaches that generate a sequence of actions that forms the subgraph (Xu et al., 2018; Bhutani et al., 2019). On domains with complex questions, like QALD and LC-QuAD, end-to-end approaches with harddecisions have also been developed. Some have primarily focused on generating SPARQL sketches (Maheshwari et al., 2019; Chen et al., 2020) where they evaluate these sketches (2-hop) by providing gold entities and ignoring the evaluation of selecting target variables or other aggregation functions like sorting and counting. (Zheng and Zhang, 2019) generates the question subgraph via filling the entity and relationship slots of 12 predefined question template. Their performance on th"
2021.findings-acl.339,P14-1090,0,0.0858204,"Missing"
2021.findings-acl.339,P17-1053,1,0.839108,"rk in KBQA focused mainly on designing parsing algorithms and (synchronous) grammars to semantically parse input questions into KB queries (Zettlemoyer and Collins, 2007; Berant et al., 2013), with a few exceptions from the information extraction perspective that directly rely on relation detection (Yao and Van Durme, 2014; Bast and Haussmann, 2015). All the above approaches train statistical machine learning models based on human-crafted features and the performance is usually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to reduce the human efforts and feature engineering. This includes two most commonly adopted directions: (1) embedding-based approaches to make the pipeline end-to-end differentiable (Bordes et al., 2015; Xu et al., 2019); (2) hard-decision approaches that generate a sequence of actions that forms the subgraph (Xu et al., 2018; Bhutani et al., 2019). On domains with complex questions, like QALD and LC-QuAD, end-to-end approaches with harddecisions"
2021.findings-acl.400,P16-1231,0,0.0767969,"Missing"
2021.findings-acl.400,C96-1058,0,0.55687,"Missing"
2021.findings-acl.400,P06-1063,0,0.197721,"Missing"
2021.findings-acl.400,W04-2407,0,0.162118,"Missing"
2021.findings-acl.400,L16-1262,0,0.017048,"Missing"
2021.findings-acl.400,D10-1069,0,0.033946,"transactions, causing the system to misinterpret Neil’s insider transactions since 2011, to all his transactions. In Will it rain tomorrow by noon?, tomorrow is attached to rain with a wrong dependency relation (dobj instead of nmod:tmod), causing the system to miss the temporal aspect of the question. A natural solution to obtain accurate domainspecific parsers is to train them on domain-specific corpora. However, obtaining domain-specific questions is difficult. Moreover, annotating questions for parse trees is tedious, prone to errors and inconsistencies, and requires linguistic expertise. Petrov et al. (2010) proposed uptraining, training a parser on the output of a slower, more accurate parser. For acceptable performance, the unlabeled corpus must be large (100,000 questions). Our method is applicable when such a large corpus is not available. Inspired by (Wang et al., 2015) who showed that semantic parsers can be built “overnight” using domain expertise, we seek to reduce the effort required to handle a new domain using domain knowledge: (1) a domain schema modeling the concepts and relationships in a domain, and (2) a knowledge base of data instances that populate the schema (Hamon et al., 2017"
2021.findings-acl.400,silveira-etal-2014-gold,0,0.0706908,"Missing"
2021.findings-acl.400,P15-1129,0,0.0720707,"Missing"
2021.naacl-industry.28,N18-3010,1,0.807785,"Missing"
2021.naacl-industry.28,P10-1014,1,0.799832,"ure 1: TECUS’ Sub-Problems (see (IBM, b) for complete list of supported concepts) To overcome the above challenges, we designed and developed the Transparent and Expert Contract Understanding System (TECUS), a commercial system that enables legal professionals to review contracts with minimal effort. TECUS first models CU as a series of text classification and extraction tasks, defined collaboratively with SMEs, to capture the information that legal experts seek when reviewing contracts. Second, it leverages SystemT, a state-of-the-art declarative text understanding engine for the enterprise (Chiticariu et al., 2010, 2018), towards developing transparent models on top of syntactic and semantic linguistic features, to mitigate a possible lack of representative labeled data and to satisfy model stability requirements. This approach enables (1) the development of stable models that explicitly capture domain knowledge without requiring large amounts of labeled data or representative samples; and (2) a data science workflow that supports systematic error analysis and incorporation of user feedback towards continuous model improvement (Section 3). TECUS is available as part of multiple commercial products incl"
2021.naacl-industry.28,2020.acl-main.442,0,0.0265121,"than state-of-the-art black-box models (see Section 4 for more details). ation at finer granularity: across classes, per-class and per-LLE towards both quality and runtime performance. Such detailed model evaluation along with a systematic model improvement workflow together enable TECUS to provide reliable guarantees of consistency and robustness of its results. 3.2.3 Error Analysis While evaluation provides an overview of model performance, model improvement requires delving deeper and analyzing individual errors to understand their root causes and inform further model development efforts (Ribeiro et al., 2020). TECUS supports root cause identification of errors through the ModelLens error analysis tool shown in Figure 5 and the associated error analysis workflow (Katsis and Wolf, 2019). Specifically, ModelLens allows model developers to perform 3.2.2 Model Evaluation the following error analysis tasks: As the CU model in TECUS evolves over time, it (1) Acquire a high-level overview of the errors is regularly evaluated for: (1) quality, in terms of through a confusion matrix that depicts the types precision, recall and accuracy towards both Nature- of misclassifications made by the model, to help Pa"
2021.naacl-industry.28,P19-3023,1,0.833725,"ue to its challenging aspects. Attribute and Metadata extraction are performed using entity extraction, enabled by SystemT. Generalizability. LLEs are manually built by model developers on top of SRL9 , to explicitly 6 Here, we use PDF as the business document format for ease of exposition. The presented techniques apply also to other document formats, such as Microsoft Word. 7 SystemT also provides additional information, such as tense and voice; please refer to (Zhu et al., 2019) for details. 8 Simplified from the actual product LLEs for readability. 9 Potentially aided by machine learning (Sen et al., 2019) 225 Figure 5: Analyzing CU Errors through the ModelLens Error Analysis Tool capture domain knowledge. Each LLE reflects patterns from not only the documents used during the development process, but also unseen contracts where similar semantic patterns appear. As a result, the CU model generalizes much better to yet unseen contracts than state-of-the-art black-box models (see Section 4 for more details). ation at finer granularity: across classes, per-class and per-LLE towards both quality and runtime performance. Such detailed model evaluation along with a systematic model improvement workflo"
2021.naacl-industry.28,W19-3320,1,0.824818,"n Figure 1. TECUS uses declarative models towards both classification and extraction tasks 5 : 5 Here, we focus on classification due to its challenging aspects. Attribute and Metadata extraction are performed using entity extraction, enabled by SystemT. Generalizability. LLEs are manually built by model developers on top of SRL9 , to explicitly 6 Here, we use PDF as the business document format for ease of exposition. The presented techniques apply also to other document formats, such as Microsoft Word. 7 SystemT also provides additional information, such as tense and voice; please refer to (Zhu et al., 2019) for details. 8 Simplified from the actual product LLEs for readability. 9 Potentially aided by machine learning (Sen et al., 2019) 225 Figure 5: Analyzing CU Errors through the ModelLens Error Analysis Tool capture domain knowledge. Each LLE reflects patterns from not only the documents used during the development process, but also unseen contracts where similar semantic patterns appear. As a result, the CU model generalizes much better to yet unseen contracts than state-of-the-art black-box models (see Section 4 for more details). ation at finer granularity: across classes, per-class and per"
2021.naacl-tutorials.3,D17-1209,0,0.0598168,"Missing"
2021.naacl-tutorials.3,2020.findings-emnlp.255,1,0.779255,"e the tutorial self-contained. For trainees interested in reading important studies before the tutorial, we recommend the following papers regarding GNNs (Kipf and Welling, 2016; Li et al., 2015; Hamilton et al., 2017), automatic graph construction for NLP (Bastings et al., 2017; Chen et al., 2020b,a), joint text and knowledge representation learning (Feng et al., 2020; Lin et al., 2020), modeling directed graphs (Xu et al., 2018; Chen et al., 2020b) and heterogeneous graphs (Bastings et al., 2017; Chen et al., 2020c), and GNN based encoder-decoder models (Xu et al., 2018; Chen et al., 2020b; Li et al., 2020). 5 Diversity Yu Chen is a Research Scientist at Facebook AI. He got his PhD degree in Computer Science from Rensselaer Polytechnic Institute. His research interests lie at the intersection of Machine Learning (Deep Learning), and Natural Language Processing, with a particular emphasis on the fast-growing field of Graph Neural Networks and their applications in various domains. His work has been published in top-ranked conferences including but not limited to NeurIPS, ICML, ICLR, AAAI, IJCAI, NAACL, KDD, WSDM, ISWC, and AMIA. He was the recipient of the Best Student Paper Award of AAAI DLGMA’2"
2021.naacl-tutorials.3,2020.acl-main.713,1,0.712552,"https://sites.google.com/a/ email.wm.edu/teddy-lfwu/. IV. (20 minutes) Hands-on Demonstration 1. A Brief Overview of the Graph4NLP Library 2. Live Demo V. (10 minutes) Conclusion and Open Directions 4 Reading List We aim to make the tutorial self-contained. For trainees interested in reading important studies before the tutorial, we recommend the following papers regarding GNNs (Kipf and Welling, 2016; Li et al., 2015; Hamilton et al., 2017), automatic graph construction for NLP (Bastings et al., 2017; Chen et al., 2020b,a), joint text and knowledge representation learning (Feng et al., 2020; Lin et al., 2020), modeling directed graphs (Xu et al., 2018; Chen et al., 2020b) and heterogeneous graphs (Bastings et al., 2017; Chen et al., 2020c), and GNN based encoder-decoder models (Xu et al., 2018; Chen et al., 2020b; Li et al., 2020). 5 Diversity Yu Chen is a Research Scientist at Facebook AI. He got his PhD degree in Computer Science from Rensselaer Polytechnic Institute. His research interests lie at the intersection of Machine Learning (Deep Learning), and Natural Language Processing, with a particular emphasis on the fast-growing field of Graph Neural Networks and their applications in various do"
2021.repl4nlp-1.1,2020.emnlp-main.195,0,0.0711365,"Missing"
2021.repl4nlp-1.1,P19-1299,0,0.0823978,"ws the results of our comparison study. Sentiment Classification Finally, we evaluate sentiment classification task on Amazon multilingual reviews dataset (Prettenhofer and Stein, 2010). It contains positive and negative reviews from 3 domains, including DVD, Music and Books, in four languages: English (en), French (fr), German (de), and Japanese (ja). For each domain, there are 1,000 positive samples and 1,000 negative samples in each language for both training and testing. We choose the following baselines: translation baseline, UMM (Xu and Wan, 2017), CLDFA (Xu and Yang, 2017) and MAN-MoE (Chen et al., 2019). For the translation baseline, we translate the training and testing data for each target language into English using Watson Language Translator5 , and trained on the mBERT model, which is more 1 hi · hj + 1). score(i, j) = ( 2 khi k khj k We also investigate two other ways for scoring function: Euclidean-Distance based and the CORAL Function (Sun et al., 2016). While Cosine scoring function performs the best, so we report it in our main experiments and ignoring the other two. 3 es Pontiki et al. (2014)H Kumar et al. (2016)H Jebbara and Cimiano (2019) Agerri and Rigau (2019)H Evaluation We te"
2021.repl4nlp-1.1,Q18-1039,0,0.0591951,"Missing"
2021.repl4nlp-1.1,S16-1049,0,0.058756,"Missing"
2021.repl4nlp-1.1,2020.acl-main.747,0,0.091198,"Missing"
2021.repl4nlp-1.1,D18-1269,0,0.0663312,"Missing"
2021.repl4nlp-1.1,J82-2005,0,0.700201,"Missing"
2021.repl4nlp-1.1,N19-1423,0,0.0205863,"during training. Ideally, when deciding an instance weight, we should compare it with all instances from the target language. But doing so would incur prohibitively excessive computational resources. We thus approximate in small batches and calculate the weights by comparing how similar the instances are to the target ones within a small batch in each training step. Instance Weighting-based Gradient Descent Vanilla mini-batch gradient descent is defined as: 2.1 Pre-trained Models We compare two multilingual versions of pretrained models for the pre-trained models: multilingual BERT (mBERT)1 (Devlin et al., 2019) and XLM-Roberta (XLMR)2 (Conneau et al., 2020). We evaluate on multiple tasks in Section 3, so there are different ways to utilize the pre-trained models. For the sentiment and document classification task, we train a fully-connected layer on top of the output of the [CLS] token, which is considered to be the representation of the input sequence. For the opinion target extraction task, we formulate it as sequence labeling task (Agerri and Rigau, 2019; Jebbara and Cimiano, 2019). To extract such opinion target tokens is to classify each token into one of the following: Beginning, Inside and Ou"
2021.repl4nlp-1.1,N16-1083,0,0.0633704,"Missing"
2021.repl4nlp-1.1,D18-1498,0,0.0393052,"Missing"
2021.repl4nlp-1.1,N19-1257,0,0.0164374,"d Models We compare two multilingual versions of pretrained models for the pre-trained models: multilingual BERT (mBERT)1 (Devlin et al., 2019) and XLM-Roberta (XLMR)2 (Conneau et al., 2020). We evaluate on multiple tasks in Section 3, so there are different ways to utilize the pre-trained models. For the sentiment and document classification task, we train a fully-connected layer on top of the output of the [CLS] token, which is considered to be the representation of the input sequence. For the opinion target extraction task, we formulate it as sequence labeling task (Agerri and Rigau, 2019; Jebbara and Cimiano, 2019). To extract such opinion target tokens is to classify each token into one of the following: Beginning, Inside and Outside of an aspect. We follow a typical IOB scheme for the task (Toh and Wang, 2014; San Vicente ´ et al., 2015; Alvarez-L´ opez et al., 2016). In this case, each token should have a label, so we have a fully-connected layer that is shared for each token. We note that it may be possible to improve all the results even further by employing more powerful task layers and modules such as conditional random fields (Lafferty et al., 2001), but keep things relatively simple since our m"
2021.repl4nlp-1.1,S14-2004,0,0.0338226,"selines: translation baseline, UMM (Xu and Wan, 2017), CLDFA (Xu and Yang, 2017) and MAN-MoE (Chen et al., 2019). For the translation baseline, we translate the training and testing data for each target language into English using Watson Language Translator5 , and trained on the mBERT model, which is more 1 hi · hj + 1). score(i, j) = ( 2 khi k khj k We also investigate two other ways for scoring function: Euclidean-Distance based and the CORAL Function (Sun et al., 2016). While Cosine scoring function performs the best, so we report it in our main experiments and ignoring the other two. 3 es Pontiki et al. (2014)H Kumar et al. (2016)H Jebbara and Cimiano (2019) Agerri and Rigau (2019)H Evaluation We test on three tasks: opinion target extraction, document classification, and sentiment classification 3 . English is the source language for all the experiments. We evaluate four settings: 1) direct adaptation with mBERT-base (mBERT), 2) mBERT with Instance Weighting (mBERT+IW), 3) direct adaption of XLMR-base (XLMR), and 4) XLMR with Instance Weighting (XLMR+IW). Opinion Target Extraction We choose SemEval 2016 Workshop Task 5 (Pontiki et al., 2016) for opinion target extraction. It includes restaurant re"
2021.repl4nlp-1.1,P10-1114,0,0.0559969,"a strong baseline (Schwenk and Li, 2018), which applies pre-trained MultiCCA word embeddings (Ammar et al., 2016) and then trained in a supervised way. Another baseline is a zero-shot method proposed by Artetxe and Schwenk (2019), which applies a single BiLSTM encoder with a shared vocabulary among all languages, and a decoder trained with parallel corpora. Artetxe and Schwenk (2019) apply mBERT as a zero-shot language transfer. Table 2 shows the results of our comparison study. Sentiment Classification Finally, we evaluate sentiment classification task on Amazon multilingual reviews dataset (Prettenhofer and Stein, 2010). It contains positive and negative reviews from 3 domains, including DVD, Music and Books, in four languages: English (en), French (fr), German (de), and Japanese (ja). For each domain, there are 1,000 positive samples and 1,000 negative samples in each language for both training and testing. We choose the following baselines: translation baseline, UMM (Xu and Wan, 2017), CLDFA (Xu and Yang, 2017) and MAN-MoE (Chen et al., 2019). For the translation baseline, we translate the training and testing data for each target language into English using Watson Language Translator5 , and trained on the"
2021.repl4nlp-1.1,S16-1174,0,0.024419,"seline, UMM (Xu and Wan, 2017), CLDFA (Xu and Yang, 2017) and MAN-MoE (Chen et al., 2019). For the translation baseline, we translate the training and testing data for each target language into English using Watson Language Translator5 , and trained on the mBERT model, which is more 1 hi · hj + 1). score(i, j) = ( 2 khi k khj k We also investigate two other ways for scoring function: Euclidean-Distance based and the CORAL Function (Sun et al., 2016). While Cosine scoring function performs the best, so we report it in our main experiments and ignoring the other two. 3 es Pontiki et al. (2014)H Kumar et al. (2016)H Jebbara and Cimiano (2019) Agerri and Rigau (2019)H Evaluation We test on three tasks: opinion target extraction, document classification, and sentiment classification 3 . English is the source language for all the experiments. We evaluate four settings: 1) direct adaptation with mBERT-base (mBERT), 2) mBERT with Instance Weighting (mBERT+IW), 3) direct adaption of XLMR-base (XLMR), and 4) XLMR with Instance Weighting (XLMR+IW). Opinion Target Extraction We choose SemEval 2016 Workshop Task 5 (Pontiki et al., 2016) for opinion target extraction. It includes restaurant reviews in five languag"
2021.repl4nlp-1.1,S15-2127,0,0.0643988,"Missing"
2021.repl4nlp-1.1,C16-1038,0,0.0188132,"omain, there are 1,000 positive samples and 1,000 negative samples in each language for both training and testing. We choose the following baselines: translation baseline, UMM (Xu and Wan, 2017), CLDFA (Xu and Yang, 2017) and MAN-MoE (Chen et al., 2019). For the translation baseline, we translate the training and testing data for each target language into English using Watson Language Translator5 , and trained on the mBERT model, which is more 1 hi · hj + 1). score(i, j) = ( 2 khi k khj k We also investigate two other ways for scoring function: Euclidean-Distance based and the CORAL Function (Sun et al., 2016). While Cosine scoring function performs the best, so we report it in our main experiments and ignoring the other two. 3 es Pontiki et al. (2014)H Kumar et al. (2016)H Jebbara and Cimiano (2019) Agerri and Rigau (2019)H Evaluation We test on three tasks: opinion target extraction, document classification, and sentiment classification 3 . English is the source language for all the experiments. We evaluate four settings: 1) direct adaptation with mBERT-base (mBERT), 2) mBERT with Instance Weighting (mBERT+IW), 3) direct adaption of XLMR-base (XLMR), and 4) XLMR with Instance Weighting (XLMR+IW)."
2021.repl4nlp-1.1,S14-2038,0,0.0182931,"ultiple tasks in Section 3, so there are different ways to utilize the pre-trained models. For the sentiment and document classification task, we train a fully-connected layer on top of the output of the [CLS] token, which is considered to be the representation of the input sequence. For the opinion target extraction task, we formulate it as sequence labeling task (Agerri and Rigau, 2019; Jebbara and Cimiano, 2019). To extract such opinion target tokens is to classify each token into one of the following: Beginning, Inside and Outside of an aspect. We follow a typical IOB scheme for the task (Toh and Wang, 2014; San Vicente ´ et al., 2015; Alvarez-L´ opez et al., 2016). In this case, each token should have a label, so we have a fully-connected layer that is shared for each token. We note that it may be possible to improve all the results even further by employing more powerful task layers and modules such as conditional random fields (Lafferty et al., 2001), but keep things relatively simple since our main goal is to evaluate instance weighting with zero-shot CLTC. θ ←θ−α k X ∇θ f (yi , gθ (xi )) (1) i=1 where α is the learning rate, θ is the parameter that we want to update, gθ (xi ) is the model p"
2021.repl4nlp-1.1,D17-1155,0,0.0653529,"Missing"
2021.repl4nlp-1.1,D19-1077,0,0.0466328,"Missing"
2021.repl4nlp-1.1,D17-1053,0,0.166397,"apply mBERT as a zero-shot language transfer. Table 2 shows the results of our comparison study. Sentiment Classification Finally, we evaluate sentiment classification task on Amazon multilingual reviews dataset (Prettenhofer and Stein, 2010). It contains positive and negative reviews from 3 domains, including DVD, Music and Books, in four languages: English (en), French (fr), German (de), and Japanese (ja). For each domain, there are 1,000 positive samples and 1,000 negative samples in each language for both training and testing. We choose the following baselines: translation baseline, UMM (Xu and Wan, 2017), CLDFA (Xu and Yang, 2017) and MAN-MoE (Chen et al., 2019). For the translation baseline, we translate the training and testing data for each target language into English using Watson Language Translator5 , and trained on the mBERT model, which is more 1 hi · hj + 1). score(i, j) = ( 2 khi k khj k We also investigate two other ways for scoring function: Euclidean-Distance based and the CORAL Function (Sun et al., 2016). While Cosine scoring function performs the best, so we report it in our main experiments and ignoring the other two. 3 es Pontiki et al. (2014)H Kumar et al. (2016)H Jebbara a"
2021.repl4nlp-1.1,P17-1130,0,0.123322,"t language transfer. Table 2 shows the results of our comparison study. Sentiment Classification Finally, we evaluate sentiment classification task on Amazon multilingual reviews dataset (Prettenhofer and Stein, 2010). It contains positive and negative reviews from 3 domains, including DVD, Music and Books, in four languages: English (en), French (fr), German (de), and Japanese (ja). For each domain, there are 1,000 positive samples and 1,000 negative samples in each language for both training and testing. We choose the following baselines: translation baseline, UMM (Xu and Wan, 2017), CLDFA (Xu and Yang, 2017) and MAN-MoE (Chen et al., 2019). For the translation baseline, we translate the training and testing data for each target language into English using Watson Language Translator5 , and trained on the mBERT model, which is more 1 hi · hj + 1). score(i, j) = ( 2 khi k khj k We also investigate two other ways for scoring function: Euclidean-Distance based and the CORAL Function (Sun et al., 2016). While Cosine scoring function performs the best, so we report it in our main experiments and ignoring the other two. 3 es Pontiki et al. (2014)H Kumar et al. (2016)H Jebbara and Cimiano (2019) Agerri an"
C16-1058,W09-1206,0,0.12668,"Missing"
C16-1058,bonial-etal-2014-propbank,0,0.0416624,"Missing"
C16-1058,W09-1207,0,0.0594337,"with n-grams (Katz, 1987): If insufficient training data exists, such models commonly backoff to lower histories (for instance, a 3-gram model may back off to a 2-gram language model). The six distance values assigned to composite features in Table 3 may be interpreted in a similar spirit since our approach broadens the search to nearest neighbors with less specific composite features if insufficient training data exists. 2.4 Easy-First Argument Labeling While argument labeling decisions are made locally, each core semantic role (labels A0 through A5) may only be assigned once per predicate (Che et al., 2009). To include this global constraint, we use a greedy approach in which already assigned core labels are removed from consideration for the remaining predictions. Our approach follows an easy-first philosophy (Goldberg and Elhadad, 2010) where clas3 For example, while active subjects are most commonly labeled A0 (agent) of a verb (”the dog ate”, ”the bird sang”), they are typically the A1 (theme) of non-agentive frames (”the wound festered”). 603 Algorithm 2 Easy-First Argument Labeling for each predicate p ∈ Unknown Sample do A←∅ C ← Candidate arguments of p, their labels and confidence value"
C16-1058,W11-0906,0,0.112382,"labels (see Figure 1 for examples). Frame labels disambiguate the predicate meaning in the context of the sentence. Role labels roughly correspond to simple questions (who, when, how, why, with whom) with regards to the disambiguated predicate. SRL has been found useful for a wide range of applications such as information extraction (Fader et al., 2011), question answering (Shen and Lapata, 2007; Maqsud et al., 2014) and machine translation (Lo et al., 2013). Current state-of-the-art SRL approaches train classifiers with bags of features (Johansson and Nugues, 2008; Bj¨orkelund et al., 2009; Choi and Palmer, 2011) to predict semantic labels for each constituent in a sentence. These approaches typically employ classifiers such as logistic regression or SVM that learn for each feature a measure of impact on the classification decision and abstract away from local contexts in specific training examples. Local bias. We argue that such approaches are not ideal for SRL due to a strong local bias of features within specific contexts. Low-frequency examples in SRL are often not noise to be abstracted away, but rather correspond to exceptions that require explicit handling. For example, consider the task of arg"
C16-1058,W99-0707,0,0.137281,"erplay of features explicit, they can be rendered as human readable statements. Classification decisions using such features can be easily interpretable for error analysis and extension. Instance-based Learning for SRL Based on these observations, we propose to use instance-based learning (Aha et al., 1991; Daelemans and Van den Bosch, 2005) for SRL. Such learning does not abstract away from specific feature contexts, but rather considers the overall similarity of a test instance to instances in the training data. It has been shown to be applicable to a range of NLP tasks such as PoS tagging (Daelemans et al., 1999), dependency parsing (Nivre et al., 2004) and word sense disambiguation (Veenstra et al., 2000). The arguably most well-known approach of this kind is k-nearest neighbors classification (kNN) in which the class label is determined as the majority label of the k most similar training examples (Cover and Hart, 1967). We propose to identify nearest neighbors using composite features, i.e. instances that share the most similar combination of atomic features. We use a function to assign to each composite of atomic features a discrete distance value, effectively rendering the search for nearest neig"
C16-1058,D11-1142,0,0.0810261,"of annotating predicate-argument structures in sentences with shallow semantic information. One prominent labeling scheme for the English language is the Proposition Bank (Palmer et al., 2005), which annotates predicates with frame labels and arguments with role labels (see Figure 1 for examples). Frame labels disambiguate the predicate meaning in the context of the sentence. Role labels roughly correspond to simple questions (who, when, how, why, with whom) with regards to the disambiguated predicate. SRL has been found useful for a wide range of applications such as information extraction (Fader et al., 2011), question answering (Shen and Lapata, 2007; Maqsud et al., 2014) and machine translation (Lo et al., 2013). Current state-of-the-art SRL approaches train classifiers with bags of features (Johansson and Nugues, 2008; Bj¨orkelund et al., 2009; Choi and Palmer, 2011) to predict semantic labels for each constituent in a sentence. These approaches typically employ classifiers such as logistic regression or SVM that learn for each feature a measure of impact on the classification decision and abstract away from local contexts in specific training examples. Local bias. We argue that such approaches"
C16-1058,N10-1115,0,0.0279829,"osite features in Table 3 may be interpreted in a similar spirit since our approach broadens the search to nearest neighbors with less specific composite features if insufficient training data exists. 2.4 Easy-First Argument Labeling While argument labeling decisions are made locally, each core semantic role (labels A0 through A5) may only be assigned once per predicate (Che et al., 2009). To include this global constraint, we use a greedy approach in which already assigned core labels are removed from consideration for the remaining predictions. Our approach follows an easy-first philosophy (Goldberg and Elhadad, 2010) where clas3 For example, while active subjects are most commonly labeled A0 (agent) of a verb (”the dog ate”, ”the bird sang”), they are typically the A1 (theme) of non-agentive frames (”the wound festered”). 603 Algorithm 2 Easy-First Argument Labeling for each predicate p ∈ Unknown Sample do A←∅ C ← Candidate arguments of p, their labels and confidence value in sorted order by confidence for c ∈ C with the highest confidence value in C do Remove c from C if Label of c ∈ / Set of labels in A then Add c to A end if end for end for sifications for all predicate arguments are ordered by confide"
C16-1058,W09-1201,0,0.0288135,"Missing"
C16-1058,D08-1008,0,0.122046,"s predicates with frame labels and arguments with role labels (see Figure 1 for examples). Frame labels disambiguate the predicate meaning in the context of the sentence. Role labels roughly correspond to simple questions (who, when, how, why, with whom) with regards to the disambiguated predicate. SRL has been found useful for a wide range of applications such as information extraction (Fader et al., 2011), question answering (Shen and Lapata, 2007; Maqsud et al., 2014) and machine translation (Lo et al., 2013). Current state-of-the-art SRL approaches train classifiers with bags of features (Johansson and Nugues, 2008; Bj¨orkelund et al., 2009; Choi and Palmer, 2011) to predict semantic labels for each constituent in a sentence. These approaches typically employ classifiers such as logistic regression or SVM that learn for each feature a measure of impact on the classification decision and abstract away from local contexts in specific training examples. Local bias. We argue that such approaches are not ideal for SRL due to a strong local bias of features within specific contexts. Low-frequency examples in SRL are often not noise to be abstracted away, but rather correspond to exceptions that require explic"
C16-1058,2013.iwslt-evaluation.5,0,0.0607808,"eling scheme for the English language is the Proposition Bank (Palmer et al., 2005), which annotates predicates with frame labels and arguments with role labels (see Figure 1 for examples). Frame labels disambiguate the predicate meaning in the context of the sentence. Role labels roughly correspond to simple questions (who, when, how, why, with whom) with regards to the disambiguated predicate. SRL has been found useful for a wide range of applications such as information extraction (Fader et al., 2011), question answering (Shen and Lapata, 2007; Maqsud et al., 2014) and machine translation (Lo et al., 2013). Current state-of-the-art SRL approaches train classifiers with bags of features (Johansson and Nugues, 2008; Bj¨orkelund et al., 2009; Choi and Palmer, 2011) to predict semantic labels for each constituent in a sentence. These approaches typically employ classifiers such as logistic regression or SVM that learn for each feature a measure of impact on the classification decision and abstract away from local contexts in specific training examples. Local bias. We argue that such approaches are not ideal for SRL due to a strong local bias of features within specific contexts. Low-frequency examp"
C16-1058,C14-2018,1,0.863765,"Missing"
C16-1058,W04-2407,0,0.0434599,"ered as human readable statements. Classification decisions using such features can be easily interpretable for error analysis and extension. Instance-based Learning for SRL Based on these observations, we propose to use instance-based learning (Aha et al., 1991; Daelemans and Van den Bosch, 2005) for SRL. Such learning does not abstract away from specific feature contexts, but rather considers the overall similarity of a test instance to instances in the training data. It has been shown to be applicable to a range of NLP tasks such as PoS tagging (Daelemans et al., 1999), dependency parsing (Nivre et al., 2004) and word sense disambiguation (Veenstra et al., 2000). The arguably most well-known approach of this kind is k-nearest neighbors classification (kNN) in which the class label is determined as the majority label of the k most similar training examples (Cover and Hart, 1967). We propose to identify nearest neighbors using composite features, i.e. instances that share the most similar combination of atomic features. We use a function to assign to each composite of atomic features a discrete distance value, effectively rendering the search for nearest neighbors as a search within a Parzen window"
C16-1058,J05-1004,0,0.848028,"g. For example, consider the task of argument labeling: Arguments that are syntactically realized as passive subjects are typically labeled A11 . However, there exist numerous low-frequency exceptions to this rule. For instance, passive subjects of certain frames (such as the frame TELL .01) are most commonly labeled A2. See Figure 1 for an example. Other examples of local bias include different types of diathesis alternation which affect specific frames and argument types (Kipper et al., 2008), the syntactic realization of higher order roles (A2 to A5) which is highly irregular among frames (Palmer et al., 2005), and the realization of roles in non-agentive frames. These phenomena are observed only in specific and often low-frequency contexts of composite features, but are highly relevant to SRL. 1 This corresponds to the linguistic intuition that active subjects are commonly thematic agents, while direct objects and passive subjects are most commonly the thematic patient or theme of a frame (van der Plas et al., 2014) 599 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 599–608, Osaka, Japan, December 11-17 2016. a) Passive subject S"
C16-1058,W97-0209,0,0.420252,"nt contexts are important. To assess the importance of individual features in their contexts, we run ablation tests in which we remove individual atomic features from composites, as summarized in Tables 4 and 5. Specifically, removing the frame feature F from argument labeling (KSRL(−F ) )), which causes all argument labeling predictions to be made without frame-specific contests, leads to the most significant decrease on F1 scores (↓2.5 pp and ↓1 pp) in our ablation tests. Omitting argument head lemma feature AL (K-SRL(−AL) ), the only feature capturing argument-level selectional preference (Resnik, 1997) in our approach, results in evident reduction on F1 scores (↓0.8 pp and ↓0.5 pp). Meanwhile, the removal of frame class feature (K-SRL−F C) impacts only the out-of-domain scenario slightly (↓0.3 pp). This observation indicates that small neighborhoods with the frame feature often suffice to capture exceptions for non-agentive verbs. Relative frequencies measures confidence. We assess our interpretation of relative frequencies in the nearest neighborhood as a measure of confidence by running a parameter sweep over θ. The results are depicted in Figures 7 and 8. As can be seen, precision improv"
C16-1058,P16-1113,0,0.204325,"confidence for assigning labels. 3.1 Experimental Setup We use the benchmark data sets from the CoNLL-2009 shared task (Hajiˇc et al., 2009) and compare our results against the top two scoring systems of the CoNLL-2009 shared task as well as two recent stateof-the-art systems: 1) C HEN (Zhao et al., 2009), which uses maximum entropy classifiers. 2) C HE (Che et al., 2009), which uses SVM classifiers. 3) M ATEPLUS (Roth and Woodsend, 2014a), a state-of-the-art extension of a previous system (Bj¨orkelund et al., 2009) that uses logistic regression classifiers and word embeddings. 4) PATH LSTM (Roth and Lapata, 2016), the current state-of-the-art which uses logistic regression classifiers for predicates and neural network models with word embeddings for arguments. Our default settings for K-SRL are k = 3 and confidence threshold θ = 0, both determined through experimentation. For both settings, we present parameter sweep experiments. We compute the precision, recall and F1 to measure the quality of the systems. In our study, we focus on verbal predicates and their roles, which we evaluate using the scoring metric of the CoNLL-2009 shared task. We recomputed the measures for the previous state-of-art syste"
C16-1058,D14-1045,0,0.238207,"ding the minimum support variable k, different components in composite features, and our interpretation of relative label frequencies in the nearest neighborhood as an indication of confidence for assigning labels. 3.1 Experimental Setup We use the benchmark data sets from the CoNLL-2009 shared task (Hajiˇc et al., 2009) and compare our results against the top two scoring systems of the CoNLL-2009 shared task as well as two recent stateof-the-art systems: 1) C HEN (Zhao et al., 2009), which uses maximum entropy classifiers. 2) C HE (Che et al., 2009), which uses SVM classifiers. 3) M ATEPLUS (Roth and Woodsend, 2014a), a state-of-the-art extension of a previous system (Bj¨orkelund et al., 2009) that uses logistic regression classifiers and word embeddings. 4) PATH LSTM (Roth and Lapata, 2016), the current state-of-the-art which uses logistic regression classifiers for predicates and neural network models with word embeddings for arguments. Our default settings for K-SRL are k = 3 and confidence threshold θ = 0, both determined through experimentation. For both settings, we present parameter sweep experiments. We compute the precision, recall and F1 to measure the quality of the systems. In our study, we"
C16-1058,D07-1002,0,0.0682164,"res in sentences with shallow semantic information. One prominent labeling scheme for the English language is the Proposition Bank (Palmer et al., 2005), which annotates predicates with frame labels and arguments with role labels (see Figure 1 for examples). Frame labels disambiguate the predicate meaning in the context of the sentence. Role labels roughly correspond to simple questions (who, when, how, why, with whom) with regards to the disambiguated predicate. SRL has been found useful for a wide range of applications such as information extraction (Fader et al., 2011), question answering (Shen and Lapata, 2007; Maqsud et al., 2014) and machine translation (Lo et al., 2013). Current state-of-the-art SRL approaches train classifiers with bags of features (Johansson and Nugues, 2008; Bj¨orkelund et al., 2009; Choi and Palmer, 2011) to predict semantic labels for each constituent in a sentence. These approaches typically employ classifiers such as logistic regression or SVM that learn for each feature a measure of impact on the classification decision and abstract away from local contexts in specific training examples. Local bias. We argue that such approaches are not ideal for SRL due to a strong loca"
C16-1058,J08-2002,0,0.0155258,"hese observations confirm our initial conjecture that SRL is affected by a strong local bias and that a small nearest neighborhood suffices to make high quality predictions. Global constraints improve argument labeling. We run an ablation test in which we make only local predictions without modeling global constraints as described in Section 2.4, which reduces the F1 -score by .6 and .8 percentage points respectively on in-domain and out-of-domain data (see K-SRLlocal in Tables 4 and 5). These results are in line with previous evaluations on the impact of modeling global argument constraints (Toutanova et al., 2008; Roth and Lapata, 2016). Frame and argument contexts are important. To assess the importance of individual features in their contexts, we run ablation tests in which we remove individual atomic features from composites, as summarized in Tables 4 and 5. Specifically, removing the frame feature F from argument labeling (KSRL(−F ) )), which causes all argument labeling predictions to be made without frame-specific contests, leads to the most significant decrease on F1 scores (↓2.5 pp and ↓1 pp) in our ablation tests. Omitting argument head lemma feature AL (K-SRL(−AL) ), the only feature capturi"
C16-1058,C14-1121,0,0.0251913,"Missing"
C16-1058,W09-1208,0,0.0258521,"nst previously published state-of-the-art systems. We also examine how different parameters of K-SRL impact its performance, including the minimum support variable k, different components in composite features, and our interpretation of relative label frequencies in the nearest neighborhood as an indication of confidence for assigning labels. 3.1 Experimental Setup We use the benchmark data sets from the CoNLL-2009 shared task (Hajiˇc et al., 2009) and compare our results against the top two scoring systems of the CoNLL-2009 shared task as well as two recent stateof-the-art systems: 1) C HEN (Zhao et al., 2009), which uses maximum entropy classifiers. 2) C HE (Che et al., 2009), which uses SVM classifiers. 3) M ATEPLUS (Roth and Woodsend, 2014a), a state-of-the-art extension of a previous system (Bj¨orkelund et al., 2009) that uses logistic regression classifiers and word embeddings. 4) PATH LSTM (Roth and Lapata, 2016), the current state-of-the-art which uses logistic regression classifiers for predicates and neural network models with word embeddings for arguments. Our default settings for K-SRL are k = 3 and confidence threshold θ = 0, both determined through experimentation. For both settings, w"
C16-1327,P16-4001,1,0.841749,"nnotation projection to automatically generate Proposition Banks from parallel corpora for new target languages (TL) (Pad´o and Lapata, 2009; Van der Plas et al., 2011; Akbik et al., 2015). This approach requires a large word-aligned corpus of English sentences and their TL translations. An English SRL system predicts semantic labels for the English sentences. These labels are then transferred along word alignments to automatically annotate the TL corpus. Recent work has shown that such auto-generated Proposition Banks can be used to train semantic role labelers for a wide range of languages (Akbik and Li, 2016). Heuristically generated frame lexicon. However, a major drawback of such approaches is that the frame lexicon is heuristically produced from available alignments, which leads to a range of errors and inconsistencies. Consider the German verb drehen. In the English-German portion of the O PEN S UB 3466 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3466–3474, Osaka, Japan, December 11-17 2016. Predicate: drehen Original drehen.01 TURN.01 Step 1: Filter drehen.01 TURN.01 motion in a new direction A0: turner A1: thing turning"
C16-1327,P15-1039,1,0.892028,"Missing"
C16-1327,bonial-etal-2014-propbank,0,0.404696,"se resources to the research community. 1 Introduction Semantic role labeling (SRL) is the task of labeling predicate-argument structure in sentences with shallow semantic information. The prominent labeling scheme for the English language is the Proposition Bank (Palmer et al., 2005), which provides a lexicon of possible frames for English verbs. Each frame corresponds to one semantic interpretation and comes with frame-specific role labels and descriptions. Over the past decade, large amounts of text data have been annotated based on these guidelines (Palmer et al., 2005; Hovy et al., 2006; Bonial et al., 2014). They enable the training of statistical SRL systems, which have proven useful for downstream applications such as information extraction (IE) (Fader et al., 2011), question answering (QA) (Shen and Lapata, 2007; Maqsud et al., 2014) and machine translation (Lo et al., 2013). However, such manual efforts are known to be highly costly. Possible frames need to be manually determined, their roles individually described, and large volumes of text data annotated accordingly. For this reason, Proposition Banks do not exist for most languages. Annotation projection. Recent research has explored the"
C16-1327,D11-1142,0,0.0304593,"mantic information. The prominent labeling scheme for the English language is the Proposition Bank (Palmer et al., 2005), which provides a lexicon of possible frames for English verbs. Each frame corresponds to one semantic interpretation and comes with frame-specific role labels and descriptions. Over the past decade, large amounts of text data have been annotated based on these guidelines (Palmer et al., 2005; Hovy et al., 2006; Bonial et al., 2014). They enable the training of statistical SRL systems, which have proven useful for downstream applications such as information extraction (IE) (Fader et al., 2011), question answering (QA) (Shen and Lapata, 2007; Maqsud et al., 2014) and machine translation (Lo et al., 2013). However, such manual efforts are known to be highly costly. Possible frames need to be manually determined, their roles individually described, and large volumes of text data annotated accordingly. For this reason, Proposition Banks do not exist for most languages. Annotation projection. Recent research has explored the possibility of using annotation projection to automatically generate Proposition Banks from parallel corpora for new target languages (TL) (Pad´o and Lapata, 2009;"
C16-1327,N06-2015,0,0.0602137,"nks. We release these resources to the research community. 1 Introduction Semantic role labeling (SRL) is the task of labeling predicate-argument structure in sentences with shallow semantic information. The prominent labeling scheme for the English language is the Proposition Bank (Palmer et al., 2005), which provides a lexicon of possible frames for English verbs. Each frame corresponds to one semantic interpretation and comes with frame-specific role labels and descriptions. Over the past decade, large amounts of text data have been annotated based on these guidelines (Palmer et al., 2005; Hovy et al., 2006; Bonial et al., 2014). They enable the training of statistical SRL systems, which have proven useful for downstream applications such as information extraction (IE) (Fader et al., 2011), question answering (QA) (Shen and Lapata, 2007; Maqsud et al., 2014) and machine translation (Lo et al., 2013). However, such manual efforts are known to be highly costly. Possible frames need to be manually determined, their roles individually described, and large volumes of text data annotated accordingly. For this reason, Proposition Banks do not exist for most languages. Annotation projection. Recent rese"
C16-1327,N15-2005,0,0.0391553,"Missing"
C16-1327,L16-1147,0,0.186635,"thing turning motion in a new direction A0: turner A1: thing turning drehen.02 Step 2: Merge SHOOT.03 FILM.01 record on film A0: recorder A1: thing recorded, filmed drehen.03 record on film A0: videographer A1: subject filmed A2: medium SHOOT.03 record on film A0: videographer A1: subject filmed A2: medium FILM.01 drehen.02 SHOOT.03 record on film A0: recorder A1: thing recorded, filmed Figure 1: Illustration of merging and filtering steps over heuristically produced frame lexicon. This process reduces the number of distinct frames for the verb drehen from 4 to 2. TITLES 2016 parallel corpus (Lison and Tiedemann, 2016), this verb is aligned to many different English frames, four of which are illustrated in Figure 1. Previous annotation projection approaches treat each alignment as a separate and valid frame of the TL verb. In this example, it is therefore heuristically determined that drehen may evoke four separate frames, namely TURN .01, TURN .02, FILM .01 and SHOOT.03. See Figure 1 (left pane) for an illustration and explanation of the four frames. The example illustrates the two main problems in heuristically produced lexicons: Incorrect frames The first problem is posed by errors in the frame lexicon."
C16-1327,2013.iwslt-evaluation.5,0,0.0240687,"2005), which provides a lexicon of possible frames for English verbs. Each frame corresponds to one semantic interpretation and comes with frame-specific role labels and descriptions. Over the past decade, large amounts of text data have been annotated based on these guidelines (Palmer et al., 2005; Hovy et al., 2006; Bonial et al., 2014). They enable the training of statistical SRL systems, which have proven useful for downstream applications such as information extraction (IE) (Fader et al., 2011), question answering (QA) (Shen and Lapata, 2007; Maqsud et al., 2014) and machine translation (Lo et al., 2013). However, such manual efforts are known to be highly costly. Possible frames need to be manually determined, their roles individually described, and large volumes of text data annotated accordingly. For this reason, Proposition Banks do not exist for most languages. Annotation projection. Recent research has explored the possibility of using annotation projection to automatically generate Proposition Banks from parallel corpora for new target languages (TL) (Pad´o and Lapata, 2009; Van der Plas et al., 2011; Akbik et al., 2015). This approach requires a large word-aligned corpus of English se"
C16-1327,C14-2018,1,0.90424,"Missing"
C16-1327,J05-1004,0,0.519495,"target languages and apply the new lexicons to automatically generate large Proposition Banks for Chinese, French and German with manually curated frames. We present a detailed evaluation in which we find that our proposed approach significantly increases the quality and consistency of the generated Proposition Banks. We release these resources to the research community. 1 Introduction Semantic role labeling (SRL) is the task of labeling predicate-argument structure in sentences with shallow semantic information. The prominent labeling scheme for the English language is the Proposition Bank (Palmer et al., 2005), which provides a lexicon of possible frames for English verbs. Each frame corresponds to one semantic interpretation and comes with frame-specific role labels and descriptions. Over the past decade, large amounts of text data have been annotated based on these guidelines (Palmer et al., 2005; Hovy et al., 2006; Bonial et al., 2014). They enable the training of statistical SRL systems, which have proven useful for downstream applications such as information extraction (IE) (Fader et al., 2011), question answering (QA) (Shen and Lapata, 2007; Maqsud et al., 2014) and machine translation (Lo et"
C16-1327,D07-1002,0,0.0268625,"eme for the English language is the Proposition Bank (Palmer et al., 2005), which provides a lexicon of possible frames for English verbs. Each frame corresponds to one semantic interpretation and comes with frame-specific role labels and descriptions. Over the past decade, large amounts of text data have been annotated based on these guidelines (Palmer et al., 2005; Hovy et al., 2006; Bonial et al., 2014). They enable the training of statistical SRL systems, which have proven useful for downstream applications such as information extraction (IE) (Fader et al., 2011), question answering (QA) (Shen and Lapata, 2007; Maqsud et al., 2014) and machine translation (Lo et al., 2013). However, such manual efforts are known to be highly costly. Possible frames need to be manually determined, their roles individually described, and large volumes of text data annotated accordingly. For this reason, Proposition Banks do not exist for most languages. Annotation projection. Recent research has explored the possibility of using annotation projection to automatically generate Proposition Banks from parallel corpora for new target languages (TL) (Pad´o and Lapata, 2009; Van der Plas et al., 2011; Akbik et al., 2015)."
C16-1327,P11-2052,0,0.394572,"Missing"
C16-1327,C14-1121,0,0.030761,"Missing"
C16-1327,N06-1055,0,0.0907611,"Missing"
C16-1327,H01-1035,0,0.231815,"y of crosslingual semantics1 . 2 Related Work Annotation projection Annotation projection takes as input a word-aligned parallel corpus of English sentences and their target language (TL) translations. An English semantic role labeler is used to predict semantic labels for the English sentences. These labels are then projected onto aligned TL words, automatically producing a TL corpus annotated with English frame and role labels. Refer to Figure 2 for illustration. The use of annotation projection to train parsers for new languages was first introduced in the context of learning a PoS tagger (Yarowsky et al., 2001). Initial work on projecting semantic labels used FrameNet (Pad´o and Lapata, 2009), but subsequent work has focused on PropBank annotation due to its broader coverage and the availability of high quality semantic role labelers for English (Van der Plas et al., 2011; van der Plas et al., 2014). Recent work has focused on increasing the accuracy of projected labels by scaling up projection to larger corpora and retraining SRL models (Van der Plas et al., 2011; van der Plas et al., 2014) as well as using filtering techniques to block labels most likely affected by translation shift (Akbik et al."
C16-2056,P16-4001,1,0.922478,"., 2015). A downside of such approaches is that extractors need to be separately created for each new language of interest, potentially blowing up costs. With this demo, we present P OLYGLOT IE, a web-based tool that allows users to create extractors over a unified, crosslingual abstraction of shallow semantics. The core advantage of our approach is that extractors need only be created once for one language against this abstraction, but can then automatically extract information from multilingual text. We base our approach on previous work in multilingual semantic parsing (Akbik et al., 2015; Akbik and Li, 2016a; Akbik and Li, 2016b). In this research, we created a semantic role labeler (SRL) capable of predicting shallow semantic frame and role labels from the English Proposition Bank (Palmer et al., 2005) for 9 languages from 4 different language groups. We propose to utilize these semantic labels as the shared feature set against which users develop extractors. This, we argue, has two advantages: First, semantic role labels have human readable, shallow semantic descriptions (such as buyer, thing bought, and price paid) allowing users even without a background in linguistics to develop extractors."
C16-2056,P15-1039,1,0.924801,"; Rockt¨aschel et al., 2015). A downside of such approaches is that extractors need to be separately created for each new language of interest, potentially blowing up costs. With this demo, we present P OLYGLOT IE, a web-based tool that allows users to create extractors over a unified, crosslingual abstraction of shallow semantics. The core advantage of our approach is that extractors need only be created once for one language against this abstraction, but can then automatically extract information from multilingual text. We base our approach on previous work in multilingual semantic parsing (Akbik et al., 2015; Akbik and Li, 2016a; Akbik and Li, 2016b). In this research, we created a semantic role labeler (SRL) capable of predicting shallow semantic frame and role labels from the English Proposition Bank (Palmer et al., 2005) for 9 languages from 4 different language groups. We propose to utilize these semantic labels as the shared feature set against which users develop extractors. This, we argue, has two advantages: First, semantic role labels have human readable, shallow semantic descriptions (such as buyer, thing bought, and price paid) allowing users even without a background in linguistics to"
C16-2056,bonial-etal-2014-propbank,0,0.0237619,"Missing"
C16-2056,P10-1014,1,0.938397,"s the primary arguments of verbs (PropBank roles A0 through A4), including information on syntactic argument structure. And a C ONTEXTS view that exposes information about adjuncts of verbs such as temporal, location and manner contexts, corresponding to optional roles in PropBank. 3 Declarative Information Extraction Against the Unified Abstraction We create extractors in a declarative fashion against these views. In declarative IE, extractors are fundamentally SQL-like queries against views that create other views that are either output as extraction results or embedded in other extractors (Chiticariu et al., 2010). This approach has the advantages of providing 269 Figure 2: Multilingual extractor for smartphone acquisitions. AQL rule and extraction results. a standard IE language and data model, and allowing for the creation of succinct and embeddable views, further simplifying the process of developing multilingual extractors. For an example of a declarative extractor, refer to Figure 2. This extractor searches for instances of a relation between buyers and the smartphone they purchase. To illustrate how the extractor works, we give details on each block of lines, referred to by the numbered blocks (1"
C16-2056,P09-1113,0,0.0192202,"lingual text and inspect extraction results. Using the UI, we discuss the challenges and potential of using unified, crosslingual semantic abstractions as basis for downstream applications. We demonstrate multilingual IE for 9 languages from 4 different language groups: English, German, French, Spanish, Japanese, Chinese, Arabic, Russian and Hindi. 1 Introduction Information Extraction (IE) is the task of automatically extracting structured information from text (Sarawagi, 2008). Current IE approaches mostly focus on monolingual data and use languagespecific feature sets to create extractors (Mintz et al., 2009; Surdeanu and Ji, 2014; Rockt¨aschel et al., 2015). A downside of such approaches is that extractors need to be separately created for each new language of interest, potentially blowing up costs. With this demo, we present P OLYGLOT IE, a web-based tool that allows users to create extractors over a unified, crosslingual abstraction of shallow semantics. The core advantage of our approach is that extractors need only be created once for one language against this abstraction, but can then automatically extract information from multilingual text. We base our approach on previous work in multilin"
C16-2056,J05-1004,0,0.0075658,"eb-based tool that allows users to create extractors over a unified, crosslingual abstraction of shallow semantics. The core advantage of our approach is that extractors need only be created once for one language against this abstraction, but can then automatically extract information from multilingual text. We base our approach on previous work in multilingual semantic parsing (Akbik et al., 2015; Akbik and Li, 2016a; Akbik and Li, 2016b). In this research, we created a semantic role labeler (SRL) capable of predicting shallow semantic frame and role labels from the English Proposition Bank (Palmer et al., 2005) for 9 languages from 4 different language groups. We propose to utilize these semantic labels as the shared feature set against which users develop extractors. This, we argue, has two advantages: First, semantic role labels have human readable, shallow semantic descriptions (such as buyer, thing bought, and price paid) allowing users even without a background in linguistics to develop extractors. Second, since the same English labels are detected across all languages, users need not be language experts in a target language to create extractors. For instance, an English speaker might use this"
C16-2056,N15-1118,0,0.0443212,"Missing"
C18-1058,D13-1160,0,0.0490721,"Missing"
C18-1058,D13-1078,0,0.0304885,"Missing"
C18-1058,C10-1032,0,0.044585,"ing algorithms that drive these knowledge-centric applications (Shen et al., 2015; Arasu and Kaushik, 2009). Unifying entity representations has been widely studied in record linkage (Christen, 2012), deduplication (Elmagarmid et al., 2007) and reference matching (McCallum et al., 2000). Typically, duplicates are identified using the attributes and/or the contextual information of entities (Zhang et al., 2010; Han et al., 2011; Shen et al., 2015). The string representation or mention of an entity forms key evidence: similar mentions likely refer to the same entity. Although deemed as crucial (Dredze et al., 2010), variations in mentions are typically handled using textual similarity like edit distance and cosine similarity (Zheng et al., 2010; Lehmann et al., 2010; Liu et al., 2013), which can be misleading. Example. Consider the mentions: (a) General Electric Corporation, (b) General Electric China Corporation, (c) GE Corp. Mentions (a) and (b) are textually similar, differing in just one token. However, they refer to different entities, owing to the location detail ‘China’. Conversely, textually dissimilar mentions (a) and (c) refer to the same entity. An entity mention is not merely a sequence of c"
C18-1058,P15-2049,0,0.0517467,"Missing"
C18-1058,D11-1142,0,0.00964199,"ts Related Work Exploiting the compositional structure of entities and attributes especially from query streams and text has received much attention in databases and NLP literature. It has mostly been used for understanding NL questions (Berant et al., 2013), noun-phrase queries (Li, 2010) or normalizing time expressions (Lee et al., 2014; Bethard, 2013). Consequently, structuring and linking information on the web (Bollacker et al., 2008; Auer et al., 2007) about entities and their attributes, has seen a rise in interest. With this information being automatically extracted from textual data (Fader et al., 2011; Carlson et al., 2010), reconciling variations in entities and attribute names has become an integral part of the effort. Some recent work (Halevy et al., 2016) has attempted to organize attribute names by learning their compositional structure. On the other hand, some have proposed complex normalization frameworks (D’Souza and Ng, 2015) for specific domains. However, we need methods that can learn structured representations for the large scale of entity types found on the web. Named entities are not atomic units and often contain other entities (Finkel and Manning, 2009). However, entity res"
C18-1058,D09-1015,0,0.0298278,"xtracted from textual data (Fader et al., 2011; Carlson et al., 2010), reconciling variations in entities and attribute names has become an integral part of the effort. Some recent work (Halevy et al., 2016) has attempted to organize attribute names by learning their compositional structure. On the other hand, some have proposed complex normalization frameworks (D’Souza and Ng, 2015) for specific domains. However, we need methods that can learn structured representations for the large scale of entity types found on the web. Named entities are not atomic units and often contain other entities (Finkel and Manning, 2009). However, entity resolution has relied largely on surface-level match of entity mentions (Riedel et al., 2010; Hoffmann et al., 2011; Xu et al., 2013). Variations are typically handled using similarity functions such as edit distance, jaccard similarity, which have limited customizability. While learning string transformation rules (Arasu et al., 2009; Singh and Gulwani, 2012) to reconcile variations has been studied in different contexts, it typically relies on a set of input-output examples. It is difficult to obtain such data for entities and their variations. We instead propose a differen"
C18-1058,P11-1055,0,0.335067,"ntary material for details 694 given the extreme low matching ratio for this dataset. ERLearn-CIKM already identifies a significant subset of the true links in such a sparse space that it is non-trivial for ERLearn-LUSTRE to have found additional 45 true links. In contrast, the ER task is more challenging for the Crystal scenario, with more matching functions (158 vs. 68) (Qian et al., 2017). ERLearn-LUSTRE could still identify additional 1544 true links, which is a significant improvement over 130k true links already identified by ERLearn-CIKM. 5 Emp-Social Relation Extraction We use MULTIR (Hoffmann et al., 2011), a state-of-the-art relation extractor trained on NY Times text (Riedel et al., 2010) with weak supervision from Freebase (Bollacker et al., 2008). The weak supervision data is generated by exactly matching the textual mentions to canonicalized entities in Freebase. We instead match to the variations of entities of types Person and Company, generated using the configurations in Table 6. We follow the approach of (Hoffmann et al., 2011) to generate supervision data, compute features and evaluate aggregate extraction. For the 2 million entities, we generate 5.3 million variations. There were 24"
C18-1058,P14-1135,0,0.0226499,"Missing"
C18-1058,D08-1003,1,0.849201,"units in the structure can potentially span multiple tokens and matchers. As a result, there are many ways to combine matchers and derive the rule. Example. A user can label ‘General Motors’ as semantic unit hname i in ‘General Motors Co.’ Tokens ‘General’ and ‘Motors’ map to matchers caps and alphaNum. The most reliable interpretation for hname i is hname ::caps{1,2}i, a combination of two adjacent matchers hcaps i. LUSTRE derives a reliable rule as the sequence of most selective matchers, where selectivity is the expected number of matches of a matcher over the set of unlabeled mentions U (Li et al., 2008). sel(pi ) = E[match(pi , m ∈ U)] with match(pi , m) being number of matches of pi over mention m. 3.4 Parsing When a new mapping rule is learned, we want to estimate its reliability in predicting structures of mentions, and update the model M (Line 17-18 of Algorithm 1). These reliability scores are used for estimating utility scores at candidate selection and for resolving ambiguities when multiple rules can parse a mention (with preference given to the most reliable rule). Following the intuition that generic rules are less reliable, we estimate reliability of a rule based on its expected n"
C18-1058,P10-1136,0,0.0501433,"Missing"
C18-1058,P13-1128,0,0.0330936,"inkage (Christen, 2012), deduplication (Elmagarmid et al., 2007) and reference matching (McCallum et al., 2000). Typically, duplicates are identified using the attributes and/or the contextual information of entities (Zhang et al., 2010; Han et al., 2011; Shen et al., 2015). The string representation or mention of an entity forms key evidence: similar mentions likely refer to the same entity. Although deemed as crucial (Dredze et al., 2010), variations in mentions are typically handled using textual similarity like edit distance and cosine similarity (Zheng et al., 2010; Lehmann et al., 2010; Liu et al., 2013), which can be misleading. Example. Consider the mentions: (a) General Electric Corporation, (b) General Electric China Corporation, (c) GE Corp. Mentions (a) and (b) are textually similar, differing in just one token. However, they refer to different entities, owing to the location detail ‘China’. Conversely, textually dissimilar mentions (a) and (c) refer to the same entity. An entity mention is not merely a sequence of characters (Arasu and Kaushik, 2009). It instead has an internal structure, specific to the type of entity. For example in Table 1, the company mentions have a hname i, optio"
C18-1058,D08-1112,0,0.0615533,"matchers. We further formalize this inTable 2: Predefined Matchers tuition in Section 3.3. We rank matchers in the following order based 1 on their selectivity: D &gt; caps&gt; alphaNum&gt; num&gt; special &gt; wild . 3.2 Candidate Selection The query strategy to determine what constitutes an informative mention is the central challenge in our active-learning setting (Line 7 of Algorithm 1). We consider a mention informative if its represents the structure of several other unlabeled mentions and its current structure is unknown or uncertain. We adopt a unified approach, combining density-weighted sampling (Settles and Craven, 2008) and uncertainty sampling (Culotta and McCallum, 2005) as it is robust to outliers and input distribution. For each unlabeled mention mi , we compute a correlation score ci (based on its structural similarity to other mentions) and an uncertainty score fi (based on its predicted structure). We then compute a utility score ui for mi , combining its correlation and uncertainty scores, and select a mention m∗ with the highest utility score for labeling. To compute correlation score ci , we need a reliable metric to measure the similarity of the structures of two mentions. Since surface-string sim"
C18-1058,P13-2117,0,0.0627461,"Missing"
C18-1058,N10-1072,0,0.0120768,"ations has been widely studied in record linkage (Christen, 2012), deduplication (Elmagarmid et al., 2007) and reference matching (McCallum et al., 2000). Typically, duplicates are identified using the attributes and/or the contextual information of entities (Zhang et al., 2010; Han et al., 2011; Shen et al., 2015). The string representation or mention of an entity forms key evidence: similar mentions likely refer to the same entity. Although deemed as crucial (Dredze et al., 2010), variations in mentions are typically handled using textual similarity like edit distance and cosine similarity (Zheng et al., 2010; Lehmann et al., 2010; Liu et al., 2013), which can be misleading. Example. Consider the mentions: (a) General Electric Corporation, (b) General Electric China Corporation, (c) GE Corp. Mentions (a) and (b) are textually similar, differing in just one token. However, they refer to different entities, owing to the location detail ‘China’. Conversely, textually dissimilar mentions (a) and (c) refer to the same entity. An entity mention is not merely a sequence of characters (Arasu and Kaushik, 2009). It instead has an internal structure, specific to the type of entity. For example in Table 1, t"
D08-1003,W00-1301,0,\N,Missing
D08-1003,X98-1004,0,\N,Missing
D08-1003,W06-1660,0,\N,Missing
D08-1003,W03-0428,0,\N,Missing
D08-1003,H05-1056,0,\N,Missing
D08-1003,P06-1141,0,\N,Missing
D10-1098,P08-1029,0,0.0571114,"Missing"
D10-1098,W06-1615,0,0.00618247,"Missing"
D10-1098,P10-1014,1,0.718581,"ule-based NER typically requires a significant amount of manual effort to (a) identify the explicit semantic changes required for the new domain (e.g., differences in entity type definition), (b) identify the portions of the (complex) core annotator that should be modified for each difference and (c) implement the required customization rules without compromising the extraction quality of the core annotator. Domain customization of rule-based NER has not received much attention in the recent literature with a few exceptions (Petasis et al., 2001; Maynard et al., 2003; Zhu et al., 2005). temT (Chiticariu et al., 2010), a general-purpose algebraic information extraction system (Sec. 3). NERL is specifically geared towards building and customizing complex NER annotators and makes it easy to understand a complex annotator that may comprise hundreds of rules. It simplifies the identification of what portions need to be modified for a given customization requirement. It also makes individual customizations easier to implement, as illustrated by the following example. Suppose we have to customize a domainindependent rule-based NER annotator for the CoNLL corpus (Tjong et al., 2003). Consider the two sports-relat"
D10-1098,D09-1015,0,0.0403189,"aden 650 Harry Road, San Jose, CA 95120, USA {chiti, rajase, yunyaoli, frreiss}@us.ibm.com, shiv@almaden.ibm.com Abstract al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). Named-entity recognition (NER) is an important task required in a wide variety of applications. While rule-based systems are appealing due to their well-known “explainability,” most, if not all, state-of-the-art results for NER tasks are based on machine learning techniques. Motivated by these results, we explore the following natural question in this paper: Are rule-based systems still a viable approach to named-entity recognition? Specifically, we have designed and implemented a high-level language NE"
D10-1098,W03-0425,0,0.0940548,"s Shivakumar Vaithyanathan IBM Research – Almaden 650 Harry Road, San Jose, CA 95120, USA {chiti, rajase, yunyaoli, frreiss}@us.ibm.com, shiv@almaden.ibm.com Abstract al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). Named-entity recognition (NER) is an important task required in a wide variety of applications. While rule-based systems are appealing due to their well-known “explainability,” most, if not all, state-of-the-art results for NER tasks are based on machine learning techniques. Motivated by these results, we explore the following natural question in this paper: Are rule-based systems still a viable approach to named-entity recognition? Specifically, we have"
D10-1098,N04-1001,0,0.01596,"Missing"
D10-1098,P06-1060,0,0.0286613,"Missing"
D10-1098,P01-1039,0,0.179729,"Missing"
D10-1098,W02-1041,0,0.311968,", shiv@almaden.ibm.com Abstract al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). Named-entity recognition (NER) is an important task required in a wide variety of applications. While rule-based systems are appealing due to their well-known “explainability,” most, if not all, state-of-the-art results for NER tasks are based on machine learning techniques. Motivated by these results, we explore the following natural question in this paper: Are rule-based systems still a viable approach to named-entity recognition? Specifically, we have designed and implemented a high-level language NERL on top of SystemT, a general-purpose algebraic information extraction system. NERL is t"
D10-1098,N06-1010,0,0.0168507,"Missing"
D10-1098,D08-1003,1,0.843372,"Missing"
D10-1098,W03-0430,0,0.0276428,"than IBM Research – Almaden 650 Harry Road, San Jose, CA 95120, USA {chiti, rajase, yunyaoli, frreiss}@us.ibm.com, shiv@almaden.ibm.com Abstract al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). Named-entity recognition (NER) is an important task required in a wide variety of applications. While rule-based systems are appealing due to their well-known “explainability,” most, if not all, state-of-the-art results for NER tasks are based on machine learning techniques. Motivated by these results, we explore the following natural question in this paper: Are rule-based systems still a viable approach to named-entity recognition? Specifically, we have designed and implemente"
D10-1098,H05-1056,0,0.0918595,"d during the development of CoreNERorig , while in ACE05, ORG and LOC entity types were split into four entity types (Organization, Location, Geo-Political Entity and Facility). Customizations such as CS and CG address the above changes in named-entity type definition and substantially improve the extraction quality of CoreNERorig . Next, we compare the extraction quality of the 2 CoNLL03dev and CoNLL03test correspond to the development and test sets for CoNLL03 respectively. customized CoreNER for CoNLL03 and Enron3 with the corresponding best published results by (Florian et al., 2003) and (Minkov et al., 2005). Tab. 7 shows that our customized CoreNER outperforms the corresponding state-of-the-art numbers for all the NER tasks on both CoNLL03 and Enron. 4 These results demonstrate that high-quality annotators can be built by customizing CoreNERorig using NERL, with the final extraction quality matching that of state-of-theart machine learning-based extractors. It is worthwhile noting that the best published results for CoNLL03 (Florian et al., 2003) were obtained by using four different classifiers (Robust Risk Minimization, Maximum Entropy, Transformation-based learning, and Hidden Markov Model) a"
D10-1098,W03-0420,0,0.178837,"nyao Li Frederick Reiss Shivakumar Vaithyanathan IBM Research – Almaden 650 Harry Road, San Jose, CA 95120, USA {chiti, rajase, yunyaoli, frreiss}@us.ibm.com, shiv@almaden.ibm.com Abstract al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). Named-entity recognition (NER) is an important task required in a wide variety of applications. While rule-based systems are appealing due to their well-known “explainability,” most, if not all, state-of-the-art results for NER tasks are based on machine learning techniques. Motivated by these results, we explore the following natural question in this paper: Are rule-based systems still a viable approach to named-entity recognition?"
D10-1098,P01-1055,0,0.0857997,"Missing"
D10-1098,sekine-nobata-2004-definition,0,0.0510473,"on Tasks Laura Chiticariu Rajasekar Krishnamurthy Yunyao Li Frederick Reiss Shivakumar Vaithyanathan IBM Research – Almaden 650 Harry Road, San Jose, CA 95120, USA {chiti, rajase, yunyaoli, frreiss}@us.ibm.com, shiv@almaden.ibm.com Abstract al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). Named-entity recognition (NER) is an important task required in a wide variety of applications. While rule-based systems are appealing due to their well-known “explainability,” most, if not all, state-of-the-art results for NER tasks are based on machine learning techniques. Motivated by these results, we explore the following natural question in this paper: Are rule-based systems still"
D10-1098,N10-1009,0,0.062668,"ose, CA 95120, USA {chiti, rajase, yunyaoli, frreiss}@us.ibm.com, shiv@almaden.ibm.com Abstract al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learningbased (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). Named-entity recognition (NER) is an important task required in a wide variety of applications. While rule-based systems are appealing due to their well-known “explainability,” most, if not all, state-of-the-art results for NER tasks are based on machine learning techniques. Motivated by these results, we explore the following natural question in this paper: Are rule-based systems still a viable approach to named-entity recognition? Specifically, we have designed and implemented a high-level language NERL on top of SystemT,"
D10-1098,W03-0419,0,0.122584,"Missing"
D10-1098,D09-1158,0,0.109989,"Missing"
D10-1098,M98-1015,0,\N,Missing
D10-1098,X98-1004,0,\N,Missing
D10-1098,M98-1004,0,\N,Missing
D10-1098,A00-1034,0,\N,Missing
D13-1079,X98-1004,0,0.311254,"successful largely due to: (1) expressivity: the language provides all primitives required for performing basic manipulation of structured data, (2) extensibility: the language can be extended with new features without fundamental changes to the language, (3) declarativity: the language allows the specification of computation logic without describing its control flow, thus allowing developers to code what the program should accomplish, rather than how to accomplish it. An earlier attempt in late 1980’s to formalize a rule language resulted in the Common Pattern Specification Language (CPSL) (Appelt and Onyshkevych, 1998). While CPSL did not succeed due to multiple drawbacks, including expressivity limitations, performance limitations, and its lack of support for core operations such as part of speech (Chiticariu et al., 2010), CPSL did gain some traction, e.g., it powers the JAPE language of the GATE open-source NLP system (Cunningham et al., 2011). Meanwhile, a number of declarative IE languages developed in the database community, including AQL (Chiticariu et al., 2010; Li et al., 2011), xLog (Shen et al., 2007), and SQL extensions (Wang et al., 2010; Jain et al., 2009), have shown that formalisms of rule-b"
D13-1079,P10-1014,1,0.663815,"led data in many real-world scenarios further increases the risk of committing to a ML-based solution. A measure of the system’s scalability and runtime efficiency, hardware costs are a function of two metrics: throughput and memory footprint. These figures, while extremely important for commercial vendors, are typically not reported in NLP literature. Nevertheless, our experience in practice suggests that ML-based approaches are much slower, and require more memory compared to rule-based approaches, whose throughput can be in the order of MB/second/core for complex extraction tasks like NER (Chiticariu et al., 2010). The other explanation. Finally, we believe that the most notable reason behind the academic community’s steering away from rule-based IE systems is the (false) perception of lack of research problems. The general attitude is one of “What’s the research in rule-based IE? Just go ahead and write the rules.” as indicated by anecdotal evidence and only implicitly stated in the literature, where any usage of rules is significantly underplayed as explained earlier. In the next section, we strive to debunk this perception. 830 3 Bridging the Gap As NLP researchers who also work regularly with busin"
D13-1079,W06-1656,0,0.0219454,"What is the source of this disconnect between research and industry? There does not appear to be a lack of interaction between the two communities. Indeed, many of the smaller companies we surveyed were founded by NLP researchers, and many of the larger vendors actively publish in the NLP literature. We believe that the disconnect arises from a difference in how the two communities measure the costs and benefits of information extraction. Table 2 summarizes the pros and cons of machine learning (ML) and rule-based IE technologies (Atzmueller and Kluegl, 2008; Grimes, 2011; Leung et al., 2011; Feldman and Rosenfeld, 2006; Guo et al., 2006; Krishnan et al., 2005; Yakushiji et al., 2006; Kluegl et al., 2009). On the surface, both academia and commercial vendors acknowledge essentially the same pros and cons for the two approaches. However, the two communities weight the pros and cons significantly differently, leading to the drastic disconnect in Figure 1. Evaluating the benefits of IE. Academic papers evaluate IE performance in terms of precision and recall over standard labeled data sets. This simple, clean, and objective measure is useful for judging competitions, but the reality of the business world is 829"
D13-1079,W06-1660,0,0.00880827,"sconnect between research and industry? There does not appear to be a lack of interaction between the two communities. Indeed, many of the smaller companies we surveyed were founded by NLP researchers, and many of the larger vendors actively publish in the NLP literature. We believe that the disconnect arises from a difference in how the two communities measure the costs and benefits of information extraction. Table 2 summarizes the pros and cons of machine learning (ML) and rule-based IE technologies (Atzmueller and Kluegl, 2008; Grimes, 2011; Leung et al., 2011; Feldman and Rosenfeld, 2006; Guo et al., 2006; Krishnan et al., 2005; Yakushiji et al., 2006; Kluegl et al., 2009). On the surface, both academia and commercial vendors acknowledge essentially the same pros and cons for the two approaches. However, the two communities weight the pros and cons significantly differently, leading to the drastic disconnect in Figure 1. Evaluating the benefits of IE. Academic papers evaluate IE performance in terms of precision and recall over standard labeled data sets. This simple, clean, and objective measure is useful for judging competitions, but the reality of the business world is 829 much more fluid a"
D13-1079,H05-1040,0,0.00842625,"esearch and industry? There does not appear to be a lack of interaction between the two communities. Indeed, many of the smaller companies we surveyed were founded by NLP researchers, and many of the larger vendors actively publish in the NLP literature. We believe that the disconnect arises from a difference in how the two communities measure the costs and benefits of information extraction. Table 2 summarizes the pros and cons of machine learning (ML) and rule-based IE technologies (Atzmueller and Kluegl, 2008; Grimes, 2011; Leung et al., 2011; Feldman and Rosenfeld, 2006; Guo et al., 2006; Krishnan et al., 2005; Yakushiji et al., 2006; Kluegl et al., 2009). On the surface, both academia and commercial vendors acknowledge essentially the same pros and cons for the two approaches. However, the two communities weight the pros and cons significantly differently, leading to the drastic disconnect in Figure 1. Evaluating the benefits of IE. Academic papers evaluate IE performance in terms of precision and recall over standard labeled data sets. This simple, clean, and objective measure is useful for judging competitions, but the reality of the business world is 829 much more fluid and less well-defined. I"
D13-1079,D11-1075,0,0.0208273,"Missing"
D13-1079,P11-4019,1,0.33747,"mpt in late 1980’s to formalize a rule language resulted in the Common Pattern Specification Language (CPSL) (Appelt and Onyshkevych, 1998). While CPSL did not succeed due to multiple drawbacks, including expressivity limitations, performance limitations, and its lack of support for core operations such as part of speech (Chiticariu et al., 2010), CPSL did gain some traction, e.g., it powers the JAPE language of the GATE open-source NLP system (Cunningham et al., 2011). Meanwhile, a number of declarative IE languages developed in the database community, including AQL (Chiticariu et al., 2010; Li et al., 2011), xLog (Shen et al., 2007), and SQL extensions (Wang et al., 2010; Jain et al., 2009), have shown that formalisms of rule-based IE systems are possible, as exemplified by (Fagin et al., 2013). However, they largely remain unknown in the NLP community. We believe now is the right time to establish a standard IE rule language, drawing from existing proposals and experience over the past 30 years. Towards this goal, IE researchers need to answer the following questions: What is the right data model to capture text, annotations over text, and their properties? Can we establish a standard declarati"
D13-1079,D12-1048,0,0.0105898,"ttensity Basis Technology Clarabridge Daedalus GATE General Sentiment HP IBM IBM Figure 2: The conference paper data (left-hand bar) from Figure 1, broken down by year of publication. The relative fractions of the three different techniques have not changed significantly over time. from pure machine learning systems was quite challenging. The papers that use a mixture of rulebased and machine learning techniques were generally written so as to obfuscate the use of rules, emphasizing the machine learning aspect of the work. Authors hid rules behind euphemisms such as “dependency restrictions” (Mausam et al., 2012), “entity type constraints” (Yao et al., 2011), or “seed dictionaries” (Putthividhya and Hu, 2011). In the commercial world, the situation is largely reversed. The right side of Figure 1 shows the result of a parallel survey of commercial entity extraction products from 54 different vendors listed in (Yuen and Koehler-Kruener, 2012). We studied analyst reports and product literature, then classified each product according to the same three categories. Table 1 shows the 41 products considered in the study 1 . We conducted this industry survey in 2013, one year after the ten-year run of NLP pape"
D13-1079,D11-1144,0,0.0377407,"Missing"
D13-1079,W06-1634,0,0.0577583,"here does not appear to be a lack of interaction between the two communities. Indeed, many of the smaller companies we surveyed were founded by NLP researchers, and many of the larger vendors actively publish in the NLP literature. We believe that the disconnect arises from a difference in how the two communities measure the costs and benefits of information extraction. Table 2 summarizes the pros and cons of machine learning (ML) and rule-based IE technologies (Atzmueller and Kluegl, 2008; Grimes, 2011; Leung et al., 2011; Feldman and Rosenfeld, 2006; Guo et al., 2006; Krishnan et al., 2005; Yakushiji et al., 2006; Kluegl et al., 2009). On the surface, both academia and commercial vendors acknowledge essentially the same pros and cons for the two approaches. However, the two communities weight the pros and cons significantly differently, leading to the drastic disconnect in Figure 1. Evaluating the benefits of IE. Academic papers evaluate IE performance in terms of precision and recall over standard labeled data sets. This simple, clean, and objective measure is useful for judging competitions, but the reality of the business world is 829 much more fluid and less well-defined. In a business context, de"
D13-1079,D11-1135,0,0.0147961,"TE General Sentiment HP IBM IBM Figure 2: The conference paper data (left-hand bar) from Figure 1, broken down by year of publication. The relative fractions of the three different techniques have not changed significantly over time. from pure machine learning systems was quite challenging. The papers that use a mixture of rulebased and machine learning techniques were generally written so as to obfuscate the use of rules, emphasizing the machine learning aspect of the work. Authors hid rules behind euphemisms such as “dependency restrictions” (Mausam et al., 2012), “entity type constraints” (Yao et al., 2011), or “seed dictionaries” (Putthividhya and Hu, 2011). In the commercial world, the situation is largely reversed. The right side of Figure 1 shows the result of a parallel survey of commercial entity extraction products from 54 different vendors listed in (Yuen and Koehler-Kruener, 2012). We studied analyst reports and product literature, then classified each product according to the same three categories. Table 1 shows the 41 products considered in the study 1 . We conducted this industry survey in 2013, one year after the ten-year run of NLP papers we studied. One would expect the industrial"
D16-1102,P16-4001,1,0.879668,"Missing"
D16-1102,P15-1039,1,0.724261,"Missing"
D16-1102,C16-1327,1,0.887762,"Missing"
D16-1102,P13-2130,0,0.0921276,"Missing"
D16-1102,D15-1076,0,0.0997008,"Missing"
D16-1102,W11-0404,0,0.0598047,"Missing"
D16-1102,J93-2004,0,0.0634043,"Missing"
D16-1102,J05-1004,0,0.0254218,"Missing"
D16-1102,P15-2067,0,0.057281,"Missing"
D16-1102,tiedemann-2012-parallel,0,0.0293399,"Missing"
D16-1102,P11-2052,0,0.342108,"Missing"
D16-1102,E14-4044,0,\N,Missing
D17-1205,C16-1058,1,0.830681,"Missing"
D17-1205,D16-1102,1,0.921611,"o produce (Hovy et al., 2006). Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled data for a range of NLP tasks (Snow et al., 2008; Hong and Baker, 2011; Franklin et al., 2011). A core advantage of crowdsourcing is that it allows the annotation workload to be scaled out among large numbers of inexpensive crowd workers. Not surprisingly, a number of recent SRL works have also attempted to leverage crowdsourcing to generate labeled training data for SRL and investigated a variety of ways of formulating crowdsourcing tasks (Fossati et al., 2013; Pavlick et al., 2015; Akbik et al., 2016). All have found that crowd feedback generally suffers from low interannotator agreement scores and often produces incorrect labels. These results seem to indicate that, regardless of the design of the task, SRL is simply too difficult to be effectively crowdsourced. Proposed Approach We observe that there are significant differences in difficulties among SRL annotation tasks, depending on factors such as the complexity of a specific sentence or the difficulty of a specific semantic role. We therefore postulate that a subset of annotation tasks is in fact suitable for crowd workers, while othe"
D17-1205,P98-1013,0,0.108102,"beling tasks is in fact appropriate for the crowd. We present a novel workflow in which we employ a classifier to identify difficult annotation tasks and route each task either to experts or crowd workers according to their difficulties. Our experimental evaluation shows that the proposed approach reduces the workload for experts by over two-thirds, and thus significantly reduces the cost of producing SRL annotation at little loss in quality. 1 Introduction Semantic role labeling (SRL) is the task of labeling the predicate-argument structures of sentences with semantic frames and their roles (Baker et al., 1998; Palmer et al., 2005). It has been found useful for a wide variety of NLP tasks such as question-answering (Shen and Lapata, 2007), information extraction (Fader et al., 2011) and machine translation (Lo et al., 2013). A major bottleneck impeding the wide adoption of SRL is the need for large amounts of labeled training data to ∗ The work was done while the author was at IBM Research - Almaden. capture broad-coverage semantics. Such data requires trained experts and is highly costly to produce (Hovy et al., 2006). Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled"
D17-1205,D11-1142,0,0.0349924,"r to experts or crowd workers according to their difficulties. Our experimental evaluation shows that the proposed approach reduces the workload for experts by over two-thirds, and thus significantly reduces the cost of producing SRL annotation at little loss in quality. 1 Introduction Semantic role labeling (SRL) is the task of labeling the predicate-argument structures of sentences with semantic frames and their roles (Baker et al., 1998; Palmer et al., 2005). It has been found useful for a wide variety of NLP tasks such as question-answering (Shen and Lapata, 2007), information extraction (Fader et al., 2011) and machine translation (Lo et al., 2013). A major bottleneck impeding the wide adoption of SRL is the need for large amounts of labeled training data to ∗ The work was done while the author was at IBM Research - Almaden. capture broad-coverage semantics. Such data requires trained experts and is highly costly to produce (Hovy et al., 2006). Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled data for a range of NLP tasks (Snow et al., 2008; Hong and Baker, 2011; Franklin et al., 2011). A core advantage of crowdsourcing is that it allows the annotation workload to"
D17-1205,D15-1076,0,0.223489,"entences (see Figure 1). While state-of-the-art SRL will predict many correct labels, some predicted labels will be incorrect, and some labels will be missing. Annotation tasks are therefore designed to detect and correct precision and recall errors. Step 2. We generate two types of annotation tasks for the study, namely C ONFIRM P REDICTION and A DD M ISSING tasks: (1) The first, C ONFIRM P RE DICTION tasks, ask users to confirm, reject or correct each predicted frame or role. This type of task addresses precision issues in the SRL. We present to workers a human-readable questionanswer pair (He et al., 2015) for each predicted label, an example of which is illustrated in Figure 2. (2) The second, A DD M ISSING tasks, address potentially missing annotation, i.e. recall issues in the SRL. We generate a question without a suggested answer and ask workers to either confirm that this role does not appear in the sentence, or supply the correct span. We identify potentially missing annotation using PropBank frame definitions; any unseen core role in a sentence is considered potentially missing. We use a manually created mapping of frameroles to questions to generate these tasks. See Table 1 for a mappin"
D17-1205,D16-1258,0,0.0136308,"ither experts familiar with PropBank, or non-expert crowd workers. Rather than routing tasks to the most appropriate workers, our proposed approach determines which SRL tasks are appropriate for crowdsourcing, and sends the remaining ones to experts. Human-in-the-loop Methods Our method is similar in the spirit of human-in-the-loop learning (Fung et al., 1992; Li et al., 2016). Humanin-the-loop learning generally aims to leverage humans to complete easy commonsense tasks, such as the recognition of objects in images (Patterson et al., 2013). Recent work also proposed humanin-the-loop parsing (He et al., 2016) to include human feedback into parsing. However, unlike these approaches, we aim to combine both experts and non-experts to address the difficulty of some SRL annotation tasks, while leveraging the crowd for the majority of tasks. 6 Conclusion In this paper, we proposed C ROWD - IN - THE -L OOP an approach for creating high-quality annotated 1920 data for SRL that leverages both crowd and expert workers. We conducted a crowdsourcing study and analyzed its results to design a classifier to distinguish between crowd-appropriate and expert-required tasks. Our experimental evaluation showed that"
D17-1205,W11-0404,0,0.481745,"r a wide variety of NLP tasks such as question-answering (Shen and Lapata, 2007), information extraction (Fader et al., 2011) and machine translation (Lo et al., 2013). A major bottleneck impeding the wide adoption of SRL is the need for large amounts of labeled training data to ∗ The work was done while the author was at IBM Research - Almaden. capture broad-coverage semantics. Such data requires trained experts and is highly costly to produce (Hovy et al., 2006). Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled data for a range of NLP tasks (Snow et al., 2008; Hong and Baker, 2011; Franklin et al., 2011). A core advantage of crowdsourcing is that it allows the annotation workload to be scaled out among large numbers of inexpensive crowd workers. Not surprisingly, a number of recent SRL works have also attempted to leverage crowdsourcing to generate labeled training data for SRL and investigated a variety of ways of formulating crowdsourcing tasks (Fossati et al., 2013; Pavlick et al., 2015; Akbik et al., 2016). All have found that crowd feedback generally suffers from low interannotator agreement scores and often produces incorrect labels. These results seem to indicat"
D17-1205,N06-2015,0,0.0390764,"predicate-argument structures of sentences with semantic frames and their roles (Baker et al., 1998; Palmer et al., 2005). It has been found useful for a wide variety of NLP tasks such as question-answering (Shen and Lapata, 2007), information extraction (Fader et al., 2011) and machine translation (Lo et al., 2013). A major bottleneck impeding the wide adoption of SRL is the need for large amounts of labeled training data to ∗ The work was done while the author was at IBM Research - Almaden. capture broad-coverage semantics. Such data requires trained experts and is highly costly to produce (Hovy et al., 2006). Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled data for a range of NLP tasks (Snow et al., 2008; Hong and Baker, 2011; Franklin et al., 2011). A core advantage of crowdsourcing is that it allows the annotation workload to be scaled out among large numbers of inexpensive crowd workers. Not surprisingly, a number of recent SRL works have also attempted to leverage crowdsourcing to generate labeled training data for SRL and investigated a variety of ways of formulating crowdsourcing tasks (Fossati et al., 2013; Pavlick et al., 2015; Akbik et al., 2016). All have"
D17-1205,E14-4044,0,0.121404,"Missing"
D17-1205,2013.iwslt-evaluation.5,0,0.0620554,"Missing"
D17-1205,P13-2130,0,0.157588,"uires trained experts and is highly costly to produce (Hovy et al., 2006). Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled data for a range of NLP tasks (Snow et al., 2008; Hong and Baker, 2011; Franklin et al., 2011). A core advantage of crowdsourcing is that it allows the annotation workload to be scaled out among large numbers of inexpensive crowd workers. Not surprisingly, a number of recent SRL works have also attempted to leverage crowdsourcing to generate labeled training data for SRL and investigated a variety of ways of formulating crowdsourcing tasks (Fossati et al., 2013; Pavlick et al., 2015; Akbik et al., 2016). All have found that crowd feedback generally suffers from low interannotator agreement scores and often produces incorrect labels. These results seem to indicate that, regardless of the design of the task, SRL is simply too difficult to be effectively crowdsourced. Proposed Approach We observe that there are significant differences in difficulties among SRL annotation tasks, depending on factors such as the complexity of a specific sentence or the difficulty of a specific semantic role. We therefore postulate that a subset of annotation tasks is in"
D17-1205,J05-1004,0,0.788261,"act appropriate for the crowd. We present a novel workflow in which we employ a classifier to identify difficult annotation tasks and route each task either to experts or crowd workers according to their difficulties. Our experimental evaluation shows that the proposed approach reduces the workload for experts by over two-thirds, and thus significantly reduces the cost of producing SRL annotation at little loss in quality. 1 Introduction Semantic role labeling (SRL) is the task of labeling the predicate-argument structures of sentences with semantic frames and their roles (Baker et al., 1998; Palmer et al., 2005). It has been found useful for a wide variety of NLP tasks such as question-answering (Shen and Lapata, 2007), information extraction (Fader et al., 2011) and machine translation (Lo et al., 2013). A major bottleneck impeding the wide adoption of SRL is the need for large amounts of labeled training data to ∗ The work was done while the author was at IBM Research - Almaden. capture broad-coverage semantics. Such data requires trained experts and is highly costly to produce (Hovy et al., 2006). Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled data for a range of N"
D17-1205,P15-2067,0,0.0779537,"Missing"
D17-1205,D07-1002,0,0.0788148,"icult annotation tasks and route each task either to experts or crowd workers according to their difficulties. Our experimental evaluation shows that the proposed approach reduces the workload for experts by over two-thirds, and thus significantly reduces the cost of producing SRL annotation at little loss in quality. 1 Introduction Semantic role labeling (SRL) is the task of labeling the predicate-argument structures of sentences with semantic frames and their roles (Baker et al., 1998; Palmer et al., 2005). It has been found useful for a wide variety of NLP tasks such as question-answering (Shen and Lapata, 2007), information extraction (Fader et al., 2011) and machine translation (Lo et al., 2013). A major bottleneck impeding the wide adoption of SRL is the need for large amounts of labeled training data to ∗ The work was done while the author was at IBM Research - Almaden. capture broad-coverage semantics. Such data requires trained experts and is highly costly to produce (Hovy et al., 2006). Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled data for a range of NLP tasks (Snow et al., 2008; Hong and Baker, 2011; Franklin et al., 2011). A core advantage of crowdsourcing"
D17-1205,D08-1027,0,0.321445,"Missing"
D17-1205,C98-1013,0,\N,Missing
K18-1043,2000.eamt-1.1,0,0.301113,"Missing"
K18-1043,D13-1005,0,0.0607974,"Missing"
K18-1043,D12-1039,0,0.05519,"Missing"
K18-1043,C69-5701,0,0.415079,"Missing"
K18-1043,D13-1019,0,0.0295624,"who has encountered autocorrected text messages, punny social media posts, or just friends with bad grammar. Although at first glance it may seem that phonetic similarity can only be quantified for audible words, this problem is often present in purely textual spaces, such as social media posts or text messages. Incorrect homophones and synophones, whether used in error or in jest, pose challenges for a wide range of NLP tasks, such as named entity identification, text normalization and spelling correction (Chung et al., 2011; Xia et al., 2006; Toutanova and Moore, 2002; Twiefel et al., 2014; Lee et al., 2013; Kessler, 2005). These tasks must therefore successfully transform incorrect words or phrases (‘hear’,’so’) to their phonetically similar correct counterparts (’here’,’sew’), which in turn requires a robust representation of phonetic similarity between word pairs. A reli1 Chinese has five tones, represented on a 1-5 scale. 444 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 444–453 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics Words 稀xi1饭fan4 喜xi2欢huan1 泄xie4愤fen4 DM S:S,FN:FN S:S,HN:HN S:S,"
K18-1043,D17-1075,0,0.0612765,"Missing"
K18-1043,W02-2017,0,0.149133,"Missing"
K18-1043,P02-1019,0,0.110661,"to ‘I can’t sew buttons,’ is familiar to anyone who has encountered autocorrected text messages, punny social media posts, or just friends with bad grammar. Although at first glance it may seem that phonetic similarity can only be quantified for audible words, this problem is often present in purely textual spaces, such as social media posts or text messages. Incorrect homophones and synophones, whether used in error or in jest, pose challenges for a wide range of NLP tasks, such as named entity identification, text normalization and spelling correction (Chung et al., 2011; Xia et al., 2006; Toutanova and Moore, 2002; Twiefel et al., 2014; Lee et al., 2013; Kessler, 2005). These tasks must therefore successfully transform incorrect words or phrases (‘hear’,’so’) to their phonetically similar correct counterparts (’here’,’sew’), which in turn requires a robust representation of phonetic similarity between word pairs. A reli1 Chinese has five tones, represented on a 1-5 scale. 444 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 444–453 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics Words 稀xi1饭fan4 喜xi2欢huan"
K18-1043,W97-1102,0,0.12657,"Missing"
K18-1043,P06-1125,0,0.460126,"dinates. Pinyin phonetic similarities are then calculated by aggregating the similarities of initial, final and tone. D IM S IM demonstrates a 7.5X improvement on mean reciprocal rank over the state-of-theart phonetic similarity approaches. 1 initial x f final i an tone 1 4 Table 1: Example Pinyins. 偶(ou2,我wo2)稀饭(xi1fan4,喜欢xi2huan1)你。 I like you. 杯具(bei1ju4,悲剧bei1ju4)啊，为一个女孩纸 (zhi2,子zi5)这么香菇(xiang1gu1,想哭 xiang2ku1)。 Sadly, I am heart broken for a girl. Table 2: Microblogs using phonetic transcription. able approach for generating phonetically similar words is equally crucial for Chinese text (Xia et al., 2006). Unfortunately, most existing phonetic similarity algorithms such as Soundex (Archives and Administration, 2007) and Double Metaphone (DM) Philips (2000) are motivated by English and designed for Indo-European languages. Words are encoded to approximate phonetic presentations by ignoring vowels (except foremost ones), which is appropriate where phonetic transcription consists of a sequence of phonemes, such as for English. In contrast, the speech sound of a Chinese character is represented by a single syllable in Pinyin consisting of two or three parts: an initial (optional), a final or compo"
K18-1043,D15-1211,0,0.0473569,"Missing"
K18-1043,D14-1037,0,0.0481626,"Missing"
N15-1045,P06-2005,0,0.0443836,"ion process was primarily seen as one of standardizing non-standard tokens found in otherwise clean text, such as numbers, dates, and acronyms (Sproat et al., 2001). However, the current popularity of Twitter and other informal texts has caused the normalization task to take on a broader meaning in these contexts, where the goal is to convert informal text into formal text that downstream applications expect. Many different approaches to social media normalization have been undertaken. These approaches often draw inspiration from other tasks such as machine translation (Pennell and Liu, 2011; Aw et al., 2006), spell checking (Choudhury et al., 2007) or 421 speech recognition (Kobus et al., 2008). Other approaches include creating automatic abbreviations via a maximum entropy classifier (Pennell and Liu, 2010), creating word association graphs (Sonmez and Ozgur, 2014), and incorporating both rules and statistical models (Beaufort et al., 2010). While most initial approaches used supervised methods, unsupervised methods have recently become popular (Cook and Stevenson, 2009; Liu et al., 2011a; Yang and Eisenstein, 2013; Li and Liu, 2014). Some work has chosen to focus on specific aspects of the norm"
N15-1045,I11-1169,1,0.848886,"l., 2011b; Ritter et al., 2011) as well as on parsing and part-ofspeech tagging (Foster et al., 2011). The other approach is normalization. Rather than tailoring a NLP tool towards the data, normalization seeks to tailor the data towards the tool. This is accomplished by transforming the data into a form more akin to the formal text that NLP tools are generally trained on. While normalization is often more straightforward and more easily applied in instances in which retraining is difficult or impractical, it has potential disadvantages as well, such as the potential loss of pragmatic nuance (Baldwin and Chai, 2011). Prior to the rise of social media, the normalization process was primarily seen as one of standardizing non-standard tokens found in otherwise clean text, such as numbers, dates, and acronyms (Sproat et al., 2001). However, the current popularity of Twitter and other informal texts has caused the normalization task to take on a broader meaning in these contexts, where the goal is to convert informal text into formal text that downstream applications expect. Many different approaches to social media normalization have been undertaken. These approaches often draw inspiration from other tasks s"
N15-1045,W09-2010,0,0.0253873,"tion have been undertaken. These approaches often draw inspiration from other tasks such as machine translation (Pennell and Liu, 2011; Aw et al., 2006), spell checking (Choudhury et al., 2007) or 421 speech recognition (Kobus et al., 2008). Other approaches include creating automatic abbreviations via a maximum entropy classifier (Pennell and Liu, 2010), creating word association graphs (Sonmez and Ozgur, 2014), and incorporating both rules and statistical models (Beaufort et al., 2010). While most initial approaches used supervised methods, unsupervised methods have recently become popular (Cook and Stevenson, 2009; Liu et al., 2011a; Yang and Eisenstein, 2013; Li and Liu, 2014). Some work has chosen to focus on specific aspects of the normalization process, such as providing good coverage (Liu et al., 2012) or building normalization dictionaries (Han et al., 2012). In all of the work mentioned above, the normalization task was seen primarily as one of converting non-standard tokens into an equivalent standard form. Similarly, many of these works defined the problem even more narrowly such that punctuation, capitalization, and multi-word replacements were ignored. However, two pieces of recent work have"
N15-1045,N13-1037,0,0.0777631,"em. We then introduce our taxonomy of normalization edits in Section 3. In Section 4, we present our evaluation methodology and present results over the three applications, using Twitter data as a representative domain. Finally, we discuss our results in Section 5 and conclude in Section 6. 2 Related Work Twitter and other social media data is littered with non-standard word forms and other informal usage patterns, making it difficult for many NLP tools to produce results comparable to what is seen on formal datasets. There are two approaches proposed in the literature to handle this problem (Eisenstein, 2013). One approach is to tailor a specific NLP tool towards the data, by using training data from the domain to help the tool learn its specific idiosyncrasies. This approach has been applied with reasonable success on named entity recognition (Liu et al., 2011b; Ritter et al., 2011) as well as on parsing and part-ofspeech tagging (Foster et al., 2011). The other approach is normalization. Rather than tailoring a NLP tool towards the data, normalization seeks to tailor the data towards the tool. This is accomplished by transforming the data into a form more akin to the formal text that NLP tools a"
N15-1045,P05-1045,0,0.00474489,"on formal text. 4.2 NER Evaluation In this section, we examine the effect of each normalization edit on a somewhat more shallow interpretation task, named entity recognition. Unlike dependency parsing which requires an understanding of every token in the text, NER must only determine whether a given token is a named entity, and if so, discover its associated entity type. The setup for evaluation of normalization edits on named entity recognition closely follows that of dependency parsing. We once again employ a tool from the suite of Stanford NLP tools, the Stanford named entity recognizer4 (Finkel et al., 2005). We also define precision and recall in a similar manner: precisionner = recallner = |EN T ∩ EN Tgold | |EN T | |EN T ∩ EN Tgold | |EN Tgold | (3) (4) Where EN T and EN Tgold are the sets of entities identified over the candidate normalization and gold standard sentences, respectively. Entities were labeled as one of three classes (person, location, or organization), and two entities were only considered a match if they both selected the same entity and the same entity class. 4.2.1 Results Table 3 shows the results of the NER ablation study. Unlike dependency parsing, only word replacement ed"
N15-1045,D12-1039,0,0.0185745,"lude creating automatic abbreviations via a maximum entropy classifier (Pennell and Liu, 2010), creating word association graphs (Sonmez and Ozgur, 2014), and incorporating both rules and statistical models (Beaufort et al., 2010). While most initial approaches used supervised methods, unsupervised methods have recently become popular (Cook and Stevenson, 2009; Liu et al., 2011a; Yang and Eisenstein, 2013; Li and Liu, 2014). Some work has chosen to focus on specific aspects of the normalization process, such as providing good coverage (Liu et al., 2012) or building normalization dictionaries (Han et al., 2012). In all of the work mentioned above, the normalization task was seen primarily as one of converting non-standard tokens into an equivalent standard form. Similarly, many of these works defined the problem even more narrowly such that punctuation, capitalization, and multi-word replacements were ignored. However, two pieces of recent work have suggested that this understanding of the normalization task is too narrow, as it ignores many other hallmarks of informal writing that are prevalent in social media data. Wang and Ng (2013) present a beam search based approach designed to handle machine"
N15-1045,D14-1011,0,0.0121778,"nt a beam search based approach designed to handle machine translation which incorporates attempts to correct mistaken punctuation and add missing words, such as forms of the verb to be. Similarly, Zhang et al. (2013) attempt to perform all actions necessary to create a formal text. In both instances the work was motivated by, and evaluated with respect to, a specific downstream application (machine translation and dependency parsing, respectively). However, not every study that tied the output to an application chose a broad interpretation of the normalization problem (Beaufort et al., 2010; Kaji and Kitsuregawa, 2014). 3 Taxonomy of Normalization Edits In order to understand the impact of individual normalization edits on downstream applications, we first need to define the space of possible normalization edits. While it is not uncommon for normalization work to present some analysis of the data, these analyses are often quite specific to the domain and datasets of interest. Because there is no agreed upon Edit Insertion Punctuation Subj. Replacement Word Be Det. Punctuation Other Slang Removal Word Contraction Capitalization Punctuation Other Word Twitter Other Figure 1: Taxonomy of normalization edits ta"
N15-1045,C08-1056,0,0.0321711,"therwise clean text, such as numbers, dates, and acronyms (Sproat et al., 2001). However, the current popularity of Twitter and other informal texts has caused the normalization task to take on a broader meaning in these contexts, where the goal is to convert informal text into formal text that downstream applications expect. Many different approaches to social media normalization have been undertaken. These approaches often draw inspiration from other tasks such as machine translation (Pennell and Liu, 2011; Aw et al., 2006), spell checking (Choudhury et al., 2007) or 421 speech recognition (Kobus et al., 2008). Other approaches include creating automatic abbreviations via a maximum entropy classifier (Pennell and Liu, 2010), creating word association graphs (Sonmez and Ozgur, 2014), and incorporating both rules and statistical models (Beaufort et al., 2010). While most initial approaches used supervised methods, unsupervised methods have recently become popular (Cook and Stevenson, 2009; Liu et al., 2011a; Yang and Eisenstein, 2013; Li and Liu, 2014). Some work has chosen to focus on specific aspects of the normalization process, such as providing good coverage (Liu et al., 2012) or building normal"
N15-1045,P14-3012,0,0.362596,"other tasks such as machine translation (Pennell and Liu, 2011; Aw et al., 2006), spell checking (Choudhury et al., 2007) or 421 speech recognition (Kobus et al., 2008). Other approaches include creating automatic abbreviations via a maximum entropy classifier (Pennell and Liu, 2010), creating word association graphs (Sonmez and Ozgur, 2014), and incorporating both rules and statistical models (Beaufort et al., 2010). While most initial approaches used supervised methods, unsupervised methods have recently become popular (Cook and Stevenson, 2009; Liu et al., 2011a; Yang and Eisenstein, 2013; Li and Liu, 2014). Some work has chosen to focus on specific aspects of the normalization process, such as providing good coverage (Liu et al., 2012) or building normalization dictionaries (Han et al., 2012). In all of the work mentioned above, the normalization task was seen primarily as one of converting non-standard tokens into an equivalent standard form. Similarly, many of these works defined the problem even more narrowly such that punctuation, capitalization, and multi-word replacements were ignored. However, two pieces of recent work have suggested that this understanding of the normalization task is t"
N15-1045,P11-2013,0,0.239824,"tion 5 and conclude in Section 6. 2 Related Work Twitter and other social media data is littered with non-standard word forms and other informal usage patterns, making it difficult for many NLP tools to produce results comparable to what is seen on formal datasets. There are two approaches proposed in the literature to handle this problem (Eisenstein, 2013). One approach is to tailor a specific NLP tool towards the data, by using training data from the domain to help the tool learn its specific idiosyncrasies. This approach has been applied with reasonable success on named entity recognition (Liu et al., 2011b; Ritter et al., 2011) as well as on parsing and part-ofspeech tagging (Foster et al., 2011). The other approach is normalization. Rather than tailoring a NLP tool towards the data, normalization seeks to tailor the data towards the tool. This is accomplished by transforming the data into a form more akin to the formal text that NLP tools are generally trained on. While normalization is often more straightforward and more easily applied in instances in which retraining is difficult or impractical, it has potential disadvantages as well, such as the potential loss of pragmatic nuance (Baldwin"
N15-1045,P11-1037,0,0.0911484,"tion 5 and conclude in Section 6. 2 Related Work Twitter and other social media data is littered with non-standard word forms and other informal usage patterns, making it difficult for many NLP tools to produce results comparable to what is seen on formal datasets. There are two approaches proposed in the literature to handle this problem (Eisenstein, 2013). One approach is to tailor a specific NLP tool towards the data, by using training data from the domain to help the tool learn its specific idiosyncrasies. This approach has been applied with reasonable success on named entity recognition (Liu et al., 2011b; Ritter et al., 2011) as well as on parsing and part-ofspeech tagging (Foster et al., 2011). The other approach is normalization. Rather than tailoring a NLP tool towards the data, normalization seeks to tailor the data towards the tool. This is accomplished by transforming the data into a form more akin to the formal text that NLP tools are generally trained on. While normalization is often more straightforward and more easily applied in instances in which retraining is difficult or impractical, it has potential disadvantages as well, such as the potential loss of pragmatic nuance (Baldwin"
N15-1045,P12-1109,0,0.0194015,"ech recognition (Kobus et al., 2008). Other approaches include creating automatic abbreviations via a maximum entropy classifier (Pennell and Liu, 2010), creating word association graphs (Sonmez and Ozgur, 2014), and incorporating both rules and statistical models (Beaufort et al., 2010). While most initial approaches used supervised methods, unsupervised methods have recently become popular (Cook and Stevenson, 2009; Liu et al., 2011a; Yang and Eisenstein, 2013; Li and Liu, 2014). Some work has chosen to focus on specific aspects of the normalization process, such as providing good coverage (Liu et al., 2012) or building normalization dictionaries (Han et al., 2012). In all of the work mentioned above, the normalization task was seen primarily as one of converting non-standard tokens into an equivalent standard form. Similarly, many of these works defined the problem even more narrowly such that punctuation, capitalization, and multi-word replacements were ignored. However, two pieces of recent work have suggested that this understanding of the normalization task is too narrow, as it ignores many other hallmarks of informal writing that are prevalent in social media data. Wang and Ng (2013) presen"
N15-1045,de-marneffe-etal-2006-generating,0,0.0121153,"Missing"
N15-1045,I11-1109,0,0.0145463,"l media, the normalization process was primarily seen as one of standardizing non-standard tokens found in otherwise clean text, such as numbers, dates, and acronyms (Sproat et al., 2001). However, the current popularity of Twitter and other informal texts has caused the normalization task to take on a broader meaning in these contexts, where the goal is to convert informal text into formal text that downstream applications expect. Many different approaches to social media normalization have been undertaken. These approaches often draw inspiration from other tasks such as machine translation (Pennell and Liu, 2011; Aw et al., 2006), spell checking (Choudhury et al., 2007) or 421 speech recognition (Kobus et al., 2008). Other approaches include creating automatic abbreviations via a maximum entropy classifier (Pennell and Liu, 2010), creating word association graphs (Sonmez and Ozgur, 2014), and incorporating both rules and statistical models (Beaufort et al., 2010). While most initial approaches used supervised methods, unsupervised methods have recently become popular (Cook and Stevenson, 2009; Liu et al., 2011a; Yang and Eisenstein, 2013; Li and Liu, 2014). Some work has chosen to focus on specific a"
N15-1045,D11-1141,0,0.0214611,"in Section 6. 2 Related Work Twitter and other social media data is littered with non-standard word forms and other informal usage patterns, making it difficult for many NLP tools to produce results comparable to what is seen on formal datasets. There are two approaches proposed in the literature to handle this problem (Eisenstein, 2013). One approach is to tailor a specific NLP tool towards the data, by using training data from the domain to help the tool learn its specific idiosyncrasies. This approach has been applied with reasonable success on named entity recognition (Liu et al., 2011b; Ritter et al., 2011) as well as on parsing and part-ofspeech tagging (Foster et al., 2011). The other approach is normalization. Rather than tailoring a NLP tool towards the data, normalization seeks to tailor the data towards the tool. This is accomplished by transforming the data into a form more akin to the formal text that NLP tools are generally trained on. While normalization is often more straightforward and more easily applied in instances in which retraining is difficult or impractical, it has potential disadvantages as well, such as the potential loss of pragmatic nuance (Baldwin and Chai, 2011). Prior"
N15-1045,D14-1037,0,0.012331,"zation task to take on a broader meaning in these contexts, where the goal is to convert informal text into formal text that downstream applications expect. Many different approaches to social media normalization have been undertaken. These approaches often draw inspiration from other tasks such as machine translation (Pennell and Liu, 2011; Aw et al., 2006), spell checking (Choudhury et al., 2007) or 421 speech recognition (Kobus et al., 2008). Other approaches include creating automatic abbreviations via a maximum entropy classifier (Pennell and Liu, 2010), creating word association graphs (Sonmez and Ozgur, 2014), and incorporating both rules and statistical models (Beaufort et al., 2010). While most initial approaches used supervised methods, unsupervised methods have recently become popular (Cook and Stevenson, 2009; Liu et al., 2011a; Yang and Eisenstein, 2013; Li and Liu, 2014). Some work has chosen to focus on specific aspects of the normalization process, such as providing good coverage (Liu et al., 2012) or building normalization dictionaries (Han et al., 2012). In all of the work mentioned above, the normalization task was seen primarily as one of converting non-standard tokens into an equival"
N15-1045,N13-1050,0,0.106932,"ewswire data. One possible solution to this problem is normalization, in which the informal text is converted into a more standard formal form. Because of this, the rise of social media data has coincided with a rise in interest in the normalization problem. Unfortunately, while many approaches to the problem exist, there are notable limitations to the ∗ Yunyao Li IBM Research - Almaden 650 Harry Road San Jose, CA 95120, USA yunyaoli@us.ibm.com Work was done while at IBM Research - Almaden. Some recent work has given credence to the idea that application-targeted normalization is appropriate (Wang and Ng, 2013; Zhang et al., 2013). However, how certain normalization actions influence the overall performance of these applications is not well understood. To address this, we design a taxonomy of possible normalization edits based on inspiration from previous work and an examination of annotated data. We then use this taxonomy to examine the importance of individual normalization actions on three different downstream applications: dependency parsing, named entity recognition, and textto-speech synthesis. The results suggest that the importance of a given normalization edit is highly dependent on the ta"
N15-1045,D13-1007,0,0.0483551,"ften draw inspiration from other tasks such as machine translation (Pennell and Liu, 2011; Aw et al., 2006), spell checking (Choudhury et al., 2007) or 421 speech recognition (Kobus et al., 2008). Other approaches include creating automatic abbreviations via a maximum entropy classifier (Pennell and Liu, 2010), creating word association graphs (Sonmez and Ozgur, 2014), and incorporating both rules and statistical models (Beaufort et al., 2010). While most initial approaches used supervised methods, unsupervised methods have recently become popular (Cook and Stevenson, 2009; Liu et al., 2011a; Yang and Eisenstein, 2013; Li and Liu, 2014). Some work has chosen to focus on specific aspects of the normalization process, such as providing good coverage (Liu et al., 2012) or building normalization dictionaries (Han et al., 2012). In all of the work mentioned above, the normalization task was seen primarily as one of converting non-standard tokens into an equivalent standard form. Similarly, many of these works defined the problem even more narrowly such that punctuation, capitalization, and multi-word replacements were ignored. However, two pieces of recent work have suggested that this understanding of the norm"
N15-1045,P13-1114,1,0.856404,"ossible solution to this problem is normalization, in which the informal text is converted into a more standard formal form. Because of this, the rise of social media data has coincided with a rise in interest in the normalization problem. Unfortunately, while many approaches to the problem exist, there are notable limitations to the ∗ Yunyao Li IBM Research - Almaden 650 Harry Road San Jose, CA 95120, USA yunyaoli@us.ibm.com Work was done while at IBM Research - Almaden. Some recent work has given credence to the idea that application-targeted normalization is appropriate (Wang and Ng, 2013; Zhang et al., 2013). However, how certain normalization actions influence the overall performance of these applications is not well understood. To address this, we design a taxonomy of possible normalization edits based on inspiration from previous work and an examination of annotated data. We then use this taxonomy to examine the importance of individual normalization actions on three different downstream applications: dependency parsing, named entity recognition, and textto-speech synthesis. The results suggest that the importance of a given normalization edit is highly dependent on the task, making the “one s"
N15-1045,P10-1079,0,\N,Missing
N18-3010,C16-2056,1,0.835976,"ns of MBs) and document collections (e.g., 500 million new tweets posted daily on Twitter; terabytes of system logs produced hourly in a 78 the modules are loaded on demand and executed. 3.2 the multilingual support via AQL without having to configure or manage any additional resources. Language expansion is enabled as described in Sec. 3.4. SystemT also has advanced primitives for semantic role labeling (SRL), the task of labeling predicate-argument structure in sentences with shallow semantic information. Such advanced primitives enable the creation of cross-lingual TU programs (see, e.g., (Akbik et al., 2016).) Expressivity AQL1 is similar in syntax to the database language SQL, chosen for its expressivity and its familiarity to enterprise developers. It provides a number of TU constructs, including primitive extraction operators for finding parts of speech, matches of regular expressions and dictionaries, as well as set operators such as sequence, union, filter and consolidate. Each operator implements a single basic atomic operation, producing and consuming sets of tuples. AQL developers create TU programs by composing these operators together into sets of rules, or statements. 3.2.3 Corpora can"
N18-3010,P12-3019,1,0.846026,"L constructs. For example, the matching of AssetClassSuffixes dictionary on lemma form in Fig. 2 is enabled by the underlying tokenizer and lemmatizer for the given language. This has two advantages: (1) isolating language-dependent primitives from the rest of the system, without requiring changes to the AQL language itself; and (2) leveraging newer primitive models as they 3.6 Integrated Development Environments SystemT provides integrated development environments (IDEs) designed for a wide spectrum of users. The professional IDE allows expert developers to create complex TU programs in AQL (Li et al., 2012). The visual IDE enables novice users and non-programmers to construct drag-and-drop TU programs without learning AQL (Li et al., 2015). Both IDEs leverage ML and humancomputer interaction techniques as those summarized in (Chiticariu et al., 2015) to support typical development life cycles of TU tasks (Fig. 5). 3.7 Empirical Evaluation In addition to theoretical studies of AQL expressivity and runtime performance, we have also evaluated SystemT empirically on multiple TU tasks. We show that extractors built in AQL yield results of comparable quality to the best published results 81 Domain App"
N18-3010,N15-1045,1,0.806629,"single basic atomic operation, producing and consuming sets of tuples. AQL developers create TU programs by composing these operators together into sets of rules, or statements. 3.2.3 Corpora can introduce a variety of additional TU challenges, including having a high degree of noise (e.g., non-standard word forms or informal usage patterns), exposing data through nonfree-text structures (e.g., tables), or existing in a difficult-to-digest format (e.g., PDF). We have extended the functionality of SystemT by creating pre-built libraries with advanced TU capabilities such as text normalization (Baldwin and Li, 2015), semantic table processing (Chen et al., 2017), and document format conversion. 3.2.1 Basic Primitives Three of the most basic operators of AQL include: Extract (E) performs character level operations such as regular expression and dictionary matching over text, creating a tuple for each match. Select (τ ) applies a predicate to a set of tuples, and outputs all those tuples that satisfy the predicate. Join (./) applies a predicate to pairs of tuples from two sets of input tuples, outputting all pairs that satisfy the predicate. AQL also provides a sequence pattern notation, similar in its gra"
N18-3010,D12-1012,1,0.846183,"in a single paper. Education Impact. SystemT is available to teachers and students under a free academic license. We have developed a full graduate-level course on text understanding using SystemT, which has been taught in several universities. A version of this class has been made available3 as a MOOC with 10,000+ students enrolled in less than 18 months. Figure 5: Development life cycles using SystemT on several competition datasets, while achieving orders of magnitude speed-up in processing time, and requiring smaller memory utilization (Krishnamurthy et al., 2009; Chiticariu et al., 2010; Nagesh et al., 2012; Wang et al., 2017). 4 5 Conclusion In this paper, we discuss new challenges posed by enterprise applications to text understanding (TU) systems. We present SystemT, an industrialstrength system for developing end-to-end TU applications in a declarative fashion. We highlight the key design decisions and discuss how they help meet the needs of the enterprise setting. SystemT has been used to build enterprise applications in a wide range of domains, and is publicly available for commercial and academic usage. Impact of SystemT Business Impact. Started as a research prototype, SystemT has been w"
N18-3010,P10-1014,1,0.894796,"se TU, no matter how well designed, might not provide all of the capabilities required by a real-world use case out-of-the-box. As such, it should be extensible to gracefully handle tasks that are not natively supported. cading grammar formalism where the input text is viewed as a sequence of annotations, and extraction rules are written as pattern/action rules over the lexical features of these annotations. Each grammar consists of a set of rules evaluated in a left-to-right fashion over the input annotations, with multiple grammars cascaded together and evaluated bottom-up. As discussed in (Chiticariu et al., 2010), grammar-based systems suffer from two fundamental limitations: expressivity and performance. Formal studies of AQL semantics have shown that AQL is strictly more expressive than regular expression-based languages such as CPSL (Sec. 3.2.5). Furthermore, the rigid evaluation order imposed in grammar-based systems has significant runtime performance implications, as the system is effectively forced into a fixed pipeline execution strategy, leaving little opportunity for global optimization. While the expressivity of grammar-based systems has been extended in different ways, such as additional b"
N18-3010,D15-2003,1,0.758581,"ves from the rest of the system, without requiring changes to the AQL language itself; and (2) leveraging newer primitive models as they 3.6 Integrated Development Environments SystemT provides integrated development environments (IDEs) designed for a wide spectrum of users. The professional IDE allows expert developers to create complex TU programs in AQL (Li et al., 2012). The visual IDE enables novice users and non-programmers to construct drag-and-drop TU programs without learning AQL (Li et al., 2015). Both IDEs leverage ML and humancomputer interaction techniques as those summarized in (Chiticariu et al., 2015) to support typical development life cycles of TU tasks (Fig. 5). 3.7 Empirical Evaluation In addition to theoretical studies of AQL expressivity and runtime performance, we have also evaluated SystemT empirically on multiple TU tasks. We show that extractors built in AQL yield results of comparable quality to the best published results 81 Domain Application Multilingual named entity extraction for document retention Element extraction and classification in legal documents (e.g. contract, regulations) Compliance Named entity extraction for document retention and regulation compliance Entity an"
N18-3010,L16-1050,0,0.0643457,"Missing"
N18-3010,D13-1079,1,0.841482,"ecent work on hardware acceleration for low level text operators such as regular expressions (Atasu et al., 2013) can be leveraged by extending the Optimizer’s search space and cost model to incorporate alternative hardware implementations of individual operators and associated cost model (Polig et al., 2018). 80 become available, without requiring changes to existing AQL programs. 3.5 A common criticism of pure machine learning (ML) systems in the enterprise world is that statistical models are opaque to the application using them, making the results difficult to explain or be quickly fixed (Chiticariu et al., 2013). SystemT addresses this challenge by using a declarative language for specifying the TU program. Since the results are produced by constructs with wellunderstood semantics, it is possible to automatically generate explanations of why a certain output was or was not produced. At the same time, SystemT has the flexibility to leverage ML techniques in the context of its overall declarative framework in two dimensions. First, primitive APIs and the user-defined interface (Sec. 3.4) allow for plugging in low-level NLP primitives, as well as trained models for higherlevel tasks such as entity extra"
P10-1014,X98-1004,0,0.927468,"Missing"
P10-1014,H05-1056,0,0.0566838,", the tuples that a given view produces are in no way constrained by the outputs of other, unrelated views, so the rigid matching priority problem never occurs. Finally, the select statement in AQL allows arbitrary predicates over the cross-product of its input tuple sets, eliminating the limited expressivity in rule patterns problem. Beyond eliminating the major limitations of CPSL grammars, AQL provides a number of other information extraction operations that even extended CPSL cannot express without custom code. Complex rule interactions. Consider an example document from the Enron corpus (Minkov et al., 2005), shown in Fig. 7, which contains a list of person names. Because the first person in the list (‘Skilling’) is referred to by only a last name, rule P2 R3 in Fig. 1 incorrectly identifies ‘Skilling, Cindy’ as a person. Consequently, the output of phase P2 of the cascading grammar contains several mistakes as shown in the figure. This problem Deployment Scenarios SystemT is designed to be usable in various deployment scenarios. It can be used as a standalone system with its own development and runtime environment. Furthermore, SystemT exposes a generic Java API that enables the integration of i"
P10-1014,W06-1658,0,0.0208609,"sharing work across different rules, or selectively skipping rule evaluations. Within this large search space, there generally exists an execution strategy that implements the rule semantics far more efficiently than the fastest transducer could. We refer the reader to (Reiss et al., 2008) for a detailed description of the types of plan the optimizer considers, as well as an experimental analysis of the performance benefits of different parts of this search space. Several parallel efforts have been made recently to improve the efficiency of IE tasks by optimizing low-level feature extraction (Ramakrishnan et al., 2006; Ramakrishnan et al., 2008; Chandel et al., 2006) or by reordering operations at a macroscopic level (Ipeirotis et al., 2006; Shen et al., 2007; Jain et al., 2009). However, to the best of our knowledge, SystemT is the only IE system in which the optimizer generates a full end-to-end plan, beginning with low-level extraction primitives and ending with the final output tuples. 3.4 Figure 7: Supporting Complex Rule Interactions used in SystemT, we now compare the two in terms of expressivity and performance. 4.1 Expressivity In Section 2, we described three expressivity limitations of CPSL gram"
P10-1014,C96-1079,0,0.0538978,"en San Jose, CA, USA {chiti,sekar,yunyaoli,rsriram,frreiss,vaithyan}@us.ibm.com Abstract Common Pattern Specification Language (CPSL) specification (Appelt and Onyshkevych, 1998). In CPSL, the input text is viewed as a sequence of annotations, and extraction rules are written as pattern/action rules over the lexical features of these annotations. In a single phase of the grammar, a set of rules are evaluated in a left-to-right fashion over the input annotations. Multiple grammar phases are cascaded together, with the evaluation proceeding in a bottom-up fashion. As demonstrated by prior work (Grishman and Sundheim, 1996), grammar-based IE systems can be effective in many scenarios. However, these systems suffer from two severe drawbacks. First, the expressivity of CPSL falls short when used for complex IE tasks over increasingly pervasive informal text (emails, blogs, discussion forums etc.). To address this limitation, grammar-based IE systems resort to significant amounts of userdefined code in the rules, combined with preand post-processing stages beyond the scope of CPSL (Cunningham et al., 2010). Second, the rigid evaluation order imposed in these systems has significant performance implications. Three d"
P10-1014,P02-1022,0,\N,Missing
P11-1091,H05-1120,0,0.0950592,"is a misspelling of attachment due to both the (relative) proximity between the words, and the fact that attachment is significantly more popular than atachment. As another example, the fact that the expression sandeep kohli is frequent in the domain increases our confidence in sadeep kohli → sandeep kohli (rather than, e.g., sadeep kohli → sudeep kohli). One can further note that, in email search, the fact that Sandeep Kohli sent multiple excel attachments increases our confidence in excell → excel. A source of statistics widely used in prior work is the query log (Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005; Li et al., 2006a; Chen et al., 2007; Sun et al., 2010). However, while query logs are abundant in the context of Web search, in many 906 other search applications (e.g. email search, desktop search, and even small-enterprise search) query logs are too scarce to provide statistical information that is sufficient for effective spelling correction. Even an email provider of a massive scale (such as GMail) may need to rely on the (possibly tiny) query log of the single user at hand, due to privacy or security concerns; moreover, as noted earlier about kohli, the statistics of one user may be rel"
P11-1091,D07-1019,0,0.0934587,"the (relative) proximity between the words, and the fact that attachment is significantly more popular than atachment. As another example, the fact that the expression sandeep kohli is frequent in the domain increases our confidence in sadeep kohli → sandeep kohli (rather than, e.g., sadeep kohli → sudeep kohli). One can further note that, in email search, the fact that Sandeep Kohli sent multiple excel attachments increases our confidence in excell → excel. A source of statistics widely used in prior work is the query log (Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005; Li et al., 2006a; Chen et al., 2007; Sun et al., 2010). However, while query logs are abundant in the context of Web search, in many 906 other search applications (e.g. email search, desktop search, and even small-enterprise search) query logs are too scarce to provide statistical information that is sufficient for effective spelling correction. Even an email provider of a massive scale (such as GMail) may need to rely on the (possibly tiny) query log of the single user at hand, due to privacy or security concerns; moreover, as noted earlier about kohli, the statistics of one user may be relevant to one user, while irrelevant t"
P11-1091,P10-1014,1,0.826293,"Missing"
P11-1091,W04-3238,0,0.866658,"edge of the exact spelling of q. The goal is to restore q, when given s. Spelling correction has been extensively studied in the literature, and we refer the reader to comprehensive summaries of prior work (Peterson, 1980; Kukich, 1992; Jurafsky and Martin, 2000; Mitton, 2010). The focus of this paper is on the special case where q is a search query, and where s instead of q is submitted to a search engine (with the goal of retrieving documents that match the search query q). Spelling correction for search queries is important, because a significant portion of posed queries may be misspelled (Cucerzan and Brill, 2004). Effective 905 Yunyao Li IBM Research–Almaden San Jose, CA 95120, USA yunyaoli@us.ibm.com spelling correction has a major effect on the experience and effort of the user, who is otherwise required to ensure the exact spellings of her queries. Furthermore, it is critical when the exact spelling is unknown (e.g., person names like Schwarzenegger). 1.1 Spelling Errors The more common and studied type of spelling error is word-to-word error: a single word w is misspelled into another single word w0 . The specific spelling errors involved include omission of a character (e.g., atachment), inclusio"
P11-1091,J00-4006,0,0.0197985,"he algorithm over existing alternatives in the email domain. 1 Introduction An abundance of applications require spelling correction, which (at the high level) is the following task. The user intends to type a chunk q of text, but types instead the chunk s that contains spelling errors (which we discuss in detail later), due to uncareful typing or lack of knowledge of the exact spelling of q. The goal is to restore q, when given s. Spelling correction has been extensively studied in the literature, and we refer the reader to comprehensive summaries of prior work (Peterson, 1980; Kukich, 1992; Jurafsky and Martin, 2000; Mitton, 2010). The focus of this paper is on the special case where q is a search query, and where s instead of q is submitted to a search engine (with the goal of retrieving documents that match the search query q). Spelling correction for search queries is important, because a significant portion of posed queries may be misspelled (Cucerzan and Brill, 2004). Effective 905 Yunyao Li IBM Research–Almaden San Jose, CA 95120, USA yunyaoli@us.ibm.com spelling correction has a major effect on the experience and effort of the user, who is otherwise required to ensure the exact spellings of her qu"
P11-1091,C90-2036,0,0.0871563,"s the user may very well mean kohls coupons if Sandeep Kohli has nothing to do with coupons (in contrast to the store chain Kohl’s). A similar example is the word nail, which is a legitimate English word, but in the context of email the query nail box is likely to be a misspelling of mail box (unless nail boxes are indeed relevant to the user’s email collection). Finally, while the word kohli is relevant to some email users (e.g., Kohli’s colleagues), it may have no meaning at all to other users. 1.2 Domain Knowledge The common approach to spelling correction utilizes statistical information (Kernighan et al., 1990; Schierle et al., 2007; Mitton, 2010). As a simple example, if we want to avoid maintaining a manually-crafted dictionary to accommodate the wealth of new terms introduced every day (e.g., ipod and ipad), we may decide that atachment is a misspelling of attachment due to both the (relative) proximity between the words, and the fact that attachment is significantly more popular than atachment. As another example, the fact that the expression sandeep kohli is frequent in the domain increases our confidence in sadeep kohli → sandeep kohli (rather than, e.g., sadeep kohli → sudeep kohli). One can"
P11-1091,P06-1129,0,0.0704718,"chment due to both the (relative) proximity between the words, and the fact that attachment is significantly more popular than atachment. As another example, the fact that the expression sandeep kohli is frequent in the domain increases our confidence in sadeep kohli → sandeep kohli (rather than, e.g., sadeep kohli → sudeep kohli). One can further note that, in email search, the fact that Sandeep Kohli sent multiple excel attachments increases our confidence in excell → excel. A source of statistics widely used in prior work is the query log (Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005; Li et al., 2006a; Chen et al., 2007; Sun et al., 2010). However, while query logs are abundant in the context of Web search, in many 906 other search applications (e.g. email search, desktop search, and even small-enterprise search) query logs are too scarce to provide statistical information that is sufficient for effective spelling correction. Even an email provider of a massive scale (such as GMail) may need to rely on the (possibly tiny) query log of the single user at hand, due to privacy or security concerns; moreover, as noted earlier about kohli, the statistics of one user may be relevant to one user"
P11-1091,P10-1028,0,0.566992,"ximity between the words, and the fact that attachment is significantly more popular than atachment. As another example, the fact that the expression sandeep kohli is frequent in the domain increases our confidence in sadeep kohli → sandeep kohli (rather than, e.g., sadeep kohli → sudeep kohli). One can further note that, in email search, the fact that Sandeep Kohli sent multiple excel attachments increases our confidence in excell → excel. A source of statistics widely used in prior work is the query log (Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005; Li et al., 2006a; Chen et al., 2007; Sun et al., 2010). However, while query logs are abundant in the context of Web search, in many 906 other search applications (e.g. email search, desktop search, and even small-enterprise search) query logs are too scarce to provide statistical information that is sufficient for effective spelling correction. Even an email provider of a massive scale (such as GMail) may need to rely on the (possibly tiny) query log of the single user at hand, due to privacy or security concerns; moreover, as noted earlier about kohli, the statistics of one user may be relevant to one user, while irrelevant to another. The focu"
P11-4019,P10-1014,1,0.93769,"traditional requirement of extraction quality remains critical, enterprise applications pose several two challenges to IE systems: 1.Scalability: Enterprise applications operate over large volumes of data, often orders of Traditionally, IE systems have been built from individual extraction components consisting of rules or machine learning models. These individual components are then connected procedurally in a programming language such as C++, Perl or Java. Such procedural logic towards IE cannot meet the increasing scalability and usability requirements in the enterprise (Doan et al., 2006; Chiticariu et al., 2010a). Three decades ago, the database community faced similar scalability and expressivity challenges in accessing structured information. The community addressed these problems by introducing a relational algebra formalism and an associated declarative query language SQL. Borrowing ideas from the database community, several systems (Doan and others, 2008; Bohannon and others, 2008; Jain et al., 2009; Krishnamurthy et al., 2008; Wang et al., 2010) have been built in recent years taking an alternative declarative approach to information extraction. Instead of using procedural logic to implement t"
P11-4019,D10-1098,1,0.803385,"Missing"
P11-4019,X98-1004,0,\N,Missing
P12-3019,P10-1014,1,0.943279,"enabling extractor development for novice IE developers. 1 Introduction Information Extraction (IE) refers to the problem of extracting structured information from unstructured or semi-structured text. It has been well-studied by the Natural Language Processing research community for a long time. In recent years, IE has emerged as a critical building block in a wide range of enterprise applications, including financial risk analysis, social media analytics and regulatory compliance, among many others. An important practical challenge driven by the use of IE in these applications is usability (Chiticariu et al., 2010c): specifically, how to enable the ease of development and maintenance of high-quality information extraction rules, also known as annotators, or extractors. Developing extractors is a notoriously laborintensive and time-consuming process. In order to ensure highly accurate and reliable results, this task is traditionally performed by trained linguists with domain expertise. As a result, extractor development is regarded as a major bottleneck in satisfying the increasing text analytics demands of enterprise applications. Hence, reducing the extractor development life cycle is a critical requi"
P12-3019,D10-1098,1,0.943382,"enabling extractor development for novice IE developers. 1 Introduction Information Extraction (IE) refers to the problem of extracting structured information from unstructured or semi-structured text. It has been well-studied by the Natural Language Processing research community for a long time. In recent years, IE has emerged as a critical building block in a wide range of enterprise applications, including financial risk analysis, social media analytics and regulatory compliance, among many others. An important practical challenge driven by the use of IE in these applications is usability (Chiticariu et al., 2010c): specifically, how to enable the ease of development and maintenance of high-quality information extraction rules, also known as annotators, or extractors. Developing extractors is a notoriously laborintensive and time-consuming process. In order to ensure highly accurate and reliable results, this task is traditionally performed by trained linguists with domain expertise. As a result, extractor development is regarded as a major bottleneck in satisfying the increasing text analytics demands of enterprise applications. Hence, reducing the extractor development life cycle is a critical requi"
P12-3019,P02-1022,0,0.023188,"loped during the Rule Development phase. The rules are profiled and further fine-tuned in the Performance Tuning phase, to ensure high runtime performance. Finally, in the Delivery phase, the rules are packaged so that they can be easily embedded in various applications. WizIE is designed to assist and enable both novice and experienced developers by providing an intuitive wizard-like interface that is informed by the best practices in extractor development throughout each of these phases. By doing so, WizIE seeks to provide the key missing pieces in a conventional IE development environment (Cunningham et al., 2002; Li et al., 2011b; Soundrarajan et al., 2011), based on our experience as expert IE developers, as well as our interactions with novice developers with general computer science background, but little text analytics experience, during the development of several enterprise applications. 3 The Development Environment In this section, we present the general functionality of WizIE in the context of extraction tasks driven by real business use cases from the media and entertainment domain. We describe WizIE in details and show how it guides and assists IE developers in a step-by-step fashion, based"
P12-3019,D08-1003,1,0.821289,"ajor bottleneck in satisfying the increasing text analytics demands of enterprise applications. Hence, reducing the extractor development life cycle is a critical requirement. Towards this goal, we have built WizIE, an IE development environment designed primarily to (1) enable developers with little or no linguistic background to write high quality extractors, and (2) reduce the overall manual effort involved in extractor development. Previous work on improving the usability of IE systems has mainly focused on reducing the manual effort involved in extractor development (Brauer et al., 2011; Li et al., 2008; Li et al., 2011a; Soderland, 1999; Liu et al., 2010). In contrast, the focus of WizIE is on lowering the extractor development entry barrier by means of a wizard-like environment that guides extractor development based on best practices drawn from the experience of trained linguists and expert developers. In doing so, WizIE also provides natural entry points for different tools focused on reducing the effort required for performing common tasks during IE development. Underlying our WizIE are a state-of-the-art IE rule language and corresponding runtime engine (Chiticariu et al., 2010a; Li et"
P12-3019,P11-4019,1,0.853324,"n satisfying the increasing text analytics demands of enterprise applications. Hence, reducing the extractor development life cycle is a critical requirement. Towards this goal, we have built WizIE, an IE development environment designed primarily to (1) enable developers with little or no linguistic background to write high quality extractors, and (2) reduce the overall manual effort involved in extractor development. Previous work on improving the usability of IE systems has mainly focused on reducing the manual effort involved in extractor development (Brauer et al., 2011; Li et al., 2008; Li et al., 2011a; Soderland, 1999; Liu et al., 2010). In contrast, the focus of WizIE is on lowering the extractor development entry barrier by means of a wizard-like environment that guides extractor development based on best practices drawn from the experience of trained linguists and expert developers. In doing so, WizIE also provides natural entry points for different tools focused on reducing the effort required for performing common tasks during IE development. Underlying our WizIE are a state-of-the-art IE rule language and corresponding runtime engine (Chiticariu et al., 2010a; Li et al., 2011b). The"
P12-3019,P11-4024,0,0.0196442,"e rules are profiled and further fine-tuned in the Performance Tuning phase, to ensure high runtime performance. Finally, in the Delivery phase, the rules are packaged so that they can be easily embedded in various applications. WizIE is designed to assist and enable both novice and experienced developers by providing an intuitive wizard-like interface that is informed by the best practices in extractor development throughout each of these phases. By doing so, WizIE seeks to provide the key missing pieces in a conventional IE development environment (Cunningham et al., 2002; Li et al., 2011b; Soundrarajan et al., 2011), based on our experience as expert IE developers, as well as our interactions with novice developers with general computer science background, but little text analytics experience, during the development of several enterprise applications. 3 The Development Environment In this section, we present the general functionality of WizIE in the context of extraction tasks driven by real business use cases from the media and entertainment domain. We describe WizIE in details and show how it guides and assists IE developers in a step-by-step fashion, based on best practices. 3.1 Task Analysis The high"
P13-1114,P06-2005,0,0.144607,"work is notable for attempting to develop normalization as a general process that could be applied to different domains. The recent rise of heavily informal writing styles such as Twitter and SMS messages set off a new round of interest in the normalization problem. Research on SMS and Twitter normalization has been roughly categorized as drawing inspiration from three other areas of NLP (Kobus et al., 2008): machine translation, spell checking, and automatic speech recognition. The statistical machine translation (SMT) metaphor was the first proposed to handle the text normalization problem (Aw et al., 2006). In this mindset, normalizing SMS can be seen as a translation task from a source language (informal) to a target language (formal), which can be undertaken with typical noisy channel based models. Work by Choudhury et al. (2007) adopted the spell checking metaphor, casting the problem in terms of character-level, rather than word-level, edits. They proposed an HMM based model that takes into account both grapheme and phoneme information. Kobus et al. (2008) undertook a hybrid approach that pulls inspiration from both the machine translation and speech recognition metaphors. Many other approa"
P13-1114,P11-1091,1,0.841703,"(Beaufort et al., 2010; Pennell and Liu, 2010), these works do not give any direct insight into how much the normalization process actually improves the performance of these systems. To our knowledge, the work presented here is the first to clearly link the output of a normalization system to the output of the downstream application. Similarly, our work is the first to prioritize domain adaptation during the new wave of text message normalization. 3 Model In this section we introduce our normalization framework, which draws inspiration from our previous work on spelling correction for search (Bao et al., 2011). 3.1 Replacement Generators Our input the original, unnormalized text, represented as a sequence x = x1 , x2 , . . . , xn of tokens xi . In this section we will use the following se1160 quence as our running example: x = Ay1 woudent2 of3 see4 0 em5 where space replaces comma for readability, and each token is subscripted by its position. Given the input x, we apply a series of replacement generators, where a replacement generator is a function that takes x as input and produces a collection of replacements. Here, a replacement is a statement of the form “replace tokens xi , . . . , xj−1 with"
P13-1114,P10-1014,1,0.882707,"Missing"
P13-1114,W02-1001,0,0.0150592,"n: Initialize each θj as zero, and obtain each αigold according to (1). Repeat T times: 1. Infer each αi∗ from xi using the current Θ; P 2. θj ← θj + i (Φj (αigold , xi )−Φj (αi∗ , xi )) for all j = 1, . . . , k. Figure 3: Learning algorithm path defined by α, and Ep(αi |xi ,Θ) Φj (αi , xi ) is the expected value of that sum (over all legal assignments αi ), assuming the current weight vector. How to efficiently compute Ep(αi |xi ,Θ) Φj (αi , xi ) in our model is unclear; naively, it requires enumerating all legal assignments. We instead opt to use a more tractable perceptron-style algorithm (Collins, 2002). Instead of computing the expectation, we simply compute Φj (αi∗ , xi ), where αi∗ is the assignment with the highest probability, generated using the current weight vector. The result is then:  X Φj (αigold , xi ) − Φj (αi∗ , xi ) i Our learning applies the following two steps iteratively. (1) Generate the most probable sequence within the current weights. (2) Update the weights by comparing the path generated in the previous step to the gold standard path. The algorithm in Figure 3 summarizes the procedure. Instantiation In this section, we discuss our instantiation of the model presented"
P13-1114,W09-2010,0,0.135497,"l) to a target language (formal), which can be undertaken with typical noisy channel based models. Work by Choudhury et al. (2007) adopted the spell checking metaphor, casting the problem in terms of character-level, rather than word-level, edits. They proposed an HMM based model that takes into account both grapheme and phoneme information. Kobus et al. (2008) undertook a hybrid approach that pulls inspiration from both the machine translation and speech recognition metaphors. Many other approaches have been examined, most of which are at least partially reliant on the above three metaphors. Cook and Stevenson (2009) perform an unsupervised method, again based on the noisy channel model. Pennell and Liu (2011) developed a CRF tagger for deletion-based abbreviation on tweets. Xue et al. (2011) incorporated orthographic, phonetic, contextual, and acronym expansion factors to normalize words in both Twitter and SMS. Liu et al. (2011) modeled the generation process from dictionary words to non-standard tokens under an unsupervised sequence labeling framework. Han and Baldwin (2011) use a classifier to detect illformed words, and then generate correction candidates based on morphophonemic similarity. Recent wo"
P13-1114,P11-1038,0,0.634243,"etaphors. Many other approaches have been examined, most of which are at least partially reliant on the above three metaphors. Cook and Stevenson (2009) perform an unsupervised method, again based on the noisy channel model. Pennell and Liu (2011) developed a CRF tagger for deletion-based abbreviation on tweets. Xue et al. (2011) incorporated orthographic, phonetic, contextual, and acronym expansion factors to normalize words in both Twitter and SMS. Liu et al. (2011) modeled the generation process from dictionary words to non-standard tokens under an unsupervised sequence labeling framework. Han and Baldwin (2011) use a classifier to detect illformed words, and then generate correction candidates based on morphophonemic similarity. Recent work has looked at the construction of normalization dictionaries (Han et al., 2012) and on improving coverage by integrating different human perspectives (Liu et al., 2012). Although it is almost universally used as a motivating factor, most normalization work does not directly focus on improving downstream applications. While a few notable exceptions highlight the need for normalization as part of textto-speech systems (Beaufort et al., 2010; Pennell and Liu, 2010),"
P13-1114,D12-1039,0,0.0257399,"model. Pennell and Liu (2011) developed a CRF tagger for deletion-based abbreviation on tweets. Xue et al. (2011) incorporated orthographic, phonetic, contextual, and acronym expansion factors to normalize words in both Twitter and SMS. Liu et al. (2011) modeled the generation process from dictionary words to non-standard tokens under an unsupervised sequence labeling framework. Han and Baldwin (2011) use a classifier to detect illformed words, and then generate correction candidates based on morphophonemic similarity. Recent work has looked at the construction of normalization dictionaries (Han et al., 2012) and on improving coverage by integrating different human perspectives (Liu et al., 2012). Although it is almost universally used as a motivating factor, most normalization work does not directly focus on improving downstream applications. While a few notable exceptions highlight the need for normalization as part of textto-speech systems (Beaufort et al., 2010; Pennell and Liu, 2010), these works do not give any direct insight into how much the normalization process actually improves the performance of these systems. To our knowledge, the work presented here is the first to clearly link the o"
P13-1114,C08-1056,0,0.128708,"2001) took the first major look at the normalization problem, citing the need for normalized text for downstream applications. Unlike later works that would primarily focus on specific noisy data sets, their work is notable for attempting to develop normalization as a general process that could be applied to different domains. The recent rise of heavily informal writing styles such as Twitter and SMS messages set off a new round of interest in the normalization problem. Research on SMS and Twitter normalization has been roughly categorized as drawing inspiration from three other areas of NLP (Kobus et al., 2008): machine translation, spell checking, and automatic speech recognition. The statistical machine translation (SMT) metaphor was the first proposed to handle the text normalization problem (Aw et al., 2006). In this mindset, normalizing SMS can be seen as a translation task from a source language (informal) to a target language (formal), which can be undertaken with typical noisy channel based models. Work by Choudhury et al. (2007) adopted the spell checking metaphor, casting the problem in terms of character-level, rather than word-level, edits. They proposed an HMM based model that takes int"
P13-1114,P11-2013,0,0.0499154,"me information. Kobus et al. (2008) undertook a hybrid approach that pulls inspiration from both the machine translation and speech recognition metaphors. Many other approaches have been examined, most of which are at least partially reliant on the above three metaphors. Cook and Stevenson (2009) perform an unsupervised method, again based on the noisy channel model. Pennell and Liu (2011) developed a CRF tagger for deletion-based abbreviation on tweets. Xue et al. (2011) incorporated orthographic, phonetic, contextual, and acronym expansion factors to normalize words in both Twitter and SMS. Liu et al. (2011) modeled the generation process from dictionary words to non-standard tokens under an unsupervised sequence labeling framework. Han and Baldwin (2011) use a classifier to detect illformed words, and then generate correction candidates based on morphophonemic similarity. Recent work has looked at the construction of normalization dictionaries (Han et al., 2012) and on improving coverage by integrating different human perspectives (Liu et al., 2012). Although it is almost universally used as a motivating factor, most normalization work does not directly focus on improving downstream applications"
P13-1114,P12-1109,0,0.0598358,"tweets. Xue et al. (2011) incorporated orthographic, phonetic, contextual, and acronym expansion factors to normalize words in both Twitter and SMS. Liu et al. (2011) modeled the generation process from dictionary words to non-standard tokens under an unsupervised sequence labeling framework. Han and Baldwin (2011) use a classifier to detect illformed words, and then generate correction candidates based on morphophonemic similarity. Recent work has looked at the construction of normalization dictionaries (Han et al., 2012) and on improving coverage by integrating different human perspectives (Liu et al., 2012). Although it is almost universally used as a motivating factor, most normalization work does not directly focus on improving downstream applications. While a few notable exceptions highlight the need for normalization as part of textto-speech systems (Beaufort et al., 2010; Pennell and Liu, 2010), these works do not give any direct insight into how much the normalization process actually improves the performance of these systems. To our knowledge, the work presented here is the first to clearly link the output of a normalization system to the output of the downstream application. Similarly, o"
P13-1114,de-marneffe-etal-2006-generating,0,0.0787757,"Missing"
P13-1114,P02-1040,0,0.092371,"ng to the need for clean text for downstream processing applications, such as syntactic parsing. However, most studies of normalization give little insight into whether and to what degree the normalization process improves 1159 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1159–1168, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics the performance of the downstream application. For instance, it is unclear how performance measured by the typical normalization evaluation metrics of word error rate and BLEU score (Papineni et al., 2002) translates into performance on a parsing task, where a well placed punctuation mark may provide more substantial improvements than changing a non-standard word form. To address this problem, this work introduces an evaluation metric that ties normalization performance directly to the performance of a downstream dependency parser. The rest of this paper is organized as follows. In Section 2 we discuss previous approaches to the normalization problem. Section 3 presents our normalization framework, including the actual normalization and learning procedures. Our instantiation of this model is pr"
P13-1114,I11-1109,0,0.0878515,"Work by Choudhury et al. (2007) adopted the spell checking metaphor, casting the problem in terms of character-level, rather than word-level, edits. They proposed an HMM based model that takes into account both grapheme and phoneme information. Kobus et al. (2008) undertook a hybrid approach that pulls inspiration from both the machine translation and speech recognition metaphors. Many other approaches have been examined, most of which are at least partially reliant on the above three metaphors. Cook and Stevenson (2009) perform an unsupervised method, again based on the noisy channel model. Pennell and Liu (2011) developed a CRF tagger for deletion-based abbreviation on tweets. Xue et al. (2011) incorporated orthographic, phonetic, contextual, and acronym expansion factors to normalize words in both Twitter and SMS. Liu et al. (2011) modeled the generation process from dictionary words to non-standard tokens under an unsupervised sequence labeling framework. Han and Baldwin (2011) use a classifier to detect illformed words, and then generate correction candidates based on morphophonemic similarity. Recent work has looked at the construction of normalization dictionaries (Han et al., 2012) and on impro"
P13-1114,D11-1141,0,0.00434051,"the phrases induced by an edge. These features are turned into binary ones by bucketing their log values. For example, on the edge from h1, 2, Ii to h2, 3, wouldi such a feature will indicate whether the frequency of I would is over a threshold. We use the Corpus of Contemporary English (Davies, 2008 ) to produce our n-gram information. Part-of-speech: Part-of-speech information can be used to produce features that encourage certain behavior, such as avoiding the deletion of noun phrases. We generate part-of-speech information over the original raw text using a Twitter part-of-speech tagger (Ritter et al., 2011). Of course, the part-of-speech information obtained this way is likely to be noisy, and we expect our learning algorithm to take that into account. Positional: Information from positions is used primarily to handle capitalization and punctuation insertion, for example, by incorporating features for capitalized words after stop punctuation or the insertion of stop punctuation at the end of the sentence. Lineage: Finally, we include binary features 1163 that indicate which generator spawned the replacement. 5 Gold I kind of want to get a new iPad. verb(get) verb(want) verb(get) precisionv = rec"
P13-1114,P13-1155,0,\N,Missing
P13-1114,P10-1079,0,\N,Missing
P13-2140,D07-1007,0,0.0184213,"ambiguity resolution tasks, the ambiguity detection problem makes general ambiguity judgments about terms, rather than resolving individual instances. By doing so, it eliminates the need for ambiguity resolution on unambiguous objects, allowing for increased throughput of IE systems on large data sets. Our solution for the term ambiguity detection Polysemy is a known problem for many NLPrelated applications. Machine translation systems can suffer, as ambiguity in the source language may lead to incorrect translations, and unambiguous sentences in one language may become ambiguous in another (Carpuat and Wu, 2007; Chan et al., 2007). Ambiguity in queries can also hinder the performance of information retrieval systems (Wang and Agichtein, 2010; Zhong and Ng, 2012). The ambiguity detection problem is similar to 807 task is based on a combined model with three distinct modules based on n-grams, ontologies, and clustering. Our initial study suggests that the combination of different modules designed for different types of ambiguity used in our solution is effective in determining whether a term is ambiguous for a given domain. Additionally, an examination of a typical use case confirms that the proposed"
P13-2140,P07-1005,0,0.0173692,"tasks, the ambiguity detection problem makes general ambiguity judgments about terms, rather than resolving individual instances. By doing so, it eliminates the need for ambiguity resolution on unambiguous objects, allowing for increased throughput of IE systems on large data sets. Our solution for the term ambiguity detection Polysemy is a known problem for many NLPrelated applications. Machine translation systems can suffer, as ambiguity in the source language may lead to incorrect translations, and unambiguous sentences in one language may become ambiguous in another (Carpuat and Wu, 2007; Chan et al., 2007). Ambiguity in queries can also hinder the performance of information retrieval systems (Wang and Agichtein, 2010; Zhong and Ng, 2012). The ambiguity detection problem is similar to 807 task is based on a combined model with three distinct modules based on n-grams, ontologies, and clustering. Our initial study suggests that the combination of different modules designed for different types of ambiguity used in our solution is effective in determining whether a term is ambiguous for a given domain. Additionally, an examination of a typical use case confirms that the proposed solution is likely t"
P13-2140,P04-1038,0,0.0196068,"Missing"
P13-2140,P10-1014,1,0.866151,"Missing"
P13-2140,D10-1073,0,0.0388125,"Missing"
P13-2140,E12-1060,0,0.0930998,"Missing"
P13-2140,D10-1012,0,0.27625,"Missing"
P13-2140,E09-1013,0,0.200362,"Missing"
P13-2140,C08-1117,0,0.0371267,"Missing"
P13-2140,N10-1055,0,0.0305697,"individual instances. By doing so, it eliminates the need for ambiguity resolution on unambiguous objects, allowing for increased throughput of IE systems on large data sets. Our solution for the term ambiguity detection Polysemy is a known problem for many NLPrelated applications. Machine translation systems can suffer, as ambiguity in the source language may lead to incorrect translations, and unambiguous sentences in one language may become ambiguous in another (Carpuat and Wu, 2007; Chan et al., 2007). Ambiguity in queries can also hinder the performance of information retrieval systems (Wang and Agichtein, 2010; Zhong and Ng, 2012). The ambiguity detection problem is similar to 807 task is based on a combined model with three distinct modules based on n-grams, ontologies, and clustering. Our initial study suggests that the combination of different modules designed for different types of ambiguity used in our solution is effective in determining whether a term is ambiguous for a given domain. Additionally, an examination of a typical use case confirms that the proposed solution is likely to be useful in improving the performance of an IE system that does not employ any disambiguation. Although the ta"
P13-2140,P12-1029,0,0.106249,"doing so, it eliminates the need for ambiguity resolution on unambiguous objects, allowing for increased throughput of IE systems on large data sets. Our solution for the term ambiguity detection Polysemy is a known problem for many NLPrelated applications. Machine translation systems can suffer, as ambiguity in the source language may lead to incorrect translations, and unambiguous sentences in one language may become ambiguous in another (Carpuat and Wu, 2007; Chan et al., 2007). Ambiguity in queries can also hinder the performance of information retrieval systems (Wang and Agichtein, 2010; Zhong and Ng, 2012). The ambiguity detection problem is similar to 807 task is based on a combined model with three distinct modules based on n-grams, ontologies, and clustering. Our initial study suggests that the combination of different modules designed for different types of ambiguity used in our solution is effective in determining whether a term is ambiguous for a given domain. Additionally, an examination of a typical use case confirms that the proposed solution is likely to be useful in improving the performance of an IE system that does not employ any disambiguation. Although the task as presented here"
P15-1039,P98-1013,0,0.359271,"Missing"
P15-1039,W09-1206,0,0.0210451,"Missing"
P15-1039,D12-1133,0,0.0379329,"Missing"
P15-1039,P13-1104,0,0.0246752,"Missing"
P15-1039,P03-1068,0,0.217288,"Missing"
P15-1039,C10-1045,0,0.0192671,"Missing"
P15-1039,2005.mtsummit-papers.11,0,0.0895771,"Missing"
P15-1039,P13-1117,0,0.200678,"Missing"
P15-1039,C14-2018,1,0.906102,"Missing"
P15-1039,P09-1033,0,0.0637787,"Missing"
P15-1039,meyers-etal-2004-annotating,0,0.021641,"Missing"
P15-1039,W07-1513,0,0.0312468,"Missing"
P15-1039,nivre-etal-2006-maltparser,0,0.0191342,"Missing"
P15-1039,J05-1004,0,0.171074,"Missing"
P15-1039,2009.mtsummit-posters.15,0,0.124887,"Missing"
P15-1039,D07-1002,0,0.056798,"Missing"
P15-1039,Q13-1001,0,0.0184665,"Missing"
P15-1039,P12-1068,0,0.0157009,"Missing"
P15-1039,C14-1121,0,0.370291,"Missing"
P15-1039,N06-1055,0,0.113307,"Missing"
P15-1039,J08-2004,0,0.0178401,"Missing"
P15-1039,H01-1035,0,0.23456,"Missing"
P15-1039,W10-1836,0,0.0959359,"Missing"
P15-1039,W11-0403,0,0.103983,"Missing"
P15-1039,W10-1814,0,0.170682,"Missing"
P15-1039,P11-2052,0,0.473047,"Missing"
P15-1039,C98-1013,0,\N,Missing
P15-1039,bojar-etal-2014-hindencorp,0,\N,Missing
P15-1039,I05-2013,0,\N,Missing
P16-4001,P15-1039,1,0.72525,"bels Alan Akbik Yunyao Li IBM Research Almaden Research Center 650 Harry Road, San Jose, CA 95120, USA {akbika,yunyaoli}@us.ibm.com Abstract Semantic role labeling (SRL) identifies the predicate-argument structure in text with semantic labels. It plays a key role in understanding natural language. In this paper, we present P OLYGLOT, a multilingual semantic role labeling system capable of semantically parsing sentences in 9 different languages from 4 different language groups. The core of P OLYGLOT are SRL models for individual languages trained with automatically generated Proposition Banks (Akbik et al., 2015). The key feature of the system is that it treats the semantic labels of the English Proposition Bank as “universal semantic labels”: Given a sentence in any of the supported languages, P OLYGLOT applies the corresponding SRL and predicts English PropBank frame and role annotation. The results are then visualized to facilitate the understanding of multilingual SRL with this unified semantic representation. 1 Figure 1: Example of P OLYGLOT predicting English PropBank labels for a simple German sentence: The verb “kaufen” is correctly identified to evoke the BUY.01 frame, while “ich” (I) is reco"
P16-4001,W09-3036,0,0.0702047,"Missing"
P16-4001,W09-1206,0,0.0873248,"Missing"
P16-4001,C10-3009,0,0.0612135,"Missing"
P16-4001,D12-1133,0,0.0322089,"s UN, OpenSubtitles UN, OpenSubtitles 24,5M 12,2M n/a 36M 14,1M 54K 1,7M 22,7M 52,4M Table 2: NLP tools and source of parallel data used for each language. Since English is the source language for annotation projection, no parallel data was required to train SRL. NLP tools: S TANFORD C ORE NLP: (Manning et al., 2014) , T N TTAGGER: (Brants, 2000), T REE TAGGER: (Schmid, 1994), K HOJA S TEMMER: (Khoja and Garside, 1999), S TANFORD PARSER: (Green and Manning, 2010), S TANFORD C ORE NLP: (Choi and McCallum, 2013), M ATE PARSER: (Bohnet, 2010), JJST: proprietary system, M ATE T RANSITION PARSER: (Bohnet and Nivre, 2012), M ALT PARSER: (Nivre et al., 2006). trained models where available. A breakdown of the preprocessing tools used for each language is given in Table 2. Data sets. In order to generate training data for P OLYGLOT, we used the following sources of parallel data: The UN corpus of official United Nations documents (Rafalovitch et al., 2009), the Europarl corpus of European parliament proceedings (Koehn, 2005), the OpenSubtitles corpus of movie subtitles (Lison and Tiedemann, 2016), the Hindencorp corpus automatically gathered from web sources (Bojar et al., 2014) and the Tatoeba corpus of languag"
P16-4001,C14-2018,1,0.859605,"Missing"
P16-4001,P81-1022,0,0.591303,"Missing"
P16-4001,A00-1031,0,0.0230388,"S TANFORD C ORE NLP, M ATE T RANSITION PARSER T N TTAGGER, M ALT PARSER JJST T REE TAGGER, M ALT PARSER S TANFORD C ORE NLP, M ATE PARSER UN, OpenSubtitles UN, OpenSubtitles #S ENTENCES n/a UN, OpenSubtitles Europarl, OpenSubtitles Hindencorp Tatoeba, OpenSubtitles UN, OpenSubtitles UN, OpenSubtitles 24,5M 12,2M n/a 36M 14,1M 54K 1,7M 22,7M 52,4M Table 2: NLP tools and source of parallel data used for each language. Since English is the source language for annotation projection, no parallel data was required to train SRL. NLP tools: S TANFORD C ORE NLP: (Manning et al., 2014) , T N TTAGGER: (Brants, 2000), T REE TAGGER: (Schmid, 1994), K HOJA S TEMMER: (Khoja and Garside, 1999), S TANFORD PARSER: (Green and Manning, 2010), S TANFORD C ORE NLP: (Choi and McCallum, 2013), M ATE PARSER: (Bohnet, 2010), JJST: proprietary system, M ATE T RANSITION PARSER: (Bohnet and Nivre, 2012), M ALT PARSER: (Nivre et al., 2006). trained models where available. A breakdown of the preprocessing tools used for each language is given in Table 2. Data sets. In order to generate training data for P OLYGLOT, we used the following sources of parallel data: The UN corpus of official United Nations documents (Rafalovitch"
P16-4001,P13-1104,0,0.020932,"UN, OpenSubtitles #S ENTENCES n/a UN, OpenSubtitles Europarl, OpenSubtitles Hindencorp Tatoeba, OpenSubtitles UN, OpenSubtitles UN, OpenSubtitles 24,5M 12,2M n/a 36M 14,1M 54K 1,7M 22,7M 52,4M Table 2: NLP tools and source of parallel data used for each language. Since English is the source language for annotation projection, no parallel data was required to train SRL. NLP tools: S TANFORD C ORE NLP: (Manning et al., 2014) , T N TTAGGER: (Brants, 2000), T REE TAGGER: (Schmid, 1994), K HOJA S TEMMER: (Khoja and Garside, 1999), S TANFORD PARSER: (Green and Manning, 2010), S TANFORD C ORE NLP: (Choi and McCallum, 2013), M ATE PARSER: (Bohnet, 2010), JJST: proprietary system, M ATE T RANSITION PARSER: (Bohnet and Nivre, 2012), M ALT PARSER: (Nivre et al., 2006). trained models where available. A breakdown of the preprocessing tools used for each language is given in Table 2. Data sets. In order to generate training data for P OLYGLOT, we used the following sources of parallel data: The UN corpus of official United Nations documents (Rafalovitch et al., 2009), the Europarl corpus of European parliament proceedings (Koehn, 2005), the OpenSubtitles corpus of movie subtitles (Lison and Tiedemann, 2016), the Hind"
P16-4001,de-marneffe-etal-2014-universal,0,0.0755217,"Missing"
P16-4001,J05-1004,0,0.342892,"d projection and bootstrapped learning to autogenerate Proposition Bank-style resources for 7 languages, namely Arabic, Chinese, French, German, Hindi, Russian and Spanish (Akbik et al., 2015). Unified semantic labels across all languages. One key difference between auto-generated PropBanks and manually created ones is that the former use English Proposition Bank labels for Introduction Semantic role labeling (SRL) is the task of labeling predicate-argument structure in sentences with shallow semantic information. One prominent labeling scheme for the English language is the Proposition Bank (Palmer et al., 2005) which annotates predicates with frame labels and arguments with role labels. Role labels roughly conform to simple questions (who, what, when, where, how much, with whom) with regards to the predicate. SRL is important for understanding natural language; it has been found useful for many applications such as information extraction (Fader et al., 2011) and question answering (Shen and Lapata, 2007; Maqsud et al., 2014). 1 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 1–6, c Berlin, Germany, August 7-12, 2016. 2016 Associati"
P16-4001,D11-1142,0,0.0228386,"ition Bank labels for Introduction Semantic role labeling (SRL) is the task of labeling predicate-argument structure in sentences with shallow semantic information. One prominent labeling scheme for the English language is the Proposition Bank (Palmer et al., 2005) which annotates predicates with frame labels and arguments with role labels. Role labels roughly conform to simple questions (who, what, when, where, how much, with whom) with regards to the predicate. SRL is important for understanding natural language; it has been found useful for many applications such as information extraction (Fader et al., 2011) and question answering (Shen and Lapata, 2007; Maqsud et al., 2014). 1 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 1–6, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Figure 2: Side by side view of English, Chinese and Spanish sentences parsed in P OLYGLOT’s Web UI. English PropBank frame and role labels are predicted for all languages: All example sentences evoke the BUY.01 frame and have constituents accordingly labeled with roles such as the buyer, the thing bought, the price paid"
P16-4001,2009.mtsummit-posters.15,0,0.0682369,"Missing"
P16-4001,C10-1045,0,0.0177535,"ANFORD C ORE NLP, M ATE PARSER UN, OpenSubtitles UN, OpenSubtitles #S ENTENCES n/a UN, OpenSubtitles Europarl, OpenSubtitles Hindencorp Tatoeba, OpenSubtitles UN, OpenSubtitles UN, OpenSubtitles 24,5M 12,2M n/a 36M 14,1M 54K 1,7M 22,7M 52,4M Table 2: NLP tools and source of parallel data used for each language. Since English is the source language for annotation projection, no parallel data was required to train SRL. NLP tools: S TANFORD C ORE NLP: (Manning et al., 2014) , T N TTAGGER: (Brants, 2000), T REE TAGGER: (Schmid, 1994), K HOJA S TEMMER: (Khoja and Garside, 1999), S TANFORD PARSER: (Green and Manning, 2010), S TANFORD C ORE NLP: (Choi and McCallum, 2013), M ATE PARSER: (Bohnet, 2010), JJST: proprietary system, M ATE T RANSITION PARSER: (Bohnet and Nivre, 2012), M ALT PARSER: (Nivre et al., 2006). trained models where available. A breakdown of the preprocessing tools used for each language is given in Table 2. Data sets. In order to generate training data for P OLYGLOT, we used the following sources of parallel data: The UN corpus of official United Nations documents (Rafalovitch et al., 2009), the Europarl corpus of European parliament proceedings (Koehn, 2005), the OpenSubtitles corpus of movie"
P16-4001,D07-1002,0,0.150131,"role labeling (SRL) is the task of labeling predicate-argument structure in sentences with shallow semantic information. One prominent labeling scheme for the English language is the Proposition Bank (Palmer et al., 2005) which annotates predicates with frame labels and arguments with role labels. Role labels roughly conform to simple questions (who, what, when, where, how much, with whom) with regards to the predicate. SRL is important for understanding natural language; it has been found useful for many applications such as information extraction (Fader et al., 2011) and question answering (Shen and Lapata, 2007; Maqsud et al., 2014). 1 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 1–6, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Figure 2: Side by side view of English, Chinese and Spanish sentences parsed in P OLYGLOT’s Web UI. English PropBank frame and role labels are predicted for all languages: All example sentences evoke the BUY.01 frame and have constituents accordingly labeled with roles such as the buyer, the thing bought, the price paid and the benefactive. experiment with the tool"
P16-4001,tiedemann-2012-parallel,0,0.0317175,"eakdown of the preprocessing tools used for each language is given in Table 2. Data sets. In order to generate training data for P OLYGLOT, we used the following sources of parallel data: The UN corpus of official United Nations documents (Rafalovitch et al., 2009), the Europarl corpus of European parliament proceedings (Koehn, 2005), the OpenSubtitles corpus of movie subtitles (Lison and Tiedemann, 2016), the Hindencorp corpus automatically gathered from web sources (Bojar et al., 2014) and the Tatoeba corpus of language learning examples1 . The data sets were obtained from the OPUS project (Tiedemann, 2012) and word aligned using the Berkeley Aligner2 . Table 2 lists the data sets used for each language and the combined number of available parallel sentences. 5 sented as a grid in which each row corresponds to one identified semantic frame. The grid highlights sentence constituents labeled with roles and includes role descriptions for better interpretability of the parsing results. Below the results of the semantic analysis the GUI shows two more detailed views of the parsing results. The first visualizes the dependency parse tree generated using W HATS W RONG W ITH M Y NLP3 , while the second ("
P16-4001,P11-2052,0,0.419019,"Missing"
P16-4001,2005.mtsummit-papers.11,0,0.0129705,"9), S TANFORD PARSER: (Green and Manning, 2010), S TANFORD C ORE NLP: (Choi and McCallum, 2013), M ATE PARSER: (Bohnet, 2010), JJST: proprietary system, M ATE T RANSITION PARSER: (Bohnet and Nivre, 2012), M ALT PARSER: (Nivre et al., 2006). trained models where available. A breakdown of the preprocessing tools used for each language is given in Table 2. Data sets. In order to generate training data for P OLYGLOT, we used the following sources of parallel data: The UN corpus of official United Nations documents (Rafalovitch et al., 2009), the Europarl corpus of European parliament proceedings (Koehn, 2005), the OpenSubtitles corpus of movie subtitles (Lison and Tiedemann, 2016), the Hindencorp corpus automatically gathered from web sources (Bojar et al., 2014) and the Tatoeba corpus of language learning examples1 . The data sets were obtained from the OPUS project (Tiedemann, 2012) and word aligned using the Berkeley Aligner2 . Table 2 lists the data sets used for each language and the combined number of available parallel sentences. 5 sented as a grid in which each row corresponds to one identified semantic frame. The grid highlights sentence constituents labeled with roles and includes role d"
P16-4001,L16-1147,0,0.0235836,"C ORE NLP: (Choi and McCallum, 2013), M ATE PARSER: (Bohnet, 2010), JJST: proprietary system, M ATE T RANSITION PARSER: (Bohnet and Nivre, 2012), M ALT PARSER: (Nivre et al., 2006). trained models where available. A breakdown of the preprocessing tools used for each language is given in Table 2. Data sets. In order to generate training data for P OLYGLOT, we used the following sources of parallel data: The UN corpus of official United Nations documents (Rafalovitch et al., 2009), the Europarl corpus of European parliament proceedings (Koehn, 2005), the OpenSubtitles corpus of movie subtitles (Lison and Tiedemann, 2016), the Hindencorp corpus automatically gathered from web sources (Bojar et al., 2014) and the Tatoeba corpus of language learning examples1 . The data sets were obtained from the OPUS project (Tiedemann, 2012) and word aligned using the Berkeley Aligner2 . Table 2 lists the data sets used for each language and the combined number of available parallel sentences. 5 sented as a grid in which each row corresponds to one identified semantic frame. The grid highlights sentence constituents labeled with roles and includes role descriptions for better interpretability of the parsing results. Below the"
P16-4001,P14-5010,0,0.00303566,"RD C ORE NLP, M ATE T RANSITION PARSER S TANFORD C ORE NLP, M ATE T RANSITION PARSER T N TTAGGER, M ALT PARSER JJST T REE TAGGER, M ALT PARSER S TANFORD C ORE NLP, M ATE PARSER UN, OpenSubtitles UN, OpenSubtitles #S ENTENCES n/a UN, OpenSubtitles Europarl, OpenSubtitles Hindencorp Tatoeba, OpenSubtitles UN, OpenSubtitles UN, OpenSubtitles 24,5M 12,2M n/a 36M 14,1M 54K 1,7M 22,7M 52,4M Table 2: NLP tools and source of parallel data used for each language. Since English is the source language for annotation projection, no parallel data was required to train SRL. NLP tools: S TANFORD C ORE NLP: (Manning et al., 2014) , T N TTAGGER: (Brants, 2000), T REE TAGGER: (Schmid, 1994), K HOJA S TEMMER: (Khoja and Garside, 1999), S TANFORD PARSER: (Green and Manning, 2010), S TANFORD C ORE NLP: (Choi and McCallum, 2013), M ATE PARSER: (Bohnet, 2010), JJST: proprietary system, M ATE T RANSITION PARSER: (Bohnet and Nivre, 2012), M ALT PARSER: (Nivre et al., 2006). trained models where available. A breakdown of the preprocessing tools used for each language is given in Table 2. Data sets. In order to generate training data for P OLYGLOT, we used the following sources of parallel data: The UN corpus of official United"
P16-4001,nivre-etal-2006-maltparser,0,\N,Missing
P16-4001,W09-1201,0,\N,Missing
P16-4001,bojar-etal-2014-hindencorp,0,\N,Missing
P19-1586,Q17-1010,0,0.0173346,"Fig. 1, our model encompasses a sequence of steps that computes attribute representations, attribute similarity and finally the record similarity for each input pair he1 , e2 i. A matching classifier uses the record similarity representation to classify the pair. For an extensive list of hyperparameters and training details we chose, see the appendix. Input Representations. For each entity record pair he1 , e2 i, we tokenize the attribute values and vectorize the words by external word embeddings to obtain input representations (W s in Fig. 1). We use the 300 dimensional fastText embeddings (Bojanowski et al., 2017), which capture subword information by producing word vectors via character n-grams. This vectorization has the benefit of well representing out-of-vocabulary words (Bojanowski et al., 2017) that frequently appear in ER attributes. For instance, venue names SIGMOD and ACL are out of vocabulary in the publicly available GloVe vectors (Pennington et al., 2014), but we clearly need to distinguish them. Attribute Representations. We build a universal bidirectional RNN on the word input representations of each attribute value and obtain attribute vectors (attr1 and attr2 in Fig. 1) by concatenating"
P19-1586,D14-1179,0,0.013357,"Missing"
P19-1586,D14-1162,0,0.0823532,"Representations. For each entity record pair he1 , e2 i, we tokenize the attribute values and vectorize the words by external word embeddings to obtain input representations (W s in Fig. 1). We use the 300 dimensional fastText embeddings (Bojanowski et al., 2017), which capture subword information by producing word vectors via character n-grams. This vectorization has the benefit of well representing out-of-vocabulary words (Bojanowski et al., 2017) that frequently appear in ER attributes. For instance, venue names SIGMOD and ACL are out of vocabulary in the publicly available GloVe vectors (Pennington et al., 2014), but we clearly need to distinguish them. Attribute Representations. We build a universal bidirectional RNN on the word input representations of each attribute value and obtain attribute vectors (attr1 and attr2 in Fig. 1) by concatenating the last hidden units from both directions. Crucially, the universal RNN allows for transfer learning between datasets of different schemas without error-prone schema mapping. We found that gated recurrent units (GRUs, Cho et al. (2014)) yielded the best performance on the dev set as compared to simple recurrent neural networks (SRNNs, Elman (1990)) and Lon"
P19-1586,N18-1202,0,0.0292209,"the labeled sample set because partitioning encourages us to choose likely false negatives more aggressively, yet false negatives tend to be more challenging to find in entity resolution due to the skewness toward the negative (Qian et al., 2017). We observed similar patterns in DBLP-Scholar and Cora. 6 Further Related Work Transfer learning has proven successful in fields such as computer vision and natural language processing, where networks for a target task is pretrained on a source task with plenty of training data (e.g. image classification (Donahue et al., 2014) and language modeling (Peters et al., 2018)). In this work, we developed a transfer learning framework for a deep ER model. Concurrent work (Thirumuruganathan et al., 2018) to ours has also proposed transfer learning on top of the features from distributed representations, but they focused on classical machine learning classifiers (e.g., logistic regression, SVMs, decision trees, random forests) and they did not con5858 Sampling Method High-Confidence Partition High-Conf.+Part. Top K Entropy Prec Recall F1 93.32 97.21 95.19±2.21 96.14 97.12 96.61±0.57 97.63 97.84 97.73±0.43 96.16 89.64 92.07±9.73 Acknowledgments Table 6: Low-resource p"
P19-1586,C14-1124,0,0.250825,"t our method achieves comparable, if not better, performance compared to state-of-the-art learning-based methods while using an order of magnitude fewer labels. 1 Introduction Entity Resolution (ER), also known as entity matching, record linkage (Fellegi and Sunter, 1969), reference reconciliation (Dong et al., 2005), and merge-purge (Hern´andez and Stolfo, 1995), identifies and links different representations of the same real-world entities. ER yields a unified and consistent view of data and serves as a crucial step in downstream applications, including knowledge base creation, text mining (Zhao et al., 2014), and social media analysis (Campbell et al., 2016). For instance, seen in Table 1 are citation data records from two databases, DBLP and Google Scholar. If one intends to build a system that analyzes citation networks of publications, it is essential to recognize publication overlaps across the databases and to integrate the data records (Pasula et al., 2002). Recent work demonstrated that deep learning (DL) models with distributed representations of words are viable alternatives to other machine learning algorithms, including support vector machines and decision trees, for performing ER (Ebr"
P19-3023,D14-1162,0,0.0807384,"Missing"
W19-3320,P15-1039,1,0.844925,"t still lacks sufficient structure to express the refined meanings discussed above. • Role and Context nodes are connected to Predicate nodes with SRL labels. Context might also be connected to other nodes, such as Conditional, as discussed above. • A Conditional node is connected to an antecedent node and a consequent node, and optionally to an “else” node. • A Conjunction node is connected to its constituents. 179 • A Relation node is connected to its constituents. 3.4 3.5 Learning features from data Using techniques similar to those used to transfer SRL and AMR from one language to another(Akbik et al., 2015; Damonte and Cohen, 2018), it is possible to transfer labeling schemes for the additional fewatures and structures discussed in this paper from one language to another. The cross-lingual transfer may also help to discover better feature sets from data. For example, by analyzing equivalent sentences in different languages, it is possible to discover additional candidates for modalilty or better classification of modality. Akbik et al. (2016) showed that it is possible to use correspondences between verb senses in two languages to discover the duplication and aliasing of verb senses. Similar te"
W19-3320,C16-1327,1,0.837892,"nected to its constituents. 3.4 3.5 Learning features from data Using techniques similar to those used to transfer SRL and AMR from one language to another(Akbik et al., 2015; Damonte and Cohen, 2018), it is possible to transfer labeling schemes for the additional fewatures and structures discussed in this paper from one language to another. The cross-lingual transfer may also help to discover better feature sets from data. For example, by analyzing equivalent sentences in different languages, it is possible to discover additional candidates for modalilty or better classification of modality. Akbik et al. (2016) showed that it is possible to use correspondences between verb senses in two languages to discover the duplication and aliasing of verb senses. Similar techniques can be applied to verb features such as tense and modality, as well as structural featues such as conditional and relational features. It is our hope that this framework provides a sufficiently versatile scafolding for the community to work together towards a more complete cross-lingual representation of meanings. Example representation An example can illustrate various aspects of this framework. Consider the sentence Had I studied"
W19-3320,W13-2322,0,0.194081,"r expressing the same meaning, not only within each language but more so across languages, making syntactical analysis cumbersome to use by downstream applications. Semantic understanding of natural language is fundamental for many applications that take natural language texts as part of their input. Semantic Role Labeling (SRL) analyzes the predicate-role structure at the shallow semantic parsing level (e.g., PropBank (Kingsbury and Palmer, 2002)). At a deeper level, Abstract Meaning Representation (AMR) provides a rooted, directional and labeled graph representing the meaning of a sentence (Banarescu et al., 2013), focusing on semantic relations between concepts such as PropBank predicate-argument structures while abstracting away from syntactic variation. Many applications require multilingual capabilities, but SRL and AMR annotation schemes designed for individual languages have languagedependent features. For example, Hajic et al. (2014); Xue et al. (2014) observed AMRs designed for different languages have differences, some accidental but others are more fundamental. Several efforts are underway to create more cross-lingual natural language resources. Universal Dependencies (UD) is a framework for"
W19-3320,N18-1104,0,0.420387,"mantic Representation Huaiyu Zhu IBM Research - Almaden 650 Harry Road, San Jose, CA 95120 huaiyu@us.ibm.com Yunyao Li IBM Research - Almaden 650 Harry Road, San Jose, CA 95120 yunyaoli@us.ibm.com Abstract cross-linguistically consistent grammatical annotation.(De Marneffe et al., 2014). The Universal Proposition Banks project aims to annotate text in different languages with a layer of ”universal” semantic role labeling annotation, by using the frame and role labels of the English Proposition Bank to label shallow semantics in sentences in new target languages(Akbik et al., 2015). Similarly, Damonte and Cohen (2018) use AMR annotations for English as a semantic representation for sentences written in other languages, utilizing an AMR parser for English and parallel corpora to learn AMR parsers for additional languages. Despite these efforts, some remaining interlanguage variations important for practical usage are not yet captured by the efforts so far. They create obstacles to a truly cross-lingual meaning representation which would enable the downstream applications be written for one language and applicable for other languages. The purpose of this paper is two-fold. One objective is to highlight some"
W19-3320,de-marneffe-etal-2014-universal,0,0.0903255,"Missing"
W19-3320,W14-5808,0,0.387738,"es the predicate-role structure at the shallow semantic parsing level (e.g., PropBank (Kingsbury and Palmer, 2002)). At a deeper level, Abstract Meaning Representation (AMR) provides a rooted, directional and labeled graph representing the meaning of a sentence (Banarescu et al., 2013), focusing on semantic relations between concepts such as PropBank predicate-argument structures while abstracting away from syntactic variation. Many applications require multilingual capabilities, but SRL and AMR annotation schemes designed for individual languages have languagedependent features. For example, Hajic et al. (2014); Xue et al. (2014) observed AMRs designed for different languages have differences, some accidental but others are more fundamental. Several efforts are underway to create more cross-lingual natural language resources. Universal Dependencies (UD) is a framework for • Like AMR, it makes use of PropBank style predicate-argument structures. • It does not have AMR style concept nodes. It does not infer relations among instances and concepts other than those expressed explicitly, nor perform co-reference resolution. • It is geared towards cross-lingual representation of logical structures, such as"
W19-3320,kingsbury-palmer-2002-treebank,0,0.459984,"cross a variety of languages. 1 Laura Chiticariu IBM Watson 650 Harry Road, San Jose, CA 95120 chiti@us.ibm.com Introduction Natural languages have many syntactic variations for expressing the same meaning, not only within each language but more so across languages, making syntactical analysis cumbersome to use by downstream applications. Semantic understanding of natural language is fundamental for many applications that take natural language texts as part of their input. Semantic Role Labeling (SRL) analyzes the predicate-role structure at the shallow semantic parsing level (e.g., PropBank (Kingsbury and Palmer, 2002)). At a deeper level, Abstract Meaning Representation (AMR) provides a rooted, directional and labeled graph representing the meaning of a sentence (Banarescu et al., 2013), focusing on semantic relations between concepts such as PropBank predicate-argument structures while abstracting away from syntactic variation. Many applications require multilingual capabilities, but SRL and AMR annotation schemes designed for individual languages have languagedependent features. For example, Hajic et al. (2014); Xue et al. (2014) observed AMRs designed for different languages have differences, some accid"
W19-3320,xue-etal-2014-interlingua,0,0.271352,"structure at the shallow semantic parsing level (e.g., PropBank (Kingsbury and Palmer, 2002)). At a deeper level, Abstract Meaning Representation (AMR) provides a rooted, directional and labeled graph representing the meaning of a sentence (Banarescu et al., 2013), focusing on semantic relations between concepts such as PropBank predicate-argument structures while abstracting away from syntactic variation. Many applications require multilingual capabilities, but SRL and AMR annotation schemes designed for individual languages have languagedependent features. For example, Hajic et al. (2014); Xue et al. (2014) observed AMRs designed for different languages have differences, some accidental but others are more fundamental. Several efforts are underway to create more cross-lingual natural language resources. Universal Dependencies (UD) is a framework for • Like AMR, it makes use of PropBank style predicate-argument structures. • It does not have AMR style concept nodes. It does not infer relations among instances and concepts other than those expressed explicitly, nor perform co-reference resolution. • It is geared towards cross-lingual representation of logical structures, such as conjunctions and c"
