2020.acl-main.208,P17-1000,0,0.227312,"Missing"
2020.acl-main.208,J84-3009,0,0.632446,"Missing"
2020.acl-main.208,P17-1041,0,0.148164,"cards and Java or Python classes implementing their behavior in a game engine, the CONCODE dataset (Iyer et al., 2018) consisting of Java documentation strings and method bodies, and the NAPS and SPoC datasets (Zavershynskyi et al., 2018; Kulal et al., 2019) consisting of pseudocode annotations and source code for programming competition problems. Past approaches to these large-scale languageto-code tasks have typically employed sequencebased models (Ling et al., 2016) that do not account for structure on the output side, or tree-based models (Allamanis et al., 2015; Rabinovich et al., 2017a; Yin and Neubig, 2017; Hayati et al., 2018; Iyer et al., 2019) that incorporate the syntax but not the semantics of the output domain. However, if we want to generate programs that can be executed successfully, the inclusion of both syntactic and semantic constraints is crucial. As shown in Figure 1, while multiple program fragments may be syntactically correct and represent plausible translations of the corresponding pseudocode, not all of them will lead to executable programs. To address this, we propose a search procedure based on semantic scaffolds, lightweight sum2283 Proceedings of the 58th Annual Meeting of"
2020.acl-main.208,D19-1545,0,0.709963,"ng their behavior in a game engine, the CONCODE dataset (Iyer et al., 2018) consisting of Java documentation strings and method bodies, and the NAPS and SPoC datasets (Zavershynskyi et al., 2018; Kulal et al., 2019) consisting of pseudocode annotations and source code for programming competition problems. Past approaches to these large-scale languageto-code tasks have typically employed sequencebased models (Ling et al., 2016) that do not account for structure on the output side, or tree-based models (Allamanis et al., 2015; Rabinovich et al., 2017a; Yin and Neubig, 2017; Hayati et al., 2018; Iyer et al., 2019) that incorporate the syntax but not the semantics of the output domain. However, if we want to generate programs that can be executed successfully, the inclusion of both syntactic and semantic constraints is crucial. As shown in Figure 1, while multiple program fragments may be syntactically correct and represent plausible translations of the corresponding pseudocode, not all of them will lead to executable programs. To address this, we propose a search procedure based on semantic scaffolds, lightweight sum2283 Proceedings of the 58th Annual Meeting of the Association for Computational Lingui"
2020.acl-main.208,D18-1192,0,0.0151197,"nts (Ling et al., 2016), and more. While much of the prior work in executable semantic parsing involves short descriptions being mapped into single-line programs, some tasks have recently been proposed that involve multiple natural language utterances on the input side and full programs on the output side, often reaching tens of lines in length and including non-trivial state manipulation. Examples include the Magic the Gathering and Hearthstone datasets (Ling et al., 2016) derived from trading cards and Java or Python classes implementing their behavior in a game engine, the CONCODE dataset (Iyer et al., 2018) consisting of Java documentation strings and method bodies, and the NAPS and SPoC datasets (Zavershynskyi et al., 2018; Kulal et al., 2019) consisting of pseudocode annotations and source code for programming competition problems. Past approaches to these large-scale languageto-code tasks have typically employed sequencebased models (Ling et al., 2016) that do not account for structure on the output side, or tree-based models (Allamanis et al., 2015; Rabinovich et al., 2017a; Yin and Neubig, 2017; Hayati et al., 2018; Iyer et al., 2019) that incorporate the syntax but not the semantics of the"
2020.acl-main.208,P17-4012,0,0.0704557,"Missing"
2020.acl-main.208,P16-1057,0,0.0737396,"Missing"
2020.acl-main.208,P17-1105,1,0.920606,"016) derived from trading cards and Java or Python classes implementing their behavior in a game engine, the CONCODE dataset (Iyer et al., 2018) consisting of Java documentation strings and method bodies, and the NAPS and SPoC datasets (Zavershynskyi et al., 2018; Kulal et al., 2019) consisting of pseudocode annotations and source code for programming competition problems. Past approaches to these large-scale languageto-code tasks have typically employed sequencebased models (Ling et al., 2016) that do not account for structure on the output side, or tree-based models (Allamanis et al., 2015; Rabinovich et al., 2017a; Yin and Neubig, 2017; Hayati et al., 2018; Iyer et al., 2019) that incorporate the syntax but not the semantics of the output domain. However, if we want to generate programs that can be executed successfully, the inclusion of both syntactic and semantic constraints is crucial. As shown in Figure 1, while multiple program fragments may be syntactically correct and represent plausible translations of the corresponding pseudocode, not all of them will lead to executable programs. To address this, we propose a search procedure based on semantic scaffolds, lightweight sum2283 Proceedings of the"
2020.acl-main.557,J99-2004,0,0.203999,"prevalent and parallel execution is limited. Stateof-the-art approaches use custom architecture components, such as the tree-structured networks of RNNG (Dyer et al., 2016) or the per-span MLPs in chart parsers (Stern et al., 2017; Kitaev et al., 2019). Approaches to inference range from autoregressive generation, to cubic-time CKY, to A* search – none of which are readily parallelizable. Our goal is to 2 Related Work Label-based parsing A variety of approaches have been proposed to mostly or entirely reduce parsing to a sequence labeling task. One family of these approaches is supertagging (Bangalore and Joshi, 1999), which is particularly common for CCG parsing. CCG imposes constraints on which supertags may form a valid derivation, necessitating complex search procedures for finding a highscoring sequence of supertags that is self-consistent. An example of how such a search procedure can be implemented is the system of Lee et al. (2016), which uses A∗ search. This search procedure is not easily parallelizable on GPU-like hardware, and has a worst-case serial running time that is exponential in the sentence length. G´omez-Rodr´ıguez and Vilares (2018) propose a different approach that fully reduces parsi"
2020.acl-main.557,N19-1423,0,0.0069262,"ar-Time Inference Nikita Kitaev and Dan Klein Computer Science Division University of California, Berkeley {kitaev, klein}@cs.berkeley.edu demonstrate a parsing algorithm that makes effective use of the latest hardware. The desiderata for our approach are (a) to maximize parallelism, (b) to minimize task-specific architecture design, and (c) to lose as little accuracy as possible compared to a state-of-the-art highly-specialized model. To do this, we propose an algorithm that reduces parsing to tagging, where all tags are predicted in parallel using a standard model architecture such as BERT (Devlin et al., 2019). Tagging is followed by a minimal inference procedure that is fast enough to schedule on the CPU because it runs in linear time with low constant factors (subject to mild assumptions). Abstract We present a constituency parsing algorithm that, like a supertagger, works by assigning labels to each word in a sentence. In order to maximally leverage current neural architectures, the model scores each word’s tags in parallel, with minimal task-specific structure. After scoring, a left-to-right reconciliation phase extracts a tree in (empirically) linear time. Our parser achieves 95.4 F1 on the WS"
2020.acl-main.557,P15-1030,1,0.843069,"jholt, 1979; van Schijndel et al., 2013; Noji et al., 2016; Shain et al., 2016, inter alia). Much of past work highlights the benefits of a left-corner formulation for memory efficiency, with implications for psycholin1 ⇐ Chart parsing Chart parsers fundamentally operate over span-aligned rather than word-aligned representations. For instance, the size of the chart in the CKY algorithm (Cocke, 1970; Kasami, 1966; Younger, 1967) is quadratic in the length of the sentence, and the algorithm itself has cubic running time. This is true for both classical methods and more recent neural approaches (Durrett and Klein, 2015; Stern et al., 2017). The construction of a chart involves a non-trivial (quadratic) computation that is specialized to parsing, and implementing the CKY algorithm on a hardware accelerator is a nontrivial and hardware-specific task. 4 ⇐ Shift-reduce transition systems A number of parsers proposed in the literature can be categorized as shift-reduce parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013). These systems rely on generating sequences of actions, which need not be evenly distributed throughout the sentence. For example, the construction of a deep"
2020.acl-main.557,N16-1024,0,0.152306,"computational costs across tasks through approaches such as pre-training and multitask learning. This places particular demands for a model to be efficient: it must parallelize, it must maximally use standard subcomponents that have been heavily optimized, but at the same time it must adequately incorporate task-specific insights and inductive biases. Against this backdrop, constituency parsing stands as a task where custom architectures are prevalent and parallel execution is limited. Stateof-the-art approaches use custom architecture components, such as the tree-structured networks of RNNG (Dyer et al., 2016) or the per-span MLPs in chart parsers (Stern et al., 2017; Kitaev et al., 2019). Approaches to inference range from autoregressive generation, to cubic-time CKY, to A* search – none of which are readily parallelizable. Our goal is to 2 Related Work Label-based parsing A variety of approaches have been proposed to mostly or entirely reduce parsing to a sequence labeling task. One family of these approaches is supertagging (Bangalore and Joshi, 1999), which is particularly common for CCG parsing. CCG imposes constraints on which supertags may form a valid derivation, necessitating complex searc"
2020.acl-main.557,D18-1162,0,0.0326613,"Missing"
2020.acl-main.557,N03-1014,0,0.0379522,"thm (Cocke, 1970; Kasami, 1966; Younger, 1967) is quadratic in the length of the sentence, and the algorithm itself has cubic running time. This is true for both classical methods and more recent neural approaches (Durrett and Klein, 2015; Stern et al., 2017). The construction of a chart involves a non-trivial (quadratic) computation that is specialized to parsing, and implementing the CKY algorithm on a hardware accelerator is a nontrivial and hardware-specific task. 4 ⇐ Shift-reduce transition systems A number of parsers proposed in the literature can be categorized as shift-reduce parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013). These systems rely on generating sequences of actions, which need not be evenly distributed throughout the sentence. For example, the construction of a deep right-branching tree might involve a series of shift actions (one per word in the sentence), followed by equally many consecutive reduce actions that all cluster at the end of the sentence. Due to the uneven alignment between actions and locations in a sentence, neural network architectures in recent shift-reduce systems (Vinyals et al., 2015; Dyer et al., 2016; Liu and Zha"
2020.acl-main.557,P98-1101,0,0.361834,"be exactly two decisions to make between one word and the next. This fixed alignment allows us to predict all actions in parallel rather than autoregressively. $ Figure 1: An example tree with the corresponding labels. The nonterminal nodes have been numbered based on an in-order traversal. guistic plausibility of the approach. We, on the other hand, demonstrate how to leverage these same considerations to achieve parallel tagging and linear time complexity of the subsequent inference procedure. Further, past work has used grammars (Rosenkrantz and Lewis, 1970), or transformed labeled trees (Johnson, 1998; Schuler et al., 2010). On the other hand, it is precisely the lack of an explicit grammar that allows us to formulate our linear-time inference algorithm. 3 Method To introduce our method, we first restrict ourselves to only consider unlabeled full binary trees (where every node has either 0 or 2 children). We defer the discussion of labeling and non-binary structure to Section 3.5. 3.1 Trees to tags Consider the example tree shown in Figure 1. The tree is fully binarized and consists of 5 terminal symbols (A,B,C,D,E) and 4 nonterminal nodes (1,2,3,4). For any full binary parse tree, the num"
2020.acl-main.557,P19-1340,1,0.807525,"ultitask learning. This places particular demands for a model to be efficient: it must parallelize, it must maximally use standard subcomponents that have been heavily optimized, but at the same time it must adequately incorporate task-specific insights and inductive biases. Against this backdrop, constituency parsing stands as a task where custom architectures are prevalent and parallel execution is limited. Stateof-the-art approaches use custom architecture components, such as the tree-structured networks of RNNG (Dyer et al., 2016) or the per-span MLPs in chart parsers (Stern et al., 2017; Kitaev et al., 2019). Approaches to inference range from autoregressive generation, to cubic-time CKY, to A* search – none of which are readily parallelizable. Our goal is to 2 Related Work Label-based parsing A variety of approaches have been proposed to mostly or entirely reduce parsing to a sequence labeling task. One family of these approaches is supertagging (Bangalore and Joshi, 1999), which is particularly common for CCG parsing. CCG imposes constraints on which supertags may form a valid derivation, necessitating complex search procedures for finding a highscoring sequence of supertags that is self-consis"
2020.acl-main.557,D16-1262,0,0.0401355,"Missing"
2020.acl-main.557,Q17-1029,0,0.012585,"erson, 2003; Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013). These systems rely on generating sequences of actions, which need not be evenly distributed throughout the sentence. For example, the construction of a deep right-branching tree might involve a series of shift actions (one per word in the sentence), followed by equally many consecutive reduce actions that all cluster at the end of the sentence. Due to the uneven alignment between actions and locations in a sentence, neural network architectures in recent shift-reduce systems (Vinyals et al., 2015; Dyer et al., 2016; Liu and Zhang, 2017) generally follow an encoder-decoder approach with autoregressive generation rather than directly assigning labels to positions in the input. Our proposed parser is also transition-based, but there are guaranteed to be exactly two decisions to make between one word and the next. This fixed alignment allows us to predict all actions in parallel rather than autoregressively. $ Figure 1: An example tree with the corresponding labels. The nonterminal nodes have been numbered based on an in-order traversal. guistic plausibility of the approach. We, on the other hand, demonstrate how to leverage the"
2020.acl-main.557,J93-2004,0,0.0759575,"Missing"
2020.acl-main.557,D16-1004,0,0.0471563,"Missing"
2020.acl-main.557,P80-1024,0,0.714789,"Meeting of the Association for Computational Linguistics, pages 6255–6261 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the problem of unseen labels (Vilares et al., 2019), but that only increases the label inventory rather than restricting it to a finite set. Our approach, on the other hand, uses just 4 labels in its simplest formulation (hence the name tetra-tagging). 2 3 D E ⇒ C → → ⇒ → B ← A ← Left-corner parsing To achieve all of our desiderata, we combine aspects of the previouslymentioned approaches with ideas drawn from a long line of work on left-corner parsing (Rosenkrantz and Lewis, 1970; Nijholt, 1979; van Schijndel et al., 2013; Noji et al., 2016; Shain et al., 2016, inter alia). Much of past work highlights the benefits of a left-corner formulation for memory efficiency, with implications for psycholin1 ⇐ Chart parsing Chart parsers fundamentally operate over span-aligned rather than word-aligned representations. For instance, the size of the chart in the CKY algorithm (Cocke, 1970; Kasami, 1966; Younger, 1967) is quadratic in the length of the sentence, and the algorithm itself has cubic running time. This is true for both classical methods and more recent neural approach"
2020.acl-main.557,W05-1513,0,0.106134,"Kasami, 1966; Younger, 1967) is quadratic in the length of the sentence, and the algorithm itself has cubic running time. This is true for both classical methods and more recent neural approaches (Durrett and Klein, 2015; Stern et al., 2017). The construction of a chart involves a non-trivial (quadratic) computation that is specialized to parsing, and implementing the CKY algorithm on a hardware accelerator is a nontrivial and hardware-specific task. 4 ⇐ Shift-reduce transition systems A number of parsers proposed in the literature can be categorized as shift-reduce parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013). These systems rely on generating sequences of actions, which need not be evenly distributed throughout the sentence. For example, the construction of a deep right-branching tree might involve a series of shift actions (one per word in the sentence), followed by equally many consecutive reduce actions that all cluster at the end of the sentence. Due to the uneven alignment between actions and locations in a sentence, neural network architectures in recent shift-reduce systems (Vinyals et al., 2015; Dyer et al., 2016; Liu and Zhang, 2017) generally fol"
2020.acl-main.557,J10-1001,0,0.21857,"decisions to make between one word and the next. This fixed alignment allows us to predict all actions in parallel rather than autoregressively. $ Figure 1: An example tree with the corresponding labels. The nonterminal nodes have been numbered based on an in-order traversal. guistic plausibility of the approach. We, on the other hand, demonstrate how to leverage these same considerations to achieve parallel tagging and linear time complexity of the subsequent inference procedure. Further, past work has used grammars (Rosenkrantz and Lewis, 1970), or transformed labeled trees (Johnson, 1998; Schuler et al., 2010). On the other hand, it is precisely the lack of an explicit grammar that allows us to formulate our linear-time inference algorithm. 3 Method To introduce our method, we first restrict ourselves to only consider unlabeled full binary trees (where every node has either 0 or 2 children). We defer the discussion of labeling and non-binary structure to Section 3.5. 3.1 Trees to tags Consider the example tree shown in Figure 1. The tree is fully binarized and consists of 5 terminal symbols (A,B,C,D,E) and 4 nonterminal nodes (1,2,3,4). For any full binary parse tree, the number of nonterminals wil"
2020.acl-main.557,C16-1092,0,0.0304617,"Missing"
2020.acl-main.557,P17-1076,1,0.929995,"s pre-training and multitask learning. This places particular demands for a model to be efficient: it must parallelize, it must maximally use standard subcomponents that have been heavily optimized, but at the same time it must adequately incorporate task-specific insights and inductive biases. Against this backdrop, constituency parsing stands as a task where custom architectures are prevalent and parallel execution is limited. Stateof-the-art approaches use custom architecture components, such as the tree-structured networks of RNNG (Dyer et al., 2016) or the per-span MLPs in chart parsers (Stern et al., 2017; Kitaev et al., 2019). Approaches to inference range from autoregressive generation, to cubic-time CKY, to A* search – none of which are readily parallelizable. Our goal is to 2 Related Work Label-based parsing A variety of approaches have been proposed to mostly or entirely reduce parsing to a sequence labeling task. One family of these approaches is supertagging (Bangalore and Joshi, 1999), which is particularly common for CCG parsing. CCG imposes constraints on which supertags may form a valid derivation, necessitating complex search procedures for finding a highscoring sequence of superta"
2020.acl-main.557,N19-1341,0,0.0130835,"Vilares (2018) propose a different approach that fully reduces parsing to sequence labeling, but the label set size is unbounded: it expands with tree depth and related properties of the input, rather than being fixed for any given language. There have been attempts to address this by adding redundant labels, where the model learns to switch between tagging schemes in an attempt to avoid 6255 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6255–6261 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the problem of unseen labels (Vilares et al., 2019), but that only increases the label inventory rather than restricting it to a finite set. Our approach, on the other hand, uses just 4 labels in its simplest formulation (hence the name tetra-tagging). 2 3 D E ⇒ C → → ⇒ → B ← A ← Left-corner parsing To achieve all of our desiderata, we combine aspects of the previouslymentioned approaches with ideas drawn from a long line of work on left-corner parsing (Rosenkrantz and Lewis, 1970; Nijholt, 1979; van Schijndel et al., 2013; Noji et al., 2016; Shain et al., 2016, inter alia). Much of past work highlights the benefits of a left-corner formulatio"
2020.acl-main.557,W09-3825,0,0.0488809,"1967) is quadratic in the length of the sentence, and the algorithm itself has cubic running time. This is true for both classical methods and more recent neural approaches (Durrett and Klein, 2015; Stern et al., 2017). The construction of a chart involves a non-trivial (quadratic) computation that is specialized to parsing, and implementing the CKY algorithm on a hardware accelerator is a nontrivial and hardware-specific task. 4 ⇐ Shift-reduce transition systems A number of parsers proposed in the literature can be categorized as shift-reduce parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013). These systems rely on generating sequences of actions, which need not be evenly distributed throughout the sentence. For example, the construction of a deep right-branching tree might involve a series of shift actions (one per word in the sentence), followed by equally many consecutive reduce actions that all cluster at the end of the sentence. Due to the uneven alignment between actions and locations in a sentence, neural network architectures in recent shift-reduce systems (Vinyals et al., 2015; Dyer et al., 2016; Liu and Zhang, 2017) generally follow an encoder-decoder"
2020.acl-main.557,P19-1230,0,0.0214378,"Missing"
2020.acl-main.557,P13-1043,0,0.0272011,"the length of the sentence, and the algorithm itself has cubic running time. This is true for both classical methods and more recent neural approaches (Durrett and Klein, 2015; Stern et al., 2017). The construction of a chart involves a non-trivial (quadratic) computation that is specialized to parsing, and implementing the CKY algorithm on a hardware accelerator is a nontrivial and hardware-specific task. 4 ⇐ Shift-reduce transition systems A number of parsers proposed in the literature can be categorized as shift-reduce parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013). These systems rely on generating sequences of actions, which need not be evenly distributed throughout the sentence. For example, the construction of a deep right-branching tree might involve a series of shift actions (one per word in the sentence), followed by equally many consecutive reduce actions that all cluster at the end of the sentence. Due to the uneven alignment between actions and locations in a sentence, neural network architectures in recent shift-reduce systems (Vinyals et al., 2015; Dyer et al., 2016; Liu and Zhang, 2017) generally follow an encoder-decoder approach with autor"
2020.emnlp-main.29,H94-1010,0,0.558939,"Missing"
2020.emnlp-main.29,P18-1033,0,0.0657336,"Missing"
2020.emnlp-main.29,J89-1008,0,0.632126,"Prediction 2 is semantically correct, and Prediction 1 is wrong. Exact string match judges prediction 2 to be wrong, which leads to false negatives. Only comparing denotations on database 1 judges prediction 1 to be correct, which leads to false positives. Test suite evaluation compares denotations on a set of databases and reduces false positives. Introduction A Text-to-SQL model translates natural language instructions to SQL queries that can be executed on databases and bridges the gap between expert programmers and non-experts. Accordingly, researchers have built a diversity of datasets (Dahl, 1989; Iyer et al., 2017; Zhong et al., 2017; Yu et al., 2018) and improved model performances (Xu et al., 2017; Suhr et al., 2018; Guo et al., 2019; Bogin et al., 2019a; Wang et al., 2020). However, evaluating the semantic accuracy of a Text-to-SQL model is a long-standing problem: we want to know whether the predicted SQL query has the same denotation as the gold for every possible database. “Single” denotation evaluation executes the predicted SQL query on one database and compares its denotation with that of the gold. It might create false positives, where a semantically different 1 Denotations"
2020.emnlp-main.29,P17-2059,0,0.030086,"Missing"
2020.emnlp-main.29,P19-1444,0,0.0348274,"e negatives. Only comparing denotations on database 1 judges prediction 1 to be correct, which leads to false positives. Test suite evaluation compares denotations on a set of databases and reduces false positives. Introduction A Text-to-SQL model translates natural language instructions to SQL queries that can be executed on databases and bridges the gap between expert programmers and non-experts. Accordingly, researchers have built a diversity of datasets (Dahl, 1989; Iyer et al., 2017; Zhong et al., 2017; Yu et al., 2018) and improved model performances (Xu et al., 2017; Suhr et al., 2018; Guo et al., 2019; Bogin et al., 2019a; Wang et al., 2020). However, evaluating the semantic accuracy of a Text-to-SQL model is a long-standing problem: we want to know whether the predicted SQL query has the same denotation as the gold for every possible database. “Single” denotation evaluation executes the predicted SQL query on one database and compares its denotation with that of the gold. It might create false positives, where a semantically different 1 Denotations: Gold: Alice, Bob Bob Predicted 1: Alice, Bob Alice, Bob Predicted 2: Alice, Bob Bob SELECT NAME FROM People Executes NAME AGE NAME AGE (missi"
2020.emnlp-main.29,P17-1089,0,0.258366,"2 is semantically correct, and Prediction 1 is wrong. Exact string match judges prediction 2 to be wrong, which leads to false negatives. Only comparing denotations on database 1 judges prediction 1 to be correct, which leads to false positives. Test suite evaluation compares denotations on a set of databases and reduces false positives. Introduction A Text-to-SQL model translates natural language instructions to SQL queries that can be executed on databases and bridges the gap between expert programmers and non-experts. Accordingly, researchers have built a diversity of datasets (Dahl, 1989; Iyer et al., 2017; Zhong et al., 2017; Yu et al., 2018) and improved model performances (Xu et al., 2017; Suhr et al., 2018; Guo et al., 2019; Bogin et al., 2019a; Wang et al., 2020). However, evaluating the semantic accuracy of a Text-to-SQL model is a long-standing problem: we want to know whether the predicted SQL query has the same denotation as the gold for every possible database. “Single” denotation evaluation executes the predicted SQL query on one database and compares its denotation with that of the gold. It might create false positives, where a semantically different 1 Denotations: Gold: Alice, Bob"
2020.emnlp-main.29,2020.acl-main.677,0,0.469856,"on database 1 judges prediction 1 to be correct, which leads to false positives. Test suite evaluation compares denotations on a set of databases and reduces false positives. Introduction A Text-to-SQL model translates natural language instructions to SQL queries that can be executed on databases and bridges the gap between expert programmers and non-experts. Accordingly, researchers have built a diversity of datasets (Dahl, 1989; Iyer et al., 2017; Zhong et al., 2017; Yu et al., 2018) and improved model performances (Xu et al., 2017; Suhr et al., 2018; Guo et al., 2019; Bogin et al., 2019a; Wang et al., 2020). However, evaluating the semantic accuracy of a Text-to-SQL model is a long-standing problem: we want to know whether the predicted SQL query has the same denotation as the gold for every possible database. “Single” denotation evaluation executes the predicted SQL query on one database and compares its denotation with that of the gold. It might create false positives, where a semantically different 1 Denotations: Gold: Alice, Bob Bob Predicted 1: Alice, Bob Alice, Bob Predicted 2: Alice, Bob Bob SELECT NAME FROM People Executes NAME AGE NAME AGE (missing WHERE) Alice 35 Alice 20 Bob 37 SELECT"
2020.emnlp-main.29,P19-1176,0,0.0212416,"Missing"
2020.emnlp-main.29,D18-1425,1,0.790782,"on 1 is wrong. Exact string match judges prediction 2 to be wrong, which leads to false negatives. Only comparing denotations on database 1 judges prediction 1 to be correct, which leads to false positives. Test suite evaluation compares denotations on a set of databases and reduces false positives. Introduction A Text-to-SQL model translates natural language instructions to SQL queries that can be executed on databases and bridges the gap between expert programmers and non-experts. Accordingly, researchers have built a diversity of datasets (Dahl, 1989; Iyer et al., 2017; Zhong et al., 2017; Yu et al., 2018) and improved model performances (Xu et al., 2017; Suhr et al., 2018; Guo et al., 2019; Bogin et al., 2019a; Wang et al., 2020). However, evaluating the semantic accuracy of a Text-to-SQL model is a long-standing problem: we want to know whether the predicted SQL query has the same denotation as the gold for every possible database. “Single” denotation evaluation executes the predicted SQL query on one database and compares its denotation with that of the gold. It might create false positives, where a semantically different 1 Denotations: Gold: Alice, Bob Bob Predicted 1: Alice, Bob Alice, Bob"
2020.emnlp-main.29,P19-1443,1,0.888653,"Missing"
2020.emnlp-main.389,W01-0713,0,0.560833,"Missing"
2020.emnlp-main.389,N19-1423,0,0.0731769,"roduction When developing a phrase structure grammar for a language, one powerful tool that linguists use is constituency tests. Given a sentence and some span within it, one type of constituency test involves modifying the sentence via some transformation (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical). If a span passes constituency tests, then linguists have evidence that it is a constituent. Motivated by this idea, as well as recent advancements in neural acceptability (grammaticality) models via pre-training (Warstadt et al., 2018; Devlin et al., 2019; Liu et al., 2019), in this paper we propose a method for unsupervised parsing that operationalizes the way linguists use constituency tests. Focusing on constituency tests that are judged via grammaticality, we begin by specifying a set of transformations that take as input a span within a sentence and output a new sentence (Section 3). Given these transformations, we then describe how to use a (potentially noisy) grammaticality model for parsing (Section 4). Specifically, we score the likelihood that a span is a constituent by applying the constituency tests and averaging their grammaticali"
2020.emnlp-main.389,N19-1116,0,0.303793,"ation of the RNNG (Dyer et al., 2016), an RNN model that defines a joint distribution over sentences and trees via shift and reduce operations. Unlike the PCFG, the URNNG makes no independence assumptions, making it more expressive but also harder to induce from scratch. Shen et al. (2018) proposed the Parsing-Reading-Predict Network (PRPN), where the latent tree structure determines the flow of information in a neural language model, and they found that optimizing for language modeling produced meaningful latent trees. On the other hand, the Deep Inside-Outside Recursive Autoencoder (DIORA) (Drozdov et al., 2019) computes a representation for each node in a tree by recursively combining child representations following the structure of the inside-outside algorithm, and it optimizes an autoencoder objective such that the representation for each leaf in the tree remains unchanged after an inside and outside pass. Extracting trees from neural language models: The Ordered Neuron (ON) model (Shen et al., 2019) modifies the LSTM to enforce a hierarchy of long- to short-term neurons, with the idea that the forget operation should naturally occur at phrase boundaries. After training on language modeling, they"
2020.emnlp-main.389,N16-1024,0,0.0287851,"model with tree-valued latent variables and optimizing it via EM, some of which can also be seen as probabilistic grammars parameterized by neural networks. For example, the compound PCFG (Kim et al., 2019a), found that the original PCFG is sufficient to induce trees if it uses a neural parameterization, and they further enhanced the model via latent sentence vectors to reduce the independence assumptions. Another model, the unsupervised recurrent neural network grammar (URNNG) (Kim et al., 2019b), uses variational inference over latent trees to perform unsupervised optimization of the RNNG (Dyer et al., 2016), an RNN model that defines a joint distribution over sentences and trees via shift and reduce operations. Unlike the PCFG, the URNNG makes no independence assumptions, making it more expressive but also harder to induce from scratch. Shen et al. (2018) proposed the Parsing-Reading-Predict Network (PRPN), where the latent tree structure determines the flow of information in a neural language model, and they found that optimizing for language modeling produced meaningful latent trees. On the other hand, the Deep Inside-Outside Recursive Autoencoder (DIORA) (Drozdov et al., 2019) computes a repr"
2020.emnlp-main.389,P19-1228,0,0.261391,"Missing"
2020.emnlp-main.389,N19-1114,0,0.139158,"edings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4798–4808, c November 16–20, 2020. 2020 Association for Computational Linguistics showing that constituency tests provide powerful inductive bias. Analyzing our parser (Section 8), we find that despite its strong numbers, it makes some mistakes that we might expect from the parser’s reliance on this class of constituency tests, like attaching modifying phrases incorrectly. As one possible solution to these shortcomings, we use our method to induce the unsupervised recurrent neural network grammar (URNNG) (Kim et al., 2019b) following the approach in Kim et al. (2019a), where we use our induced trees as supervision to initialize the RNNG model and then perform unsupervised fine tuning via language modeling. The resulting model achieves 67.9 F1 averaged over four random restarts, approaching the supervised binary tree RNNG with a gap of 4.9 points. 2 Related Work Grammar induction. There has been a long history of research on grammar induction. Here, we touch on just a couple threads of work most related to our method. Early works focused on building probabilistic context-free grammars (PCFGs) but found that ind"
2020.emnlp-main.389,P18-1249,1,0.847902,"implicitly. To induce more and specify less, another interesting line of future work would involve inducing the tests as well. Constituency Tests 4 We begin by specifying a set of constituency tests. The constituency tests we focus on involve transformation functions c : (sent, i, j) 7→ sent0 that take in a span and output a new sentence, and a judgment function g : sent 7→ {0, 1} that judges Parsing Algorithm With this set of transformations, in this section we describe how to parse sentences using a (potentially noisy) grammaticality model. In the supervised setting, Stern et al. (2017) and Kitaev and Klein (2018) 4800 showed that independently scoring each span and then choosing the tree with the best total score produced a very accurate and simple parser, while Klein and Manning (2002) showed a similar result in the unsupervised setting. Therefore, we also use a span-based approach. We will use gθ : sent 7→ [0, 1] to denote the grammaticality model with parameters θ, which outputs the probability that a given sentence is grammatical. First, we score each span by averaging the grammaticality judgments of its constituency tests, or sθ (sent, i, j) = 1 X gθ (c(sent, i, j)), |C| c∈C where C denotes the s"
2020.emnlp-main.389,P02-1017,1,0.791886,"ere has been a long history of research on grammar induction. Here, we touch on just a couple threads of work most related to our method. Early works focused on building probabilistic context-free grammars (PCFGs) but found that inducing them with expectationmaximization (EM) did not produce meaningful trees (Carroll and Charniak, 1992). We highlight some themes since then that have produced successful unsupervised parsers. Directly modeling spans rather than mediating structure through a grammar: In contrast with previous work based on probabilistic grammars, the constituent-context model of Klein and Manning (2002) proposed a different probabilistic formulation that modeled the constituency of each span directly, where each span yielded words conditioned on whether or not it was a constituent. Parsing then proceeded via minimum risk decoding (Smith and Eisner, 2006), where they chose the tree containing the maximum expected number of constituents. Explicitly defining criteria for what it means to be a constituent: Rather than designing a generative model over sentences and trees, Clark (2001) proposed that constituents could be identified based on their span statistics, e.g. the mutual information betwe"
2020.emnlp-main.389,P06-2101,0,0.793338,"onmaximization (EM) did not produce meaningful trees (Carroll and Charniak, 1992). We highlight some themes since then that have produced successful unsupervised parsers. Directly modeling spans rather than mediating structure through a grammar: In contrast with previous work based on probabilistic grammars, the constituent-context model of Klein and Manning (2002) proposed a different probabilistic formulation that modeled the constituency of each span directly, where each span yielded words conditioned on whether or not it was a constituent. Parsing then proceeded via minimum risk decoding (Smith and Eisner, 2006), where they chose the tree containing the maximum expected number of constituents. Explicitly defining criteria for what it means to be a constituent: Rather than designing a generative model over sentences and trees, Clark (2001) proposed that constituents could be identified based on their span statistics, e.g. the mutual information between the left and right contexts of the span. Finding external signals of constituency: To perform noun compound bracketings (“[ liver cell ] line” vs “liver [ cell line ]”), Nakov and Hearst (2005) extracted a series of features from Web text, like the freq"
2020.emnlp-main.389,P10-1130,0,0.0354157,"criteria for what it means to be a constituent: Rather than designing a generative model over sentences and trees, Clark (2001) proposed that constituents could be identified based on their span statistics, e.g. the mutual information between the left and right contexts of the span. Finding external signals of constituency: To perform noun compound bracketings (“[ liver cell ] line” vs “liver [ cell line ]”), Nakov and Hearst (2005) extracted a series of features from Web text, like the frequency of “liver-cell line” vs “liver cellline.” With a similar idea of extracting signal from Web text, Spitkovsky et al. (2010) found evidence for constituency from HTML markup, e.g. hyperlinks and italicized phrases. Designing neural latent variable models: Many works have taken the approach of designing a neural language model with tree-valued latent variables and optimizing it via EM, some of which can also be seen as probabilistic grammars parameterized by neural networks. For example, the compound PCFG (Kim et al., 2019a), found that the original PCFG is sufficient to induce trees if it uses a neural parameterization, and they further enhanced the model via latent sentence vectors to reduce the independence assum"
2020.emnlp-main.389,P17-1076,1,0.866168,"bias or encode it more implicitly. To induce more and specify less, another interesting line of future work would involve inducing the tests as well. Constituency Tests 4 We begin by specifying a set of constituency tests. The constituency tests we focus on involve transformation functions c : (sent, i, j) 7→ sent0 that take in a span and output a new sentence, and a judgment function g : sent 7→ {0, 1} that judges Parsing Algorithm With this set of transformations, in this section we describe how to parse sentences using a (potentially noisy) grammaticality model. In the supervised setting, Stern et al. (2017) and Kitaev and Klein (2018) 4800 showed that independently scoring each span and then choosing the tree with the best total score produced a very accurate and simple parser, while Klein and Manning (2002) showed a similar result in the unsupervised setting. Therefore, we also use a span-based approach. We will use gθ : sent 7→ [0, 1] to denote the grammaticality model with parameters θ, which outputs the probability that a given sentence is grammatical. First, we score each span by averaging the grammaticality judgments of its constituency tests, or sθ (sent, i, j) = 1 X gθ (c(sent, i, j)), |"
2020.emnlp-main.389,C18-1011,0,0.0445906,"Missing"
2020.emnlp-main.389,2021.ccl-1.108,0,0.0783071,"Missing"
2020.emnlp-main.389,J93-2004,0,0.072178,"Missing"
2020.emnlp-main.389,W05-0603,0,0.109682,"stituent. Parsing then proceeded via minimum risk decoding (Smith and Eisner, 2006), where they chose the tree containing the maximum expected number of constituents. Explicitly defining criteria for what it means to be a constituent: Rather than designing a generative model over sentences and trees, Clark (2001) proposed that constituents could be identified based on their span statistics, e.g. the mutual information between the left and right contexts of the span. Finding external signals of constituency: To perform noun compound bracketings (“[ liver cell ] line” vs “liver [ cell line ]”), Nakov and Hearst (2005) extracted a series of features from Web text, like the frequency of “liver-cell line” vs “liver cellline.” With a similar idea of extracting signal from Web text, Spitkovsky et al. (2010) found evidence for constituency from HTML markup, e.g. hyperlinks and italicized phrases. Designing neural latent variable models: Many works have taken the approach of designing a neural language model with tree-valued latent variables and optimizing it via EM, some of which can also be seen as probabilistic grammars parameterized by neural networks. For example, the compound PCFG (Kim et al., 2019a), found"
2020.tacl-1.36,P96-1009,0,0.132698,"uces the same result, or an alternative interpretation that is also contextually appropriate. Count 8 Related work 3 The view of dialogue as an interactive process of shared plan synthesis dates back to Grosz and Sidner’s earliest work on discourse structure (1986; 1988). That work represents the state of a dialogue as a predicate recognizing whether a desired piece of information has been communicated or change in world state effected. Goals can be refined via questions and corrections from both users and agents. The only systems to attempt full versions of this shared-plans framework (e.g., Allen et al., 1996; Rich et al., 2001) required inputs that could be parsed under a predefined grammar. Subsequent research on dialogue understanding has largely focused on two simpler subtasks: Contextual semantic parsing approaches focus on complex language understanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2"
2020.tacl-1.36,D18-1547,0,0.165132,"65 1,052 3,315 Dataflow inline .729 .696 .665 .606 .642 .533 .574 .465 .697 .631 .565 .474 Dataflow inline refer inline both TRADE Table 2: SMCalFlow results. Agent action accuracy is significantly higher than a baseline without metacomputation, especially on turns that involve a reference (Ref. Turns) or revision (Rev. Turns) to earlier turns in the dialogue (p &lt; 10−6 , McNemar’s test). Joint Goal Dialogue Prefix .467 .447 .467 .454 .220 .202 .205 .168 3.07 2.97 2.90 2.73 Table 3: MultiWOZ 2.1 test set results. TRADE (Wu et al., 2019) results are from the public implementation. “Joint Goal” (Budzianowski et al., 2018) is average dialogue state exact-match, “Dialogue” is average dialogue-level exact-match, and “Prefix” is the average number of turns before an incorrect prediction. Within each column, the best result is boldfaced, along with all results that are not significantly worse (p &lt; 0.05, paired permutation test). Moreover, all of “Dataflow,” “inline refer,” and “inline both” have higher dialogue accuracy than TRADE (p &lt; 0.005). els that train on inlined metacomputation. These experiments make it possible to evaluate the importance of explicit dataflow manipulation compared to a standard contextual s"
2020.tacl-1.36,D19-1459,0,0.0502204,"bling side-byside comparisons and experiments with alternative representations. We provide full conversion scripts for MultiWOZ. 567 9 teractive dialogues. It is assumed that any user intent can be represented with a flat structure consisting of a categorical dialogue act and a mapping between a fixed set of slots and string-valued fillers. Existing fine-grained dialogue act schemes (Stolcke et al., 2000) can distinguish among a range of communicative intents not modeled by our approach, and slot-filling representations have historically been easier to predict (Zue et al., 1994) and annotate (Byrne et al., 2019). But while recent variants support interaction between related slots (Budzianowski et al., 2018) and fixed-depth hierarchies of slots (Gupta et al., 2018), modern slot-filling approaches remain limited in their support for semantic compositionality. By contrast, our approach supports user requests corresponding to general compositional programs. Conclusions We have presented a representational framework for task-oriented dialogue modeling based on dataflow graphs, in which dialogue agents predict a sequence of compositional updates to a graphical state representation. This approach makes it p"
2020.tacl-1.36,P17-1167,0,0.0181575,"bout underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse with explicit graph operators. Slot-filling approaches (Pieraccini et al., 1992) model simpler utterances in the context of full, inTable 4: Manual classification of 100 model errors on the SMCalFlow dataset. The largest categories are underprediction (omitting steps from agent programs), entity linking (errors in extraction of entities from user utterances, fencing (classifying a user request as outof-scope), and ambiguity (user utterances with multiple possible inter"
2020.tacl-1.36,P17-4012,0,0.0122901,"propriate type constraint, provided that the reference resolution heuristic would retrieve the correct string from earlier in the dataflow. This covers references like the same day. Otherwise, our re-annotation retains the literal string value. Data statistics are shown in Table 1. To the best of our knowledge, SMCalFlow is the largest annotated task-oriented dialogue dataset to date. Compared to MultiWOZ, it features a larger user vocabulary, a more complex space of statemanipulation primitives, and a long tail of agent programs built from numerous function calls and deep composition. 7 NMT (Klein et al., 2017) pointer-generator network (See et al., 2017), a sequence-to-sequence model that can copy tokens from the source sequence while decoding. Our goal is to demonstrate that dataflow-based representations benefit standard neural model architectures. Dataflowspecific modeling might improve on this baseline, and we leave this as a challenge for future work. For each user turn i, we linearize the target program into a sequence of tokens zi . This must be predicted from the dialogue context— namely the concatenated source sequence xi−c zi−c · · · xi−1 zi−1 xi (for SMCalFlow) or xi−c yi−c · · · xi−1 yi"
2020.tacl-1.36,J94-4002,0,0.175555,"tate representations. While a complete description of dataflow-based language generation is beyond the scope of this paper, we briefly describe the components of the generation system relevant to the understanding system presented here. 3 Reference resolution In a dialogue, entities that have been introduced once may be referred to again. In dataflow dialogues, the entities available for reference are given by the nodes in the dataflow graph. Entities are salient to conversation participants to different degrees, and their relative salience determines the ways in which they may be referenced (Lappin and Leass, 1994). For example, it generally refers to the most salient non-human entity, while more specific expressions like the Friday meeting are needed to refer to accessible but less salient entities. Not all references to entities are overt: if the agent says “You have a meeting tomorrow” and the user responds “What time?”, the agent must predict the implicit reference to a salient event. 559 Dataflow pointers We have seen that refer is used to find referents for referring expressions. In general, these referents may be existing dataflow nodes or new subgraphs for newly mentioned entities. We now give m"
2020.tacl-1.36,J86-3001,0,0.780827,"Missing"
2020.tacl-1.36,W18-6322,0,0.0167485,"ser utterances with multiple possible interpretations). See §7 for discussion. Error analysis Beyond the quantitative results shown in Tables 2–3, we manually analyzed 100 SMCalFlow turns where our model mispredicted. Table 4 breaks down the errors by type. Three categories involve straightforward parsing errors. In underprediction errors, the model fails to predict some computation (e.g., a search constraint or property extractor) specified in the user request. This behavior is not specific to our system: under-length predictions are also welldocumented in neural machine translation systems (Murray and Chiang, 2018). In entity linking errors, the model correctly identifies the presence of an entity mention in the input utterance, but uses it incorrectly in the input plan. Sometimes the entity that appears in the plan is hallucinated, appearing nowhere in the utterance; sometimes the entity is cast to a wrong type (e.g., locations interpreted as event names) used in the wrong field or extracted with wrong boundaries. In fencing errors, the model interprets an out-of-scope user utterance as an interpretable command, or vice-versa versions of the full dataset, and inlined and non-inlined versions of our mod"
2020.tacl-1.36,D18-1300,0,0.0282209,"It is assumed that any user intent can be represented with a flat structure consisting of a categorical dialogue act and a mapping between a fixed set of slots and string-valued fillers. Existing fine-grained dialogue act schemes (Stolcke et al., 2000) can distinguish among a range of communicative intents not modeled by our approach, and slot-filling representations have historically been easier to predict (Zue et al., 1994) and annotate (Byrne et al., 2019). But while recent variants support interaction between related slots (Budzianowski et al., 2018) and fixed-depth hierarchies of slots (Gupta et al., 2018), modern slot-filling approaches remain limited in their support for semantic compositionality. By contrast, our approach supports user requests corresponding to general compositional programs. Conclusions We have presented a representational framework for task-oriented dialogue modeling based on dataflow graphs, in which dialogue agents predict a sequence of compositional updates to a graphical state representation. This approach makes it possible to represent and learn from complex, natural dialogues. Future work might focus on improving prediction by introducing learned implementations of r"
2020.tacl-1.36,H90-1021,0,0.582952,"ted or change in world state effected. Goals can be refined via questions and corrections from both users and agents. The only systems to attempt full versions of this shared-plans framework (e.g., Allen et al., 1996; Rich et al., 2001) required inputs that could be parsed under a predefined grammar. Subsequent research on dialogue understanding has largely focused on two simpler subtasks: Contextual semantic parsing approaches focus on complex language understanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach"
2020.tacl-1.36,D14-1162,0,0.0839852,"with a separator token that indicates the speaker (user or agent). Our formulation of context for MultiWOZ is standard (e.g., Wu et al., 2019). We take the source and target vocabularies to consist of all words that occur in (respectively) the source and target sequences in training data, as just defined. The model is trained using the Adam optimizer (Kingma and Ba, 2015) with the maximum likelihood objective. We use 0.001 as the learning rate. Training ends when there have been two different epochs that increased the development loss. We use Glove800B-300d (cased) and Glove6B300d (uncased) (Pennington et al., 2014) to initialize the vocabulary embeddings for the SMCalFlow and MultiWoZ experiments, respectively. The context window size c, hidden layer size d, number of hidden layers l, and dropout rates r are selected based on the agent action accuracy (for SMCalFlow) or dialogue-level exact match (for MultiWoZ) on the development set from {2, 4, 10}, {256, 300, 320, 384}, {1, 2, 3}, {0.3, 0.5, 0.7} respectively. Approximate 1-best decoding uses a beam of size 5. Quantitative evaluation Table 2 shows results for the SMCalFlow dataset. We report program accuracy: specifically, exact-match accuracy of the"
2020.tacl-1.36,P17-1062,0,0.0717515,"Missing"
2020.tacl-1.36,P19-1078,0,0.0158945,"work. For each user turn i, we linearize the target program into a sequence of tokens zi . This must be predicted from the dialogue context— namely the concatenated source sequence xi−c zi−c · · · xi−1 zi−1 xi (for SMCalFlow) or xi−c yi−c · · · xi−1 yi−1 xi (for MultiWOZ 2.1). Here c is a context window size, xj is the user utterance at user turn j, yj is the agent’s naturallanguage response, and zj is the linearized agent program. Each sequence xj , yj , or zj begins with a separator token that indicates the speaker (user or agent). Our formulation of context for MultiWOZ is standard (e.g., Wu et al., 2019). We take the source and target vocabularies to consist of all words that occur in (respectively) the source and target sequences in training data, as just defined. The model is trained using the Adam optimizer (Kingma and Ba, 2015) with the maximum likelihood objective. We use 0.001 as the learning rate. Training ends when there have been two different epochs that increased the development loss. We use Glove800B-300d (cased) and Glove6B300d (uncased) (Pennington et al., 2014) to initialize the vocabulary embeddings for the SMCalFlow and MultiWoZ experiments, respectively. The context window s"
2020.tacl-1.36,P17-1099,0,0.0599554,"Missing"
2020.tacl-1.36,J00-3003,0,0.766051,"Missing"
2020.tacl-1.36,N18-1203,0,0.0172756,"goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse with explicit graph operators. Slot-filling approaches (Pieraccini et al., 1992) model simpler utterances in the context of full, inTable 4: Manual classification of 100 model errors on the SMCalFlow dataset. The largest categories are underprediction (omitting steps from agent programs), entity linking (errors in extraction of entities from user utterances, fencing (classifying a user request as outof-scope), and ambiguity (user utterances with multiple possible interpretations). See §7"
2020.tacl-1.36,P19-1443,0,0.056047,"state effected. Goals can be refined via questions and corrections from both users and agents. The only systems to attempt full versions of this shared-plans framework (e.g., Allen et al., 1996; Rich et al., 2001) required inputs that could be parsed under a predefined grammar. Subsequent research on dialogue understanding has largely focused on two simpler subtasks: Contextual semantic parsing approaches focus on complex language understanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse wi"
2020.tacl-1.36,Q14-1042,0,0.0241672,", Allen et al., 1996; Rich et al., 2001) required inputs that could be parsed under a predefined grammar. Subsequent research on dialogue understanding has largely focused on two simpler subtasks: Contextual semantic parsing approaches focus on complex language understanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse with explicit graph operators. Slot-filling approaches (Pieraccini et al., 1992) model simpler utterances in the context of full, inTable 4: Manual classification of 100 model errors o"
2020.tacl-1.36,E17-1042,0,0.105146,"Missing"
2020.tacl-1.36,P09-1110,0,0.038465,"derstanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse with explicit graph operators. Slot-filling approaches (Pieraccini et al., 1992) model simpler utterances in the context of full, inTable 4: Manual classification of 100 model errors on the SMCalFlow dataset. The largest categories are underprediction (omitting steps from agent programs), entity linking (errors in extraction of entities from user utterances, fencing (classifying a user request as outof-scope), and ambiguity (user utterances with mul"
2020.tacl-1.36,W16-3601,0,0.0677409,"Missing"
2020.tacl-1.36,H94-1037,0,0.573739,"odel’s test set predictions, enabling side-byside comparisons and experiments with alternative representations. We provide full conversion scripts for MultiWOZ. 567 9 teractive dialogues. It is assumed that any user intent can be represented with a flat structure consisting of a categorical dialogue act and a mapping between a fixed set of slots and string-valued fillers. Existing fine-grained dialogue act schemes (Stolcke et al., 2000) can distinguish among a range of communicative intents not modeled by our approach, and slot-filling representations have historically been easier to predict (Zue et al., 1994) and annotate (Byrne et al., 2019). But while recent variants support interaction between related slots (Budzianowski et al., 2018) and fixed-depth hierarchies of slots (Gupta et al., 2018), modern slot-filling approaches remain limited in their support for semantic compositionality. By contrast, our approach supports user requests corresponding to general compositional programs. Conclusions We have presented a representational framework for task-oriented dialogue modeling based on dataflow graphs, in which dialogue agents predict a sequence of compositional updates to a graphical state repres"
2021.acl-long.284,P16-1154,0,0.0345369,"proposed value v is a constant, we embed it by applying the utterance encoder on a string rendering of the value. The set of constants is automatically extracted from the training data (see Appendix B). Copies. Copies are string values that correspond to substrings of the user utterance (e.g., person names). String values can only enter the program through copying, as they are not in the set of constants (i.e., they cannot be “hallucinated” by the model; see Pasupat and Liang, 2015; Nie et al., 2019). One might try to construct an approach based on a standard token-based copy mechanism (e.g., Gu et al., 2016). However, this would allow copying non-contiguous spans and would also require marginalizing over identical tokens as opposed to spans, resulting in more ambiguity. Instead, we propose a mechanism that enables the decoder to copy contiguous spans directly from the utterance. Its goal is to produce a score for each of the U (U + 1)/2 possible utterance spans. Na¨ıvely, this would result in a computational cost that is quadratic in the utterance length U , and so we instead chose a simple scoring model that avoids it. Similar to Stern et al. (2017) and Kuribayashi et al. (2019), we assume that"
2021.acl-long.284,P17-1097,0,0.0537063,"Missing"
2021.acl-long.284,P16-1002,0,0.0525653,"Missing"
2021.acl-long.284,C12-1083,1,0.706699,"Missing"
2021.acl-long.284,P17-4012,0,0.015823,"2 87.1 86.2 87.0 87.1 86.7 86.9 80.2 75.0 86.9 87.1 88.3 86.5 88.1 88.3 87.4 88.2 80.6 76.5 87.4 88.3 (b) Ablation study. Table 2: Validation set exact match accuracy across varying amounts of training data (each subset is sampled uniformly at random). The best results in each case are shown in bold red and are underlined. In order to further understand the performance characteristics of our model and quantify the impact of each modeling contribution, we also compare to a variety of other models and ablated versions of our model. We implemented the following baselines: – Seq2Seq: The OpenNMT (Klein et al., 2017) implementation of a pointer-generator network (See et al., 2017) that predicts linearized plans represented as S-expressions and is able to copy tokens from the utterance while decoding. This model is very similar to the model used by Semantic Machines et al. (2020) and represents the current state-of-the-art for SMC AL F LOW.8 – Seq2Tree: The same as Seq2Seq, except that it generates invocations in a top-down, pre-order program traversal. Each invocation is embedded as a unique item in the output vocabulary. Note that SMC ALFLOW contains re-entrant programs represented with LISP-style let bi"
2021.acl-long.284,D17-1160,1,0.835119,"s sminfo@microsoft.com Abstract the output (Suhr et al., 2018). While this approach can capture arbitrary dependencies between inputs and outputs, it comes at the cost of sample- and computational inefficiency. We propose a new “value-agnostic” approach to contextual semantic parsing driven by type-based representations of the dialogue history and functionbased representations of the generated programs. Types and functions have long served as a foundation for formal reasoning about programs, but their use in neural semantic parsing has been limited, e.g., to constraining the hypothesis space (Krishnamurthy et al., 2017), guiding data augmentation (Jia and Liang, 2016), and coarsening in coarse-to-fine models (Dong and Lapata, 2018). We show that representing conversation histories and partial programs via the types and functions they contain enables fast, accurate, and sample-efficient contextual semantic parsing. We propose a neural encoder– decoder contextual semantic parsing model which, in contrast to prior work: Conversational semantic parsers map user utterances to executable programs given dialogue histories composed of previous utterances, programs, and system responses. Existing parsers typically co"
2021.acl-long.284,P19-1464,0,0.0282221,"sed copy mechanism (e.g., Gu et al., 2016). However, this would allow copying non-contiguous spans and would also require marginalizing over identical tokens as opposed to spans, resulting in more ambiguity. Instead, we propose a mechanism that enables the decoder to copy contiguous spans directly from the utterance. Its goal is to produce a score for each of the U (U + 1)/2 possible utterance spans. Na¨ıvely, this would result in a computational cost that is quadratic in the utterance length U , and so we instead chose a simple scoring model that avoids it. Similar to Stern et al. (2017) and Kuribayashi et al. (2019), we assume that the score for a span factorizes, and define the embedding of each span value as the concatenation of the contextual embeddings of the first and last tokens of the span, v˜ = [hkuttstart ; hkuttend ]. To compute the copy scores we also concatenate hi,a dec with itself in Equation 8. Entities. Entities are treated the same way as copies, except that instead of scoring all spans of the input, we only score spans proposed by the external entity proposers discussed in §2.1. Specifically, the proposers provide the model with a list of candidate entities that are each described by an"
2021.acl-long.284,P14-1135,0,0.0237253,"urs include SMBOP (Rubin and Berant, 2020) and BUSTLE (Odena et al., 2020). Context-Dependent Semantic Parsing. Prior work on conversational semantic parsing mainly focuses on the decoder, with few efforts on incorporating the dialogue history information in the encoder. Recent work on context-dependent semantic parsing (e.g., Suhr et al., 2018; Yu et al., 2019) conditions on explicit representations of user utterances and programs with a neural encoder. While this results in highly expressive models, it also increases the risk of overfitting. Contrary to this, Zettlemoyer and Collins (2009), Lee et al. (2014) and Semantic Machines et al. (2020) do not use context to resolve references at all. They instead predict context-independent logical forms that are resolved in a separate step. Our approach occupies a middle ground: when combined with local program representations, types, even without any value information, provide enough information to resolve context-dependent meanings that cannot be derived from isolated sentences. The specific mechanism we use to do this “infuses” contextual type information into input sentence representations, in a manner reminiscent of attention flow models from the QA"
2021.acl-long.284,D16-1262,0,0.0663869,"Missing"
2021.acl-long.284,P17-1099,0,0.0104458,"88.3 87.4 88.2 80.6 76.5 87.4 88.3 (b) Ablation study. Table 2: Validation set exact match accuracy across varying amounts of training data (each subset is sampled uniformly at random). The best results in each case are shown in bold red and are underlined. In order to further understand the performance characteristics of our model and quantify the impact of each modeling contribution, we also compare to a variety of other models and ablated versions of our model. We implemented the following baselines: – Seq2Seq: The OpenNMT (Klein et al., 2017) implementation of a pointer-generator network (See et al., 2017) that predicts linearized plans represented as S-expressions and is able to copy tokens from the utterance while decoding. This model is very similar to the model used by Semantic Machines et al. (2020) and represents the current state-of-the-art for SMC AL F LOW.8 – Seq2Tree: The same as Seq2Seq, except that it generates invocations in a top-down, pre-order program traversal. Each invocation is embedded as a unique item in the output vocabulary. Note that SMC ALFLOW contains re-entrant programs represented with LISP-style let bindings. Both the Seq2Tree and Seq2Seq are unaware of the special"
2021.acl-long.284,2020.acl-main.703,0,0.0165634,"Unlike Seq2Seq and Seq2Tree, this model can only produce well-formed and well-typed programs. It also makes use of the same entity proposers (§2.1) similar to our model, and it can atomically copy spans of up to 15 tokens by treating them as additional proposed entities. Furthermore, it uses the linear history encoder that is described in the next paragraph. Like our model, re-entrancies are represented as references to previous outputs in the predicted sequence. Table 3: Validation set exact match accuracy for singleturn semantic parsing datasets. Note that Aghajanyan et al. (2020) use BART (Lewis et al., 2020), a large pretrained encoder. The best results for each dataset are shown in bold red and are underlined. mechanism with a linear function over a multihot embedding of the history types. The results, shown in Table 2b, indicate that all of our features play a role in improving accuracy. Perhaps most importantly though, the “value dependence” ablation shows that our function-based program representations are indeed important, and the “previous turn” ablation shows that our typebased program representations are also important. Furthermore, the impact of both these modeling decisions grows larger"
2021.acl-long.284,D16-1183,0,0.0136943,"ork in neural semantic parsing and also context-dependent semantic parsing. Neural Semantic Parsing. While there was a brief period of interest in using unstructured sequence models for semantic parsing (e.g., Andreas 3673 et al., 2013; Dong and Lapata, 2016), most research on semantic parsing has used tree- or graph-shaped decoders that exploit program structure. Most such approaches use this structure as a constraint while decoding, filling in function arguments one-at-atime, in either a top-down fashion (e.g., Dong and Lapata, 2016; Krishnamurthy et al., 2017) or a bottom-up fashion (e.g., Misra and Artzi, 2016; Cheng et al., 2018). Both directions can suffer from exposure bias and search errors during decoding: in top-down when there’s no way to realize an argument of a given type in the current context, and in bottom-up when there are no functions in the programming language that combine the predicted arguments. To this end, there has been some work on global search with guarantees for neural semantic parsers (e.g., Lee et al., 2016) but it is expensive and makes certain strong assumptions. In contrast to this prior work, we use program structure not just as a decoder constraint but as a source of"
2021.acl-long.284,P19-1256,0,0.0121943,"lues that are always proposed, so the decoder always has the option of generating them. If the source s for the proposed value v is a constant, we embed it by applying the utterance encoder on a string rendering of the value. The set of constants is automatically extracted from the training data (see Appendix B). Copies. Copies are string values that correspond to substrings of the user utterance (e.g., person names). String values can only enter the program through copying, as they are not in the set of constants (i.e., they cannot be “hallucinated” by the model; see Pasupat and Liang, 2015; Nie et al., 2019). One might try to construct an approach based on a standard token-based copy mechanism (e.g., Gu et al., 2016). However, this would allow copying non-contiguous spans and would also require marginalizing over identical tokens as opposed to spans, resulting in more ambiguity. Instead, we propose a mechanism that enables the decoder to copy contiguous spans directly from the utterance. Its goal is to produce a score for each of the U (U + 1)/2 possible utterance spans. Na¨ıvely, this would result in a computational cost that is quadratic in the utterance length U , and so we instead chose a sim"
2021.acl-long.284,K17-1026,0,0.0600814,"Missing"
2021.acl-long.284,N18-1203,0,0.034024,"Missing"
2021.acl-long.284,D14-1135,0,0.0604812,"Missing"
2021.acl-long.284,D18-2002,0,0.0252359,"Missing"
2021.acl-long.284,P19-1443,0,0.104352,"ate that simple representations are key to effective generalization in conversational semantic parsing. 1 1. uses a compact yet informative representation of discourse context in the encoder that considers only the types of salient entities that were predicted by the model in previous turns or that appeared in the execution results of the predicted programs, and Introduction Conversational semantic parsers, which translate natural language utterances into executable programs while incorporating conversational context, play an increasingly central role in systems for interactive data analysis (Yu et al., 2019), instruction following (Guu et al., 2017), and task-oriented dialogue (Zettlemoyer and Collins, 2009). An example of this task is shown in Figure 1. Typical models are based on an autoregressive sequence prediction approach, in which a detailed representation of the dialogue history is concatenated to the input sequence, and predictors condition on this sequence and all previously generated components of 2. conditions the decoder state on the sequence of function invocations so far, without conditioning on any concrete values passed as arguments to the functions. Our model substantially impro"
2021.acl-long.284,D07-1071,0,0.257344,"Missing"
2021.acl-long.284,P09-1110,0,0.310471,"ure 2: Illustration of the revise meta-computation operator (§2.1) used in our program representations. This operator can remove the need to copy program fragments from the dialogue history. 2.1 Preliminaries Our approach assumes that programs have type annotations on all values and function calls, similar to the setting of Krishnamurthy et al. (2017).1 Furthermore, we assume that program prediction is local in that it does not require program fragments to be copied from the dialogue history (but may still depend on history in other ways). Several formalisms, including the typed references of Zettlemoyer and Collins (2009) and the meta-computation operators of Semantic Machines et al. (2020), make it possible to produce local program annotations even for dialogues like the one depicted in Figure 2, which reuse past computations. We transformed the datasets in our experiments to use such metacomputation operators (see Appendix C). We also optionally make use of entity proposers, similar to Krishnamurthy et al. (2017), which annotate spans from the current utterance with typed values. For example, the span “one” in “Change it to one” might be annotated with the value 1 of type Number. These values are scored by t"
2021.acl-long.284,P19-1009,0,0.0467415,"Missing"
2021.acl-long.284,N15-1162,0,0.054247,"Missing"
2021.emnlp-main.163,P15-1029,0,0.0276025,"resolution and generation due to the spatial grounding. models of pragmatics have also been used for dialogue, and while these approaches plan or infer across multiple turns (which our work does not do explicitly), they have either involved ungrounded settings (Kim et al., 2020) or constrained language (Vogel et al., 2013; Khani et al., 2018). Referring expressions. A long line of past work on referring expression grounding has tackled generation (Dale, 1989; Dale and Reiter, 1995; Viethen et al., 2011; Krahmer and van Deemter, 2012), interpretation (Schlangen et al., 2009; Liu et al., 2013; Kennington and Schlangen, 2015) or both (Heeman, 1991; Mao et al., 2016; Yu et al., 2017). Closest to ours is the work of Takmaz et al. (2020), which builds models for reference interpretation and generation in the rich PhotoBook corpus (Haber et al., 2019), focusing on a non-interactive setting with static evaluation on reference chains extracted from human-human dialogues. 8 Collaborative games. The closest work on dialogue systems for collaborative grounded tasks has focused on tasks with different properties from ours, as discussed in Section 1. A closely related task to the shared visual reference game we pursue here i"
2021.emnlp-main.163,Q18-1037,0,0.0274979,"imum worker overall skills up to the 68th percentile. 2137 tracking entities identified in text (Williams et al., 2017; He et al., 2017). We also find improvements from an entity-centric approach with a structured memory, although our domain involves more challenging entity resolution and generation due to the spatial grounding. models of pragmatics have also been used for dialogue, and while these approaches plan or infer across multiple turns (which our work does not do explicitly), they have either involved ungrounded settings (Kim et al., 2020) or constrained language (Vogel et al., 2013; Khani et al., 2018). Referring expressions. A long line of past work on referring expression grounding has tackled generation (Dale, 1989; Dale and Reiter, 1995; Viethen et al., 2011; Krahmer and van Deemter, 2012), interpretation (Schlangen et al., 2009; Liu et al., 2013; Kennington and Schlangen, 2015) or both (Heeman, 1991; Mao et al., 2016; Yu et al., 2017). Closest to ours is the work of Takmaz et al. (2020), which builds models for reference interpretation and generation in the rich PhotoBook corpus (Haber et al., 2019), focusing on a non-interactive setting with static evaluation on reference chains extra"
2021.emnlp-main.163,2020.emnlp-main.65,0,0.0426506,"ignificant at the p ≤ 0.05 level by a one-tailed t-test for minimum worker overall skills up to the 68th percentile. 2137 tracking entities identified in text (Williams et al., 2017; He et al., 2017). We also find improvements from an entity-centric approach with a structured memory, although our domain involves more challenging entity resolution and generation due to the spatial grounding. models of pragmatics have also been used for dialogue, and while these approaches plan or infer across multiple turns (which our work does not do explicitly), they have either involved ungrounded settings (Kim et al., 2020) or constrained language (Vogel et al., 2013; Khani et al., 2018). Referring expressions. A long line of past work on referring expression grounding has tackled generation (Dale, 1989; Dale and Reiter, 1995; Viethen et al., 2011; Krahmer and van Deemter, 2012), interpretation (Schlangen et al., 2009; Liu et al., 2013; Kennington and Schlangen, 2015) or both (Heeman, 1991; Mao et al., 2016; Yu et al., 2017). Closest to ours is the work of Takmaz et al. (2020), which builds models for reference interpretation and generation in the rich PhotoBook corpus (Haber et al., 2019), focusing on a non-int"
2021.emnlp-main.163,P19-1651,0,0.0969994,"t. Our pragmatic genparts of their own context. eration procedure selects referents to describe as We present a grounded pragmatic dialogue sys- well as choosing how to describe them, for example tem which collaborates successfully with people focusing on the light one (Figure 1). 2130 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2130–2147 c November 7–11, 2021. 2021 Association for Computational Linguistics Much past work that has constructed systems for grounded collaborative dialogue has focused on settings that have asymmetric player roles (Kim et al., 2019; de Vries et al., 2018; Das et al., 2018, 2017), are fully-observable, or are grounded in symbolic attributes (He et al., 2017). In contrast, we focus on the O NE C OMMON corpus and task (Udagawa and Aizawa, 2019), which is symmetric, partially-observable, and has relatively complex spatial and perceptual grounding. These traits necessitate complex dialogue strategies such as common grounding, coordination, clarification questions, and nuanced acknowledgment (Udagawa and Aizawa, 2019), leading to the task being challenging even for pairs of human partners. Past work on O NE C OMMON has focuse"
2021.emnlp-main.163,J12-1006,0,0.0359706,"Missing"
2021.emnlp-main.163,W13-4010,0,0.0340169,"challenging entity resolution and generation due to the spatial grounding. models of pragmatics have also been used for dialogue, and while these approaches plan or infer across multiple turns (which our work does not do explicitly), they have either involved ungrounded settings (Kim et al., 2020) or constrained language (Vogel et al., 2013; Khani et al., 2018). Referring expressions. A long line of past work on referring expression grounding has tackled generation (Dale, 1989; Dale and Reiter, 1995; Viethen et al., 2011; Krahmer and van Deemter, 2012), interpretation (Schlangen et al., 2009; Liu et al., 2013; Kennington and Schlangen, 2015) or both (Heeman, 1991; Mao et al., 2016; Yu et al., 2017). Closest to ours is the work of Takmaz et al. (2020), which builds models for reference interpretation and generation in the rich PhotoBook corpus (Haber et al., 2019), focusing on a non-interactive setting with static evaluation on reference chains extracted from human-human dialogues. 8 Collaborative games. The closest work on dialogue systems for collaborative grounded tasks has focused on tasks with different properties from ours, as discussed in Section 1. A closely related task to the shared visua"
2021.emnlp-main.163,P19-1059,0,0.0262788,"ics (Grice, 1975) builds on a large body of work in the RSA framework (Frank and Goodman, 2012; Goodman and Frank, 2016), which models how speakers and listeners reason about each other to communicate successfully. The most similar applications to ours in past work on computational pragmatics have been to single-turn grounded reference tasks (rather than dialogue), with much smaller and unstructured spaces of referents than ours,11 such as discriminative image captioning (Vedantam et al., 2017; Andreas and Klein, 2016; CohnGordon et al., 2018) and referent identification (Monroe et al., 2017; McDowell and Goodman, 2019; White et al., 2020). Explicit speaker–listener Conclusion We presented a modular, reference-centric approach to a challenging partially-observable grounded collaborative dialogue task. Our approach is centered around a structured referent grounding module, which we use (1) to interpret a partner’s utterances and (2) to enable a pragmatic generation procedure that encourages the agent’s utterances to be able to be understood in context. We perform, for the first time, human evaluations on the full dialogue task, finding that our system cooperates with people substantially more successfully th"
2021.emnlp-main.163,Q17-1023,0,0.114703,"ency and informativity during pragmatic utterance generation (Section 4 and Algorithm 1). A set of paired candidate referents (from the mention prediction module) and utterances (from the utterance generation module) is rescored using L(r, u) (Equation 1), a weighted geometric mean of scores from the mention prediction, utterance, and reference resolution modules. The pair of referent and utterance that maximizes this score is chosen as a response. This objective generalizes the typical RSA setup (as implemented by the weighted pragmatic inference objective of e.g., Andreas and Klein 2016 and Monroe et al. 2017), which chooses how to describe a given context (i.e., choosing an utterance u), to also choose what context to describe (i.e., choosing the referents r). Our objective also models the tradeoff, explored in past work on referring expression generation (Dale, 1989; Jordan and Walker, 2005; Viethen et al., 2011), between producing utterances relevant in the discourse and world context and producing utterances that are discriminative. We use PU and PM to model discourse and world relevance, PS to model discriminability, and the weights w to empirically model the tradeoff between them. Given the c"
2021.emnlp-main.163,P19-1537,0,0.020156,"logue systems for collaborative grounded tasks has focused on tasks with different properties from ours, as discussed in Section 1. A closely related task to the shared visual reference game we pursue here is the PhotoBook task (Haber et al., 2019), although a dialogue system has not been constructed for it. Other work on grounded collaborative language games includes collection games (Potts, 2012; Suhr et al., 2019), navigation and interactive question games (Thomason et al., 2019; Nguyen and Daumé III, 2019; Ilinykh et al., 2019), and construction tasks (Wang et al., 2017; Kim et al., 2019; Narayan-Chen et al., 2019). Pragmatics. Our approach to pragmatics (Grice, 1975) builds on a large body of work in the RSA framework (Frank and Goodman, 2012; Goodman and Frank, 2016), which models how speakers and listeners reason about each other to communicate successfully. The most similar applications to ours in past work on computational pragmatics have been to single-turn grounded reference tasks (rather than dialogue), with much smaller and unstructured spaces of referents than ours,11 such as discriminative image captioning (Vedantam et al., 2017; Andreas and Klein, 2016; CohnGordon et al., 2018) and referent"
2021.emnlp-main.163,D19-1063,0,0.0544546,"Missing"
2021.emnlp-main.163,2020.findings-emnlp.67,0,0.149286,"domain involves reasoning not only about attributes of individual dots but also spatially and comparatively within a single referring expression (e.g., a line of three dots) or across referring expressions (e.g., a large grey dot left of a smaller dot). To predict a sequence of referents r = r1:K from the K referring expression representations z 1:K extracted above, we use a linear-chain CRF (Lafferty et al., 2001) with neural potentials to parameterize PR (r1:K |z 1:K , w, M ). This architecture generalizes the reference resolution and choice selection models of Udagawa and Aizawa (2020) and Udagawa et al. (2020) to model, in the output structure, relationships between dots, both inside and across referring expressions. There are three different types of potentials, designed to model language-conditioned features of individual dots d in a referent r, φ; relationships within a referent, ψ, and transitions between successive referents, ω. Given these potentials, the 6 The bidirectional encoder only has access to the utterances that have been produced so far, i.e., u1:t when the agent is generating utterance ut+1 . exp X ! f (rk , z k ) + ψ(rk , z k ) + ω(rk:k+1 , z k:k+1 ) , k P where f (r, z) = d∈r φ(d"
2021.findings-acl.334,D15-1075,0,0.116311,"Missing"
2021.findings-acl.334,2020.emnlp-main.21,0,0.0195624,"werful with more data. How easy is it to obtain more model predictions? In our paper, the main bottleneck is pretraining. However, once the pretrained models are released, individual researchers can download them and only need to repeat the cheaper finetuning procedure. Furthermore, model prediction data are undershared: while many recent research papers share their code or even model weights to help reproduce the results, it is not yet a standard practice to share all the model predictions. Since many researches follow almost the same recipe of pretraining and finetuning (McCoy et al., 2020; Desai and Durrett, 2020; Dodge et al., 2020), much computation can be saved if model predictions are shared. On the other hand, as the state of the art model size is increasing at a staggering speed7 , most researchers will not be able to run inference on a single instance. The trend that models are becoming larger and more similar necessitate more prediction sharing. Meta-Labels and Other Predictions Data mining is more powerful with more types of information. One way to add information to each instance is to assign “meta-labels”. In the HANS (McCoy et al., 2019) dataset, the authors tag each instance with a heuris"
2021.findings-acl.334,N18-1202,0,0.126329,"Missing"
2021.findings-acl.334,2020.acl-main.703,0,0.0440998,"Missing"
2021.findings-acl.334,W18-5446,0,0.0489612,"Missing"
2021.findings-acl.334,2020.acl-main.597,0,0.0574635,"Missing"
2021.findings-emnlp.244,2021.emnlp-main.468,0,0.0424842,"the model only learns shallow statistical correlations from meta-tuning rather than “more sophisticated reasoning skills"". For example, the word “exciting"" might occur in positive reviews more. This is unlikely, given that larger models are consistently better than smaller or randomly initialized ones. To explain this performance gap, larger models must have learned to use more complicated features during meta-tuning. Relation to Meta/Multitask-Learning Our method is closely related to, but different from meta-learning (Yin, 2020; Murty et al., 2021) and multi-task learning (Ye et al., 2021; Aghajanyan et al., 2021). Both meta-learning and multitask-learning typically involve at least a few examples from the target task; in our setup, however, the model does not learn from any target task examples. The “meta” in our name does not mean “meta-learning”, but reflects the fact that our model learns from a meta-dataset of tasks. Nevertheless, our framework can be easily adapted to a few-shot learning setup, which enables the language model to learn to learn from incontext examples (see below). Since this approach models the learning process as a sequence classification problem, it can be seen as a form of met"
2021.findings-emnlp.244,2020.emnlp-main.717,0,0.040921,"g our experiments in Section 5 to prevent selection bias. Is the sentence ungrammatical? Is this text expressing a need for evacuation? Is this text about Society and Culture? Is this a spam? Figure 3: Some example manually annotated label descriptions (questions). Three of the authors manually wrote 441 questions in total, and each of them is proofread by at least another author. is proofread by at least another author. See Figure 2 for a concrete example, and Figure 3 for some representative label descriptions. Additionally, some datasets contain thousands of labels (Chalkidis et al., 2019; Allaway and McKeown, 2020). In this case, we use templates to automatically synthesize label descriptions and exclude them from evaluation. Grouping similar datasets Our goal is to test the models’ ability to generalize to tasks that are different enough from the training tasks. Therefore, at test time, we need to exclude not only the same dataset that appeared in the meta-tuning phase, but also ones that are similar. This poses a challenge: whether two datasets perform the same task involves subjective opinion, Unifying the dataset format We convert each and there is no universally agreed definition. On classification"
2021.findings-emnlp.244,2020.findings-emnlp.148,0,0.0345018,"istance""; or alternatively, it can be longer but more accurate: “people need an allied health professional who supports the work of physicians and other health professionals"". How to scalably generate more accurate and detailed label descriptions without expert efforts will be another future direction. Optimizing Prompts Our work is complementary to recent works that optimize the prompts to achieve better accuracy. Even if our metatuned model is specialized in answering prompts, it might still react very differently towards different prompts. For example, in the stance classification dataset (Barbieri et al., 2020), we annotated two label descriptions (prompts) for the same label: “Does this post support atheism?"" and “Is the post against having religious beliefs?"". They have similar meanings, but the former has much lower accuracy than the later. We conjecture that this is because the model cannot ground abstract concepts like “atheism"". Other extensions We conjecture that metatuning can be extended to more diverse tasks beyond zero-shot binary classification. To extend to multi-label classification, we need to develop a procedure to resolve the labels when the model predicts positive for more than one"
2021.findings-emnlp.244,S19-2007,0,0.0463079,"Missing"
2021.naacl-main.190,2020.findings-emnlp.301,1,0.779134,"derson et al., 2018; Wallace et al., 2019). to harms, we argue that biased systems force To address this issue, recent work has turned marginalized users to code-switch or hide their towards detoxifying LMs: reducing toxic generidentity and that these systems can contribute to ations without affecting perplexity or generation social stigmas. For solutions, we discuss improved quality on nontoxic inputs. Existing detoxificaprocedures for data annotation and model training tion strategies involve techniques such as finetunthat may help debias detoxification techniques. ing LMs on nontoxic data (Gehman et al., 2020) or 2390 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2390–2397 June 6–11, 2021. ©2021 Association for Computational Linguistics 0 0 GPT-2 DAPT baseline DAPT PPLM PPLM GeDi GeDi 540 555 540 425 425 425 425 328 328 222 222 202 160202 160 70 70 WAE WAE Toxic Toxic 156 156 133 133 95 62 95 62 WAE WAE 193 193 AAE AAE Nontoxic Nontoxic 384 384 129 129 Methods and Experimental Setup The goal of detoxification is to mitigate the frequency of toxic generations (also called hate speech or offensive"
2021.naacl-main.190,2020.emnlp-main.473,0,0.173571,"when models are conditioned on WAE texts. Worse yet, generation quality is noticeably worse when conditioned on AAE texts, demonstrating unwanted biases. See Table 1 for qualitative examples. 3.1 Automatic Evaluation Using Perplexity We first perform intrinsic evaluations of each detoxification technique by computing the perplexity of detoxified models on various datasets. Note that we are not generating from the LM in this evaluation.3 White-Aligned English Perplexity We first evaluate the perplexity on White-Aligned English (WAE) text that is either toxic or nontoxic. We use WAE tweets from Groenwold et al. (2020).4 The detoxification techniques are effective at removing toxicity: the perplexity on toxic data increases substantially (Figure 1, toxic evaluation set). All techniques also cause a (smaller) increase in the perplexity on nontoxic WAE tweets, which shows that detoxification comes at some cost to the LM’s utility. Part of this increase likely results from distribution shift: the detoxification methods are trained on comments data, but our evaluation sets come from Twitter. Identity Mentions and AAE Perplexity We next evaluate the perplexity of the detoxified LMs on nontoxic language that may"
2021.naacl-main.190,2020.acl-main.740,1,0.88201,"Missing"
2021.naacl-main.190,S18-2005,0,0.0293062,"tems work effectively for them. Aside from being an annoyance, this is also a microaggression that poses psychological harms and may discourage AAE speakers from engaging with NLP systems whatsoever. Biases Are Not Limited to Detoxification Although we have focused on problems with detoxification in this paper, similar failures will occur whenever controllable generation methods are used. For example, a common goal is to control the sentiment of generated text (Dathathri et al., 2020; Krause et al., 2020). Unfortunately, since sentiment datasets are often biased against certain racial groups (Kiritchenko and Mohammad, 2018), controlling the sentiment of text will also affect which races are discussed. 6 Future Work: Towards Bias-Free Detoxification The harms that we have identified occur largely due to spurious correlations in toxicity datasets. A natural direction for future work is to thus improve datasets, for example, by changing the annotation procedure (Sap et al., 2019) or labeling scheme (Kennedy et al., 2020; Sap et al., 2020). Unfortunately, this can also make collecting annotations more expensive. As an alternative or in addition to higher quality data, there is growing interest in training accurate m"
2021.naacl-main.190,P19-1163,1,0.759833,"methods are used. For example, a common goal is to control the sentiment of generated text (Dathathri et al., 2020; Krause et al., 2020). Unfortunately, since sentiment datasets are often biased against certain racial groups (Kiritchenko and Mohammad, 2018), controlling the sentiment of text will also affect which races are discussed. 6 Future Work: Towards Bias-Free Detoxification The harms that we have identified occur largely due to spurious correlations in toxicity datasets. A natural direction for future work is to thus improve datasets, for example, by changing the annotation procedure (Sap et al., 2019) or labeling scheme (Kennedy et al., 2020; Sap et al., 2020). Unfortunately, this can also make collecting annotations more expensive. As an alternative or in addition to higher quality data, there is growing interest in training accurate models in the presence of biased data (Oren et al., 2019; Clark et al., 2019). Unfortunately, state-of-the-art debiasing methods are still far from perfect (Zhou et al., 2021). We plan to explore new methods for debiasing both datasets and models in future work. References Su Lin Blodgett, Solon Barocas, Hal Daum´e III, and Hanna Wallach. 2020. Language (tech"
2021.naacl-main.190,2020.acl-main.486,1,0.752566,"he sentiment of generated text (Dathathri et al., 2020; Krause et al., 2020). Unfortunately, since sentiment datasets are often biased against certain racial groups (Kiritchenko and Mohammad, 2018), controlling the sentiment of text will also affect which races are discussed. 6 Future Work: Towards Bias-Free Detoxification The harms that we have identified occur largely due to spurious correlations in toxicity datasets. A natural direction for future work is to thus improve datasets, for example, by changing the annotation procedure (Sap et al., 2019) or labeling scheme (Kennedy et al., 2020; Sap et al., 2020). Unfortunately, this can also make collecting annotations more expensive. As an alternative or in addition to higher quality data, there is growing interest in training accurate models in the presence of biased data (Oren et al., 2019; Clark et al., 2019). Unfortunately, state-of-the-art debiasing methods are still far from perfect (Zhou et al., 2021). We plan to explore new methods for debiasing both datasets and models in future work. References Su Lin Blodgett, Solon Barocas, Hal Daum´e III, and Hanna Wallach. 2020. Language (technology) is power: A critical survey of “bias” in NLP. In ACL"
2021.naacl-main.190,2020.findings-emnlp.291,0,0.0187544,"7 et al., 2019; Oliva et al., 2020). Also note that in As mentioned in Section 3.1, some of the quality issues all of the above cases, increasing the detoxificacan be attributed to domain shift. 2393 tion strength (e.g., longer finetuning for DAPT or higher ω for GeDi) exacerbates these problems. In our experiments, we test multiple detoxification methods to show that this bias is not linked to a specific technique, but instead to the process of detoxification in the presence of biased supervised data. In fact, other controllable generation techniques, including prompts (Wallace et al., 2019; Sheng et al., 2020; Shin et al., 2020) or conditional LMs (Keskar et al., 2019) will likely exhibit the same type of biases. 5 Harms of Detoxification Our results demonstrate that the current state of detoxification poses representational harms (Blodgett et al., 2020) to minority groups. We discuss the concrete impacts of these harms below. In-group Harms Detoxified LMs are deployed in downstream NLP systems in which they directly engage with end users. In addition to LMs not being able to generate minority identity mentions and minority dialects, our results suggest that detoxified LMs also struggle to underst"
2021.naacl-main.190,2020.emnlp-main.346,1,0.66064,"a et al., 2020). Also note that in As mentioned in Section 3.1, some of the quality issues all of the above cases, increasing the detoxificacan be attributed to domain shift. 2393 tion strength (e.g., longer finetuning for DAPT or higher ω for GeDi) exacerbates these problems. In our experiments, we test multiple detoxification methods to show that this bias is not linked to a specific technique, but instead to the process of detoxification in the presence of biased supervised data. In fact, other controllable generation techniques, including prompts (Wallace et al., 2019; Sheng et al., 2020; Shin et al., 2020) or conditional LMs (Keskar et al., 2019) will likely exhibit the same type of biases. 5 Harms of Detoxification Our results demonstrate that the current state of detoxification poses representational harms (Blodgett et al., 2020) to minority groups. We discuss the concrete impacts of these harms below. In-group Harms Detoxified LMs are deployed in downstream NLP systems in which they directly engage with end users. In addition to LMs not being able to generate minority identity mentions and minority dialects, our results suggest that detoxified LMs also struggle to understand these aspects of"
2021.naacl-main.190,D19-1221,1,0.924737,"because such corpora are too large to filter granu2019). These correlations cause detoxification larly (Roller et al., 2020), they inevitably contain techniques to steer generations away from AAE so-called toxic examples: undesirable language and minority identity mentions because they often such as expletives, slurs, or other offensive and consider these aspects of language to be toxic. threatening speech. When trained on such data, We conclude by outlining concrete harms and LMs inevitably learn to generate toxic text (Henpossible solutions to these biases. With regard derson et al., 2018; Wallace et al., 2019). to harms, we argue that biased systems force To address this issue, recent work has turned marginalized users to code-switch or hide their towards detoxifying LMs: reducing toxic generidentity and that these systems can contribute to ations without affecting perplexity or generation social stigmas. For solutions, we discuss improved quality on nontoxic inputs. Existing detoxificaprocedures for data annotation and model training tion strategies involve techniques such as finetunthat may help debias detoxification techniques. ing LMs on nontoxic data (Gehman et al., 2020) or 2390 Proceedings o"
2021.naacl-main.190,2021.eacl-main.274,1,0.799422,"ified occur largely due to spurious correlations in toxicity datasets. A natural direction for future work is to thus improve datasets, for example, by changing the annotation procedure (Sap et al., 2019) or labeling scheme (Kennedy et al., 2020; Sap et al., 2020). Unfortunately, this can also make collecting annotations more expensive. As an alternative or in addition to higher quality data, there is growing interest in training accurate models in the presence of biased data (Oren et al., 2019; Clark et al., 2019). Unfortunately, state-of-the-art debiasing methods are still far from perfect (Zhou et al., 2021). We plan to explore new methods for debiasing both datasets and models in future work. References Su Lin Blodgett, Solon Barocas, Hal Daum´e III, and Hanna Wallach. 2020. Language (technology) is power: A critical survey of “bias” in NLP. In ACL. Stigmatization of Language Detoxified models also have a propensity to avoid certain topics, e.g., Christopher Clark, Mark Yatskar, and Luke Zettlementioning a minority identity term. As a practimoyer. 2019. Don’t take the easy way out: Ensemcal example, the (detoxified) Microsoft Zo chatbot ble based methods for avoiding known dataset biases. In EMN"
2021.naacl-main.190,D19-1432,0,0.0248088,"ll also affect which races are discussed. 6 Future Work: Towards Bias-Free Detoxification The harms that we have identified occur largely due to spurious correlations in toxicity datasets. A natural direction for future work is to thus improve datasets, for example, by changing the annotation procedure (Sap et al., 2019) or labeling scheme (Kennedy et al., 2020; Sap et al., 2020). Unfortunately, this can also make collecting annotations more expensive. As an alternative or in addition to higher quality data, there is growing interest in training accurate models in the presence of biased data (Oren et al., 2019; Clark et al., 2019). Unfortunately, state-of-the-art debiasing methods are still far from perfect (Zhou et al., 2021). We plan to explore new methods for debiasing both datasets and models in future work. References Su Lin Blodgett, Solon Barocas, Hal Daum´e III, and Hanna Wallach. 2020. Language (technology) is power: A critical survey of “bias” in NLP. In ACL. Stigmatization of Language Detoxified models also have a propensity to avoid certain topics, e.g., Christopher Clark, Mark Yatskar, and Luke Zettlementioning a minority identity term. As a practimoyer. 2019. Don’t take the easy way o"
2021.naacl-main.276,D14-1162,0,0.0875381,"Shakespeare’s originals. 7 All models and baselines use GPT2 tokenization. tionally, use rate of words in W is a poor metric, because a model can score highly by e.g., simply returning the words in W, without generalizing to the full topic that W approximates. Instead, we adopt a notion of success which requires the model to generalize the bag W to the full topic. The remaining metrics are measures of quality and diversity. 1. Success, the average number of distinct words in a heldout bag W 0 which appear in the model output. Specifically, for each word in W, we add to W 0 the closest GloVe (Pennington et al., 2014) word by cosine similarity, such that the new word does not contain (and is not contained by) any word in W. (This excludes e.g., most plurals.) Usage of distinct words in W 0 measures the model’s ability to generalize W to other on-topic words, of which W 0 is a non-exhaustive set. This is our main metric. 2. Grammaticality, identical to the couplet task. 3. Perplexity, identical to the couplet task. 4. Distinctness, defined as in the couplet task. However, it is calculated separately within the 60 generations for each topic, and then averaged over the 7 topics. Additionally, following the ev"
2021.naacl-main.276,2013.iwslt-papers.14,0,0.0129185,"INETUNE and especially W DEC. In contrast, F UDGE does not touch the original P (X), largely avoiding F INETUNE’s distribution shift problem on this task. Finally, F UDGE outperforms the strong gradientbased P PLM method, despite requiring access only to G’s output logits. Non-reliance on gradients means F UDGE is also many times faster than P PLM, which takes a few hours compared to F UDGE’s 15 minutes for the full set of 420 generations on our hardware. Sometimes we do not even have gradients: for example, gradients are unavailable in the API for GPT3 at time of writing. Translation Corpus (Post et al., 2013), a collection of transcribed Spanish conversations with English translations. Both the source Spanish and target English are highly informal and disfluent. Salesky et al. (2019) augment the Fisher dataset with additional parallel English translations, rewritten to be more fluent (and hence more formal); see Table 7 for an example. Our task is to translate the original informal Spanish to into more formal English. However, we assume that Salesky et al. (2019)’s fluent references are unavailable during training. entonces de verdad sí sí pero entonces tu estudiando para es es digo es más porque"
2021.naacl-main.373,S16-1168,0,0.101774,"n to large taxonomic trees. Models trained on medium-sized taxonomies generalize poorly to large taxonomies. Future work can improve the usage of global tree structure with CTP. rate siblinghood information. Mao et al. (2018) propose a reinforcement learning based approach that combines the stages of hypernym detection and hypernym organization. In addition to the task of constructing medium-sized W ORD N ET subtrees, they show that their approach can leverage global structure to construct much larger taxonomies from the SemEval-2016 Task 13 benchmark dataset, which contain hundreds of terms (Bordea et al., 2016b). Shang et al. (2020) apply graph neural networks and show that they improve performance in constructing large taxonomies in the SemEval-2016 Task 13 dataset. Another relevant line of work involves extracting structured declarative knowledge from pretrained language models. For instance, Bouraoui et al. (2019) showed that a wide range of relations can be extracted from pretrained language models such as BERT. Our work differs in that we consider tree structures and incorporate web glosses. Bosselut et al. (2019) use pretrained models to generate explicit open-text descriptions of commonsense"
2021.naacl-main.373,P19-1470,0,0.021127,"s from the SemEval-2016 Task 13 benchmark dataset, which contain hundreds of terms (Bordea et al., 2016b). Shang et al. (2020) apply graph neural networks and show that they improve performance in constructing large taxonomies in the SemEval-2016 Task 13 dataset. Another relevant line of work involves extracting structured declarative knowledge from pretrained language models. For instance, Bouraoui et al. (2019) showed that a wide range of relations can be extracted from pretrained language models such as BERT. Our work differs in that we consider tree structures and incorporate web glosses. Bosselut et al. (2019) use pretrained models to generate explicit open-text descriptions of commonsense knowledge. Other work has focused on extracting knowledge of relations between entities (Petroni et al., 2019; Jiang et al., 2020). Blevins and Zettlemoyer (2020) use a similar approach to ours for word sense disambiguation, and encode glosses with pretrained models. language models can produce improved taxonomic trees. The gain from accessing web glosses shows that incorporating both implicit knowledge of input terms and explicit textual descriptions of knowledge is a promising way to extract relational knowledg"
2021.naacl-main.373,D14-1162,0,0.085457,"edictions (C) (Section 2.2). We then reconcile the edges of this graph into a taxonomic tree (E) (Section 2.3). Optionally, we provide the model ranked web-retrieved glosses (Section 2.4). We re-order the glosses based on relevance to the current subtree (Z). a similar shape to a plate or bowl, (2) (metonymically) A specific type of prepared food, and (3) (mining) A trough in which ore is measured. We reorder the glosses based on their relevance to the current subtree. We define relevance of a given context div to subtree T as the cosine similarity between the average of the GloVe embeddings (Pennington et al., 2014) of the words in div (with stopwords removed), to the average of the GloVe embeddings of all terms v1 , ..., vn in the subtree. This produces a reordered list of glosses (1) (n) dv , ..., dv . We then use the input sequence containing the (1) (n) reordered glosses “[CLS] vi dvi , ..., dvi . [SEP] (1) (n) vj dvj , ..., dvj ” to fine-tune the pretrained models on pairs of terms (vi , vj ). subtrees of height 3 (this corresponds to trees containing 4 nodes in the longest path from the root to any leaf) that contain between 10 and 50 terms. This dataset comprises 761 English trees, with 533/114/11"
2021.naacl-main.373,D19-1005,0,0.0204239,"dels of such corpora can be beneficial in 1 Introduction constructing taxonomies. We focus on the task proA variety of NLP tasks use taxonomic information, posed by Bansal et al. (2014), where the task is to including question answering (Miller, 1998) and organize a set of input terms into a taxonomic tree. information retrieval (Yang and Wu, 2012). Tax- We convert this dataset into nine other languages onomies are also used as a resource for building using synset alignments collected in O PEN M ULTI knowledge and systematicity into neural models LINGUAL W ORDNET and evaluate our approach in (Peters et al., 2019; Geiger et al., 2020; Talmor these languages. et al., 2020). NLP systems often retrieve taxonomic CTP first finetunes pretrained language modinformation from lexical databases such as W ORD - els to predict the likelihood of pairwise parentN ET (Miller, 1998), which consists of taxonomies child relations, producing a graph of parenthood that contain semantic relations across many do- scores. Then it reconciles these predictions with mains. While manually curated taxonomies pro- a maximum spanning tree algorithm, creating a vide useful information, they are incomplete and tree-structured taxon"
2021.naacl-main.373,D19-1250,0,0.0180915,"in constructing large taxonomies in the SemEval-2016 Task 13 dataset. Another relevant line of work involves extracting structured declarative knowledge from pretrained language models. For instance, Bouraoui et al. (2019) showed that a wide range of relations can be extracted from pretrained language models such as BERT. Our work differs in that we consider tree structures and incorporate web glosses. Bosselut et al. (2019) use pretrained models to generate explicit open-text descriptions of commonsense knowledge. Other work has focused on extracting knowledge of relations between entities (Petroni et al., 2019; Jiang et al., 2020). Blevins and Zettlemoyer (2020) use a similar approach to ours for word sense disambiguation, and encode glosses with pretrained models. language models can produce improved taxonomic trees. The gain from accessing web glosses shows that incorporating both implicit knowledge of input terms and explicit textual descriptions of knowledge is a promising way to extract relational knowledge from pretrained models. Error analyses suggest specific avenues of future work, such as improving predictions for subtrees corresponding to abstractions, or explicitly modeling the global s"
2021.naacl-main.373,2016.gwc-1.43,0,0.0673824,"Missing"
2021.naacl-main.373,W13-4302,0,0.0798647,"Missing"
2021.naacl-main.373,2020.emnlp-main.502,0,0.0298998,"Missing"
2021.naacl-main.373,P18-2057,0,0.0471289,"Missing"
2021.naacl-main.81,Q13-1005,0,0.0404136,"Place the cellphone down ... Figure 1: At evaluation time, an instruction following agent may need to generalize both to novel chains of subgoals encountered during training as well as to completely new environments. In the generalization condition above, the agent must generalize to multiple pickup actions (in green) at test time, whereas only single ones were seen at training, as well as to a new house. We propose a modular architecture to handle these cases. Work on grounded instruction following (MacMahon et al., 2006; Vogel and Jurafsky, 2010; Tellex et al., 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013) has recently been driven by sequence-to-sequence models (Mei et al., 2016; In this work, we improve compositional genHermann et al., 2017), which allow end-to-end eralization in instruction following with modugrounding of linguistically-rich instructions into equally-rich visual contexts (Misra et al., 2018; lar networks, which have been successful in nonembodied language grounding tasks (Andreas et al., Anderson et al., 2018; Chen et al., 2019). These 2016; Hu et al., 2017; Cirik et al., 2018; Yu et al., sequence-to-sequence models are monolithic: they consist of a single network structure w"
2021.naacl-main.81,P19-1655,1,0.800081,"These subgoal types are used to chain together modules (right) to carry out instructions in the environment. Each module is a separatelyparameterized sequence-to-sequence model that conditions on an attended representation of the instruction sequence, the visual observations, and the action taken at the previous timestep. Modules pass recurrent hidden states to each other. because of their composable structure (Devin et al., 2017; Andreas et al., 2017; Bahdanau et al., 2019; Purushwalkam et al., 2019), and that they can generalize to new environments or domains through module specialization (Hu et al., 2019; Blukis et al., 2020). However, all this work has either focused on grounding tasks without a temporal component or used a network structure which is not predicted from language. 2 Modular Instruction Following Networks We focus on following instructions in embodied tasks involving navigation and complex object interactions, as shown in Figure 2. In training, each set of full instructions (e.g. “Turn right and cross the room ... Place the vase on the coffee table to the left of the computer.”) is paired with a demonstration of image observations We propose a modular architecture for embodied"
2021.naacl-main.81,N16-1030,0,0.0362051,"hub.com/rcorona/modular_compositional_alfred. ters of each module to specialize for its subgoal. 1034 2.1 Instruction-Based Controller Our instruction-based controller is trained to segment a full instruction into sub-instructions and predict the subgoal type for each sub-instruction. We use a linear chain CRF (Lafferty et al., 2001) that conditions on a bidirectional-LSTM encoding of the full instruction and predicts tags for each word, which determine the segmentation and sequence of subgoal types. This model is based on standard neural segmentation and labelling models (Huang et al., 2015; Lample et al., 2016). We train the controller on the ground-truth instruction segmentations and subgoal sequence labels, and in evaluation use the model to predict segmentations and their associated subgoal sequences (Figure 2, top left). This predicted sequence of subgoals determines the order to execute the modules (Figure 2, right). We use a BIO chunking scheme to jointly segment the instruction and predict a subgoal label for each segment. Formally, for a full instruction of length N , the controller defines a distribution over subgoal tags s1:N for each word given the instruction x1:N as p(s1:N |x1:N ) ∝ exp"
2021.naacl-main.81,P10-1083,0,0.0525623,"... Grab the cellphone off of the chair there. Turn left and walk ... Place the cellphone down ... Figure 1: At evaluation time, an instruction following agent may need to generalize both to novel chains of subgoals encountered during training as well as to completely new environments. In the generalization condition above, the agent must generalize to multiple pickup actions (in green) at test time, whereas only single ones were seen at training, as well as to a new house. We propose a modular architecture to handle these cases. Work on grounded instruction following (MacMahon et al., 2006; Vogel and Jurafsky, 2010; Tellex et al., 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013) has recently been driven by sequence-to-sequence models (Mei et al., 2016; In this work, we improve compositional genHermann et al., 2017), which allow end-to-end eralization in instruction following with modugrounding of linguistically-rich instructions into equally-rich visual contexts (Misra et al., 2018; lar networks, which have been successful in nonembodied language grounding tasks (Andreas et al., Anderson et al., 2018; Chen et al., 2019). These 2016; Hu et al., 2017; Cirik et al., 2018; Yu et al., sequence-to-se"
2021.naacl-main.81,D18-1287,0,0.0382802,"Missing"
D07-1072,A00-2018,0,0.055178,"typically a poor model as it makes overly strong independence assumptions. As a result, many generative approaches to parsing construct refinements of the treebank grammar which are more suitable for the modeling task. Lexical methods split each pre-terminal symbol into many subsymbols, one for each word, and then focus on smoothing sparse lexical statis688 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 688–697, Prague, June 2007. 2007 Association for Computational Linguistics tics (Collins, 1999; Charniak, 2000). Unlexicalized methods refine the grammar in a more conservative fashion, splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006). We apply our HDP-PCFG-GR model to automatically learn the number of subsymbols for each symbol. 2 Models based on Dirichlet processes In document clustering, for example, each data point xi is a document represented by its termfrequency vector. Each component (cluster) z has multinomial parameters φz which specifies a distribution F (·; φz ) over words. It is"
D07-1072,P07-1035,0,0.551821,"Missing"
D07-1072,P06-1085,0,0.0253881,"treebank parsing (Charniak, 1996; Collins, 1999). An important question when learning PCFGs is how many grammar symbols to allocate to the learning algorithm based on the amount of available data. The question of “how many clusters (symbols)?” has been tackled in the Bayesian nonparametrics literature via Dirichlet process (DP) mixture models (Antoniak, 1974). DP mixture models have since been extended to hierarchical Dirichlet processes (HDPs) and HDP-HMMs (Teh et al., 2006; Beal et al., 2002) and applied to many different types of clustering/induction problems in NLP (Johnson et al., 2006; Goldwater et al., 2006). In this paper, we present the hierarchical Dirichlet process PCFG (HDP-PCFG). a nonparametric As models increase in complexity, so does the uncertainty over parameter estimates. In this regime, point estimates are unreliable since they do not take into account the fact that there are different amounts of uncertainty in the various components of the parameters. The HDP-PCFG is a Bayesian model which naturally handles this uncertainty. We present an efficient variational inference algorithm for the HDP-PCFG based on a structured mean-field approximation of the true posterior over parameters. T"
D07-1072,P03-1054,1,0.0589178,"ore suitable for the modeling task. Lexical methods split each pre-terminal symbol into many subsymbols, one for each word, and then focus on smoothing sparse lexical statis688 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 688–697, Prague, June 2007. 2007 Association for Computational Linguistics tics (Collins, 1999; Charniak, 2000). Unlexicalized methods refine the grammar in a more conservative fashion, splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006). We apply our HDP-PCFG-GR model to automatically learn the number of subsymbols for each symbol. 2 Models based on Dirichlet processes In document clustering, for example, each data point xi is a document represented by its termfrequency vector. Each component (cluster) z has multinomial parameters φz which specifies a distribution F (·; φz ) over words. It is customary to use a conjugate Dirichlet prior G0 = Dirichlet(α0 , . . . , α0 ) over the multinomial parameters, which can be interpreted as adding α0 − 1 pseudocounts for each word. 2.2 At th"
D07-1072,P05-1010,0,0.344389,"ling task. Lexical methods split each pre-terminal symbol into many subsymbols, one for each word, and then focus on smoothing sparse lexical statis688 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 688–697, Prague, June 2007. 2007 Association for Computational Linguistics tics (Collins, 1999; Charniak, 2000). Unlexicalized methods refine the grammar in a more conservative fashion, splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006). We apply our HDP-PCFG-GR model to automatically learn the number of subsymbols for each symbol. 2 Models based on Dirichlet processes In document clustering, for example, each data point xi is a document represented by its termfrequency vector. Each component (cluster) z has multinomial parameters φz which specifies a distribution F (·; φz ) over words. It is customary to use a conjugate Dirichlet prior G0 = Dirichlet(α0 , . . . , α0 ) over the multinomial parameters, which can be interpreted as adding α0 − 1 pseudocounts for each word. 2.2 At the heart of the HDP-PCFG"
D07-1072,P06-1055,1,0.592342,"ds split each pre-terminal symbol into many subsymbols, one for each word, and then focus on smoothing sparse lexical statis688 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 688–697, Prague, June 2007. 2007 Association for Computational Linguistics tics (Collins, 1999; Charniak, 2000). Unlexicalized methods refine the grammar in a more conservative fashion, splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006). We apply our HDP-PCFG-GR model to automatically learn the number of subsymbols for each symbol. 2 Models based on Dirichlet processes In document clustering, for example, each data point xi is a document represented by its termfrequency vector. Each component (cluster) z has multinomial parameters φz which specifies a distribution F (·; φz ) over words. It is customary to use a conjugate Dirichlet prior G0 = Dirichlet(α0 , . . . , α0 ) over the multinomial parameters, which can be interpreted as adding α0 − 1 pseudocounts for each word. 2.2 At the heart of the HDP-PCFG is the Dirichlet proce"
D07-1072,J03-4003,0,\N,Missing
D07-1093,X93-1024,0,0.298158,"are fully observed while the vl and ib forms are automatically reconstructed. Figure 6 also shows a specific example of the evolution of the Latin VERBUM (word/verb), along with the specific edits employed by the model. While quantitative evaluation such as measuring edit distance is helpful for comparing results, it is also illuminating to consider the plausibility of the learned parameters in a historical light, which we do here briefly. In particular, we consider rules on the branch between la and vl, for which we have historical evidence. For example, documents such as the Appendix Probi (Baehrens, 1922) provide indications of orthographic confusions which resulted from the growing gap between Classical Latin and Vulgar Latin phonology around the 3rd and 4th centuries AD. The Appendix lists common misspellings of Latin words, from which phonological changes can be inferred. On the la to vl branch, rules for word-final deletion of classical case markers dominate the list (rules ranks 1 and 3 for deletion of final /s/, ranks 2 and 4 for deletion of final /m/). It is indeed likely that these were generally eliminated in Vulgar Latin. For the deletion of the /m/, the Appendix Probi contains pairs"
D07-1093,J03-1002,0,0.00339804,"corpus we created for these experiments. 4.1 Corpus In order to train and evaluate our system, we compiled a corpus of Romance cognate words. The raw data was taken from three sources: the wiktionary.org website, a Bible parallel corpus (Resnik et al., 1999) and the Europarl corpus (Koehn, 2002). From an XML dump of the Wiktionary data, we extracted multilingual translations, which provide a list of word tuples in a large number of languages, including a few ancient languages. The Europarl and the biblical data were processed and aligned in the standard way, using combined GIZA++ alignments (Och and Ney, 2003). We performed our experiments with four languages from the Romance family (Latin, Italian, Spanish, and Portuguese). For each of these languages, we used a simple in-house rule-based system to convert the words into their IPA representations.2 After augmenting our alignments with the transitive closure3 of the Europarl, Bible and Wiktionary data, we filtered out non-cognate words by thresholding the ratio of edit distance to word length.4 The preprocessing is constraining in that we require that all the elements of a tuple to be cognates, which leaves out a significant portion of the data beh"
D07-1093,W97-1101,0,0.0355457,"sadvantages: it does not capture the fact that the cognate is closer between Spanish and Portuguese than between French and Spanish, nor do the resulting models let us conclude anything about the regular processes which caused these languages to diverge. Also, the existing cognate data has been curated at a relatively high cost. In our work, we track each word using an automatically obtained cognate list. While our cognates may be noisier, we compensate by modeling phonological changes rather than boolean mutations in cognate sets. There has been other computational work in this broad domain. Venkataraman et al. (1997) describe an information theoretic measure of the distance between two dialects of Chinese. Like our approach, they use a probabilistic edit model as a formalization of the phonological process. However, they do not consider the question of reconstruction or inference in multi-node phylogenies, nor do they present a learning algorithm for such models. Finally, for the specific application of cognate prediction in machine translation, essentially transliteration, there have been several approaches, including Kondrak (2002). However, the phenomena of interest, and therefore the models, are extre"
D07-1094,P05-1010,0,0.132895,"Missing"
D07-1094,P06-1055,1,0.930124,"er (Young and Woodland, 1994). Finally, to model complex emission densities, states emit mixtures of multivariate Gaussians. This standard structure is shown schematically in Figure 1. While this rich structure is phonetically well-motivated and empirically successful, so much structural bias may be unnecessary, or even harmful. For example in the domain of syntactic parsing with probabilistic context-free grammars (PCFGs), a surprising recent result is that automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure (Petrov et al., 2006). In this paper, we consider a much more automatic, data-driven approach to learning HMM structure for acoustic modeling, analagous to the approach taken by Petrov et al. (2006) for learning PCFGs. We start with a minimal monophone HMM in which there is a single state for each (context-independent) phone. Moreover, the emission model for each state is a single multivariate Gaussian (over the standard MFCC acoustic features). We then iteratively refine this minimal HMM through state splitting, adding complexity as needed. States in the refined HMMs are always substates of the original HMM and a"
D08-1012,J92-4003,0,0.0340262,"gham and Mannila, 2001). Although our best performance does not come from random projections, we still obtain substantial speed-ups over a single pass fine decoder when using random projections in coarse passes. 4.2 Frequency clustering In frequency clustering, we allocate words to clusters by frequency. At each level, the most frequent words go into one cluster and the rarest words go into another one. Concretely, we sort the words in a given cluster by frequency and split the cluster so that the two halves have equal token mass. This approach can be seen as a radically simplified version of Brown et al. (1992). It can, and does, result in highly imbalanced cluster hierarchies. 4.3 HMM clustering An approach found to be effective by Petrov and Klein (2007) for coarse-to-fine parsing is to use likelihood-based hierarchical EM training. We adopt this approach here by identifying each cluster with a latent state in an HMM and determinizing the emissions so that each word type is emitted 90000 70000 60000 29.2 29 BLEU Perplexity 29.4 HMM JCluster Frequency Random 80000 50000 40000 30000 28.2 10000 0 0 28 100 1 2 3 4 5 6 7 8 9 10 Number of bits in coarse language model Figure 5: Results of coarse languag"
D08-1012,N06-1022,0,0.0389392,"s, Zhang and Gildea (2008) consider an approach in which the results of a bigram pass are used as an A* heuristic to guide a trigram pass. In their two-pass approach, the coarse bigram pass becomes computationally dominant. Our work differs in two ways. First, we use posterior pruning rather than A* search. Unlike A* search, posterior pruning allows multipass methods. Not only are posterior pruning methods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. 109 0,1-NP-0,1 LM Order To follow related work and to focus on the effects of the language model, we present translation results under an inversion transduction grammar (ITG) translation model (Wu, 1997) trained on the Europarl corpus (Koehn, 2005), described in detail in Section 3, and using a trigram language model. We show that, on a range of languages, our coarse-tofine decoding approach greatly outperfor"
D08-1012,P05-1033,0,0.0934293,"ation via a backpointer. See Wu (1996) or Melamed (2004) for a detailed exposition. Once we integrate an n-gram language model, the state space becomes lexicalized and combining dynamic programming items becomes more difficult. Each state is now parametrized by the initial and final n−1 words in the target language hypothesis: ln−1 , ..., l1 -i Xj -r1 , ..., rn−1 . Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). Decoding with an integrated language model is computationally expensive for two reasons: (1) the need to keep track of a large number of lexicalized hypotheses for each source span, and (2) the need to frequently query the large language model for each hypothesis combination. Multipass coarse-to-fine decoding can alleviate both computational issues. We start by decoding in an extremely coarse bigram search space, where there are very few possible translations. We compute standard inside/outside probabilities (iS/oS), as follows. Consider the application of non-inverted binary rule: we combin"
D08-1012,W97-0302,0,0.043808,"mework like ours, Zhang and Gildea (2008) consider an approach in which the results of a bigram pass are used as an A* heuristic to guide a trigram pass. In their two-pass approach, the coarse bigram pass becomes computationally dominant. Our work differs in two ways. First, we use posterior pruning rather than A* search. Unlike A* search, posterior pruning allows multipass methods. Not only are posterior pruning methods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. 109 0,1-NP-0,1 LM Order To follow related work and to focus on the effects of the language model, we present translation results under an inversion transduction grammar (ITG) translation model (Wu, 1997) trained on the Europarl corpus (Koehn, 2005), described in detail in Section 3, and using a trigram language model. We show that, on a range of languages, our coarse-tofine decoding app"
D08-1012,N07-1052,1,0.832804,"n their two-pass approach, the coarse bigram pass becomes computationally dominant. Our work differs in two ways. First, we use posterior pruning rather than A* search. Unlike A* search, posterior pruning allows multipass methods. Not only are posterior pruning methods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. 109 0,1-NP-0,1 LM Order To follow related work and to focus on the effects of the language model, we present translation results under an inversion transduction grammar (ITG) translation model (Wu, 1997) trained on the Europarl corpus (Koehn, 2005), described in detail in Section 3, and using a trigram language model. We show that, on a range of languages, our coarse-tofine decoding approach greatly outperforms baseline beam pruning and bigram-to-trigram pruning on time-to-BLEU plots, reducing decoding times by up to a factor of 50 compared to sin"
D08-1012,P07-1019,0,0.0685306,"thogonal axis of abstraction: the size of the target language. The introduction of abstract languages gives better control over the granularity of the search space and provides a richer set of intermediate problems, allowing us to adapt the level of refinement of the intermediate, coarse passes to minimize total computation. Beyond coarse-to-fine approaches, other related approaches have also been demonstrated for syntactic MT. For example, Venugopal et al. (2007) considers a greedy first pass with a full model followed by a second pass which bounds search to a region near the greedy results. Huang and Chiang (2007) searches with the full model, but makes assumptions about the the amount of reordering the language model can trigger in order to limit exploration. 2.2 Language Model Projections When decoding in a syntactic translation model with an n-gram language model, search states are specified by a grammar nonterminal X as well as the the n-1 left-most target side words ln−1 , . . . , l1 and right-most target side words r1 , . . . , rn−1 of the generated hypothesis. We denote the resulting lexicalized state as ln−1 , . . . , l1 -X-r1 , . . . , rn−1 . Assuming a vocabulary V and grammar symbol set G, t"
D08-1012,N03-1016,1,0.831454,"o guide a trigram pass. In their two-pass approach, the coarse bigram pass becomes computationally dominant. Our work differs in two ways. First, we use posterior pruning rather than A* search. Unlike A* search, posterior pruning allows multipass methods. Not only are posterior pruning methods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. 109 0,1-NP-0,1 LM Order To follow related work and to focus on the effects of the language model, we present translation results under an inversion transduction grammar (ITG) translation model (Wu, 1997) trained on the Europarl corpus (Koehn, 2005), described in detail in Section 3, and using a trigram language model. We show that, on a range of languages, our coarse-tofine decoding approach greatly outperforms baseline beam pruning and bigram-to-trigram pruning on time-to-BLEU plots, reducing decoding times by up to a fact"
D08-1012,P07-2045,0,0.0103353,"Missing"
D08-1012,2005.mtsummit-papers.11,0,0.0299132,"eed to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. 109 0,1-NP-0,1 LM Order To follow related work and to focus on the effects of the language model, we present translation results under an inversion transduction grammar (ITG) translation model (Wu, 1997) trained on the Europarl corpus (Koehn, 2005), described in detail in Section 3, and using a trigram language model. We show that, on a range of languages, our coarse-tofine decoding approach greatly outperforms baseline beam pruning and bigram-to-trigram pruning on time-to-BLEU plots, reducing decoding times by up to a factor of 50 compared to single pass decoding. In addition, coarse-to-fine decoding increases BLEU scores by up to 0.4 points. This increase is a mixture of improved search and subtly advantageous coarseto-fine effects which are further discussed below. 01,10-NP-00,10 010,100-NP-000,100 the,report-NP-these,states 3 π ..."
D08-1012,P04-1083,0,0.0161719,"ally, binary inverted productions invert the order of their children (X → hY Zi). These productions are associated with rewrite weights in the 111 standard way. Without a language model, SCFG decoding is just like (monolingual) CFG parsing. The dynamic programming states are specified by i Xj , where hi, ji is a source sentence span and X is a nonterminal. The only difference is that whenever we apply a CFG production on the source side, we need to remember the corresponding synchronous production on the target side and store the best obtainable translation via a backpointer. See Wu (1996) or Melamed (2004) for a detailed exposition. Once we integrate an n-gram language model, the state space becomes lexicalized and combining dynamic programming items becomes more difficult. Each state is now parametrized by the initial and final n−1 words in the target language hypothesis: ln−1 , ..., l1 -i Xj -r1 , ..., rn−1 . Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). Decoding with an integrated language mode"
D08-1012,P03-1021,0,0.00535411,"be attributed primarily to the substantially richer distortion model used by Moses. The multipass coarse-to-fine architecture that we have introduced presents many choice points. In the following, we investigate various axes individually. We present our findings as BLEU-to-time plots, where the tradeoffs were generated by varying the complexity and the number of coarse passes, as well as the pruning thresholds and beam sizes. Unless otherwise noted, the experiments are on SpanishEnglish using trigram language models. When different decoder settings are applied to the same model, MERT weights (Och, 2003) from the unprojected single pass setup are used and are kept constant across runs. In particular, the same MERT weights are used for all coarse passes; note that this slightly disadvantages the multipass runs, which use MERT weights optimized for the single pass decoder. models perform at pruning, we ran our decoder several times, varying only the clustering source. In each case, we used a 2-bit trigram model as a single coarse pass, followed by a fine output pass. Figure 6 shows that we can obtain significant improvements over the single-pass baseline regardless of the clustering. To no grea"
D08-1012,N07-1051,1,0.772129,"08) consider an approach in which the results of a bigram pass are used as an A* heuristic to guide a trigram pass. In their two-pass approach, the coarse bigram pass becomes computationally dominant. Our work differs in two ways. First, we use posterior pruning rather than A* search. Unlike A* search, posterior pruning allows multipass methods. Not only are posterior pruning methods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. 109 0,1-NP-0,1 LM Order To follow related work and to focus on the effects of the language model, we present translation results under an inversion transduction grammar (ITG) translation model (Wu, 1997) trained on the Europarl corpus (Koehn, 2005), described in detail in Section 3, and using a trigram language model. We show that, on a range of languages, our coarse-tofine decoding approach greatly outperforms baseline beam pruning"
D08-1012,P06-1055,1,0.434994,"pproach most relevant to the current work is Zhang and Gildea (2008), which begins with an initial bigram pass and uses the resulting chart to guide Central to coarse-to-fine language projection is the construction of sequences of word clusterings (see Figure 1). The clusterings are deterministic mappings from words to clusters, with the property that each clustering refines the previous one. There are many choice points in this process, including how these clusterings are obtained and how much refinement is optimal for each pass. We demonstrate that likelihood-based hierarchical EM training (Petrov et al., 2006) and cluster-based language modeling methods (Goodman, 2001) are superior to both rank-based and random-projection methods. In addition, we demonstrate that more than two passes are beneficial and show that our computation is equally distributed over all passes. In our experiments, passes with less than 16-cluster language models are most advantageous, and even a single pass with just two word clusters can reduce decoding time greatly. 108 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 108–116, c Honolulu, October 2008. 2008 Association for Comput"
D08-1012,N07-1063,0,0.0344929,"ts used to encode the target language vocabulary is varied along the x-axis. The language model order is varied along the y-axis. Second, we focus on an orthogonal axis of abstraction: the size of the target language. The introduction of abstract languages gives better control over the granularity of the search space and provides a richer set of intermediate problems, allowing us to adapt the level of refinement of the intermediate, coarse passes to minimize total computation. Beyond coarse-to-fine approaches, other related approaches have also been demonstrated for syntactic MT. For example, Venugopal et al. (2007) considers a greedy first pass with a full model followed by a second pass which bounds search to a region near the greedy results. Huang and Chiang (2007) searches with the full model, but makes assumptions about the the amount of reordering the language model can trigger in order to limit exploration. 2.2 Language Model Projections When decoding in a syntactic translation model with an n-gram language model, search states are specified by a grammar nonterminal X as well as the the n-1 left-most target side words ln−1 , . . . , l1 and right-most target side words r1 , . . . , rn−1 of the gene"
D08-1012,P96-1021,0,0.0677399,"→ [Y Z]). Finally, binary inverted productions invert the order of their children (X → hY Zi). These productions are associated with rewrite weights in the 111 standard way. Without a language model, SCFG decoding is just like (monolingual) CFG parsing. The dynamic programming states are specified by i Xj , where hi, ji is a source sentence span and X is a nonterminal. The only difference is that whenever we apply a CFG production on the source side, we need to remember the corresponding synchronous production on the target side and store the best obtainable translation via a backpointer. See Wu (1996) or Melamed (2004) for a detailed exposition. Once we integrate an n-gram language model, the state space becomes lexicalized and combining dynamic programming items becomes more difficult. Each state is now parametrized by the initial and final n−1 words in the target language hypothesis: ln−1 , ..., l1 -i Xj -r1 , ..., rn−1 . Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). Decoding with an integr"
D08-1012,J97-3002,0,0.21094,"ethods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. 109 0,1-NP-0,1 LM Order To follow related work and to focus on the effects of the language model, we present translation results under an inversion transduction grammar (ITG) translation model (Wu, 1997) trained on the Europarl corpus (Koehn, 2005), described in detail in Section 3, and using a trigram language model. We show that, on a range of languages, our coarse-tofine decoding approach greatly outperforms baseline beam pruning and bigram-to-trigram pruning on time-to-BLEU plots, reducing decoding times by up to a factor of 50 compared to single pass decoding. In addition, coarse-to-fine decoding increases BLEU scores by up to 0.4 points. This increase is a mixture of improved search and subtly advantageous coarseto-fine effects which are further discussed below. 01,10-NP-00,10 010,100-N"
D08-1012,P03-1019,0,0.0142955,"itional memory requirement, but it is negligible. As we will see in our experiments (Section 5) the largest gains can be obtained with extremely coarse language models. In particular, the largest coarse model we use in our best multipass decoder uses a 4-bit encoding and hence has only 16 distinct words (or at most 4096 trigrams). 3 Inversion Transduction Grammars While our approach applies in principle to a variety of machine translation systems (phrase-based or syntactic), we will use the inversion transduction grammar (ITG) approach of Wu (1997) to facilitate comparison with previous work (Zens and Ney, 2003; Zhang and Gildea, 2008) as well as to focus on language model complexity. ITGs are a subclass of synchronous context-free grammars (SCFGs) where there are only three kinds of rules. Preterminal unary productions produce terminal strings on both sides (words or phrases): X → e/f . Binary in-order productions combine two phrases monotonically (X → [Y Z]). Finally, binary inverted productions invert the order of their children (X → hY Zi). These productions are associated with rewrite weights in the 111 standard way. Without a language model, SCFG decoding is just like (monolingual) CFG parsing"
D08-1012,P08-1025,0,0.365941,"a synchronous CFG translation model is very efficient, requiring only a variant of the CKY algorithm. As in monolingual parsing, dynamic programming items are simply indexed by a source language span and a syntactic label. Complexity arises when n-gram language model scoring is added, because items must now be distinguished by their initial and final few target language words for purposes of later combination. This lexically exploded search space is a root cause of inefficiency in decoding, and several methods have been suggested to combat it. The approach most relevant to the current work is Zhang and Gildea (2008), which begins with an initial bigram pass and uses the resulting chart to guide Central to coarse-to-fine language projection is the construction of sequences of word clusterings (see Figure 1). The clusterings are deterministic mappings from words to clusters, with the property that each clustering refines the previous one. There are many choice points in this process, including how these clusterings are obtained and how much refinement is optimal for each pass. We demonstrate that likelihood-based hierarchical EM training (Petrov et al., 2006) and cluster-based language modeling methods (Go"
D08-1033,W06-3123,0,0.481906,"Alternatively, phrase pair frequencies can be learned via a probabilistic model of phrase alignment, but this approach has presented several practical challenges. In this paper, we address the two most significant challenges in phrase alignment modeling. The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008). Previous phrase alignment work has sacrificed consistency for efficiency, employing greedy hill-climbing algorithms and constraining inference with word alignments (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). We describe a Gibbs sampler that consistently and efficiently approximates expectations, using only polynomial-time computable operators. Despite the combinatorial complexity of the phrase alignment space, our sampled phrase pair expectations are guaranteed to converge to the true posterior distributions under the model (in theory) and do converge to effective values (in practice). The second challenge in learning phrase alignments is avoiding a degenerate behavior of the general model class: as with many models which can choose between large and small structures, the larger structures win o"
D08-1033,W07-0403,0,0.376506,"hat is, we could not run EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under P (z|x, θ) have been approximated before in order to run EM. Marcu and Wong (2002) employed local search from a heuristic initialization and collected alignment counts during a hill climb through the alignment space. DeNero et al. (2006) instead proposed an exponential-time dynamic program pruned using word alignments. Subsequent work has relied heavily on word alignments to constrain inference, even under reordering models that admit polynomial-time E-steps (Cherry and Lin, 2007; Zhang et al., 2008). None of these approximations are consistent, and they offer no method of measuring their biases. Gibbs sampling is not only consistent in the limit, but also allows us to add Bayesian priors conveniently (section 4). Of course, sampling has liabilities as well: we do not know in advance how long we need to run the sampler to approximate the desired expectations “closely enough.” Snyder and Barzilay (2008) describe a Gibbs sampler for a bilingual morphology model very similar in structure to ours. However, the basic sampling step they propose – resampling all segmentation"
D08-1033,P08-2007,1,0.27816,"r in training bitexts. Since bitexts do not come segmented and aligned into phrase pairs, these counts are typically gathered by fixing a word alignment and applying phrase extraction heuristics to this word-aligned training corpus. Alternatively, phrase pair frequencies can be learned via a probabilistic model of phrase alignment, but this approach has presented several practical challenges. In this paper, we address the two most significant challenges in phrase alignment modeling. The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008). Previous phrase alignment work has sacrificed consistency for efficiency, employing greedy hill-climbing algorithms and constraining inference with word alignments (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). We describe a Gibbs sampler that consistently and efficiently approximates expectations, using only polynomial-time computable operators. Despite the combinatorial complexity of the phrase alignment space, our sampled phrase pair expectations are guaranteed to converge to the true posterior distributions under the model (in theory) and do converge to effective values"
D08-1033,W06-3105,1,0.836239,"ned training corpus. Alternatively, phrase pair frequencies can be learned via a probabilistic model of phrase alignment, but this approach has presented several practical challenges. In this paper, we address the two most significant challenges in phrase alignment modeling. The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008). Previous phrase alignment work has sacrificed consistency for efficiency, employing greedy hill-climbing algorithms and constraining inference with word alignments (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). We describe a Gibbs sampler that consistently and efficiently approximates expectations, using only polynomial-time computable operators. Despite the combinatorial complexity of the phrase alignment space, our sampled phrase pair expectations are guaranteed to converge to the true posterior distributions under the model (in theory) and do converge to effective values (in practice). The second challenge in learning phrase alignments is avoiding a degenerate behavior of the general model class: as with many models which can choose between large and small structures, the la"
D08-1033,P07-1035,0,0.03404,"Missing"
D08-1033,P06-1085,0,0.581666,"from the last by some small local change. The samples zi are guaranteed (in the limit) to consistently approximate the conditional distribution P (z|x, θ) (or P (z|x) later). Therefore, the average counts of phrase pairs in the samples converge to expected counts under the model. Normalizing these expected counts yields estimates for the features φ(e|f ) and φ(f |e). Gibbs sampling is not new to the natural language processing community (Teh, 2006; Johnson et al., 2007). However, it is usually used as a search procedure akin to simulated annealing, rather than for approximating expectations (Goldwater et al., 2006; Finkel et al., 2007). Our application is also atypical 316 for an NLP application in that we use an approximate sampler not only to include Bayesian prior information (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). That is, we could not run EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under P (z|x, θ) have been approximated before in order to run EM. Marcu and Wong (2002) employed local search from a heuristic initialization and collected alignment counts d"
D08-1033,P07-1107,1,0.305536,"now has the general form P (x, z, θJ ); all other model parameters have been fixed. Instead of searching for a suitable θJ ,9 we sample from the posterior distribution P (z|x) with θJ marginalized out. To this end, we convert our Gibbs sampler into a collapsed Gibbs sampler10 using the Chinese Restaurant Process (CRP) representation of the DP (Aldous, 1985). With the CRP, we avoid the problem of explicitely representing samples from the DP. CRP-based samplers have served the community well in related language tasks, such as word segmentation and coreference resolution (Goldwater et al., 2006; Haghighi and Klein, 2007). Under this representation, the probability of each sampling outcome is a simple expression in terms of the state of the rest of the training corpus (the Markov blanket), rather than explicitly using θJ . Let zm be the set of aligned phrase pair tokens observed in the rest of the corpus. Then, when he, f i is aligned (that is, neither e nor f are null), the conditional probability for a pair he, f i takes the form: τ (he, f i|zm ) = counthe,f i (zm ) + α · M0 (he, f i) , |zm |+ α where counthe,f i (zm ) is the number of times that he, f i appears in zm . We can write this expression thanks to"
D08-1033,N07-1018,0,0.0201427,", state z0 , which sets all latent variables to some initial configuration. We then produce a sequence of sample states zi , each of which differs from the last by some small local change. The samples zi are guaranteed (in the limit) to consistently approximate the conditional distribution P (z|x, θ) (or P (z|x) later). Therefore, the average counts of phrase pairs in the samples converge to expected counts under the model. Normalizing these expected counts yields estimates for the features φ(e|f ) and φ(f |e). Gibbs sampling is not new to the natural language processing community (Teh, 2006; Johnson et al., 2007). However, it is usually used as a search procedure akin to simulated annealing, rather than for approximating expectations (Goldwater et al., 2006; Finkel et al., 2007). Our application is also atypical 316 for an NLP application in that we use an approximate sampler not only to include Bayesian prior information (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). That is, we could not run EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under P (z|x, θ) have been a"
D08-1033,P07-2045,0,0.0111405,"tally share the same lexical content. Details of these fringe conditions have been omitted for space, but were included in our implementation. 12 The largest phrase pair found was 13 English words by 7 Spanish words. 321 Translation Results We evaluate our new estimates using the baseline translation pipeline from the 2007 Statistical Machine Translation Workshop shared task. 5.1 4.5 ∼ DP (Pe , α0 ) . Baseline System We trained Moses on all Spanish-English Europarl sentences up to length 20 (177k sentences) using GIZA++ Model 4 word alignments and the growdiag-final-and combination heuristic (Koehn et al., 2007; Och and Ney, 2003; Koehn, 2002), which performed better than any alternative combination heuristic.13 The baseline estimates (Heuristic) come from extracting phrases up to length 7 from the word alignment. We used a bidirectional lexicalized distortion model that conditions on both foreign and English phrases, along with their orientations. Our 5-gram language model was trained on 38.3 million words of Europarl using Kneser-Ney smoothing. We report results with and without lexical weighting, denoted lex. We tuned and tested on development corpora for the 2006 translation workshop. The parame"
D08-1033,W07-0734,0,0.0139473,"DP HDP DP-composed HDP-composed DP-smooth HDP-smooth Heuristic + lex DP-smooth + lex HDP-smooth + lex Phrase Pair Count 4.4M 0.6M 0.3M 3.7M 3.1M 4.8M 4.6M 4.4M 4.8M 4.6M NIST BLEU 29.8 28.8 29.1 30.1 30.1 30.1 30.2 30.5 30.4 30.7 Exact Match 5.3 METEOR 52.4 51.7 52.0 52.7 52.6 52.5 52.7 52.9 53.0 53.2 Table 1: BLEU results for learned distributions improve over a heuristic baseline. Estimate labels are described fully in section 5.3. The label lex indicates the addition of a lexical weighting feature. scored with lowercased, tokenized NIST BLEU, and exact match METEOR (Papineni et al., 2002; Lavie and Agarwal, 2007). The baseline system gives a BLEU score of 29.8, which increases to 30.5 with lex, as shown in Table 1. For reference, training on all sentences of length less than 40 (the shared task baseline default) gives 32.4 BLEU with lex. 5.2 Learned Distribution Performance We initialized the sampler with a configuration derived from the word alignments generated by the baseline. We greedily constructed a phrase alignment from the word alignment by identifying minimal phrase pairs consistent with the word alignment in each region of the sentence. We then ran the sampler for 100 iterations through the"
D08-1033,W02-1018,0,0.725912,"tics to this word-aligned training corpus. Alternatively, phrase pair frequencies can be learned via a probabilistic model of phrase alignment, but this approach has presented several practical challenges. In this paper, we address the two most significant challenges in phrase alignment modeling. The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008). Previous phrase alignment work has sacrificed consistency for efficiency, employing greedy hill-climbing algorithms and constraining inference with word alignments (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). We describe a Gibbs sampler that consistently and efficiently approximates expectations, using only polynomial-time computable operators. Despite the combinatorial complexity of the phrase alignment space, our sampled phrase pair expectations are guaranteed to converge to the true posterior distributions under the model (in theory) and do converge to effective values (in practice). The second challenge in learning phrase alignments is avoiding a degenerate behavior of the general model class: as with many models which can choose between large and sma"
D08-1033,D07-1038,0,0.125653,"positional constraint (Cherry and Lin, 2007; Zhang et al., 2008). However, the problem lies with the model, and therefore should be corrected in the model, rather than the inference procedure. Model-based solutions appear in the literature as well, though typically combined with word alignment constraints on inference. A sparse Dirichlet prior coupled with variational EM was explored by Zhang et al. (2008), but it did not avoid the degenerate solution. Moore and Quirk (2007) proposed a new conditional model structure that does not cause large and small phrases to compete for probability mass. May and Knight (2007) added additional model terms to balance the cost of long and short derivations in a syntactic alignment model. 6 For experiments, we ran the sampler for 100 iterations. 319 4.2 A Dirichlet Process Prior We control this degenerate behavior by placing a Dirichlet process (DP) prior over θJ , the distribution over aligned phrase pairs (Ferguson, 1973). If we were to assume a maximum number K of phrase pair types, a (finite) Dirichlet distribution would be an appropriate prior. A draw from a Kdimensional Dirichlet distribution is a list of K real numbers in [0, 1] that sum to one, which can be in"
D08-1033,W07-0715,0,0.0825004,"e procedure, for example with word alignments and linguistic features (Birch et al., 2006), or by disallowing large phrase pairs using a noncompositional constraint (Cherry and Lin, 2007; Zhang et al., 2008). However, the problem lies with the model, and therefore should be corrected in the model, rather than the inference procedure. Model-based solutions appear in the literature as well, though typically combined with word alignment constraints on inference. A sparse Dirichlet prior coupled with variational EM was explored by Zhang et al. (2008), but it did not avoid the degenerate solution. Moore and Quirk (2007) proposed a new conditional model structure that does not cause large and small phrases to compete for probability mass. May and Knight (2007) added additional model terms to balance the cost of long and short derivations in a syntactic alignment model. 6 For experiments, we ran the sampler for 100 iterations. 319 4.2 A Dirichlet Process Prior We control this degenerate behavior by placing a Dirichlet process (DP) prior over θJ , the distribution over aligned phrase pairs (Ferguson, 1973). If we were to assume a maximum number K of phrase pair types, a (finite) Dirichlet distribution would be"
D08-1033,J03-1002,0,0.0360836,"lexical content. Details of these fringe conditions have been omitted for space, but were included in our implementation. 12 The largest phrase pair found was 13 English words by 7 Spanish words. 321 Translation Results We evaluate our new estimates using the baseline translation pipeline from the 2007 Statistical Machine Translation Workshop shared task. 5.1 4.5 ∼ DP (Pe , α0 ) . Baseline System We trained Moses on all Spanish-English Europarl sentences up to length 20 (177k sentences) using GIZA++ Model 4 word alignments and the growdiag-final-and combination heuristic (Koehn et al., 2007; Och and Ney, 2003; Koehn, 2002), which performed better than any alternative combination heuristic.13 The baseline estimates (Heuristic) come from extracting phrases up to length 7 from the word alignment. We used a bidirectional lexicalized distortion model that conditions on both foreign and English phrases, along with their orientations. Our 5-gram language model was trained on 38.3 million words of Europarl using Kneser-Ney smoothing. We report results with and without lexical weighting, denoted lex. We tuned and tested on development corpora for the 2006 translation workshop. The parameters for each phras"
D08-1033,P03-1021,0,0.0251313,"nation heuristic.13 The baseline estimates (Heuristic) come from extracting phrases up to length 7 from the word alignment. We used a bidirectional lexicalized distortion model that conditions on both foreign and English phrases, along with their orientations. Our 5-gram language model was trained on 38.3 million words of Europarl using Kneser-Ney smoothing. We report results with and without lexical weighting, denoted lex. We tuned and tested on development corpora for the 2006 translation workshop. The parameters for each phrase table were tuned separately using minimum error rate training (Och, 2003). Results are 13 Sampling iteration time scales quadratically with sentence length. Short sentences were chosen to speed up our experiment cycle. Estimate Heuristic DP HDP DP-composed HDP-composed DP-smooth HDP-smooth Heuristic + lex DP-smooth + lex HDP-smooth + lex Phrase Pair Count 4.4M 0.6M 0.3M 3.7M 3.1M 4.8M 4.6M 4.4M 4.8M 4.6M NIST BLEU 29.8 28.8 29.1 30.1 30.1 30.1 30.2 30.5 30.4 30.7 Exact Match 5.3 METEOR 52.4 51.7 52.0 52.7 52.6 52.5 52.7 52.9 53.0 53.2 Table 1: BLEU results for learned distributions improve over a heuristic baseline. Estimate labels are described fully in section 5."
D08-1033,P02-1040,0,0.103231,"cle. Estimate Heuristic DP HDP DP-composed HDP-composed DP-smooth HDP-smooth Heuristic + lex DP-smooth + lex HDP-smooth + lex Phrase Pair Count 4.4M 0.6M 0.3M 3.7M 3.1M 4.8M 4.6M 4.4M 4.8M 4.6M NIST BLEU 29.8 28.8 29.1 30.1 30.1 30.1 30.2 30.5 30.4 30.7 Exact Match 5.3 METEOR 52.4 51.7 52.0 52.7 52.6 52.5 52.7 52.9 53.0 53.2 Table 1: BLEU results for learned distributions improve over a heuristic baseline. Estimate labels are described fully in section 5.3. The label lex indicates the addition of a lexical weighting feature. scored with lowercased, tokenized NIST BLEU, and exact match METEOR (Papineni et al., 2002; Lavie and Agarwal, 2007). The baseline system gives a BLEU score of 29.8, which increases to 30.5 with lex, as shown in Table 1. For reference, training on all sentences of length less than 40 (the shared task baseline default) gives 32.4 BLEU with lex. 5.2 Learned Distribution Performance We initialized the sampler with a configuration derived from the word alignments generated by the baseline. We greedily constructed a phrase alignment from the word alignment by identifying minimal phrase pairs consistent with the word alignment in each region of the sentence. We then ran the sampler for 1"
D08-1033,P08-1084,0,0.00762606,"m pruned using word alignments. Subsequent work has relied heavily on word alignments to constrain inference, even under reordering models that admit polynomial-time E-steps (Cherry and Lin, 2007; Zhang et al., 2008). None of these approximations are consistent, and they offer no method of measuring their biases. Gibbs sampling is not only consistent in the limit, but also allows us to add Bayesian priors conveniently (section 4). Of course, sampling has liabilities as well: we do not know in advance how long we need to run the sampler to approximate the desired expectations “closely enough.” Snyder and Barzilay (2008) describe a Gibbs sampler for a bilingual morphology model very similar in structure to ours. However, the basic sampling step they propose – resampling all segmentations and alignments for a sequence at once – requires a #P-hard computation. While this asymptotic complexity was apparently not prohibitive in the case of morphological alignment, where the sequences are short, it is prohibitive in phrase alignment, where the sentences are often very long. 3.2 Sampling with the S WAP Operator Our Gibbs sampler repeatedly applies each of five operators to each position in each training sentence pa"
D08-1033,P06-1124,0,0.110307,"d alignment, state z0 , which sets all latent variables to some initial configuration. We then produce a sequence of sample states zi , each of which differs from the last by some small local change. The samples zi are guaranteed (in the limit) to consistently approximate the conditional distribution P (z|x, θ) (or P (z|x) later). Therefore, the average counts of phrase pairs in the samples converge to expected counts under the model. Normalizing these expected counts yields estimates for the features φ(e|f ) and φ(f |e). Gibbs sampling is not new to the natural language processing community (Teh, 2006; Johnson et al., 2007). However, it is usually used as a search procedure akin to simulated annealing, rather than for approximating expectations (Goldwater et al., 2006; Finkel et al., 2007). Our application is also atypical 316 for an NLP application in that we use an approximate sampler not only to include Bayesian prior information (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). That is, we could not run EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under"
D08-1033,P08-1012,0,0.552863,"un EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under P (z|x, θ) have been approximated before in order to run EM. Marcu and Wong (2002) employed local search from a heuristic initialization and collected alignment counts during a hill climb through the alignment space. DeNero et al. (2006) instead proposed an exponential-time dynamic program pruned using word alignments. Subsequent work has relied heavily on word alignments to constrain inference, even under reordering models that admit polynomial-time E-steps (Cherry and Lin, 2007; Zhang et al., 2008). None of these approximations are consistent, and they offer no method of measuring their biases. Gibbs sampling is not only consistent in the limit, but also allows us to add Bayesian priors conveniently (section 4). Of course, sampling has liabilities as well: we do not know in advance how long we need to run the sampler to approximate the desired expectations “closely enough.” Snyder and Barzilay (2008) describe a Gibbs sampler for a bilingual morphology model very similar in structure to ours. However, the basic sampling step they propose – resampling all segmentations and alignments for"
D08-1033,J93-2003,0,\N,Missing
D08-1033,2006.amta-papers.2,0,\N,Missing
D08-1091,abeille-etal-2000-building,0,0.148692,"Missing"
D08-1091,P05-1038,0,0.0263235,"Missing"
D08-1091,P05-1022,0,0.810022,"best parsing numbers on several metrics, for several domains and languages. Discriminative parsing has been investigated before, such as in Johnson (2001), Clark and Curran (2004), Henderson (2004), Koo and Collins (2005), Turian et al. (2007), Finkel et al. (2008), and, most similarly, in Petrov and Klein (2008). However, in all of these cases, the final parsing performance fell short of the best generative models by several percentage points or only short sentences were used. Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (Charniak and Johnson, 2005; Huang, 2008). Multi-scale grammars, in contrast, give higher accuracies using smaller grammars than previous work in this direction, outperforming top generative models in grammar size and in parsing accuracy. 2 Latent Variable Parsing Treebanks are typically not annotated with fully detailed syntactic structure. Rather, they present only a coarse trace of the true underlying processes. As a result, learning a grammar for parsing requires the estimation of a more highly articulated model than the naive CFG embodied by such treebanks. A manual approach might take the category NP and subdivide"
D08-1091,N06-1022,0,0.047385,"Missing"
D08-1091,A00-2018,0,0.13979,"parsing accuracies with the smallest reported grammars. 1 Introduction In latent variable approaches to parsing (Matsuzaki et al., 2005; Petrov et al., 2006), one models an observed treebank of coarse parse trees using a grammar over more refined, but unobserved, derivation trees. The parse trees represent the desired output of the system, while the derivation trees represent the typically much more complex underlying syntactic processes. In recent years, latent variable methods have been shown to produce grammars which are as good as, or even better than, earlier parsing work (Collins, 1999; Charniak, 2000). In particular, in Petrov et al. (2006) we exhibited a very accurate We introduce multi-scale grammars, in which some productions reference fine categories, while others reference coarse categories (see Figure 2). We use the general framework of hidden variable CRFs (Lafferty et al., 2001; Koo and Collins, 2005), where gradient-based optimization maximizes the likelihood of the observed variables, here parse trees, summing over log-linearly scored derivations. With multi-scale grammars, it is natural to refine productions rather than categories. As a result, a category such as NP can be compl"
D08-1091,P04-1014,0,0.0245713,"put features integrally in our dynamic program. Our multi-scale grammars are 3 orders of magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of Petrov et al. (2006). 867 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 867–876, c Honolulu, October 2008. 2008 Association for Computational Linguistics In addition, we exhibit the best parsing numbers on several metrics, for several domains and languages. Discriminative parsing has been investigated before, such as in Johnson (2001), Clark and Curran (2004), Henderson (2004), Koo and Collins (2005), Turian et al. (2007), Finkel et al. (2008), and, most similarly, in Petrov and Klein (2008). However, in all of these cases, the final parsing performance fell short of the best generative models by several percentage points or only short sentences were used. Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (Charniak and Johnson, 2005; Huang, 2008). Multi-scale grammars, in contrast, give higher accuracies using smaller grammars than previous work in this direction, outperforming top g"
D08-1091,P08-1109,0,0.159821,"magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of Petrov et al. (2006). 867 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 867–876, c Honolulu, October 2008. 2008 Association for Computational Linguistics In addition, we exhibit the best parsing numbers on several metrics, for several domains and languages. Discriminative parsing has been investigated before, such as in Johnson (2001), Clark and Curran (2004), Henderson (2004), Koo and Collins (2005), Turian et al. (2007), Finkel et al. (2008), and, most similarly, in Petrov and Klein (2008). However, in all of these cases, the final parsing performance fell short of the best generative models by several percentage points or only short sentences were used. Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (Charniak and Johnson, 2005; Huang, 2008). Multi-scale grammars, in contrast, give higher accuracies using smaller grammars than previous work in this direction, outperforming top generative models in grammar size and in parsing accuracy. 2 Latent Variable Parsing Tr"
D08-1091,W01-0521,0,0.0561878,"Missing"
D08-1091,P04-1013,0,0.0179826,"n our dynamic program. Our multi-scale grammars are 3 orders of magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of Petrov et al. (2006). 867 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 867–876, c Honolulu, October 2008. 2008 Association for Computational Linguistics In addition, we exhibit the best parsing numbers on several metrics, for several domains and languages. Discriminative parsing has been investigated before, such as in Johnson (2001), Clark and Curran (2004), Henderson (2004), Koo and Collins (2005), Turian et al. (2007), Finkel et al. (2008), and, most similarly, in Petrov and Klein (2008). However, in all of these cases, the final parsing performance fell short of the best generative models by several percentage points or only short sentences were used. Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (Charniak and Johnson, 2005; Huang, 2008). Multi-scale grammars, in contrast, give higher accuracies using smaller grammars than previous work in this direction, outperforming top generative models i"
D08-1091,P08-1067,0,0.1807,"eral metrics, for several domains and languages. Discriminative parsing has been investigated before, such as in Johnson (2001), Clark and Curran (2004), Henderson (2004), Koo and Collins (2005), Turian et al. (2007), Finkel et al. (2008), and, most similarly, in Petrov and Klein (2008). However, in all of these cases, the final parsing performance fell short of the best generative models by several percentage points or only short sentences were used. Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (Charniak and Johnson, 2005; Huang, 2008). Multi-scale grammars, in contrast, give higher accuracies using smaller grammars than previous work in this direction, outperforming top generative models in grammar size and in parsing accuracy. 2 Latent Variable Parsing Treebanks are typically not annotated with fully detailed syntactic structure. Rather, they present only a coarse trace of the true underlying processes. As a result, learning a grammar for parsing requires the estimation of a more highly articulated model than the naive CFG embodied by such treebanks. A manual approach might take the category NP and subdivide it into one s"
D08-1091,J98-4004,0,0.194826,"r grammars than previous work in this direction, outperforming top generative models in grammar size and in parsing accuracy. 2 Latent Variable Parsing Treebanks are typically not annotated with fully detailed syntactic structure. Rather, they present only a coarse trace of the true underlying processes. As a result, learning a grammar for parsing requires the estimation of a more highly articulated model than the naive CFG embodied by such treebanks. A manual approach might take the category NP and subdivide it into one subcategory NPˆS for subjects and another subcategory NPˆVP for objects (Johnson, 1998; Klein and Manning, 2003). However, rather than devising linguistically motivated features or splits, latent variable parsing takes a fully automated approach, in which each symbol is split into unconstrained subcategories. 2.1 Latent Variable Grammars Latent variable grammars augment the treebank trees with latent variables at each node. This creates a set of (exponentially many) derivations over split categories for each of the original parse trees over unsplit categories. For each observed category A we now have a set of latent subcategories Ax . For example, NP might be split into NP1 thr"
D08-1091,P01-1042,0,0.131099,"nefit of some input features integrally in our dynamic program. Our multi-scale grammars are 3 orders of magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of Petrov et al. (2006). 867 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 867–876, c Honolulu, October 2008. 2008 Association for Computational Linguistics In addition, we exhibit the best parsing numbers on several metrics, for several domains and languages. Discriminative parsing has been investigated before, such as in Johnson (2001), Clark and Curran (2004), Henderson (2004), Koo and Collins (2005), Turian et al. (2007), Finkel et al. (2008), and, most similarly, in Petrov and Klein (2008). However, in all of these cases, the final parsing performance fell short of the best generative models by several percentage points or only short sentences were used. Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (Charniak and Johnson, 2005; Huang, 2008). Multi-scale grammars, in contrast, give higher accuracies using smaller grammars than previous work in this direc"
D08-1091,P03-1054,1,0.0687685,"previous work in this direction, outperforming top generative models in grammar size and in parsing accuracy. 2 Latent Variable Parsing Treebanks are typically not annotated with fully detailed syntactic structure. Rather, they present only a coarse trace of the true underlying processes. As a result, learning a grammar for parsing requires the estimation of a more highly articulated model than the naive CFG embodied by such treebanks. A manual approach might take the category NP and subdivide it into one subcategory NPˆS for subjects and another subcategory NPˆVP for objects (Johnson, 1998; Klein and Manning, 2003). However, rather than devising linguistically motivated features or splits, latent variable parsing takes a fully automated approach, in which each symbol is split into unconstrained subcategories. 2.1 Latent Variable Grammars Latent variable grammars augment the treebank trees with latent variables at each node. This creates a set of (exponentially many) derivations over split categories for each of the original parse trees over unsplit categories. For each observed category A we now have a set of latent subcategories Ax . For example, NP might be split into NP1 through NP8 . The parameters"
D08-1091,H05-1064,0,0.0495169,"the desired output of the system, while the derivation trees represent the typically much more complex underlying syntactic processes. In recent years, latent variable methods have been shown to produce grammars which are as good as, or even better than, earlier parsing work (Collins, 1999; Charniak, 2000). In particular, in Petrov et al. (2006) we exhibited a very accurate We introduce multi-scale grammars, in which some productions reference fine categories, while others reference coarse categories (see Figure 2). We use the general framework of hidden variable CRFs (Lafferty et al., 2001; Koo and Collins, 2005), where gradient-based optimization maximizes the likelihood of the observed variables, here parse trees, summing over log-linearly scored derivations. With multi-scale grammars, it is natural to refine productions rather than categories. As a result, a category such as NP can be complex in some regions of the grammar while remaining simpler in other regions. Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features (Taskar et al., 2004), giving the benefit of some input features integrally in our"
D08-1091,D07-1072,1,0.509376,"1 Latent Variable Grammars Latent variable grammars augment the treebank trees with latent variables at each node. This creates a set of (exponentially many) derivations over split categories for each of the original parse trees over unsplit categories. For each observed category A we now have a set of latent subcategories Ax . For example, NP might be split into NP1 through NP8 . The parameters of the refined productions Ax → By Cz , where Ax is a subcategory of A, By of B, and Cz of C, can then be estimated in various ways; past work has included both generative 868 (Matsuzaki et al., 2005; Liang et al., 2007) and discriminative approaches (Petrov and Klein, 2008). We take the discriminative log-linear approach here. Note that the comparison is only between estimation methods, as Smith and Johnson (2007) show that the model classes are the same. 2.2 Log-Linear Latent Variable Grammars In a log-linear latent variable grammar, each production r = Ax → By Cz is associated with a multiplicative weight φr (Johnson, 2001; Petrov and Klein, 2008) (sometimes we will use the log-weight θr when convenient). The probability of a derivation t of a sentence w is proportional to the product of the weights of its"
D08-1091,J93-2004,0,0.0418058,"Missing"
D08-1091,P05-1010,0,0.52518,"grammar are refined to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars of Petrov et al. (2006). In addition, our discriminative approach integrally admits features beyond local tree configurations. We present a multiscale training method along with an efficient CKY-style dynamic program. On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars. 1 Introduction In latent variable approaches to parsing (Matsuzaki et al., 2005; Petrov et al., 2006), one models an observed treebank of coarse parse trees using a grammar over more refined, but unobserved, derivation trees. The parse trees represent the desired output of the system, while the derivation trees represent the typically much more complex underlying syntactic processes. In recent years, latent variable methods have been shown to produce grammars which are as good as, or even better than, earlier parsing work (Collins, 1999; Charniak, 2000). In particular, in Petrov et al. (2006) we exhibited a very accurate We introduce multi-scale grammars, in which some p"
D08-1091,N06-1040,0,0.0110067,"not appeal to the underlying single scale grammar. However, in the present work, we use our multiscale grammars only to compute expectations of the underlying grammars in an efficient, implicit way. 5 Learning Sparse Multi-Scale Grammars We now consider how to discriminatively learn multi-scale grammars by iterative splitting productions. There are two main concerns. First, because multi-scale grammars are most effective when many productions share the same weight, sparsity is very desirable. In the present work, we exploit L1 -regularization, though other techniques such as structural zeros (Mohri and Roark, 2006) could also potentially be used. Second, training requires repeated parsing, so we use coarse-to-fine chart caching to greatly accelerate each iteration. 870 We directly optimize this non-convex objective function using a numerical gradient based method (LBFGS (Nocedal and Wright, 1999) in our implementation). To handle the non-diferentiability of the L1 -regularization term R(θ) we use the orthant-wise method of Andrew and Gao (2007). Fitting the loglinear model involves the following derivatives:   ∂Lcond (θ) X = Eθ [fr (t)|Ti ] − Eθ [fr (t)|wi ] ∂θr i where the first term is the expected"
D08-1091,N07-1051,1,0.623478,"y are still dense at the production level, which we address here. As in Petrov et al. (2006), we arrange our subcategories into a hierarchy, as shown in Figure 1. In practice, the construction of the hierarchy is tightly coupled to a split-based learning process (see Section 5). We use the naming convention that an original category A becomes A0 and A1 in the first round; A0 then becoming A00 and A01 in the second round, and so on. We will use x ˆ ≻ x to indicate that the subscript or subcategory x is a refinement of x ˆ.1 We 1 Conversely, x ˆ is a coarser version of x, or, in the language of Petrov and Klein (2007), x ˆ is a projection of x. θr¯ * S i n g l e  s c a l e p r o d u c t i o n s u M l t i  s c a l e p r o d u c t i o n θrˆ s → D T 0 0 0 → t h e + D T 0 0 1 D T 0 1 D T 0 1 → t h 5 0 → t 0 e + h 5 . 0 } e + 0 . 7 . * D T 0 0 → t h e + D T 0 1 D T 0 1 0 → t h 5 . 0 . 3 → e 3 + 7 1 0 1 + 1 → t h e + D T 1 0 0 → t h 0 + 0 0 0 0 5 0 . 0 + 0 1 5 0 . 0 + 1 1 1 0 7 0 . 3 + 1 1 1 1 2 + 0 1 0 0 2 1 . 1 + 0 1 2 1 . 1 + 2 0 2 1 . 1 + 1 . 1 D T 1 D T 1 1 D T 1 0 1 1 → 0 t → h t e h + 2 . 1 + 2 . 1 + 2 . 1 e 1 2 . 1 1 → t h → t h e } 1 0 + T 1 → t . 1 2 0 D 2 e + 1 1 1 2 e + 0 1 h e + 2 . 5 . 0 1 0 1 0"
D08-1091,P06-1055,1,0.760875,"scriminative utility. In this paper, we present a discriminative approach which addresses both of these limitations. We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement. The model is formally a latent variable CRF grammar over trees, learned by iteratively splitting grammar productions (not categories). Different regions of the grammar are refined to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars of Petrov et al. (2006). In addition, our discriminative approach integrally admits features beyond local tree configurations. We present a multiscale training method along with an efficient CKY-style dynamic program. On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars. 1 Introduction In latent variable approaches to parsing (Matsuzaki et al., 2005; Petrov et al., 2006), one models an observed treebank of coarse parse trees using a grammar over more refined, but unobserved, derivation trees. The parse trees represent the desired output"
D08-1091,A97-1014,0,0.0727901,"Missing"
D08-1091,J07-4003,0,0.0169931,"or each of the original parse trees over unsplit categories. For each observed category A we now have a set of latent subcategories Ax . For example, NP might be split into NP1 through NP8 . The parameters of the refined productions Ax → By Cz , where Ax is a subcategory of A, By of B, and Cz of C, can then be estimated in various ways; past work has included both generative 868 (Matsuzaki et al., 2005; Liang et al., 2007) and discriminative approaches (Petrov and Klein, 2008). We take the discriminative log-linear approach here. Note that the comparison is only between estimation methods, as Smith and Johnson (2007) show that the model classes are the same. 2.2 Log-Linear Latent Variable Grammars In a log-linear latent variable grammar, each production r = Ax → By Cz is associated with a multiplicative weight φr (Johnson, 2001; Petrov and Klein, 2008) (sometimes we will use the log-weight θr when convenient). The probability of a derivation t of a sentence w is proportional to the product of the weights of its productions r: Y P (t|w) ∝ φr r∈t The score of a parse T is then the sum of the scores of its derivations: X P (t|w) P (T |w) = t∈T 3 Hierarchical Refinement Grammar refinement becomes challenging"
D08-1091,W04-3201,1,0.951658,"framework of hidden variable CRFs (Lafferty et al., 2001; Koo and Collins, 2005), where gradient-based optimization maximizes the likelihood of the observed variables, here parse trees, summing over log-linearly scored derivations. With multi-scale grammars, it is natural to refine productions rather than categories. As a result, a category such as NP can be complex in some regions of the grammar while remaining simpler in other regions. Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features (Taskar et al., 2004), giving the benefit of some input features integrally in our dynamic program. Our multi-scale grammars are 3 orders of magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of Petrov et al. (2006). 867 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 867–876, c Honolulu, October 2008. 2008 Association for Computational Linguistics In addition, we exhibit the best parsing numbers on several metrics, for several domains and languages. Discriminative parsing has been investigated befo"
D08-1091,J03-4003,0,\N,Missing
D08-1092,2004.tmi-1.14,0,0.0130265,"ting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation. 1 Introduction Methods for machine translation (MT) have increasingly leveraged not only the formal machinery of syntax (Wu, 1997; Chiang, 2007; Zhang et al., 2008), but also linguistic tree structures of either the source side (Huang et al., 2006; Marton and Resnik, 2008; Quirk et al., 2005), the target side (Yamada and Knight, 2001; Galley et al., 2004; Zollmann et al., 2006; Shen et al., 2008), or both (Och et al., 2003; Aue et al., 2004; Ding and Palmer, 2005). These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality. Unfortunately, parsing general bitexts well can be a challenge for newswiretrained treebank parsers for many reasons, including out-of-domain input and tokenization issues. Formally, we present a log-linear model over triples of source trees, target trees, and node-tonode tree alignments between them. We consider a set of core features which capture the scores of monolingual parsers as well as measures of syntactic alignment. Our model condit"
D08-1092,W00-1201,0,0.0592648,"roadly one-to-one in our word alignment model. S CALED I NSIDE B OTH = S CALED I N S RC O UT T RG = S CALED I N T RG O UT S RC = I NSIDE B OTH p |i(n) |· |i(n0 )| I N S RC O UT T RG p |i(n) |· |o(n0 )| I N T RG O UT S RC p |o(n) |· |i(n0 )| Head word alignment features: When considering a node pair (n, n0 ), especially one which dominates a large area, the above measures treat all spanned words as equally important. However, lexical heads are generally more representative than other spanned words. Let h select the headword of a node according to standard head percolation rules (Collins, 2003; Bikel and Chiang, 2000). A LIGN H EADW ORD = a(h(n), h(n0 )) v∈i(n) v 0 ∈i(n0 ) X X H ARD I N S RC O UT T RG = H ARDA LIGN H EADW ORD = δ(h(n), h(n0 )) a(v, v 0 ) v∈i(n) v 0 ∈o(n0 ) I N T RG O UT S RC = X X 3.3 a(v, v 0 ) v∈o(n) v 0 ∈i(n0 ) 2 It is of course possible to learn good alignments using lexical indicator functions or other direct techniques, but given our very limited training data, it is advantageous to leverage counts from an unsupervised alignment system. 879 Tree Structure Features We also consider features that measure correspondences between the tree structures themselves. Span difference: We expect"
D08-1092,P05-1022,0,0.0447355,"practice it converges after a few iterations given sufficient training data. We initialize the procedure by setting w0 as defined above. 4.2 = arg max exp(w&gt; φ(t, a, t0 )) exp(w&gt; φ(g, a0 (g, g 0 ), g 0 )) &gt; 0 0 (t,t0 ) exp(w φ(t, a0 (t, t ), t )) w∗ = arg max P Pseudo-gold Trees When training our model, we approximate the sets of all trees with k-best lists, T and T 0 , produced by monolingual parsers. Since these sets are not guaranteed to contain the gold trees g and g 0 , our next approximation is to define a set of pseudo-gold trees, following previous work in monolingual parse reranking (Charniak and Johnson, 2005). We define Tˆ (Tˆ0 ) as the F1 -optimal subset of T (T 0 ). We then modify (4) to reflect the fact that we are seeking to maximize the likelihood of trees in this subset: X w∗ = arg max P(t, t0 |s, s0 , w) (5) w (t,t0 )∈(Tˆ,Tˆ0 ) where P(t, t0 |s, s0 , w) = P maxa exp(w&gt; φ(t, a, t0 )) &gt; ¯ ¯0 (t¯,t¯0 )∈(T,T 0 ) maxa exp(w φ(t, a, t )) (6) 4.3 Training Set Pruning To reduce the time and space requirements for training, we do not always use the full k-best lists. To prune the set T , we rank all the trees in T from 1 to k, according to their log likelihood under the baseline parsing model, and f"
D08-1092,J07-2003,0,0.00679193,"treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables. The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation. 1 Introduction Methods for machine translation (MT) have increasingly leveraged not only the formal machinery of syntax (Wu, 1997; Chiang, 2007; Zhang et al., 2008), but also linguistic tree structures of either the source side (Huang et al., 2006; Marton and Resnik, 2008; Quirk et al., 2005), the target side (Yamada and Knight, 2001; Galley et al., 2004; Zollmann et al., 2006; Shen et al., 2008), or both (Och et al., 2003; Aue et al., 2004; Ding and Palmer, 2005). These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality. Unfortunately, parsing general bitexts well can be a challenge for newswiretrained treebank parsers for many reasons, including out-of-domain inp"
D08-1092,J03-4003,0,0.0167912,"ts tend to be broadly one-to-one in our word alignment model. S CALED I NSIDE B OTH = S CALED I N S RC O UT T RG = S CALED I N T RG O UT S RC = I NSIDE B OTH p |i(n) |· |i(n0 )| I N S RC O UT T RG p |i(n) |· |o(n0 )| I N T RG O UT S RC p |o(n) |· |i(n0 )| Head word alignment features: When considering a node pair (n, n0 ), especially one which dominates a large area, the above measures treat all spanned words as equally important. However, lexical heads are generally more representative than other spanned words. Let h select the headword of a node according to standard head percolation rules (Collins, 2003; Bikel and Chiang, 2000). A LIGN H EADW ORD = a(h(n), h(n0 )) v∈i(n) v 0 ∈i(n0 ) X X H ARD I N S RC O UT T RG = H ARDA LIGN H EADW ORD = δ(h(n), h(n0 )) a(v, v 0 ) v∈i(n) v 0 ∈o(n0 ) I N T RG O UT S RC = X X 3.3 a(v, v 0 ) v∈o(n) v 0 ∈i(n0 ) 2 It is of course possible to learn good alignments using lexical indicator functions or other direct techniques, but given our very limited training data, it is advantageous to leverage counts from an unsupervised alignment system. 879 Tree Structure Features We also consider features that measure correspondences between the tree structures themselves. S"
D08-1092,P07-1003,1,0.598052,"arsing general bitexts well can be a challenge for newswiretrained treebank parsers for many reasons, including out-of-domain input and tokenization issues. Formally, we present a log-linear model over triples of source trees, target trees, and node-tonode tree alignments between them. We consider a set of core features which capture the scores of monolingual parsers as well as measures of syntactic alignment. Our model conditions on the input sentence pair and so features can and do reference input characteristics such as posterior distributions from a word-level aligner (Liang et al., 2006; DeNero and Klein, 2007). Our training data is the translated section of the Chinese treebank (Xue et al., 2002; Bies et al., 2007), so at training time correct trees are observed on both the source and target side. Gold tree alignments are not present and so are induced as latent variables using an iterative training procedure. To make the process efficient and modular to existing monolingual parsers, we introduce several approximations: use of k-best lists in candidate generation, an adaptive bound to avoid considering all k 2 combinations, and Viterbi approximations to alignment posteriors. 877 Proceedings of the"
D08-1092,P05-1067,0,0.0138573,"trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation. 1 Introduction Methods for machine translation (MT) have increasingly leveraged not only the formal machinery of syntax (Wu, 1997; Chiang, 2007; Zhang et al., 2008), but also linguistic tree structures of either the source side (Huang et al., 2006; Marton and Resnik, 2008; Quirk et al., 2005), the target side (Yamada and Knight, 2001; Galley et al., 2004; Zollmann et al., 2006; Shen et al., 2008), or both (Och et al., 2003; Aue et al., 2004; Ding and Palmer, 2005). These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality. Unfortunately, parsing general bitexts well can be a challenge for newswiretrained treebank parsers for many reasons, including out-of-domain input and tokenization issues. Formally, we present a log-linear model over triples of source trees, target trees, and node-tonode tree alignments between them. We consider a set of core features which capture the scores of monolingual parsers as well as measures of syntactic alignment. Our model conditions on the input senten"
D08-1092,N04-1035,0,0.028942,"onolingual parser baselines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation. 1 Introduction Methods for machine translation (MT) have increasingly leveraged not only the formal machinery of syntax (Wu, 1997; Chiang, 2007; Zhang et al., 2008), but also linguistic tree structures of either the source side (Huang et al., 2006; Marton and Resnik, 2008; Quirk et al., 2005), the target side (Yamada and Knight, 2001; Galley et al., 2004; Zollmann et al., 2006; Shen et al., 2008), or both (Och et al., 2003; Aue et al., 2004; Ding and Palmer, 2005). These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality. Unfortunately, parsing general bitexts well can be a challenge for newswiretrained treebank parsers for many reasons, including out-of-domain input and tokenization issues. Formally, we present a log-linear model over triples of source trees, target trees, and node-tonode tree alignments between them. We consider a set of core features which capture the sc"
D08-1092,P06-1121,0,0.0109643,"peed/performance tradeoff. However, when reranking our entire MT corpus, we used a value of 200, sacrificing a tiny bit of performance for an extra factor of 2 in speed.8 6.4 Joint Parsing Ch F1 Eng F1 84.95 76.75 86.23 78.43 86.64 79.27 86.61 79.10 86.71 79.37 86.67 79.47 Machine Translation To test the impact of joint parsing on syntactic MT systems, we compared the results of training an MT system with two different sets of trees: those produced by the baseline parsers, and those produced by our joint parser. For this evaluation, we used a syntactic system based on Galley et al. (2004) and Galley et al. (2006), which extracts tree-to-string transducer rules based on target-side trees. We trained the system on 150,000 Chinese-English sentence pairs from the training corpus of Wang et al. (2007), and used a large (close to 5 billion tokens) 4-gram lanBLEU Baseline 18.7 Joint 21.1 Moses 18.8 Table 7: MT comparison on a syntactic system trained with trees output from either baseline monolingual parsers or our joint parser. To facilitate relative comparison, the Moses (Koehn et al., 2007) number listed reflects the default Moses configuration, including its full distortion model, and standard training p"
D08-1092,2006.amta-papers.8,0,0.0194092,"irs, with alignments treated as latent variables. The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation. 1 Introduction Methods for machine translation (MT) have increasingly leveraged not only the formal machinery of syntax (Wu, 1997; Chiang, 2007; Zhang et al., 2008), but also linguistic tree structures of either the source side (Huang et al., 2006; Marton and Resnik, 2008; Quirk et al., 2005), the target side (Yamada and Knight, 2001; Galley et al., 2004; Zollmann et al., 2006; Shen et al., 2008), or both (Och et al., 2003; Aue et al., 2004; Ding and Palmer, 2005). These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality. Unfortunately, parsing general bitexts well can be a challenge for newswiretrained treebank parsers for many reasons, including out-of-domain input and tokenization issues. Formally, we present a log-linear model over triples of source trees, target"
D08-1092,P07-2045,0,0.00277628,"ose produced by our joint parser. For this evaluation, we used a syntactic system based on Galley et al. (2004) and Galley et al. (2006), which extracts tree-to-string transducer rules based on target-side trees. We trained the system on 150,000 Chinese-English sentence pairs from the training corpus of Wang et al. (2007), and used a large (close to 5 billion tokens) 4-gram lanBLEU Baseline 18.7 Joint 21.1 Moses 18.8 Table 7: MT comparison on a syntactic system trained with trees output from either baseline monolingual parsers or our joint parser. To facilitate relative comparison, the Moses (Koehn et al., 2007) number listed reflects the default Moses configuration, including its full distortion model, and standard training pipeline. guage model for decoding. We tuned and evaluated BLEU (Papineni et al., 2001) on separate held-out sets of sentences of up to length 40 from the same corpus. The results are in Table 7, showing that joint parsing yields a BLEU increase of 2.4.9 8 Conclusions By jointly parsing (and aligning) sentences in a translation pair, it is possible to exploit mutual constraints that improve the quality of syntactic analyses over independent monolingual parsing. We presented a joi"
D08-1092,N06-1014,1,0.119273,"ty. Unfortunately, parsing general bitexts well can be a challenge for newswiretrained treebank parsers for many reasons, including out-of-domain input and tokenization issues. Formally, we present a log-linear model over triples of source trees, target trees, and node-tonode tree alignments between them. We consider a set of core features which capture the scores of monolingual parsers as well as measures of syntactic alignment. Our model conditions on the input sentence pair and so features can and do reference input characteristics such as posterior distributions from a word-level aligner (Liang et al., 2006; DeNero and Klein, 2007). Our training data is the translated section of the Chinese treebank (Xue et al., 2002; Bies et al., 2007), so at training time correct trees are observed on both the source and target side. Gold tree alignments are not present and so are induced as latent variables using an iterative training procedure. To make the process efficient and modular to existing monolingual parsers, we introduce several approximations: use of k-best lists in candidate generation, an adaptive bound to avoid considering all k 2 combinations, and Viterbi approximations to alignment posteriors"
D08-1092,P08-1114,0,0.0147671,"treated as latent variables. The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation. 1 Introduction Methods for machine translation (MT) have increasingly leveraged not only the formal machinery of syntax (Wu, 1997; Chiang, 2007; Zhang et al., 2008), but also linguistic tree structures of either the source side (Huang et al., 2006; Marton and Resnik, 2008; Quirk et al., 2005), the target side (Yamada and Knight, 2001; Galley et al., 2004; Zollmann et al., 2006; Shen et al., 2008), or both (Och et al., 2003; Aue et al., 2004; Ding and Palmer, 2005). These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality. Unfortunately, parsing general bitexts well can be a challenge for newswiretrained treebank parsers for many reasons, including out-of-domain input and tokenization issues. Formally, we present a log-linear model over triples of source trees, target trees, and node-tonode t"
D08-1092,2001.mtsummit-papers.68,0,0.0123359,"-side trees. We trained the system on 150,000 Chinese-English sentence pairs from the training corpus of Wang et al. (2007), and used a large (close to 5 billion tokens) 4-gram lanBLEU Baseline 18.7 Joint 21.1 Moses 18.8 Table 7: MT comparison on a syntactic system trained with trees output from either baseline monolingual parsers or our joint parser. To facilitate relative comparison, the Moses (Koehn et al., 2007) number listed reflects the default Moses configuration, including its full distortion model, and standard training pipeline. guage model for decoding. We tuned and evaluated BLEU (Papineni et al., 2001) on separate held-out sets of sentences of up to length 40 from the same corpus. The results are in Table 7, showing that joint parsing yields a BLEU increase of 2.4.9 8 Conclusions By jointly parsing (and aligning) sentences in a translation pair, it is possible to exploit mutual constraints that improve the quality of syntactic analyses over independent monolingual parsing. We presented a joint log-linear model over source trees, target trees, and node-to-node alignments between them, which is used to select an optimal tree pair from a k-best list. On Chinese treebank data, this procedure im"
D08-1092,N07-1051,1,0.100535,"tatistical parsers, but only the Chinese side is correct. The gold English parse shown in b) is further down in the 100-best list, despite being more consistent with the gold Chinese parse. The circles show where the two parses differ. Note that in b), the ADVP and PP nodes correspond nicely to Chinese tree nodes, whereas the correspondence for nodes in a), particularly the SBAR node, is less clear. We evaluate our system primarily as a parser and secondarily as a component in a machine translation pipeline. For both English and Chinese, we begin with the state-of-the-art parsers presented in Petrov and Klein (2007) as a baseline. Joint parse selection improves the English trees by 2.5 F1 and the Chinese trees by 1.8 F1 . While other Chinese treebank parsers do not have access to English side translations, this Chinese figure does outperform all published monolingual Chinese treebank results on an equivalent split of the data. As MT motivates this work, another valuable evaluation is the effect of joint selection on downstream MT quality. In an experiment using a syntactic MT system, we find that rules extracted from joint parses results in an increase of 2.4 BLEU points over rules extracted from indepen"
D08-1092,P05-1034,0,0.0619576,"les. The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation. 1 Introduction Methods for machine translation (MT) have increasingly leveraged not only the formal machinery of syntax (Wu, 1997; Chiang, 2007; Zhang et al., 2008), but also linguistic tree structures of either the source side (Huang et al., 2006; Marton and Resnik, 2008; Quirk et al., 2005), the target side (Yamada and Knight, 2001; Galley et al., 2004; Zollmann et al., 2006; Shen et al., 2008), or both (Och et al., 2003; Aue et al., 2004; Ding and Palmer, 2005). These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality. Unfortunately, parsing general bitexts well can be a challenge for newswiretrained treebank parsers for many reasons, including out-of-domain input and tokenization issues. Formally, we present a log-linear model over triples of source trees, target trees, and node-tonode tree alignments betwee"
D08-1092,P08-1066,0,0.00962363,"dicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation. 1 Introduction Methods for machine translation (MT) have increasingly leveraged not only the formal machinery of syntax (Wu, 1997; Chiang, 2007; Zhang et al., 2008), but also linguistic tree structures of either the source side (Huang et al., 2006; Marton and Resnik, 2008; Quirk et al., 2005), the target side (Yamada and Knight, 2001; Galley et al., 2004; Zollmann et al., 2006; Shen et al., 2008), or both (Och et al., 2003; Aue et al., 2004; Ding and Palmer, 2005). These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality. Unfortunately, parsing general bitexts well can be a challenge for newswiretrained treebank parsers for many reasons, including out-of-domain input and tokenization issues. Formally, we present a log-linear model over triples of source trees, target trees, and node-tonode tree alignments between them. We consider a set of core features which capture the scores of monolingual parsers as well as meas"
D08-1092,W04-3207,0,0.734633,"Missing"
D08-1092,J97-3002,0,0.298728,"e Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables. The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation. 1 Introduction Methods for machine translation (MT) have increasingly leveraged not only the formal machinery of syntax (Wu, 1997; Chiang, 2007; Zhang et al., 2008), but also linguistic tree structures of either the source side (Huang et al., 2006; Marton and Resnik, 2008; Quirk et al., 2005), the target side (Yamada and Knight, 2001; Galley et al., 2004; Zollmann et al., 2006; Shen et al., 2008), or both (Och et al., 2003; Aue et al., 2004; Ding and Palmer, 2005). These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality. Unfortunately, parsing general bitexts well can be a challenge for newswiretrained treebank parsers for many reasons, including out"
D08-1092,C02-1145,0,0.0129126,"easons, including out-of-domain input and tokenization issues. Formally, we present a log-linear model over triples of source trees, target trees, and node-tonode tree alignments between them. We consider a set of core features which capture the scores of monolingual parsers as well as measures of syntactic alignment. Our model conditions on the input sentence pair and so features can and do reference input characteristics such as posterior distributions from a word-level aligner (Liang et al., 2006; DeNero and Klein, 2007). Our training data is the translated section of the Chinese treebank (Xue et al., 2002; Bies et al., 2007), so at training time correct trees are observed on both the source and target side. Gold tree alignments are not present and so are induced as latent variables using an iterative training procedure. To make the process efficient and modular to existing monolingual parsers, we introduce several approximations: use of k-best lists in candidate generation, an adaptive bound to avoid considering all k 2 combinations, and Viterbi approximations to alignment posteriors. 877 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 877–886, c H"
D08-1092,P01-1067,0,0.0728294,"rforms state-of-the-art monolingual parser baselines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation. 1 Introduction Methods for machine translation (MT) have increasingly leveraged not only the formal machinery of syntax (Wu, 1997; Chiang, 2007; Zhang et al., 2008), but also linguistic tree structures of either the source side (Huang et al., 2006; Marton and Resnik, 2008; Quirk et al., 2005), the target side (Yamada and Knight, 2001; Galley et al., 2004; Zollmann et al., 2006; Shen et al., 2008), or both (Och et al., 2003; Aue et al., 2004; Ding and Palmer, 2005). These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality. Unfortunately, parsing general bitexts well can be a challenge for newswiretrained treebank parsers for many reasons, including out-of-domain input and tokenization issues. Formally, we present a log-linear model over triples of source trees, target trees, and node-tonode tree alignments between them. We consider a set of core features"
D08-1092,P08-1012,0,0.00839981,"model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables. The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation. 1 Introduction Methods for machine translation (MT) have increasingly leveraged not only the formal machinery of syntax (Wu, 1997; Chiang, 2007; Zhang et al., 2008), but also linguistic tree structures of either the source side (Huang et al., 2006; Marton and Resnik, 2008; Quirk et al., 2005), the target side (Yamada and Knight, 2001; Galley et al., 2004; Zollmann et al., 2006; Shen et al., 2008), or both (Och et al., 2003; Aue et al., 2004; Ding and Palmer, 2005). These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality. Unfortunately, parsing general bitexts well can be a challenge for newswiretrained treebank parsers for many reasons, including out-of-domain input and tokenization i"
D08-1092,2006.iwslt-evaluation.20,0,0.0282558,"elines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation. 1 Introduction Methods for machine translation (MT) have increasingly leveraged not only the formal machinery of syntax (Wu, 1997; Chiang, 2007; Zhang et al., 2008), but also linguistic tree structures of either the source side (Huang et al., 2006; Marton and Resnik, 2008; Quirk et al., 2005), the target side (Yamada and Knight, 2001; Galley et al., 2004; Zollmann et al., 2006; Shen et al., 2008), or both (Och et al., 2003; Aue et al., 2004; Ding and Palmer, 2005). These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality. Unfortunately, parsing general bitexts well can be a challenge for newswiretrained treebank parsers for many reasons, including out-of-domain input and tokenization issues. Formally, we present a log-linear model over triples of source trees, target trees, and node-tonode tree alignments between them. We consider a set of core features which capture the scores of monolingual par"
D08-1092,P02-1040,0,\N,Missing
D09-1120,D08-1031,0,0.481136,"izes the merger or separation of clusters quadratically in the size of the cluster. • b3 (Amit and Baldwin, 1998): For each mention, form the intersection between the predicted cluster and the true cluster for that mention. The precision is the ratio of the intersection and the true cluster sizes and recall the ratio of the intersection to the predicted sizes; F1 is given by the harmonic mean over precision and recall from all mentions. Data Sets In this work we use the following data sets: Development: (see Section 3) • ACE2004-ROTH-DEV: Dev set split of the ACE 2004 training set utilized in Bengston and Roth (2008). The ACE data also annotates pre-nominal mentions which we map onto nominals. 68 documents and 4,536 mentions. • MUC (Vilain et al., 1995): For each true cluster, compute the number of predicted clusters which need to be merged to cover the true cluster. Divide this quantity by true cluster size minus one. Recall is given by the same procedure with predicated and true clusters reversed.4 Testing: (see Section 4) • ACE2004-CULOTTA-TEST: Test set split of the ACE 2004 training set utilized in Culotta et al. (2007) and Bengston and Roth (2008). Consists of 107 documents.2 • CEAF (Luo, 2005): For"
D09-1120,A00-2018,0,0.0379183,"008). Consists of 107 documents.2 • CEAF (Luo, 2005): For a similarity function between predicted and true clusters, CEAF scores the best match between true and predicted clusters using this function. We use the φ3 similarity function from Luo (2005). • ACE2004-NWIRE: ACE 2004 Newswire set to compare against Poon and Domingos (2008). Consists of 128 documents and 11,413 mentions; intersects with the other ACE data sets. • MUC-6-TEST: MUC6 formal evaluation set consisting of 30 documents and 2,068 mentions. Unlabeled: (see Section 3.2) • BLIPP: 1.8 million sentences of newswire parsed with the Charniak (2000) parser. No labeled coreference data; used for mining semantic information. • WIKI: 25k articles of English Wikipedia abstracts parsed by the Klein and Manning (2003) parser.3 No labeled coreference data; used for mining semantic information. 1 Other mention types exist and are annotated (such as prenominal), which are treated as nominals in this work. 2 The evaluation set was not made available to nonparticipants. 3 Wikipedia abstracts consist of roughly the first paragraph of the corresponding article Evaluation 3 System Description In this section we develop our system and report developmen"
D09-1120,N07-1011,0,0.190539,"ic constraints like the binding theory, the i-within-i filter, and appositive constructions restrict reference by configuration. Semantic constraints like selectional compatibility (e.g. a spokesperson can announce things) and subsumption (e.g. Microsoft is a company) rule out many possible referents. Finally, discourse phenomena such as salience and centering theory are assumed to heavily influence reference preferences. As these varied factors have given rise to a multitude of weak features, recent work has focused on how best to learn to combine them using models over reference structures (Culotta et al., 2007; Denis and Baldridge, 2007; Klenner and Ailloud, 2007). In this work, we break from the standard view. Instead, we consider a vastly more modular system in which coreference is predicted from a deterministic function of a few rich features. In particular, we assume a three-step process. First, a selfcontained syntactic module carefully represents syntactic structures using an augmented parser and extracts syntactic paths from mentions to potential antecedents. Some of these paths can be ruled in or out by deterministic but conservative syntactic constraints. Importantly, the bulk of the work"
D09-1120,N07-1030,0,0.0660612,"e binding theory, the i-within-i filter, and appositive constructions restrict reference by configuration. Semantic constraints like selectional compatibility (e.g. a spokesperson can announce things) and subsumption (e.g. Microsoft is a company) rule out many possible referents. Finally, discourse phenomena such as salience and centering theory are assumed to heavily influence reference preferences. As these varied factors have given rise to a multitude of weak features, recent work has focused on how best to learn to combine them using models over reference structures (Culotta et al., 2007; Denis and Baldridge, 2007; Klenner and Ailloud, 2007). In this work, we break from the standard view. Instead, we consider a vastly more modular system in which coreference is predicted from a deterministic function of a few rich features. In particular, we assume a three-step process. First, a selfcontained syntactic module carefully represents syntactic structures using an augmented parser and extracts syntactic paths from mentions to potential antecedents. Some of these paths can be ruled in or out by deterministic but conservative syntactic constraints. Importantly, the bulk of the work in the syntactic module is"
D09-1120,P08-2012,0,0.833199,"valuation metrics in Section 4. At a high level, our system resembles a pairwise coreference model (Soon et al., 1999; Ng and Cardie, 2002; Bengston and Roth, 2008); for each mention mi , we select either a single-best antecedent amongst the previous mentions m1 , . . . , mi−1 , or the NULL mention to indicate the underlying entity has not yet been evoked. Mentions are linearly ordered according to the position of the mention head with ties being broken by the larger node coming first. 4 The MUC measure is problematic when the system predicts many more clusters than actually exist (Luo, 2005; Finkel and Manning, 2008); also, singleton clusters do not contribute to evaluation. 1153 While much research (Ng and Cardie, 2002; Culotta et al., 2007; Haghighi and Klein, 2007; Poon and Domingos, 2008; Finkel and Manning, 2008) has explored how to reconcile pairwise decisions to form coherent clusters, we simply take the transitive closure of our pairwise decision (as in Ng and Cardie (2002) and Bengston and Roth (2008)) which can and does cause system errors. In contrast to most recent research, our pairwise decisions are not made with a learned model which outputs a probability or confidence, but instead for each"
D09-1120,P05-1045,0,0.011558,"only with matching head mentions, agreement is only a concern for pronouns. Traditional linguistic theory stipulates that coreferent mentions must agree in number, person, gender, and entity type (e.g. animacy). Here, we implement person, number and entity type agreement.6 A number feature is assigned to each mention deterministically based on the head and its POS tag. For entity type, we use NER labels. Ideally, we would like to have information about the entity type of each referential NP, however this information is not easily obtainable. Instead, we opt to utilize the Stanford NER tagger (Finkel et al., 2005) over the sentences in a document and annotate each NP with the NER label assigned to that mention head. For each mention, when its NP is assigned an NER label we allow it to only be compatible with that NER label.7 For pronouns, we deterministically assign a set of compatible NER values (e.g. personal pronouns can only be a PER6 Gender agreement, while important for general coreference resolution, did not contribute to the errors in our largely newswire data sets. 7 Or allow it to be compatible with all NER labels if the NER tagger doesn’t predict a label. florida lebanese arafat aol assad st"
D09-1120,J95-2003,0,0.211278,"Missing"
D09-1120,P07-1107,1,0.861764,"oth, 2008); for each mention mi , we select either a single-best antecedent amongst the previous mentions m1 , . . . , mi−1 , or the NULL mention to indicate the underlying entity has not yet been evoked. Mentions are linearly ordered according to the position of the mention head with ties being broken by the larger node coming first. 4 The MUC measure is problematic when the system predicts many more clusters than actually exist (Luo, 2005; Finkel and Manning, 2008); also, singleton clusters do not contribute to evaluation. 1153 While much research (Ng and Cardie, 2002; Culotta et al., 2007; Haghighi and Klein, 2007; Poon and Domingos, 2008; Finkel and Manning, 2008) has explored how to reconcile pairwise decisions to form coherent clusters, we simply take the transitive closure of our pairwise decision (as in Ng and Cardie (2002) and Bengston and Roth (2008)) which can and does cause system errors. In contrast to most recent research, our pairwise decisions are not made with a learned model which outputs a probability or confidence, but instead for each mention mi , we select an antecedent amongst m1 , . . . , mi−1 or the NULL mention as follows: • Syntactic Constraint: Based on syntactic configurations"
D09-1120,C92-2082,0,0.0115198,"nd its constrained antecedent. 10 The resulting set of compatible head words, while large, covers a little more than half of the examples given in Table 1. The problem is that these highly-reliable syntactic configurations are too sparse and cannot capture all the entity information present. For instance, the first sentence of Wikipedia abstract for Al Gore is: http://en.wikipedia.org/wiki/AOL The required lexical pattern X who served as Y is a general appositive-like pattern that almost surely indicates coreference. Rather than opt to manually create a set of these coreference patterns as in Hearst (1992), we instead opt to automatically extract these patterns from large corpora as in Snow et al. (2004) and Phillips and Riloff (2007). We take a simple bootstrapping technique: given a set of mention pairs extracted from appositives and predicate-nominative configurations, we extract counts over tree fragments between nodes which have occurred in this set of head pairs (see Figure 5); the tree fragments are formed by annotating the internal nodes in the tree path with the head word and POS along with the subcategorization. We limit the paths extracted in this way in several ways: paths are only"
D09-1120,P03-1054,1,0.0387527,"and predicted clusters using this function. We use the φ3 similarity function from Luo (2005). • ACE2004-NWIRE: ACE 2004 Newswire set to compare against Poon and Domingos (2008). Consists of 128 documents and 11,413 mentions; intersects with the other ACE data sets. • MUC-6-TEST: MUC6 formal evaluation set consisting of 30 documents and 2,068 mentions. Unlabeled: (see Section 3.2) • BLIPP: 1.8 million sentences of newswire parsed with the Charniak (2000) parser. No labeled coreference data; used for mining semantic information. • WIKI: 25k articles of English Wikipedia abstracts parsed by the Klein and Manning (2003) parser.3 No labeled coreference data; used for mining semantic information. 1 Other mention types exist and are annotated (such as prenominal), which are treated as nominals in this work. 2 The evaluation set was not made available to nonparticipants. 3 Wikipedia abstracts consist of roughly the first paragraph of the corresponding article Evaluation 3 System Description In this section we develop our system and report developmental results on ACE2004-ROTHDEV (see Section 2.1); we report pairwise F1 figures here, but report on many more evaluation metrics in Section 4. At a high level, our sy"
D09-1120,P04-1018,0,0.0814213,"! ! &quot;&quot; , !!! ! NP-APPOS#1 )( ) ))) ) ((( (( subject of the [exhibition]2 & NN NNP NNP painter Pablo Picasso Picasso (a) (b) Figure 3: NP structure annotation: In (a) we have the raw parse from the Klein and Manning (2003) parser with the mentions annotated by entity. In (b), we demonstrate the annotation we have added. NER labels are added to all NP according to the NER label given to the head (see Section 3.1.1). Appositive NPs are also annotated. Hashes indicate forced coreferent nodes guarantee coreference. The one exploited most in coreference work (Soon et al., 1999; Ng and Cardie, 2002; Luo et al., 2004; Culotta et al., 2007; Poon and Domingos, 2008; Bengston and Roth, 2008) is the appositive construction. Here, we represent apposition as a syntactic feature of an NP indicating that it is coreferent with its parent NP (e.g. it is an exception to the i-within-i constraint that parent and child NPs cannot be coreferent). We deterministically mark a node as NP-APPOS (see Figure 3) when it is the third child in of a parent NP whose expansion begins with (NP , NP), and there is not a conjunction in the expansion (to avoid marking elements in a list as appositive). Role Appositives: During develop"
D09-1120,H05-1004,0,0.747574,"and Roth (2008). The ACE data also annotates pre-nominal mentions which we map onto nominals. 68 documents and 4,536 mentions. • MUC (Vilain et al., 1995): For each true cluster, compute the number of predicted clusters which need to be merged to cover the true cluster. Divide this quantity by true cluster size minus one. Recall is given by the same procedure with predicated and true clusters reversed.4 Testing: (see Section 4) • ACE2004-CULOTTA-TEST: Test set split of the ACE 2004 training set utilized in Culotta et al. (2007) and Bengston and Roth (2008). Consists of 107 documents.2 • CEAF (Luo, 2005): For a similarity function between predicted and true clusters, CEAF scores the best match between true and predicted clusters using this function. We use the φ3 similarity function from Luo (2005). • ACE2004-NWIRE: ACE 2004 Newswire set to compare against Poon and Domingos (2008). Consists of 128 documents and 11,413 mentions; intersects with the other ACE data sets. • MUC-6-TEST: MUC6 formal evaluation set consisting of 30 documents and 2,068 mentions. Unlabeled: (see Section 3.2) • BLIPP: 1.8 million sentences of newswire parsed with the Charniak (2000) parser. No labeled coreference data;"
D09-1120,P02-1014,0,0.383334,"information. 1 Other mention types exist and are annotated (such as prenominal), which are treated as nominals in this work. 2 The evaluation set was not made available to nonparticipants. 3 Wikipedia abstracts consist of roughly the first paragraph of the corresponding article Evaluation 3 System Description In this section we develop our system and report developmental results on ACE2004-ROTHDEV (see Section 2.1); we report pairwise F1 figures here, but report on many more evaluation metrics in Section 4. At a high level, our system resembles a pairwise coreference model (Soon et al., 1999; Ng and Cardie, 2002; Bengston and Roth, 2008); for each mention mi , we select either a single-best antecedent amongst the previous mentions m1 , . . . , mi−1 , or the NULL mention to indicate the underlying entity has not yet been evoked. Mentions are linearly ordered according to the position of the mention head with ties being broken by the larger node coming first. 4 The MUC measure is problematic when the system predicts many more clusters than actually exist (Luo, 2005; Finkel and Manning, 2008); also, singleton clusters do not contribute to evaluation. 1153 While much research (Ng and Cardie, 2002; Culott"
D09-1120,D08-1067,0,0.588127,"Missing"
D09-1120,D08-1068,0,0.479898,"r. Divide this quantity by true cluster size minus one. Recall is given by the same procedure with predicated and true clusters reversed.4 Testing: (see Section 4) • ACE2004-CULOTTA-TEST: Test set split of the ACE 2004 training set utilized in Culotta et al. (2007) and Bengston and Roth (2008). Consists of 107 documents.2 • CEAF (Luo, 2005): For a similarity function between predicted and true clusters, CEAF scores the best match between true and predicted clusters using this function. We use the φ3 similarity function from Luo (2005). • ACE2004-NWIRE: ACE 2004 Newswire set to compare against Poon and Domingos (2008). Consists of 128 documents and 11,413 mentions; intersects with the other ACE data sets. • MUC-6-TEST: MUC6 formal evaluation set consisting of 30 documents and 2,068 mentions. Unlabeled: (see Section 3.2) • BLIPP: 1.8 million sentences of newswire parsed with the Charniak (2000) parser. No labeled coreference data; used for mining semantic information. • WIKI: 25k articles of English Wikipedia abstracts parsed by the Klein and Manning (2003) parser.3 No labeled coreference data; used for mining semantic information. 1 Other mention types exist and are annotated (such as prenominal), which ar"
D09-1120,M95-1005,0,0.892162,"ntersection between the predicted cluster and the true cluster for that mention. The precision is the ratio of the intersection and the true cluster sizes and recall the ratio of the intersection to the predicted sizes; F1 is given by the harmonic mean over precision and recall from all mentions. Data Sets In this work we use the following data sets: Development: (see Section 3) • ACE2004-ROTH-DEV: Dev set split of the ACE 2004 training set utilized in Bengston and Roth (2008). The ACE data also annotates pre-nominal mentions which we map onto nominals. 68 documents and 4,536 mentions. • MUC (Vilain et al., 1995): For each true cluster, compute the number of predicted clusters which need to be merged to cover the true cluster. Divide this quantity by true cluster size minus one. Recall is given by the same procedure with predicated and true clusters reversed.4 Testing: (see Section 4) • ACE2004-CULOTTA-TEST: Test set split of the ACE 2004 training set utilized in Culotta et al. (2007) and Bengston and Roth (2008). Consists of 107 documents.2 • CEAF (Luo, 2005): For a similarity function between predicted and true clusters, CEAF scores the best match between true and predicted clusters using this funct"
D09-1120,J03-4003,0,\N,Missing
D09-1120,J01-4004,0,\N,Missing
D09-1147,D08-1023,0,0.100647,"Missing"
D09-1147,W07-0403,0,0.0138536,"n the n-gram counts of each. When training with CoBLEU, we replace e with expected counts and maximize θ. In consensus decoding, we replace r with expected counts and maximize e. Several other efficient consensus decoding pro1423 5 Experiments We compared CoBLEU training with an implementation of minimum error rate training on two language pairs. 5.1 Model Our optimization procedure is in principle tractable for any syntactic translation system. For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). Phrasal ITG models have been shown to perform comparably to the state-ofthe art phrase-based system Moses (Koehn et al., 2007) when using the same phrase table (Petrov et al., 2008). We extract a phrase table using the Moses pipeline, based on Model 4 word alignments generated from GIZA++ (Och and Ney, 2003). Our final ITG grammar includes the five standard Moses features, an n-gram language model, a length feature that counts the number of target words, a feature that counts the number of monotonic ITG rewrites, and a feature that counts the number of inverted ITG rewrites. 5.2 Data We extr"
D09-1147,D08-1024,0,0.0743063,"computable over packed forests of translations generated by machine translation systems. The gradient includes expectations of products of features and n-gram counts, a quantity that has not appeared in previous work. We present a new dynamic program which allows the efficient computation of these quantities over translation forests. The resulting gradient ascent procedure does not require any k-best approximations. Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al., 2008) and large-margin training (Chiang et al., 2008). We developed CoBLEU primarily to complement consensus decoding, which it does; it produces higher BLEU scores than coupling MERT with consensus decoding. However, we found an additional empirical benefit: CoBLEU is less prone to overfitting than MERT, even when using Viterbi decoding. In experiments, models trained to maximize tuning set BLEU using MERT consistently degraded in performance from tuning to test set, while CoBLEU-trained models generalized more robustly. As a result, we found that optimizing CoBLEU improved test set performance reliably using consensus decoding and occasionally"
D09-1147,P09-1064,1,0.928192,"decoding procedures. An unexpected second benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding. 1 Introduction Increasing evidence suggests that machine translation decoders should not search for a single top scoring Viterbi derivation, but should instead choose a translation that is sensitive to the model’s entire predictive distribution. Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected counts (Tromble et al., 2008; DeNero et al., 2009; Li et al., 2009; Kumar et al., 2009). This change in decoding objective suggests a complementary change in tuning objective, to one that optimizes expected n-gram counts directly. The ubiquitous minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly boost the aggregated posterior probability of desirable n-grams (Och, 2003). We therefore propose an alternative objective function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to maximize the expected counts of the n-grams that appear in reference translations. To main"
D09-1147,P02-1001,0,0.101743,"n integrated language model decoder. These contexts are required both to correctly compute the model score of derivations and to compute clipped n-gram counts. To speed our computations, we use the cube pruning method of Huang and Chiang (2007) with a fixed beam size. For regularization, we added an L2 penalty on the size of θ to the CoBLEU objective, a simple addition for gradient ascent. We did not find that our performance varied very much for moderate levels of regularization. 3.6 Related Work The calculation of expected counts can be formulated using the expectation semiring framework of Eisner (2002), though that work does not show how to compute expected products of counts which are needed for our gradient calculations. Concurrently with this work, Li and Eisner (2009) have generalized Eisner (2002) to compute expected products of counts on translation forests. The training algorithm of Kakade et al. (2002) makes use of a dynamic program similar to ours, though specialized to the case of sequence models. 4 Consensus Decoding Once model parameters θ are learned, we must select an appropriate decoding objective. Several new decoding approaches have been proposed recently that leverage some"
D09-1147,P07-1019,0,0.0988077,"the rule, while head(h) refers to the “head” or “parent”. A forest of translations is built by combining the nodes vi using h to form a new node u = head(h). Each forest node consists of a grammar symbol and target language boundary words used to track n-grams. In the above, we keep one boundary word for each node, which allows us to track bigrams. In this section, we develop an analytical expression for the gradient of CoBLEU, then discuss how to efficiently compute the value of the objective function and gradient. 3.1 Translation Model Form We first assume the general hypergraph setting of Huang and Chiang (2007), namely, that derivations under our translation model form a hypergraph. This framework allows us to speak about both phrase-based and syntax-based translation in a unified framework. We define a probability distribution over derivations d via θ as: Z(fi ) = X Eθ [c(φk , d)|fi ] (2) Eθ [`n (d)|fi ] (3) Eθ [c(φk , d) · `n (d)|fi ] (4) P where `n (d) = gn c(gn , d) is the sum of all ngrams on derivation d (its “length”). The first expectation is an expected count of the kth feature φk over all derivations of fi . The second is an expected length, the total expected count of all ngrams in deriva"
D09-1147,P07-2045,0,0.0256167,"e replace r with expected counts and maximize e. Several other efficient consensus decoding pro1423 5 Experiments We compared CoBLEU training with an implementation of minimum error rate training on two language pairs. 5.1 Model Our optimization procedure is in principle tractable for any syntactic translation system. For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). Phrasal ITG models have been shown to perform comparably to the state-ofthe art phrase-based system Moses (Koehn et al., 2007) when using the same phrase table (Petrov et al., 2008). We extract a phrase table using the Moses pipeline, based on Model 4 word alignments generated from GIZA++ (Och and Ney, 2003). Our final ITG grammar includes the five standard Moses features, an n-gram language model, a length feature that counts the number of target words, a feature that counts the number of monotonic ITG rewrites, and a feature that counts the number of inverted ITG rewrites. 5.2 Data We extracted phrase tables from the SpanishEnglish and French-English sections of the Europarl corpus, which include approximately 8.5"
D09-1147,P09-1019,0,0.162126,"ond benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding. 1 Introduction Increasing evidence suggests that machine translation decoders should not search for a single top scoring Viterbi derivation, but should instead choose a translation that is sensitive to the model’s entire predictive distribution. Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected counts (Tromble et al., 2008; DeNero et al., 2009; Li et al., 2009; Kumar et al., 2009). This change in decoding objective suggests a complementary change in tuning objective, to one that optimizes expected n-gram counts directly. The ubiquitous minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly boost the aggregated posterior probability of desirable n-grams (Och, 2003). We therefore propose an alternative objective function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to maximize the expected counts of the n-grams that appear in reference translations. To maintain consistency across the translatio"
D09-1147,D09-1005,0,0.103725,"ed our computations, we use the cube pruning method of Huang and Chiang (2007) with a fixed beam size. For regularization, we added an L2 penalty on the size of θ to the CoBLEU objective, a simple addition for gradient ascent. We did not find that our performance varied very much for moderate levels of regularization. 3.6 Related Work The calculation of expected counts can be formulated using the expectation semiring framework of Eisner (2002), though that work does not show how to compute expected products of counts which are needed for our gradient calculations. Concurrently with this work, Li and Eisner (2009) have generalized Eisner (2002) to compute expected products of counts on translation forests. The training algorithm of Kakade et al. (2002) makes use of a dynamic program similar to ours, though specialized to the case of sequence models. 4 Consensus Decoding Once model parameters θ are learned, we must select an appropriate decoding objective. Several new decoding approaches have been proposed recently that leverage some notion of consensus over the many weighted derivations in a translation forest. In this paper, we adopt the fast consensus decoding procedure of DeNero et al. (2009), which"
D09-1147,P09-1067,0,0.362973,"An unexpected second benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding. 1 Introduction Increasing evidence suggests that machine translation decoders should not search for a single top scoring Viterbi derivation, but should instead choose a translation that is sensitive to the model’s entire predictive distribution. Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected counts (Tromble et al., 2008; DeNero et al., 2009; Li et al., 2009; Kumar et al., 2009). This change in decoding objective suggests a complementary change in tuning objective, to one that optimizes expected n-gram counts directly. The ubiquitous minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly boost the aggregated posterior probability of desirable n-grams (Och, 2003). We therefore propose an alternative objective function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to maximize the expected counts of the n-grams that appear in reference translations. To maintain consistency"
D09-1147,D08-1076,0,0.254574,"at this function and its gradient are efficiently computable over packed forests of translations generated by machine translation systems. The gradient includes expectations of products of features and n-gram counts, a quantity that has not appeared in previous work. We present a new dynamic program which allows the efficient computation of these quantities over translation forests. The resulting gradient ascent procedure does not require any k-best approximations. Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al., 2008) and large-margin training (Chiang et al., 2008). We developed CoBLEU primarily to complement consensus decoding, which it does; it produces higher BLEU scores than coupling MERT with consensus decoding. However, we found an additional empirical benefit: CoBLEU is less prone to overfitting than MERT, even when using Viterbi decoding. In experiments, models trained to maximize tuning set BLEU using MERT consistently degraded in performance from tuning to test set, while CoBLEU-trained models generalized more robustly. As a result, we found that optimizing CoBLEU improved test set performance re"
D09-1147,J03-1002,0,0.00628979,"te training on two language pairs. 5.1 Model Our optimization procedure is in principle tractable for any syntactic translation system. For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). Phrasal ITG models have been shown to perform comparably to the state-ofthe art phrase-based system Moses (Koehn et al., 2007) when using the same phrase table (Petrov et al., 2008). We extract a phrase table using the Moses pipeline, based on Model 4 word alignments generated from GIZA++ (Och and Ney, 2003). Our final ITG grammar includes the five standard Moses features, an n-gram language model, a length feature that counts the number of target words, a feature that counts the number of monotonic ITG rewrites, and a feature that counts the number of inverted ITG rewrites. 5.2 Data We extracted phrase tables from the SpanishEnglish and French-English sections of the Europarl corpus, which include approximately 8.5 million words of bitext for each of the language pairs (Koehn, 2002). We used a trigram language model trained on the entire corpus of English parliamentary proceedings provided with"
D09-1147,P03-1021,0,0.135668,"predictive distribution. Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected counts (Tromble et al., 2008; DeNero et al., 2009; Li et al., 2009; Kumar et al., 2009). This change in decoding objective suggests a complementary change in tuning objective, to one that optimizes expected n-gram counts directly. The ubiquitous minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly boost the aggregated posterior probability of desirable n-grams (Och, 2003). We therefore propose an alternative objective function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to maximize the expected counts of the n-grams that appear in reference translations. To maintain consistency across the translation pipeline, we formulate CoBLEU to share the functional form of BLEU used for evaluation. As a result, CoBLEU optimizes exactly the quantities that drive efficient consensus decoding techniques and precisely mirrors the objective used for fast consensus decoding in DeNero et al. (2009). CoBLEU is a continuous and (mostly) different"
D09-1147,P02-1040,0,0.0861394,"unts us3 -12 ing CoBP(R, F, θ): g min{Eθ [c(g, d)|f ], c(g, r)} &quot; g Eθ [c(g, d)|f ] Figure 1: (a) A simple hypothesis space of translations for a single sentence containing three alternatives, each with two features. The hypotheses are scored under a log-linear model with parameters θ equal to the identity vector. (b) The expected counts of all bigrams that appear in the computation of consensus bigram precision. 2 Model: TM + !LM • LM g = Pm P i=1 where Eθ [c(g2 , d)|fi ] = Our proposed objective function maximizes ngram precision by adapting the BLEU evaluation metric as a tuning objective (Papineni et al., 2002). To simplify exposition, we begin by adapting a simpler metric: bigram precision. Bigram Precision Tuning Let the tuning corpus consist of source sentences F = f1 . . . fm and human-generated references R = r1 . . . rm , one reference for each source sentence. Let ei be a translation of fi , and let E = e1 . . . em be a corpus of translations, one for each source sentence. A simple evaluation score for E is its bigram precision BP(R, E): Pm P BP(R, E) = i=1 min{E c(g2 , ri )}2 2 , d)|fi ],!LM θ [c(g 0 Parameter: Pm P (1) E [c(g , d)|f ] 2 i θ i=1 g 2 (b) Objectives as functions of ! LM Consen"
D09-1147,D08-1012,1,0.873057,"Missing"
D09-1147,P06-2101,0,0.165268,"in Table 1. In Spanish-English, CoBLEU slightly outperformed MERT under the same initialization, while the opposite pattern appears for French-English. The best test set performance in both language pairs was the third condition, in which CoBLEU training was initialized with MERT. This condition also gave the highest CoBLEU objective value. This pattern indicates that CoBLEU is a useful objective for translation with consensus decoding, but that the gradient ascent optimization is getting stuck in local maxima during tuning. This issue can likely be addressed with annealing, as described in (Smith and Eisner, 2006). Interestingly, the brevity penatly results in French indicate that, even though CoBLEU did Spanish Test 30.2 30.9 French Tune Test 32.0 31.0 31.7 30.9 Tune 32.5 30.5 ∆ -2.3 +0.4 ∆ -1.0 -0.8 Table 2: Performance measured by BLEU using Viterbi decoding indicates that CoBLEU is less prone to overfitting than MERT. not outperform MERT in a statistically significant way, CoBLEU tends to find shorter sentences with higher n-gram precision than MERT. Table 1 displays a second benefit of CoBLEU training: compared to MERT training, CoBLEU performance degrades less from tuning to test set. In Spanish,"
D09-1147,D08-1065,0,0.119162,"downstream consensus decoding procedures. An unexpected second benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding. 1 Introduction Increasing evidence suggests that machine translation decoders should not search for a single top scoring Viterbi derivation, but should instead choose a translation that is sensitive to the model’s entire predictive distribution. Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected counts (Tromble et al., 2008; DeNero et al., 2009; Li et al., 2009; Kumar et al., 2009). This change in decoding objective suggests a complementary change in tuning objective, to one that optimizes expected n-gram counts directly. The ubiquitous minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly boost the aggregated posterior probability of desirable n-grams (Och, 2003). We therefore propose an alternative objective function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to maximize the expected counts of the n-grams that appear in reference"
D09-1147,J97-3002,0,0.0373861,"mpute the similarity of a sentence e to a reference r based on the n-gram counts of each. When training with CoBLEU, we replace e with expected counts and maximize θ. In consensus decoding, we replace r with expected counts and maximize e. Several other efficient consensus decoding pro1423 5 Experiments We compared CoBLEU training with an implementation of minimum error rate training on two language pairs. 5.1 Model Our optimization procedure is in principle tractable for any syntactic translation system. For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). Phrasal ITG models have been shown to perform comparably to the state-ofthe art phrase-based system Moses (Koehn et al., 2007) when using the same phrase table (Petrov et al., 2008). We extract a phrase table using the Moses pipeline, based on Model 4 word alignments generated from GIZA++ (Och and Ney, 2003). Our final ITG grammar includes the five standard Moses features, an n-gram language model, a length feature that counts the number of target words, a feature that counts the number of monotonic ITG rewrites, and a"
D09-1147,J07-2003,0,\N,Missing
D10-1040,P09-1010,0,0.0379509,"ing, pages 410–419, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics fall out naturally as consequences of the model. An effective way to empirically explore the pragmatic aspects of language is to work in the grounded setting, where the basic idea is to map language to some representation of the non-linguistic world (Yu and Ballard, 2004; Feldman and Narayanan, 2004; Fleischman and Roy, 2007; Chen and Mooney, 2008; Frank et al., 2009; Liang et al., 2009). Along similar lines, past work has also focused on interpreting natural language instructions (Branavan et al., 2009; Eisenstein et al., 2009; Kollar et al., 2010), which takes into account the goal of the communication. This work differs from ours in that it does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between multiple agents that takes into account contextual ambiguities. We present our pragmatic model in a grounded setting where a speaker must describe a target object to a listener via spatial description (such as i"
D10-1040,D09-1100,0,0.0224414,"IT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics fall out naturally as consequences of the model. An effective way to empirically explore the pragmatic aspects of language is to work in the grounded setting, where the basic idea is to map language to some representation of the non-linguistic world (Yu and Ballard, 2004; Feldman and Narayanan, 2004; Fleischman and Roy, 2007; Chen and Mooney, 2008; Frank et al., 2009; Liang et al., 2009). Along similar lines, past work has also focused on interpreting natural language instructions (Branavan et al., 2009; Eisenstein et al., 2009; Kollar et al., 2010), which takes into account the goal of the communication. This work differs from ours in that it does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between multiple agents that takes into account contextual ambiguities. We present our pragmatic model in a grounded setting where a speaker must describe a target object to a listener via spatial description (such as in the example given above"
D10-1040,P09-1011,1,0.755924,"We show that Gricean maxims 410 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 410–419, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics fall out naturally as consequences of the model. An effective way to empirically explore the pragmatic aspects of language is to work in the grounded setting, where the basic idea is to map language to some representation of the non-linguistic world (Yu and Ballard, 2004; Feldman and Narayanan, 2004; Fleischman and Roy, 2007; Chen and Mooney, 2008; Frank et al., 2009; Liang et al., 2009). Along similar lines, past work has also focused on interpreting natural language instructions (Branavan et al., 2009; Eisenstein et al., 2009; Kollar et al., 2010), which takes into account the goal of the communication. This work differs from ours in that it does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between multiple agents that takes into account contextual ambiguities. We present our pragmatic mode"
D10-1040,P07-1121,0,0.0107279,"ight choose one of the following two utterances: (a) right of O2 (b) on O3 Although both utterances are semantically correct, (a) is ambiguous between O1 and O3, whereas (b) unambiguously identifies O1 as the target object, and should therefore be preferred over (a). In this paper, we present a game-theoretic model that captures this communication-oriented aspect of language interpretation and generation. Successful communication can be broken down into semantics and pragmatics. Most computational work on interpreting language focuses on compositional semantics (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Piantadosi et al., 2008), which is concerned with verifying the truth of a sentence. However, what is missing from this truthoriented view is the pragmatic aspect of language— that language is used to accomplish an end goal, as exemplified by speech acts (Austin, 1962). Indeed, although both utterances (a) and (b) are semantically valid, only (b) is pragmatically felicitous: (a) is ambiguous and therefore violates the Gricean maxim of manner (Grice, 1975). To capture this maxim, we develop a model of pragmatics based on game theory, in the spirit of J¨ager (2008) but extended to the stochast"
D10-1049,N04-1015,0,0.069203,"weather forecasts, obtaining results comparable to state-ofthe-art domain-specific systems both in terms of BLEU scores and human evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. Dan Klein UC Berkeley Berkeley, CA 94720 klein@cs.berkeley.edu We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those"
D10-1049,P06-1130,0,0.0327956,"Missing"
D10-1049,W04-0601,0,0.0188729,"ning results comparable to state-ofthe-art domain-specific systems both in terms of BLEU scores and human evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. Dan Klein UC Berkeley Berkeley, CA 94720 klein@cs.berkeley.edu We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those records (output). We use"
D10-1049,W06-1417,0,0.0965803,"decisions, arranged hierarchically and each trained discriminatively. We deployed our system in three different domains—Robocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-ofthe-art domain-specific systems both in terms of BLEU scores and human evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. Dan Klein UC Berkeley Berkeley, CA 94720 kle"
D10-1049,P02-1003,0,0.0172526,"(White et al., 2007), HPSG (Nakanishi et al., 2005), and LFG (Cahill and van Genabith, 2006). In each of the above cases, the decomposable structure of the tree/grammar enables tractability. However, we saw that it was important to include features that captured long-range dependencies. Our model is also similar in spirit to Ratnaparkhi (2002) in the use of non-local features, but we operate at three levels of hierarchy to include both content selection and surface realization. One issue that arises with long-range dependencies is the lack of efficient algorithms for finding the optimal text. Koller and Striegnitz (2002) perform surface realization of a flat semantics, which is NPhard, so they recast the problem as non-projective dependency parsing. Ratnaparkhi (2002) uses beam search to find an approximate solution. We found that a greedy approach obtained better results than beam search; Belz (2008) found greedy approaches to be effective as well. 7 Conclusion We have developed a simple yet powerful generation system that combines both content selection and surface realization in a domain independent way. Despite our approach being domain-independent, we were able to obtain performance comparable to the sta"
D10-1049,P09-1011,1,0.0757203,"and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. Dan Klein UC Berkeley Berkeley, CA 94720 klein@cs.berkeley.edu We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those records (output). We use the model of Liang et al. (2009) to automatically induce the correspondences between words in the text and the actual database records mentioned. We break up the full generation process into a sequence of local decisions, training a log-linear classifier for each type of decision. We use a simple but expressive set of domain-independent features, where each decision is allowed to depend on the entire history of previous decisions, as in the model of Ratnaparkhi (2002). These long-range contextual dependencies turn out to be critical for accurate generation. More specifically, our model is defined in terms of three types of d"
D10-1049,D09-1042,0,0.0478593,"e focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. Dan Klein UC Berkeley Berkeley, CA 94720 klein@cs.berkeley.edu We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those records (output). We use the model of Liang et al. (2009) to automatically induce the correspondences between words in the text and the actual database records mentio"
D10-1049,W05-1510,0,0.0559178,"form, represented as a tree, into a parse of the text. Soricut and Marcu (2006) uses tree structures called WIDLexpressions (the acronym corresponds to four operations akin to the rewrite rules of a grammar) to represent the realization process, and, like our approach, operates in a log-linear framework. Belz (2008) and Belz and Kow (2009) also perform surface realization from a PCFG-like grammar. Lu et al. (2009) 511 uses a conditional random field model over trees. Other authors have performed surface realization using various grammar formalisms, for instance CCG (White et al., 2007), HPSG (Nakanishi et al., 2005), and LFG (Cahill and van Genabith, 2006). In each of the above cases, the decomposable structure of the tree/grammar enables tractability. However, we saw that it was important to include features that captured long-range dependencies. Our model is also similar in spirit to Ratnaparkhi (2002) in the use of non-local features, but we operate at three levels of hierarchy to include both content selection and surface realization. One issue that arises with long-range dependencies is the lack of efficient algorithms for finding the optimal text. Koller and Striegnitz (2002) perform surface realiz"
D10-1049,P02-1040,0,0.0895557,"T IME, fields that span numbers and wind directions; and for ROBOCUP, fields that span words starting with purple or pink. For each record ri , we define Ti so that BASE(Ti ) and C OARSE(Ti ) are the corresponding two extracted templates. We restrict Fi to the set of abstracted fields in the C OARSE template 5 Experiments We now present an empirical evaluation of our system on our three domains—ROBOCUP, S UM T IME, and W EATHER G OV. 5.1 Evaluation Metrics Automatic Evaluation To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al., 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output. To evaluate macro content selection, we measured the F1 score (the harmonic mean of precision and recall) of the set of records chosen with respect to the human-annotated set of records. Human Evaluation We conducted a human evaluation using Amazon Mechanical Turk. For each domain, we chose 100 scenarios randomly from the test set. We ran each system under consideration on each of these scenarios, and presented each resulting output to 10 evaluators.2 Evaluators were gi"
D10-1049,P06-1139,0,0.119784,"troduces a generative model of the text given the world state, and in some ways is similar in spirit to our model. Although that model is capable of generation in principle, it was designed for unsupervised induction of hidden alignments (which is exactly what we use it for). Even if combined with a language model, generated text was much worse than our baseline. The prominent approach for surface realization is rendering the text from a grammar. Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the text. Soricut and Marcu (2006) uses tree structures called WIDLexpressions (the acronym corresponds to four operations akin to the rewrite rules of a grammar) to represent the realization process, and, like our approach, operates in a log-linear framework. Belz (2008) and Belz and Kow (2009) also perform surface realization from a PCFG-like grammar. Lu et al. (2009) 511 uses a conditional random field model over trees. Other authors have performed surface realization using various grammar formalisms, for instance CCG (White et al., 2007), HPSG (Nakanishi et al., 2005), and LFG (Cahill and van Genabith, 2006). In each of th"
D10-1049,W09-0607,0,0.0541974,"anged hierarchically and each trained discriminatively. We deployed our system in three different domains—Robocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-ofthe-art domain-specific systems both in terms of BLEU scores and human evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. Dan Klein UC Berkeley Berkeley, CA 94720 klein@cs.berkeley.edu We"
D10-1049,2007.mtsummit-ucnlg.4,0,0.0747759,"rammars that map a logical form, represented as a tree, into a parse of the text. Soricut and Marcu (2006) uses tree structures called WIDLexpressions (the acronym corresponds to four operations akin to the rewrite rules of a grammar) to represent the realization process, and, like our approach, operates in a log-linear framework. Belz (2008) and Belz and Kow (2009) also perform surface realization from a PCFG-like grammar. Lu et al. (2009) 511 uses a conditional random field model over trees. Other authors have performed surface realization using various grammar formalisms, for instance CCG (White et al., 2007), HPSG (Nakanishi et al., 2005), and LFG (Cahill and van Genabith, 2006). In each of the above cases, the decomposable structure of the tree/grammar enables tractability. However, we saw that it was important to include features that captured long-range dependencies. Our model is also similar in spirit to Ratnaparkhi (2002) in the use of non-local features, but we operate at three levels of hierarchy to include both content selection and surface realization. One issue that arises with long-range dependencies is the lack of efficient algorithms for finding the optimal text. Koller and Striegnit"
D10-1049,P07-1121,0,0.0310771,"uman evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. Dan Klein UC Berkeley Berkeley, CA 94720 klein@cs.berkeley.edu We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those records (output). We use the model of Liang et al. (2009) to automatically induce the correspondences between words in the t"
D10-1049,W09-0603,0,\N,Missing
D11-1029,D07-1093,1,0.897144,"Missing"
D11-1029,N09-1008,1,0.895478,"Missing"
D11-1029,P08-1088,1,0.95558,"identification problems. The objective simultaneously scores a matching between two alphabets and a matching between two lexicons, each in a different language. We introduce a simple coordinate descent procedure that efficiently finds effective solutions to the resulting combinatorial optimization problem. Our system requires only a list of words in both languages as input, yet it competes with and surpasses several state-of-the-art systems that are both substantially more complex and make use of more information. 1 Introduction 2011), and bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008). We consider a common element, which is a model wherein there are character-level correspondences and word-level correspondences, with the word matching parameterized by the character one. This approach subsumes a range of past tasks, though of course past work has specialized in interesting ways. Past work has emphasized the modeling aspect, where here we use a parametrically simplistic model, but instead emphasize inference. 2 Decipherment as Two-Level Optimization Our method represents two matchings, one at the alphabet level and one at the lexicon level. A vector of variables x specifies"
D11-1029,P10-1105,1,0.948406,"ges, we attempt to induce the correspondence between alphabets and identify the cognates pairs present in the lexicons. The system we propose accomplishes this by defining a simple combinatorial optimization problem that is a function of both the alphabet and cognate matchings, and then induces correspondences by optimizing the objective using a block coordinate descent procedure. There is a range of past work that has variously investigated cognate detection (Kondrak, 2001; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e 1 1 In practice we set  = lu +l . lu + lv is the maximum v et al., 2009; Hall and Klein, 2010), character-level number of edit operations between words u and v. This nordecipherment (Knight and Yamada, 1999; Knight malization insures that edit distances are between 0 and 1 for et al., 2006; Snyder et al., 2010; Ravi and Knight, all pairs of words. 313 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 313–321, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics free. Now, the objective P P that we will minimize can be stated simply: u v yuv · E DIT D IST(u, v; x), the sum of the edit distances between th"
D11-1029,W99-0906,0,0.448585,"lexicons. The system we propose accomplishes this by defining a simple combinatorial optimization problem that is a function of both the alphabet and cognate matchings, and then induces correspondences by optimizing the objective using a block coordinate descent procedure. There is a range of past work that has variously investigated cognate detection (Kondrak, 2001; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e 1 1 In practice we set  = lu +l . lu + lv is the maximum v et al., 2009; Hall and Klein, 2010), character-level number of edit operations between words u and v. This nordecipherment (Knight and Yamada, 1999; Knight malization insures that edit distances are between 0 and 1 for et al., 2006; Snyder et al., 2010; Ravi and Knight, all pairs of words. 313 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 313–321, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics free. Now, the objective P P that we will minimize can be stated simply: u v yuv · E DIT D IST(u, v; x), the sum of the edit distances between the matched words, where the edit distance function is parameterized by the alphabet matching. Without restriction"
D11-1029,P06-2065,0,0.413488,"Missing"
D11-1029,W02-0902,0,0.132245,"herment and cognate pair identification problems. The objective simultaneously scores a matching between two alphabets and a matching between two lexicons, each in a different language. We introduce a simple coordinate descent procedure that efficiently finds effective solutions to the resulting combinatorial optimization problem. Our system requires only a list of words in both languages as input, yet it competes with and surpasses several state-of-the-art systems that are both substantially more complex and make use of more information. 1 Introduction 2011), and bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008). We consider a common element, which is a model wherein there are character-level correspondences and word-level correspondences, with the word matching parameterized by the character one. This approach subsumes a range of past tasks, though of course past work has specialized in interesting ways. Past work has emphasized the modeling aspect, where here we use a parametrically simplistic model, but instead emphasize inference. 2 Decipherment as Two-Level Optimization Our method represents two matchings, one at the alphabet level and one at the lexicon level. A vector o"
D11-1029,2005.mtsummit-papers.11,0,0.0586318,"Missing"
D11-1029,N01-1014,0,0.0282772,"spondence between the alphabets of the two languages exists, but is unknown. Given only two lists of words, the lexicons of both languages, we attempt to induce the correspondence between alphabets and identify the cognates pairs present in the lexicons. The system we propose accomplishes this by defining a simple combinatorial optimization problem that is a function of both the alphabet and cognate matchings, and then induces correspondences by optimizing the objective using a block coordinate descent procedure. There is a range of past work that has variously investigated cognate detection (Kondrak, 2001; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e 1 1 In practice we set  = lu +l . lu + lv is the maximum v et al., 2009; Hall and Klein, 2010), character-level number of edit operations between words u and v. This nordecipherment (Knight and Yamada, 1999; Knight malization insures that edit distances are between 0 and 1 for et al., 2006; Snyder et al., 2010; Ravi and Knight, all pairs of words. 313 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 313–321, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics fr"
D11-1029,P11-1025,0,0.0913072,"Missing"
D11-1029,P10-1107,0,0.818257,"t is a function of both the alphabet and cognate matchings, and then induces correspondences by optimizing the objective using a block coordinate descent procedure. There is a range of past work that has variously investigated cognate detection (Kondrak, 2001; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e 1 1 In practice we set  = lu +l . lu + lv is the maximum v et al., 2009; Hall and Klein, 2010), character-level number of edit operations between words u and v. This nordecipherment (Knight and Yamada, 1999; Knight malization insures that edit distances are between 0 and 1 for et al., 2006; Snyder et al., 2010; Ravi and Knight, all pairs of words. 313 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 313–321, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics free. Now, the objective P P that we will minimize can be stated simply: u v yuv · E DIT D IST(u, v; x), the sum of the edit distances between the matched words, where the edit distance function is parameterized by the alphabet matching. Without restrictions on the matchings x and y this objective can always be driven to zero by either mapping all characters t"
D11-1032,D07-1093,1,0.808353,"Missing"
D11-1032,N09-1008,1,0.79244,"Missing"
D11-1032,P07-1009,0,0.0718107,"Missing"
D11-1032,N09-1067,0,0.0355873,"Missing"
D11-1032,D09-1011,0,0.0835337,"anguage model. We describe the parameters and procedures for these operations in 7.1. 6.2 Scale Even though inference by message-passing in our model is tractable, we needed to make certain concessions to make inference acceptably fast. These choices mainly affect how we represent distributions over strings. First, we need to model distributions and messages over words on the internal nodes of a phylogeny. The natural choice in this scenario is to use weighted finite automata (Mohri et al., 1996). Automata have been used to successfully model distributions of strings for inferring morphology (Dreyer and Eisner, 2009) as well as cognate detection (Hall and Klein, 2010). Even in models that would be tractable with “ordinary” messages, inference with automata quickly becomes intractable, because the size of the automata grow exponentially with the number of messages passed. Therefore, approximations must be used. Dreyer and Eisner (2009) used a mixture of a k-best list and a unigram language model, while Hall and Klein (2010) used an approximation procedure that projected complex automata to simple, tractable automata using a modified KL divergence. While either approach could be used here in principle, we f"
D11-1032,N01-1014,0,0.306426,"e a cognate in each group, each language in the tree has an associated “survival” variable Sg,` , where a word may be lost on that branch (and its descendants) instead of evolving. Once the words are generated, they are then “permuted” so that the cognacy relationships 1 Both of these models therefore are insensitive to geographic and historical factors that cannot be easily approximated by this tree. See Nichols (1992) for an excellent discussion of these factors. 2 One could easily envision allowing the meaning of a word to change as well. Modeling this semantic drift has been considered by Kondrak (2001). In the ABVD, however, any semantic drift has already been elided, since the database has coarsened glosses to the extent that there is no meaningful way to model semantic drift given our data. 346 φ φ G wpt wes WPaiWPaiwPai wPaiwPaiwPaiwPaiwPai π L IAta WFor (b) HK10 φ Evolution In this section we describe two models, one based on Hall and Klein (2010)—which we call HK10—and another new model that shares some connection to parsimony methods in computational biology, which we call PARSIM. Both are generative models that describe the evolution of words w` from a set of languages {`} in a cogna"
D11-1032,J94-3004,0,0.118359,"eveloped for large-scale reconstruction of phylogenies (Ringe et al., 2002; Daum´e III and Campbell, 2007; Daum´e III, 2009; Nerbonne, 2010) and ancestral word forms of known cognate sets (Oakes, 2000; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2009), enabling linguists to explore deep historical relationships in an automated fashion. However, computational approaches thus far have not been able to offer the same kind of scale for identifying cognates. Previous work in cognate identification has largely focused on identifying cognates in pairs of languages (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 344 2001; Mulloni, 2007), with a few recent exceptions that can find sets in a handful of languages (Bergsma and Kondrak, 2007; Hall and Klein, 2010). While it may seem surprising that cognate detection has not successfully scaled to large numbers of languages, the task poses challenges not seen in reconstruction and phylogeny inference. For instance, morphological innovations and irregular sound changes can completely obscure relationships between words in different languages. However, in the case of reconstruction, an unexplainable word is simply that: one can still c"
D11-1032,N01-1020,0,0.154035,"al approaches have been developed for large-scale reconstruction of phylogenies (Ringe et al., 2002; Daum´e III and Campbell, 2007; Daum´e III, 2009; Nerbonne, 2010) and ancestral word forms of known cognate sets (Oakes, 2000; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2009), enabling linguists to explore deep historical relationships in an automated fashion. However, computational approaches thus far have not been able to offer the same kind of scale for identifying cognates. Previous work in cognate identification has largely focused on identifying cognates in pairs of languages (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 344 2001; Mulloni, 2007), with a few recent exceptions that can find sets in a handful of languages (Bergsma and Kondrak, 2007; Hall and Klein, 2010). While it may seem surprising that cognate detection has not successfully scaled to large numbers of languages, the task poses challenges not seen in reconstruction and phylogeny inference. For instance, morphological innovations and irregular sound changes can completely obscure relationships between words in different languages. However, in the case of reconstruction, an unexplainable word is sim"
D11-1032,P07-3005,0,0.376643,"Missing"
D11-1032,P10-1105,1,\N,Missing
D12-1001,J93-2004,0,\N,Missing
D12-1001,N12-1052,0,\N,Missing
D12-1001,D10-1120,0,\N,Missing
D12-1001,D09-1086,0,\N,Missing
D12-1001,W06-2920,0,\N,Missing
D12-1001,N09-1009,0,\N,Missing
D12-1001,J03-4003,0,\N,Missing
D12-1001,J08-4003,0,\N,Missing
D12-1001,P10-1131,1,\N,Missing
D12-1001,P05-1012,0,\N,Missing
D12-1001,P11-1070,1,\N,Missing
D12-1001,P08-1068,0,\N,Missing
D12-1001,P11-1156,0,\N,Missing
D12-1001,P06-1111,1,\N,Missing
D12-1001,P06-1055,1,\N,Missing
D12-1001,P11-1061,0,\N,Missing
D12-1001,petrov-etal-2012-universal,0,\N,Missing
D12-1001,P09-1042,0,\N,Missing
D12-1001,D11-1006,0,\N,Missing
D12-1001,P04-1061,1,\N,Missing
D12-1001,D11-1005,0,\N,Missing
D12-1001,2005.mtsummit-papers.11,0,\N,Missing
D12-1001,D07-1096,0,\N,Missing
D12-1001,N06-1014,1,\N,Missing
D12-1079,W02-2002,0,0.0724364,"Missing"
D12-1079,C94-2195,0,0.136715,"Missing"
D12-1079,H92-1022,0,0.210735,"Missing"
D12-1079,P93-1035,0,0.256362,"Missing"
D12-1079,J95-4004,0,0.719511,"ail constraints on rule extraction that may not be borne out by semantic alignments. To the extent that there are simply divergences in the syntactic structure of the two languages, it will often be impossible to construct syntax trees that are simultaneously in full agreement with monolingual linguistic theories and with the alignments between sentences in both languages. To see this, consider the English tree in Figure 1a, taken from the English side of the English Chinese Translation Treebank (Bies et al., 2007). The In this work, we develop a method based on transformation-based learning (Brill, 1995) for automatically acquiring a sequence of tree transformations of the sort in Figure 1. Once the transformation sequence has been learned, it can be deterministically applied to any parsed sentences, yielding new parse trees with constituency structures that agree better with the bilingual alignments yet remain consistent across the corpus. In particular, we use this method to learn a transformation sequence for the English trees in a set of English to Chinese MT training data. In experiments with a string-to-tree translation system, we show resulting improvements of up to 0.9 BLEU. A great d"
D12-1079,N10-1015,1,0.878466,"train a rule extraction system to transform the subtrees that make up individual translation rules using a manually constructed set of transformations similar to those learned by our system. Instead of modifying the MT system to work around the input annotations, our system modifies the input itself in order to improve downstream translation. Most systems of this sort learn how to modify word alignments to agree better with the syntactic parse trees (DeNero and Klein, 2007; Fossum et al., 2008), but there has also been other work directly related to improving agreement by modifying the trees. Burkett et al. (2010) train a bilingual parsing model that uses bilingual agreement features to improve parsing accuracy. More closely related to the present work, Katz-Brown et al. (2011) retrain a parser to directly optimize a word reordering metric in order to improve a downstream machine translation system that uses dependency parses in a preprocessing reordering step. Our system is in the same basic spirit, using a proxy evaluation metric (agreement with alignments; see Section 2 for details) to improve performance on a downstream translation 864 task. However, we are concerned more generally with the goal of"
D12-1079,P10-1146,0,0.0345079,"一 步 是 S TO+VB VP TO VB ADVP The first step is to select team members 挑 第一 (a) Before 步 是 挑 (b) After Figure 1: An example tree transformation merging a VB node with the TO sibling of its parent VP. Before the transformation (a), the bolded VP cannot be extracted as a translation rule, but afterwards (b), both this VP and the newly created TO+VB node are extractable. for learning translation rules (Mi and Huang, 2008; Zhang et al., 2009), or by learning rules that encode syntactic information but do not strictly adhere to constituency boundaries (Zollmann et al., 2006; Marton and Resnik, 2008; Chiang, 2010). The most closely related MT system is that of Zhao et al. (2011), who train a rule extraction system to transform the subtrees that make up individual translation rules using a manually constructed set of transformations similar to those learned by our system. Instead of modifying the MT system to work around the input annotations, our system modifies the input itself in order to improve downstream translation. Most systems of this sort learn how to modify word alignments to agree better with the syntactic parse trees (DeNero and Klein, 2007; Fossum et al., 2008), but there has also been oth"
D12-1079,P11-2031,0,0.0204161,"1000 2000 3000 4000 Number of Transformations 5000 Figure 10: Transformation results on a subset of the MT training data. The training and test sets are disjoint in order to measure how well the learned transformation sequence generalizes. Once again, we plot the average improvement over the baseline trees. Though 5151 transformations were learned from the training set, the maximum test set agreement was achieved at 630 transformations, with an average improvement of 2.60. 642 sentence pairs from the NIST MT04 and MT05 data sets, using the BLEU metric (Papineni et al., 2001). As discussed by Clark et al. (2011), the optimizer included with Moses (MERT, Och, 2003) is not always particularly stable, and results (even on the tuning set) can vary dramatically across tuning runs. To mitigate this effect, we first used the Moses training scripts to extract a table of translation rules for each set of English trees. Then, for each rule table, we ran MERT 11 times and selected the parameters that achieved the maximum tuning BLEU to use for decoding the test set. Table 3 shows the results of our translation experiments. The best translation results are achieved by using the first 139 transformations, giving"
D12-1079,P07-1003,1,0.845127,"boundaries (Zollmann et al., 2006; Marton and Resnik, 2008; Chiang, 2010). The most closely related MT system is that of Zhao et al. (2011), who train a rule extraction system to transform the subtrees that make up individual translation rules using a manually constructed set of transformations similar to those learned by our system. Instead of modifying the MT system to work around the input annotations, our system modifies the input itself in order to improve downstream translation. Most systems of this sort learn how to modify word alignments to agree better with the syntactic parse trees (DeNero and Klein, 2007; Fossum et al., 2008), but there has also been other work directly related to improving agreement by modifying the trees. Burkett et al. (2010) train a bilingual parsing model that uses bilingual agreement features to improve parsing accuracy. More closely related to the present work, Katz-Brown et al. (2011) retrain a parser to directly optimize a word reordering metric in order to improve a downstream machine translation system that uses dependency parses in a preprocessing reordering step. Our system is in the same basic spirit, using a proxy evaluation metric (agreement with alignments; s"
D12-1079,P03-2041,0,0.214289,"constituency structures that agree better with the bilingual alignments yet remain consistent across the corpus. In particular, we use this method to learn a transformation sequence for the English trees in a set of English to Chinese MT training data. In experiments with a string-to-tree translation system, we show resulting improvements of up to 0.9 BLEU. A great deal of research in syntactic machine translation has been devoted to handling the inherent syntactic divergence between source and target languages. Some systems attempt to model the differences directly (Yamada and Knight, 2001; Eisner, 2003), but most recent work focuses on reducing the sensitivity of the rule-extraction procedure to the constituency decisions made by 1-best syntactic parsers, either by using forest-based methods 863 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 863–872, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics S S NP VP VBZ NP S VP VBZ VP VP TO VP VB ADVP The first step is to select team members 第一 步 是 S TO+VB VP TO VB ADVP The first step is to select team members 挑 第一 (a"
D12-1079,W08-0306,0,0.0520607,"al., 2006; Marton and Resnik, 2008; Chiang, 2010). The most closely related MT system is that of Zhao et al. (2011), who train a rule extraction system to transform the subtrees that make up individual translation rules using a manually constructed set of transformations similar to those learned by our system. Instead of modifying the MT system to work around the input annotations, our system modifies the input itself in order to improve downstream translation. Most systems of this sort learn how to modify word alignments to agree better with the syntactic parse trees (DeNero and Klein, 2007; Fossum et al., 2008), but there has also been other work directly related to improving agreement by modifying the trees. Burkett et al. (2010) train a bilingual parsing model that uses bilingual agreement features to improve parsing accuracy. More closely related to the present work, Katz-Brown et al. (2011) retrain a parser to directly optimize a word reordering metric in order to improve a downstream machine translation system that uses dependency parses in a preprocessing reordering step. Our system is in the same basic spirit, using a proxy evaluation metric (agreement with alignments; see Section 2 for detai"
D12-1079,N04-1035,0,0.425537,"Missing"
D12-1079,2006.amta-papers.8,0,0.196888,"Missing"
D12-1079,D11-1017,0,0.0177336,"ose learned by our system. Instead of modifying the MT system to work around the input annotations, our system modifies the input itself in order to improve downstream translation. Most systems of this sort learn how to modify word alignments to agree better with the syntactic parse trees (DeNero and Klein, 2007; Fossum et al., 2008), but there has also been other work directly related to improving agreement by modifying the trees. Burkett et al. (2010) train a bilingual parsing model that uses bilingual agreement features to improve parsing accuracy. More closely related to the present work, Katz-Brown et al. (2011) retrain a parser to directly optimize a word reordering metric in order to improve a downstream machine translation system that uses dependency parses in a preprocessing reordering step. Our system is in the same basic spirit, using a proxy evaluation metric (agreement with alignments; see Section 2 for details) to improve performance on a downstream translation 864 task. However, we are concerned more generally with the goal of creating trees that are more compatible with a wide range of syntactically-informed translation systems, particularly those that extract translation rules based on sy"
D12-1079,P07-2045,0,0.00522795,"ata, analogous to Figure 8. The same general patterns hold, although we do see that the automatically annotated data is more idiosyncratic and so more than twice as many transformations are learned before training set agreement stops improving, even though the training set sizes are roughly the same.8 Furthermore, test set generalization in the automatic annotation setting is a little bit worse, with later transformations tending to actually hurt test set agreement. For our machine translation experiments, we used the string-to-tree syntactic pipeline included in the current version of Moses (Koehn et al., 2007). Our training bitext was approximately 21.8 million words, and the sentences and word alignments were the same for all experiments; the only difference between each experiment was the English trees, for which we tested a range of transformation sequence prefixes (including a 0-length prefix, which just yields the original trees, as a baseline). Since the transformed trees tended to be more finely articulated, and increasing the number of unique spans often helps with rule extraction (Wang et al., 2007), we equalized the span count by also testing binarized versions of each set of trees, using"
D12-1079,N06-1014,1,0.740853,"tuned for the same annotators (parsers and word aligners) we are evaluating with. In particular, we found that though training on the English Chinese Translation Treebank produces clean, interpretable rules, preliminary experiments showed little to no improvement from using these rules for MT, primarily because actual alignments are not only noisier but also systematically different from gold ones. Thus, all rules used for MT experiments were learned from automatically annotated text. For our Chinese to English translation experiments, we generated word alignments using the Berkeley Aligner (Liang et al., 2006) with default settings. We used an MT pipeline that conditions on target-side syntax, so our initial state annotator was the Berkeley Parser (Petrov and Klein, 2007), trained on a modified English treebank that has been adapted to match standard MT tokenization and capitalization schemes. As mentioned in Section 5, we could, in principle train on all 500k sentences of our MT training data. However, this would be quite slow: each iteration of the training procedure requires iterating through all n training sentences6 once for each of the m candidate transformations, for a total cost of O(nm) wh"
D12-1079,J93-2004,0,0.0454772,"rees and word alignments in bilingual corpora. Using the manually annotated English Chinese Translation Treebank, we show how our method automatically discovers transformations that accommodate differences in English and Chinese syntax. Furthermore, when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU improvement over baseline trees. 1 Introduction Monolingually, many Treebank conventions are more or less equally good. For example, the English WSJ treebank (Marcus et al., 1993) attaches verbs to objects rather than to subjects, and it attaches prepositional modifiers outside of all quantifiers and determiners. The former matches most linguistic theories while the latter does not, but to a monolingual parser, these conventions are equally learnable. However, once bilingual data is involved, such treebank conventions entail constraints on rule extraction that may not be borne out by semantic alignments. To the extent that there are simply divergences in the syntactic structure of the two languages, it will often be impossible to construct syntax trees that are simulta"
D12-1079,P08-1114,0,0.0800624,"to select team members 第一 步 是 S TO+VB VP TO VB ADVP The first step is to select team members 挑 第一 (a) Before 步 是 挑 (b) After Figure 1: An example tree transformation merging a VB node with the TO sibling of its parent VP. Before the transformation (a), the bolded VP cannot be extracted as a translation rule, but afterwards (b), both this VP and the newly created TO+VB node are extractable. for learning translation rules (Mi and Huang, 2008; Zhang et al., 2009), or by learning rules that encode syntactic information but do not strictly adhere to constituency boundaries (Zollmann et al., 2006; Marton and Resnik, 2008; Chiang, 2010). The most closely related MT system is that of Zhao et al. (2011), who train a rule extraction system to transform the subtrees that make up individual translation rules using a manually constructed set of transformations similar to those learned by our system. Instead of modifying the MT system to work around the input annotations, our system modifies the input itself in order to improve downstream translation. Most systems of this sort learn how to modify word alignments to agree better with the syntactic parse trees (DeNero and Klein, 2007; Fossum et al., 2008), but there ha"
D12-1079,D08-1022,0,0.0740914,"e Learning, pages 863–872, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics S S NP VP VBZ NP S VP VBZ VP VP TO VP VB ADVP The first step is to select team members 第一 步 是 S TO+VB VP TO VB ADVP The first step is to select team members 挑 第一 (a) Before 步 是 挑 (b) After Figure 1: An example tree transformation merging a VB node with the TO sibling of its parent VP. Before the transformation (a), the bolded VP cannot be extracted as a translation rule, but afterwards (b), both this VP and the newly created TO+VB node are extractable. for learning translation rules (Mi and Huang, 2008; Zhang et al., 2009), or by learning rules that encode syntactic information but do not strictly adhere to constituency boundaries (Zollmann et al., 2006; Marton and Resnik, 2008; Chiang, 2010). The most closely related MT system is that of Zhao et al. (2011), who train a rule extraction system to transform the subtrees that make up individual translation rules using a manually constructed set of transformations similar to those learned by our system. Instead of modifying the MT system to work around the input annotations, our system modifies the input itself in order to improve downstream tr"
D12-1079,P03-1021,0,0.0136854,"Transformation results on a subset of the MT training data. The training and test sets are disjoint in order to measure how well the learned transformation sequence generalizes. Once again, we plot the average improvement over the baseline trees. Though 5151 transformations were learned from the training set, the maximum test set agreement was achieved at 630 transformations, with an average improvement of 2.60. 642 sentence pairs from the NIST MT04 and MT05 data sets, using the BLEU metric (Papineni et al., 2001). As discussed by Clark et al. (2011), the optimizer included with Moses (MERT, Och, 2003) is not always particularly stable, and results (even on the tuning set) can vary dramatically across tuning runs. To mitigate this effect, we first used the Moses training scripts to extract a table of translation rules for each set of English trees. Then, for each rule table, we ran MERT 11 times and selected the parameters that achieved the maximum tuning BLEU to use for decoding the test set. Table 3 shows the results of our translation experiments. The best translation results are achieved by using the first 139 transformations, giving a BLEU improvement of more than 0.9 over the stronges"
D12-1079,2001.mtsummit-papers.68,0,0.0165343,"9 630 5151 6 5 Training Test 4 3 2 1 0 0 1000 2000 3000 4000 Number of Transformations 5000 Figure 10: Transformation results on a subset of the MT training data. The training and test sets are disjoint in order to measure how well the learned transformation sequence generalizes. Once again, we plot the average improvement over the baseline trees. Though 5151 transformations were learned from the training set, the maximum test set agreement was achieved at 630 transformations, with an average improvement of 2.60. 642 sentence pairs from the NIST MT04 and MT05 data sets, using the BLEU metric (Papineni et al., 2001). As discussed by Clark et al. (2011), the optimizer included with Moses (MERT, Och, 2003) is not always particularly stable, and results (even on the tuning set) can vary dramatically across tuning runs. To mitigate this effect, we first used the Moses training scripts to extract a table of translation rules for each set of English trees. Then, for each rule table, we ran MERT 11 times and selected the parameters that achieved the maximum tuning BLEU to use for decoding the test set. Table 3 shows the results of our translation experiments. The best translation results are achieved by using t"
D12-1079,N07-1051,1,0.77206,"n Treebank produces clean, interpretable rules, preliminary experiments showed little to no improvement from using these rules for MT, primarily because actual alignments are not only noisier but also systematically different from gold ones. Thus, all rules used for MT experiments were learned from automatically annotated text. For our Chinese to English translation experiments, we generated word alignments using the Berkeley Aligner (Liang et al., 2006) with default settings. We used an MT pipeline that conditions on target-side syntax, so our initial state annotator was the Berkeley Parser (Petrov and Klein, 2007), trained on a modified English treebank that has been adapted to match standard MT tokenization and capitalization schemes. As mentioned in Section 5, we could, in principle train on all 500k sentences of our MT training data. However, this would be quite slow: each iteration of the training procedure requires iterating through all n training sentences6 once for each of the m candidate transformations, for a total cost of O(nm) where m grows (albeit sublinearly) with n. Since the 6 By using a simple hashing scheme to keep track of triggering environments, this cost can be reduced greatly but"
D12-1079,W95-0107,0,0.270052,"Missing"
D12-1079,P98-2188,0,0.166929,"Missing"
D12-1079,D07-1078,0,0.0781527,"s, we used the string-to-tree syntactic pipeline included in the current version of Moses (Koehn et al., 2007). Our training bitext was approximately 21.8 million words, and the sentences and word alignments were the same for all experiments; the only difference between each experiment was the English trees, for which we tested a range of transformation sequence prefixes (including a 0-length prefix, which just yields the original trees, as a baseline). Since the transformed trees tended to be more finely articulated, and increasing the number of unique spans often helps with rule extraction (Wang et al., 2007), we equalized the span count by also testing binarized versions of each set of trees, using the leftbranching and right-branching binarization scripts included with Moses.9 We tuned on 1000 sentence pairs and tested on NP VP (a) A RTICULATEhS,NP,VPi PP PP ... IN NP... ... IN A B ... A B (b) F LATTEN I N C ONTEXThPP,NP,IN,righti VP ... VP VBN ... VP ... VBN VP ... (c) P ROMOTEhVP,VP,VBN,lefti VP TO VP VB ... VP TO+VB VP TO VB ... (d) A DOPThVP,TO,VP,VB,lefti PP VBG PP IN ... VP VBG+IN PP VBG IN ... 7 (e) A DOPThPP,VBG,PP,IN,lefti Figure 9: Illustrations of the top 5 transformations from Table"
D12-1079,P01-1067,0,0.222546,"ding new parse trees with constituency structures that agree better with the bilingual alignments yet remain consistent across the corpus. In particular, we use this method to learn a transformation sequence for the English trees in a set of English to Chinese MT training data. In experiments with a string-to-tree translation system, we show resulting improvements of up to 0.9 BLEU. A great deal of research in syntactic machine translation has been devoted to handling the inherent syntactic divergence between source and target languages. Some systems attempt to model the differences directly (Yamada and Knight, 2001; Eisner, 2003), but most recent work focuses on reducing the sensitivity of the rule-extraction procedure to the constituency decisions made by 1-best syntactic parsers, either by using forest-based methods 863 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 863–872, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics S S NP VP VBZ NP S VP VBZ VP VP TO VP VB ADVP The first step is to select team members 第一 步 是 S TO+VB VP TO VB ADVP The first step is to select team"
D12-1079,P09-1020,0,0.142759,"3–872, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics S S NP VP VBZ NP S VP VBZ VP VP TO VP VB ADVP The first step is to select team members 第一 步 是 S TO+VB VP TO VB ADVP The first step is to select team members 挑 第一 (a) Before 步 是 挑 (b) After Figure 1: An example tree transformation merging a VB node with the TO sibling of its parent VP. Before the transformation (a), the bolded VP cannot be extracted as a translation rule, but afterwards (b), both this VP and the newly created TO+VB node are extractable. for learning translation rules (Mi and Huang, 2008; Zhang et al., 2009), or by learning rules that encode syntactic information but do not strictly adhere to constituency boundaries (Zollmann et al., 2006; Marton and Resnik, 2008; Chiang, 2010). The most closely related MT system is that of Zhao et al. (2011), who train a rule extraction system to transform the subtrees that make up individual translation rules using a manually constructed set of transformations similar to those learned by our system. Instead of modifying the MT system to work around the input annotations, our system modifies the input itself in order to improve downstream translation. Most syste"
D12-1079,P11-1085,0,0.024795,"members 挑 第一 (a) Before 步 是 挑 (b) After Figure 1: An example tree transformation merging a VB node with the TO sibling of its parent VP. Before the transformation (a), the bolded VP cannot be extracted as a translation rule, but afterwards (b), both this VP and the newly created TO+VB node are extractable. for learning translation rules (Mi and Huang, 2008; Zhang et al., 2009), or by learning rules that encode syntactic information but do not strictly adhere to constituency boundaries (Zollmann et al., 2006; Marton and Resnik, 2008; Chiang, 2010). The most closely related MT system is that of Zhao et al. (2011), who train a rule extraction system to transform the subtrees that make up individual translation rules using a manually constructed set of transformations similar to those learned by our system. Instead of modifying the MT system to work around the input annotations, our system modifies the input itself in order to improve downstream translation. Most systems of this sort learn how to modify word alignments to agree better with the syntactic parse trees (DeNero and Klein, 2007; Fossum et al., 2008), but there has also been other work directly related to improving agreement by modifying the t"
D12-1079,2006.iwslt-evaluation.20,0,0.0252369,"ADVP The first step is to select team members 第一 步 是 S TO+VB VP TO VB ADVP The first step is to select team members 挑 第一 (a) Before 步 是 挑 (b) After Figure 1: An example tree transformation merging a VB node with the TO sibling of its parent VP. Before the transformation (a), the bolded VP cannot be extracted as a translation rule, but afterwards (b), both this VP and the newly created TO+VB node are extractable. for learning translation rules (Mi and Huang, 2008; Zhang et al., 2009), or by learning rules that encode syntactic information but do not strictly adhere to constituency boundaries (Zollmann et al., 2006; Marton and Resnik, 2008; Chiang, 2010). The most closely related MT system is that of Zhao et al. (2011), who train a rule extraction system to transform the subtrees that make up individual translation rules using a manually constructed set of transformations similar to those learned by our system. Instead of modifying the MT system to work around the input annotations, our system modifies the input itself in order to improve downstream translation. Most systems of this sort learn how to modify word alignments to agree better with the syntactic parse trees (DeNero and Klein, 2007; Fossum et"
D12-1079,W06-3601,0,\N,Missing
D12-1079,P02-1040,0,\N,Missing
D12-1079,H93-1047,0,\N,Missing
D12-1079,C98-2183,0,\N,Missing
D12-1079,H89-1038,0,\N,Missing
D12-1091,W10-1703,0,0.0145689,"ll systems that have been evaluated in published work. For each pair of these systems we could run a comparison and compute both δ(x) and p-value(x). While obtaining such data is not generally feasible, for several tasks there are public competitions to which systems are submitted by many researchers. Some of these competitions make system outputs publicly available. We obtained system outputs from the TAC 2008 workshop on automatic summarization (Dang and Owczarzak, 2008), the CoNLL 2007 shared task on dependency parsing (Nivre et al., 2007), and the WMT 2010 workshop on machine translation (Callison-Burch et al., 2010). For cases where the metric linearly decomposes over sentences, the mean of δ(x(i) ) is δ(x). By the central limit theorem, the distribution will be symmetric for large test sets; for small test sets it may not. 3 Note that the bootstrap procedure given only approximates the true significance level, with multiple sources of approximation error. One is the error introduced from using a finite number of bootstrap samples. Another comes from the assumption that the bootstrap samples reflect the underlying population distribution. A third is the assumption that the mean bootstrap gain is the test"
D12-1091,1993.eamt-1.1,0,0.348588,"a constant, the metric gain we actually observed. Traditionally, if p(δ(X) &gt; δ(x)|H0 ) < 0.05, we say that the observed value of δ(x) is sufficiently unlikely that we should reject H0 (i.e. accept that A’s victory was real and not just a random fluke). We refer to p(δ(X) &gt; δ(x)|H0 ) as p-value(x). In most cases p-value(x) is not easily computable and must be approximated. The type of approximation depends on the particular hypothesis testing method. Various methods have been used in the NLP community (Gillick and Cox, 1989; Yeh, 2000; Riezler and Maxwell, 2005). We use the paired bootstrap1 (Efron and Tibshirani, 1993) because it is one 1 Riezler and Maxwell (2005) argue the benefits of approximate randomization testing, introduced by Noreen (1989). However, this method is ill-suited to the type of hypothesis we are testing. Our null hypothesis does not condition on the test data, and therefore the bootstrap is a better choice. 996 Draw b bootstrap samples x(i) of size n by sampling with replacement from x. 2. Initialize s = 0. 3. For each x(i) increment s if δ(x(i) ) &gt; 2δ(x). 4. Estimate p-value(x) ≈ sb 1. Figure 1: The bootstrap procedure. In all of our experiments we use b = 106 , which is more than suff"
D12-1091,P09-1104,1,0.559336,"rated using the same base model type and comparisons between systems generated using different base model types are shown separately. of the same model type the computed p-value < 0.05 threshold is 0.28 BLEU. For comparisons between systems of different model types the threshold is 0.37 BLEU. 3.2.4 Word Alignment Now that we have validated our simple model of system variation on two tasks, we go on to generate plots for tasks that do not have competitions with publicly available system outputs. The first task is English-French word alignment, where we use three base models: the ITG aligner of Haghighi et al. (2009), the joint HMM aligner of Liang et al. (2006), and GIZA++ (Och and Ney, 2003). The last two aligners are unsupervised, while the first is supervised. We train the unsupervised word aligners using the 1.1M sentence pair Hansard training corpus, resampling 20 training sets of the same size.6 Following Haghighi et al. (2009), we train the supervised ITG aligner using the first 337 sentence pairs of the hand-aligned Hansard test set; again, we resample 20 training sets of the same size as the original data. We test on the remaining 100 hand-aligned sentence pairs from the Hansard test set. Unlike"
D12-1091,P03-1054,1,0.0361194,"erall, the spread of this plot is larger than previous ones. This may be due to the small size of the test set, or possibly some additional variance introduced by unsupervised training. For comparisons between systems of the same model type the p-value < 0.05 threshold is 0.50 AER. For comparisons between systems of different model types the threshold is 1.12 AER. 3.2.5 Constituency Parsing Finally, before we move on to further types of analysis, we run an experiment for the task of constituency parsing. We use three base models: the Berkeley parser (Petrov et al., 2006), the Stanford parser (Klein and Manning, 2003), and Dan Bikel’s implementation (Bikel, 2004) of the Collins parser (Collins, 1999). We use sections 2-21 of the WSJ corpus (Marcus et al., 1993), which consists of 38K sentences and parses, as a training set. We resample 10 training sets of size 38K, 10 of size 19K, and 10 of size 9K, and use these to train systems. We test on section 23. The results are shown in Figure 8. For comparisons between systems of the same model type, the p-value < 0.05 threshold is 0.47 F1. For comparisons between systems of different model types the threshold is 0.57 F1. 4.1 Varying the Size Figure 9 plots compar"
D12-1091,P07-2045,0,0.0217932,"shold is 1.51 unlabeled dependency accuracy. These results indicate that the similarity of the systems being compared is an important factor. As mentioned, rulesof-thumb derived from such thresholds cannot be applied blindly, but, in special cases where two systems are known to be correlated, the former threshold should be preferred over the latter. For example, during development most comparisons are made between incremental variants of the same system. If adding a feature to a supervised parser increases un1000 3.2.3 Machine Translation Our two base models for machine translation are Moses (Koehn et al., 2007) and Joshua (Li et al., 2009). We use 1.4M sentence pairs from the German-English portion of the WMT-provided Europarl (Koehn, 2005) and news commentary corpora as the original training set. We resample 75 training sets, 20 of 1.4M sentence pairs, 29 of 350K sentence pairs, and 26 of 88K sentence pairs. This yields a total of 150 system outputs on the system combination portion of the German-English WMT 2010 news test set. The results of the pairwise comparisons of all 150 system outputs are shown in Figure 6, along with the results of the WMT 2010 workshop system comparisons from Figure 4. Th"
D12-1091,W04-3250,0,0.704346,"ls are computed, how well does the standard i.i.d. notion of significance hold up in practical settings where future distributions are neither independent nor identically distributed, such as across domains? We explore this question using a range of test set variations for constituency parsing. 1 Introduction It is, or at least should be, nearly universal that NLP evaluations include statistical significance tests to validate metric gains. As important as significance testing is, relatively few papers have empirically investigated its practical properties. Those that do focus on single tasks (Koehn, 2004; Zhang et al., 2004) or on the comparison of alternative hypothesis tests (Gillick and Cox, 1989; Yeh, 2000; Bisani and Ney, 2004; Riezler and Maxwell, 2005). In this paper, we investigate two aspects of the empirical behavior of paired significance tests for NLP systems. For example, all else equal, larger metric gains will tend to be more significant. However, what does this relationship look like and how reliable is it? What should be made of the conventional wisdom that often springs up that a certain metric gain is roughly the point of significance for a given task (e.g. 0.4 F1 in parsin"
D12-1091,2005.mtsummit-papers.11,0,0.0263925,"r. As mentioned, rulesof-thumb derived from such thresholds cannot be applied blindly, but, in special cases where two systems are known to be correlated, the former threshold should be preferred over the latter. For example, during development most comparisons are made between incremental variants of the same system. If adding a feature to a supervised parser increases un1000 3.2.3 Machine Translation Our two base models for machine translation are Moses (Koehn et al., 2007) and Joshua (Li et al., 2009). We use 1.4M sentence pairs from the German-English portion of the WMT-provided Europarl (Koehn, 2005) and news commentary corpora as the original training set. We resample 75 training sets, 20 of 1.4M sentence pairs, 29 of 350K sentence pairs, and 26 of 88K sentence pairs. This yields a total of 150 system outputs on the system combination portion of the German-English WMT 2010 news test set. The results of the pairwise comparisons of all 150 system outputs are shown in Figure 6, along with the results of the WMT 2010 workshop system comparisons from Figure 4. The natural comparisons from the WMT 2010 workshop align well with the comparisons between synthetically varied models. Again, the dif"
D12-1091,W09-0424,0,0.0421198,"cy accuracy. These results indicate that the similarity of the systems being compared is an important factor. As mentioned, rulesof-thumb derived from such thresholds cannot be applied blindly, but, in special cases where two systems are known to be correlated, the former threshold should be preferred over the latter. For example, during development most comparisons are made between incremental variants of the same system. If adding a feature to a supervised parser increases un1000 3.2.3 Machine Translation Our two base models for machine translation are Moses (Koehn et al., 2007) and Joshua (Li et al., 2009). We use 1.4M sentence pairs from the German-English portion of the WMT-provided Europarl (Koehn, 2005) and news commentary corpora as the original training set. We resample 75 training sets, 20 of 1.4M sentence pairs, 29 of 350K sentence pairs, and 26 of 88K sentence pairs. This yields a total of 150 system outputs on the system combination portion of the German-English WMT 2010 news test set. The results of the pairwise comparisons of all 150 system outputs are shown in Figure 6, along with the results of the WMT 2010 workshop system comparisons from Figure 4. The natural comparisons from th"
D12-1091,N06-1014,1,0.395006,"ons between systems generated using different base model types are shown separately. of the same model type the computed p-value < 0.05 threshold is 0.28 BLEU. For comparisons between systems of different model types the threshold is 0.37 BLEU. 3.2.4 Word Alignment Now that we have validated our simple model of system variation on two tasks, we go on to generate plots for tasks that do not have competitions with publicly available system outputs. The first task is English-French word alignment, where we use three base models: the ITG aligner of Haghighi et al. (2009), the joint HMM aligner of Liang et al. (2006), and GIZA++ (Och and Ney, 2003). The last two aligners are unsupervised, while the first is supervised. We train the unsupervised word aligners using the 1.1M sentence pair Hansard training corpus, resampling 20 training sets of the same size.6 Following Haghighi et al. (2009), we train the supervised ITG aligner using the first 337 sentence pairs of the hand-aligned Hansard test set; again, we resample 20 training sets of the same size as the original data. We test on the remaining 100 hand-aligned sentence pairs from the Hansard test set. Unlike previous plots, the points corresponding to c"
D12-1091,W04-1013,0,0.00888053,"n in Figure 1. 2 Note that many authors have used a variant where the event tallied on the x(i) is whether δ(x(i) ) < 0, rather than δ(x(i) ) &gt; 2δ(x). If the mean of δ(x(i) ) is δ(x), and if the distribution of δ(x(i) ) is symmetric, then these two versions will be equivalent. 3 1 0.9 1 - p-value As mentioned, a major benefit of the bootstrap is that any evaluation metric can be used to compute δ(x).3 We run the bootstrap using several metrics: F1-measure for constituency parsing, unlabeled dependency accuracy for dependency parsing, alignment error rate (AER) for word alignment, ROUGE score (Lin, 2004) for summarization, and BLEU score for machine translation.4 We report all metrics as percentages. 0.8 Different research groups Same research group 0.7 0.6 Experiments 0.5 Our first goal is to explore the relationship between metric gain, δ(x), and statistical significance, p-value(x), for a range of NLP tasks. In order to say anything meaningful, we will need to see both δ(x) and p-value(x) for many pairs of systems. 3.1 Natural Comparisons Ideally, for a given task and test set we could obtain outputs from all systems that have been evaluated in published work. For each pair of these system"
D12-1091,J93-2004,0,0.0433698,"ntroduced by unsupervised training. For comparisons between systems of the same model type the p-value < 0.05 threshold is 0.50 AER. For comparisons between systems of different model types the threshold is 1.12 AER. 3.2.5 Constituency Parsing Finally, before we move on to further types of analysis, we run an experiment for the task of constituency parsing. We use three base models: the Berkeley parser (Petrov et al., 2006), the Stanford parser (Klein and Manning, 2003), and Dan Bikel’s implementation (Bikel, 2004) of the Collins parser (Collins, 1999). We use sections 2-21 of the WSJ corpus (Marcus et al., 1993), which consists of 38K sentences and parses, as a training set. We resample 10 training sets of size 38K, 10 of size 19K, and 10 of size 9K, and use these to train systems. We test on section 23. The results are shown in Figure 8. For comparisons between systems of the same model type, the p-value < 0.05 threshold is 0.47 F1. For comparisons between systems of different model types the threshold is 0.57 F1. 4.1 Varying the Size Figure 9 plots comparisons for machine translation on variously sized initial segments of the WMT 2010 news test set. Similarly, Figure 10 plots comparisons for consti"
D12-1091,H05-1066,0,0.0213104,"Missing"
D12-1091,nivre-etal-2006-maltparser,0,0.00996693,"from natural comparisons. We expect that each new system will be different, but that systems originating from the same base model will be highly correlated. This provides a useful division of comparisons: those between systems built with the same model, and those between systems built with different models. The first class can be used to approximate comparisons of systems that are expected to be specially correlated, and the latter for comparisons of systems that are not. 3.2.2 Dependency Parsing We use three base models for dependency parsing: MST parser (McDonald et al., 2005), Maltparser (Nivre et al., 2006), and the ensemble parser of Surdeanu and Manning (2010). We use the CoNLL 2007 Chinese training set, which consists of 57K sentences. We resample 5 training sets of 57K sentences, 10 training sets of 28K sentences, and 10 training sets of 14K sentences. Together, this yields a total of 75 system outputs on the CoNLL 2007 Chinese test set, 25 systems for each base model type. The score ranges of all the base models overlap. This ensures that for each pair of model types we will be able to see comparisons where the metric gains are small. The results of the pairwise comparisons of all 75 system"
D12-1091,J03-1002,0,0.015359,"ng different base model types are shown separately. of the same model type the computed p-value < 0.05 threshold is 0.28 BLEU. For comparisons between systems of different model types the threshold is 0.37 BLEU. 3.2.4 Word Alignment Now that we have validated our simple model of system variation on two tasks, we go on to generate plots for tasks that do not have competitions with publicly available system outputs. The first task is English-French word alignment, where we use three base models: the ITG aligner of Haghighi et al. (2009), the joint HMM aligner of Liang et al. (2006), and GIZA++ (Och and Ney, 2003). The last two aligners are unsupervised, while the first is supervised. We train the unsupervised word aligners using the 1.1M sentence pair Hansard training corpus, resampling 20 training sets of the same size.6 Following Haghighi et al. (2009), we train the supervised ITG aligner using the first 337 sentence pairs of the hand-aligned Hansard test set; again, we resample 20 training sets of the same size as the original data. We test on the remaining 100 hand-aligned sentence pairs from the Hansard test set. Unlike previous plots, the points corresponding to comparisons between systems with"
D12-1091,P03-1021,0,0.0221932,"mization testing, introduced by Noreen (1989). However, this method is ill-suited to the type of hypothesis we are testing. Our null hypothesis does not condition on the test data, and therefore the bootstrap is a better choice. 996 Draw b bootstrap samples x(i) of size n by sampling with replacement from x. 2. Initialize s = 0. 3. For each x(i) increment s if δ(x(i) ) &gt; 2δ(x). 4. Estimate p-value(x) ≈ sb 1. Figure 1: The bootstrap procedure. In all of our experiments we use b = 106 , which is more than sufficient for the bootstrap estimate of p-value(x) to stabilize. of the most widely used (Och, 2003; Bisani and Ney, 2004; Zhang et al., 2004; Koehn, 2004), and because it can be easily applied to any performance metric, even complex metrics like F1-measure or BLEU (Papineni et al., 2002). Note that we could perform the experiments described in this paper using another method, such as the paired Student’s ttest. To the extent that the assumptions of the t-test are met, it is likely that the results would be very similar to those we present here. 2.2 The Bootstrap The bootstrap estimates p-value(x) though a combination of simulation and approximation, drawing many simulated test sets x(i) an"
D12-1091,P02-1040,0,0.0880721,"ta, and therefore the bootstrap is a better choice. 996 Draw b bootstrap samples x(i) of size n by sampling with replacement from x. 2. Initialize s = 0. 3. For each x(i) increment s if δ(x(i) ) &gt; 2δ(x). 4. Estimate p-value(x) ≈ sb 1. Figure 1: The bootstrap procedure. In all of our experiments we use b = 106 , which is more than sufficient for the bootstrap estimate of p-value(x) to stabilize. of the most widely used (Och, 2003; Bisani and Ney, 2004; Zhang et al., 2004; Koehn, 2004), and because it can be easily applied to any performance metric, even complex metrics like F1-measure or BLEU (Papineni et al., 2002). Note that we could perform the experiments described in this paper using another method, such as the paired Student’s ttest. To the extent that the assumptions of the t-test are met, it is likely that the results would be very similar to those we present here. 2.2 The Bootstrap The bootstrap estimates p-value(x) though a combination of simulation and approximation, drawing many simulated test sets x(i) and counting how often A sees an accidental advantage of δ(x) or greater. How can we get sample test sets x(i) ? We lack the ability to actually draw new test sets from the underlying populati"
D12-1091,P06-1055,1,0.328321,"two models are particularly correlated. Overall, the spread of this plot is larger than previous ones. This may be due to the small size of the test set, or possibly some additional variance introduced by unsupervised training. For comparisons between systems of the same model type the p-value < 0.05 threshold is 0.50 AER. For comparisons between systems of different model types the threshold is 1.12 AER. 3.2.5 Constituency Parsing Finally, before we move on to further types of analysis, we run an experiment for the task of constituency parsing. We use three base models: the Berkeley parser (Petrov et al., 2006), the Stanford parser (Klein and Manning, 2003), and Dan Bikel’s implementation (Bikel, 2004) of the Collins parser (Collins, 1999). We use sections 2-21 of the WSJ corpus (Marcus et al., 1993), which consists of 38K sentences and parses, as a training set. We resample 10 training sets of size 38K, 10 of size 19K, and 10 of size 9K, and use these to train systems. We test on section 23. The results are shown in Figure 8. For comparisons between systems of the same model type, the p-value < 0.05 threshold is 0.47 F1. For comparisons between systems of different model types the threshold is 0.57"
D12-1091,W05-0908,0,0.17337,"independent nor identically distributed, such as across domains? We explore this question using a range of test set variations for constituency parsing. 1 Introduction It is, or at least should be, nearly universal that NLP evaluations include statistical significance tests to validate metric gains. As important as significance testing is, relatively few papers have empirically investigated its practical properties. Those that do focus on single tasks (Koehn, 2004; Zhang et al., 2004) or on the comparison of alternative hypothesis tests (Gillick and Cox, 1989; Yeh, 2000; Bisani and Ney, 2004; Riezler and Maxwell, 2005). In this paper, we investigate two aspects of the empirical behavior of paired significance tests for NLP systems. For example, all else equal, larger metric gains will tend to be more significant. However, what does this relationship look like and how reliable is it? What should be made of the conventional wisdom that often springs up that a certain metric gain is roughly the point of significance for a given task (e.g. 0.4 F1 in parsing or 0.5 BLEU In order for our results to be meaningful, we must have access to the outputs of many of NLP systems. Public competitions, such as the well-know"
D12-1091,N10-1091,0,0.0128219,"ew system will be different, but that systems originating from the same base model will be highly correlated. This provides a useful division of comparisons: those between systems built with the same model, and those between systems built with different models. The first class can be used to approximate comparisons of systems that are expected to be specially correlated, and the latter for comparisons of systems that are not. 3.2.2 Dependency Parsing We use three base models for dependency parsing: MST parser (McDonald et al., 2005), Maltparser (Nivre et al., 2006), and the ensemble parser of Surdeanu and Manning (2010). We use the CoNLL 2007 Chinese training set, which consists of 57K sentences. We resample 5 training sets of 57K sentences, 10 training sets of 28K sentences, and 10 training sets of 14K sentences. Together, this yields a total of 75 system outputs on the CoNLL 2007 Chinese test set, 25 systems for each base model type. The score ranges of all the base models overlap. This ensures that for each pair of model types we will be able to see comparisons where the metric gains are small. The results of the pairwise comparisons of all 75 system outputs are shown in Figure 5, along with the results o"
D12-1091,C00-2137,0,0.528095,"future distributions are neither independent nor identically distributed, such as across domains? We explore this question using a range of test set variations for constituency parsing. 1 Introduction It is, or at least should be, nearly universal that NLP evaluations include statistical significance tests to validate metric gains. As important as significance testing is, relatively few papers have empirically investigated its practical properties. Those that do focus on single tasks (Koehn, 2004; Zhang et al., 2004) or on the comparison of alternative hypothesis tests (Gillick and Cox, 1989; Yeh, 2000; Bisani and Ney, 2004; Riezler and Maxwell, 2005). In this paper, we investigate two aspects of the empirical behavior of paired significance tests for NLP systems. For example, all else equal, larger metric gains will tend to be more significant. However, what does this relationship look like and how reliable is it? What should be made of the conventional wisdom that often springs up that a certain metric gain is roughly the point of significance for a given task (e.g. 0.4 F1 in parsing or 0.5 BLEU In order for our results to be meaningful, we must have access to the outputs of many of NLP s"
D12-1091,J04-4004,0,\N,Missing
D12-1091,J03-4003,0,\N,Missing
D12-1091,zhang-etal-2004-interpreting,0,\N,Missing
D12-1091,D07-1096,0,\N,Missing
D12-1096,H91-1060,0,0.579673,"Missing"
D12-1096,P11-1048,0,0.0156051,"hat can be em1055 ployed. One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000). While re-ranking has led to gains in performance (Charniak and Johnson, 2005), there has been limited analysis of how effectively rerankers are using the set of available options. Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008; Ng et al., 2010; Auli and Lopez, 2011; Ng and Curran, 2012). In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker. The oracle results use the parse in each K-best list with the highest F-score. While this may not give the true oracle result, as F-score does not factor over sentences, it gives a close approximation. The table has the same columns as Table 2, but the ranges on the bars now reflect the min and max for these sets. While there is improvement on all errors when using the reranker, there is very little additional gain beyond the first 5-10 parses. Even for"
D12-1096,D11-1037,0,0.0179858,"ement of the oracle is considerably higher than that of the reranker, particularly compared to the differences for other errors, suggesting that the reranker lacks the features necessary to make the decision better than the parser. The other interesting outlier is NP internal structure, which continues to make improvements for longer lists, unlike the other error types. 5.2 Out-of-Domain Parsing performance drops considerably when shifting outside of the domain a parser was trained on (Gildea, 2001). Clegg and Shepherd (2005) evaluated parsers qualitatively on node types and rule productions. Bender et al. (2011) designed a Wikipedia test set to evaluate parsers on dependencies representing ten specific linguistic phenomena. To provide a deeper understanding of the errors arising when parsing outside of the newswire domain, we analyse performance of the Charniak parser with reranker and self-trained model on the eight parts of the Brown corpus (Marcus et al., 1056 Corpus WSJ 23 Brown F Brown G Brown K Brown L Brown M Brown N Brown P Brown R G-Web Blogs G-Web Email Description Sentences Av. Length Newswire 2416 23.5 Popular 3164 23.4 Biographies 3279 25.5 General 3881 17.2 Mystery 3714 15.7 Science 881"
D12-1096,J04-4004,0,0.0152738,"from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes i"
D12-1096,P11-1045,0,0.014823,"into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner"
D12-1096,P06-2006,0,0.0118261,"o account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are stro"
D12-1096,cer-etal-2010-parsing,0,0.0400421,"Missing"
D12-1096,P05-1022,0,0.0927601,"For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. System F R Exact Speed"
D12-1096,A00-2018,0,0.0745506,"ation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the"
D12-1096,W05-1102,0,0.0192051,"using on the rows for K = 2 we can also see two interesting outliers. The PP attachment improvement of the oracle is considerably higher than that of the reranker, particularly compared to the differences for other errors, suggesting that the reranker lacks the features necessary to make the decision better than the parser. The other interesting outlier is NP internal structure, which continues to make improvements for longer lists, unlike the other error types. 5.2 Out-of-Domain Parsing performance drops considerably when shifting outside of the domain a parser was trained on (Gildea, 2001). Clegg and Shepherd (2005) evaluated parsers qualitatively on node types and rule productions. Bender et al. (2011) designed a Wikipedia test set to evaluate parsers on dependencies representing ten specific linguistic phenomena. To provide a deeper understanding of the errors arising when parsing outside of the newswire domain, we analyse performance of the Charniak parser with reranker and self-trained model on the eight parts of the Brown corpus (Marcus et al., 1056 Corpus WSJ 23 Brown F Brown G Brown K Brown L Brown M Brown N Brown P Brown R G-Web Blogs G-Web Email Description Sentences Av. Length Newswire 2416 23."
D12-1096,P97-1003,0,0.0512335,"rectly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderso"
D12-1096,J03-4003,0,0.0837746,"2 Background Most attempts to understand the behaviour of constituency parsers have focused on overall evaluation metrics. The three main methods are intrinsic evaluation with PARSEVAL, evaluation on dependencies extracted from the constituency parse, and evaluation on downstream tasks that rely on parsing. Intrinsic evaluation with PARSEVAL, which calculates precision and recall over labeled tree nodes, is a useful indicator of overall performance, but does not pinpoint which structures the parser has most difficulty with. Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves ex"
D12-1096,W11-2927,0,0.0149009,"r, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk 1049 and Corston-Oliver, 2006), information extraction (Miyao et al., 2008), textual entailment (Yuret et al., 2010), or semantic dependencies (Dridan and Oepen, 2011). While some of these approaches give a better sense of the impact of parse errors, they require integration into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers w"
D12-1096,W11-2920,0,0.0121497,"ly classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004)."
D12-1096,foster-van-genabith-2008-parser,0,0.0378682,"Missing"
D12-1096,W01-0521,0,0.0158019,"ngle error. Focusing on the rows for K = 2 we can also see two interesting outliers. The PP attachment improvement of the oracle is considerably higher than that of the reranker, particularly compared to the differences for other errors, suggesting that the reranker lacks the features necessary to make the decision better than the parser. The other interesting outlier is NP internal structure, which continues to make improvements for longer lists, unlike the other error types. 5.2 Out-of-Domain Parsing performance drops considerably when shifting outside of the domain a parser was trained on (Gildea, 2001). Clegg and Shepherd (2005) evaluated parsers qualitatively on node types and rule productions. Bender et al. (2011) designed a Wikipedia test set to evaluate parsers on dependencies representing ten specific linguistic phenomena. To provide a deeper understanding of the errors arising when parsing outside of the newswire domain, we analyse performance of the Charniak parser with reranker and self-trained model on the eight parts of the Brown corpus (Marcus et al., 1056 Corpus WSJ 23 Brown F Brown G Brown K Brown L Brown M Brown N Brown P Brown R G-Web Blogs G-Web Email Description Sentences A"
D12-1096,W07-2202,0,0.0204252,"e nodes, is a useful indicator of overall performance, but does not pinpoint which structures the parser has most difficulty with. Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and va"
D12-1096,D09-1121,0,0.019996,"arsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk 1049 and Corston-Oliver, 2006), information extraction (Miyao"
D12-1096,N03-1014,0,0.0513104,"s (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. System F R Exact Speed ENHANCED TRAINING / SYSTEMS Charniak-SR 92.07 92.44 91.70 44.87 1.8 Charniak-R 91.41 91.78 91.04 44.04 1.8 Charniak-S 91.02 91.16 90.89 40.77 1.8 S P NP VP PRP VBD He was STANDARD PARSERS Berkeley Charniak SSN BUBS Bikel Collins-3 Collins-2 Collins-1 Stanford"
D12-1096,P04-1013,0,0.0234916,"unlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. System F R Exact Speed ENHANCED TRAINING / SYSTEMS Charniak-SR 92.07 92.44 91.70 44.87 1.8 Charniak-R 91.41 91.78 91.04 44.04 1.8 Charniak-S 91.02 91.16 90.89 40.77 1.8 S P NP VP PRP VBD He was STANDARD PARSERS Berkeley Charniak SSN BUBS Bikel Collins-3 Collins-2 Collins-1 Stanford-L Stanford-U 90.0"
D12-1096,P08-1067,0,0.0174942,"limits the range of features that can be em1055 ployed. One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000). While re-ranking has led to gains in performance (Charniak and Johnson, 2005), there has been limited analysis of how effectively rerankers are using the set of available options. Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008; Ng et al., 2010; Auli and Lopez, 2011; Ng and Curran, 2012). In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker. The oracle results use the parse in each K-best list with the highest F-score. While this may not give the true oracle result, as F-score does not factor over sentences, it gives a close approximation. The table has the same columns as Table 2, but the ranges on the bars now reflect the min and max for these sets. While there is improvement on all errors when using the reranker, there is very little additional gain"
D12-1096,W03-2401,0,0.0081588,"rs is not taken into account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for c"
D12-1096,P03-1054,1,0.0285971,"er grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. System F R Exact Speed ENHANCED TRAINING / SYSTEMS Charniak-SR 92.07 92.44 91.70 44.87 1.8 Charniak-R 91.41 91.78 91.04 44.04 1.8 Charniak-S 91.02 91.16 90.89 40.77 1.8 S P NP VP PRP VBD He was STANDARD PARSERS Berkeley Charniak SSN BUBS Bikel Collins-3 Collins-2 Collins-1 Stanford-L Stanford-U 90.06 89.71 89.42 88.50 88.16 87.66 87.62 87.09 86.42 85.78 90.30 89.88 89.96 88.57 88.23 87.82 87.77 87.29 86.35 86.48 89."
D12-1096,J93-2004,0,0.04189,"Missing"
D12-1096,N06-1020,0,0.214552,"with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSN (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. System F R Exact Speed ENHANCED TRAINING / SYSTEMS Charniak-SR 92.07 92.44"
D12-1096,D07-1013,0,0.0423525,"method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk 1049 and Corston-Oliver, 2006), information extraction (Miyao et al., 2008), textual entailment (Yuret et al., 2010), or semantic depend"
D12-1096,P08-1006,0,0.0174097,"2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk 1049 and Corston-Oliver, 2006), information extraction (Miyao et al., 2008), textual entailment (Yuret et al., 2010), or semantic dependencies (Dridan and Oepen, 2011). While some of these approaches give a better sense of the impact of parse errors, they require integration into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range o"
D12-1096,U10-1014,1,0.836424,"nge of features that can be em1055 ployed. One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000). While re-ranking has led to gains in performance (Charniak and Johnson, 2005), there has been limited analysis of how effectively rerankers are using the set of available options. Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008; Ng et al., 2010; Auli and Lopez, 2011; Ng and Curran, 2012). In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker. The oracle results use the parse in each K-best list with the highest F-score. While this may not give the true oracle result, as F-score does not factor over sentences, it gives a close approximation. The table has the same columns as Table 2, but the ranges on the bars now reflect the min and max for these sets. While there is improvement on all errors when using the reranker, there is very little additional gain beyond the first"
D12-1096,C10-1094,0,0.0190793,"Missing"
D12-1096,N07-1051,1,0.581759,"into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models,"
D12-1096,P06-1055,1,0.700645,"y require integration into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised pa"
D12-1096,W06-1608,0,0.0132702,"Missing"
D12-1096,W11-2907,0,0.01367,"l indicator of overall performance, but does not pinpoint which structures the parser has most difficulty with. Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008;"
D12-1096,S10-1009,0,0.0191066,"Missing"
D12-1105,P89-1018,0,0.38798,"re 2(b) illustrates this approximation. Here, q(T ) is a product of M structurally identical factors, one for each of the annotated components. We will approximate each model fm by its corresponding f˜m . Thus, there is one colorcoordinated approximate factor for each component of the model in Figure 2(a). There are multiple choices for the structure of these factors, but we focus on anchored PCFGs. Anchored PCFGs have productions of the form i Aj → i Bk k Cj , where i, k, and j are indexes into the sentence. Here, i Aj is a symbol representing building the base symbol A over the span [i, j]. Billott and Lang (1989) introduced anchored CFGs as “shared forests,” and Matsuzaki et al. (2005) have previously used these grammars for finding an approximate one-best tree in a latent variable parser. Note that, even though an anchored grammar is unannotated, because it is sentence specific it can represent many complex properties of the full grammar’s posterior distribution for a given sentence. For example, it might express a preference for whether a PP token attaches to a particular verb or to that verb’s object noun phrase in a particular sentence. Before continuing, note that a pointwise product of anchored"
D12-1105,N12-1004,1,0.83832,"M is the number of models, and A is the maximum number of annotations for any given rule. 5.5 Other Inference Algorithms To our knowledge, expectation propagation has been used only once in the NLP community; Daum´e III and Marcu (2006) employed an unstructured version in a Bayesian model of extractive summarization. Therefore, it is worth describing how EP differs from more familiar techniques. EP can be thought of as a more flexible generalization of belief propagation, which has been used several times in NLP (Smith and Eisner, 2008; Niehues and Vogel, 2008; Cromi`eres and Kurohashi, 2009; Burkett and Klein, 2012). In particular, EP allows for the arbitrary choice of messages (the f˜m ), meaning that we can use structured messages like anchored PCFGs. Mean field (Saul and Jordan, 1996) is another approximate inference technique that allows for structured approximations (Xing et al., 2003; Burkett et al., 2010), but here the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do not pursue that approach here. Dual decomposition (Dantzig and Wolfe, 1960; Komodak"
D12-1105,N10-1015,1,0.854699,"ummarization. Therefore, it is worth describing how EP differs from more familiar techniques. EP can be thought of as a more flexible generalization of belief propagation, which has been used several times in NLP (Smith and Eisner, 2008; Niehues and Vogel, 2008; Cromi`eres and Kurohashi, 2009; Burkett and Klein, 2012). In particular, EP allows for the arbitrary choice of messages (the f˜m ), meaning that we can use structured messages like anchored PCFGs. Mean field (Saul and Jordan, 1996) is another approximate inference technique that allows for structured approximations (Xing et al., 2003; Burkett et al., 2010), but here the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do not pursue that approach here. Dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) has recently become popular in the community (Rush et al., 2010; Koo et al., 2010). In fact, EP can be seen as a particular kind of dual decomposition of the log normalization constant log Z(w, θ) that is optimized with message passing rather than (sub-)gradient descent or LP relaxati"
D12-1105,P97-1003,0,0.620878,"paper, we argue for grammars with factored annotations, that is, grammars with annotations that have structured component parts that are partially decoupled. Our annotated grammars can include both latent and explicit annotations, as illustrated in Figure 1(d), and we demonstrate that these factored grammars outperform parsers with unstructured annotations. Introduction Many high-performance PCFG parsers take an initially simple base grammar over treebank labels like NP and enrich it with deeper syntactic features to improve accuracy. This broad characterization includes lexicalized parsers (Collins, 1997), unlexicalized parsers (Klein and Manning, 2003), and latent variable parsers (Matsuzaki et al., 2005). Figures 1(a), 1(b), and 1(c) show small examples of contextfree trees that have been annotated in these ways. When multi-part annotations are used in the same grammar, systems have generally multiplied these annotations together, in the sense that an NP that After discussing the factored representation, we describe a method for parsing with factored annotations, using an approximate inference technique called expectation propagation (Minka, 2001). Our algorithm has runtime linear in the num"
D12-1105,E09-1020,0,0.0699767,"Missing"
D12-1105,J85-1006,0,0.738824,", 2009; Burkett and Klein, 2012). In particular, EP allows for the arbitrary choice of messages (the f˜m ), meaning that we can use structured messages like anchored PCFGs. Mean field (Saul and Jordan, 1996) is another approximate inference technique that allows for structured approximations (Xing et al., 2003; Burkett et al., 2010), but here the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do not pursue that approach here. Dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) has recently become popular in the community (Rush et al., 2010; Koo et al., 2010). In fact, EP can be seen as a particular kind of dual decomposition of the log normalization constant log Z(w, θ) that is optimized with message passing rather than (sub-)gradient descent or LP relaxations. Indeed, Minka (2001) argues that the EP objective is more efficiently optimized with message 1153 passing than with gradient updates. This assertion should be examined for the structured models common in NLP, but that is beyond the scope of this paper. Finally, note that EP, like bel"
D12-1105,P06-1039,0,0.11263,"Missing"
D12-1105,W06-1638,0,0.0194603,"s HPSG (Pollard and Sag, 1994) and Minimalism (Chomsky, 1992) do not ascribe unstructured conjunctions of annotations to phrasal categories. Rather, phrasal categories are associated with sequences of metadata that control their function. For instance, an NP might have annotations to the effect that it is singular, masculine, and nominative, with perhaps further information about its animacy or other aspects of the head noun. Thus, it is appealing for a grammar to be able to model these (somewhat) orthogonal notions, but most models have no mechanism to encourage this. As a notable exception, Dreyer and Eisner (2006) tried to capture this kind of insight by allowing factored annotations to pass unchanged from parent label to child label, though they were not 1147 able to demonstrate substantial gains in accuracy. Moreover, there has been to our knowledge no attempt to employ both latent and non-latent annotations at the same time. There is good reason for this: lexicalized or highly annotated grammars like those of Collins (1997) or Klein and Manning (2003) have a very large number of states and an even larger number of rules. Further annotating these rules with latent annotations would produce an infeasi"
D12-1105,P08-1109,0,0.214041,"Missing"
D12-1105,P96-1024,0,0.0159095,"ize trees using Collins’ head rules (Collins, 1997). Each discriminative parser was trained using the Adaptive Gradient variant of Stochastic Gradient Descent (Duchi et al., 2010). Smaller models were seeded from larger models. That is, before training a grammar of 5 models with 1 latent bit each, we started with weights from a parser with 4 factored bits. Initial experiments suggested this step did not affect final performance, but greatly decreased total training time, especially for the latent variable parsers. For extracting a one-best tree, we use a version of the Max-Recall algorithm of Goodman (1996). When using EP or ADF, we initialized the core approximation q to the uniform distribution over unpruned trees. ADF 84.3 84.1 83.8 82.3 Parsing EP Exact 84.5 84.5 84.6 84.5 84.5 84.9 82.1 82.2 90 Petrov 82.5 78.7 81.5 82.6 88 F1 Training ADF EP Exact Indep. 84 Table 1: The effect of algorithm choice for training and parsing on a product of two 2-state parsers on F1. Petrov is the product parser of Petrov (2010), and Indep. refers to independently trained models. For comparison, a fourstate parser achieves a score of 83.2. When counting parameters, we consider the number of parameters per bina"
D12-1105,P03-1054,1,0.387408,"P-dominated would have a single unstructured PCFG symbol that encoded all three facts. In addition, modulo backoff or smoothing, that unstructured symbol would often have rewrite parameters entirely distinct from, say, the indefinite but otherwise similar variant of the symbol (Klein and Manning, 2003). Therefore, when designing a grammar, one would have to carefully weigh new contextual annotations. Should a definiteness annotation be included, doubling the number of NPs in the grammar and perhaps overly fragmenting statistics? Or should it be excluded, thereby losing important distinctions? Klein and Manning (2003) discuss exactly such trade-offs and omit annotations that were helpful on their own because they were not worth the combinatorial or statistical cost when combined with other annotations. PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguisticallymotivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm tha"
D12-1105,D10-1125,0,0.0356074,"g that we can use structured messages like anchored PCFGs. Mean field (Saul and Jordan, 1996) is another approximate inference technique that allows for structured approximations (Xing et al., 2003; Burkett et al., 2010), but here the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do not pursue that approach here. Dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) has recently become popular in the community (Rush et al., 2010; Koo et al., 2010). In fact, EP can be seen as a particular kind of dual decomposition of the log normalization constant log Z(w, θ) that is optimized with message passing rather than (sub-)gradient descent or LP relaxations. Indeed, Minka (2001) argues that the EP objective is more efficiently optimized with message 1153 passing than with gradient updates. This assertion should be examined for the structured models common in NLP, but that is beyond the scope of this paper. Finally, note that EP, like belief propagation but unlike mean field, is not guaranteed to converge, though in practice it usually seems to"
D12-1105,J93-2004,0,0.0407954,"agenda The president’s agenda Figure 1: Parse trees using four different annotation schemes: (a) Lexicalized annotation like that in Collins (1997); (b) Unlexicalized annotation like that in Klein and Manning (2003); (c) Latent annotation like that in Matsuzaki et al. (2005); and (d) the factored, mixed annotations we argue for in our paper. We demonstrate the empirical effectiveness of our approach in two ways. First, we efficiently train a latent-variable grammar with 8 disjoint one-bit latent annotation factors, with scores as high as 89.7 F1 on length ≤40 sentences from the Penn Treebank (Marcus et al., 1993). This latent variable parser outscores the best of Petrov and Klein (2008a)’s comparable parsers while using two orders of magnitude fewer parameters. Second, we combine our latent variable factors with lexicalized and unlexicalized annotations, resulting in our best F1 score of 89.4 on all sentences. 2 Intuitions Modern theories of grammar such as HPSG (Pollard and Sag, 1994) and Minimalism (Chomsky, 1992) do not ascribe unstructured conjunctions of annotations to phrasal categories. Rather, phrasal categories are associated with sequences of metadata that control their function. For instanc"
D12-1105,P05-1010,0,0.678497,"at have structured component parts that are partially decoupled. Our annotated grammars can include both latent and explicit annotations, as illustrated in Figure 1(d), and we demonstrate that these factored grammars outperform parsers with unstructured annotations. Introduction Many high-performance PCFG parsers take an initially simple base grammar over treebank labels like NP and enrich it with deeper syntactic features to improve accuracy. This broad characterization includes lexicalized parsers (Collins, 1997), unlexicalized parsers (Klein and Manning, 2003), and latent variable parsers (Matsuzaki et al., 2005). Figures 1(a), 1(b), and 1(c) show small examples of contextfree trees that have been annotated in these ways. When multi-part annotations are used in the same grammar, systems have generally multiplied these annotations together, in the sense that an NP that After discussing the factored representation, we describe a method for parsing with factored annotations, using an approximate inference technique called expectation propagation (Minka, 2001). Our algorithm has runtime linear in the number of annotation factors in the grammar, improving on the na¨ıve algorithm, which has runtime exponent"
D12-1105,W08-0303,0,0.0293226,"M AG0 |w |, where I is the maximum number of iterations, M is the number of models, and A is the maximum number of annotations for any given rule. 5.5 Other Inference Algorithms To our knowledge, expectation propagation has been used only once in the NLP community; Daum´e III and Marcu (2006) employed an unstructured version in a Bayesian model of extractive summarization. Therefore, it is worth describing how EP differs from more familiar techniques. EP can be thought of as a more flexible generalization of belief propagation, which has been used several times in NLP (Smith and Eisner, 2008; Niehues and Vogel, 2008; Cromi`eres and Kurohashi, 2009; Burkett and Klein, 2012). In particular, EP allows for the arbitrary choice of messages (the f˜m ), meaning that we can use structured messages like anchored PCFGs. Mean field (Saul and Jordan, 1996) is another approximate inference technique that allows for structured approximations (Xing et al., 2003; Burkett et al., 2010), but here the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do not pursue that approach"
D12-1105,N07-1051,1,0.875654,"Manning (2003). Finally, we use a straightforward discriminative latent variable model much like that of Petrov and Klein (2008a). Here, each symbol is given a latent annotation, referred to as a substate. Typically, these substates correlate at least loosely with linguistic phenomena. For instance, NP-1 might be associated with possessive NPs, while NP-3 might be for adjuncts. Often, these latent integers are considered as bit strings, with each bit indicating one latent annotation. Prior work in this area has considered the effect of splitting and merging these states (Petrov et al., 2006; Petrov and Klein, 2007), as well as “multiscale” grammars (Petrov and Klein, 2008b). With two states (or one bit of annotation), our version of this parser gets 81.7 F1, edging out the comparable parser of Petrov and Klein (2008a). On the other hand, our parser gets 83.2 with four states (two bits), short of the performance of prior work.1 1 Much of the difference stems from the different binarization scheme we employ. We use head-outward binarization, rather than the left-branching binarization they employed. This change was to enable integrating lexicalization with our other models. 1148 Model Representation We em"
D12-1105,D08-1091,1,0.527927,"annotation schemes: (a) Lexicalized annotation like that in Collins (1997); (b) Unlexicalized annotation like that in Klein and Manning (2003); (c) Latent annotation like that in Matsuzaki et al. (2005); and (d) the factored, mixed annotations we argue for in our paper. We demonstrate the empirical effectiveness of our approach in two ways. First, we efficiently train a latent-variable grammar with 8 disjoint one-bit latent annotation factors, with scores as high as 89.7 F1 on length ≤40 sentences from the Penn Treebank (Marcus et al., 1993). This latent variable parser outscores the best of Petrov and Klein (2008a)’s comparable parsers while using two orders of magnitude fewer parameters. Second, we combine our latent variable factors with lexicalized and unlexicalized annotations, resulting in our best F1 score of 89.4 on all sentences. 2 Intuitions Modern theories of grammar such as HPSG (Pollard and Sag, 1994) and Minimalism (Chomsky, 1992) do not ascribe unstructured conjunctions of annotations to phrasal categories. Rather, phrasal categories are associated with sequences of metadata that control their function. For instance, an NP might have annotations to the effect that it is singular, masculi"
D12-1105,P06-1055,1,0.945151,"version of Klein and Manning (2003). Finally, we use a straightforward discriminative latent variable model much like that of Petrov and Klein (2008a). Here, each symbol is given a latent annotation, referred to as a substate. Typically, these substates correlate at least loosely with linguistic phenomena. For instance, NP-1 might be associated with possessive NPs, while NP-3 might be for adjuncts. Often, these latent integers are considered as bit strings, with each bit indicating one latent annotation. Prior work in this area has considered the effect of splitting and merging these states (Petrov et al., 2006; Petrov and Klein, 2007), as well as “multiscale” grammars (Petrov and Klein, 2008b). With two states (or one bit of annotation), our version of this parser gets 81.7 F1, edging out the comparable parser of Petrov and Klein (2008a). On the other hand, our parser gets 83.2 with four states (two bits), short of the performance of prior work.1 1 Much of the difference stems from the different binarization scheme we employ. We use head-outward binarization, rather than the left-branching binarization they employed. This change was to enable integrating lexicalization with our other models. 1148 M"
D12-1105,N10-1003,0,0.324461,"we compile all annotation components into unstructured annotations, we can end up with a total grammar size of O(AM G0 ), and so in general parsing time scales exponentially with the number of annotation components. Thus, if we use latent annotations and the hierarchical splitting approach of Petrov et al. (2006), then the grammar has size O(8S G0 ), where S is the number of times the grammar was split in two. Therefore, the size of annotated grammars can reach intractable levels very quickly, particularly in the case of latent annotations, where all combinations of annotations are possible. Petrov (2010) considered an approach to slowing this growth down by using a set of M independently trained parsers Pm , and parsed using the product of the scores from each parser as the score for the tree. This approach worked largely because training was intractable: if the training algorithm could reach the global optimum, then this approach might have yielded no gain. However, because the optimization technique is local, the same algorithm produced multiple grammars. In what follows, we propose another solution that exploits the factored structure of our grammar with expectation propagation. Crucially,"
D12-1105,D10-1001,0,0.0379081,"(the f˜m ), meaning that we can use structured messages like anchored PCFGs. Mean field (Saul and Jordan, 1996) is another approximate inference technique that allows for structured approximations (Xing et al., 2003; Burkett et al., 2010), but here the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do not pursue that approach here. Dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) has recently become popular in the community (Rush et al., 2010; Koo et al., 2010). In fact, EP can be seen as a particular kind of dual decomposition of the log normalization constant log Z(w, θ) that is optimized with message passing rather than (sub-)gradient descent or LP relaxations. Indeed, Minka (2001) argues that the EP objective is more efficiently optimized with message 1153 passing than with gradient updates. This assertion should be examined for the structured models common in NLP, but that is beyond the scope of this paper. Finally, note that EP, like belief propagation but unlike mean field, is not guaranteed to converge, though in practice"
D12-1105,D08-1016,0,0.0242937,"that takes time  3 O IM AG0 |w |, where I is the maximum number of iterations, M is the number of models, and A is the maximum number of annotations for any given rule. 5.5 Other Inference Algorithms To our knowledge, expectation propagation has been used only once in the NLP community; Daum´e III and Marcu (2006) employed an unstructured version in a Bayesian model of extractive summarization. Therefore, it is worth describing how EP differs from more familiar techniques. EP can be thought of as a more flexible generalization of belief propagation, which has been used several times in NLP (Smith and Eisner, 2008; Niehues and Vogel, 2008; Cromi`eres and Kurohashi, 2009; Burkett and Klein, 2012). In particular, EP allows for the arbitrary choice of messages (the f˜m ), meaning that we can use structured messages like anchored PCFGs. Mean field (Saul and Jordan, 1996) is another approximate inference technique that allows for structured approximations (Xing et al., 2003; Burkett et al., 2010), but here the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do"
D13-1027,D08-1031,0,0.183745,"y We analyzed all of the 2011 CoNLL task systems, as well as several publicly available systems. For the shared task systems we used the output data from the task itself, provided by the organizers. For the publicly available systems we used the default configurations. Finally, we included another run of the Stanford system, with their OntoNotes-tuned parameters (S TANFORD - T). The publicly available systems we used are: B ERKELEY (Durrett and Klein, 2013), IMS (Bj¨orkelund and Farkas, 2012), S TANFORD (Lee et al., 2013), R ECONCILE (Stoyanov et al., 2010), BART (Versley et al., 2008), UIUC (Bengtson and Roth, 2008), and C HERRY P ICKER (Rahman and Ng, 2009). The systems from the shared task are listed in Table 1 and in the references. The most frequent error across all systems is Divided Entity. Unlike parsing errors (Kummerfeld et al., 2012), improvements are not monotonic, with better systems often making more errors of one type when decreasing the frequency of another type. One outlier is the Irwin et al. (2011) system, which makes very few mistakes in five categories, but many in the last two. This reflects a high precision, low recall approach, where clusters are only formed when there is high conf"
D13-1027,W12-4503,0,0.0171127,"Missing"
D13-1027,W11-1905,0,0.0431993,"Missing"
D13-1027,W11-1907,0,0.0387752,"Missing"
D13-1027,W11-1904,0,0.0217879,"Missing"
D13-1027,W11-1915,0,0.0207774,"Missing"
D13-1027,W12-4504,0,0.0212077,"sion (Ng and Cardie, 2002; Haghighi and Klein, 2009). More fine consideration of some subtasks does occur, for example, anaphoricity detection, which has been recognized as a key challenge in coreference resolution for decades and regularly has separate results reported (Paice and Husk, 1987; Sobha et al., 2011; Yuan et al., 2012; Bj¨orkelund and Farkas, 2012; Zhekova et al., 2012). Some work has also included anecdotal discussion of specific error types or manual classification of a small set of errors, but these approaches do not effectively quantify the relative impact of different errors (Chen and Ng, 2012; Martschat et al., 2012; Haghighi and Klein, 2009). In a recent paper, Holen (2013) presented a detailed manual analysis that considered a more comprehensive set of error types, but their focus was on exploring the shortcomings of current metrics, rather than understanding the behavior of current systems. The detailed investigation presented by Stoyanov et al. (2009) is the closest to the work we present here. First, they measured accuracy improvements when their system was given gold annotations for three subtasks of coreference resolution: mention detection, named entity recognition, and an"
D13-1027,W11-1921,0,0.0387128,"Missing"
D13-1027,D13-1203,1,0.304001,"nflated Entities. Each remaining Split operation is mapped to a Conflated Entity error, e.g. Figure 3(vii), and the blue and red entities in Figure 2. 4 Methodology We analyzed all of the 2011 CoNLL task systems, as well as several publicly available systems. For the shared task systems we used the output data from the task itself, provided by the organizers. For the publicly available systems we used the default configurations. Finally, we included another run of the Stanford system, with their OntoNotes-tuned parameters (S TANFORD - T). The publicly available systems we used are: B ERKELEY (Durrett and Klein, 2013), IMS (Bj¨orkelund and Farkas, 2012), S TANFORD (Lee et al., 2013), R ECONCILE (Stoyanov et al., 2010), BART (Versley et al., 2008), UIUC (Bengtson and Roth, 2008), and C HERRY P ICKER (Rahman and Ng, 2009). The systems from the shared task are listed in Table 1 and in the references. The most frequent error across all systems is Divided Entity. Unlike parsing errors (Kummerfeld et al., 2012), improvements are not monotonic, with better systems often making more errors of one type when decreasing the frequency of another type. One outlier is the Irwin et al. (2011) system, which makes very few"
D13-1027,D09-1120,1,0.782568,"., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The only common forms of further analysis are results for anaphoricity detection and scores for each mention type (nominal, pronoun, proper). Two exceptions are: the detailed analysis of the Reconcile system by Stoyanov et al. (2009), and the multi-system comparisons in the CoNLL shared task reports (Pradhan et al., 2011, 2012). A common approach to performance analysis is to calculate scores for nominals, pronouns and proper names separately, but this is a very coarse division (Ng and Cardie, 2002; Haghighi and Klein, 2009). More fine consideration of some subtasks does occur, for example, anaphoricity detection, which has been recognized as a key challenge in coreference resolution for decades and regularly has separate results reported (Paice and Husk, 1987; Sobha et al., 2011; Yuan et al., 2012; Bj¨orkelund and Farkas, 2012; Zhekova et al., 2012). Some work has also included anecdotal discussion of specific error types or manual classification of a small set of errors, but these approaches do not effectively quantify the relative impact of different errors (Chen and Ng, 2012; Martschat et al., 2012; Haghighi"
D13-1027,N13-2001,0,0.0612545,"y improvements when partial gold annotations are provided (Stoyanov et al., 2009; Pradhan et al., 2011; Pradhan et al., 2012). For coreference resolution the drawback of this approach is that decisions are often interdependent, and so even partial gold information is extremely informative. Also, previous work only considered errors by counting links, which does not capture certain errors in a natural way, e.g. when a system incorrectly divides a large entity into two parts, each with multiple mentions. Recent work has considered some of these issues, but only with small scale manual analysis (Holen, 2013). Using our tool we perform two studies to understand similarities and differences between systems. First, we compare the error distributions on coreference resolution of all of the systems from the CoNLL 2011 shared task plus several publicly available systems. This comparison adds to the analysis from the shared task by illustrating the substantial variation in the types of errors different systems make. Second, we investigate the aggregate behavior of ten state-of-the-art systems, providing a detailed characterization of each error type. This investigation identifies key outstanding challen"
D13-1027,N06-2015,0,0.0153185,"ill Missing Mentions and Missing Entities because systems do not always choose an antecedent, leaving a mention as a singleton, which is then ignored. While this broad comparison gives a complete view of the range of errors present, it is still a coarse representation. In the next section, we characterize the common errors on a finer level by breaking down each error type by a range of properties. 5 6 Our tool processes the CoNLL task output, with no other information required. During development, and when choosing examples for this paper, we used the development set of the CoNLL shared task (Hovy et al., 2006; Pradhan et al., 2007; Pradhan et al., 2011). The results we present in the rest of the paper are all for the test set. Using the development set would have been misleading, as the entrants in the shared task used it to tune their systems. 4.1 Systems Broad System Comparison Table 1 presents the frequency of errors for each system and F-Scores for standard metrics1 on the test set of the 2011 CoNLL shared task. Each bar is filled in proportion to the number of errors the system made, with a full bar corresponding to the number of errors listed in the bottom row. The metrics provide an effecti"
D13-1027,W11-1913,0,0.0168803,"we used are: B ERKELEY (Durrett and Klein, 2013), IMS (Bj¨orkelund and Farkas, 2012), S TANFORD (Lee et al., 2013), R ECONCILE (Stoyanov et al., 2010), BART (Versley et al., 2008), UIUC (Bengtson and Roth, 2008), and C HERRY P ICKER (Rahman and Ng, 2009). The systems from the shared task are listed in Table 1 and in the references. The most frequent error across all systems is Divided Entity. Unlike parsing errors (Kummerfeld et al., 2012), improvements are not monotonic, with better systems often making more errors of one type when decreasing the frequency of another type. One outlier is the Irwin et al. (2011) system, which makes very few mistakes in five categories, but many in the last two. This reflects a high precision, low recall approach, where clusters are only formed when there is high confidence. The third section of Table 1 shows results for systems that were run with gold noun phrase span information. This reduces all errors slightly, though most noticeably Extra Mention, Missing Mention, and Span Error. On inspection of the remaining Span Errors we found that many are due to inconsistencies regarding the inclusion of the possessive. The final section of the table shows results for syste"
D13-1027,W11-1912,0,0.0387162,"Missing"
D13-1027,W11-1910,0,0.034988,"Missing"
D13-1027,W11-1916,1,0.897324,"Missing"
D13-1027,D12-1096,1,0.724273,"s we used the default configurations. Finally, we included another run of the Stanford system, with their OntoNotes-tuned parameters (S TANFORD - T). The publicly available systems we used are: B ERKELEY (Durrett and Klein, 2013), IMS (Bj¨orkelund and Farkas, 2012), S TANFORD (Lee et al., 2013), R ECONCILE (Stoyanov et al., 2010), BART (Versley et al., 2008), UIUC (Bengtson and Roth, 2008), and C HERRY P ICKER (Rahman and Ng, 2009). The systems from the shared task are listed in Table 1 and in the references. The most frequent error across all systems is Divided Entity. Unlike parsing errors (Kummerfeld et al., 2012), improvements are not monotonic, with better systems often making more errors of one type when decreasing the frequency of another type. One outlier is the Irwin et al. (2011) system, which makes very few mistakes in five categories, but many in the last two. This reflects a high precision, low recall approach, where clusters are only formed when there is high confidence. The third section of Table 1 shows results for systems that were run with gold noun phrase span information. This reduces all errors slightly, though most noticeably Extra Mention, Missing Mention, and Span Error. On inspect"
D13-1027,W11-1914,0,0.0182854,"Missing"
D13-1027,W11-1902,0,0.200383,"Missing"
D13-1027,J13-4004,0,0.0131888,"Entity error, e.g. Figure 3(vii), and the blue and red entities in Figure 2. 4 Methodology We analyzed all of the 2011 CoNLL task systems, as well as several publicly available systems. For the shared task systems we used the output data from the task itself, provided by the organizers. For the publicly available systems we used the default configurations. Finally, we included another run of the Stanford system, with their OntoNotes-tuned parameters (S TANFORD - T). The publicly available systems we used are: B ERKELEY (Durrett and Klein, 2013), IMS (Bj¨orkelund and Farkas, 2012), S TANFORD (Lee et al., 2013), R ECONCILE (Stoyanov et al., 2010), BART (Versley et al., 2008), UIUC (Bengtson and Roth, 2008), and C HERRY P ICKER (Rahman and Ng, 2009). The systems from the shared task are listed in Table 1 and in the references. The most frequent error across all systems is Divided Entity. Unlike parsing errors (Kummerfeld et al., 2012), improvements are not monotonic, with better systems often making more errors of one type when decreasing the frequency of another type. One outlier is the Irwin et al. (2011) system, which makes very few mistakes in five categories, but many in the last two. This refle"
D13-1027,W11-1917,0,0.0423735,"Missing"
D13-1027,H05-1004,0,0.12809,"t analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task. 1 Introduction Metrics produce measurements that concisely summarize performance on the full range of error types, and for coreference resolution there has been extensive work on developing effective metrics (Luo, 2005; Recasens and Hovy, 2011). However, it is also valuable to tease apart the errors to understand their relative importance. Previous investigations of coreference errors have focused on quantifying the importance of subtasks such as named entity recognition and anaphoricity detection, typically by measuring accuracy improvements when partial gold annotations are provided (Stoyanov et al., 2009; Pradhan et al., 2011; Pradhan et al., 2012). For coreference resolution the drawback of this approach is that decisions are often interdependent, and so even partial gold information is extremely inform"
D13-1027,W12-4511,0,0.0409602,"Missing"
D13-1027,P02-1014,0,0.362954,"as MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The only common forms of further analysis are results for anaphoricity detection and scores for each mention type (nominal, pronoun, proper). Two exceptions are: the detailed analysis of the Reconcile system by Stoyanov et al. (2009), and the multi-system comparisons in the CoNLL shared task reports (Pradhan et al., 2011, 2012). A common approach to performance analysis is to calculate scores for nominals, pronouns and proper names separately, but this is a very coarse division (Ng and Cardie, 2002; Haghighi and Klein, 2009). More fine consideration of some subtasks does occur, for example, anaphoricity detection, which has been recognized as a key challenge in coreference resolution for decades and regularly has separate results reported (Paice and Husk, 1987; Sobha et al., 2011; Yuan et al., 2012; Bj¨orkelund and Farkas, 2012; Zhekova et al., 2012). Some work has also included anecdotal discussion of specific error types or manual classification of a small set of errors, but these approaches do not effectively quantify the relative impact of different errors (Chen and Ng, 2012; Martsc"
D13-1027,W11-1906,0,0.0454902,"Missing"
D13-1027,W11-1901,0,0.423362,"etrics produce measurements that concisely summarize performance on the full range of error types, and for coreference resolution there has been extensive work on developing effective metrics (Luo, 2005; Recasens and Hovy, 2011). However, it is also valuable to tease apart the errors to understand their relative importance. Previous investigations of coreference errors have focused on quantifying the importance of subtasks such as named entity recognition and anaphoricity detection, typically by measuring accuracy improvements when partial gold annotations are provided (Stoyanov et al., 2009; Pradhan et al., 2011; Pradhan et al., 2012). For coreference resolution the drawback of this approach is that decisions are often interdependent, and so even partial gold information is extremely informative. Also, previous work only considered errors by counting links, which does not capture certain errors in a natural way, e.g. when a system incorrectly divides a large entity into two parts, each with multiple mentions. Recent work has considered some of these issues, but only with small scale manual analysis (Holen, 2013). Using our tool we perform two studies to understand similarities and differences between"
D13-1027,W12-4501,0,0.176046,"ments that concisely summarize performance on the full range of error types, and for coreference resolution there has been extensive work on developing effective metrics (Luo, 2005; Recasens and Hovy, 2011). However, it is also valuable to tease apart the errors to understand their relative importance. Previous investigations of coreference errors have focused on quantifying the importance of subtasks such as named entity recognition and anaphoricity detection, typically by measuring accuracy improvements when partial gold annotations are provided (Stoyanov et al., 2009; Pradhan et al., 2011; Pradhan et al., 2012). For coreference resolution the drawback of this approach is that decisions are often interdependent, and so even partial gold information is extremely informative. Also, previous work only considered errors by counting links, which does not capture certain errors in a natural way, e.g. when a system incorrectly divides a large entity into two parts, each with multiple mentions. Recent work has considered some of these issues, but only with small scale manual analysis (Holen, 2013). Using our tool we perform two studies to understand similarities and differences between systems. First, we com"
D13-1027,D09-1101,0,0.148211,"s, as well as several publicly available systems. For the shared task systems we used the output data from the task itself, provided by the organizers. For the publicly available systems we used the default configurations. Finally, we included another run of the Stanford system, with their OntoNotes-tuned parameters (S TANFORD - T). The publicly available systems we used are: B ERKELEY (Durrett and Klein, 2013), IMS (Bj¨orkelund and Farkas, 2012), S TANFORD (Lee et al., 2013), R ECONCILE (Stoyanov et al., 2010), BART (Versley et al., 2008), UIUC (Bengtson and Roth, 2008), and C HERRY P ICKER (Rahman and Ng, 2009). The systems from the shared task are listed in Table 1 and in the references. The most frequent error across all systems is Divided Entity. Unlike parsing errors (Kummerfeld et al., 2012), improvements are not monotonic, with better systems often making more errors of one type when decreasing the frequency of another type. One outlier is the Irwin et al. (2011) system, which makes very few mistakes in five categories, but many in the last two. This reflects a high precision, low recall approach, where clusters are only formed when there is high confidence. The third section of Table 1 shows"
D13-1027,W11-1903,0,0.0425142,"Missing"
D13-1027,W11-1922,0,0.0230353,"Missing"
D13-1027,P09-1074,0,0.473254,"task. 1 Introduction Metrics produce measurements that concisely summarize performance on the full range of error types, and for coreference resolution there has been extensive work on developing effective metrics (Luo, 2005; Recasens and Hovy, 2011). However, it is also valuable to tease apart the errors to understand their relative importance. Previous investigations of coreference errors have focused on quantifying the importance of subtasks such as named entity recognition and anaphoricity detection, typically by measuring accuracy improvements when partial gold annotations are provided (Stoyanov et al., 2009; Pradhan et al., 2011; Pradhan et al., 2012). For coreference resolution the drawback of this approach is that decisions are often interdependent, and so even partial gold information is extremely informative. Also, previous work only considered errors by counting links, which does not capture certain errors in a natural way, e.g. when a system incorrectly divides a large entity into two parts, each with multiple mentions. Recent work has considered some of these issues, but only with small scale manual analysis (Holen, 2013). Using our tool we perform two studies to understand similarities a"
D13-1027,P10-2029,0,0.0632463,"i), and the blue and red entities in Figure 2. 4 Methodology We analyzed all of the 2011 CoNLL task systems, as well as several publicly available systems. For the shared task systems we used the output data from the task itself, provided by the organizers. For the publicly available systems we used the default configurations. Finally, we included another run of the Stanford system, with their OntoNotes-tuned parameters (S TANFORD - T). The publicly available systems we used are: B ERKELEY (Durrett and Klein, 2013), IMS (Bj¨orkelund and Farkas, 2012), S TANFORD (Lee et al., 2013), R ECONCILE (Stoyanov et al., 2010), BART (Versley et al., 2008), UIUC (Bengtson and Roth, 2008), and C HERRY P ICKER (Rahman and Ng, 2009). The systems from the shared task are listed in Table 1 and in the references. The most frequent error across all systems is Divided Entity. Unlike parsing errors (Kummerfeld et al., 2012), improvements are not monotonic, with better systems often making more errors of one type when decreasing the frequency of another type. One outlier is the Irwin et al. (2011) system, which makes very few mistakes in five categories, but many in the last two. This reflects a high precision, low recall app"
D13-1027,W11-1920,0,0.0411596,"Missing"
D13-1027,W11-1908,0,0.031567,"Missing"
D13-1027,P08-4003,0,0.0255457,"ies in Figure 2. 4 Methodology We analyzed all of the 2011 CoNLL task systems, as well as several publicly available systems. For the shared task systems we used the output data from the task itself, provided by the organizers. For the publicly available systems we used the default configurations. Finally, we included another run of the Stanford system, with their OntoNotes-tuned parameters (S TANFORD - T). The publicly available systems we used are: B ERKELEY (Durrett and Klein, 2013), IMS (Bj¨orkelund and Farkas, 2012), S TANFORD (Lee et al., 2013), R ECONCILE (Stoyanov et al., 2010), BART (Versley et al., 2008), UIUC (Bengtson and Roth, 2008), and C HERRY P ICKER (Rahman and Ng, 2009). The systems from the shared task are listed in Table 1 and in the references. The most frequent error across all systems is Divided Entity. Unlike parsing errors (Kummerfeld et al., 2012), improvements are not monotonic, with better systems often making more errors of one type when decreasing the frequency of another type. One outlier is the Irwin et al. (2011) system, which makes very few mistakes in five categories, but many in the last two. This reflects a high precision, low recall approach, where clusters are onl"
D13-1027,M95-1005,0,0.631901,"tities that contain a small number of mentions. This work presents a comprehensive investigation of common errors in coreference resolution, identifying particular issues worth focusing on in future research. Our analysis tool is available at code.google.com/p/berkeley-coreference-analyser/. 265 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 265–277, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Background Most coreference work focuses on accuracy improvements, as measured by metrics such as MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The only common forms of further analysis are results for anaphoricity detection and scores for each mention type (nominal, pronoun, proper). Two exceptions are: the detailed analysis of the Reconcile system by Stoyanov et al. (2009), and the multi-system comparisons in the CoNLL shared task reports (Pradhan et al., 2011, 2012). A common approach to performance analysis is to calculate scores for nominals, pronouns and proper names separately, but this is a very coarse division (Ng and Cardie, 2002; Haghighi"
D13-1027,W11-1911,0,0.0503829,"Missing"
D13-1027,W11-1919,0,0.0292982,"Missing"
D13-1027,W12-4507,0,0.0108366,"econcile system by Stoyanov et al. (2009), and the multi-system comparisons in the CoNLL shared task reports (Pradhan et al., 2011, 2012). A common approach to performance analysis is to calculate scores for nominals, pronouns and proper names separately, but this is a very coarse division (Ng and Cardie, 2002; Haghighi and Klein, 2009). More fine consideration of some subtasks does occur, for example, anaphoricity detection, which has been recognized as a key challenge in coreference resolution for decades and regularly has separate results reported (Paice and Husk, 1987; Sobha et al., 2011; Yuan et al., 2012; Bj¨orkelund and Farkas, 2012; Zhekova et al., 2012). Some work has also included anecdotal discussion of specific error types or manual classification of a small set of errors, but these approaches do not effectively quantify the relative impact of different errors (Chen and Ng, 2012; Martschat et al., 2012; Haghighi and Klein, 2009). In a recent paper, Holen (2013) presented a detailed manual analysis that considered a more comprehensive set of error types, but their focus was on exploring the shortcomings of current metrics, rather than understanding the behavior of current systems. The de"
D13-1027,W11-1918,0,0.0243807,"Missing"
D13-1027,W12-4509,0,0.0282044,"Missing"
D13-1027,W11-1909,0,0.127598,"Missing"
D13-1027,D08-1067,0,\N,Missing
D13-1087,D11-1029,1,0.884174,"homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340. 2 1 Introduction What can a million restarts do for decipherment? EM frequently gets stuck in local optima, so running between ten and a hundred random restarts is common practice (Knight et al., 2006; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). But, how important are random restarts and how many random restarts does it take to saturate gains in accuracy? We find that the answer depends on the cipher. We look at both Zodiac 408, a famous homophonic substitution cipher, and a more difficult homophonic cipher constructed to match properties of the famously unsolved Zodiac 340. Gains in accuracy saturate after only a hundred random restarts for Zodiac 408, but for the constructed cipher we see large gains Decipherment Model Various types of ciphers have been tackled by the NLP community with great success (Knight et al., 2006; Snyder e"
D13-1087,P06-2065,0,0.439962,"random restarts. For particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340. 2 1 Introduction What can a million restarts do for decipherment? EM frequently gets stuck in local optima, so running between ten and a hundred random restarts is common practice (Knight et al., 2006; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). But, how important are random restarts and how many random restarts does it take to saturate gains in accuracy? We find that the answer depends on the cipher. We look at both Zodiac 408, a famous homophonic substitution cipher, and a more difficult homophonic cipher constructed to match properties of the famously unsolved Zodiac 340. Gains in accuracy saturate after only a hundred random restarts for Zodiac 408, but for the constructed cipher we see large gains Decipherment Model Various types of ciphers have been tackled by the NLP c"
D13-1087,D08-1085,0,0.154595,"Missing"
D13-1087,P11-1025,0,0.318369,"particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340. 2 1 Introduction What can a million restarts do for decipherment? EM frequently gets stuck in local optima, so running between ten and a hundred random restarts is common practice (Knight et al., 2006; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). But, how important are random restarts and how many random restarts does it take to saturate gains in accuracy? We find that the answer depends on the cipher. We look at both Zodiac 408, a famous homophonic substitution cipher, and a more difficult homophonic cipher constructed to match properties of the famously unsolved Zodiac 340. Gains in accuracy saturate after only a hundred random restarts for Zodiac 408, but for the constructed cipher we see large gains Decipherment Model Various types of ciphers have been tackled by the NLP community with great suc"
D13-1087,P10-1107,0,0.223343,"n, 2011). But, how important are random restarts and how many random restarts does it take to saturate gains in accuracy? We find that the answer depends on the cipher. We look at both Zodiac 408, a famous homophonic substitution cipher, and a more difficult homophonic cipher constructed to match properties of the famously unsolved Zodiac 340. Gains in accuracy saturate after only a hundred random restarts for Zodiac 408, but for the constructed cipher we see large gains Decipherment Model Various types of ciphers have been tackled by the NLP community with great success (Knight et al., 2006; Snyder et al., 2010; Ravi and Knight, 2011). Many of these approaches learn an encryption key by maximizing the score of the decrypted message under a language model. We focus on homophonic substitution ciphers, where the encryption key is a 1-to-many mapping from a plaintext alphabet to a cipher alphabet. We use a simple method introduced by Knight et al. (2006): the EM algorithm (Dempster et al., 1977) is used to learn the emission parameters of an HMM that has a character trigram language model as a backbone and the ciphertext as the observed sequence of emissions. This means that we learn a multinomial over"
D13-1195,U11-1006,0,0.094359,"These parameters suggest a set of design principles for peak performance: 1900 4. Minimize main memory access and use L2 cache to speed it up. Lets look in more detail at how to achieve this. 3 Anatomy of an Efficient GPU Parser High performance on the GPU requires us to minimize code divergence. This suggests that we do not use a lexicalized grammar or a grammar that is sensitive to the position of a span within the sentence. These kinds of grammars—while highly accurate— have irregular memory access patterns that conflict with SIMD execution. Instead, an unlexicalized approach like that of Johnson (2011) or Klein and Manning (2003), or a latent variable approach like that of Matsuzaki et al. (2005) or Petrov et al. (2006) are more appropriate. We opt for the latter kind: latent variable grammars are fairly small, and their accuracies rival lexicalized approaches. Our GPU-ized inside algorithm maintains two data structures: parse charts that store scores for each labeled span, as usual, and a “workspace” that is used to actually perform the updates of the inside algorithm. Schematically, this memory layout is represented in Figure 1. A queue is maintained CPU-side that enqueues work items of t"
D13-1195,P03-1054,1,0.0323976,"suggest a set of design principles for peak performance: 1900 4. Minimize main memory access and use L2 cache to speed it up. Lets look in more detail at how to achieve this. 3 Anatomy of an Efficient GPU Parser High performance on the GPU requires us to minimize code divergence. This suggests that we do not use a lexicalized grammar or a grammar that is sensitive to the position of a span within the sentence. These kinds of grammars—while highly accurate— have irregular memory access patterns that conflict with SIMD execution. Instead, an unlexicalized approach like that of Johnson (2011) or Klein and Manning (2003), or a latent variable approach like that of Matsuzaki et al. (2005) or Petrov et al. (2006) are more appropriate. We opt for the latter kind: latent variable grammars are fairly small, and their accuracies rival lexicalized approaches. Our GPU-ized inside algorithm maintains two data structures: parse charts that store scores for each labeled span, as usual, and a “workspace” that is used to actually perform the updates of the inside algorithm. Schematically, this memory layout is represented in Figure 1. A queue is maintained CPU-side that enqueues work items of the form (s, p, l, r), where"
D13-1195,J93-2004,0,0.0430066,"Missing"
D13-1195,P05-1010,0,0.0412912,"mize main memory access and use L2 cache to speed it up. Lets look in more detail at how to achieve this. 3 Anatomy of an Efficient GPU Parser High performance on the GPU requires us to minimize code divergence. This suggests that we do not use a lexicalized grammar or a grammar that is sensitive to the position of a span within the sentence. These kinds of grammars—while highly accurate— have irregular memory access patterns that conflict with SIMD execution. Instead, an unlexicalized approach like that of Johnson (2011) or Klein and Manning (2003), or a latent variable approach like that of Matsuzaki et al. (2005) or Petrov et al. (2006) are more appropriate. We opt for the latter kind: latent variable grammars are fairly small, and their accuracies rival lexicalized approaches. Our GPU-ized inside algorithm maintains two data structures: parse charts that store scores for each labeled span, as usual, and a “workspace” that is used to actually perform the updates of the inside algorithm. Schematically, this memory layout is represented in Figure 1. A queue is maintained CPU-side that enqueues work items of the form (s, p, l, r), where s is a sentence, and p, l, and r specify the index in the parse char"
D13-1195,P06-1055,1,0.645001,"use L2 cache to speed it up. Lets look in more detail at how to achieve this. 3 Anatomy of an Efficient GPU Parser High performance on the GPU requires us to minimize code divergence. This suggests that we do not use a lexicalized grammar or a grammar that is sensitive to the position of a span within the sentence. These kinds of grammars—while highly accurate— have irregular memory access patterns that conflict with SIMD execution. Instead, an unlexicalized approach like that of Johnson (2011) or Klein and Manning (2003), or a latent variable approach like that of Matsuzaki et al. (2005) or Petrov et al. (2006) are more appropriate. We opt for the latter kind: latent variable grammars are fairly small, and their accuracies rival lexicalized approaches. Our GPU-ized inside algorithm maintains two data structures: parse charts that store scores for each labeled span, as usual, and a “workspace” that is used to actually perform the updates of the inside algorithm. Schematically, this memory layout is represented in Figure 1. A queue is maintained CPU-side that enqueues work items of the form (s, p, l, r), where s is a sentence, and p, l, and r specify the index in the parse chart for parent, left child"
D13-1195,W11-2921,0,0.504011,"Constituency parsing with high accuracy (e.g. latent variable) grammars remains a computational challenge. The O(Gs3 ) complexity of full CKY parsing for a grammar with G rules and sentence length s, is daunting. Even with a host of pruning heuristics, the high cost of constituency parsing limits its uses. The most recent Berkeley latent variable grammar for instance, has 1.7 million rules and requires about a billion rule evaluations for inside scoring of a single length-30 sentence. GPUs have previously been used to accelerate CKY evaluation, but gains over CPU parsers were modest. e.g. in Yi et al. (2011) a GPU parser is described for the Berkeley Parser grammar which achieves 5 sentences per second on the first 1000 sentences of Penn Treebank 1. Grammar compilation, which allows registerto-register code for application of grammar rules. This gives an order of magnitude (10x) speedup over alternative approaches that use shared memory. 2. Symbol/rule blocking of the grammar to respect register, constant and instruction cache limits. This is precondition for 1 above, and the details of the partitioning have a big (> 4x) effect on performance. 3. Sub-block partitioning to distribute rules across"
D13-1195,N07-1051,1,\N,Missing
D13-1203,P12-1041,1,0.723037,"Missing"
D13-1203,D08-1031,0,0.347489,"ses and NER tags for each document. All experiments use system mentions except where otherwise indicated. For each experiment, we report MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric. All metrics are computed using version 5 of the official CoNLL scorer.3 3 A Mention-Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2 This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3 Note that this version of the scorer implements a modified version of B 3 , described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new cluster (Denis and Baldr"
D13-1203,P06-1005,0,0.836276,"ttom-left cell). As we were able to incorporate syntax with shallow features, so too might we hope to incorporate semantics. However, the semantic information contained even in a coreference corpus of thousands of documents is insufficient to generalize to unseen data,8 so system designers have turned to external resources such as semantic classes derived from WordNet (Soon et al., 2001), WordNet hypernymy or synonymy (Stoyanov et al., 2010), semantic similarity computed from online resources (Ponzetto and Strube, 2006), named entity type features, gender and number match using the dataset of Bergsma and Lin (2006), and features from unsupervised clusters (Hendrickx and Daelemans, 2007; Durrett et al., 2013). In this section, we consider the following subset of these information sources: included in the OntoNotes dataset due to choices in the annotation standard. Second, we divide mentions by their type, pronominal versus nominal/proper; we then further subdivide nominals and propers based on whether or not the head word of the mention has appeared as the head of a previous mention in the document. Table 4 shows the results of our analysis. In each cell, we show the fraction of mentions that we correctl"
D13-1203,W12-4503,0,0.397898,"Missing"
D13-1203,W11-1905,0,0.0610802,"Missing"
D13-1203,W10-4305,0,0.0182212,"Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2 This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3 Note that this version of the scorer implements a modified version of B 3 , described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghig"
D13-1203,W12-4504,0,0.0141952,"impact pronoun resolution, and they allow speaker match to capture effects like I and you being coreferent when the speakers differ. features with regularization also means that we organically get distinctions among different mention types without having to choose a level of granularity a priori, unlike the distinct classifiers employed by Denis and Baldridge (2008). In terms of architecture, many coreference systems operate in a pipelined fashion, making partial decisions about coreference or pruning arcs before full resolution. Some systems use separate rule-based and learning-based passes (Chen and Ng, 2012; Fernandes et al., 2012), a series of learning-based passes (Bj¨orkelund and Farkas, 2012), or referentiality classifiers that prune the set of mentions before resolution (Rahman and Ng, 2009; Bj¨orkelund and Farkas, 2012; Recasens et al., 2013b). By contrast, our system resolves all mentions in one pass and does not need pruning: the S URFACE system can train in less than two hours without any subsampling of coreference arcs, and rule-based pruning of coreference arcs actually causes our system to perform less well, since our features can learn valuable information from these negative exampl"
D13-1203,D08-1069,0,0.672721,"and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2 This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3 Note that this version of the scorer implements a modified version of B 3 , described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Durrett et al., 2013), we will show that a mention-synchronous approach is sufficient to get state-of-the-art performance on its own. 3.1 Mention Detection Our system first identifies a set of predicted mentions from text annotated with parses and"
D13-1203,P13-1012,1,0.797376,"the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Durrett et al., 2013), we will show that a mention-synchronous approach is sufficient to get state-of-the-art performance on its own. 3.1 Mention Detection Our system first identifies a set of predicted mentions from text annotated with parses and named entity tags. We extract three distinct types of mentions: proper mentions from all named entity chunks except for those labeled as QUANTITY, CARDINAL, or PERCENT, pronominal mentions from single words tagged with PRP or PRP$, and nominal mentions from all other maximal NP projections. These basic rules are similar to those of Lee et al. (2011), except that their sy"
D13-1203,N10-1112,0,0.0427848,"of its gold antecedents are among the set of system predicted mentions. Given t training examples of the form (xk , Ck∗ ), we write the following likelihood function:   t X X `(w) = log  P 0 (a|xk ) + λkwk1 k=1 a∈A(Ck∗ ) where P 0 (a|xk ) ∝ P (a|xk ) exp(l(a, Ck∗ )) with l(a, C ∗ ) being a real-valued loss function. The loss 4 Because of this marginalization over latent antecedent choices, our objective is non-convex. here plays an analogous role to the loss in structured max-margin objectives; incorporating it into a conditional likelihood objective is a technique called softmax-margin (Gimpel and Smith, 2010). Our loss function l(a, C ∗ ) is a weighted linear combination of three error types, examples of which are shown in Figure 1. A false anaphor (FA) error occurs when ai is chosen to be anaphoric when it should start a new cluster. A false new (FN) error occurs in the opposite case, when ai wrongly indicates a new cluster when it should be anaphoric. Finally, a wrong link (WL) error occurs when the antecedent chosen for ai is the wrong antecedent (but ai is indeed anaphoric). Our final parameterized loss function is a weighted sum of the counts of these three error types: l(a, C ∗ ) = αFA FA(a,"
D13-1203,J95-2003,0,0.399887,"able to substantially outperform the other two, the two best publicly-available English coreference systems. Bolded values are significant with p < 0.05 according to a bootstrap resampling test. ence features, just implicitly. For example, rather than having rules targeting person, number, gender, or animacy of mentions, we use conjunctions with pronoun identity, which contains this information. Rather than explicitly writing a feature targeting definiteness, our indicators on the first word of a mention will capture this and other effects. And finally, rather than targeting centering theory (Grosz et al., 1995) with rule-based features identifying syntactic positions (Stoyanov et al., 2010; Haghighi and Klein, 2010), our features on word context can identify configurational clues like whether a mention is preceded or followed by a verb, and therefore whether it is likely in subject or object position.5 Not only are data-driven features able to capture the same phenomena as heuristic-driven features, but they do so at a finer level of granularity, and can therefore model more patterns in the data. To contrast these two types of features, we experiment with three ablated versions of our system, where"
D13-1203,D09-1120,1,0.780034,"Missing"
D13-1203,N10-1061,1,0.953109,"(2010), that was used for the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Durrett et al., 2013), we will show that a mention-synchronous approach is sufficient to get state-of-the-art performance on its own. 3.1 Mention Detection Our system first identifies a set of predicted mentions from text annotated with parses and named entity tags. We extract three distinct types of mentions: proper mentions from all named entity chunks except for those labeled as QUANTITY, CARDINAL, or PERCENT, pronominal mentions from single words tagged with PRP or PRP$, and nominal mentions from all other maximal NP projections. These basic rules are similar to those of Lee et al. (2011"
D13-1203,N06-2015,0,0.326909,"Missing"
D13-1203,P13-1049,0,0.0178986,"simple a feature set as possible, we are able to outperform even these sophisticated systems by a wide margin. 7 Related Work Many of the individual features we employ in the F I NAL feature set have appeared in other coreference systems (Bj¨orkelund and Nugues, 2011; Rahman and Ng, 2011b; Fernandes et al., 2012). However, other authors have often emphasized bilexical features on head pairs, whereas our features are heavily monolexical. For feature conjunctions, other authors have exploited three classes (Lee et al., 2011) or automatically learned conjunction schemes (Fernandes et al., 2012; Lassalle and Denis, 2013), but to our knowledge we are the first to do fine-grained modeling of every pronoun. Inclusion of a hierarchy of 10 Discrepancies between scores here and those printed in Pradhan et al. (2012) arise from two sources: improvements to the system of Lee et al. (2011) since the first CoNLL shared task, and a fix to the scoring of B 3 in the official scorer since results of the two CoNLL shared tasks were released. Unfortunately, because of this bug in the scoring program, direct comparison to the printed results of the other highest-scoring English systems, Fernandes et al. (2012) and Martschat e"
D13-1203,W11-1902,0,0.308117,"Missing"
D13-1203,P04-1018,0,0.0209192,"ary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2 This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3 Note that this version of the scorer implements a modified version of B 3 , described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Durrett et al., 2013), we will show that a mention-synchronous approach is sufficient to get state-of-the"
D13-1203,H05-1004,0,0.962662,"Missing"
D13-1203,W12-4511,0,0.0391712,"Missing"
D13-1203,N06-1025,0,0.241475,"Missing"
D13-1203,W11-1901,0,0.266008,"Missing"
D13-1203,W12-4501,0,0.623001,"ave appeared in other coreference systems (Bj¨orkelund and Nugues, 2011; Rahman and Ng, 2011b; Fernandes et al., 2012). However, other authors have often emphasized bilexical features on head pairs, whereas our features are heavily monolexical. For feature conjunctions, other authors have exploited three classes (Lee et al., 2011) or automatically learned conjunction schemes (Fernandes et al., 2012; Lassalle and Denis, 2013), but to our knowledge we are the first to do fine-grained modeling of every pronoun. Inclusion of a hierarchy of 10 Discrepancies between scores here and those printed in Pradhan et al. (2012) arise from two sources: improvements to the system of Lee et al. (2011) since the first CoNLL shared task, and a fix to the scoring of B 3 in the official scorer since results of the two CoNLL shared tasks were released. Unfortunately, because of this bug in the scoring program, direct comparison to the printed results of the other highest-scoring English systems, Fernandes et al. (2012) and Martschat et al. (2012), is impossible. 1979 Feature name Features of the S URFACE system Features on the current mention [ ANAPHORIC ] + [ CURRENT ANCESTRY ] Features on the antecedent [ ANTECEDENT ANCES"
D13-1203,D09-1101,0,0.457636,"ed in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Durrett et al., 2013), we will show that a mention-synchronous approach is sufficient to get state-of-the-art performance on its own. 3.1 Mention Detection Our system first identifies a set of predicted mentions from text annotated with parses and named entity tags. We extract three distinct types of mentions: proper mentions from all named entity chunks except for those labeled as QUANTITY, CARDINAL, or PERCENT, pronominal mentions from single words tagged with PRP or PRP$, and nominal mentions from all other maximal NP projections. These basic rules are similar to"
D13-1203,P11-1082,0,0.158493,"herefore improve pronoun resolution substantially; however, these features generally only improve pronoun resolution. Full results for our S URFACE and F INAL feature sets are shown in Table 7. Again, we compare to Lee et al. (2011) and Bj¨orkelund and Farkas (2012).10 Despite our system’s emphasis on one-pass resolution with as simple a feature set as possible, we are able to outperform even these sophisticated systems by a wide margin. 7 Related Work Many of the individual features we employ in the F I NAL feature set have appeared in other coreference systems (Bj¨orkelund and Nugues, 2011; Rahman and Ng, 2011b; Fernandes et al., 2012). However, other authors have often emphasized bilexical features on head pairs, whereas our features are heavily monolexical. For feature conjunctions, other authors have exploited three classes (Lee et al., 2011) or automatically learned conjunction schemes (Fernandes et al., 2012; Lassalle and Denis, 2013), but to our knowledge we are the first to do fine-grained modeling of every pronoun. Inclusion of a hierarchy of 10 Discrepancies between scores here and those printed in Pradhan et al. (2012) arise from two sources: improvements to the system of Lee et al. (2011"
D13-1203,N13-1110,0,0.0377356,"ithout having to choose a level of granularity a priori, unlike the distinct classifiers employed by Denis and Baldridge (2008). In terms of architecture, many coreference systems operate in a pipelined fashion, making partial decisions about coreference or pruning arcs before full resolution. Some systems use separate rule-based and learning-based passes (Chen and Ng, 2012; Fernandes et al., 2012), a series of learning-based passes (Bj¨orkelund and Farkas, 2012), or referentiality classifiers that prune the set of mentions before resolution (Rahman and Ng, 2009; Bj¨orkelund and Farkas, 2012; Recasens et al., 2013b). By contrast, our system resolves all mentions in one pass and does not need pruning: the S URFACE system can train in less than two hours without any subsampling of coreference arcs, and rule-based pruning of coreference arcs actually causes our system to perform less well, since our features can learn valuable information from these negative examples. Prec. MUC Rec. F1 S TANFORD IMS S URFACE * F INAL * 61.62 66.67 68.42 68.97 59.34 58.20 60.80 63.47 60.46 62.15 64.39 66.10 S TANFORD IMS F INAL * 60.91 68.15 66.81 62.13 61.60 66.04 61.51 64.71 66.43 B3 CEAFe Prec. Rec. F1 Prec. Rec. CoNLL"
D13-1203,N13-1071,0,0.154885,"Missing"
D13-1203,J01-4004,0,0.982701,"ndard automatic parses and NER tags for each document. All experiments use system mentions except where otherwise indicated. For each experiment, we report MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric. All metrics are computed using version 5 of the official CoNLL scorer.3 3 A Mention-Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2 This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3 Note that this version of the scorer implements a modified version of B 3 , described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new"
D13-1203,P09-1074,0,0.0552793,"Missing"
D13-1203,P10-2029,0,0.189469,"ents use system mentions except where otherwise indicated. For each experiment, we report MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric. All metrics are computed using version 5 of the official CoNLL scorer.3 3 A Mention-Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2 This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3 Note that this version of the scorer implements a modified version of B 3 , described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAFe is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. 1972 each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention"
D13-1203,M95-1005,0,0.94123,"Missing"
D13-1203,W12-4502,0,\N,Missing
D13-1203,D08-1067,0,\N,Missing
D15-1032,D13-1203,1,0.83022,"ng dev set performance over five training iterations, except for constituency parsing, where we took five measurements, 4k instances apart. For the cutting plane methods we cached constraints in memory to save time, but the memory cost was too great to run batch cutting plane on constituency parsing (over 60 Gb), and so is not included in the results. Coreference Resolution This gives an example of training when there are multiple gold outputs for each instance. The system we consider uses latent links between mentions in the same cluster, marginalizing over the possibilities during learning (Durrett and Klein, 2013). Since the model decomposes across mentions, we train by treating them as independent predictions with multiple gold outputs, comparing the inferred link with the gold link that is scored highest under the current model. We use the system’s weighted loss function, and the same data as for NER. 4 Observations From the results in Figure 1 and during tuning, we can make several observations about these optimization methods’ performance on these tasks. Constituency Parsing We considered two different systems. The first uses only sparse indicator features (Hall et al., 2014), while the second is p"
D15-1032,Q14-1037,1,0.270541,"Inference involves solving an integer linear program, the loss function is bigram recall, and the data is from the TAC shared tasks (Dang and Owczarzak, 2008; Dang and Owczarzak, 2009). Tasks and Systems We considered tasks covering a range of structured output spaces, from sequences to non-projective trees. Most of the corresponding systems use models designed for likelihood-based structured prediction. Some use sparse indicator features, while others use dense continuous-valued features. Named Entity Recognition This task provides a case of sequence prediction. We used the NER component of Durrett and Klein (2014)’s entity stack, training it independently of the other components. We define the loss as the number of incorrectly labelled words, and train on the CoNLL 2012 division of OntoNotes (Pradhan et al., 2007). 3.1 Tuning For each method we tuned hyperparameters by considering a grid of values and measuring dev set performance over five training iterations, except for constituency parsing, where we took five measurements, 4k instances apart. For the cutting plane methods we cached constraints in memory to save time, but the memory cost was too great to run batch cutting plane on constituency parsin"
D15-1032,P11-1049,1,0.722984,"standard division of the English Universal Dependencies (Agi´c et al., 2015). The built-in training method for MST parser is averaged, 1-best MIRA, which we include for comparison purposes. 3 Summarization With this task, we explore a case in which there is relatively little training data and the model uses a small number of dense features. The system uses a linear model with features considering counts of bigrams in the input document collection. The system forms the output summary by selecting a subset of the sentences in the input collection that does not exceed a fixed word-length limit (Berg-Kirkpatrick et al., 2011). Inference involves solving an integer linear program, the loss function is bigram recall, and the data is from the TAC shared tasks (Dang and Owczarzak, 2008; Dang and Owczarzak, 2009). Tasks and Systems We considered tasks covering a range of structured output spaces, from sequences to non-projective trees. Most of the corresponding systems use models designed for likelihood-based structured prediction. Some use sparse indicator features, while others use dense continuous-valued features. Named Entity Recognition This task provides a case of sequence prediction. We used the NER component of"
D15-1032,P15-1030,1,0.0373143,"Missing"
D15-1032,Q13-1017,0,0.0198867,"touch the weights corresponding to the (usually sparse) nonzero directions in the current batch’s subgradient. Algorithm 1 gives pseudocode for our implementation, which was based on Dyer (2013). Margin Cutting Plane (Tsochantaridis et al., 2004) Solves a sequence of quadratic programs (QP), each of which is an approximation to the dual formulation of the margin-based learning problem. At each iteration, the current QP is refined by adding additional active constraints. We solve each approximate QP using Sequential Minimal Optimization (Platt, 1999; Taskar et al., 2004). Online Cutting Plane (Chang and Yih, 2013) A modified form of cutting plane that only partially solves the QP on each iteration, operating in the dual space and optimizing a single dual variable on each iteration. We use a variant of Chang and Yih (2013) for the L1 loss margin objective. 2.2 Likelihood Stochastic Gradient Descent The built-in training method for many of the systems was softmax-margin likelihood optimization (Gimpel and Smith, 2010) via subgradient descent with either AdaGrad or AdaDelta (Duchi et al., 2011; Zeiler, 2012). We include results with each system’s default settings as a point of comparison. Online Primal Su"
D15-1032,P05-1022,0,0.0439671,"perceptron. Second, for max-margin objectives, primal optimization methods are often more robust and progress faster than dual methods. This advantage is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple online primal subgradient descent method that, despite being rarely mentioned in the literature, is surprisingly effective in relation to its alternatives. 1 Introduction Structured discriminative models have proven effective across a range of tasks in NLP including tagging (Lafferty et al., 2001; Collins, 2002), reranking parses (Charniak and Johnson, 2005), and many more (Taskar, 2004; Smith, 2011). Common approaches to training such models include margin methods, likelihood methods, and mistake-driven procedures like the averaged perceptron algorithm. In this paper, we primarily consider the relative empirical behavior of several online optimization methods for margin-based objectives, with secondary attention to other approaches for calibration. It is increasingly common to train structured models using a max-margin objective that incorporates a loss function that decomposes in the 2 Learning Algorithms We implemented a range of optimization"
D15-1032,C96-1058,0,0.0684883,"dified form of the perceptron that uses loss-augmented decoding and makes the smallest update necessary to give a margin at least as large as the loss of each solution. MIRA is generally presented as being related to the perceptron because it does not explicitly optimize a global objective, but it also has connections to margin methods, as explored by Chiang (2012). We consider one-best decoding, where the quadratic program for determining the magnitude of the update has a closed form. Dependency Parsing We used the first-order MST parser in two modes, Eisner’s algorithm for projective trees (Eisner, 1996; McDonald et al., 2005b), and the Chu-Liu-Edmonds algorithm for non-projective trees (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005a). The loss function was the number of arcs with an incorrect parent or label, and we used the standard division of the English Universal Dependencies (Agi´c et al., 2015). The built-in training method for MST parser is averaged, 1-best MIRA, which we include for comparison purposes. 3 Summarization With this task, we explore a case in which there is relatively little training data and the model uses a small number of dense features. The system uses a l"
D15-1032,N10-1112,0,0.0573708,"Missing"
D15-1032,P14-1022,1,0.5352,"Missing"
D15-1032,W02-1001,0,0.6832,"outperform both likelihood and the perceptron. Second, for max-margin objectives, primal optimization methods are often more robust and progress faster than dual methods. This advantage is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple online primal subgradient descent method that, despite being rarely mentioned in the literature, is surprisingly effective in relation to its alternatives. 1 Introduction Structured discriminative models have proven effective across a range of tasks in NLP including tagging (Lafferty et al., 2001; Collins, 2002), reranking parses (Charniak and Johnson, 2005), and many more (Taskar, 2004; Smith, 2011). Common approaches to training such models include margin methods, likelihood methods, and mistake-driven procedures like the averaged perceptron algorithm. In this paper, we primarily consider the relative empirical behavior of several online optimization methods for margin-based objectives, with secondary attention to other approaches for calibration. It is increasingly common to train structured models using a max-margin objective that incorporates a loss function that decomposes in the 2 Learning Alg"
D15-1032,J93-2004,0,0.0552861,"hese instance-wise subgradients to optimize the global objective using AdaGrad (Duchi et al., 2011) with either L1 or L2 regularization. The simplest implementation of AdaGrad touches every weight when doing the update 2.3 Mistake Driven Averaged Perceptron (Freund and Schapire, 1999; Collins, 2002) On a mistake, weights for features on the system output are decremented and weights for features on the gold output are incre274 mented. Weights are averaged over the course of training, and decoding is not loss-augmented. of incorrect rule productions, and use the standard Penn Treebank division (Marcus et al., 1993). Margin Infused Relaxed Algorithm (Crammer and Singer, 2003) A modified form of the perceptron that uses loss-augmented decoding and makes the smallest update necessary to give a margin at least as large as the loss of each solution. MIRA is generally presented as being related to the perceptron because it does not explicitly optimize a global objective, but it also has connections to margin methods, as explored by Chiang (2012). We consider one-best decoding, where the quadratic program for determining the magnitude of the update has a closed form. Dependency Parsing We used the first-order"
D15-1032,P05-1012,0,0.0382023,"the perceptron that uses loss-augmented decoding and makes the smallest update necessary to give a margin at least as large as the loss of each solution. MIRA is generally presented as being related to the perceptron because it does not explicitly optimize a global objective, but it also has connections to margin methods, as explored by Chiang (2012). We consider one-best decoding, where the quadratic program for determining the magnitude of the update has a closed form. Dependency Parsing We used the first-order MST parser in two modes, Eisner’s algorithm for projective trees (Eisner, 1996; McDonald et al., 2005b), and the Chu-Liu-Edmonds algorithm for non-projective trees (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005a). The loss function was the number of arcs with an incorrect parent or label, and we used the standard division of the English Universal Dependencies (Agi´c et al., 2015). The built-in training method for MST parser is averaged, 1-best MIRA, which we include for comparison purposes. 3 Summarization With this task, we explore a case in which there is relatively little training data and the model uses a small number of dense features. The system uses a linear model with featur"
D15-1032,H05-1066,0,0.0798595,"the perceptron that uses loss-augmented decoding and makes the smallest update necessary to give a margin at least as large as the loss of each solution. MIRA is generally presented as being related to the perceptron because it does not explicitly optimize a global objective, but it also has connections to margin methods, as explored by Chiang (2012). We consider one-best decoding, where the quadratic program for determining the magnitude of the update has a closed form. Dependency Parsing We used the first-order MST parser in two modes, Eisner’s algorithm for projective trees (Eisner, 1996; McDonald et al., 2005b), and the Chu-Liu-Edmonds algorithm for non-projective trees (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005a). The loss function was the number of arcs with an incorrect parent or label, and we used the standard division of the English Universal Dependencies (Agi´c et al., 2015). The built-in training method for MST parser is averaged, 1-best MIRA, which we include for comparison purposes. 3 Summarization With this task, we explore a case in which there is relatively little training data and the model uses a small number of dense features. The system uses a linear model with featur"
D15-1032,W04-3201,1,0.893268,"able loss-augmented decoding, and trained these models with six different methods. We have released our learning code as a Java library.1 Our results provide support for the conventional wisdom that margin-based optimization is broadly effective, frequently outperforming likelihood optimization and the perceptron algorithm. We also found that directly optimizing the primal structured margin objective based on subgradients calculated from single training instances is surprisingly effective, performing consistently well across all tasks. Despite the convexity of structured maxmargin objectives (Taskar et al., 2004; Tsochantaridis et al., 2004), the many ways to optimize them are not equally effective in practice. We compare a range of online optimization methods over a variety of structured NLP tasks (coreference, summarization, parsing, etc) and find several broad trends. First, margin methods do tend to outperform both likelihood and the perceptron. Second, for max-margin objectives, primal optimization methods are often more robust and progress faster than dual methods. This advantage is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple o"
D15-1138,W14-1607,1,0.909453,"ossible to behave correctly in environments containing novel strings or other features unseen during training. This aspect of the syntax–semantics interface has been troublesome for some logic-based approaches: while past work has used related machinery for selecting lexicon entries (Berant and Liang, 2014) or for rewriting logical forms (Kwiatkowski et al., 2013), the relationship between text and the environment has ultimately been mediated by a discrete (and indeed finite) inventory of predicates. Several recent papers have investigated simple grounded models with realvalued output spaces (Andreas and Klein, 2014; McMahan and Stone, 2015), but we are unaware of any fully compositional system in recent literature that can incorporate observations of these kinds. Formally, we assume access to a joining feature function φ : (2L × 2L ) → Rd . As with grounding graphs, our goal is to make the general framework as flexible as possible, and for individual experiments have chosen φ to emulate modeling decisions from previous work. 1168 4 Then we have2 Model As noted in the introduction, we approach instruction following as a sequence prediction problem. Thus we must place a distribution over sequences of acti"
D15-1138,Q13-1005,0,0.480505,"eld, with alignment potentials to relate instructions to actions and transition potentials to encode the environment model (Figure 3). Explicitly modeling sequenceto-sequence alignments between text and actions allows flexible reasoning about action sequences, enabling the agent to determine which actions are specified (perhaps redundantly) by text, and which actions must be performed automatically (in order to satisfy pragmatic constraints on interpretation). Treating instruction following as a sequence prediction problem, rather than a series of independent decisions (Branavan et al., 2009; Artzi and Zettlemoyer, 2013), makes it possible to use general-purpose planning machinery, greatly increasing inferential power. The fragment of semantics necessary to complete most instruction-following tasks is essentially predicate–argument structure, with limited influence from quantification and scoping. Thus the problem of sentence interpretation can reasonably be modeled as one of finding an alignment between language and the environment it describes. We allow this structure-to-structure alignment— an “overlay” of language onto the world—to be mediated by linguistic structure (in the form of dependency parses) and"
D15-1138,D14-1134,0,0.5413,"Missing"
D15-1138,P14-1133,0,0.0173303,"must eventually combine features provided by parse trees with features provided by the environment. Examples here might include simple conjunctions (word=yellow ∧ rgb=(0.5, 0.5, 0.0)) or more complicated computations like edit distance between landmark names and lexical items. Features of the latter kind make it possible to behave correctly in environments containing novel strings or other features unseen during training. This aspect of the syntax–semantics interface has been troublesome for some logic-based approaches: while past work has used related machinery for selecting lexicon entries (Berant and Liang, 2014) or for rewriting logical forms (Kwiatkowski et al., 2013), the relationship between text and the environment has ultimately been mediated by a discrete (and indeed finite) inventory of predicates. Several recent papers have investigated simple grounded models with realvalued output spaces (Andreas and Klein, 2014; McMahan and Stone, 2015), but we are unaware of any fully compositional system in recent literature that can incorporate observations of these kinds. Formally, we assume access to a joining feature function φ : (2L × 2L ) → Rd . As with grounding graphs, our goal is to make the gene"
D15-1138,P09-1010,0,0.187612,"d conditional random field, with alignment potentials to relate instructions to actions and transition potentials to encode the environment model (Figure 3). Explicitly modeling sequenceto-sequence alignments between text and actions allows flexible reasoning about action sequences, enabling the agent to determine which actions are specified (perhaps redundantly) by text, and which actions must be performed automatically (in order to satisfy pragmatic constraints on interpretation). Treating instruction following as a sequence prediction problem, rather than a series of independent decisions (Branavan et al., 2009; Artzi and Zettlemoyer, 2013), makes it possible to use general-purpose planning machinery, greatly increasing inferential power. The fragment of semantics necessary to complete most instruction-following tasks is essentially predicate–argument structure, with limited influence from quantification and scoping. Thus the problem of sentence interpretation can reasonably be modeled as one of finding an alignment between language and the environment it describes. We allow this structure-to-structure alignment— an “overlay” of language onto the world—to be mediated by linguistic structure (in the"
D15-1138,P11-1028,0,0.0526053,"o model compositionality in language or actions. Agents in this family have been evaluated on a variety of tasks, including map reading (Anderson et al., 1991) and gameplay (Branavan et al., 2009). Though both families address the same class of instruction-following problems, they have been applied to a totally disjoint set of tasks. It should be emphasized that there is nothing inherent to policy learning that prevents the use of compositional structure, and nothing inherent to general compositional models that prevents more complicated dependence on environment state. Indeed, previous work (Branavan et al., 2011; Narasimhan et al., 2015) uses aspects of both to solve a different class of gameplay problems. In some sense, our goal in this paper is simply to combine the strengths of semantic parsers and linear policy estimators for fully general instruction following. As we shall see, however, this requires changes to many aspects of representation, learning and inference. 3 Representations We wish to train a model capable of following commands in a simulated environment. We do so by presenting the model with a sequence of training pairs (x, y), where each x is a sequence of natural language instructio"
D15-1138,J93-2003,0,0.0744857,"(2) provides context-dependent interpretation of text by means of the structured scoring function ψ(x, y; θ), described in the next section. Formally, we associate with each instruction xi a sequence-to-sequence alignment variable ai ∈ 1 . . . n (recalling that n is the number of actions).  n X p(y,a|x; θ) ∝ exp ψ(n) + ψ(yj ) j=1 + m X n X 1[aj = i] ψ(xi , yj )  (1) i=1 j=1 We additionally place a monotonicity constraint on the alignment variables. This model is globally normalized, and for a fixed alignment is equivalent to a linear-chain CRF. In this sense it is analogous to IBM Model I (Brown et al., 1993), with the structured potentials ψ(xi , yj ) taking the place of lexical translation probabilities. While alignment models from machine translation have previously been used to align words to fragments of semantic parses (Wong and Mooney, 2006; Pourdamghani et al., 2014), we are unaware of such models being used to align entire instruction sequences to demonstrations. Action structure: aligning words to percepts Intuitively, this scoring function ψ(x, y) should capture how well a given utterance describes an action. If neither the utterances nor the actions had structure (i.e. both could be re"
D15-1138,P12-1045,0,0.0229318,"Missing"
D15-1138,C12-1083,1,0.357068,"Missing"
D15-1138,D12-1040,0,0.0864582,"Missing"
D15-1138,Q14-1042,0,0.00563031,"e family of approaches is based on learning a policy over primitive actions directly (Branavan et al., 2009; Vogel and Jurafsky, 2010).1 Policybased approaches instantiate a Markov decision process representing the action domain, and apply standard supervised or reinforcement-learning approaches to learn a function for greedily selecting among actions. In linear policy approximators, natural language instructions are incorporated directly into state observations, and reading order 1 This is distinct from semantic parsers in which greedy inference happens to have an interpretation as a policy (Vlachos and Clark, 2014). 1166 becomes part of the action selection process. Almost all existing policy-learning approaches make use of an unstructured parameterization, with a single (flat) feature vector representing all text and observations. Such approaches are thus restricted to problems that are simple enough (and have small enough action spaces) to be effectively characterized in this fashion. While there is a great deal of flexibility in the choice of feature function (which is free to inspect the current and future state of the environment, the whole instruction sequence, etc.), standard linear policy estima"
D15-1138,P10-1083,0,0.231116,"ic formal language for describing agent behavior. Thus it is extremely difficult to work with environments that cannot be modeled with a fixed inventory of predicates (e.g. those involving novel strings or arbitrary real quantities). Much of contemporary work in this family is evaluated on the maze navigation task introduced by MacMahon et al. (2006). Dukes (2013) also introduced a “blocks world” task for situated parsing of spatial robot commands. Linear policy estimators An alternative family of approaches is based on learning a policy over primitive actions directly (Branavan et al., 2009; Vogel and Jurafsky, 2010).1 Policybased approaches instantiate a Markov decision process representing the action domain, and apply standard supervised or reinforcement-learning approaches to learn a function for greedily selecting among actions. In linear policy approximators, natural language instructions are incorporated directly into state observations, and reading order 1 This is distinct from semantic parsers in which greedy inference happens to have an interpretation as a policy (Vlachos and Clark, 2014). 1166 becomes part of the action selection process. Almost all existing policy-learning approaches make use o"
D15-1138,P13-1022,0,0.0489834,"g in a variety of sophisticated approaches. Despite superficial similarity to the previous navigation task, the language and plans required for this task are quite different. The proportion of instructions to actions is much higher (so redundancy much lower), and the interpretation of language is highly compositional. As can be seen in Table 3, we outperform a number of systems purpose-built for this navigation task. We also outperform both variants of our system, most conspicuously the variant without grounding graphs. This highlights the importance of compositional structure. Recent work by Kim and Mooney (2013) and Artzi et al. (2014) has achieved better results; these systems make use of techniques and resources (respectively, discriminative reranking and a seed lexicon of handannotated logical forms) that are largely orthogonal to the ones used here, and might be applied to improve our own results as well. Puzzle solving The last task we consider is the Crossblock task studied by Branavan et al. (2009) (Figure 1c). Here, again, natural language is used to specify a sequence of actions, in this case the solution to a simple game. The environment is simple enough to be captured with a flat feature 4"
D15-1138,N06-1056,0,0.0330591,"ate the importance of our contributions in both compositional semantics and search over plans. We have released all code for this project at github.com/jacobandreas/instructions. 2 Related work Existing work on instruction following can be roughly divided into two families: semantic parsers and linear policy estimators. Semantic parsers Parser-based approaches (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Kim and Mooney, 2013) map from text into a formal language representing commands. These take familiar structured prediction models for semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), and train them with task-provided supervision. Instead of attempting to match the structure of a manually-annotated semantic parse, semantic parsers for instruction following are trained to maximize a reward signal provided by black-box execution of the predicted command in the environment. (It is possible to think of response-based learning for question answering (Liang et al., 2013) as a special case.) This approach uses a well-studied mechanism for compositional interpretation of language, but is subject to certain limitations. Because the environment is manipulated only through black-box"
D15-1138,D13-1161,0,0.0201035,"with features provided by the environment. Examples here might include simple conjunctions (word=yellow ∧ rgb=(0.5, 0.5, 0.0)) or more complicated computations like edit distance between landmark names and lexical items. Features of the latter kind make it possible to behave correctly in environments containing novel strings or other features unseen during training. This aspect of the syntax–semantics interface has been troublesome for some logic-based approaches: while past work has used related machinery for selecting lexicon entries (Berant and Liang, 2014) or for rewriting logical forms (Kwiatkowski et al., 2013), the relationship between text and the environment has ultimately been mediated by a discrete (and indeed finite) inventory of predicates. Several recent papers have investigated simple grounded models with realvalued output spaces (Andreas and Klein, 2014; McMahan and Stone, 2015), but we are unaware of any fully compositional system in recent literature that can incorporate observations of these kinds. Formally, we assume access to a joining feature function φ : (2L × 2L ) → Rd . As with grounding graphs, our goal is to make the general framework as flexible as possible, and for individual"
D15-1138,J13-2005,1,0.400173,"and Zettlemoyer, 2013; Kim and Mooney, 2013) map from text into a formal language representing commands. These take familiar structured prediction models for semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), and train them with task-provided supervision. Instead of attempting to match the structure of a manually-annotated semantic parse, semantic parsers for instruction following are trained to maximize a reward signal provided by black-box execution of the predicted command in the environment. (It is possible to think of response-based learning for question answering (Liang et al., 2013) as a special case.) This approach uses a well-studied mechanism for compositional interpretation of language, but is subject to certain limitations. Because the environment is manipulated only through black-box execution of the completed semantic parse, there is no way to incorporate current or future environment state into the scoring function. It is also in general necessary to hand-engineer a task-specific formal language for describing agent behavior. Thus it is extremely difficult to work with environments that cannot be modeled with a fixed inventory of predicates (e.g. those involving"
D15-1138,Q15-1008,0,0.0207009,"ly in environments containing novel strings or other features unseen during training. This aspect of the syntax–semantics interface has been troublesome for some logic-based approaches: while past work has used related machinery for selecting lexicon entries (Berant and Liang, 2014) or for rewriting logical forms (Kwiatkowski et al., 2013), the relationship between text and the environment has ultimately been mediated by a discrete (and indeed finite) inventory of predicates. Several recent papers have investigated simple grounded models with realvalued output spaces (Andreas and Klein, 2014; McMahan and Stone, 2015), but we are unaware of any fully compositional system in recent literature that can incorporate observations of these kinds. Formally, we assume access to a joining feature function φ : (2L × 2L ) → Rd . As with grounding graphs, our goal is to make the general framework as flexible as possible, and for individual experiments have chosen φ to emulate modeling decisions from previous work. 1168 4 Then we have2 Model As noted in the introduction, we approach instruction following as a sequence prediction problem. Thus we must place a distribution over sequences of actions conditioned on instruc"
D15-1138,D15-1001,0,0.0047215,"y in language or actions. Agents in this family have been evaluated on a variety of tasks, including map reading (Anderson et al., 1991) and gameplay (Branavan et al., 2009). Though both families address the same class of instruction-following problems, they have been applied to a totally disjoint set of tasks. It should be emphasized that there is nothing inherent to policy learning that prevents the use of compositional structure, and nothing inherent to general compositional models that prevents more complicated dependence on environment state. Indeed, previous work (Branavan et al., 2011; Narasimhan et al., 2015) uses aspects of both to solve a different class of gameplay problems. In some sense, our goal in this paper is simply to combine the strengths of semantic parsers and linear policy estimators for fully general instruction following. As we shall see, however, this requires changes to many aspects of representation, learning and inference. 3 Representations We wish to train a model capable of following commands in a simulated environment. We do so by presenting the model with a sequence of training pairs (x, y), where each x is a sequence of natural language instructions (x1 , x2 , . . . , xm )"
D15-1138,P15-1142,0,0.0182281,"ingle (structured) random variable. However, the two kinds of alignments are treated differently for purposes of inference, so it is useful to maintain a notational distinction. 1169 and that each lexical item is associated with a small set of functional forms. Here we simply allow all words to license all predicates, multiple words to specify the same predicate, and some edges to be skipped. We instead rely on a scoring function to impose soft versions of the hard constraints typically provided by a grammar. Related models have previously been used for question answering (Reddy et al., 2014; Pasupat and Liang, 2015). For the moment let us introduce variables b to denote these structure-to-structure alignments. (As will be seen in the following section, it is straightforward to marginalize over all choices of b. Thus the structure-to-structure alignments are never explicitly instantiated during inference, and do not appear in the final form of ψ(x, y).) For a fixed alignment, we define ψ(x, y, b) according to a recurrence relation. Let xi be the ith word of the sentence, and let y j be the jth node in the action graph (under some topological ordering). Let c(i) and c(j) give the indices of the dependents"
D15-1138,D14-1048,0,0.0349502,"al or simulated environment, in response to a sequence of natural language commands. Examples include giving navigational directions to robots and providing hints to automated game-playing agents. Plans specified with natural language exhibit compositionality both at the level of individual actions and at the overall sequence level. This paper describes a framework for learning to follow instructions by leveraging structure at both levels. Our primary contribution is a new, alignmentbased approach to grounded compositional semantics. Building on related logical approaches (Reddy et al., 2014; Pourdamghani et al., 2014), we recast instruction following as a pair of nested, structured alignment problems. Given instructions and a candidate plan, the model infers a sequenceto-sequence alignment between sentences and atomic actions. Within each sentence–action pair, the model infers a structure-to-structure alignment between the syntax of the sentence and a graphbased representation of the action. At a high level, our agent is a block-structured, graph-valued conditional random field, with alignment potentials to relate instructions to actions and transition potentials to encode the environment model (Figure 3)."
D15-1138,Q14-1030,0,0.0921343,"e of actions in a real or simulated environment, in response to a sequence of natural language commands. Examples include giving navigational directions to robots and providing hints to automated game-playing agents. Plans specified with natural language exhibit compositionality both at the level of individual actions and at the overall sequence level. This paper describes a framework for learning to follow instructions by leveraging structure at both levels. Our primary contribution is a new, alignmentbased approach to grounded compositional semantics. Building on related logical approaches (Reddy et al., 2014; Pourdamghani et al., 2014), we recast instruction following as a pair of nested, structured alignment problems. Given instructions and a candidate plan, the model infers a sequenceto-sequence alignment between sentences and atomic actions. Within each sentence–action pair, the model infers a structure-to-structure alignment between the syntax of the sentence and a graphbased representation of the action. At a high level, our agent is a block-structured, graph-valued conditional random field, with alignment potentials to relate instructions to actions and transition potentials to encode the e"
D15-1138,P11-1060,1,\N,Missing
D16-1125,W09-3704,0,0.0144615,"014). In order for the players to win, S’s description d must be pragmatic: it must be informative, fluent, concise, and must ultimately encode an understanding of L’s behavior. In Figure 1, for example, the owl is wearing a hat and the owl is sitting in the tree are both accurate descriptions of the target image, but only the second allows a human listener to succeed with high probability. RG is the focus of many papers in the computational pragmatics literature: it provides a concrete generation task while eliciting a broad range of pragmatic behaviors, including conversational implicature (Benotti and Traum, 2009) and context dependence (Smith et al., 2013). Existing computational models of pragmatics can be divided into two broad lines of work, which we term the direct and derived approaches. Direct models (see Section 2 for examples) are based on a representation of S. They learn pragmatic behavior by example. Beginning with datasets annotated for the specific task they are trying to 1173 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1173–1182, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics solve (e.g. examples of hu"
D16-1125,D13-1197,0,0.0530538,"Missing"
D16-1125,W07-2307,0,0.0344898,"Missing"
D16-1125,D10-1040,1,0.785838,"(2013) collect a set of human-generated referring expressions about abstract representations of sets of colored blocks. Given a set of blocks to describe, their model directly learns a maximumentropy distribution over the set of logical expressions whose denotation is the target set. Other research, focused on referring expression generation from a computer vision perspective, includes that of Mao et al. (2015) and Kazemzadeh et al. (2014). Derived pragmatics Derived approaches, sometimes referred to as “rational speech acts” models, include those of Smith et al. (2013), Vogel et al. (2013), Golland et al. (2010), and Monroe and Potts (2015). These couple template-driven language generation with probabilistic or game-theoretic reasoning frameworks to produce contextually appropriate language: intelligent listeners reason about the behavior of reflexive speakers, and even higher-order speakers reason about these listeners. Experiments (Frank et al., 2009) show that derived approaches explain human behavior well, but both computational and representational issues restrict their application to simple reference games. They require domainspecific engineering, controlled world representations, and pragmatic"
D16-1125,D14-1086,0,0.0586745,", and between computational efficiency and expressive power.1 2 Related Work Direct pragmatics As an example of the direct approach mentioned in the introduction, FitzGerald et al. (2013) collect a set of human-generated referring expressions about abstract representations of sets of colored blocks. Given a set of blocks to describe, their model directly learns a maximumentropy distribution over the set of logical expressions whose denotation is the target set. Other research, focused on referring expression generation from a computer vision perspective, includes that of Mao et al. (2015) and Kazemzadeh et al. (2014). Derived pragmatics Derived approaches, sometimes referred to as “rational speech acts” models, include those of Smith et al. (2013), Vogel et al. (2013), Golland et al. (2010), and Monroe and Potts (2015). These couple template-driven language generation with probabilistic or game-theoretic reasoning frameworks to produce contextually appropriate language: intelligent listeners reason about the behavior of reflexive speakers, and even higher-order speakers reason about these listeners. Experiments (Frank et al., 2009) show that derived approaches explain human behavior well, but both computa"
D16-1125,N15-1174,0,0.0195886,"Missing"
D16-1125,P05-1044,0,0.0212148,"owing optimization problem: max W X j log pL0 (1|dj , rj , r0 ) (5) Here r0 is a random distractor chosen uniformly from the training set. For each training example (ri , di ), this objective attempts to maximize the probability that the model chooses ri as the referent of di over a random distractor. This contrastive objective ensures that our approach is applicable even when there is not a naturally-occurring source of target–distractor pairs, as previous work (Golland et al., 2010; Monroe and Potts, 2015) has required. Note that this can also be viewed as a version of the loss described by Smith and Eisner (2005), where it approximates a likelihood objective that encourages L0 to prefer ri to every other possible referent simultaneously. Literal speaker As in the figure, the literal speaker is obtained by composing a referent encoder with a describer, as follows: e = Er (f (r)) pS0 (d|r) = Dd (d|e) As with the listener, the literal speaker should be understood as producing a distribution over strings. It is trained by maximizing the conditional likelihood of captions in the training data: X max log pS0 (di |ri ) (6) W i These base models are intended to be the minimal learned equivalents of the hand-e"
D16-1125,Q14-1017,0,0.0181275,"ding instruction following (Anderson et al., 1991) and discourse analysis (Jurafsky et al., 1997). 1 Models, human annotations, and code to generate all tables and figures in this paper can be found at http://github. com/jacobandreas/pragma. Representing language and the world In addition to the pragmatics literature, the approach proposed in this paper relies extensively on recently developed tools for multimodal processing of language and unstructured representations like images. These includes both image retrieval models, which select an image from a collection given a textual description (Socher et al., 2014), and neural conditional language models, which take a content representation and emit a string (Donahue et al., 2015). ref features FC desc desc Sum ReLU FC Softmax choice referent sentence (c) choice ranker R wordn Approach FC ReLU FC Softmax wordn+1 referent Our goal is to produce a model that can play the role of the speaker S in RG. Specifically, given a target referent (e.g. scene or object) r and a distractor r0 , the model must produce a description d that uniquely identifies r. For training, we have access to a set of non-contrastively captioned referents {(ri , di )}: each training d"
D16-1125,N13-1127,0,0.233778,"on, FitzGerald et al. (2013) collect a set of human-generated referring expressions about abstract representations of sets of colored blocks. Given a set of blocks to describe, their model directly learns a maximumentropy distribution over the set of logical expressions whose denotation is the target set. Other research, focused on referring expression generation from a computer vision perspective, includes that of Mao et al. (2015) and Kazemzadeh et al. (2014). Derived pragmatics Derived approaches, sometimes referred to as “rational speech acts” models, include those of Smith et al. (2013), Vogel et al. (2013), Golland et al. (2010), and Monroe and Potts (2015). These couple template-driven language generation with probabilistic or game-theoretic reasoning frameworks to produce contextually appropriate language: intelligent listeners reason about the behavior of reflexive speakers, and even higher-order speakers reason about these listeners. Experiments (Frank et al., 2009) show that derived approaches explain human behavior well, but both computational and representational issues restrict their application to simple reference games. They require domainspecific engineering, controlled world represe"
D17-1015,D10-1040,1,0.768771,"of interpretation. Spatial descriptors are also present in the task of generating 3D scenes given natural language descriptions. Compared to a recent model by Chang et al. (2017) for scene generation, our model works with lower-level 3D percepts rather than libraries of segmented and tagged objects. We are also able to incorporate learning of vocabulary, perception, and linguistic structure into a single neural network that is trainable end-to-end. Related Work Our task includes some of the same elements as referring-expression generation and interpretation. Past work on these tasks includes Golland et al. (2010), Krishnamurthy and Kollar (2013), Socher et al. (2014) and Kazemzadeh et al. (2014). A key difference is that spatial descriptors (as modeled in this paper) refer to locations in space, rather than to objects alone. For example, Krishnamurthy and Kollar (2013) convert natural language to a logical form that is matched against image segments, an approach that is only capable of reasoning about objects already present in the scene (and not skipped over by the segmentation process). Our model’s ability to reason over spatial regions also differentiates it from past approaches to tasks beyond ref"
D17-1015,D14-1086,0,0.196013,"g 3D scenes given natural language descriptions. Compared to a recent model by Chang et al. (2017) for scene generation, our model works with lower-level 3D percepts rather than libraries of segmented and tagged objects. We are also able to incorporate learning of vocabulary, perception, and linguistic structure into a single neural network that is trainable end-to-end. Related Work Our task includes some of the same elements as referring-expression generation and interpretation. Past work on these tasks includes Golland et al. (2010), Krishnamurthy and Kollar (2013), Socher et al. (2014) and Kazemzadeh et al. (2014). A key difference is that spatial descriptors (as modeled in this paper) refer to locations in space, rather than to objects alone. For example, Krishnamurthy and Kollar (2013) convert natural language to a logical form that is matched against image segments, an approach that is only capable of reasoning about objects already present in the scene (and not skipped over by the segmentation process). Our model’s ability to reason over spatial regions also differentiates it from past approaches to tasks beyond referring expressions, such as the work by Tellex et al. (2011) on natural-language com"
D17-1015,Q13-1016,0,0.102165,"tial descriptors are also present in the task of generating 3D scenes given natural language descriptions. Compared to a recent model by Chang et al. (2017) for scene generation, our model works with lower-level 3D percepts rather than libraries of segmented and tagged objects. We are also able to incorporate learning of vocabulary, perception, and linguistic structure into a single neural network that is trainable end-to-end. Related Work Our task includes some of the same elements as referring-expression generation and interpretation. Past work on these tasks includes Golland et al. (2010), Krishnamurthy and Kollar (2013), Socher et al. (2014) and Kazemzadeh et al. (2014). A key difference is that spatial descriptors (as modeled in this paper) refer to locations in space, rather than to objects alone. For example, Krishnamurthy and Kollar (2013) convert natural language to a logical form that is matched against image segments, an approach that is only capable of reasoning about objects already present in the scene (and not skipped over by the segmentation process). Our model’s ability to reason over spatial regions also differentiates it from past approaches to tasks beyond referring expressions, such as the w"
D17-1015,Q14-1017,0,0.0312154,"in the task of generating 3D scenes given natural language descriptions. Compared to a recent model by Chang et al. (2017) for scene generation, our model works with lower-level 3D percepts rather than libraries of segmented and tagged objects. We are also able to incorporate learning of vocabulary, perception, and linguistic structure into a single neural network that is trainable end-to-end. Related Work Our task includes some of the same elements as referring-expression generation and interpretation. Past work on these tasks includes Golland et al. (2010), Krishnamurthy and Kollar (2013), Socher et al. (2014) and Kazemzadeh et al. (2014). A key difference is that spatial descriptors (as modeled in this paper) refer to locations in space, rather than to objects alone. For example, Krishnamurthy and Kollar (2013) convert natural language to a logical form that is matched against image segments, an approach that is only capable of reasoning about objects already present in the scene (and not skipped over by the segmentation process). Our model’s ability to reason over spatial regions also differentiates it from past approaches to tasks beyond referring expressions, such as the work by Tellex et al. ("
D17-1178,P15-2142,0,0.0337055,"xternal parsers. Intuitively, because a generative parser defines a joint distribution over sentences and parse trees, probability mass will be allocated unevenly between a small number of common structural actions and a large vocabulary of lexical items. This imbalance is a primary cause of failure for search procedures in which these two types of actions compete directly. A notion of equal competition among hypotheses is then desirable, an idea that has previously been explored in generative models for constituency parsing (Henderson, 2003) and dependency parsing (Titov and Henderson, 2010; Buys and Blunsom, 2015), among other tasks. We describe a related state-augmented beam search for neural generative constituency parsers in which lexical actions compete only with each other rather than with structural actions. Applying this inference procedure to the generative model of Choe and Charniak (2016), we find that it yields a self-contained generative parser that achieves high performance. Beyond this, we propose an enhanced candidate selection strategy that yields significant improvements for all beam sizes. Additionally, motivated by the look-ahead heuristic used in the top-down parsers of Roark (2001)"
D17-1178,D10-1066,0,0.0145252,"ng other tasks. We describe a related state-augmented beam search for neural generative constituency parsers in which lexical actions compete only with each other rather than with structural actions. Applying this inference procedure to the generative model of Choe and Charniak (2016), we find that it yields a self-contained generative parser that achieves high performance. Beyond this, we propose an enhanced candidate selection strategy that yields significant improvements for all beam sizes. Additionally, motivated by the look-ahead heuristic used in the top-down parsers of Roark (2001) and Charniak (2010), we also experiment with a simple coarse pruning function that allows us to reduce the number of states expanded per candidate by several times without compromising accuracy. Using our final search procedure, we surpass prior state-ofthe-art results among single-model parsers on the Penn Treebank, obtaining an F1 score of 92.56. 2 Common Framework The generative neural parsers of Dyer et al. (2016) and Choe and Charniak (2016) can be unified under a common shift-reduce framework. Both systems build parse trees in left-to-right depth-first order by executing a sequence of actions, as illustrat"
D17-1178,D16-1257,0,0.490237,"ved state-of-the-art results for constituency parsing. However, without a feasible search procedure, their use has so far been limited to reranking the output of external parsers in which decoding is more tractable. We describe an alternative to the conventional action-level beam search used for discriminative neural models that enables us to decode directly in these generative models. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve accuracy while exploring significantly less of the search space. Applied to the model of Choe and Charniak (2016), our inference procedure obtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior state-of-the-art results for single-model systems. 1 Introduction A recent line of work has demonstrated the success of generative neural models for constituency parsing (Dyer et al., 2016; Choe and Charniak, 2016). As with discriminative neural parsers, these models lack a dynamic program for exact inference due to their modeling of unbounded dependencies. However, while discriminative neural parsers are able to obtain strong results using greedy search (Dyer et al., 2016) or beam search with a sma"
D17-1178,D16-1001,0,0.310389,"Missing"
D17-1178,P17-1076,1,0.781881,"Missing"
D17-1178,N16-1024,0,0.326468,"d for discriminative neural models that enables us to decode directly in these generative models. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve accuracy while exploring significantly less of the search space. Applied to the model of Choe and Charniak (2016), our inference procedure obtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior state-of-the-art results for single-model systems. 1 Introduction A recent line of work has demonstrated the success of generative neural models for constituency parsing (Dyer et al., 2016; Choe and Charniak, 2016). As with discriminative neural parsers, these models lack a dynamic program for exact inference due to their modeling of unbounded dependencies. However, while discriminative neural parsers are able to obtain strong results using greedy search (Dyer et al., 2016) or beam search with a small beam (Vinyals et al., 2015), we find that a simple action-level approach fails outright in the generative setting. Perhaps because of this, the application of generative neural models has so far been restricted to reranking the output of external parsers. Intuitively, because a ge"
D17-1178,P17-2025,1,0.721082,"ypically has much lower overall probability than a plausible parse, but the model’s myopic comparison between structural and lexical actions prevents reasonable candidates from staying on the beam. Action-level beam search with beam size 1000 obtains an F1 score of just 52.97 on the development set. 5 had Word-Level Search The imbalance between the probabilities of structural and lexical actions suggests that the two kinds of actions should not compete against each other within a beam. This leads us to consider an augmented state space in which they are kept separate by design, as was done by Fried et al. (2017). In conventional action-level beam search, hypotheses are grouped by the length of their action history |A|. Letting Ai denote the set of actions taken since the ith shift action, we instead group hypotheses by the pair (i, |Ai |), where i ranges between 0 and the length of the sentence. Let k denote the target beam size. The search process begins with the empty hypothesis in the (0, 0) bucket. Word-level steps are then taken according to the following procedure for i = 0, 1, . . . , up to the length of the sentence (inclusive). Beginning with the (i, 0) bucket, the successors of each hypothe"
D17-1178,N03-1014,0,0.0779357,"eural models has so far been restricted to reranking the output of external parsers. Intuitively, because a generative parser defines a joint distribution over sentences and parse trees, probability mass will be allocated unevenly between a small number of common structural actions and a large vocabulary of lexical items. This imbalance is a primary cause of failure for search procedures in which these two types of actions compete directly. A notion of equal competition among hypotheses is then desirable, an idea that has previously been explored in generative models for constituency parsing (Henderson, 2003) and dependency parsing (Titov and Henderson, 2010; Buys and Blunsom, 2015), among other tasks. We describe a related state-augmented beam search for neural generative constituency parsers in which lexical actions compete only with each other rather than with structural actions. Applying this inference procedure to the generative model of Choe and Charniak (2016), we find that it yields a self-contained generative parser that achieves high performance. Beyond this, we propose an enhanced candidate selection strategy that yields significant improvements for all beam sizes. Additionally, motivat"
D17-1178,Q17-1004,0,0.151331,"Missing"
D17-1178,J93-2004,0,0.0610297,".2 Figure 1: A parse tree and the action sequence that produced it, corresponding to the sentence “He had an idea.” The tree is constructed in left-toright depth-first order. The tree contains only nonterminals and words; part-of-speech tags are not included. O PEN(X) and C LOSE(X) are rendered as “(X” and “X)” for brevity. t=1 T Y S) -0.0 -0.0 -1.4 -3.9 -4.0 (S (NP He NP) (VP had (NP an idea NP) VP) . S) T Y -0.5 NP an idea P (a1 , . . . , aT ) = -0.0 -0.1 Model and Training Setup We reimplemented the generative model described in Choe and Charniak (2016) and trained it on the Penn Treebank (Marcus et al., 1993) using 1 The model described in Dyer et al. (2016) has only a single C LOSE action, whereas the model described in Choe and Charniak (2016) annotates C LOSE(X) actions with their nonterminals. We present the more general version here. -4.6 Figure 2: A plot of the action log probabilities log P (at |a1 , . . . , at−1 ) for the example in Figure 1 under our main model. We observe that O PEN and C LOSE actions have much higher probability than S HIFT actions. This imbalance is responsible for the failure of standard action-level beam search. their published hyperparameters and preprocessing. Howe"
D17-1178,J01-2004,0,0.426119,"unsom, 2015), among other tasks. We describe a related state-augmented beam search for neural generative constituency parsers in which lexical actions compete only with each other rather than with structural actions. Applying this inference procedure to the generative model of Choe and Charniak (2016), we find that it yields a self-contained generative parser that achieves high performance. Beyond this, we propose an enhanced candidate selection strategy that yields significant improvements for all beam sizes. Additionally, motivated by the look-ahead heuristic used in the top-down parsers of Roark (2001) and Charniak (2010), we also experiment with a simple coarse pruning function that allows us to reduce the number of states expanded per candidate by several times without compromising accuracy. Using our final search procedure, we surpass prior state-ofthe-art results among single-model parsers on the Penn Treebank, obtaining an F1 score of 92.56. 2 Common Framework The generative neural parsers of Dyer et al. (2016) and Choe and Charniak (2016) can be unified under a common shift-reduce framework. Both systems build parse trees in left-to-right depth-first order by executing a sequence of a"
D17-1178,P12-1046,0,0.154191,"Missing"
D17-1311,D13-1197,0,0.0477602,"Missing"
D17-1311,W14-1618,0,0.0389403,"lable at http://github. com/jacobandreas/rnn-syn. One of the distinguishing features of natural language is compositionality: the existence of operations like negation and coordination that can be applied to utterances with predictable effects on meaning. RNN models trained for natural language processing tasks have been found to learn representations that encode some of this compositional structure—for example, sentence representations for machine translation encode explicit features for certain syntactic phenomena (Shi et al., 2016) and represent some semantic relationships translationally (Levy et al., 2014). It is thus natural to ask whether these “language-like” structures also arise spontaneously in models trained directly from an environment signal. Rather than using language as a form of supervision, we propose to use it as a probe—exploiting post-hoc statistical correspondences between natural language descriptions and neural encodings to discover regular structure in representation space. To do this, we need to find (vector, string) pairs with matching semantics, which requires first aligning unpaired examples of human–human 2893 Proceedings of the 2017 Conference on Empirical Methods in N"
D17-1311,D16-1159,0,0.020168,"2017), even without natural language training data. 1 Code and data are available at http://github. com/jacobandreas/rnn-syn. One of the distinguishing features of natural language is compositionality: the existence of operations like negation and coordination that can be applied to utterances with predictable effects on meaning. RNN models trained for natural language processing tasks have been found to learn representations that encode some of this compositional structure—for example, sentence representations for machine translation encode explicit features for certain syntactic phenomena (Shi et al., 2016) and represent some semantic relationships translationally (Levy et al., 2014). It is thus natural to ask whether these “language-like” structures also arise spontaneously in models trained directly from an environment signal. Rather than using language as a form of supervision, we propose to use it as a probe—exploiting post-hoc statistical correspondences between natural language descriptions and neural encodings to discover regular structure in representation space. To do this, we need to find (vector, string) pairs with matching semantics, which requires first aligning unpaired examples of"
D17-1311,P17-1022,1,0.76693,"ze classification accuracy on randomly-generated scenes and target sets of the same form as in the G EN X dataset. 3 Approach We are not concerned with the RNN model’s raw performance on this task (it achieves nearly perfect accuracy). Instead, our goal is to explore what kinds of messages the model computes in order to achieve this accuracy—and specifically whether these messages contain high-level semantics and low-level structure similar to the referring expressions produced by humans. But how do we judge semantic equivalence between natural language and vector representations? Here, as in Andreas et al. (2017), we adopt an approach inspired by formal semantics, and represent the meaning of messages via their truth conditions (Figure 1). For every problem instance W in the dataset, we have access to one or more human messages e(W ) as well as the RNN encoding f (W ). The truth-conditional account of meaning suggests that we should judge e and f to be equivalent if they designate the same set of of objects in the world (Davidson, 1967). But it is not enough to compare their predictions solely in the context where they were generated—testing if JeKW = Jf KW — because any pair of models that achieve pe"
D17-1311,D14-1067,0,0.0246126,"ior essentially random.) Conversely, if there is a first-class notion of negation, we should be able to select an arbitrary representation vector f with an associated referring expression e, apply some transformation N to f , and be able to predict a priori how the decoder model will interpret the representation N f —i.e. in correspondence with ¬e. Here we make the strong assumption that the negation operation is not only predictable but linear. Previous work has found that linear operators are powerful enough to capture many hierarchical and relational structures (Paccanaro and Hinton, 2002; Bordes et al., 2014). Using examples (f, f 0 ) collected from the training set as described above, we compute P the least-squares ˆ estimate N = arg minN ||N f − f 0 ||22 . To evaluate, we collect example representations from the test set that are equivalent to known logical forms, and measure how frequently model behaviors rep(N f ) agree with the logical predictions x.red(x) x.red(x) x.blue(x) 2 x.red(x) 1 (b) blue(x) x.blue(x) yellow(x) 0 1 x.red(x) yellow(x) 2 3 x.yellow(x) 4 3 2 1 0 1 2 3 4 Figure 3: Principal components of structured message transformations discovered by our experiments. (a) Negation: black"
D17-1311,W14-4012,0,0.100823,"Missing"
D19-1225,P13-1021,1,0.814248,"he structure of each underlying form in order to be human readable – as shown in Figure 1. Modeling these stylistic attributes and how they compose with underlying character structure could aid typographic analysis and even allow for automatic generation of novel fonts. Further, the variability of these stylistic features presents a challenge for optical character recognition systems, which typically presume a library of known fonts. In the case of historical document recognition, for example, this problem is more pronounced due to the wide range of lost, ancestral fonts present in such data (Berg-Kirkpatrick et al., 2013; BergKirkpatrick and Klein, 2014). Models that capture this wide stylistic variation of glyph images may eventually be useful for improving optical character recognition on unknown fonts. In this work we present a probabilistic latent variable model capable of disentangling stylistic features of fonts from the underlying structure of each character. Our model represents the style of each font as a vector-valued latent variable, and parameterizes the structure of each character as a learned embedding. Critically, each style latent variable is shared by all characters within a font, while chara"
D19-1225,P14-2020,1,0.872487,"Missing"
H05-1010,J90-2002,0,0.468181,"Missing"
H05-1010,W02-1001,0,0.0585841,") for some user provided feature mapping f and abbreviate w f (x, y) =   jk yjk w f (xjk ). We can include in the feature vector the identity of the two words, their relative positions in their respective sentences, their part-of-speech tags, their string similarity (for detecting cognates), and so on. At this point, one can imagine estimating a linear matching model in multiple ways, including using conditional likelihood estimation, an averaged perceptron update (see which matchings are proposed and adjust the weights according to the diﬀerence between the guessed and target structures (Collins, 2002)), or in large-margin fashion. Conditional likelihood estimation using a log-linear model P (y |x) = 1  Zw (x) exp{w f (x, y)} requires summing over all matchings to compute the normalization Zw (x), which is #P-complete (Valiant, 1979). In our experiments, we therefore investigated the averaged perceptron in addition to the large-margin method outlined below. 2.1 Large-margin estimation We follow the large-margin formulation of Taskar et al. (2005a). Our input is a set of training instances {(xi , yi )}m i=1 , where each instance consists of a sentence pair xi and a target alignment yi . We"
H05-1010,C04-1032,0,0.305369,"nment of that pair. The alignment 73 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 73–80, Vancouver, October 2005. 2005 Association for Computational Linguistics for the sentence pair is then the highest scoring matching under some constraints, for example the requirement that matchings be one-to-one. This view of alignment as graph matching is not, in itself, new: Melamed (2000) uses competitive linking to greedily construct matchings where the pair score is a measure of wordto-word association, and Matusov et al. (2004) ﬁnd exact maximum matchings where the pair scores come from the alignment posteriors of generative models. Tiedemann (2003) proposes incorporating a variety of word association “clues” into a greedy linking algorithm. What we contribute here is a principled approach for tractable and eﬃcient learning of the alignment score sjk (e, f ) as a function of arbitrary features of that token pair. This contribution opens up the possibility of doing the kind of feature engineering for alignment that has been so successful for other NLP tasks. We ﬁrst present the algorithm for large margin estimation o"
H05-1010,J00-2004,0,0.54123,"ich each pair of words (ej , fk ) in a sentence pair (e, f ) is associated with a score sjk (e, f ) reﬂecting the desirability of the alignment of that pair. The alignment 73 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 73–80, Vancouver, October 2005. 2005 Association for Computational Linguistics for the sentence pair is then the highest scoring matching under some constraints, for example the requirement that matchings be one-to-one. This view of alignment as graph matching is not, in itself, new: Melamed (2000) uses competitive linking to greedily construct matchings where the pair score is a measure of wordto-word association, and Matusov et al. (2004) ﬁnd exact maximum matchings where the pair scores come from the alignment posteriors of generative models. Tiedemann (2003) proposes incorporating a variety of word association “clues” into a greedy linking algorithm. What we contribute here is a principled approach for tractable and eﬃcient learning of the alignment score sjk (e, f ) as a function of arbitrary features of that token pair. This contribution opens up the possibility of doing the kind"
H05-1010,W03-0301,0,0.0416674,"Missing"
H05-1010,J03-1002,0,0.374631,"about such features as orthographic similarity (for detecting cognates), presence of the pair in various dictionaries, similarity of the frequency of the two words, choices made by other alignment systems on this sentence pair, and so on. While clever models can implicitly capture some of these information sources, it takes considerable work, and can make the resulting models quite complex. A second drawback of generative translation models is that, since they are learned with EM, they require extensive processing of large amounts of data to achieve good performance. While tools like GIZA++ (Och and Ney, 2003) do make it easier to build on the long history of the generative IBM approach, they also underscore how complex high-performance generative models can, and have, become. In this paper, we present a discriminative approach to word alignment. Word alignment is cast as a maximum weighted matching problem (Cormen et al., 1990) in which each pair of words (ej , fk ) in a sentence pair (e, f ) is associated with a score sjk (e, f ) reﬂecting the desirability of the alignment of that pair. The alignment 73 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Nat"
H05-1010,E03-1026,0,0.0837238,"Missing"
H05-1010,C96-2141,0,0.949688,"he alignment in Figure 1(c), where one improvement may be attributable to the short pair feature – it has stopped proposing the-de, partially because the short pair feature downweights the score of that pair. A clearer example of these features making a diﬀerence is shown in Figure 2, where both the exact-match and character overlap fea3 The learned response was in fact close to a Gaussian, but harsher near zero displacement. tures are used. One source of constraint which our model still does not explicitly capture is the ﬁrst-order dependency between alignment positions, as in the HMM model (Vogel et al., 1996) and IBM models 4+. The the-le error in Figure 1(c) is symptomatic of this lack. In particular, it is a slightly better pair according to the Dice value than the correct the-les. However, the latter alignment has the advantage that major-grands follows it. To use this information source, we included a feature which gives the Dice value of the words following the pair.4 We also added a wordfrequency feature whose value is the absolute diﬀerence in log rank of the words, discouraging very common words from translating to very rare ones. Finally, we threw in bilexical features of the pairs of top"
J13-2005,P11-1060,1,0.174971,"Missing"
J13-2005,D11-1039,0,0.223903,"evised submission received: 19 February 2012; accepted for publication: 18 April 2012. doi:10.1162/COLI a 00127 No rights reserved. This work was authored as part of the Contributor’s ofﬁcial duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 39, Number 2 required for various semantics-related tasks (Zettlemoyer and Collins 2005; Branavan et al. 2009; Liang, Jordan, and Klein 2009; Clarke et al. 2010; Artzi and Zettlemoyer 2011; Goldwasser et al. 2011). In this article, we develop new techniques to learn accurate semantic parsers from even weaker supervision. We demonstrate our techniques on the concrete task of building a system to answer questions given a structured database of facts; see Figure 1 for an example in the domain of U.S. geography. This problem of building natural language interfaces to databases (NLIDBs) has a long history in NLP, starting from the early days of artiﬁcial intelligence with systems such as L UNAR (Woods, Kaplan, and Webber 1972), C HAT-80 (Warren and Pereira 1982), and many others (se"
J13-2005,P02-1041,0,0.0305829,"dependencies such as those arising from anaphora. For example, in the phrase a state with a river that traverses its capital, its binds to state, but this dependence cannot be captured in a tree structure. A solution is to simply add an edge between the its node and the state node that forces the two nodes to have the same value. The result is still a well-deﬁned CSP, though not a treestructured one. The situation would become trickier if we were to integrate the other relations (aggregate, mark, and execute). We might be able to incorporate some ideas from Hybrid Logic Dependency Semantics (Baldridge and Kruijff 2002; White 2006), given that hybrid logic extends the tree structures of modal logic with nominals, thereby allowing a node to freely reference other nodes. In this article, however, we will stick to trees and leave the full exploration of non-trees for future work. 2.4.2 Computation of Join Relations. So far, we have given a declarative deﬁnition of the denotation zw of a DCS tree z with only join relations. Now we will show how to compute zw efﬁciently. Recall that the denotation is the set of feasible values for the root node. In general, ﬁnding the solution to a CSP is NP-hard, but for tr"
J13-2005,C04-1180,0,0.0384063,"Missing"
J13-2005,P09-1010,0,0.0460809,"E-mail: klein@cs.berkeley.edu. Submission received: 12 September 2011; revised submission received: 19 February 2012; accepted for publication: 18 April 2012. doi:10.1162/COLI a 00127 No rights reserved. This work was authored as part of the Contributor’s ofﬁcial duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 39, Number 2 required for various semantics-related tasks (Zettlemoyer and Collins 2005; Branavan et al. 2009; Liang, Jordan, and Klein 2009; Clarke et al. 2010; Artzi and Zettlemoyer 2011; Goldwasser et al. 2011). In this article, we develop new techniques to learn accurate semantic parsers from even weaker supervision. We demonstrate our techniques on the concrete task of building a system to answer questions given a structured database of facts; see Figure 1 for an example in the domain of U.S. geography. This problem of building natural language interfaces to databases (NLIDBs) has a long history in NLP, starting from the early days of artiﬁcial intelligence with systems such as L UNAR (Woods, Ka"
J13-2005,P11-1028,0,0.0638172,"Missing"
J13-2005,P10-1129,0,0.0180014,"Missing"
J13-2005,W10-2903,0,0.738828,"12 September 2011; revised submission received: 19 February 2012; accepted for publication: 18 April 2012. doi:10.1162/COLI a 00127 No rights reserved. This work was authored as part of the Contributor’s ofﬁcial duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 39, Number 2 required for various semantics-related tasks (Zettlemoyer and Collins 2005; Branavan et al. 2009; Liang, Jordan, and Klein 2009; Clarke et al. 2010; Artzi and Zettlemoyer 2011; Goldwasser et al. 2011). In this article, we develop new techniques to learn accurate semantic parsers from even weaker supervision. We demonstrate our techniques on the concrete task of building a system to answer questions given a structured database of facts; see Figure 1 for an example in the domain of U.S. geography. This problem of building natural language interfaces to databases (NLIDBs) has a long history in NLP, starting from the early days of artiﬁcial intelligence with systems such as L UNAR (Woods, Kaplan, and Webber 1972), C HAT-80 (Warren and Pereir"
J13-2005,D09-1100,0,0.0262575,"Missing"
J13-2005,W05-0602,0,0.0145139,"ances. In response, against the backdrop of a statistical revolution in NLP during the 1990s, researchers began to build systems that could learn from examples, with the hope of overcoming the limitations of rule-based methods. One of the earliest statistical efforts was the C HILL system (Zelle and Mooney 1996), which learned a shift-reduce semantic parser. Since then, there has been a healthy line of work yielding increasingly more accurate semantic parsers by using new semantic representations and machine learning techniques (Miller et al. 1996; Zelle and Mooney 1996; Tang and Mooney 2001; Ge and Mooney 2005; Kate, Wong, and Mooney 2005; Zettlemoyer and Collins 2005; Kate and Mooney 2006; Wong and Mooney 2006; Kate and Mooney 2007; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Kwiatkowski et al. 2010, 2011). Although statistical methods provided advantages such as robustness and portability, however, their application in semantic parsing achieved only limited success. One of the main obstacles was that these methods depended crucially on having examples of utterances paired with logical forms, and this requires substantial human effort to obtain. Furthermore, the annotators must be proﬁcien"
J13-2005,P11-1149,0,0.0998202,"19 February 2012; accepted for publication: 18 April 2012. doi:10.1162/COLI a 00127 No rights reserved. This work was authored as part of the Contributor’s ofﬁcial duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 39, Number 2 required for various semantics-related tasks (Zettlemoyer and Collins 2005; Branavan et al. 2009; Liang, Jordan, and Klein 2009; Clarke et al. 2010; Artzi and Zettlemoyer 2011; Goldwasser et al. 2011). In this article, we develop new techniques to learn accurate semantic parsers from even weaker supervision. We demonstrate our techniques on the concrete task of building a system to answer questions given a structured database of facts; see Figure 1 for an example in the domain of U.S. geography. This problem of building natural language interfaces to databases (NLIDBs) has a long history in NLP, starting from the early days of artiﬁcial intelligence with systems such as L UNAR (Woods, Kaplan, and Webber 1972), C HAT-80 (Warren and Pereira 1982), and many others (see Androutsopoulos, Ritchi"
J13-2005,P06-1063,0,0.0215908,"Missing"
J13-2005,P06-1115,0,0.712544,"ing the 1990s, researchers began to build systems that could learn from examples, with the hope of overcoming the limitations of rule-based methods. One of the earliest statistical efforts was the C HILL system (Zelle and Mooney 1996), which learned a shift-reduce semantic parser. Since then, there has been a healthy line of work yielding increasingly more accurate semantic parsers by using new semantic representations and machine learning techniques (Miller et al. 1996; Zelle and Mooney 1996; Tang and Mooney 2001; Ge and Mooney 2005; Kate, Wong, and Mooney 2005; Zettlemoyer and Collins 2005; Kate and Mooney 2006; Wong and Mooney 2006; Kate and Mooney 2007; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Kwiatkowski et al. 2010, 2011). Although statistical methods provided advantages such as robustness and portability, however, their application in semantic parsing achieved only limited success. One of the main obstacles was that these methods depended crucially on having examples of utterances paired with logical forms, and this requires substantial human effort to obtain. Furthermore, the annotators must be proﬁcient in some formal language, which drastically reduces the size of the annotator po"
J13-2005,D10-1119,0,0.113656,"Missing"
J13-2005,D11-1140,0,0.662189,"Missing"
J13-2005,P09-1011,1,0.879128,"Missing"
J13-2005,N09-1069,1,0.74662,"Missing"
J13-2005,D08-1082,0,0.0334655,"Missing"
J13-2005,J93-2004,0,0.0450344,"Missing"
J13-2005,P96-1008,0,0.137423,"cult to scale up, both to other domains and to more complex utterances. In response, against the backdrop of a statistical revolution in NLP during the 1990s, researchers began to build systems that could learn from examples, with the hope of overcoming the limitations of rule-based methods. One of the earliest statistical efforts was the C HILL system (Zelle and Mooney 1996), which learned a shift-reduce semantic parser. Since then, there has been a healthy line of work yielding increasingly more accurate semantic parsers by using new semantic representations and machine learning techniques (Miller et al. 1996; Zelle and Mooney 1996; Tang and Mooney 2001; Ge and Mooney 2005; Kate, Wong, and Mooney 2005; Zettlemoyer and Collins 2005; Kate and Mooney 2006; Wong and Mooney 2006; Kate and Mooney 2007; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Kwiatkowski et al. 2010, 2011). Although statistical methods provided advantages such as robustness and portability, however, their application in semantic parsing achieved only limited success. One of the main obstacles was that these methods depended crucially on having examples of utterances paired with logical forms, and this requires substantial hum"
J13-2005,P06-1055,1,0.165372,"uld trigger only city because city is a prototype word, but town would trigger all the NN predicates (city, state, country, etc.) because it is not a prototype word. Prototype triggers require only a modest amount of domain-speciﬁc supervision (see the right side of Figure 22 for the entire list for G EO and J OBS). In fact, as we’ll see in Section 4.2, prototype triggers are not absolutely required to obtain good accuracies, but they give an extra boost and also improve computational efﬁciency by reducing the set of candidate DCS trees. 13 To perform POS tagging, we used the Berkeley Parser (Petrov et al. 2006), trained on the WSJ Treebank (Marcus, Marcinkiewicz, and Santorini 1993) and the Question Treebank (Judge, Cahill, and v. Genabith 2006)—thanks to Slav Petrov for providing the trained parser. 431 Computational Linguistics Volume 39, Number 2 Figure 22 Lexical triggers used in our experiments. Finally, to determine triggering, we stem all words using the Porter stemmer (Porter 1980), so that mountains triggers the same predicates as mountain. We also decompose superlatives into two words (e.g., largest is mapped to most large), allowing us to construct the logical form more compositionally. 4"
J13-2005,P03-1067,0,0.0845904,"Missing"
J13-2005,P10-1083,0,0.0353523,"Missing"
J13-2005,J82-3002,0,0.456079,"larke et al. 2010; Artzi and Zettlemoyer 2011; Goldwasser et al. 2011). In this article, we develop new techniques to learn accurate semantic parsers from even weaker supervision. We demonstrate our techniques on the concrete task of building a system to answer questions given a structured database of facts; see Figure 1 for an example in the domain of U.S. geography. This problem of building natural language interfaces to databases (NLIDBs) has a long history in NLP, starting from the early days of artiﬁcial intelligence with systems such as L UNAR (Woods, Kaplan, and Webber 1972), C HAT-80 (Warren and Pereira 1982), and many others (see Androutsopoulos, Ritchie, and Thanisch [1995] for an overview). We believe NLIDBs provide an appropriate starting point for semantic parsing because they lead directly to practical systems, and they allow us to temporarily sidestep intractable philosophical questions on how to represent meaning in general. Early NLIDBs were quite successful in their respective limited domains, but because these systems were constructed from manually built rules, they became difﬁcult to scale up, both to other domains and to more complex utterances. In response, against the backdrop of a"
J13-2005,N06-1056,0,0.608415,"hers began to build systems that could learn from examples, with the hope of overcoming the limitations of rule-based methods. One of the earliest statistical efforts was the C HILL system (Zelle and Mooney 1996), which learned a shift-reduce semantic parser. Since then, there has been a healthy line of work yielding increasingly more accurate semantic parsers by using new semantic representations and machine learning techniques (Miller et al. 1996; Zelle and Mooney 1996; Tang and Mooney 2001; Ge and Mooney 2005; Kate, Wong, and Mooney 2005; Zettlemoyer and Collins 2005; Kate and Mooney 2006; Wong and Mooney 2006; Kate and Mooney 2007; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Kwiatkowski et al. 2010, 2011). Although statistical methods provided advantages such as robustness and portability, however, their application in semantic parsing achieved only limited success. One of the main obstacles was that these methods depended crucially on having examples of utterances paired with logical forms, and this requires substantial human effort to obtain. Furthermore, the annotators must be proﬁcient in some formal language, which drastically reduces the size of the annotator pool, dampening any hope"
J13-2005,P07-1121,0,0.654479,"from examples, with the hope of overcoming the limitations of rule-based methods. One of the earliest statistical efforts was the C HILL system (Zelle and Mooney 1996), which learned a shift-reduce semantic parser. Since then, there has been a healthy line of work yielding increasingly more accurate semantic parsers by using new semantic representations and machine learning techniques (Miller et al. 1996; Zelle and Mooney 1996; Tang and Mooney 2001; Ge and Mooney 2005; Kate, Wong, and Mooney 2005; Zettlemoyer and Collins 2005; Kate and Mooney 2006; Wong and Mooney 2006; Kate and Mooney 2007; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Kwiatkowski et al. 2010, 2011). Although statistical methods provided advantages such as robustness and portability, however, their application in semantic parsing achieved only limited success. One of the main obstacles was that these methods depended crucially on having examples of utterances paired with logical forms, and this requires substantial human effort to obtain. Furthermore, the annotators must be proﬁcient in some formal language, which drastically reduces the size of the annotator pool, dampening any hope of acquiring enough data to fulﬁll the visi"
J13-2005,D07-1071,0,0.712536,"he hope of overcoming the limitations of rule-based methods. One of the earliest statistical efforts was the C HILL system (Zelle and Mooney 1996), which learned a shift-reduce semantic parser. Since then, there has been a healthy line of work yielding increasingly more accurate semantic parsers by using new semantic representations and machine learning techniques (Miller et al. 1996; Zelle and Mooney 1996; Tang and Mooney 2001; Ge and Mooney 2005; Kate, Wong, and Mooney 2005; Zettlemoyer and Collins 2005; Kate and Mooney 2006; Wong and Mooney 2006; Kate and Mooney 2007; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Kwiatkowski et al. 2010, 2011). Although statistical methods provided advantages such as robustness and portability, however, their application in semantic parsing achieved only limited success. One of the main obstacles was that these methods depended crucially on having examples of utterances paired with logical forms, and this requires substantial human effort to obtain. Furthermore, the annotators must be proﬁcient in some formal language, which drastically reduces the size of the annotator pool, dampening any hope of acquiring enough data to fulﬁll the vision of learning highly accurate"
J13-2005,D13-1160,1,\N,Missing
N03-1016,J98-2004,0,0.762807,"coverage grammars and long sentences, even cubic algorithms can be far too expensive in practice. Two primary types of methods for accelerating parse selection have been proposed. Roark (2001) and Ratnaparkhi (1999) use a beam-search strategy, in which only the best n parses are tracked at any moment. Parsing time is linear and can be made arbitrarily fast by reducing n. This is a greedy strategy, and the actual Viterbi (highest probability) parse can be pruned from the beam because, while it is globally optimal, it may not be locally optimal at every parse stage. Chitrao and Grishman (1990), Caraballo and Charniak (1998), Charniak et al. (1998), and Collins (1999) describe best-first parsing, which is intended for a tabular item-based framework. In best-first parsing, one builds a figure-of-merit (FOM) over parser items, and uses the FOM to decide the order in which agenda items should be processed. This approach also dramatically reduces the work done during parsing, though it, too, gives no guarantee that the first parse returned is the actual Viterbi parse (nor does it maintain a worst-case cubic time bound). We discuss best-first parsing further in section 3.3. Both of these speed-up techniques are based"
N03-1016,W98-1115,0,0.422114,"ences, even cubic algorithms can be far too expensive in practice. Two primary types of methods for accelerating parse selection have been proposed. Roark (2001) and Ratnaparkhi (1999) use a beam-search strategy, in which only the best n parses are tracked at any moment. Parsing time is linear and can be made arbitrarily fast by reducing n. This is a greedy strategy, and the actual Viterbi (highest probability) parse can be pruned from the beam because, while it is globally optimal, it may not be locally optimal at every parse stage. Chitrao and Grishman (1990), Caraballo and Charniak (1998), Charniak et al. (1998), and Collins (1999) describe best-first parsing, which is intended for a tabular item-based framework. In best-first parsing, one builds a figure-of-merit (FOM) over parser items, and uses the FOM to decide the order in which agenda items should be processed. This approach also dramatically reduces the work done during parsing, though it, too, gives no guarantee that the first parse returned is the actual Viterbi parse (nor does it maintain a worst-case cubic time bound). We discuss best-first parsing further in section 3.3. Both of these speed-up techniques are based on greedy models of pars"
N03-1016,H90-1053,0,0.0156624,"ever, when dealing with wide-coverage grammars and long sentences, even cubic algorithms can be far too expensive in practice. Two primary types of methods for accelerating parse selection have been proposed. Roark (2001) and Ratnaparkhi (1999) use a beam-search strategy, in which only the best n parses are tracked at any moment. Parsing time is linear and can be made arbitrarily fast by reducing n. This is a greedy strategy, and the actual Viterbi (highest probability) parse can be pruned from the beam because, while it is globally optimal, it may not be locally optimal at every parse stage. Chitrao and Grishman (1990), Caraballo and Charniak (1998), Charniak et al. (1998), and Collins (1999) describe best-first parsing, which is intended for a tabular item-based framework. In best-first parsing, one builds a figure-of-merit (FOM) over parser items, and uses the FOM to decide the order in which agenda items should be processed. This approach also dramatically reduces the work done during parsing, though it, too, gives no guarantee that the first parse returned is the actual Viterbi parse (nor does it maintain a worst-case cubic time bound). We discuss best-first parsing further in section 3.3. Both of these"
N03-1016,W97-0302,0,0.0947193,"to create new edges. For example, NP:[0,2] might be removed from the agenda, and, if there were a rule S → NP VP and VP :[2,8] was already entered into the chart, the edge S:[0,8] would be formed, and added to the agenda if it were not in the chart already. The way an A* parser differs from a classic chart parser is that, like a best-first parser, agenda edges are processed according to a priority. In best-first parsing, this priority is called a figure-of-merit (FOM), and is based on various approximations to P (e|s), the fraction of parses of a sentence s which include an edge e (though see Goodman (1997) for an alternative notion of FOM). Edges which seem promising are explored first; others can wait on the agenda indefinitely. Note that even if we did know P (e|s) exactly, we still would not know whether e occurs in any best parse of s. Nonetheless, good FOMs empirically lead quickly to good parses. Best-first parsing aims to find a (hopefully good) parse quickly, but gives no guarantee that the first parse discovered is the Viterbi parse, nor does it allow one to recognize the Viterbi parse when it is found. In A* parsing, we wish to construct priorities which will speed up parsing, yet sti"
N03-1016,J98-4004,0,0.0807007,"rie encodings of rules. S1 XLR N SXL Outside-Trie Rules NP → XNP→ · NN NN 0.4 XNP→ · NN → DT JJ 0.75 XNP→ · NN → DT NN 0.25 SXR Edges Blocked F Original Rules NP → DT JJ NN 0.3 NP → DT NN NN 0.1 Figure 6: Fraction of edges saved by using various estimate methods, for two rule encodings. O - TRIE is a deterministic right-branching trie encoding (Leermakers, 1992) with weights pushed left (Mohri, 1997). I - TRIE is a non-deterministic leftbranching trie with weights on rule entry as in Charniak et al. (1998). states, such as by annotating nodes with their parent and even grandparent categories (Johnson, 1998). This annotation multiplies out the state space, giving a much larger grammar, and projecting back to the unannotated state set can be used as an outside estimate. Second, and perhaps more importantly, this technique can be applied to lexical parsing, where the state projections are onto the delexicalized PCFG symbols and/or onto the word-word dependency structures. This is particularly effective when the tree model takes a certain factored form; see Klein and Manning (2003) for details. 3.3 Parsing Performance Following (Charniak et al., 1998), we parsed unseen sentences of length 18–26 from"
N03-1016,W01-1812,1,0.920619,"een words). An inside parse of an edge e = X:[i, j] is a derivation in G from X to i wj . Let βG (e, s) denote the log-probability of a best inside parse of e (its Viterbi inside score).1 We will drop the G, s, and even e when context permits. Our parser, like a best-first parser, maintains estimates b(e, s) of β(e, s) which begin at −∞, only increase over time, and always represent the score of the best parses of their edges e discovered so far. Optimality means that for any e, b(e, s) will equal βG (e, s) when e is removed from the agenda. If one uses b(e, s) to prioritize edges, we show in Klein and Manning (2001a), that the parser is optimal over arbitrary PCFGs, and a wide range of control strategies. This is proved using an extension of Dijkstra’s algorithm to a certain kind of hypergraph associated with parsing, shown in figure 1(b): parse items are nodes in the hypergraph, hyperarcs take sets of parse items to their result item, and hyperpaths map to parses. Reachability from start corresponds to parseability, and shortest paths to Viterbi parses. 1 Our use of inside score and outside score evokes the same picture as talk about inside and outside probabilities, but note that in this paper inside"
N03-1016,P01-1044,1,0.840817,"een words). An inside parse of an edge e = X:[i, j] is a derivation in G from X to i wj . Let βG (e, s) denote the log-probability of a best inside parse of e (its Viterbi inside score).1 We will drop the G, s, and even e when context permits. Our parser, like a best-first parser, maintains estimates b(e, s) of β(e, s) which begin at −∞, only increase over time, and always represent the score of the best parses of their edges e discovered so far. Optimality means that for any e, b(e, s) will equal βG (e, s) when e is removed from the agenda. If one uses b(e, s) to prioritize edges, we show in Klein and Manning (2001a), that the parser is optimal over arbitrary PCFGs, and a wide range of control strategies. This is proved using an extension of Dijkstra’s algorithm to a certain kind of hypergraph associated with parsing, shown in figure 1(b): parse items are nodes in the hypergraph, hyperarcs take sets of parse items to their result item, and hyperpaths map to parses. Reachability from start corresponds to parseability, and shortest paths to Viterbi parses. 1 Our use of inside score and outside score evokes the same picture as talk about inside and outside probabilities, but note that in this paper inside"
N03-1016,J97-2003,0,0.024176,"S1 SX R SX M LR S S1 X LR SX 100 90 80 70 60 50 40 30 20 10 0 SX TRUE SX L SX S NULL U LL SXR NULL Inside-Trie Rules NP → XDT JJ NN 0.3 NP → XDT NN NN 0.1 XDT JJ → DT JJ 1.0 XDT NN → DT NN 1.0 Figure 5: Two trie encodings of rules. S1 XLR N SXL Outside-Trie Rules NP → XNP→ · NN NN 0.4 XNP→ · NN → DT JJ 0.75 XNP→ · NN → DT NN 0.25 SXR Edges Blocked F Original Rules NP → DT JJ NN 0.3 NP → DT NN NN 0.1 Figure 6: Fraction of edges saved by using various estimate methods, for two rule encodings. O - TRIE is a deterministic right-branching trie encoding (Leermakers, 1992) with weights pushed left (Mohri, 1997). I - TRIE is a non-deterministic leftbranching trie with weights on rule entry as in Charniak et al. (1998). states, such as by annotating nodes with their parent and even grandparent categories (Johnson, 1998). This annotation multiplies out the state space, giving a much larger grammar, and projecting back to the unannotated state set can be used as an outside estimate. Second, and perhaps more importantly, this technique can be applied to lexical parsing, where the state projections are onto the delexicalized PCFG symbols and/or onto the word-word dependency structures. This is particularl"
N03-1016,J01-2004,0,0.0257984,"chieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time. 1 Introduction PCFG parsing algorithms with worst-case cubic-time bounds are well-known. However, when dealing with wide-coverage grammars and long sentences, even cubic algorithms can be far too expensive in practice. Two primary types of methods for accelerating parse selection have been proposed. Roark (2001) and Ratnaparkhi (1999) use a beam-search strategy, in which only the best n parses are tracked at any moment. Parsing time is linear and can be made arbitrarily fast by reducing n. This is a greedy strategy, and the actual Viterbi (highest probability) parse can be pruned from the beam because, while it is globally optimal, it may not be locally optimal at every parse stage. Chitrao and Grishman (1990), Caraballo and Charniak (1998), Charniak et al. (1998), and Collins (1999) describe best-first parsing, which is intended for a tabular item-based framework. In best-first parsing, one builds a"
N03-1016,J03-4003,0,\N,Missing
N03-1033,A00-1031,0,0.11107,"Missing"
N03-1033,P98-1029,0,0.133187,"Missing"
N03-1033,J95-4004,0,0.136762,"2 ), or centered tags (t−1 , t0 , t+1 ) respectively. Again, with roughly equivalent feature sets, the left context is better than the right, and the centered context is better than either unidirectional context. line for this task high, while substantial annotator noise creates an unknown upper bound on the task. 3.2 Lexicalization Lexicalization has been a key factor in the advance of statistical parsing models, but has been less exploited for tagging. Words surrounding the current word have been occasionally used in taggers, such as (Ratnaparkhi, 1996), Brill’s transformation based tagger (Brill, 1995), and the HMM model of Lee et al. (2000), but nevertheless, the only lexicalization consistently included in tagging models is the dependence of the part of speech tag of a word on the word itself. In maximum entropy models, joint features which look at surrounding words and their tags, as well as joint features of the current word and surrounding words are in principle straightforward additions, but have not been incorporated into previous models. We have found these features to be very useful. We explore here lexicalization both alone and in combination with preceding and following tag histo"
N03-1033,A88-1019,0,0.138214,"Missing"
N03-1033,W02-1001,0,0.159577,"ned to maximize the conditional likelihood over the training data of that node. At test time, the sequence with the highest product of local conditional scores is calculated and returned. We can always find the exact maximizing sequence, but only in the case of an acyclic net is it guaranteed to be the maximum likelihood sequence. 3 Experiments The part of speech tagged data used in our experiments is the Wall Street Journal data from Penn Treebank III (Marcus et al., 1994). We extracted tagged sentences from the parse trees.5 We split the data into training, development, and test sets as in (Collins, 2002). Table 1 lists character5 Note that these tags (and sentences) are not identical to those obtained from the tagged/pos directories of the same disk: hundreds of tags in the RB/RP/IN set were changed to be more consistent in the parsed/mrg version. Maybe we were the last to discover this, but we’ve never seen it in print. istics of the three splits.6 Except where indicated for the model BEST, all results are on the development set. One innovation in our reporting of results is that we present whole-sentence accuracy numbers as well as the traditional per-tag accuracy measure (over all tokens,"
N03-1033,P99-1069,0,0.144081,"Missing"
N03-1033,W02-1002,1,0.136285,"he tag at a certain position, the obvious thing to do is to explicitly include in the local model all predictive features, no matter on which side of the target position they lie. There are two good formal reasons to expect that a model explicitly conditioning on both sides at each position, like figure 1(c) could be advantageous. First, because of smoothing effects and interaction with other conditioning features (like the words), left-to-right factors like P(t0 |t−1 , w0 ) do not always suffice when t0 is implicitly needed to determine t−1 . For example, consider a case of observation bias (Klein and Manning, 2002) for a first-order left-toright CMM. The word to has only one tag (TO) in the PTB tag set. The TO tag is often preceded by nouns, but rarely by modals (MD). In a sequence will to fight, that trend indicates that will should be a noun rather than a modal verb. However, that effect is completely lost in a CMM like (a): P(twill |will, hstar ti) prefers the modal tagging, and P(TO |to, twill ) is roughly 1 regardless of twill . While the model has an arrow between the two tag positions, that path of influence is severed.3 The same problem exists in the other direction. If we use the symmetric righ"
N03-1033,P00-1034,0,0.118545,"Missing"
N03-1033,W96-0213,0,0.139267,"one wants to find a coherent sentence interpretation). Further, while some tag errors matter much more than others, to a first cut getting a single tag wrong in many of the more common ways (e.g., proper noun vs. common noun; noun vs. verb) would lead to errors in a subsequent processor such as an information extraction system or a parser that would greatly degrade results for the entire sentence. Finally, the fact that the measure has much more dynamic range has some appeal when reporting tagging results. The per-state models in this paper are log-linear models, building upon the models in (Ratnaparkhi, 1996) and (Toutanova and Manning, 2000), though some models are in fact strictly simpler. The features in the models are defined using templates; there are different templates for rare words aimed at learning the correct tags for unknown words.7 We present the results of three classes of experiments: experiments with directionality, experiments with lexicalization, and experiments with smoothing. 3.1 Experiments with Directionality In this section, we report experiments using log-linear CMMs to populate nets with various structures, exploring the relative value of neighboring words’ tags. Table 2 l"
N03-1033,P99-1023,0,0.086251,"Missing"
N03-1033,W00-1308,1,0.229218,"erent sentence interpretation). Further, while some tag errors matter much more than others, to a first cut getting a single tag wrong in many of the more common ways (e.g., proper noun vs. common noun; noun vs. verb) would lead to errors in a subsequent processor such as an information extraction system or a parser that would greatly degrade results for the entire sentence. Finally, the fact that the measure has much more dynamic range has some appeal when reporting tagging results. The per-state models in this paper are log-linear models, building upon the models in (Ratnaparkhi, 1996) and (Toutanova and Manning, 2000), though some models are in fact strictly simpler. The features in the models are defined using templates; there are different templates for rare words aimed at learning the correct tags for unknown words.7 We present the results of three classes of experiments: experiments with directionality, experiments with lexicalization, and experiments with smoothing. 3.1 Experiments with Directionality In this section, we report experiments using log-linear CMMs to populate nets with various structures, exploring the relative value of neighboring words’ tags. Table 2 lists the discussed networks. All n"
N03-1033,W99-0606,1,\N,Missing
N03-1033,J93-2004,0,\N,Missing
N03-1033,C98-1029,0,\N,Missing
N06-1014,P04-1061,1,0.146867,"= argmax θ + P x,z P |x; θ1 )p2 (z |x; θ2 ), q(z; x) log p1 (x, z; θ1 ) q(z; x) log p2 (x, z; θ2 ), x,z where Zx is a normalization constant. The M-step decouples neatly into two independent optimization problems, which lead to single model updates using the expected counts from q(z; x). To compute Zx in the E-step, we must sum the product of two model posteriors over the set of possible zs with nonzero probability under both models. In general, if both posterior distributions over the latent variables z decompose in the same tractable manner, as in the context-free grammar induction work of Klein and Manning (2004), the summation could be carried out efficiently, for example using dynamic programming. In our case, we would have to sum over the set of alignments where each word in English is aligned to at most one word in French and each word in French is aligned to at most one word in English. Unfortunately, for even very simple models such as IBM 1 or 2, computing the normalization constant over this set of alignments is a #P -complete problem, by a reduction from counting matchings in a bipartite graph (Valiant, 1979). We could perhaps attempt to compute q using a variety of approximate probabilistic"
N06-1014,N03-1017,0,0.0774831,"but the words between them are not good translations of each other. If the intervening English words were null-aligned, we would have to pay a big distortion penalty for jumping 4 positions. On the other hand, if the edge (i+2, j+2) were included, that penalty would be mitigated. The translation cost for forcing that edge is smaller than the distortion cost. 4.2 BLEU evaluation To see whether our improvement in AER also improves BLEU score, we aligned 100K EnglishFrench sentences from the Europarl corpus and tested on 3000 sentences of length 5–15. Using GIZA++ model 4 alignments and Pharaoh (Koehn et al., 2003), we achieved a BLEU score of 0.3035. By using alignments from our jointly trained HMMs instead, we get a BLEU score of 0.3051. While this improvement is very modest, we are currently investigating alternative ways of interfacing with phrase table construction to make a larger impact on translation quality. 5 Related Work Our approach is similar in spirit to co-training, where two classifiers, complementary by the virtue of having different views of the data, are trained jointly to encourage agreement (Blum and Mitchell, 1998; Collins and Singer, 1999). One key difference in our work is that w"
N06-1014,C04-1032,0,0.0868641,"Missing"
N06-1014,P04-1066,0,0.438888,"er ` a la r´ eunion et en avons inform´ e le cojo en cons´ equence . E→F: 89.9/93.6/8.7 F→E: 92.2/93.5/7.3 Intersection: 96.5/91.4/5.7 Figure 1: An example of the Viterbi output of a pair of independently trained HMMs (top) and a pair of jointly trained HMMs (bottom), both trained on 1.1 million sentences. Rounded boxes denote possible alignments, square boxes are sure alignments, and solid boxes are model predictions. For each model, the overall Precision/Recall/AER on the development set is given. See Section 4 for details. this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions. Intersection eliminates the spurious alignments, but at the expense of recall. Intersection after training produces alignments that both models agree on. The joint training procedure we describe below builds on this idea by encouraging the models to agree during training. Consider the output of the jointly trained HMMs in Figure 1 (bottom). The garbage-collecting rare word is 106 no longer a problem. Not only are the individual E→F and F→E jointly-trained models better than their independently-trained counterparts, the jointlytrained intersected model also p"
N06-1014,H05-1011,0,0.0429226,"Missing"
N06-1014,C96-2141,0,0.989159,"Missing"
N06-1014,J03-1002,0,0.151201,"R on the standard English-French Hansards task. To our knowledge, this is the lowest published unsupervised AER result, and it is competitive with supervised approaches. Furthermore, our approach is very practical: it is no harder to implement than a standard HMM model, and joint training is no slower than the standard training of two HMM models. Finally, we show that word alignments from our system can be used in a phrasebased translation system to modestly improve BLEU score. 2 Alignment models: IBM 1, 2 and HMM We briefly review the sequence-based word alignment models (Brown et al., 1994; Och and Ney, 2003) and describe some of the choices in our implementation. All three modelsP are generative models of the form p(f |e) = a p(a, f |e), where e = (e1 , . . . , eI ) is the English sentence, f = (f1 , . . . , fJ ) is the French sentence, and a = (a1 , . . . , aJ ) is the (asymmetric) alignment which specifies the position of an English word aligned to each French word. All three models factor in the following way: p(a, f |e) = J Y pd (aj |aj− , j)pt (fj |eaj ), (1) j=1 where j− is the position of the last non-null-aligned French word before position j.2 The translation parameters pt (fj |eaj ) are"
N06-1014,H05-1010,1,0.627741,"Missing"
N06-1014,W99-0613,0,0.0563426,"5. Using GIZA++ model 4 alignments and Pharaoh (Koehn et al., 2003), we achieved a BLEU score of 0.3035. By using alignments from our jointly trained HMMs instead, we get a BLEU score of 0.3051. While this improvement is very modest, we are currently investigating alternative ways of interfacing with phrase table construction to make a larger impact on translation quality. 5 Related Work Our approach is similar in spirit to co-training, where two classifiers, complementary by the virtue of having different views of the data, are trained jointly to encourage agreement (Blum and Mitchell, 1998; Collins and Singer, 1999). One key difference in our work is that we rely exclusively on data likelihood to guide the two models in an unsupervised manner, rather than relying on an initial handful of labeled examples. The idea of exploiting agreement between two latent variable models is not new; there has been substantial previous work on leveraging the strengths of two complementary models. Klein and Manning (2004) combine two complementary models for grammar induction, one that models constituency and one that models dependency, in a manner broadly similar to the current work. Aside from investigating a different"
N06-1014,H05-1012,0,0.0809116,"Missing"
N06-1014,J93-2003,0,\N,Missing
N06-1015,J90-2002,0,0.326638,"Missing"
N06-1015,W02-1001,0,0.0274772,"ks. First, they require extensive tuning and processing of large amounts of data which, for the better-performing models, is Recently, Moore (2005) proposed a discriminative model in which pairs of sentences (e, f ) and proposed alignments a are scored using a linear combination of arbitrary features computed from the tuples (a, e, f ). While there are no restrictions on the form of the model features, the problem of finding the highest scoring alignment is very difficult and involves heuristic search. Moreover, the parameters of the model must be estimated using averaged perceptron training (Collins, 2002), which can be unstable. In contrast, Taskar et al. (2005) cast word alignment as a maximum weighted matching problem, in which each pair of words (ej , fk ) in a sentence pair (e, f ) is associated with a score sjk (e, f ) reflecting the desirability of the alignment of that pair. Importantly, this problem is computationally tractable. The alignment for the sentence pair is the highest scoring matching under constraints (such as the constraint that matchings be one-to-one). The scoring model sjk (e, f ) can be based on a rich feature set defined on word pairs (ej , fk ) and their context, inc"
N06-1015,N03-1017,0,0.010122,"Missing"
N06-1015,N06-1014,1,0.681615,"be solved efficiently by making use of a linear relaxation of QAP for the min-max formulation of large-margin estimation (Taskar, 2004). We show that these two extensions yield significant improvements in error rates when compared to the bipartite matching model. The addition of a fertility model improves the AER by 0.4. Modeling first-order interactions improves the AER by 1.8. Combining the two extensions results in an improve113 ment in AER of 2.3, yielding alignments of better quality than intersected IBM Model 4. Moreover, including predictions of bi-directional IBM Model 4 and model of Liang et al. (2006) as features, we achieve an absolute AER of 3.8 on the EnglishFrench Hansards alignment task—the best AER result published on this task to date. 2 Models We begin with a quick summary of the maximum weight bipartite matching model in (Taskar et al., 2005). More precisely, nodes V = V s ∪ V t correspond to words in the “source” (V s ) and “target” (V t ) sentences, and edges E = {jk : j ∈ V s , k ∈ V t } correspond to alignments between word pairs.1 The edge weights sjk represent the degree to which word j in one sentence can be translated using the word k in the other sentence. The predicted a"
N06-1015,W03-0301,0,0.135372,"Missing"
N06-1015,H05-1011,0,0.344188,"nment is a key component of most endto-end statistical machine translation systems. The standard approach to word alignment is to construct directional generative models (Brown et al., 1990; Och and Ney, 2003), which produce a sentence in one language given the sentence in another language. While these models require sentence-aligned bitexts, they can be trained with no further supervision, using EM. Generative alignment models do, however, have serious drawbacks. First, they require extensive tuning and processing of large amounts of data which, for the better-performing models, is Recently, Moore (2005) proposed a discriminative model in which pairs of sentences (e, f ) and proposed alignments a are scored using a linear combination of arbitrary features computed from the tuples (a, e, f ). While there are no restrictions on the form of the model features, the problem of finding the highest scoring alignment is very difficult and involves heuristic search. Moreover, the parameters of the model must be estimated using averaged perceptron training (Collins, 2002), which can be unstable. In contrast, Taskar et al. (2005) cast word alignment as a maximum weighted matching problem, in which each"
N06-1015,J03-1002,0,0.0258002,"Missing"
N06-1015,H05-1010,1,0.870332,"ing of large amounts of data which, for the better-performing models, is Recently, Moore (2005) proposed a discriminative model in which pairs of sentences (e, f ) and proposed alignments a are scored using a linear combination of arbitrary features computed from the tuples (a, e, f ). While there are no restrictions on the form of the model features, the problem of finding the highest scoring alignment is very difficult and involves heuristic search. Moreover, the parameters of the model must be estimated using averaged perceptron training (Collins, 2002), which can be unstable. In contrast, Taskar et al. (2005) cast word alignment as a maximum weighted matching problem, in which each pair of words (ej , fk ) in a sentence pair (e, f ) is associated with a score sjk (e, f ) reflecting the desirability of the alignment of that pair. Importantly, this problem is computationally tractable. The alignment for the sentence pair is the highest scoring matching under constraints (such as the constraint that matchings be one-to-one). The scoring model sjk (e, f ) can be based on a rich feature set defined on word pairs (ej , fk ) and their context, including measures of association, orthography, relative posi"
N06-1015,C96-2141,0,0.573205,"Missing"
N06-1041,W01-0713,0,0.00845908,"prototype word to take only its given label(s) at training time. As we show in section 5, this does not work well in practice because this constraint on the model is very sparse. In providing a prototype, however, we generally mean something stronger than a constraint on that word. In particular, we may intend that words which are in some sense similar to a prototype generally be given the same label(s) as that prototype. 4.3 Distributional Similarity In syntactic distributional clustering, words are grouped on the basis of the vectors of their preceeding and following words (Sch¨utze, 1995; Clark, 2001). The underlying linguistic idea is that replacing a word with another word of the same syntactic category should preserve syntactic well-formedness (Radford, 1988). We present more details in section 5, but for now assume that a similarity function over word types is given. Suppose further that for each non-prototype word type w, we have a subset of prototypes, Sw , which are known to be distributionally similar to w (above some threshold). We would like our model to relate the tags of w to those of Sw . One approach to enforcing the distributional assumption in a sequence model is by supplem"
N06-1041,P05-1046,1,0.331653,"legal tags for each word. Such dictionaries are large and embody a great deal of lexical knowledge. A prototype list, in contrast, is extremely compact. 3 Tasks and Related Work: Extraction Label ROOMATES RESTRICTIONS UTILITIES AVAILABLE SIZE PHOTOS RENT CONTACT FEATURES NEIGHBORHOOD ADDRESS BOUNDARY Figure 2: Prototype list derived from the development set of the CLASSIFIEDS data. The BOUNDARY field is not present in the original annotation, but added to model boundaries (see Section 5.3). The starred tokens are the results of collapsing of basic entities during pre-processing as is done in (Grenager et al., 2005) ments the model learns to segment with a reasonable match to the target structure. In section 5.3, we discuss an approach to this task which does not require customization of model structure, but rather centers on feature engineering. 4 Grenager et al. (2005) presents an unsupervised approach to an information extraction task, called CLASSIFIEDS here, which involves segmenting classified advertisements into topical sections (see figure 1(c)). Labels in this domain tend to be “sticky” in that the correct annotation tends to consist of multi-element fields of the same label. The overall approac"
N06-1041,C02-1145,0,0.0491726,"Missing"
N06-1041,E95-1020,0,0.951958,"Missing"
N06-1041,P05-1044,0,0.442729,"ighly related work here. One approach to unsupervised learning of partof-speech models is to induce HMMs from unlabeled data in a maximum-likelihood framework. For example, Merialdo (1991) presents experiments learning HMMs using EM. Merialdo’s results most famously show that re-estimation degrades accuracy unless almost no examples are labeled. Less famously, his results also demonstrate that reestimation can improve tagging accuracies to some degree in the fully unsupervised case. One recent and much more successful approach to part-of-speech learning is contrastive estimation, presented in Smith and Eisner (2005). They utilize task-specific comparison neighborhoods for part-ofspeech tagging to alter their objective function. Both of these works require specification of the legal tags for each word. Such dictionaries are large and embody a great deal of lexical knowledge. A prototype list, in contrast, is extremely compact. 3 Tasks and Related Work: Extraction Label ROOMATES RESTRICTIONS UTILITIES AVAILABLE SIZE PHOTOS RENT CONTACT FEATURES NEIGHBORHOOD ADDRESS BOUNDARY Figure 2: Prototype list derived from the development set of the CLASSIFIEDS data. The BOUNDARY field is not present in the original a"
N06-1041,I05-3005,0,0.0226211,"Missing"
N06-1041,J93-2004,0,\N,Missing
N06-1041,J94-2001,0,\N,Missing
N07-1051,P05-1022,0,0.727191,"uage-specific tuning. 1 Introduction Treebank parsing comprises two problems: learning, in which we must select a model given a treebank, and inference, in which we must select a parse for a sentence given the learned model. Previous work has shown that high-quality unlexicalized PCFGs can be learned from a treebank, either by manual annotation (Klein and Manning, 2003) or automatic state splitting (Matsuzaki et al., 2005; Petrov et al., 2006). In particular, we demonstrated in Petrov et al. (2006) that a hierarchically split PCFG could exceed the accuracy of lexicalized PCFGs (Collins, 1999; Charniak and Johnson, 2005). However, many questions about inference with such split PCFGs remain open. In this work, we present 1. an effective method for pruning in split PCFGs 2. a comparison of objective functions for inference in split PCFGs, 3. experiments on automatic splitting for languages other than English. In Sec. 3, we present a novel coarse-to-fine processing scheme for hierarchically split PCFGs. Our In Sec. 4, we consider the well-known issue of inference objectives in split PCFGs. As in many model families (Steedman, 2000; Vijay-Shanker and Joshi, 1985), split PCFGs have a derivation / parse distinction"
N07-1051,N06-1022,0,0.577631,"Missing"
N07-1051,J99-1004,0,0.010554,"ections Recall that our final state-split grammars G come, by their construction process, with an ontogeny of grammars Gi where each grammar is a (partial) splitting of the preceding one. This gives us a natural chain of projections πi→j which projects backwards along this ontogeny of grammars (see Fig. 1). Of course, training also gives us parameters for the grammars, but only the chain of projections is needed. Note that the projected estimates need not 1 Whether or not the system has solutions depends on the parameters of the grammar. In particular, G may be improper, though the results of Chi (1999) imply that G will be proper if it is the maximum-likelihood estimate of a finite treebank. 407 (and in general will not) recover the original parameters exactly, nor would we want them to. Instead they take into account any smoothing, substate drift, and so on which occurred by the final grammar. Starting from the base grammar, we run the projection process for each stage in the sequence, calculating πi (chained incremental projections would also be possible). For the remainder of the paper, except where noted otherwise, all coarser grammars’ estimates are these reconstructions, rather than t"
N07-1051,N06-1043,0,0.0211945,"ees may be far from that of the treebank. Third, the meanings of the split states can and do drift between splitting stages. Fourth, and most importantly, we may wish to project grammars for which treebank estimation is problematic, for example, grammars which are more refined than the observed treebank grammars. Our method effectively avoids all of these problems by rebuilding and refitting the pruning grammars on the fly from the final grammar. 3.2.1 Estimating Projected Grammars Fortunately, there is a well worked-out notion of estimating a grammar from an infinite distribution over trees (Corazza and Satta, 2006). In particular, we can estimate parameters for a projected grammar π(G) from the tree distribution induced by G (which can itself be estimated in any manner). The earliest work that we are aware of on estimating models from models in this way is that of Nederhof (2005), who considers the case of learning language mod406 els from other language models. Corazza and Satta (2006) extend these methods to the case of PCFGs and tree distributions. The generalization of maximum likelihood estimation is to find the estimates for π(G) with minimum KL divergence from the tree distribution induced by G."
N07-1051,H05-1100,0,0.0553983,"osed form. very similar results (see Fig. 2), the M AX -RULE P RODUCT algorithm consistently outperformed the other two. Overall, the closed-form options were superior to the reranking ones, except on exact match, where the gains from correctly calculating the risk outweigh the losses from the truncation of the candidate set. 5 Multilingual Parsing Most research on parsing has focused on English and parsing performance on other languages is generally significantly lower.3 Recently, there have been some attempts to adapt parsers developed for English to other languages (Levy and Manning, 2003; Cowan and Collins, 2005). Adapting lexicalized parsers to other languages in not a trivial task as it requires at least the specification of head rules, and has had limited success. Adapting unlexicalized parsers appears to be equally difficult: Levy and Manning (2003) adapt the unlexicalized parser of Klein and Manning (2003) to Chinese, but even after significant efforts on choosing category splits, only modest performance gains are reported. In contrast, automatically learned grammars like the one of Matsuzaki et al. (2005) and Petrov et al. (2006) require a treebank for training but no additional human input. One"
N07-1051,W06-1638,0,0.02718,"This procedure gives an ontogeny of grammars Gi , where G = Gn is the final grammar. Empirically, the gains on the English Penn treebank level off after 6 rounds. In Petrov et al. (2006), some simple smoothing is also shown to be effective. It is interesting to note that these grammars capture many of the “structural zeros” described by Mohri and Roark (2006) and pruning rules with probability below e−10 reduces the grammar size drastically without influencing parsing performance. Some of our methods and conclusions are relevant to all state-split grammars, such as Klein and Manning (2003) or Dreyer and Eisner (2006), while others apply most directly to the hierarchical case. 3 Search When working with large grammars, it is standard to prune the search space in some way. In the case of lexicalized grammars, the unpruned chart often will not even fit in memory for long sentences. Several proven techniques exist. Collins (1999) combines a punctuation rule which eliminates many spans entirely, and then uses span-synchronous beams to prune in a bottom-up fashion. Charniak et al. (1998) 405 introduces best-first parsing, in which a figure-ofmerit prioritizes agenda processing. Most relevant to our work is Char"
N07-1051,P05-1039,0,0.0132973,"Missing"
N07-1051,W06-1673,0,0.0205353,",0,n) TG = argmaxT TG = argmaxT TG = argmaxT Q e∈T P Q q(e) e∈T q(e) e∈T q(e) Figure 3: Different objectives for parsing with posteriors, yielding comparable results. A, B, C are nonterminal symbols, x, y, z are latent annotations and i, j, k are between-word indices. Hence (Ax , i, j) denotes a constituent labeled with Ax spanning from i to j. Furthermore, we write e = (A → B C, i, j, k) for brevity. form. Exact match (likelihood) has this property. In general, however, we can approximate the expectation with samples from P (T |w, G). The method for sampling derivations of a PCFG is given in Finkel et al. (2006) and Johnson et al. (2007). It requires a single inside-outside computation per sentence and is then efficient per sample. Note that for split grammars, a posterior parse sample can be drawn by sampling a derivation and projecting away the substates. Fig. 2 shows the results of the following experiment. We constructed 10-best lists from the full grammar G in Sec. 2 using the parser of Petrov et al. (2006). We then took the same grammar and extracted 500-sample lists using the method of Finkel et al. (2006). The minimum risk parse candidate was selected for various loss functions. As can be see"
N07-1051,W01-0521,0,0.0825259,"y of features which cannot be captured by a generative model. Space does not permit a thorough exposition of our analysis, but as in the case of English (Petrov et al., 2006), the learned subcategories exhibit interesting linguistic interpretations. In German, for example, the model learns subcategories for different cases and genders. 5.2 Corpus Variation Related to cross language generalization is the generalization across domains for the same language. It is well known that a model trained on the Wall Street Journal loses significantly in performance when evaluated on the Brown Corpus (see Gildea (2001) for more details and the exact setup of their experiment, which we duplicated here). Recently McClosky et al. (2006) came to the conclusion that this performance drop is not due to overfitting the WSJ data. Fig. 4 shows the performance on the Brown corpus during hierarchical training. While the F1 score on the WSJ is rising we observe a drop in performance after the 5th iteration, suggesting that some overfitting is occurring. ≤ 40 words all LP LR LP LR ENGLISH Charniak et al. (2005) 90.1 90.1 89.5 89.6 Petrov et al. (2006) 90.3 90.0 89.8 89.6 This Paper 90.7 90.5 90.2 89.9 Parser ENGLISH (re"
N07-1051,P96-1024,0,0.21727,"arse-to-fine processing scheme for hierarchically split PCFGs. Our In Sec. 4, we consider the well-known issue of inference objectives in split PCFGs. As in many model families (Steedman, 2000; Vijay-Shanker and Joshi, 1985), split PCFGs have a derivation / parse distinction. The split PCFG directly describes a generative model over derivations, but evaluation is sensitive only to the coarser treebank symbols. While the most probable parse problem is NP-complete (Sima’an, 1992), several approximate methods exist, including n-best reranking by parse likelihood, the labeled bracket algorithm of Goodman (1996), and a variational approximation introduced in Matsuzaki et al. (2005). We present experiments which explicitly minimize various evaluation risks over a candidate set using samples from the split PCFG, and relate those conditions to the existing non-sampling algorithms. We demonstrate that n-best reranking according to likelihood is superior for exact match, and that the non-reranking methods are superior for maximizing F1 . A specific contribution is to discuss the role of unary productions, which previous work has glossed over, but which is important in understanding why the various methods"
N07-1051,N07-1018,0,0.27516,"rgmaxT TG = argmaxT Q e∈T P Q q(e) e∈T q(e) e∈T q(e) Figure 3: Different objectives for parsing with posteriors, yielding comparable results. A, B, C are nonterminal symbols, x, y, z are latent annotations and i, j, k are between-word indices. Hence (Ax , i, j) denotes a constituent labeled with Ax spanning from i to j. Furthermore, we write e = (A → B C, i, j, k) for brevity. form. Exact match (likelihood) has this property. In general, however, we can approximate the expectation with samples from P (T |w, G). The method for sampling derivations of a PCFG is given in Finkel et al. (2006) and Johnson et al. (2007). It requires a single inside-outside computation per sentence and is then efficient per sample. Note that for split grammars, a posterior parse sample can be drawn by sampling a derivation and projecting away the substates. Fig. 2 shows the results of the following experiment. We constructed 10-best lists from the full grammar G in Sec. 2 using the parser of Petrov et al. (2006). We then took the same grammar and extracted 500-sample lists using the method of Finkel et al. (2006). The minimum risk parse candidate was selected for various loss functions. As can be seen, in most cases, risk min"
N07-1051,P03-1054,1,0.366151,"minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning. 1 Introduction Treebank parsing comprises two problems: learning, in which we must select a model given a treebank, and inference, in which we must select a parse for a sentence given the learned model. Previous work has shown that high-quality unlexicalized PCFGs can be learned from a treebank, either by manual annotation (Klein and Manning, 2003) or automatic state splitting (Matsuzaki et al., 2005; Petrov et al., 2006). In particular, we demonstrated in Petrov et al. (2006) that a hierarchically split PCFG could exceed the accuracy of lexicalized PCFGs (Collins, 1999; Charniak and Johnson, 2005). However, many questions about inference with such split PCFGs remain open. In this work, we present 1. an effective method for pruning in split PCFGs 2. a comparison of objective functions for inference in split PCFGs, 3. experiments on automatic splitting for languages other than English. In Sec. 3, we present a novel coarse-to-fine process"
N07-1051,P03-1056,0,0.292247,"cting assumptions, in closed form. very similar results (see Fig. 2), the M AX -RULE P RODUCT algorithm consistently outperformed the other two. Overall, the closed-form options were superior to the reranking ones, except on exact match, where the gains from correctly calculating the risk outweigh the losses from the truncation of the candidate set. 5 Multilingual Parsing Most research on parsing has focused on English and parsing performance on other languages is generally significantly lower.3 Recently, there have been some attempts to adapt parsers developed for English to other languages (Levy and Manning, 2003; Cowan and Collins, 2005). Adapting lexicalized parsers to other languages in not a trivial task as it requires at least the specification of head rules, and has had limited success. Adapting unlexicalized parsers appears to be equally difficult: Levy and Manning (2003) adapt the unlexicalized parser of Klein and Manning (2003) to Chinese, but even after significant efforts on choosing category splits, only modest performance gains are reported. In contrast, automatically learned grammars like the one of Matsuzaki et al. (2005) and Petrov et al. (2006) require a treebank for training but no a"
N07-1051,J93-2004,0,0.0589627,"Missing"
N07-1051,P05-1010,0,0.949072,"tical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning. 1 Introduction Treebank parsing comprises two problems: learning, in which we must select a model given a treebank, and inference, in which we must select a parse for a sentence given the learned model. Previous work has shown that high-quality unlexicalized PCFGs can be learned from a treebank, either by manual annotation (Klein and Manning, 2003) or automatic state splitting (Matsuzaki et al., 2005; Petrov et al., 2006). In particular, we demonstrated in Petrov et al. (2006) that a hierarchically split PCFG could exceed the accuracy of lexicalized PCFGs (Collins, 1999; Charniak and Johnson, 2005). However, many questions about inference with such split PCFGs remain open. In this work, we present 1. an effective method for pruning in split PCFGs 2. a comparison of objective functions for inference in split PCFGs, 3. experiments on automatic splitting for languages other than English. In Sec. 3, we present a novel coarse-to-fine processing scheme for hierarchically split PCFGs. Our In Sec"
N07-1051,P06-1043,0,0.257607,"our analysis, but as in the case of English (Petrov et al., 2006), the learned subcategories exhibit interesting linguistic interpretations. In German, for example, the model learns subcategories for different cases and genders. 5.2 Corpus Variation Related to cross language generalization is the generalization across domains for the same language. It is well known that a model trained on the Wall Street Journal loses significantly in performance when evaluated on the Brown Corpus (see Gildea (2001) for more details and the exact setup of their experiment, which we duplicated here). Recently McClosky et al. (2006) came to the conclusion that this performance drop is not due to overfitting the WSJ data. Fig. 4 shows the performance on the Brown corpus during hierarchical training. While the F1 score on the WSJ is rising we observe a drop in performance after the 5th iteration, suggesting that some overfitting is occurring. ≤ 40 words all LP LR LP LR ENGLISH Charniak et al. (2005) 90.1 90.1 89.5 89.6 Petrov et al. (2006) 90.3 90.0 89.8 89.6 This Paper 90.7 90.5 90.2 89.9 Parser ENGLISH (reranked) Charniak et al. (2005)4 92.4 91.6 91.8 91.0 GERMAN Dubey (2005) F1 76.3 This Paper 80.8 80.7 80.1 80.1 CHINES"
N07-1051,N06-1040,0,0.0174503,", for example DT might become DT-1 and DT-2. The refined grammar is estimated using a variant of the forward-backward algorithm (Matsuzaki et al., 2005). After a splitting stage, many splits are rolled back based on (an approximation to) their likelihood gain. This procedure gives an ontogeny of grammars Gi , where G = Gn is the final grammar. Empirically, the gains on the English Penn treebank level off after 6 rounds. In Petrov et al. (2006), some simple smoothing is also shown to be effective. It is interesting to note that these grammars capture many of the “structural zeros” described by Mohri and Roark (2006) and pruning rules with probability below e−10 reduces the grammar size drastically without influencing parsing performance. Some of our methods and conclusions are relevant to all state-split grammars, such as Klein and Manning (2003) or Dreyer and Eisner (2006), while others apply most directly to the hierarchical case. 3 Search When working with large grammars, it is standard to prune the search space in some way. In the case of lexicalized grammars, the unpruned chart often will not even fit in memory for long sentences. Several proven techniques exist. Collins (1999) combines a punctuatio"
N07-1051,J05-2002,0,0.0109786,"han the observed treebank grammars. Our method effectively avoids all of these problems by rebuilding and refitting the pruning grammars on the fly from the final grammar. 3.2.1 Estimating Projected Grammars Fortunately, there is a well worked-out notion of estimating a grammar from an infinite distribution over trees (Corazza and Satta, 2006). In particular, we can estimate parameters for a projected grammar π(G) from the tree distribution induced by G (which can itself be estimated in any manner). The earliest work that we are aware of on estimating models from models in this way is that of Nederhof (2005), who considers the case of learning language mod406 els from other language models. Corazza and Satta (2006) extend these methods to the case of PCFGs and tree distributions. The generalization of maximum likelihood estimation is to find the estimates for π(G) with minimum KL divergence from the tree distribution induced by G. Since π(G) is a grammar over coarser symbols, we fit π(G) to the distribution G induces over π-projected trees: P (π(T )|G). The proofs of the general case are given in Corazza and Satta (2006), but the resulting procedure is quite intuitive. Given a (fully observed) tr"
N07-1051,P06-1055,1,0.770953,", we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning. 1 Introduction Treebank parsing comprises two problems: learning, in which we must select a model given a treebank, and inference, in which we must select a parse for a sentence given the learned model. Previous work has shown that high-quality unlexicalized PCFGs can be learned from a treebank, either by manual annotation (Klein and Manning, 2003) or automatic state splitting (Matsuzaki et al., 2005; Petrov et al., 2006). In particular, we demonstrated in Petrov et al. (2006) that a hierarchically split PCFG could exceed the accuracy of lexicalized PCFGs (Collins, 1999; Charniak and Johnson, 2005). However, many questions about inference with such split PCFGs remain open. In this work, we present 1. an effective method for pruning in split PCFGs 2. a comparison of objective functions for inference in split PCFGs, 3. experiments on automatic splitting for languages other than English. In Sec. 3, we present a novel coarse-to-fine processing scheme for hierarchically split PCFGs. Our In Sec. 4, we consider the w"
N07-1051,A97-1014,0,0.0456117,"Missing"
N07-1051,N04-1032,0,0.00799659,"Missing"
N07-1051,W06-1666,0,0.0158259,"re X is an evaluation symbol (such as NP) and k is some indicator of a subcategory, such as a parent annotation. G induces a derivation distribution P (T |G) over trees T labeled with split symbols. This distribution in turn induces a parse distribution P (T ′ |G) = P (π(T )|G) over (projected) trees with P unsplit evaluation symbols, where P (T ′ |G) = T :T ′ =π(T ) P (T |G). We now have several choices of how to select a tree given these posterior distributions over trees. In this section, we present experiments with the various options and explicitly relate them to parse risk minimization (Titov and Henderson, 2006). 408 G0 Nonterminals 98 Rules 3,700 No pruning 52 min X-bar pruning 8 min C-to-F (no loss) 6 min F1 for above 64.8 C-to-F (lossy) 6 min F1 for above 64.3 G2 G4 G6 219 498 1140 19,600 126,100 531,200 99 min 288 min 1612 min 14 min 30 min 111 min 12 min 16 min 20 min 85.2 89.7 91.2 8 min 9 min 11 min 84.7 89.4 91.1 Table 1: Grammar sizes, parsing times and accuracies for hierarchically split PCFGs with and without hierarchical coarse-tofine parsing on our development set (1578 sentences with 40 or less words from section 22 of the Penn Treebank). For comparison the parser of Charniak and Johnso"
N07-1051,P85-1011,0,0.243768,"eed the accuracy of lexicalized PCFGs (Collins, 1999; Charniak and Johnson, 2005). However, many questions about inference with such split PCFGs remain open. In this work, we present 1. an effective method for pruning in split PCFGs 2. a comparison of objective functions for inference in split PCFGs, 3. experiments on automatic splitting for languages other than English. In Sec. 3, we present a novel coarse-to-fine processing scheme for hierarchically split PCFGs. Our In Sec. 4, we consider the well-known issue of inference objectives in split PCFGs. As in many model families (Steedman, 2000; Vijay-Shanker and Joshi, 1985), split PCFGs have a derivation / parse distinction. The split PCFG directly describes a generative model over derivations, but evaluation is sensitive only to the coarser treebank symbols. While the most probable parse problem is NP-complete (Sima’an, 1992), several approximate methods exist, including n-best reranking by parse likelihood, the labeled bracket algorithm of Goodman (1996), and a variational approximation introduced in Matsuzaki et al. (2005). We present experiments which explicitly minimize various evaluation risks over a candidate set using samples from the split PCFG, and rel"
N07-1051,C02-1145,0,0.0573703,"Missing"
N07-1051,H86-1020,0,\N,Missing
N07-1051,C02-1126,0,\N,Missing
N07-1051,J03-4003,0,\N,Missing
N07-1052,N04-1035,0,0.0372704,"onolingual outside score αs (X [i, j]) is the minimal costs for any completion of the edge. Hence, αs (X [i, j]) ≤ cs (os ) and αt (X [k, l]) ≤ ct (ot ). Admissibility follows. 3.2 Experiments We demonstrate our technique using the synchronous grammar formalism of tree-to-tree transducers (Knight and Graehl, 2004). In each weighted rule, an aligned pair of nonterminals generates two ordered lists of children. The non-terminals in each list must align one-to-one to the non-terminals in the other, while the terminals are placed freely on either side. Figure 3(a) shows an example rule. Following Galley et al. (2004), we learn a grammar by projecting English syntax onto a foreign language via word-level alignments, as in figure 3(b).7 We parsed 1200 English-Spanish sentences using a grammar learned from 40,000 sentence pairs of the English-Spanish Europarl corpus.8 Figure 4(a) shows that A∗ expands substantially fewer states while searching for the optimal parse with our op7 The bilingual corpus consists of translation pairs with fixed English parses and word alignments. Rules were scored by their relative frequencies. 8 Rare words were replaced with their parts of speech to limit the memory consumption o"
N07-1052,N04-1014,0,0.0271972,"synchronous tree. o projects monolingual completions os and ot which have well-defined costs cs (os ) and ct (ot ) under Gs and Gt respectively. Their sum cs (os ) + ct (ot ) will underestimate αG (e) by pointwise admissibility. Furthermore, the heuristic we compute underestimates this sum. Recall that the monolingual outside score αs (X [i, j]) is the minimal costs for any completion of the edge. Hence, αs (X [i, j]) ≤ cs (os ) and αt (X [k, l]) ≤ ct (ot ). Admissibility follows. 3.2 Experiments We demonstrate our technique using the synchronous grammar formalism of tree-to-tree transducers (Knight and Graehl, 2004). In each weighted rule, an aligned pair of nonterminals generates two ordered lists of children. The non-terminals in each list must align one-to-one to the non-terminals in the other, while the terminals are placed freely on either side. Figure 3(a) shows an example rule. Following Galley et al. (2004), we learn a grammar by projecting English syntax onto a foreign language via word-level alignments, as in figure 3(b).7 We parsed 1200 English-Spanish sentences using a grammar learned from 40,000 sentence pairs of the English-Spanish Europarl corpus.8 Figure 4(a) shows that A∗ expands substan"
N07-1052,P04-1084,0,0.0128318,"ompletion costs, we more heavily penalize overestimating the cost by the constant C. 2.2 Bounding Search Error In the case where we allow pointwise inadmissibility, i.e. variables γa− , we can bound our search er− ror. Suppose γmax = maxa∈A γa− and that L∗ is the length of the longest optimal solution for the − , original problem. Then, h(s) ≤ h∗ (s) + L∗ γmax ∀s ∈ S. This -admissible heuristic (Ghallab and − .3 Allard, 1982) bounds our search error by L∗ γmax 3 Bitext Parsing In bitext parsing, one jointly infers a synchronous phrase structure tree over a sentence ws and its translation wt (Melamed et al., 2004; Wu, 1997). Bitext parsing is a natural candidate task for our approximate factoring technique. A synchronous tree projects monolingual phrase structure trees onto each sentence. However, the costs assigned by a weighted synchronous grammar (WSG) G do not typically factor into independent monolingual WCFGs. We can, however, produce a useful surrogate: a pair of monolingual WCFGs with structures projected by G and weights that, when combined, underestimate the costs of G. Parsing optimally relative to a synchronous grammar using a dynamic program requires time O(n6 ) in the length of the sente"
N07-1052,P00-1056,0,0.0597597,"is slow or even intractable. One general technique to increase efficiency while preserving optimality is A∗ search (Hart et al., 1968); however, successfully using A∗ search is challenging in practice. The design of admissible (or nearly admissible) heuristics which are both effective (close to actual completion costs) and also efficient to compute is a difficult, open problem in most domains. As a result, most work on search has focused on non-optimal methods, such as beam search or pruning based on approximate models (Collins, 1999), though in certain cases admissible heuristics are known (Och and Ney, 2000; Zhang and Gildea, 2006). For example, Klein and Manning (2003) show a class of projection-based A∗ estimates, but their application is limited to models which have a very restrictive kind of score decomposition. In this work, we broaden their projectionbased technique to give A∗ estimates for models which do not factor in this restricted way. Like Klein and Manning (2003), we focus on search problems where there are multiple projections or “views” of the structure, for example lexical parsing, in which trees can be projected onto either their CFG backbone or their lexical attachments. We use"
N07-1052,J97-3002,0,0.107366,"re heavily penalize overestimating the cost by the constant C. 2.2 Bounding Search Error In the case where we allow pointwise inadmissibility, i.e. variables γa− , we can bound our search er− ror. Suppose γmax = maxa∈A γa− and that L∗ is the length of the longest optimal solution for the − , original problem. Then, h(s) ≤ h∗ (s) + L∗ γmax ∀s ∈ S. This -admissible heuristic (Ghallab and − .3 Allard, 1982) bounds our search error by L∗ γmax 3 Bitext Parsing In bitext parsing, one jointly infers a synchronous phrase structure tree over a sentence ws and its translation wt (Melamed et al., 2004; Wu, 1997). Bitext parsing is a natural candidate task for our approximate factoring technique. A synchronous tree projects monolingual phrase structure trees onto each sentence. However, the costs assigned by a weighted synchronous grammar (WSG) G do not typically factor into independent monolingual WCFGs. We can, however, produce a useful surrogate: a pair of monolingual WCFGs with structures projected by G and weights that, when combined, underestimate the costs of G. Parsing optimally relative to a synchronous grammar using a dynamic program requires time O(n6 ) in the length of the sentence (Wu, 19"
N07-1052,W06-1627,0,0.0770748,"tractable. One general technique to increase efficiency while preserving optimality is A∗ search (Hart et al., 1968); however, successfully using A∗ search is challenging in practice. The design of admissible (or nearly admissible) heuristics which are both effective (close to actual completion costs) and also efficient to compute is a difficult, open problem in most domains. As a result, most work on search has focused on non-optimal methods, such as beam search or pruning based on approximate models (Collins, 1999), though in certain cases admissible heuristics are known (Och and Ney, 2000; Zhang and Gildea, 2006). For example, Klein and Manning (2003) show a class of projection-based A∗ estimates, but their application is limited to models which have a very restrictive kind of score decomposition. In this work, we broaden their projectionbased technique to give A∗ estimates for models which do not factor in this restricted way. Like Klein and Manning (2003), we focus on search problems where there are multiple projections or “views” of the structure, for example lexical parsing, in which trees can be projected onto either their CFG backbone or their lexical attachments. We use general optimization tec"
N07-1052,J03-4003,0,\N,Missing
N09-1008,P98-1043,0,0.700062,"The ancestral form in this case has been presumed to be /taNis/ in Blust (1993). We are interested in models which take as input many such word tuples, each representing a cognate group, along with a language tree, and induce word forms for hidden ancestral languages. The traditional approach to this problem has been the comparative method, in which reconstructions are done manually using assumptions about the relative probability of different kinds of sound change (Hock, 1986). There has been work attempting to automate part (Durham and Rogers, 1969; Eastlack, 1977; Lowe and Mazaudon, 1994; Covington, 1998; 65 First, linguists triangulate reconstructions from many languages, while past work has been limited to small numbers of languages. For example, Oakes (2000) used four languages to reconstruct Proto-Malayo-Javanic (PMJ) and Bouchard-Cˆot´e et al. (2008) used two languages to reconstruct Classical Latin (La). We revisit these small datasets and show that our method significantly outperforms these previous systems. However, we also show that our method can be applied to a much larger data set (Greenhill et al., 2008), reconstructing ProtoOceanic (POc) from 64 modern languages. In addition, pe"
N09-1008,D08-1113,0,0.0372375,"]. MARKEDNESS consists of language-specific ngram indicator functions for all symbols in Σ. Only unigram and bigram features are used for computational reasons, but we show in Section 5 that this already captures important constraints. Examples in Figure 1 (d): the bigram indicator 1[(n g)@Kw] (Kw stands for Kwara’ae, a language of the Solomon Islands), the unigram indicators 1[(n)@Kw] and 1[(g)@Kw]. FAITHFULNESS consists of indicators for mutation events of the form 1[x → y], where x ∈ Σ, y ∈ S . Examples: 1[N → n], 1[N → n@Kw]. Feature templates similar to these can be found for instance in Dreyer et al. (2008) and Chen (2003), in the context of string-to-string transduction. Note also the connection with stochastic OT (Goldwater and Johnson, 2003; Wilson, 2006), where a loglinear model mediates markedness and faithfulness of the production of an output form from an underlying input form. 3.3 Parameter sharing Data sparsity is a significant challenge in protolanguage reconstruction. While the experiments we present here use an order of magnitude more languages than previous computational approaches, the increase in observed data also brings with it additional unknowns in the form of intermediate pro"
N09-1008,C69-0501,0,0.468563,"on of word forms in several Oceanic languages, all meaning to cry. The ancestral form in this case has been presumed to be /taNis/ in Blust (1993). We are interested in models which take as input many such word tuples, each representing a cognate group, along with a language tree, and induce word forms for hidden ancestral languages. The traditional approach to this problem has been the comparative method, in which reconstructions are done manually using assumptions about the relative probability of different kinds of sound change (Hock, 1986). There has been work attempting to automate part (Durham and Rogers, 1969; Eastlack, 1977; Lowe and Mazaudon, 1994; Covington, 1998; 65 First, linguists triangulate reconstructions from many languages, while past work has been limited to small numbers of languages. For example, Oakes (2000) used four languages to reconstruct Proto-Malayo-Javanic (PMJ) and Bouchard-Cˆot´e et al. (2008) used two languages to reconstruct Classical Latin (La). We revisit these small datasets and show that our method significantly outperforms these previous systems. However, we also show that our method can be applied to a much larger data set (Greenhill et al., 2008), reconstructing Pr"
N09-1008,A00-2038,0,0.339903,"rm the CENTROID baseline (1.79). 5.3 Incorporating prior linguistic knowledge The model also supports the addition of prior linguistic knowledge. This takes the form of feature templates with more internal structure. We performed experiments with an additional feature template: STRUCT-FAITHFULNESS is a structured version of FAITHFULNESS , replacing x and y with their natural classes Nβ (x) and Nβ (y) where β indexes types of classes, ranging over {manner, place, phonation, isOral, isCentral, height, backness, roundedness}. This feature set is reminiscent of the featurized rep72 resentation of Kondrak (2000). We compared the performance of the system with and without STRUCT- FAITHFULNESS to check if the algorithm can recover the structure of natural classes in an unsupervised fashion. We found that with 2 or 4 observed languages, FAITHFULNESS underperformed STRUCT- FAITHFULNESS, but for larger trees, the difference was not significant. FAITH FULNESS even slightly outperformed its structured cousin with 16 observed languages. 6 Conclusion By enriching our model to include important features like markedness, and by scaling up to much larger data sets than were previously possible, we obtained subst"
N09-1008,J94-3004,0,0.58952,"ages, all meaning to cry. The ancestral form in this case has been presumed to be /taNis/ in Blust (1993). We are interested in models which take as input many such word tuples, each representing a cognate group, along with a language tree, and induce word forms for hidden ancestral languages. The traditional approach to this problem has been the comparative method, in which reconstructions are done manually using assumptions about the relative probability of different kinds of sound change (Hock, 1986). There has been work attempting to automate part (Durham and Rogers, 1969; Eastlack, 1977; Lowe and Mazaudon, 1994; Covington, 1998; 65 First, linguists triangulate reconstructions from many languages, while past work has been limited to small numbers of languages. For example, Oakes (2000) used four languages to reconstruct Proto-Malayo-Javanic (PMJ) and Bouchard-Cˆot´e et al. (2008) used two languages to reconstruct Classical Latin (La). We revisit these small datasets and show that our method significantly outperforms these previous systems. However, we also show that our method can be applied to a much larger data set (Greenhill et al., 2008), reconstructing ProtoOceanic (POc) from 64 modern languages"
N09-1008,C98-1043,0,\N,Missing
N09-1026,J98-2004,0,0.0963594,"Missing"
N09-1026,P05-1033,0,0.212091,"Missing"
N09-1026,D07-1079,0,0.0420296,"Missing"
N09-1026,N04-1035,0,0.470513,"Missing"
N09-1026,P06-1121,0,0.278456,"Missing"
N09-1026,P07-1019,0,0.0526295,"nts. These parameters were tuned on a development set. 233 Language Model Integration Large n-gram language models (LMs) are critical to the performance of machine translation systems. Recent innovations have managed the complexity of LM integration using multi-pass architectures. Zhang and Gildea (2008) describes a coarse-to-fine approach that iteratively increases the order of the LM. Petrov et al. (2008) describes an additional coarse-to-fine hierarchy over language projections. Both of these approaches integrate LMs via bottomup dynamic programs that employ beam search. As an alternative, Huang and Chiang (2007) describes a forest-based reranking algorithm called cube growing, which also employs beam search, but focuses computation only where necessary in a top-down pass through a parse forest. In this section, we show that the coarse-to-fine idea of constraining each pass using marginal predictions of the previous pass also applies effectively to cube growing. Max marginal predictions from the parse can substantially reduce LM integration time. 6.1 Language Model Forest Reranking Parsing produces a forest of derivations, where each edge in the forest holds its Viterbi (or one-best) derivation under"
N09-1026,P03-1054,1,0.0267897,"quires. (b) Chomsky S ! NNPnormal no dabaform, una bofetada DT NN verde Because transducer rules are very flat and contain speS cific lexical items, binarization introduces a large number of intermediate grammar symbols. Rule size affectDT parsing complexity whether (c) and lexicalization NNP NN the grammar is binarized explicitly (Zhang et al., did not binarized slap the green witch 2006) orMary implicitly using Early-style intermediate symbols (Zollmann et al., 2006). Moreover, no daba una bofetada a la bruja verde theMaria resulting binary rules cannot be Markovized to merge symbols, as in Klein and Manning (2003), because each rule is associated with a target-side tree that cannot be abstracted. We also do not restrict the form of rules in the Right-branching 8,095 grammar, a common technique in syntactic machine Left-branching translation. For instance, Zollmann 5,871 et al. (2006) follow Chiang (2005) in disallowing adjacent nonGreedy 1,101 terminals. Optimal (ILP) Watanabe 443 et al. (2006) limit grammars to Griebach-Normal form. However, general tree 0 3,000 excellent 6,000 translation 9,000 transducer grammars provide performance (Galley et al., 2006), and so we focus on parsing with all availabl"
N09-1026,D07-1104,0,0.0305308,"mic program is organized into spans Sij and computes the Viterbi score w(i, j, X) for each edge Sij [X], the weight of the maximum parse over words i+1 to j, rooted at symbol X. For each Sij , computation proceeds in three phases: binary, lexical, and unary. 4.1 w(i, j, X) = max(wl (i, j, X), wb (i, j, X)). To efficiently compute mappings, we store lexical rules in a trie (or suffix array) – a searchable graph that indexes rules according to their sequence of lexical items and non-terminals. This data structure has been used similarly to index whole training sentences for efficient retrieval (Lopez, 2007). To find all rules that map onto a span, we traverse the trie using depth-first search. 4.3 max r=X→X1 X2 w(i, j, X) = j−1 ωr max w(i, k, X1 ) · w(k, j, X2 ). k=i+1 The quantities w(i, k, X1 ) and w(k, j, X2 ) will have already been computed by the dynamic program. The work in this phase is cubic in sentence length. Applying Lexical Rules On the other hand, lexical rules in LNF can be applied without binarization, because they only apply to particular spans that contain the appropriate lexical items. For a given Sij , we first compute all the legal mappings of each rule onto the span. A mappi"
N09-1026,W06-1606,0,0.0341851,"Missing"
N09-1026,N07-1051,1,0.838206,"Missing"
N09-1026,P06-1055,1,0.723838,"ogram of the size of rules applicable to a typical 30-word sentence appears in Figure 2. The grammar includes 149 grammatical symbols, an augmentation of the Penn Treebank symbol set. To evaluate, we decoded 300 sentences of up to 40 words in length from the NIST05 Arabic-English test set. 3 Efficient Grammar Encodings Monolingual parsing with a source-projected transducer grammar is a natural first pass in multi-pass decoding. These grammars are qualitatively different from syntactic analysis grammars, such as the lexicalized grammars of Charniak (1997) or the heavily state-split grammars of Petrov et al. (2006). 70,000 52,500 35,000 17,500 0 NNP1 did not slap DT2 green NN3 (a) InSthis ! section, we develop an appropriate grammar encoding that NNP daba unaefficient bofetadaparsing. a DT2 NN3 verde 1 noenables 7+ It is problematic to convert these grammars into which aCKY requires. (b) Chomsky S ! NNPnormal no dabaform, una bofetada DT NN verde Because transducer rules are very flat and contain speS cific lexical items, binarization introduces a large number of intermediate grammar symbols. Rule size affectDT parsing complexity whether (c) and lexicalization NNP NN the grammar is binarized explicitly"
N09-1026,D08-1012,1,0.9397,"be a coarse-to-fine pruning scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time. The resulting translations improve by 1.3 BLEU. 1 Introduction Current approaches to syntactic machine translation typically include two statistical models: a syntactic transfer model and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 227 Given that parsing is well-studied in the monolingual case, it is worth asking why MT grammars are not simply like those used for syntactic analysis. There are several good reasons. The most important is that MT grammars must do both analysis and generation. To generate, it is natural to memorize larger lexical chunks, and so rules are highly lexicalized. Second, syntax diverges between languages, an"
N09-1026,D08-1018,0,0.183564,"o ensure that the sequences X1 . . . Xk and Xk+1 . . . Xn can be constructed. As baselines, we used left-branching (where k = 1 always) and right-branching (where k = n − 1) binarizations. We also tested a greedy binarization approach, choosing k to minimize the number of grammar symbols introduced. We first try to select k such that both X1:k and Xk+1:n are already in the grammar. If no such k exists, we select k such that one of the intermediate types generated is already  used. If no such k exists again, we choose k = 21 n . This policy only creates new intermediate types when necessary. Song et al. (2008) propose a similar greedy approach to binarization that uses corpus statistics to select common types rather than explicitly reusing types that have already been introduced. Finally, we computed an optimal binarization that explicitly minimizes the number of symbols in the resulting grammar. We cast the minimization as an integer linear program (ILP). Let V be the set of all base non-terminal symbols in the grammar. We introduce an indicator variable TY for each symbol Y ∈ V + to indicate that Y is used in the grammar. Y can be either a base non-terminal symbol Xi or an intermediate symbol X1:"
N09-1026,N07-1063,0,0.0332326,"Missing"
N09-1026,P06-1098,0,0.0486143,"Missing"
N09-1026,J97-3002,0,0.0777932,"me. The resulting translations improve by 1.3 BLEU. 1 Introduction Current approaches to syntactic machine translation typically include two statistical models: a syntactic transfer model and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 227 Given that parsing is well-studied in the monolingual case, it is worth asking why MT grammars are not simply like those used for syntactic analysis. There are several good reasons. The most important is that MT grammars must do both analysis and generation. To generate, it is natural to memorize larger lexical chunks, and so rules are highly lexicalized. Second, syntax diverges between languages, and each divergence expands the minimal domain of translation rules, so rules are large and flat. Finally, we see most rules very few times,"
N09-1026,P08-1025,0,0.122277,"runing scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time. The resulting translations improve by 1.3 BLEU. 1 Introduction Current approaches to syntactic machine translation typically include two statistical models: a syntactic transfer model and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 227 Given that parsing is well-studied in the monolingual case, it is worth asking why MT grammars are not simply like those used for syntactic analysis. There are several good reasons. The most important is that MT grammars must do both analysis and generation. To generate, it is natural to memorize larger lexical chunks, and so rules are highly lexicalized. Second, syntax diverges between languages, and each divergence expands"
N09-1026,N06-1033,0,0.278249,"Missing"
N09-1026,W06-3119,0,0.0979856,"Missing"
N09-1026,J99-4004,0,\N,Missing
N09-1063,W96-0212,0,0.137971,"ained in the standard way. The agenda is initialized with I(Ti , i, i + 1) for i = 0 . . . n − 1. The algorithm terminates when I(R, 0, n) is popped off the queue. Priorities are in general different than weights. Whenever an edge e’s score changes, its priority p(e), which may or may not depend on its score, may improve. Edges are promoted accordingly in the agenda if their priorities improve. In the simplest case, the priorities are simply the βe estimates, which gives a correct uniform cost search wherein the root edge is guaranteed to have its correct inside score estimate at termination (Caraballo and Charniak, 1996). A∗ parsing (Klein and Manning, 2003b) is a special case of such an agenda-driven parser in which the priority function p takes the form p(e) = βe + h(e), where e = I(X, i, j) and h(·) is some approximation of e’s Viterbi outside cost (its completion cost). If h is consistent, then the A∗ algorithm guarantees that whenever an edge comes off the agenda, its weight is its true Viterbi inside cost. In particular, this guarantee implies that the first edge representing the root I(R, 0, n) will be scored with the true Viterbi score for the sentence. 2.2 Hierarchical A∗ In the standard A∗ case the"
N09-1063,W98-1115,0,0.0817525,"iority function maintains the set of states explored in CKY-based CTF, but does not necessarily explore those states in the same order. 3 3.1 Experiments Evaluation Our focus is parsing speed. Thus, we would ideally evaluate our algorithms in terms of CPU time. However, this measure is problematic: CPU time is influenced by a variety of factors, including the architecture of the hardware, low-level implementation details, and other running processes, all of which are hard to normalize. It is common to evaluate best-first parsers in terms of edges popped off the agenda. This measure is used by Charniak et al. (1998) and Klein and Manning (2003b). However, when edges from grammars of varying size are processed on the same agenda, the number of successor edges per edge popped changes depending on what grammar the edge was constructed from. In particular, edges in more refined grammars are more expensive than edges in coarser grammars. Thus, our basic unit of measurement will be edges pushed onto the agenda. We found in our experiments that this was well correlated with CPU time. 400 0 100 200 300 Edges pushed (billions) 424 86.6 78.2 58.8 60.1 8.83 7.12 1.98 UCS A* 3 HA* 3 HA* 3-5 HA* 0-5 CTF 3 CTF 3-5 CTF"
N09-1063,N06-1022,0,0.182444,"-heuristics. Multi-level hierarchies are helpful in both approaches, but are more effective in the coarseto-fine case because of accumulated slack in A∗ heuristics. 1 Introduction The grammars used by modern parsers are extremely large, rendering exhaustive parsing impractical. For example, the lexicalized grammars of Collins (1997) and Charniak (1997) and the statesplit grammars of Petrov et al. (2006) are all too large to construct unpruned charts in memory. One effective approach is coarse-to-fine pruning, in which a small, coarse grammar is used to prune edges in a large, refined grammar (Charniak et al., 2006). Indeed, coarse-to-fine is even more effective when a hierarchy of successive approximations is used (Charniak et al., 2006; Petrov and Klein, 2007). In particular, Petrov and Klein (2007) generate a sequence of approximations to a highly subcategorized grammar, parsing with each in turn. Despite its practical success, coarse-to-fine pruning is approximate, with no theoretical guarantees 557 on optimality. Another line of work has explored A∗ search methods, in which simpler problems are used not for pruning, but for prioritizing work in the full search space (Klein and Manning, 2003a; Haghig"
N09-1063,P97-1003,0,0.021993,"rengths of each method. Overall, coarse-to-fine is much faster for moderate levels of search errors, but below a certain threshold A∗ is superior. In addition, we present the first experiments on hierarchical A∗ parsing, in which computation of heuristics is itself guided by meta-heuristics. Multi-level hierarchies are helpful in both approaches, but are more effective in the coarseto-fine case because of accumulated slack in A∗ heuristics. 1 Introduction The grammars used by modern parsers are extremely large, rendering exhaustive parsing impractical. For example, the lexicalized grammars of Collins (1997) and Charniak (1997) and the statesplit grammars of Petrov et al. (2006) are all too large to construct unpruned charts in memory. One effective approach is coarse-to-fine pruning, in which a small, coarse grammar is used to prune edges in a large, refined grammar (Charniak et al., 2006). Indeed, coarse-to-fine is even more effective when a hierarchy of successive approximations is used (Charniak et al., 2006; Petrov and Klein, 2007). In particular, Petrov and Klein (2007) generate a sequence of approximations to a highly subcategorized grammar, parsing with each in turn. Despite its practical"
N09-1063,N07-1052,1,0.785905,"2006). Indeed, coarse-to-fine is even more effective when a hierarchy of successive approximations is used (Charniak et al., 2006; Petrov and Klein, 2007). In particular, Petrov and Klein (2007) generate a sequence of approximations to a highly subcategorized grammar, parsing with each in turn. Despite its practical success, coarse-to-fine pruning is approximate, with no theoretical guarantees 557 on optimality. Another line of work has explored A∗ search methods, in which simpler problems are used not for pruning, but for prioritizing work in the full search space (Klein and Manning, 2003a; Haghighi et al., 2007). In particular, Klein and Manning (2003a) investigated A∗ for lexicalized parsing in a factored model. In that case, A∗ vastly improved the search in the lexicalized grammar, with provable optimality. However, their bottleneck was clearly shown to be the exhaustive parsing used to compute the A∗ heuristic itself. It is not obvious, however, how A∗ can be stacked in a hierarchical or multi-pass way to speed up the computation of such complex heuristics. In this paper, we address three open questions regarding efficient hierarchical search. First, can a hierarchy of A∗ bounds be used, analogous"
N09-1063,N03-1016,1,0.366239,"grammar (Charniak et al., 2006). Indeed, coarse-to-fine is even more effective when a hierarchy of successive approximations is used (Charniak et al., 2006; Petrov and Klein, 2007). In particular, Petrov and Klein (2007) generate a sequence of approximations to a highly subcategorized grammar, parsing with each in turn. Despite its practical success, coarse-to-fine pruning is approximate, with no theoretical guarantees 557 on optimality. Another line of work has explored A∗ search methods, in which simpler problems are used not for pruning, but for prioritizing work in the full search space (Klein and Manning, 2003a; Haghighi et al., 2007). In particular, Klein and Manning (2003a) investigated A∗ for lexicalized parsing in a factored model. In that case, A∗ vastly improved the search in the lexicalized grammar, with provable optimality. However, their bottleneck was clearly shown to be the exhaustive parsing used to compute the A∗ heuristic itself. It is not obvious, however, how A∗ can be stacked in a hierarchical or multi-pass way to speed up the computation of such complex heuristics. In this paper, we address three open questions regarding efficient hierarchical search. First, can a hierarchy of A∗"
N09-1063,J93-2004,0,0.0351287,"Missing"
N09-1063,J03-1006,0,0.204266,"d, and may or may not be correct at termination, depending on the properties of p. The parser maintains an agenda (a priority queue of edges), as well as a chart (or closed list in search terminology) of edges already processed. The fundamental operation of the algorithm is to pop the best (lowest) priority edge e from the agenda, put it into the chart, and enqueue any edges which can be built by combining e with other edges in the chart. The combination of two adjacent edges into a larger edge is shown graphically in Figure 1 and as a weighted deduction rule in Table 1 (Shieber et al., 1995; Nederhof, 2003). When an edge a is built from adjacent edges b and c and a rule r, its current score βa is compared to βb + βc + wr and updated if necessary. To allow reconstruction of best parses, backpointers are maintained in the standard way. The agenda is initialized with I(Ti , i, i + 1) for i = 0 . . . n − 1. The algorithm terminates when I(R, 0, n) is popped off the queue. Priorities are in general different than weights. Whenever an edge e’s score changes, its priority p(e), which may or may not depend on its score, may improve. Edges are promoted accordingly in the agenda if their priorities improv"
N09-1063,N07-1051,1,0.936948,"n A∗ heuristics. 1 Introduction The grammars used by modern parsers are extremely large, rendering exhaustive parsing impractical. For example, the lexicalized grammars of Collins (1997) and Charniak (1997) and the statesplit grammars of Petrov et al. (2006) are all too large to construct unpruned charts in memory. One effective approach is coarse-to-fine pruning, in which a small, coarse grammar is used to prune edges in a large, refined grammar (Charniak et al., 2006). Indeed, coarse-to-fine is even more effective when a hierarchy of successive approximations is used (Charniak et al., 2006; Petrov and Klein, 2007). In particular, Petrov and Klein (2007) generate a sequence of approximations to a highly subcategorized grammar, parsing with each in turn. Despite its practical success, coarse-to-fine pruning is approximate, with no theoretical guarantees 557 on optimality. Another line of work has explored A∗ search methods, in which simpler problems are used not for pruning, but for prioritizing work in the full search space (Klein and Manning, 2003a; Haghighi et al., 2007). In particular, Klein and Manning (2003a) investigated A∗ for lexicalized parsing in a factored model. In that case, A∗ vastly impro"
N09-1063,P03-1054,1,\N,Missing
N09-1063,P06-1055,1,\N,Missing
N09-1069,W02-1001,0,0.0509712,"y small amount of noise to initialize the parameters, the variance due to initialization is systematically smaller than the variance due to permutation. sEM` is less sensitive to initialization than EM, but additional variance is created by randomly permuting the examples. Overall, the accuracy of sEM` is more variable than that of EM, but not by a large amount. 5 Discussion and related work As datasets increase in size, the demand for online algorithms has grown in recent years. One sees this clear trend in the supervised NLP literature— examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al., 2005), exponentiated gradient algorithms (Collins et al., 2008), stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few. Empirically, online methods are of618 ten faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, onlin"
N09-1069,P08-1109,0,0.0237836,"ve to initialization than EM, but additional variance is created by randomly permuting the examples. Overall, the accuracy of sEM` is more variable than that of EM, but not by a large amount. 5 Discussion and related work As datasets increase in size, the demand for online algorithms has grown in recent years. One sees this clear trend in the supervised NLP literature— examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al., 2005), exponentiated gradient algorithms (Collins et al., 2008), stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few. Empirically, online methods are of618 ten faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Kuo et al., 2008). Indeed, as we have shown, applying online EM do"
N09-1069,P07-1094,0,0.0085397,"bro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Kuo et al., 2008). Indeed, as we have shown, applying online EM does require some care, and some variants (including incremental EM) can fail catastrophically in face of local optima. Stepwise EM provides finer control via its optimization parameters and has proven quite successful. One family of methods that resembles incremental EM includes collapsed samplers for Bayesian models—for example, Goldwater et al. (2006) and Goldwater and Griffiths (2007). These samplers keep track of a sample of the latent variables for each example, akin to the sufficient statistics that we store in incremental EM. In contrast, stepwise EM does not require this storage and operates more in the spirit of a truly online algorithm. Besides speed, online algorithms are of interest for two additional reasons. First, in some applications, we receive examples sequentially and would like to estimate a model in real-time, e.g., in the clustering of news articles. Second, since humans learn sequentially, studying online EM might suggest new connections to cognitive me"
N09-1069,P06-1085,0,0.0125539,"2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Kuo et al., 2008). Indeed, as we have shown, applying online EM does require some care, and some variants (including incremental EM) can fail catastrophically in face of local optima. Stepwise EM provides finer control via its optimization parameters and has proven quite successful. One family of methods that resembles incremental EM includes collapsed samplers for Bayesian models—for example, Goldwater et al. (2006) and Goldwater and Griffiths (2007). These samplers keep track of a sample of the latent variables for each example, akin to the sufficient statistics that we store in incremental EM. In contrast, stepwise EM does not require this storage and operates more in the spirit of a truly online algorithm. Besides speed, online algorithms are of interest for two additional reasons. First, in some applications, we receive examples sequentially and would like to estimate a model in real-time, e.g., in the clustering of news articles. Second, since humans learn sequentially, studying online EM might sugg"
N09-1069,D07-1031,0,0.0115303,"-of-speech tagging, document classification, word segmentation, and word alignment. 1 Introduction In unsupervised NLP tasks such as tagging, parsing, and alignment, one wishes to induce latent linguistic structures from raw text. Probabilistic modeling has emerged as a dominant paradigm for these problems, and the EM algorithm has been a driving force for learning models in a simple and intuitive manner. However, on some tasks, EM can converge slowly. For instance, on unsupervised part-ofspeech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007). The slowness of EM is mainly due to its batch nature: Parameters are updated only once after each pass through the data. When parameter estimates are still rough or if there is high redundancy in the data, computing statistics on the entire dataset just to make one update can be wasteful. In this paper, we investigate two flavors of online EM—incremental EM (Neal and Hinton, 1998) and stepwise EM (Sato and Ishii, 2000; Capp´e and Moulines, 2009), both of which involve updating parameters after each example or after a mini-batch 611 (subset) of examples. Online algorithms have the potential t"
N09-1069,P08-1046,0,0.0325478,"Missing"
N09-1069,I08-4003,0,0.0150271,"stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few. Empirically, online methods are of618 ten faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Kuo et al., 2008). Indeed, as we have shown, applying online EM does require some care, and some variants (including incremental EM) can fail catastrophically in face of local optima. Stepwise EM provides finer control via its optimization parameters and has proven quite successful. One family of methods that resembles incremental EM includes collapsed samplers for Bayesian models—for example, Goldwater et al. (2006) and Goldwater and Griffiths (2007). These samplers keep track of a sample of the latent variables for each example, akin to the sufficient statistics that we store in incremental EM. In contrast,"
N09-1069,P08-1100,1,0.875983,"aking updates more frequently. However, these updates can be seen as noisy approximations to the full batch update, and this noise can in fact impede learning. This tradeoff between speed and stability is familiar to online algorithms for convex supervised learning problems—e.g., Perceptron, MIRA, stochastic gradient, etc. Unsupervised learning raises two additional issues: (1) Since the EM objective is nonconvex, we often get convergence to different local optima of varying quality; and (2) we evaluate on accuracy metrics which are at best loosely correlated with the EM likelihood objective (Liang and Klein, 2008). We will see that these issues can lead to surprising results. In Section 4, we present a thorough investigation of online EM, mostly focusing on stepwise EM since it dominates incremental EM. For stepwise EM, we find that choosing a good stepsize and mini-batch size is important but can fortunately be done adequately without supervision. With a proper choice, stepwise EM reaches the same performance as batch EM, but much more quickly. Moreover, it can even surpass the performance of batch EM. Our results are particularly striking on part-of-speech tagging: Batch EM crawls to an accuracy of 5"
N09-1069,P05-1012,0,0.0313674,"ameters, the variance due to initialization is systematically smaller than the variance due to permutation. sEM` is less sensitive to initialization than EM, but additional variance is created by randomly permuting the examples. Overall, the accuracy of sEM` is more variable than that of EM, but not by a large amount. 5 Discussion and related work As datasets increase in size, the demand for online algorithms has grown in recent years. One sees this clear trend in the supervised NLP literature— examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al., 2005), exponentiated gradient algorithms (Collins et al., 2008), stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few. Empirically, online methods are of618 ten faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, increm"
N09-1069,J03-1002,0,0.0153304,"Missing"
N09-1069,P07-1049,0,0.0100166,"Missing"
N09-1069,J01-3002,0,0.118831,"Missing"
N10-1014,J93-2003,0,0.0703269,"the sequence fl . The parameter pl is a left length distribution. The probabilities pmid , pright , decompose in the same way, except substituting a separate length distribution pm and pr for pl . For the T ERMINAL rule, we emit ft with a similarly decomposed distribution pterm using length distribution pw . We define the probability of generating a foreign word fj as p(fj |A, eA ) = X i∈eA 1 pt (fj |ei ) |eA | with i ∈ eA denoting an index ranging over the indices of the English words contained in eA . The reader may recognize the above expressions as the probability assigned by IBM Model 1 (Brown et al., 1993) of generating the words fl given the words eA , with one important difference – the length m of the foreign sentence is often not modeled, so the term pl (|fl |= m |A) is set to a constant and ignored. Parameterizing this length allows our model to effectively control the number of words produced at different levels of the derivation. It is worth noting how each parameter affects the model’s behavior. The pt distribution is a standard “translation” table, familiar from the IBM Models. The pinv distribution is a “distortion” parameter, and models the likelihood of inverting non-terminals B and"
N10-1014,N10-1015,1,0.831836,"sis is “correct”, but “good enough” without resorting to more computationally complicated models. In general, our model follows an “extract as much as possible” approach. We hypothesize that this approach will capture important syntactic generalizations, but it also risks including low-quality rules. It is an empirical question whether this approach is effective, and we investigate this issue further in Section 5.3. There are possibilities for improving our model’s treatment of syntactic divergence. One option is to allow the model to select trees which are more consistent with the alignment (Burkett et al., 2010), which our model can do since it permits efficient inference over forests. The second is to modify the generative process slightly, perhaps by including the “clone” operator of Gildea (2003). 123 Parameter Estimation The parameters of our model can be efficiently estimated in an unsupervised fashion using the Expectation-Maximization (EM) algorithm. The Estep requires the computation of expected counts under our model for each multinomial parameter. We omit the details of obtaining expected counts for each distribution, since they can be obtained using simple arithmetic from a single quantity"
N10-1014,P06-2014,0,0.621519,"e and target languages through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a s"
N10-1014,W07-0403,0,0.0769298,"ngle n-ary English tree obtained from a monolingual parser. However, it is worth noting that the English side of the ITG derivation is not completely fixed. Where our English trees are more than binary branching, we permit any binarization in our dynamic program. For efficiency, we also ruled out span alignments that are extremely lopsided, for example, a 1-word English span aligned to a 20-word foreign span. Specifically, we pruned any span alignment in which one side is more than 5 times larger than the other. Finally, we employ pruning based on highprecision alignments from simpler models (Cherry and Lin, 2007; Haghighi et al., 2009). We compute word-to-word alignments by finding all word pairs which have a posterior of at least 0.7 according to both forward and reverse IBM Model 1 parameters, and prune any span pairs which invalidate more than 3 of these alignments. In total, this pruning reSpan Syntactic Alignment GIZA++ Rule Syntactic Alignment GIZA++ P 50.9 56.1 P 39.6 46.2 R 83.0 67.3 R 40.3 34.7 F1 63.1 61.2 F1 39.9 39.6 Table 3: Alignment quality results for our syntactic aligner and our GIZA++ baseline. duced computation from approximately 1.5 seconds per sentence to about 0.3 seconds per s"
N10-1014,P05-1033,1,0.713885,"e being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline. 1 Introduction Syntactic machine translation has advanced significantly in recent years, and multiple variants currently achieve state-of-the-art translation quality. Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al., 2006), the source side (Huang et al., 2006), or both (Liu et al., 2009). Still others induce their syntax from the data (Chiang, 2005). Despite differences in detail, the vast majority of syntactic methods share a critical dependence on word alignments. In particular, they infer syntactic correspondences between the source and target languages through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponent"
N10-1014,D09-1037,0,0.0675543,"s permits the extraction of the general rule (PP → IN1 NP2 ; 在 NP2 IN1 ), and the more lexicalized (PP → before NP ; 在 NP 之前). 3.1 Parameterization In principle, our model could have one parameter for each instantiation r of a rule type. This model would have an unmanageable number of parameters, producing both computational and modeling issues – it is well known that unsupervised models with large numbers of parameters are prone to degenerate analyses of the data (DeNero et al., 2006). One solution might be to apply an informed prior with a computationally tractable inference procedure (e.g. Cohn and Blunsom (2009) or Liu and Gildea (2009)). We opt here for the simpler, statistically more robust solution of making independence assumptions to keep the number of parameters at a reasonable level. Concretely, we define the probability of the B I NARY M ONO rule,4 p(r = A → B C; fl B fm C fr |A, eA ) which conditions on the root of the rule A and the English yield eA , as pg (A → B C |A, eA ) · pinv (I |B, C)· plef t (fl |A, eA )·pmid (fm |A, eA )·pright (fr |A, eA ) In words, we assume that the rule probability decomposes into a monolingual PCFG grammar probability pg , an inversion probability pinv , and a"
N10-1014,P07-1003,1,0.954134,"through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model"
N10-1014,W06-3105,1,0.843418,"ctly aligns to before, while 在 functions as a generic preposition, which our model handles by attaching it to the PP. This analysis permits the extraction of the general rule (PP → IN1 NP2 ; 在 NP2 IN1 ), and the more lexicalized (PP → before NP ; 在 NP 之前). 3.1 Parameterization In principle, our model could have one parameter for each instantiation r of a rule type. This model would have an unmanageable number of parameters, producing both computational and modeling issues – it is well known that unsupervised models with large numbers of parameters are prone to degenerate analyses of the data (DeNero et al., 2006). One solution might be to apply an informed prior with a computationally tractable inference procedure (e.g. Cohn and Blunsom (2009) or Liu and Gildea (2009)). We opt here for the simpler, statistically more robust solution of making independence assumptions to keep the number of parameters at a reasonable level. Concretely, we define the probability of the B I NARY M ONO rule,4 p(r = A → B C; fl B fm C fr |A, eA ) which conditions on the root of the rule A and the English yield eA , as pg (A → B C |A, eA ) · pinv (I |B, C)· plef t (fl |A, eA )·pmid (fm |A, eA )·pright (fr |A, eA ) In words,"
N10-1014,N09-1026,1,0.844954,"ic system of Galley et al. (2006). For the translation rules extracted from our data, we computed standard features based on relative frequency counts, and tuned their weights using MERT (Och, 2003). We also included a language model feature, using a 5-gram language model trained on 220 million words of English text using the SRILM Toolkit (Stolcke, 2002). For tuning and test data, we used a subset of the NIST MT04 and MT05 with sentences of length at most 40. We used the first 1000 sentences of this set for tuning and the remaining 642 sentences as test data. We used the decoder described in DeNero et al. (2009) during both tuning and testing. We provide final tune and test set results in Table 4. Our alignments produce a 1.0 BLEU improvement over the baseline. Our reported syntactic results were obtained when rules were thresholded by count; we discuss this in the next section. 5.3 Analysis As discussed in Section 3.4, our aligner is designed to extract many rules, which risks inadvertently extracting low-quality rules. To quantify this, we 8 http://code.google.com/p/berkeleyparser/ 9 first examined the number of rules extracted by our 5 iterations of model 1, 5 iterations of HMM, 3 iterations align"
N10-1014,W08-0306,1,0.944423,"nts from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model is best suited to systems which use source- or target-side trees onl"
N10-1014,N04-1035,1,0.906585,"RB VBN drastically fallen !易 trade ""差 surplus 大幅度 drastically 少 fall 了 (past) Figure 1: A single incorrect alignment removes an extractable node, and hence several desirable rules. We represent correct extractable nodes in bold, spurious extractable nodes with a *, and incorrectly blocked extractable nodes in bold strikethrough. chief purpose is to align nodes in the syntactic parse in one language to spans in the other – an alignment we will refer to as a “syntactic” alignment. These alignments are employed by standard syntactic rule extraction algorithms, for example, the GHKM algorithm of Galley et al. (2004). Following that work, we will assume parses are present in the target language, though our model applies in either direction. Currently, although syntactic systems make use of syntactic alignments, these alignments must be induced indirectly from word-level alignments. Previous work has discussed at length the poor interaction of word-alignments with syntactic rule extraction (DeNero and Klein, 2007; Fossum et al., 2008). For completeness, we provide a brief example of this interaction, but for a more detailed discussion we refer the reader to these presentations. 2.1 Interaction with Word Al"
N10-1014,P06-1121,1,0.926158,"source sentence to nodes in a target parse tree. We show that our model produces syntactically consistent analyses where possible, while being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline. 1 Introduction Syntactic machine translation has advanced significantly in recent years, and multiple variants currently achieve state-of-the-art translation quality. Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al., 2006), the source side (Huang et al., 2006), or both (Liu et al., 2009). Still others induce their syntax from the data (Chiang, 2005). Despite differences in detail, the vast majority of syntactic methods share a critical dependence on word alignments. In particular, they infer syntactic correspondences between the source and target languages through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spur"
N10-1014,P03-1011,0,0.487993,"use of in the next section. Although several binarizations are possible, we give one such binarization and its associated probabilities in Table 2. 3.4 Robustness to Syntactic Divergence Generally speaking, ITG grammars have proven more useful without the monolingual syntactic constraints imposed by a target parse tree. When derivations are restricted to respect a target-side parse tree, many desirable alignments are ruled out when the syntax of the two languages diverges, and alignment quality drops precipitously (Zhang and Gildea, 2004), though attempts have been made to address this issue (Gildea, 2003). Our model is designed to degrade gracefully in the case of syntactic divergence. Because it can produce foreign words at any level of the derivation, our model can effectively back off to a variant of Model 1 in the case where an ITG derivation that both respects the target parse tree and the desired word-level alignments cannot be found. For example, consider the sentence pair fragment in Figure 3. It is not possible to produce an ITG derivation of this fragment that both respects the English tree and also aligns all foreign words to their obvious English counterparts. Our model handles thi"
N10-1014,P96-1024,0,0.0818471,"er sentence to about 0.3 seconds per sentence, a speed-up of a factor of 5. 4.3 Decoding Given a trained model, we extract a tree-to-string alignment as follows: we compute, for each node in the English tree, the posterior probability of a particular foreign span assignment using the same dynamic program needed for EM. We then compute the set of span assignments which maximizes the sum of these posteriors, constrained such that the foreign span assignments nest in the obvious way. This algorithm is a natural synchronous generalization of the monolingual Maximum Constituents Parse algorithm of Goodman (1996). 5 5.1 Experiments Alignment Quality We first evaluated our alignments against gold standard annotations. Our training data consisted of the 2261 manually aligned and translated sentences of the Chinese Treebank (Bies et al., 2007) and approximately half a million unlabeled sentences of parallel Chinese-English newswire. The unlabeled data was subsampled (Li et al., 2009) from a larger corpus by selecting sentences which have good tune and test set coverage, and limited to sentences of length at most 40. We parsed the English side of the training data with the Berkeley parser.8 For our baseli"
N10-1014,P09-1104,1,0.872906,"e obtained from a monolingual parser. However, it is worth noting that the English side of the ITG derivation is not completely fixed. Where our English trees are more than binary branching, we permit any binarization in our dynamic program. For efficiency, we also ruled out span alignments that are extremely lopsided, for example, a 1-word English span aligned to a 20-word foreign span. Specifically, we pruned any span alignment in which one side is more than 5 times larger than the other. Finally, we employ pruning based on highprecision alignments from simpler models (Cherry and Lin, 2007; Haghighi et al., 2009). We compute word-to-word alignments by finding all word pairs which have a posterior of at least 0.7 according to both forward and reverse IBM Model 1 parameters, and prune any span pairs which invalidate more than 3 of these alignments. In total, this pruning reSpan Syntactic Alignment GIZA++ Rule Syntactic Alignment GIZA++ P 50.9 56.1 P 39.6 46.2 R 83.0 67.3 R 40.3 34.7 F1 63.1 61.2 F1 39.9 39.6 Table 3: Alignment quality results for our syntactic aligner and our GIZA++ baseline. duced computation from approximately 1.5 seconds per sentence to about 0.3 seconds per sentence, a speed-up of a"
N10-1014,W06-3601,1,0.930085,"arse tree. We show that our model produces syntactically consistent analyses where possible, while being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline. 1 Introduction Syntactic machine translation has advanced significantly in recent years, and multiple variants currently achieve state-of-the-art translation quality. Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al., 2006), the source side (Huang et al., 2006), or both (Liu et al., 2009). Still others induce their syntax from the data (Chiang, 2005). Despite differences in detail, the vast majority of syntactic methods share a critical dependence on word alignments. In particular, they infer syntactic correspondences between the source and target languages through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a l"
N10-1014,W09-0424,0,0.0225108,"ximizes the sum of these posteriors, constrained such that the foreign span assignments nest in the obvious way. This algorithm is a natural synchronous generalization of the monolingual Maximum Constituents Parse algorithm of Goodman (1996). 5 5.1 Experiments Alignment Quality We first evaluated our alignments against gold standard annotations. Our training data consisted of the 2261 manually aligned and translated sentences of the Chinese Treebank (Bies et al., 2007) and approximately half a million unlabeled sentences of parallel Chinese-English newswire. The unlabeled data was subsampled (Li et al., 2009) from a larger corpus by selecting sentences which have good tune and test set coverage, and limited to sentences of length at most 40. We parsed the English side of the training data with the Berkeley parser.8 For our baseline alignments, we used GIZA++, trained in the standard way.9 We used the grow-diag-final alignment heuristic, as we found it outperformed union in early experiments. We trained our unsupervised syntactic aligner on the concatenation of the labelled and unlabelled data. As is standard in unsupervised alignment models, we initialized the translation parameters pt by first tr"
N10-1014,N06-1014,1,0.807301,"t set coverage, and limited to sentences of length at most 40. We parsed the English side of the training data with the Berkeley parser.8 For our baseline alignments, we used GIZA++, trained in the standard way.9 We used the grow-diag-final alignment heuristic, as we found it outperformed union in early experiments. We trained our unsupervised syntactic aligner on the concatenation of the labelled and unlabelled data. As is standard in unsupervised alignment models, we initialized the translation parameters pt by first training 5 iterations of IBM Model 1 using the joint training algorithm of Liang et al. (2006), and then trained our model for 5 EM iterations. We extracted syntactic rules using a re-implementation of the Galley et al. (2006) algorithm from both our syntactic alignments and the GIZA++ alignments. We handle null-aligned words by extracting every consistent derivation, and extracted composed rules consisting of at most 3 minimal rules. We evaluate our alignments against the gold standard in two ways. We calculated Span F-score, which compares the set of extractable nodes paired with a foreign span, and Rule F-score (Fossum et al., 2008) over minimal rules. The results are shown in Table"
N10-1014,D09-1136,0,0.010795,"the general rule (PP → IN1 NP2 ; 在 NP2 IN1 ), and the more lexicalized (PP → before NP ; 在 NP 之前). 3.1 Parameterization In principle, our model could have one parameter for each instantiation r of a rule type. This model would have an unmanageable number of parameters, producing both computational and modeling issues – it is well known that unsupervised models with large numbers of parameters are prone to degenerate analyses of the data (DeNero et al., 2006). One solution might be to apply an informed prior with a computationally tractable inference procedure (e.g. Cohn and Blunsom (2009) or Liu and Gildea (2009)). We opt here for the simpler, statistically more robust solution of making independence assumptions to keep the number of parameters at a reasonable level. Concretely, we define the probability of the B I NARY M ONO rule,4 p(r = A → B C; fl B fm C fr |A, eA ) which conditions on the root of the rule A and the English yield eA , as pg (A → B C |A, eA ) · pinv (I |B, C)· plef t (fl |A, eA )·pmid (fm |A, eA )·pright (fr |A, eA ) In words, we assume that the rule probability decomposes into a monolingual PCFG grammar probability pg , an inversion probability pinv , and a probability of left, mid"
N10-1014,P06-1077,0,0.0477303,"r model can be efficiently trained on large parallel corpora. When compared to standard word-alignmentbacked baselines, our model produces more consistent analyses of parallel sentences, leading to high-count, high-quality transfer rules. End-toend translation experiments demonstrate that these higher quality rules improve translation quality by 1.0 BLEU over a word-alignment-backed baseline. 2 Syntactic Rule Extraction Our model is intended for use in syntactic translation models which make use of syntactic parses on either the target (Galley et al., 2006) or source side (Huang et al., 2006; Liu et al., 2006). Our model’s 118 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 118–126, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics S NP VP DT* NN NN VBZ the trade surplus has ADVP RB VBN drastically fallen !易 trade ""差 surplus 大幅度 drastically 少 fall 了 (past) Figure 1: A single incorrect alignment removes an extractable node, and hence several desirable rules. We represent correct extractable nodes in bold, spurious extractable nodes with a *, and incorrectly blocked extractable nodes in bold strikethrough. chie"
N10-1014,P09-1063,0,0.0941769,"Missing"
N10-1014,J93-2004,0,0.0347448,"rminal symbol A. We modify p(fj |eA ) from the previous section to allow production of fj directly from the non-terminal7 A: p(fj |eA ) = pnt · p(fj |A) X 1 + (1 − pnt ) · pt (fj |ei ) |eA | i∈e A where pnt is a global binomial parameter which controls how often such alignments are made. This necessitates the inclusion of parameters like pt ( 的 |NP) into our translation table. Generally, these parameters do not contain much information, but rather function like a traditional N ULL rooted at some position in the tree. However, in some cases, the particular annotation used by the Penn Treebank (Marcus et al., 1993) (and hence most parsers) allows for some interesting parameters to be learned. For example, we found that our aligner often matched the Chinese word 了, which marks the past tense (among other things), to the preterminals VBD and VBN, which denote the English simple past and perfect tense. Additionally, Chinese measure words like 个 and 名 often align to the CD (numeral) preterminal. These generalizations can be quite useful – where a particular number might predict a measure word quite poorly, the generalization that measure words co-occur with the CD tag is very robust. 7 For terminal symbols"
N10-1014,D07-1038,1,0.932126,"bination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model is best suited to systems which use source- or"
N10-1014,P04-1066,0,0.0389842,"pw parameterizes the number of words produced for a particular English word e, functioning similarly to the “fertilities” employed by IBM Models 3 and 4 (Brown et al., 1993). This allows us to model, for example, the tendency of English determiners the and a translate to nothing in the Chinese, and of English names to align to multiple Chinese words. In general, we expect an English word to usually align to one Chinese word, and so we place a weak Dirichlet prior on on the pe distribution which puts extra mass on 1-length word sequences. This is helpful for avoiding the “garbage collection” (Moore, 2004) problem for rare words. 3.2 Exploiting Non-Terminal Labels There are often foreign words that do not correspond well to any English word, which our model must also handle. We elected for a simple augmentation to our model to account for these words. When generating foreign word sequences f at a non-terminal (i.e. via the U NARY or B INARY productions), we also allow for the production of foreign words from the non-terminal symbol A. We modify p(fj |eA ) from the previous section to allow production of fj directly from the non-terminal7 A: p(fj |eA ) = pnt · p(fj |A) X 1 + (1 − pnt ) · pt (fj"
N10-1014,J03-1002,0,0.0124084,"a span of the foreign sentence which (1) contains every source word that aligns to a target word in the yield of the node and (2) contains no source words that align outside that yield. Only nodes for which a non-empty span satisfying (1) and (2) exists may form the root or leaf of a translation rule; for that reason, we will refer to these nodes as extractable nodes. Since extractable nodes are inferred based on word alignments, spurious word alignments can rule out otherwise desirable extraction points. For exam119 ple, consider the alignment in Figure 1. This alignment, produced by GIZA++ (Och and Ney, 2003), contains 4 correct alignments (the filled circles), but incorrectly aligns the to the Chinese past tense marker 了 (the hollow circle). This mistaken alignment produces the incorrect rule (DT → the ; 了), and also blocks the extraction of (VBN → fallen ; 减少 了). More high-level syntactic transfer rules are also ruled out, for example, the “the insertion rule” (NP → the NN1 NN2 ; NN1 NN2 ) and the high-level (S → NP1 VP2 ; NP1 VP2 ). 3 A Syntactic Alignment Model The most common approach to avoiding these problems is to inject knowledge about syntactic constraints into a word alignment model (Ch"
N10-1014,P03-1021,0,0.0378323,"core, which compares the set of extractable nodes paired with a foreign span, and Rule F-score (Fossum et al., 2008) over minimal rules. The results are shown in Table 3. By both measures, our syntactic aligner effectively trades recall for precision when compared to our baseline, slightly increasing overall F-score. 5.2 Translation Quality For our translation system, we used a reimplementation of the syntactic system of Galley et al. (2006). For the translation rules extracted from our data, we computed standard features based on relative frequency counts, and tuned their weights using MERT (Och, 2003). We also included a language model feature, using a 5-gram language model trained on 220 million words of English text using the SRILM Toolkit (Stolcke, 2002). For tuning and test data, we used a subset of the NIST MT04 and MT05 with sentences of length at most 40. We used the first 1000 sentences of this set for tuning and the remaining 642 sentences as test data. We used the decoder described in DeNero et al. (2009) during both tuning and testing. We provide final tune and test set results in Table 4. Our alignments produce a 1.0 BLEU improvement over the baseline. Our reported syntactic re"
N10-1014,J97-3002,0,0.936111,"to word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model is best suited to systems which use source- or target-side trees only. The design of our model is such that, for divergent structures, a structurally integrated backoff to flatter word-level (or null) analyses is available. Therefore, our model is empirically robust to the case where syntactic divergence between languages prevents syntactically accurate ITG derivations. We show that, with appropriate pruning, our model can be efficiently trained on large parallel corpora. When compared"
N10-1014,P01-1067,1,0.669279,"e spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model is best suited to systems which use source- or target-side trees only. The design of our model is such that, for divergent structures, a structurally integrated backoff to flatter word-level (or null) analyses is available. Therefore, our model is empiri"
N10-1014,C04-1060,0,0.0186119,"our parameterization permits efficient inference algorithms which we will make use of in the next section. Although several binarizations are possible, we give one such binarization and its associated probabilities in Table 2. 3.4 Robustness to Syntactic Divergence Generally speaking, ITG grammars have proven more useful without the monolingual syntactic constraints imposed by a target parse tree. When derivations are restricted to respect a target-side parse tree, many desirable alignments are ruled out when the syntax of the two languages diverges, and alignment quality drops precipitously (Zhang and Gildea, 2004), though attempts have been made to address this issue (Gildea, 2003). Our model is designed to degrade gracefully in the case of syntactic divergence. Because it can produce foreign words at any level of the derivation, our model can effectively back off to a variant of Model 1 in the case where an ITG derivation that both respects the target parse tree and the desired word-level alignments cannot be found. For example, consider the sentence pair fragment in Figure 3. It is not possible to produce an ITG derivation of this fragment that both respects the English tree and also aligns all forei"
N10-1014,N06-1033,1,0.856635,"ns can be quite useful – where a particular number might predict a measure word quite poorly, the generalization that measure words co-occur with the CD tag is very robust. 7 For terminal symbols E, this production is not possible. 122 3.3 Membership in ITG The generative process which describes our model contains a class of grammars larger than the computationally efficient class of ITG grammars. Fortunately, the parameterization described above not only reduces the number of parameters to a manageable level, but also introduces independence assumptions which permit synchronous binarization (Zhang et al., 2006) of our grammar. Any SCFG that can be synchronously binarized is an ITG, meaning that our parameterization permits efficient inference algorithms which we will make use of in the next section. Although several binarizations are possible, we give one such binarization and its associated probabilities in Table 2. 3.4 Robustness to Syntactic Divergence Generally speaking, ITG grammars have proven more useful without the monolingual syntactic constraints imposed by a target parse tree. When derivations are restricted to respect a target-side parse tree, many desirable alignments are ruled out when"
N10-1015,D08-1092,1,0.842581,"sic HMM alignments. 1 Introduction Current syntactic machine translation (MT) systems build synchronous context free grammars from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006). Extracting such grammars requires that bilingual word alignments and monolingual syntactic parses be compatible. Because of this, much recent work in both word alignment and parsing has focused on changing aligners to make use of syntactic information (DeNero and Klein, 2007; May and Knight, 2007; Fossum et al., 2008) or changing parsers to make use of word alignments (Smith and Smith, 2004; Burkett and Klein, 2008; Snyder et al., 2009). In the first case, however, parsers do not exploit bilingual information. In the second, word alignment is performed with a model that does not exploit syntactic information. This work presents a single, joint model for parsing and word alignment that allows both pieces to influence one another simultaneously. While building a joint model seems intuitive, there is no easy way to characterize how word alignments and syntactic parses should relate to each other in general. In the ideal situation, each pair of sentences in a bilingual corpus could be syntactically parsed u"
N10-1015,P07-1003,1,0.596185,"and joint parsing on the parallel portion of the Chinese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments. 1 Introduction Current syntactic machine translation (MT) systems build synchronous context free grammars from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006). Extracting such grammars requires that bilingual word alignments and monolingual syntactic parses be compatible. Because of this, much recent work in both word alignment and parsing has focused on changing aligners to make use of syntactic information (DeNero and Klein, 2007; May and Knight, 2007; Fossum et al., 2008) or changing parsers to make use of word alignments (Smith and Smith, 2004; Burkett and Klein, 2008; Snyder et al., 2009). In the first case, however, parsers do not exploit bilingual information. In the second, word alignment is performed with a model that does not exploit syntactic information. This work presents a single, joint model for parsing and word alignment that allows both pieces to influence one another simultaneously. While building a joint model seems intuitive, there is no easy way to characterize how word alignments and syntactic pars"
N10-1015,W08-0306,0,0.0655276,"he Chinese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments. 1 Introduction Current syntactic machine translation (MT) systems build synchronous context free grammars from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006). Extracting such grammars requires that bilingual word alignments and monolingual syntactic parses be compatible. Because of this, much recent work in both word alignment and parsing has focused on changing aligners to make use of syntactic information (DeNero and Klein, 2007; May and Knight, 2007; Fossum et al., 2008) or changing parsers to make use of word alignments (Smith and Smith, 2004; Burkett and Klein, 2008; Snyder et al., 2009). In the first case, however, parsers do not exploit bilingual information. In the second, word alignment is performed with a model that does not exploit syntactic information. This work presents a single, joint model for parsing and word alignment that allows both pieces to influence one another simultaneously. While building a joint model seems intuitive, there is no easy way to characterize how word alignments and syntactic parses should relate to each other in general. I"
N10-1015,N04-1035,0,0.0768812,"encouraged but not forced to synchronize with the parses. Our model gives absolute improvements of 3.3 F1 for English parsing, 2.1 F1 for Chinese parsing, and 5.5 F1 for word alignment over each task’s independent baseline, giving the best reported results for both Chinese-English word alignment and joint parsing on the parallel portion of the Chinese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments. 1 Introduction Current syntactic machine translation (MT) systems build synchronous context free grammars from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006). Extracting such grammars requires that bilingual word alignments and monolingual syntactic parses be compatible. Because of this, much recent work in both word alignment and parsing has focused on changing aligners to make use of syntactic information (DeNero and Klein, 2007; May and Knight, 2007; Fossum et al., 2008) or changing parsers to make use of word alignments (Smith and Smith, 2004; Burkett and Klein, 2008; Snyder et al., 2009). In the first case, however, parsers do not exploit bilingual information. In the second, word alignment is performed with a model th"
N10-1015,P09-1104,1,0.233768,"synchronization features couples the parses and alignments, but makes exact inference in the model intractable; we show how to use a variational mean field approximation, both for computing approximate feature expectations during training, and for performing approximate joint inference at test time. We train our joint model on the parallel, gold word-aligned portion of the Chinese treebank. When evaluated on parsing and word alignment, this model significantly improves over independentlytrained baselines: the monolingual parser of Petrov and Klein (2007) and the discriminative word aligner of Haghighi et al. (2009). It also improves over the discriminative, bilingual parsing model of Burkett and Klein (2008), yielding the highest joint parsing F1 numbers on this data set. Finally, our model improves word alignment in the context of translation, leading to a 1.2 BLEU increase over using HMM word alignments. grammar (SCFG) (Shieber and Schabes, 1990). Figure 1(a) gives a simple example of generation from a log-linearly parameterized synchronous grammar, together with its features. With the SCFG restriction, we can sum over the necessary structures using the O(n6 ) bitext inside-outside algorithm, making P"
N10-1015,P08-1067,0,0.00674886,"onolingual parsing model scores. For each unrefined anchored production i Aj → i Bk Cj , we compute the marginal probability P(i Aj ,i Bk ,k Cj |s) under the monolingual parser (these are equivalent to the maxrule scores from Petrov and Klein 2007). We only include productions where this probability is greater than 10−20 . Note that at training time, we are not guaranteed that the gold trees will be included in the pruned forest. Because of this, we replace the gold trees ti , t0i with oracle trees from the pruned forest, which can be found efficiently using a variant of the inside algorithm (Huang, 2008). Pruning Although we can approximate the expectations from (4) in polynomial time using our mean field distribution, in practice we must still prune the ITG forests and monolingual parse forests to allow tractable inference. We prune our ITG forests using the same 132 t,w,t0 X a∈A(w) P(t, a, t0 |s, s0 ) Of course, this is also intractable, so we once again resort to our mean field approximation. This yields the approximate solution: X (t∗ , w∗ , t0∗ ) = argmax Q(t, a, t0 ) t,w,t0 a∈A(w) However, recall that Q incorporates the model’s mutual constraint into the variational parameters, which fa"
N10-1015,W08-0402,0,0.0161082,"provements over the independent models. For parsing, we improve absolute F1 over the monolingual parsers by 2.1 in Chinese, and by 3.3 in English. For word alignment, we improve absolute F1 by 5.5 over the non-syntactic ITG word aligner. In addition, our English parsing results are better than those of the Burkett and Klein (2008) bilingual reranker, the current top-performing English-Chinese bilingual parser, despite ours using a much simpler set of synchronization features. 8.3 Machine Translation We further tested our alignments by using them to train the Joshua machine translation system (Li and Khudanpur, 2008). Table 3 describes the results of our experiments. For all of the systems, we tuned HMM ITG Joint Rules 1.1M 1.5M 1.5M Tune 29.0 29.9 29.6 Test 29.4 30.4† 30.6 Table 3: Tune and test BLEU results for machine translation systems built with different alignment tools. † indicates a statistically significant difference between a system’s test performance and the one above it. 134 on 1000 sentences of the NIST 2004 and 2005 machine translation evaluations, and tested on 400 sentences of the NIST 2006 MT evaluation. Our training set consisted of 250k sentences of newswire distributed with the GALE"
N10-1015,N06-1014,1,0.441667,"ion standard. For example, the gapped pattern from Figure 4 captures the standard that English word the is always aligned to the Chinese head noun in a noun phrase. We featurize these non-terminals with features similar to those of Haghighi et al. (2009), and all of the alignment results we report in Section 8.2 (both joint and ITG) employ these features. 8.2 Parsing and Word Alignment To compute features that depend on external models, we needed to train an unsupervised word aligner and monolingual English and Chinese parsers. The unsupervised word aligner was a pair of jointly trained HMMs (Liang et al., 2006), trained on the FBIS corpus. We used the Berkeley Parser (Petrov and Klein, 2007) for both monolingual parsers, with the Chinese parser trained on the full Chinese treebank, and the English parser trained on a concatenation of the Penn WSJ corpus (Marcus et al., 1993) and the English side of train.6 We compare our parsing results to the monolingual parsing models and to the English-Chinese bilingual reranker of Burkett and Klein (2008), trained on the same dataset. The results are in Table 1. For word alignment, we compare to 6 To avoid overlap in the data used to train the monolingual parser"
N10-1015,J93-2004,0,0.0427758,"he alignment results we report in Section 8.2 (both joint and ITG) employ these features. 8.2 Parsing and Word Alignment To compute features that depend on external models, we needed to train an unsupervised word aligner and monolingual English and Chinese parsers. The unsupervised word aligner was a pair of jointly trained HMMs (Liang et al., 2006), trained on the FBIS corpus. We used the Berkeley Parser (Petrov and Klein, 2007) for both monolingual parsers, with the Chinese parser trained on the full Chinese treebank, and the English parser trained on a concatenation of the Penn WSJ corpus (Marcus et al., 1993) and the English side of train.6 We compare our parsing results to the monolingual parsing models and to the English-Chinese bilingual reranker of Burkett and Klein (2008), trained on the same dataset. The results are in Table 1. For word alignment, we compare to 6 To avoid overlap in the data used to train the monolingual parsers and the joint model, at training time, we used a separate version of the Chinese parser, trained only on articles 400-1151 (omitting articles in train). For English parsing, we deemed it insufficient to entirely omit the Chinese treebank data from the monolingual par"
N10-1015,P05-1010,0,0.0206428,"n here. 4.1 Parsing The monolingual parsing features we use are simply parsing model scores under the parser of Petrov and Klein (2007). While that parser uses heavily refined PCFGs with rule probabilities defined at the refined symbol level, we interact with its posterior distribution via posterior marginal probabilities over unrefined symbols. In particular, to each unrefined anchored production i Aj → i Bk Cj , we associate a single feature whose value is the marginal quantity log P(i Bk Cj |i Aj , s) under the monolingual parser. These scores are the same as the variational rule scores of Matsuzaki et al. (2005).4 4.2 Alignment We begin with the same set of alignment features as Haghighi et al. (2009), which are defined only for terminal bispans. In addition, we include features on nonterminal bispans, including a bias feature, features that measure the difference in size between the source and target spans, features that measure the difference in relative sentence position between the source and target spans, and features that measure the density of word-to-word alignment posteriors under a separate unsupervised word alignment model. 4 Of course the structure of our model permits any of the addition"
N10-1015,D07-1038,0,0.115588,"parallel portion of the Chinese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments. 1 Introduction Current syntactic machine translation (MT) systems build synchronous context free grammars from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006). Extracting such grammars requires that bilingual word alignments and monolingual syntactic parses be compatible. Because of this, much recent work in both word alignment and parsing has focused on changing aligners to make use of syntactic information (DeNero and Klein, 2007; May and Knight, 2007; Fossum et al., 2008) or changing parsers to make use of word alignments (Smith and Smith, 2004; Burkett and Klein, 2008; Snyder et al., 2009). In the first case, however, parsers do not exploit bilingual information. In the second, word alignment is performed with a model that does not exploit syntactic information. This work presents a single, joint model for parsing and word alignment that allows both pieces to influence one another simultaneously. While building a joint model seems intuitive, there is no easy way to characterize how word alignments and syntactic parses should relate to ea"
N10-1015,N07-1051,1,0.518137,"nfluence between the three component grammars. The presence of synchronization features couples the parses and alignments, but makes exact inference in the model intractable; we show how to use a variational mean field approximation, both for computing approximate feature expectations during training, and for performing approximate joint inference at test time. We train our joint model on the parallel, gold word-aligned portion of the Chinese treebank. When evaluated on parsing and word alignment, this model significantly improves over independentlytrained baselines: the monolingual parser of Petrov and Klein (2007) and the discriminative word aligner of Haghighi et al. (2009). It also improves over the discriminative, bilingual parsing model of Burkett and Klein (2008), yielding the highest joint parsing F1 numbers on this data set. Finally, our model improves word alignment in the context of translation, leading to a 1.2 BLEU increase over using HMM word alignments. grammar (SCFG) (Shieber and Schabes, 1990). Figure 1(a) gives a simple example of generation from a log-linearly parameterized synchronous grammar, together with its features. With the SCFG restriction, we can sum over the necessary structu"
N10-1015,W05-0908,0,0.0127833,"valuations, and tested on 400 sentences of the NIST 2006 MT evaluation. Our training set consisted of 250k sentences of newswire distributed with the GALE project, all of which were sub-sampled to have high Ngram overlap with the tune and test sets. All of our sentences were of length at most 40 words. When building the translation grammars, we used Joshua’s default “tight” phrase extraction option. We ran MERT for 4 iterations, optimizing 20 weight vectors per iteration on a 200-best list. Table 3 gives the results. On the test set, we also ran the approximate randomization test suggested by Riezler and Maxwell (2005). We found that our joint parsing and alignment system significantly outperformed the HMM aligner, but the improvement over the ITG aligner was not statistically significant. 9 Conclusion The quality of statistical machine translation models depends crucially on the quality of word alignments and syntactic parses for the bilingual training corpus. Our work presented the first joint model for parsing and alignment, demonstrating that we can improve results on both of these tasks, as well as on downstream machine translation, by allowing parsers and word aligners to simultaneously inform one ano"
N10-1015,C90-3045,0,0.0540428,"allel, gold word-aligned portion of the Chinese treebank. When evaluated on parsing and word alignment, this model significantly improves over independentlytrained baselines: the monolingual parser of Petrov and Klein (2007) and the discriminative word aligner of Haghighi et al. (2009). It also improves over the discriminative, bilingual parsing model of Burkett and Klein (2008), yielding the highest joint parsing F1 numbers on this data set. Finally, our model improves word alignment in the context of translation, leading to a 1.2 BLEU increase over using HMM word alignments. grammar (SCFG) (Shieber and Schabes, 1990). Figure 1(a) gives a simple example of generation from a log-linearly parameterized synchronous grammar, together with its features. With the SCFG restriction, we can sum over the necessary structures using the O(n6 ) bitext inside-outside algorithm, making P(t, a, t0 |s, s0 ) relatively efficient to compute expectations under. Unfortunately, an SCFG requires that all the constituents of each tree, from the root down to the words, are generated perfectly in tandem. The resulting inability to model any level of syntactic divergence prevents accurate modeling of the individual monolingual trees"
N10-1015,W06-3104,0,0.0304171,"Indeed, it is sometimes the case that large pieces of a sentence pair are completely asynchronous and can only be explained monolingually. Our model exploits synchronization where possible to perform more accurately on both word alignment and parsing, but also allows independent models to dictate pieces of parse trees and word alignments when synchronization is impossible. This notion of “weak synchronization” is parameterized and estimated from data to maximize the likelihood of the correct parses and word alignments. Weak synchronization is closely related to the quasi-synchronous models of Smith and Eisner (2006; 2009) and the bilingual parse reranking model of Burkett and Klein (2008), but those models assume that the word alignment of a sentence pair is known and fixed. To simultaneously model both parses and align127 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 127–135, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics ments, our model loosely couples three separate combinatorial structures: monolingual trees in the source and target languages, and a synchronous ITG alignment that links the two languages"
N10-1015,D09-1086,0,0.0220488,"Missing"
N10-1015,W04-3207,0,0.0854769,"m MT evaluation over basic HMM alignments. 1 Introduction Current syntactic machine translation (MT) systems build synchronous context free grammars from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006). Extracting such grammars requires that bilingual word alignments and monolingual syntactic parses be compatible. Because of this, much recent work in both word alignment and parsing has focused on changing aligners to make use of syntactic information (DeNero and Klein, 2007; May and Knight, 2007; Fossum et al., 2008) or changing parsers to make use of word alignments (Smith and Smith, 2004; Burkett and Klein, 2008; Snyder et al., 2009). In the first case, however, parsers do not exploit bilingual information. In the second, word alignment is performed with a model that does not exploit syntactic information. This work presents a single, joint model for parsing and word alignment that allows both pieces to influence one another simultaneously. While building a joint model seems intuitive, there is no easy way to characterize how word alignments and syntactic parses should relate to each other in general. In the ideal situation, each pair of sentences in a bilingual corpus could"
N10-1015,P09-1009,0,0.0179584,"roduction Current syntactic machine translation (MT) systems build synchronous context free grammars from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006). Extracting such grammars requires that bilingual word alignments and monolingual syntactic parses be compatible. Because of this, much recent work in both word alignment and parsing has focused on changing aligners to make use of syntactic information (DeNero and Klein, 2007; May and Knight, 2007; Fossum et al., 2008) or changing parsers to make use of word alignments (Smith and Smith, 2004; Burkett and Klein, 2008; Snyder et al., 2009). In the first case, however, parsers do not exploit bilingual information. In the second, word alignment is performed with a model that does not exploit syntactic information. This work presents a single, joint model for parsing and word alignment that allows both pieces to influence one another simultaneously. While building a joint model seems intuitive, there is no easy way to characterize how word alignments and syntactic parses should relate to each other in general. In the ideal situation, each pair of sentences in a bilingual corpus could be syntactically parsed using a synchronous con"
N10-1015,H05-1010,1,0.5259,"ignments for our test sentence pairs. Ideally, given the sentence pair (s, s0 ), we would find: qψ (t)I(n ∈ t) qψ (t0 )I(n0 ∈ t0 ) (t∗ , w∗ , t0∗ ) = argmax P(t, w, t0 |s, s0 ) qψ (a)I(b ∈ a) t,w,t0 = argmax Since dynamic programs exist for summing over each of the individual factors, these expectations can be computed in polynomial time. 6.1 basic idea as Haghighi et al. (2009), but we employ a technique that allows us to be more aggressive. Where Haghighi et al. (2009) pruned bispans based on how many unsupervised HMM alignments were violated, we first train a maximum-matching word aligner (Taskar et al., 2005) using our supervised data set, which has only half the precision errors of the unsupervised HMM. We then prune every bispan which violates at least three alignments from the maximum-matching aligner. When compared to pruning the bitext forest of our model with Haghighi et al. (2009)’s HMM technique, this new technique allows us to maintain the same level of accuracy while cutting the number of bispans in half. In addition to pruning the bitext forests, we also prune the syntactic parse forests using the monolingual parsing model scores. For each unrefined anchored production i Aj → i Bk Cj ,"
N10-1015,J97-3002,0,0.473696,"l triples (t, a, t0 ) without some assumptions about how the features φ(t, a, t0 , s, s0 ) decompose. One natural solution is to restrict our candidate triples to those given by a synchronous context free 128 We propose a joint model which still gives probabilities on triples (t, a, t0 ). However, instead of using SCFG rules to synchronously enforce the tree constraints on t and t0 , we only require that each of t and t0 be well-formed under separate monolingual CFGs. In order to permit efficient enumeration of all possible alignments a, we also restrict a to the set of unlabeled ITG bitrees (Wu, 1997), though again we do not require that a relate to t or t0 in any particular way. Although this assumption does limit the space of possible word-level alignments, for the domain we consider (Chinese-English word alignment), the reduced space still contains almost all empirically observed alignments (Haghighi et al., 2009).1 For 1 See Section 8.1 for some new terminal productions required to make this true for the parallel Chinese treebank. S S Features NP !( (IP, b0, S), s, s’ ) !( (NP, b1, NP), s, s’ ) !( (VP, b2, VP), s, s’ ) VP NP AP NP b1 b0 φF (IP, s) φF (NP, s) Parsing IP b1 VP b0 b2 φA ("
N10-1015,2006.iwslt-evaluation.20,0,0.0259423,"orced to synchronize with the parses. Our model gives absolute improvements of 3.3 F1 for English parsing, 2.1 F1 for Chinese parsing, and 5.5 F1 for word alignment over each task’s independent baseline, giving the best reported results for both Chinese-English word alignment and joint parsing on the parallel portion of the Chinese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments. 1 Introduction Current syntactic machine translation (MT) systems build synchronous context free grammars from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006). Extracting such grammars requires that bilingual word alignments and monolingual syntactic parses be compatible. Because of this, much recent work in both word alignment and parsing has focused on changing aligners to make use of syntactic information (DeNero and Klein, 2007; May and Knight, 2007; Fossum et al., 2008) or changing parsers to make use of word alignments (Smith and Smith, 2004; Burkett and Klein, 2008; Snyder et al., 2009). In the first case, however, parsers do not exploit bilingual information. In the second, word alignment is performed with a model that does not exploit synt"
N10-1015,W90-0102,0,\N,Missing
N10-1061,D08-1031,0,0.857291,"antic constraints, which are highly contingent on lexical meaning and world knowledge. Perhaps because configurational features are inherently easier to learn from small data sets, past work has often emphasized them over semantic knowledge. Of course, all state-of-the-art coreference systems have needed to capture semantic compatibility to some degree. As an example of nominal headword compatibility, a “president” can be a “leader” but cannot be not an “increase.” Past systems have often computed the compatibility of specific headword pairs, extracted either from lexical resources (Ng, 2007; Bengston and Roth, 2008; Rahman and Ng, 2009), web statistics (Yang et al., 2005), or surface syntactic patterns (Haghighi and Klein, 2009). While the pairwise approach has high precision, it is neither realistic nor scalable to explicitly enumerate all pairs of compatible word pairs. A more compact approach has been to rely on named-entity recognition (NER) systems to give coarse-grained entity types for each mention (Soon et al., 1999; Ng and Cardie, 2002). Unfortunately, current systems use small inventories of types and so provide little constraint. In general, coreference errors in state-of-theart systems are f"
N10-1061,A00-2018,0,0.0297567,"Missing"
N10-1061,W99-0613,0,0.101167,"Missing"
N10-1061,N07-1011,0,0.61825,"ify the mentions themselves and their boundaries automatically. Our system deterministically extracts mention boundaries from parse trees (Section 5.2). We utilized no coreference annotation during training, but did use minimal prototype information to prime the learning of entity types (Section 5.3). 5.1 Datasets For evaluation, we used standard coreference data sets derived from the ACE corpora: 6 Forcing appositive coreference is essential for tying proper and nominal entity type vocabulary. 390 • A04CU: Train/dev/test split of the newswire portion of the ACE 2004 training set7 utilized in Culotta et al. (2007), Bengston and Roth (2008) and Stoyanov et al. (2009). Consists of 90/68/38 documents respectively. • A05ST: Train/test split of the newswire portion of the ACE 2005 training set utilized in Stoyanov et al. (2009). Consists of 57/24 documents respectively. • A05RA: Train/test split of the ACE 2005 training set utilized in Rahman and Ng (2009). Consists of 482/117 documents respectively. For all experiments, we evaluated on the dev and test sets above. To train, we included the text of all documents above, though of course not looking at either their mention boundaries or reference annotations"
N10-1061,de-marneffe-etal-2006-generating,0,0.00475737,"Missing"
N10-1061,N09-1019,0,0.11135,"nd Klein (2006). For each type of interest, we provided a (possiblyempty) prototype list of proper and nominal head words, as well as a list of allowed pronouns. For instance, for the PERSON type we might provide: NAM NOM PRO Bush, Gore, Hussein president, minister, official he, his, she, him, her, you, ... The prototypes were used as follows: Any entity with a prototype on any proper or nominal head word attribute list (Section 3.1) was constrained to have the specified type; i.e. the qk factor (Section 4) places probability one on that single type. Similarly to Haghighi and Klein (2007) and Elsner et al. (2009), we biased these types’ pronoun distributions to the allowed set of pronouns. In general, the choice of entity types to prime with prototypes is a domain-specific question. For experiments here, we utilized the types which are annotated in the ACE coreference data: person (PERS), organization (ORG), geo-political entity (GPE), weapon (WEA), vehicle (VEH), location 391 (LOC), and facility (FAC). Since the person type in ACE conflates individual persons with groups of people (e.g., soldier vs. soldiers), we added the group (GROUP) type and generated a prototype specification. We obtained our pr"
N10-1061,J95-2003,0,0.303386,"ocess is described as: Entity Assignment For each mention position, i = 1, . . . , n, Draw antecedent position Ai ∈ {1, . . . , i}: P (Ai = j|X) ∝ sπ (i, j; X) ( ZAi , if Ai < i Zi = K + 1, otherwise Here, K denotes the number of entities allocated in the first i-1 mention positions. This process is an instance of the sequential distance-dependent Chinese Restaurant Process (DD-CRP) of Blei and Frazier (2009). During inference, we variously exploit both the A and Z representations (Section 4). For nominal and pronoun mentions, there are several well-studied anaphora cues, including centering (Grosz et al., 1995), nearness (Hobbs, 1978), and deterministic constraints, which have all been utilized in prior coreference work (Soon et al., 1999; Ng and Cardie, 2002). In order to combine these cues, we take a log-linear, feature-based approach and parametrize sπ (i, j; X) = exp{π > fX (i, j)}, where fX (i, j) is a feature vector over mention positions i and j, and π is a parameter vector; the features may freely condition on X. We utilize the following features between a mention and an antecedent: tree distance, sentence distance, and the syntactic positions (subject, object, and oblique) of the mention an"
N10-1061,P07-1107,1,0.936714,"stic nor scalable to explicitly enumerate all pairs of compatible word pairs. A more compact approach has been to rely on named-entity recognition (NER) systems to give coarse-grained entity types for each mention (Soon et al., 1999; Ng and Cardie, 2002). Unfortunately, current systems use small inventories of types and so provide little constraint. In general, coreference errors in state-of-theart systems are frequently due to poor models of semantic compatibility (Haghighi and Klein, 2009). In this work, we take a primarily unsupervised approach to coreference resolution, broadly similar to Haghighi and Klein (2007), which addresses this issue. Our generative model exploits a large inventory of distributional entity types, including standard NER types like PERSON and ORG, as well as more refined types like WEAPON and VEHICLE. For each type, distributions over typical heads, modifiers, and governors are learned from large amounts of unlabeled data, capturing type-level semantic information (e.g. “spokesman” is a likely head for a PER SON ). Each entity inherits from a type but captures entity-level semantic information (e.g. “giant” may be a likely head for the Microsoft entity but not all ORGs). Separate"
N10-1061,D09-1120,1,0.513018,"onal features are inherently easier to learn from small data sets, past work has often emphasized them over semantic knowledge. Of course, all state-of-the-art coreference systems have needed to capture semantic compatibility to some degree. As an example of nominal headword compatibility, a “president” can be a “leader” but cannot be not an “increase.” Past systems have often computed the compatibility of specific headword pairs, extracted either from lexical resources (Ng, 2007; Bengston and Roth, 2008; Rahman and Ng, 2009), web statistics (Yang et al., 2005), or surface syntactic patterns (Haghighi and Klein, 2009). While the pairwise approach has high precision, it is neither realistic nor scalable to explicitly enumerate all pairs of compatible word pairs. A more compact approach has been to rely on named-entity recognition (NER) systems to give coarse-grained entity types for each mention (Soon et al., 1999; Ng and Cardie, 2002). Unfortunately, current systems use small inventories of types and so provide little constraint. In general, coreference errors in state-of-theart systems are frequently due to poor models of semantic compatibility (Haghighi and Klein, 2009). In this work, we take a primarily"
N10-1061,P02-1014,0,0.843589,"annot be not an “increase.” Past systems have often computed the compatibility of specific headword pairs, extracted either from lexical resources (Ng, 2007; Bengston and Roth, 2008; Rahman and Ng, 2009), web statistics (Yang et al., 2005), or surface syntactic patterns (Haghighi and Klein, 2009). While the pairwise approach has high precision, it is neither realistic nor scalable to explicitly enumerate all pairs of compatible word pairs. A more compact approach has been to rely on named-entity recognition (NER) systems to give coarse-grained entity types for each mention (Soon et al., 1999; Ng and Cardie, 2002). Unfortunately, current systems use small inventories of types and so provide little constraint. In general, coreference errors in state-of-theart systems are frequently due to poor models of semantic compatibility (Haghighi and Klein, 2009). In this work, we take a primarily unsupervised approach to coreference resolution, broadly similar to Haghighi and Klein (2007), which addresses this issue. Our generative model exploits a large inventory of distributional entity types, including standard NER types like PERSON and ORG, as well as more refined types like WEAPON and VEHICLE. For each type,"
N10-1061,P05-1020,0,0.119865,"Missing"
N10-1061,P06-1055,1,0.0603696,"Missing"
N10-1061,D09-1101,0,0.715996,"are highly contingent on lexical meaning and world knowledge. Perhaps because configurational features are inherently easier to learn from small data sets, past work has often emphasized them over semantic knowledge. Of course, all state-of-the-art coreference systems have needed to capture semantic compatibility to some degree. As an example of nominal headword compatibility, a “president” can be a “leader” but cannot be not an “increase.” Past systems have often computed the compatibility of specific headword pairs, extracted either from lexical resources (Ng, 2007; Bengston and Roth, 2008; Rahman and Ng, 2009), web statistics (Yang et al., 2005), or surface syntactic patterns (Haghighi and Klein, 2009). While the pairwise approach has high precision, it is neither realistic nor scalable to explicitly enumerate all pairs of compatible word pairs. A more compact approach has been to rely on named-entity recognition (NER) systems to give coarse-grained entity types for each mention (Soon et al., 1999; Ng and Cardie, 2002). Unfortunately, current systems use small inventories of types and so provide little constraint. In general, coreference errors in state-of-theart systems are frequently due to poor"
N10-1061,M95-1005,0,0.910659,"Missing"
N10-1061,P05-1021,0,0.00895818,"ng and world knowledge. Perhaps because configurational features are inherently easier to learn from small data sets, past work has often emphasized them over semantic knowledge. Of course, all state-of-the-art coreference systems have needed to capture semantic compatibility to some degree. As an example of nominal headword compatibility, a “president” can be a “leader” but cannot be not an “increase.” Past systems have often computed the compatibility of specific headword pairs, extracted either from lexical resources (Ng, 2007; Bengston and Roth, 2008; Rahman and Ng, 2009), web statistics (Yang et al., 2005), or surface syntactic patterns (Haghighi and Klein, 2009). While the pairwise approach has high precision, it is neither realistic nor scalable to explicitly enumerate all pairs of compatible word pairs. A more compact approach has been to rely on named-entity recognition (NER) systems to give coarse-grained entity types for each mention (Soon et al., 1999; Ng and Cardie, 2002). Unfortunately, current systems use small inventories of types and so provide little constraint. In general, coreference errors in state-of-theart systems are frequently due to poor models of semantic compatibility (Ha"
N10-1061,N06-1041,1,\N,Missing
N10-1061,J03-4003,0,\N,Missing
N10-1061,D08-1067,0,\N,Missing
N10-1061,P09-1074,0,\N,Missing
N10-1061,J01-4004,0,\N,Missing
N10-1082,J92-4003,0,0.11493,"r T OKEN, we consider three other alternative samplers. First, annealing (T OKENanneal ) is a commonly-used technique to improve mixing, where (3) is raised to some inverse temperature.7 In Figure 5(a)–(c), we see that unlike T YPE, T OKENanneal does not improve over T OKEN uniformly: it hurts for the HMM, improves slightly for the USM, and makes no difference for the PTSG. Although annealing does increase mobility of the sampler, this mobility is undirected, whereas type-based sampling increases mobility in purely model-driven directions. Unlike past work that operated on types (Wolff, 1988; Brown et al., 1992; Stolcke and Omohundro, 1994), type-based sampling makes stochastic choices, and moreover, these choices are reversible. Is this stochasticity important? To answer this, we consider a variant of T YPE, T YPEgreedy : instead of sampling from (7), T YPEgreedy considers a type block S and sets bs to 0 for all s ∈ S if p(bS = (0, . . . , 0) |· · · ) > p(bS = (1, . . . , 1) |· · · ); else it sets bs to 1 for all s ∈ S. From Figure 5(a)–(c), we see that greediness is disastrous for the HMM, hurts a little for USM, and makes no difference on the PTSG. These results show that stochasticity can indeed"
N10-1082,N09-1062,0,0.287322,"not deal with the strong type-based coupling (e.g., all instances of a word should be tagged similarly). The type-based sampler we will present is designed exactly to tackle this coupling, which we argue is stronger and more important to deal with in unsupervised learning. Figure 1 depicts the updates made by each of the three samplers. We tested our sampler on three models: a Bayesian HMM for part-of-speech induction (Goldwater and Griffiths, 2007), a nonparametric Bayesian model for word segmentation (Goldwater et al., 2006), and a nonparametric Bayesian model of tree substitution grammars (Cohn et al., 2009; Post and Gildea, 2009). Empirically, we find that typebased sampling improves performance and is less sensitive to initialization (Section 5). The ascending factorial function arises from marginalizing Dirichlet distributions and is responsible the rich-gets-richer phenomenon: the larger n is, more we gain by increasing it.  n possibilithe particular z uniformly out of the m ties. Figure 2(b) shows the effectiveness of this typebased sampler. This simple example exposes the fundamental challenge of multimodality in unsupervised learning. Both m = 0 and m = n are modes due to the rich-gets-r"
N10-1082,N09-1026,1,0.834248,"g step on the trees involving Markovization, binarization, and collapsing of unary chains; words occurring once are replaced with one of 50 “unknown word” tokens, using base distributions {µr } that penalize the size of trees, and sampling the hyperparameters (see Cohn et al. (2009) for details). 6 To evaluate, we created a grammar where the rule probabilities are the mean values under the PTSG distribution: this involves taking a weighted combination (based on the concentration parameters) of the rule counts from the PTSG samples and the PCFG-derived base distribution. We used the decoder of DeNero et al. (2009) to parse. 579 improve the likelihood but actually hurt parsing accuracy, suggesting that the PTSG model is overfitting. To better understand the gains from T YPE over T OKEN, we consider three other alternative samplers. First, annealing (T OKENanneal ) is a commonly-used technique to improve mixing, where (3) is raised to some inverse temperature.7 In Figure 5(a)–(c), we see that unlike T YPE, T OKENanneal does not improve over T OKEN uniformly: it hurts for the HMM, improves slightly for the USM, and makes no difference for the PTSG. Although annealing does increase mobility of the sampler,"
N10-1082,D08-1036,0,0.10051,"ng large blocks is computationally expensive. Past work for clustering models maintained tractability by using Metropolis-Hastings proposals (Dahl, 2003) or introducing auxiliary variables (Swendsen and Wang, 1987; Liang et al., 2007). In contrast, our type-based sampler simply identifies tractable 580 blocks based on exchangeability. Other methods for learning latent-variable models include EM, variational approximations, and uncollapsed samplers. All of these methods maintain distributions over (or settings of) the latent variables of the model and update the representation iteratively (see Gao and Johnson (2008) for an overview in the context of POS induction). However, these methods are at the core all token-based, since they only update variables in a single example at a time.8 Blocking variables by type—the key idea of this paper—is a fundamental departure from tokenbased methods. Though type-based changes have also been proposed (Brown et al., 1992; Stolcke and Omohundro, 1994), these methods operated greedily, and in Section 5.1, we saw that being greedy led to more brittle results. By working in a sampling framework, we were able bring type-based changes to fruition. 8 While EM technically upda"
N10-1082,P07-1094,0,0.108526,". To combat the problems associated with tokenbased algorithms, we propose a new sampling algorithm that operates on types. Our sampler would, for example, be able to change all occurrences of ((subject verb) object) to (subject (verb object)) in one step. These type-based operations are reminiscent of the type-based grammar operations of early chunkmerge systems (Wolff, 1988; Stolcke and Omohundro, 1994), but we work within a sampling framework for increased robustness. In NLP, perhaps the the most simple and popular sampler is the token-based Gibbs sampler,1 used in Goldwater et al. (2006), Goldwater and Griffiths (2007), and many others. By sampling only one 1 In NLP, this is sometimes referred to as simply the collapsed Gibbs sampler. 573 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 573–581, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 2 Basic Idea via a Motivating Example The key technical problem we solve in this paper is finding a block of variables which are both highly coupled and yet tractable to sample jointly. This section illustrates the main idea behind type-based sampling on a small word segmentatio"
N10-1082,P06-1085,0,0.249744,"resulting in slow mixing. To combat the problems associated with tokenbased algorithms, we propose a new sampling algorithm that operates on types. Our sampler would, for example, be able to change all occurrences of ((subject verb) object) to (subject (verb object)) in one step. These type-based operations are reminiscent of the type-based grammar operations of early chunkmerge systems (Wolff, 1988; Stolcke and Omohundro, 1994), but we work within a sampling framework for increased robustness. In NLP, perhaps the the most simple and popular sampler is the token-based Gibbs sampler,1 used in Goldwater et al. (2006), Goldwater and Griffiths (2007), and many others. By sampling only one 1 In NLP, this is sometimes referred to as simply the collapsed Gibbs sampler. 573 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 573–581, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 2 Basic Idea via a Motivating Example The key technical problem we solve in this paper is finding a block of variables which are both highly coupled and yet tractable to sample jointly. This section illustrates the main idea behind type-based samp"
N10-1082,N06-1041,1,0.561614,"ing on marginal like4 A site could be sampled more than once if it belonged to more than one type block during the iteration (recall that types depend on z and thus could change during sampling). lihood (3) and accuracy for our three models: • HMM: We learned a K = 45 state HMM on the Wall Street Journal (WSJ) portion of the Penn Treebank (49208 sentences, 45 tags) for part-ofspeech induction. We fixed αr to 0.1 and µr to uniform for all r. For accuracy, we used the standard metric based on greedy mapping, where each state is mapped to the POS tag that maximizes the number of correct matches (Haghighi and Klein, 2006). We did not use a tagging dictionary. • USM: We learned a USM model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al. (2006) (9790 sentences) for word segmentation. We fixed α0 to 0.1. The base distribution µ0 penalizes the length of words (see Goldwater et al. (2009) for details). For accuracy, we used word token F1 . • PTSG: We learned a PTSG model on sections 2– 21 of the WSJ treebank.5 For accuracy, we used EVALB parsing F1 on section 22.6 Note this is a supervised task with latent-variables, whereas the other two are purely unsupervised. 5.1 Basic Comparis"
N10-1082,P06-1055,1,0.111558,"Missing"
N10-1082,P09-2012,0,0.169842,"trong type-based coupling (e.g., all instances of a word should be tagged similarly). The type-based sampler we will present is designed exactly to tackle this coupling, which we argue is stronger and more important to deal with in unsupervised learning. Figure 1 depicts the updates made by each of the three samplers. We tested our sampler on three models: a Bayesian HMM for part-of-speech induction (Goldwater and Griffiths, 2007), a nonparametric Bayesian model for word segmentation (Goldwater et al., 2006), and a nonparametric Bayesian model of tree substitution grammars (Cohn et al., 2009; Post and Gildea, 2009). Empirically, we find that typebased sampling improves performance and is less sensitive to initialization (Section 5). The ascending factorial function arises from marginalizing Dirichlet distributions and is responsible the rich-gets-richer phenomenon: the larger n is, more we gain by increasing it.  n possibilithe particular z uniformly out of the m ties. Figure 2(b) shows the effectiveness of this typebased sampler. This simple example exposes the fundamental challenge of multimodality in unsupervised learning. Both m = 0 and m = n are modes due to the rich-gets-richer property which ari"
N10-1083,H05-1009,0,0.0250886,": D ICTIONARY: S TEM: P REFIX: C HARACTER: (e = ·, y = ·) (dist(y, e) = ·) ((y, e) ∈ D) for dictionary D. (stem(e) = ·, y = ·) for Porter stemmer. (prefix(e) = ·, y = ·) for prefixes of length 4. (e = ·, charAt(y, i) = ·) for index i in the Chinese word. These features correspond to several common augmentations of word alignment models, such as adding dictionary priors and truncating long words, but here we integrate them all coherently into a single model. 6.3 Word Alignment Data and Evaluation We evaluate on the standard hand-aligned portion of the NIST 2002 Chinese-English development set (Ayan et al., 2005). The set is annotated with sure S and possible P alignments. We measure alignment quality using alignment error rate (AER) (Och and Ney, 2000). We train the models on 10,000 sentences of FBIS Chinese-English newswire. This is not a large-scale experiment, but large enough to be relevant for lowresource languages. LBFGS experiments are not provided because computing expectations in these models is too computationally intensive to run for many iterations. Hence, EM training is a more appropriate optimization approach: computing the Mstep gradient requires only summing over word type pairs, whil"
N10-1083,N09-1009,0,0.61572,"noulli STOP probabilities θd,c,STOP capture the valence of a particular head. For this type, the decision d is whether or not to stop generating arguments, and the context c contains the current head h, direction δ and adjacency adj. If a head’s stop probability is high, it will be encouraged to accept few arguments. The ATTACH multinomial probability distributions θd,c,ATTACH capture attachment preferences of heads. For this type, a decision d is an argument token a, and the context c consists of a head h and a direction δ. We take the same approach as previous work (Klein and Manning, 2004; Cohen and Smith, 2009) and use gold POS tags in place of words. 3 Haghighi and Klein (2006) achieve higher accuracies by making use of labeled prototypes. We do not use any external information. 586 5.2 Grammar Induction Features One way to inject knowledge into a dependency model is to encode the similarity between the various morphological variants of nouns and verbs. We encode this similarity by incorporating features into both the STOP and the ATTACH probabilities. The attachment features appear below; the stop feature templates are similar and are therefore omitted. (a = ·, h = ·, δ = ·) Generalize the morphol"
N10-1083,P07-1003,1,0.716969,"ages. LBFGS experiments are not provided because computing expectations in these models is too computationally intensive to run for many iterations. Hence, EM training is a more appropriate optimization approach: computing the Mstep gradient requires only summing over word type pairs, while the marginal likelihood gradient needed for LBFGS requires summing over training sentence alignments. The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and us588 For both IBM Model 1 and the HMM alignment model, EM training with feature-enhanced models outperforms the standard multinomial models, by 2.4 and 3.8 AER respectively.6 As expected, large positive weights are assigned to both the dictionary and edit distance features. Stem and character features also contribute to the performance gain. 7 Word Segmentation Finally, we show that it is possible to improve upon the simple and effective word segmentation model presented in Liang and Klein (2009) by adding phonological features. Unsupervised word segmentation is the task of identifying"
N10-1083,A94-1009,0,0.149142,"h task, we show that declaring a few linguistically motivated feature templates yields state-of-the-art results. 2 Models We start by explaining our feature-enhanced model for part-of-speech (POS) induction. This particular example illustrates our approach to adding features to unsupervised models in a well-known NLP task. We then explain how the technique applies more generally. 2.1 Example: Part-of-Speech Induction POS induction consists of labeling words in text with POS tags. A hidden Markov model (HMM) is a standard model for this task, used in both a frequentist setting (Merialdo, 1994; Elworthy, 1994) and in a Bayesian setting (Goldwater and Griffiths, 2007; Johnson, 2007). A POS HMM generates a sequence of words in order. In each generation step, an observed word emission yi and a hidden successor POS tag zi+1 are generated independently, conditioned on the current POS tag zi . This process continues until an absorbing stop state is generated by the transition model. There are two types of conditional distributions in the model—emission and transition probabilities— that are both multinomial probability distributions. The joint likelihood factors into these distributions: exp hw, f (y, z,"
N10-1083,P07-1094,0,0.456947,"stically motivated feature templates yields state-of-the-art results. 2 Models We start by explaining our feature-enhanced model for part-of-speech (POS) induction. This particular example illustrates our approach to adding features to unsupervised models in a well-known NLP task. We then explain how the technique applies more generally. 2.1 Example: Part-of-Speech Induction POS induction consists of labeling words in text with POS tags. A hidden Markov model (HMM) is a standard model for this task, used in both a frequentist setting (Merialdo, 1994; Elworthy, 1994) and in a Bayesian setting (Goldwater and Griffiths, 2007; Johnson, 2007). A POS HMM generates a sequence of words in order. In each generation step, an observed word emission yi and a hidden successor POS tag zi+1 are generated independently, conditioned on the current POS tag zi . This process continues until an absorbing stop state is generated by the transition model. There are two types of conditional distributions in the model—emission and transition probabilities— that are both multinomial probability distributions. The joint likelihood factors into these distributions: exp hw, f (y, z, EMIT)i ′ y ′ exp hw, f (y , z, EMIT )i θy,z,EMIT (w) = P"
N10-1083,P06-1085,0,0.0182129,"ution: θl,LENGTH = exp(−l1.6 ) when 1 ≤ l ≤ 10. Their model is deficient since it is possible to generate 6 The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al., 2009). lengths that are inconsistent with the actual lengths of the generated segments. The likelihood equation is given by: P (Y = y, Z = z) = θSTOP |z| Y   (1 − θSTOP ) θzi ,SEGMENT exp(−|zi |1.6 ) i=1 7.2 Segmentation Data and Evaluation We train and test on the phonetic version of the Bernstein-Ratner corpus (1987). This is the same set-up used by Liang and Klein (2009), Goldwater et al. (2006), and Johnson and Goldwater (2009). This corpus consists of 9790 child-directed utterances transcribed using a phonetic representation. We measure segment F1 score on the entire corpus. We run all word segmentation models for 300 iterations with 10 random initializations and report the mean and standard deviation of F1 in Table 1. 7.3 Segmentation Features The SEGMENT multinomial is the important distribution in this model. We use the following features: BASIC: L ENGTH: N UMBER -VOWELS: P HONO -C LASS -P REF: P HONO -C LASS -P REF: (z = ·) (length(z) = ·) (numVowels(z) = ·) (prefix(coarsePhone"
N10-1083,P09-1104,1,0.664262,". For this type, there is no context and the decision is the particular string generated. In order to avoid the degenerate MLE that assigns mass only to single segment sentences it is helpful to independently generate a length for each segment from a fixed distribution. Liang and Klein (2009) constrain individual segments to have maximum length 10 and generate lengths from the following distribution: θl,LENGTH = exp(−l1.6 ) when 1 ≤ l ≤ 10. Their model is deficient since it is possible to generate 6 The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al., 2009). lengths that are inconsistent with the actual lengths of the generated segments. The likelihood equation is given by: P (Y = y, Z = z) = θSTOP |z| Y   (1 − θSTOP ) θzi ,SEGMENT exp(−|zi |1.6 ) i=1 7.2 Segmentation Data and Evaluation We train and test on the phonetic version of the Bernstein-Ratner corpus (1987). This is the same set-up used by Liang and Klein (2009), Goldwater et al. (2006), and Johnson and Goldwater (2009). This corpus consists of 9790 child-directed utterances transcribed using a phonetic representation. We measure segment F1 score on the entire corpus. We run all word"
N10-1083,N09-1036,0,0.361345,"lish sentence e. The likelihood of both models takes the form: Y p(zj = i|zj−1 ) · θyj ,ei ,ALIGN P (y, z|e) = j 4 Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. 587 WSJ WSJ10 – 1.0 5.0 Grammar Induction Basic-DMV Feature-DMV Inference EM LBFGS EM LBFGS EM EM LBFGS (Cohen and Smith, 2009) Basic-DMV EM Feature-DMV EM LBFGS (Cohen and Smith, 2009) Word Alignment Basic-Model 1 EM Feature-Model 1 EM Basic-HMM EM Feature-HMM EM Word Segmentation Basic-Unigram EM Feature-Unigram EM LBFGS (Johnson and Goldwater, 2009) BR NIST ChEn 5.4 Grammar Induction Results We are able to outperform Cohen and Smith’s (2009) best system, which requires a more complicated variational inference method, on both English and Chinese data sets. Their system achieves an accuracy of 61.3 for English and an accuracy of 51.9 for Chinese.4 Our feature-enhanced model, trained using the direct gradient approach, achieves an accuracy of 63.0 for English, and an accuracy of 53.6 for Chinese. To our knowledge, our method for featurebased dependency parse induction outperforms all existing methods that make the same set of conditional in"
N10-1083,D07-1031,0,0.900802,"lates yields state-of-the-art results. 2 Models We start by explaining our feature-enhanced model for part-of-speech (POS) induction. This particular example illustrates our approach to adding features to unsupervised models in a well-known NLP task. We then explain how the technique applies more generally. 2.1 Example: Part-of-Speech Induction POS induction consists of labeling words in text with POS tags. A hidden Markov model (HMM) is a standard model for this task, used in both a frequentist setting (Merialdo, 1994; Elworthy, 1994) and in a Bayesian setting (Goldwater and Griffiths, 2007; Johnson, 2007). A POS HMM generates a sequence of words in order. In each generation step, an observed word emission yi and a hidden successor POS tag zi+1 are generated independently, conditioned on the current POS tag zi . This process continues until an absorbing stop state is generated by the transition model. There are two types of conditional distributions in the model—emission and transition probabilities— that are both multinomial probability distributions. The joint likelihood factors into these distributions: exp hw, f (y, z, EMIT)i ′ y ′ exp hw, f (y , z, EMIT )i θy,z,EMIT (w) = P This feature-ba"
N10-1083,P04-1061,1,0.924351,"margin of 12.4. These results show that the direct gradient approach can offer additional boosts in performance when used with a feature-enhanced model. We also outperform the globally normalized MRF, which uses the same set of features and which we train using a direct gradient approach. To the best of our knowledge, our system achieves the best performance to date on the WSJ corpus for totally unsupervised POS tagging.3 5 Grammar Induction We next apply our technique to a grammar induction task: the unsupervised learning of dependency parse trees via the dependency model with valence (DMV) (Klein and Manning, 2004). A dependency parse is a directed tree over tokens in a sentence. Each edge of the tree specifies a directed dependency from a head token to a dependent, or argument token. Thus, the number of dependencies in a parse is exactly the number of tokens in the sentence, not counting the artificial root token. 5.1 Dependency Model with Valence The DMV defines a probability distribution over dependency parse trees. In this head-outward attachment model, a parse and the word tokens are derived together through a recursive generative process. For each token generated so far, starting with the root, a"
N10-1083,N09-1069,1,0.872403,"models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and us588 For both IBM Model 1 and the HMM alignment model, EM training with feature-enhanced models outperforms the standard multinomial models, by 2.4 and 3.8 AER respectively.6 As expected, large positive weights are assigned to both the dictionary and edit distance features. Stem and character features also contribute to the performance gain. 7 Word Segmentation Finally, we show that it is possible to improve upon the simple and effective word segmentation model presented in Liang and Klein (2009) by adding phonological features. Unsupervised word segmentation is the task of identifying word boundaries in sentences where spaces have been removed. For a sequence of characters y = (y1 , ..., yn ), a segmentation is a sequence of segments z = (z1 , ..., z|z |) such that z is a partition of y and each zi is a contiguous subsequence of y. Unsupervised models for this task infer word boundaries from corpora of sentences of characters without ever seeing examples of well-formed words. 7.1 Unigram Double-Exponential Model Liang and Klein’s (2009) unigram doubleexponential model corresponds to"
N10-1083,N06-1014,1,0.244933,"each optimization procedure for 100 iterations. The results are reported in Table 1. κ – – – – κ – 0.2 0.2 Eval Many-1 63.1 (1.3) 59.6 (6.9) 68.1 (1.7) 75.5 (1.1) Dir 47.8 48.3 63.0 61.3 42.5 49.9 53.6 51.9 AER 38.0 35.6 33.8 30.0 F1 76.9 (0.1) 84.5 (0.5) 88.0 (0.1) 87 Table 1: Locally normalized feature-based models outperform all proposed baselines for all four tasks. LBFGS outperformed EM in all cases where the algorithm was sufficiently fast to run. Details of each experiment appear in the main text. The distortion term p(zj = i|zj−1 ) is uniform in Model 1, and Markovian in the HMM. See Liang et al. (2006) for details on the specific variant of the distortion model of the HMM that we used. We use these standard distortion models in both the baseline and feature-enhanced word alignment systems. The bilexical emission model θy,e,ALIGN differentiates our feature-enhanced system from the baseline system. In the former, the emission model is a standard conditional multinomial that represents the probability that decision word y is generated from context word e, while in our system, the emission model is re-parameterized as a logistic regression model and feature-enhanced. Many supervised feature-bas"
N10-1083,J93-2004,0,0.0463736,"ate for words with certain orthographic properties. We use only the BASIC features for transitions. For an emission with word y and tag z, we use the following feature templates: (y = ·, z = ·) Check if y contains digit and conjoin with z: (containsDigit(y) = ·, z = ·) C ONTAINS -H YPHEN: (containsHyphen(x) = ·, z = ·) I NITIAL -C AP: Check if the first letter of y is capitalized: (isCap(y) = ·, z = ·) N-G RAM: Indicator functions for character ngrams of up to length 3 present in y. BASIC: C ONTAINS -D IGIT: 4.2 POS Induction Data and Evaluation We train and test on the entire WSJ tag corpus (Marcus et al., 1993). We attempt the most difficult version of this task where the only information our system can make use of is the unlabeled text itself. In particular, we do not make use of a tagging dictionary. We use 45 tag clusters, the number of POS tags that appear in the WSJ corpus. There is an identifiability issue when evaluating inferred tags. In order to measure accuracy on the hand-labeled corpus, we map each cluster to the tag that gives the highest accuracy, the many-1 evaluation approach (Johnson, 2007). We run all POS induction models for 1000 iterations, with 10 random initializations. The mea"
N10-1083,J94-2001,0,0.262389,"entation. In each task, we show that declaring a few linguistically motivated feature templates yields state-of-the-art results. 2 Models We start by explaining our feature-enhanced model for part-of-speech (POS) induction. This particular example illustrates our approach to adding features to unsupervised models in a well-known NLP task. We then explain how the technique applies more generally. 2.1 Example: Part-of-Speech Induction POS induction consists of labeling words in text with POS tags. A hidden Markov model (HMM) is a standard model for this task, used in both a frequentist setting (Merialdo, 1994; Elworthy, 1994) and in a Bayesian setting (Goldwater and Griffiths, 2007; Johnson, 2007). A POS HMM generates a sequence of words in order. In each generation step, an observed word emission yi and a hidden successor POS tag zi+1 are generated independently, conditioned on the current POS tag zi . This process continues until an absorbing stop state is generated by the transition model. There are two types of conditional distributions in the model—emission and transition probabilities— that are both multinomial probability distributions. The joint likelihood factors into these distributions:"
N10-1083,C96-2141,0,0.109148,"or our Chinese experiments, we use the same corpus and training/test split as Cohen and Smith 6 Word Alignment Word alignment is a core machine learning component of statistical machine translation systems, and one of the few NLP tasks that is dominantly solved using unsupervised techniques. The purpose of word alignment models is to induce a correspondence between the words of a sentence and the words of its translation. 6.1 Word Alignment Models We consider two classic generative alignment models that are both used heavily today, IBM Model 1 (Brown et al., 1994) and the HMM alignment model (Ney and Vogel, 1996). These models generate a hidden alignment vector z and an observed foreign sentence y, all conditioned on an observed English sentence e. The likelihood of both models takes the form: Y p(zj = i|zj−1 ) · θyj ,ei ,ALIGN P (y, z|e) = j 4 Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. 587 WSJ WSJ10 – 1.0 5.0 Grammar Induction Basic-DMV Feature-DMV Inference EM LBFGS EM LBFGS EM EM LBFGS (Cohen and Smith, 2009) Basic-DMV EM Feature-DMV EM LBFGS (Cohen and Smith, 2009) Word Alignment Bas"
N10-1083,P00-1056,0,0.0467888,"er. (prefix(e) = ·, y = ·) for prefixes of length 4. (e = ·, charAt(y, i) = ·) for index i in the Chinese word. These features correspond to several common augmentations of word alignment models, such as adding dictionary priors and truncating long words, but here we integrate them all coherently into a single model. 6.3 Word Alignment Data and Evaluation We evaluate on the standard hand-aligned portion of the NIST 2002 Chinese-English development set (Ayan et al., 2005). The set is annotated with sure S and possible P alignments. We measure alignment quality using alignment error rate (AER) (Och and Ney, 2000). We train the models on 10,000 sentences of FBIS Chinese-English newswire. This is not a large-scale experiment, but large enough to be relevant for lowresource languages. LBFGS experiments are not provided because computing expectations in these models is too computationally intensive to run for many iterations. Hence, EM training is a more appropriate optimization approach: computing the Mstep gradient requires only summing over word type pairs, while the marginal likelihood gradient needed for LBFGS requires summing over training sentence alignments. The final alignments, in both the basel"
N10-1083,P05-1044,0,0.794494,"in the form of conditional independence structure, which means that injecting it is both tricky (because the connection between independence and knowledge is subtle) and timeconsuming (because new structure often necessitates new inference algorithms). In this paper, we present a range of experiments wherein we improve existing unsupervised models by declaratively adding richer features. In particular, we parameterize the local multinomials of existThe idea of using features in unsupervised learning is neither new nor even controversial. Many top unsupervised results use feature-based models (Smith and Eisner, 2005; Haghighi and Klein, 2006). However, such approaches have presented their own barriers, from challenging normalization problems, to neighborhood design, to the need for complex optimization procedures. As a result, most work still focuses on the stable and intuitive approach of using the EM algorithm to optimize data likelihood in locally normalized, generative models. The primary contribution of this paper is to demonstrate the clear empirical success of a simple and accessible approach to unsupervised learning with features, which can be optimized by using standard NLP building blocks. We c"
N10-1083,J07-4003,0,0.0056588,"malized models. For models parameterized by standard multinomials, EM optimizes L(θ) = log Pθ (Y = y) (Dempster et al., 1977). The E-step computes expected counts for each tuple of decision d, context c, and multinomial type t: ed,c,t ← Eθ 2 &quot; X i∈I # (Xi = d, Xπ(i) = c, t) Y = y (2) The locally normalized model class is actually equivalent to its globally normalized counterpart when the former meets the following three conditions: (1) The graphical model is a directed tree. (2) The BASIC features are included in f . (3) We do not include regularization in the model (κ = 0). This follows from Smith and Johnson (2007). 584 ed,c,t d′ ed′ ,c,t θd,c,t ← P Normalizing expected counts in this way maximizes the expected complete log likelihood with respect to the current model parameters. EM can likewise optimize L(w) for our locally normalized models with logistic parameterizations. The E-step first precomputes multinomial parameters from w for each decision, context, and type: exphw, f (d, c, t)i ′ d′ exphw, f (d , c, t)i θd,c,t (w) ← P Then, expected counts e are computed according to Equation 2. In the case of POS induction, expected counts are computed with the forwardbackward algorithm in both the standard"
N10-1083,C02-1145,0,0.0134758,"inese data sets. Their system achieves an accuracy of 61.3 for English and an accuracy of 51.9 for Chinese.4 Our feature-enhanced model, trained using the direct gradient approach, achieves an accuracy of 63.0 for English, and an accuracy of 53.6 for Chinese. To our knowledge, our method for featurebased dependency parse induction outperforms all existing methods that make the same set of conditional independence assumptions as the DMV. Reg κ – 0.1 1.0 1.0 κ – 0.05 10.0 CTB10 Model POS Induction Basic-HMM Feature-MRF Feature-HMM (2009). We train on sections 1-270 of the Penn Chinese Treebank (Xue et al., 2002), similarly reduced (CTB10). We test on sections 271-300 of CTB10, and use sections 400-454 as a development set. The DMV is known to be sensitive to initialization. We use the deterministic harmonic initializer from Klein and Manning (2004). We ran each optimization procedure for 100 iterations. The results are reported in Table 1. κ – – – – κ – 0.2 0.2 Eval Many-1 63.1 (1.3) 59.6 (6.9) 68.1 (1.7) 75.5 (1.1) Dir 47.8 48.3 63.0 61.3 42.5 49.9 53.6 51.9 AER 38.0 35.6 33.8 30.0 F1 76.9 (0.1) 84.5 (0.5) 88.0 (0.1) 87 Table 1: Locally normalized feature-based models outperform all proposed baselin"
N10-1083,J93-2003,0,\N,Missing
N10-1083,N06-1041,1,\N,Missing
N10-1083,P01-1027,0,\N,Missing
N10-1083,P06-1111,1,\N,Missing
N12-1004,P06-1002,0,0.0236752,"but the structure of the model is such that two conflicting phrase pairs are unlikely to simultaneously have high posterior probability. Most publicly available translation systems expect word-level alignments as input. These can also be generated by applying posterior thresholding, aligning target word i to source word j whenever baij (sure) ≥ t.9 7 Experiments Our experiments are performed on Chinese-toEnglish alignment. We trained and evaluated all models on the NIST MT02 test set, which consists of 150 training and 191 test sentences and has been used previously in alignment experiments (Ayan and Dorr, 2006; Haghighi et al., 2009; DeNero and Klein, 2010). The unsupervised HMM word aligner used to generate features for the model was trained on 11.3 million words of FBIS newswire data. We test three models: the Viterbi ITG model of DeNero and Klein (2010), our BP ITG model that uses the ITG factor, and our BP Relaxed model that replaces the ITG factor with the O NE S PAN factors. In all of our experiments, the phrase length d was set to 3.10 7.1 Phrase Alignment We tested the models by computing precision and recall on extracted phrase pairs, relative to the gold phrase pairs of up to length 3 ind"
N12-1004,W06-3123,0,0.0569948,"Missing"
N12-1004,P09-1088,0,0.0433621,"Missing"
N12-1004,W07-0403,0,0.0424701,"Missing"
N12-1004,J07-2003,0,0.0509103,"wn as blue vertical lines and source spans as red horizontal lines. Because there is a sure link at a48 , σ8f = [4, 4] does not include the possible link at a38 . However, f7 only has possible links, so σ7f = [5, 6] is the span containing those. f9 is null-aligned, so σ9f = [−1, ∞], which blocks all phrase pairs containing f9 from being extracted. 2.1 Extraction Sets from Word Alignments The mapping from a word alignment to the set of licensed phrase pairs π(a) is based on the standard rule extraction procedures used in most modern statistical systems (Koehn et al., 2003; Galley et al., 2006; Chiang, 2007), but extended to handle possible links (DeNero and Klein, 2010). We start by using a to find a projection from each target word ei onto a source span, represented as blue vertical lines in Figure 1. Similarly, source words project onto target spans (red horizontal lines in Figure 1). π(a) contains a phrase pair iff every word in the target span projects within the source span and vice versa. Figure 1 contains an example for d = 2. Formally, the mapping introduces a set of spans σ. We represent the spans as variables whose values are intervals, where σie = [k, `] means that the target word ei"
N12-1004,E09-1020,0,0.187559,"Missing"
N12-1004,D07-1079,0,0.0233998,"c to convert these to phrase alignments (Koehn et al., 2003). There have been many attempts over the last decade to develop model-based approaches to the phrase alignment problem (Marcu and Wong, 2002; Birch et al., 2006; DeNero et al., 2008; Blunsom et al., 2009). However, most of these have met with limited success compared to the simpler heuristic method. One key problem with typical models of phrase alignment is that they choose a single (latent) segmentation, giving rise to undesirable modeling biases (DeNero et al., 2006) and reducing coverage, which in turn reduces translation quality (DeNeefe et al., 2007; DeNero et al., 2008). On the other hand, the extraction heuristic identifies many overlapping options, and achieves high coverage. In response to these effects, the recent phrase alignment work of DeNero and Klein (2010) models extraction sets: collections of overlapping phrase pairs that are consistent with an underlying word alignment. Their extraction set model is empirically very accurate. However, the ability to model overlapping – and therefore non-local – features comes at a high computational cost. DeNero and Klein (2010) handle this in part by imposing a structural ITG constraint (W"
N12-1004,P10-1147,1,0.196918,"6; DeNero et al., 2008; Blunsom et al., 2009). However, most of these have met with limited success compared to the simpler heuristic method. One key problem with typical models of phrase alignment is that they choose a single (latent) segmentation, giving rise to undesirable modeling biases (DeNero et al., 2006) and reducing coverage, which in turn reduces translation quality (DeNeefe et al., 2007; DeNero et al., 2008). On the other hand, the extraction heuristic identifies many overlapping options, and achieves high coverage. In response to these effects, the recent phrase alignment work of DeNero and Klein (2010) models extraction sets: collections of overlapping phrase pairs that are consistent with an underlying word alignment. Their extraction set model is empirically very accurate. However, the ability to model overlapping – and therefore non-local – features comes at a high computational cost. DeNero and Klein (2010) handle this in part by imposing a structural ITG constraint (Wu, 1997) on the underlying word alignments. This permits a polynomial-time algorithm, but it is still O(n6 ), with a large constant factor once the state space is appropriately enriched to capture overlap. Therefore, they"
N12-1004,W06-3105,1,0.793194,"Li and Khudanpur, 2008; Galley et al., 2004), typically using a deterministic heuristic to convert these to phrase alignments (Koehn et al., 2003). There have been many attempts over the last decade to develop model-based approaches to the phrase alignment problem (Marcu and Wong, 2002; Birch et al., 2006; DeNero et al., 2008; Blunsom et al., 2009). However, most of these have met with limited success compared to the simpler heuristic method. One key problem with typical models of phrase alignment is that they choose a single (latent) segmentation, giving rise to undesirable modeling biases (DeNero et al., 2006) and reducing coverage, which in turn reduces translation quality (DeNeefe et al., 2007; DeNero et al., 2008). On the other hand, the extraction heuristic identifies many overlapping options, and achieves high coverage. In response to these effects, the recent phrase alignment work of DeNero and Klein (2010) models extraction sets: collections of overlapping phrase pairs that are consistent with an underlying word alignment. Their extraction set model is empirically very accurate. However, the ability to model overlapping – and therefore non-local – features comes at a high computational cost."
N12-1004,D08-1033,1,0.889402,"Missing"
N12-1004,N04-1035,0,0.074998,"Missing"
N12-1004,P06-1121,0,0.0361397,"Target spans are shown as blue vertical lines and source spans as red horizontal lines. Because there is a sure link at a48 , σ8f = [4, 4] does not include the possible link at a38 . However, f7 only has possible links, so σ7f = [5, 6] is the span containing those. f9 is null-aligned, so σ9f = [−1, ∞], which blocks all phrase pairs containing f9 from being extracted. 2.1 Extraction Sets from Word Alignments The mapping from a word alignment to the set of licensed phrase pairs π(a) is based on the standard rule extraction procedures used in most modern statistical systems (Koehn et al., 2003; Galley et al., 2006; Chiang, 2007), but extended to handle possible links (DeNero and Klein, 2010). We start by using a to find a projection from each target word ei onto a source span, represented as blue vertical lines in Figure 1. Similarly, source words project onto target spans (red horizontal lines in Figure 1). π(a) contains a phrase pair iff every word in the target span projects within the source span and vice versa. Figure 1 contains an example for d = 2. Formally, the mapping introduces a set of spans σ. We represent the spans as variables whose values are intervals, where σie = [k, `] means that the"
N12-1004,P09-1104,1,0.900081,"lignments a. First, it requires that each word is aligned to at most one relatively short subspan of the other sentence. This is a linguistically plausible constraint, as it is rarely the case that a single word will translate to an extremely long phrase, or to multiple widely separated phrases.3 The other constraint imposed by the ITG factor is the ITG reordering constraint. This constraint is imposed primarily for reasons of computational tractability: the standard dynamic program for bitext parsing depends on ITG reordering (Wu, 1997). While this constraint is not dramatically restrictive (Haghighi et al., 2009), it is plausible that removing it would permit the model to produce better alignments. We tested this hypothesis by developing a new model that enforces only the constraint that each word align to one limited-length subspan, which can be viewed as a generalization of the atmost-one-to-one constraint frequently considered in the word-alignment literature (Taskar et al., 2005; Cromi`eres and Kurohashi, 2009). Our new model has almost exactly the same form as the previous one. The only difference is that A is replaced with a new family of simpler factors: O NE S PAN . For each target word ei (an"
N12-1004,N03-1017,0,0.0139706,"(e.g. π5667 = true). Target spans are shown as blue vertical lines and source spans as red horizontal lines. Because there is a sure link at a48 , σ8f = [4, 4] does not include the possible link at a38 . However, f7 only has possible links, so σ7f = [5, 6] is the span containing those. f9 is null-aligned, so σ9f = [−1, ∞], which blocks all phrase pairs containing f9 from being extracted. 2.1 Extraction Sets from Word Alignments The mapping from a word alignment to the set of licensed phrase pairs π(a) is based on the standard rule extraction procedures used in most modern statistical systems (Koehn et al., 2003; Galley et al., 2006; Chiang, 2007), but extended to handle possible links (DeNero and Klein, 2010). We start by using a to find a projection from each target word ei onto a source span, represented as blue vertical lines in Figure 1. Similarly, source words project onto target spans (red horizontal lines in Figure 1). π(a) contains a phrase pair iff every word in the target span projects within the source span and vice versa. Figure 1 contains an example for d = 2. Formally, the mapping introduces a set of spans σ. We represent the spans as variables whose values are intervals, where σie = ["
N12-1004,P07-2045,0,0.00304488,"els to run for 2, 3, 5, 10, and 20 iterations. The results are shown in Figure 5. Neither model benefits from running more iterations than used to obtain the results in Figure 4, but each can be sped up by a factor of almost 1.5x in exchange for a modest (&lt; 1 F1 ) drop in accuracy. 11 The speed advantage of Viterbi ITG over BP ITG comes from Viterbi ITG’s aggressive beaming. 36 Model BLEU Baseline Viterbi ITG BP Relaxed 32.8 33.5 33.6 Relative Improve. +0.0 +0.7 +0.8 Hours to Train/Align 5 831 39 Table 2: Machine translation results. 7.3 Translation We ran translation experiments using Moses (Koehn et al., 2007), which we trained on a 22.1 million word parallel corpus from the GALE program. We compared alignments generated by the baseline HMM model, the Viterbi ITG model and the Relaxed BP model.12 The systems were tuned and evaluated on sentences up to length 40 from the NIST MT04 and MT05 test sets. The results, shown in Table 2, show that the BP Relaxed model achives a 0.8 BLEU improvement over the HMM baseline, comparable to that of the Viterbi ITG model, but taking a fraction of the time,13 making the BP Relaxed model a practical alternative for real translation applications. 12 Following a simp"
N12-1004,W08-0402,0,0.0267086,"Missing"
N12-1004,N06-1014,1,0.735956,"oring factors all take the form exp(w · φ), and so can be described in terms of their respective local feature vectors, φ. Depending on the values of the variables each factor depends on, the factor can be active or inactive. Features are only extracted for active factors; otherwise φ is empty and the factor produces a value of 1. S URE L INK . Each word alignment variable aij has a corresponding S URE L INK factor Lij to incorporate scores from the features φa (aij ). Lij is active whenever aij = sure. φa (aij ) includes posteriors from unsupervised jointly trained HMM word alignment models (Liang et al., 2006), dictionary A agk a11 a21 L11 a12 a|e|k ahk Skf σkf Nkf ai1 L21 ah� ag� Li1 a|e|� S�f σ�f N�f a22 L12 L22 a1j aij L1j ag|f| ah|f| Sge She σge σhe Pghk� πghk� Lij Nge (a) ITG factor Nhe Rghk� (b) S PAN and E XTRACT factors Figure 2: A factor graph representation of the ITG-based extraction set model. For visual clarity, we draw the graph separated into two components: one containing the factors that only neighbor word link variables, and one containing the remaining factors. and identical word features, a position distortion feature, and features for numbers and punctuation. P HRASE PAIR . For"
N12-1004,W02-1018,0,0.0611679,"Missing"
N12-1004,W08-0303,0,0.0550904,"re accurate. First, given the model of DeNero and Klein (2010), we decompose it into factors that admit an efficient BP approximation. BP is an inference technique that can be used to efficiently approximate posterior marginals on variables in a graphical model; here the marginals of interest are the phrase pair posteriors. BP has only recently come into use in the NLP community, but it has been shown to be effective in other complex structured classification tasks, such as dependency parsing (Smith and Eisner, 2008). There has also been some prior success in using BP for both discriminative (Niehues and Vogel, 2008) and generative (Cromi`eres and Kurohashi, 2009) word alignment models. By aligning all phrase pairs whose posterior under BP exceeds some fixed threshold, our BP approximation of the model of DeNero and Klein (2010) can 29 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 29–38, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics achieve a comparable phrase pair F1 . Furthermore, because we have posterior marginals rather than a single Viterbi derivation, we can explicitly force th"
N12-1004,D08-1016,0,0.652189,"del’s ITG-based structural formulation, resulting in a new model that is simultaneously faster and more accurate. First, given the model of DeNero and Klein (2010), we decompose it into factors that admit an efficient BP approximation. BP is an inference technique that can be used to efficiently approximate posterior marginals on variables in a graphical model; here the marginals of interest are the phrase pair posteriors. BP has only recently come into use in the NLP community, but it has been shown to be effective in other complex structured classification tasks, such as dependency parsing (Smith and Eisner, 2008). There has also been some prior success in using BP for both discriminative (Niehues and Vogel, 2008) and generative (Cromi`eres and Kurohashi, 2009) word alignment models. By aligning all phrase pairs whose posterior under BP exceeds some fixed threshold, our BP approximation of the model of DeNero and Klein (2010) can 29 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 29–38, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics achieve a comparable phrase pair F1 . Furthermore, b"
N12-1004,H05-1010,1,0.790761,"his constraint is imposed primarily for reasons of computational tractability: the standard dynamic program for bitext parsing depends on ITG reordering (Wu, 1997). While this constraint is not dramatically restrictive (Haghighi et al., 2009), it is plausible that removing it would permit the model to produce better alignments. We tested this hypothesis by developing a new model that enforces only the constraint that each word align to one limited-length subspan, which can be viewed as a generalization of the atmost-one-to-one constraint frequently considered in the word-alignment literature (Taskar et al., 2005; Cromi`eres and Kurohashi, 2009). Our new model has almost exactly the same form as the previous one. The only difference is that A is replaced with a new family of simpler factors: O NE S PAN . For each target word ei (and each source word fj ) we include a hard constraint factor e |&lt; d Uie (respectively Ujf ). Uie is satisfied iff |σi,p e (length limit) and either σi,p = [−1, ∞] or ∀j ∈ e , a 6= off (no gaps), with σ e as in Eq. (3). Figσi,p ij i,p ure 3 shows the portion of the factor graph from Figure 2a redrawn with the O NE S PAN factors replacing the ITG factor. As Figure 3 shows, ther"
N12-1004,J97-3002,0,0.419886,"7; DeNero et al., 2008). On the other hand, the extraction heuristic identifies many overlapping options, and achieves high coverage. In response to these effects, the recent phrase alignment work of DeNero and Klein (2010) models extraction sets: collections of overlapping phrase pairs that are consistent with an underlying word alignment. Their extraction set model is empirically very accurate. However, the ability to model overlapping – and therefore non-local – features comes at a high computational cost. DeNero and Klein (2010) handle this in part by imposing a structural ITG constraint (Wu, 1997) on the underlying word alignments. This permits a polynomial-time algorithm, but it is still O(n6 ), with a large constant factor once the state space is appropriately enriched to capture overlap. Therefore, they use a heavily beamed Viterbi search procedure to find a reasonable alignment within an acceptable time frame. In this paper, we show how to use belief propagation (BP) to improve on the model’s ITG-based structural formulation, resulting in a new model that is simultaneously faster and more accurate. First, given the model of DeNero and Klein (2010), we decompose it into factors that"
N12-1004,P05-1059,0,0.0776025,"Missing"
N12-1004,P08-1012,0,0.0188397,"ch σjf and corresponding row j of a. E XTRACT. For each phrase pair variable πghk` we have a factor Pghk` to ensure that πghk` = true iff it is licensed by the span projections σ. As shown in Figure 2b, in addition to πghk` , Pghk` depends on the range of span variables σie for i ∈ [g, h] and σjf for j ∈ [k, `]. Pghk` is satisfied when πghk` = true and the relations in Eq. (5) all hold, or when πghk` = false and at least one of those relations does not hold. in at most one such terminal production, and that the full set of terminal block productions is consistent with ITG reordering patterns (Zhang et al., 2008). U1f 3 U1e U2e Uie a11 a21 ai1 Relaxing the ITG Constraint L11 U2f The ITG factor can be viewed as imposing two different types of constraints on allowable word alignments a. First, it requires that each word is aligned to at most one relatively short subspan of the other sentence. This is a linguistically plausible constraint, as it is rarely the case that a single word will translate to an extremely long phrase, or to multiple widely separated phrases.3 The other constraint imposed by the ITG factor is the ITG reordering constraint. This constraint is imposed primarily for reasons of comput"
N12-1004,2006.amta-papers.2,0,\N,Missing
N15-1027,P14-1129,0,0.0587568,"ving model quality. 1 Introduction This paper investigates the theoretical properties of log-linear models trained to make their unnormalized scores approximately sum to one. Recent years have seen a resurgence of interest in log-linear approaches to language modeling. This includes both conventional log-linear models (Rosenfeld, 1994; Biadsy et al., 2014) and neural networks with a log-linear output layer (Bengio et al., 2006). On a variety of tasks, these LMs have produced substantial gains over conventional generative models based on counting n-grams. Successes include machine translation (Devlin et al., 2014) and speech recognition (Graves et al., 2013). However, log-linear LMs come at a significant cost for computational efficiency. In order to output a well-formed probability distribution over words, such models must typically calculate a normalizing constant whose computational cost grows linearly in the size of the vocabulary. Fortunately, many applications of LMs remain well-behaved even if LM scores do not actually correspond to probability distributions. For example, if a machine translation decoder uses output from a pre-trained LM as a feature inside a larger model, it suffices to have al"
N15-1027,D13-1140,0,0.0241032,"main well-behaved even if LM scores do not actually correspond to probability distributions. For example, if a machine translation decoder uses output from a pre-trained LM as a feature inside a larger model, it suffices to have all output scores on approximately the same scale, even if these do not sum to one for every LM context. There has thus been considerable research interest around training procedures capable of ensuring that unnormalized outputs for every context are “close” to a probability distribution. We are aware of at least two such techniques: noisecontrastive estimation (NCE) (Vaswani et al., 2013; Gutmann and Hyv¨arinen, 2010) and explicit penalization of the log-normalizer (Devlin et al., 2014). Both approaches have advantages and disadvantages. NCE allows fast training by dispensing with the need to ever compute a normalizer. Explicit penalization requires full normalizers to be computed during training but parameterizes the relative importance of the likelihood and the “sum-to-one” constraint, allowing system designers to tune the objective for optimal performance. While both NCE and explicit penalization are observed to work in practice, their theoretical properties have not been"
N15-1029,N01-1016,0,0.134289,"nson and Charniak, 2004), causing researchers to employ other approaches such as pipelines of sequence models (Qian and Liu, 2013) or incremental syntactic systems (Honnibal and Johnson, 2014). Second, human processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al., 1997), so an automatic system must employ features targeting all levels of the perceptual stack to achieve high performance. In spite of this, the primary thread of work in the NLP community has focused on identifying disfluencies based only on lexicosyntactic cues (Heeman and Allen, 1994; Charniak and Johnson, 2001; Snover et al., 2004; Rasooli and Tetreault, 2013). A separate line of work has therefore attempted to build systems that leverage prosody as well as lexical information (Shriberg et al., 1997; Liu et al., 2003; Kim et al., 2004; Liu et al., 2006), though often with mixed success. In this work, we present a model for disfluency detection that improves upon model structures used in past work and leverages additional prosodic information. Our model is a semi-Markov conditional random field that distinguishes disfluent chunks (to be deleted) from fluent chunks (everything else), as shown in Figu"
N15-1029,N10-1112,0,0.00949571,"input sentence. In our model we constrain the transitions so that fluent spans can only be followed by disfluent spans. For this task, the spans we are predicting correspond directly to the reparanda of disfluencies, since these are the parts of the input sentences that should be removed. Note that our feature function can jointly inspect both the beginning and ending of the disfluency; we will describe the features of this form more specifically in Section 3.2.2. To train our model, we maximize conditional log likelihood of the training data augmented with a loss function via softmax-margin (Gimpel and Smith, 2010). Specifically, during training, we maxPd 0 imize L(θ) = i=1 log pθ (¯ s|x), where p0θ (¯ s|x) = ∗ pθ (¯ s|x) exp (`(¯ s, s¯ )). We take the loss function Surrounding POS: (VB, WRB) Unigrams: determine, how, you Bigrams: (determine, how), (how, you) POS Unigrams: VB, WRB, PRP POS Bigrams: (VB, WRB), (WRB, PRP) O O B I E O O Beginning POS: (VB, WRB) O Fluent Disfluent Ending POS: (VBP, WRB) Fluent to determine how you address how you weigh… to determine how you address how you weigh… TO VB WRB PRP VBP WRB PRP VBP WRB PRP VBP Word duplicate length: 2 POS duplicate length: 3 Figure 3: Span featur"
N15-1029,P94-1041,0,0.0376611,"allel substructures (Johnson and Charniak, 2004), causing researchers to employ other approaches such as pipelines of sequence models (Qian and Liu, 2013) or incremental syntactic systems (Honnibal and Johnson, 2014). Second, human processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al., 1997), so an automatic system must employ features targeting all levels of the perceptual stack to achieve high performance. In spite of this, the primary thread of work in the NLP community has focused on identifying disfluencies based only on lexicosyntactic cues (Heeman and Allen, 1994; Charniak and Johnson, 2001; Snover et al., 2004; Rasooli and Tetreault, 2013). A separate line of work has therefore attempted to build systems that leverage prosody as well as lexical information (Shriberg et al., 1997; Liu et al., 2003; Kim et al., 2004; Liu et al., 2006), though often with mixed success. In this work, we present a model for disfluency detection that improves upon model structures used in past work and leverages additional prosodic information. Our model is a semi-Markov conditional random field that distinguishes disfluent chunks (to be deleted) from fluent chunks (everyt"
N15-1029,Q14-1011,0,0.136149,"anguage in that it contains frequent disfluencies, or parts of an utterance that are corrected by the speaker. Removing these disfluencies is desirable in order to clean the input for use in downstream NLP tasks. However, automatically identifying disfluencies is challenging for a number of reasons. First, disfluencies are a syntactic phenomenon, but defy standard context-free parsing models due to their parallel substructures (Johnson and Charniak, 2004), causing researchers to employ other approaches such as pipelines of sequence models (Qian and Liu, 2013) or incremental syntactic systems (Honnibal and Johnson, 2014). Second, human processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al., 1997), so an automatic system must employ features targeting all levels of the perceptual stack to achieve high performance. In spite of this, the primary thread of work in the NLP community has focused on identifying disfluencies based only on lexicosyntactic cues (Heeman and Allen, 1994; Charniak and Johnson, 2001; Snover et al., 2004; Rasooli and Tetreault, 2013). A separate line of work has therefore attempted to build systems that leverage prosody as well as lexical informa"
N15-1029,N10-1005,0,0.145169,"stics work also enables novel prosodic features that compute pauses and word duration based on alignments to the speech signal itself, allowing the model to capture acoustic cues like pauses and hesitations that have proven useful for disfluency detection in earlier work (Shriberg et al., 1997). Such information has been exploited by NLP systems in the past via ToBI break indices (Silverman et al., 1992), a mid-level prosodic abstraction that might be indicative of disfluencies. These have been incorporated into syntactic parsers with some success (Kahn et al., 2005; Dreyer and Shafran, 2007; Huang and Harper, 2010), but we find that using features on predicted breaks is ineffective compared to directly using acoustic indicators. Our implementation of a baseline CRF model already achieves results comparable to those of a highperformance system based on pipelined inference (Qian and Liu, 2013). Our semi-CRF with span features improves on this, and adding prosodic indicators gives additional gains. Our final system gets an F-score of 85.4, which is 1.3 F1 better than the best prior reported F-score on this dataset (Honnibal and Johnson, 2014). 2 Experimental Setup Throughout this work, we make use of the S"
N15-1029,P04-1005,0,0.0972961,"signal and predicts disfluencies as complete chunks using a semi-Markov conditional random field. Introduction Spoken language is fundamentally different from written language in that it contains frequent disfluencies, or parts of an utterance that are corrected by the speaker. Removing these disfluencies is desirable in order to clean the input for use in downstream NLP tasks. However, automatically identifying disfluencies is challenging for a number of reasons. First, disfluencies are a syntactic phenomenon, but defy standard context-free parsing models due to their parallel substructures (Johnson and Charniak, 2004), causing researchers to employ other approaches such as pipelines of sequence models (Qian and Liu, 2013) or incremental syntactic systems (Honnibal and Johnson, 2014). Second, human processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al., 1997), so an automatic system must employ features targeting all levels of the perceptual stack to achieve high performance. In spite of this, the primary thread of work in the NLP community has focused on identifying disfluencies based only on lexicosyntactic cues (Heeman and Allen, 1994; Charniak and Johnson, 20"
N15-1029,H05-1030,0,0.314468,"15. 2015 Association for Computational Linguistics work also enables novel prosodic features that compute pauses and word duration based on alignments to the speech signal itself, allowing the model to capture acoustic cues like pauses and hesitations that have proven useful for disfluency detection in earlier work (Shriberg et al., 1997). Such information has been exploited by NLP systems in the past via ToBI break indices (Silverman et al., 1992), a mid-level prosodic abstraction that might be indicative of disfluencies. These have been incorporated into syntactic parsers with some success (Kahn et al., 2005; Dreyer and Shafran, 2007; Huang and Harper, 2010), but we find that using features on predicted breaks is ineffective compared to directly using acoustic indicators. Our implementation of a baseline CRF model already achieves results comparable to those of a highperformance system based on pipelined inference (Qian and Liu, 2013). Our semi-CRF with span features improves on this, and adding prosodic indicators gives additional gains. Our final system gets an F-score of 85.4, which is 1.3 F1 better than the best prior reported F-score on this dataset (Honnibal and Johnson, 2014). 2 Experiment"
N15-1029,N04-1018,0,0.0409339,"s complex and mixes acoustic and syntactic indicators (Cutler et al., 1997), so an automatic system must employ features targeting all levels of the perceptual stack to achieve high performance. In spite of this, the primary thread of work in the NLP community has focused on identifying disfluencies based only on lexicosyntactic cues (Heeman and Allen, 1994; Charniak and Johnson, 2001; Snover et al., 2004; Rasooli and Tetreault, 2013). A separate line of work has therefore attempted to build systems that leverage prosody as well as lexical information (Shriberg et al., 1997; Liu et al., 2003; Kim et al., 2004; Liu et al., 2006), though often with mixed success. In this work, we present a model for disfluency detection that improves upon model structures used in past work and leverages additional prosodic information. Our model is a semi-Markov conditional random field that distinguishes disfluent chunks (to be deleted) from fluent chunks (everything else), as shown in Figure 1. By making chunk-level predictions, we can incorporate not only standard tokenlevel features but also features that can consider the entire reparandum and the start of the repair, enabling our model to easily capture paralle"
N15-1029,P06-1055,1,0.144346,"an F-score of 85.4, which is 1.3 F1 better than the best prior reported F-score on this dataset (Honnibal and Johnson, 2014). 2 Experimental Setup Throughout this work, we make use of the Switchboard corpus using the train/test splits specified by Johnson and Charniak (2004) and used in other work. We use the provided transcripts and gold alignments between the text and the speech signal. We follow the same preprocessing regimen as past work: we remove partial words, punctuation, and capitalization to make the input more realistic.2 Finally, we use predicted POS tags from the Berkeley parser (Petrov et al., 2006) trained on Switchboard. 3 Model Past work on disfluency detection has employed CRFs to predict disfluencies using a IOBES tag set (Qian and Liu, 2013). An example of this is shown in Figure 2. One major shortcoming of this model is that beginning and ending of a disfluency are not decided jointly: because features in the CRF are local 2 As described in Honnibal and Johnson (2014), we computed features over sentences with filler words (um and uh) and the phrases I mean and you know removed. 258 to emissions and transitions, features in this model cannot recognize that a proposed disfluency beg"
N15-1029,N13-1102,0,0.517586,"ken language is fundamentally different from written language in that it contains frequent disfluencies, or parts of an utterance that are corrected by the speaker. Removing these disfluencies is desirable in order to clean the input for use in downstream NLP tasks. However, automatically identifying disfluencies is challenging for a number of reasons. First, disfluencies are a syntactic phenomenon, but defy standard context-free parsing models due to their parallel substructures (Johnson and Charniak, 2004), causing researchers to employ other approaches such as pipelines of sequence models (Qian and Liu, 2013) or incremental syntactic systems (Honnibal and Johnson, 2014). Second, human processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al., 1997), so an automatic system must employ features targeting all levels of the perceptual stack to achieve high performance. In spite of this, the primary thread of work in the NLP community has focused on identifying disfluencies based only on lexicosyntactic cues (Heeman and Allen, 1994; Charniak and Johnson, 2001; Snover et al., 2004; Rasooli and Tetreault, 2013). A separate line of work has therefore attempted to"
N15-1029,D13-1013,0,0.159911,"employ other approaches such as pipelines of sequence models (Qian and Liu, 2013) or incremental syntactic systems (Honnibal and Johnson, 2014). Second, human processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al., 1997), so an automatic system must employ features targeting all levels of the perceptual stack to achieve high performance. In spite of this, the primary thread of work in the NLP community has focused on identifying disfluencies based only on lexicosyntactic cues (Heeman and Allen, 1994; Charniak and Johnson, 2001; Snover et al., 2004; Rasooli and Tetreault, 2013). A separate line of work has therefore attempted to build systems that leverage prosody as well as lexical information (Shriberg et al., 1997; Liu et al., 2003; Kim et al., 2004; Liu et al., 2006), though often with mixed success. In this work, we present a model for disfluency detection that improves upon model structures used in past work and leverages additional prosodic information. Our model is a semi-Markov conditional random field that distinguishes disfluent chunks (to be deleted) from fluent chunks (everything else), as shown in Figure 1. By making chunk-level predictions, we can inc"
N15-1029,N04-4040,0,0.220286,"using researchers to employ other approaches such as pipelines of sequence models (Qian and Liu, 2013) or incremental syntactic systems (Honnibal and Johnson, 2014). Second, human processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al., 1997), so an automatic system must employ features targeting all levels of the perceptual stack to achieve high performance. In spite of this, the primary thread of work in the NLP community has focused on identifying disfluencies based only on lexicosyntactic cues (Heeman and Allen, 1994; Charniak and Johnson, 2001; Snover et al., 2004; Rasooli and Tetreault, 2013). A separate line of work has therefore attempted to build systems that leverage prosody as well as lexical information (Shriberg et al., 1997; Liu et al., 2003; Kim et al., 2004; Liu et al., 2006), though often with mixed success. In this work, we present a model for disfluency detection that improves upon model structures used in past work and leverages additional prosodic information. Our model is a semi-Markov conditional random field that distinguishes disfluent chunks (to be deleted) from fluent chunks (everything else), as shown in Figure 1. By making chunk"
N15-1109,P14-2020,1,0.788188,"Missing"
N15-1109,P13-1021,1,0.598859,"imilar to data used to train the LM. and where indigenous-language orthographies were first being developed (Baddeley and Voeste, 2013). The 349 digital facsimiles in the Primeros Libros collection are characteristic of this trend. Produced during the first century of Spanish colonization, they represent the introduction of printing technology into the Americas, and reflect the (sometimes conflicted) priorities of the nascent colony, from religious orthodoxy to conversion and education. For our experiments, we focus on multilingual documents in three languages: Spanish, Latin, and Nahuatl. As Berg-Kirkpatrick et al. (2013) show, a language model built on contemporaneous data will perform better than modern data. For this reason, we collected 15–17th century texts from Project Gutenberg,1 producing Spanish and Latin corpora of more than one million characters each. Due to its relative scarcity, we augmented the Nahuatl corpus with a private collection of transcribed colonial documents. 3 Baseline System The starting point for our work is the Ocular system described by Berg-Kirkpatrick et al. (2013). The fonts used in historical documents are usually unknown and can vary drastically from document to document. Ocu"
N15-1109,W14-3907,0,0.0441324,"eir modern forms. Fortunately, this can be done in our approach by running the variability rewrite rules “backward” as a post-processing step. Further technical improvements may be made by having the system automatically attempt to bootstrap the identification of spelling variants, a process that could complement our approach through an active learning setup. Additionally, since even our relatively simple unsupervised code-switch language modeling approach yielded improvements to OCR performance, it may be justified to attempt the adaptation of more complex code-switch recognition techniques (Solorio et al., 2014). The automatic transcription of the Primeros Libros collection has significant implications for scholars of the humanities interested in the role that inscription and transmission play in colonial history. For example, there are parallels between the way that the Spanish transformed indigenous languages into Latin-like writing systems (removing “noise” like phonemes that do not exist in Latin), and the way that the OCR tool transforms historical printed documents into unicode (removing “noise” like artifacts of the printing process and physical changes to the pages); in both instances, arguab"
N16-1150,D13-1184,0,0.229046,"rforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).1 1 Introduction One of the major challenges of entity linking is resolving contextually polysemous mentions. For example, Germany may refer to a nation, to that nation’s government, or even to a soccer team. Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013; Durrett and Klein, 2014). But an even simpler approach is to use context information from just the words in the source document itself to make sure the entity is being resolved sensibly in context. In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods. In this work, we model semantic similarity between a mention’s source document context and its potential entity targe"
N16-1150,D07-1074,0,0.615559,"nd fed into a final logistic regression layer (maintaining end-to-end inference and learning of the filters). When trained with backpropagation, the convolutional networks should learn to map text into vector spaces that are informative about whether the document and entity are related or not. 2.2 Integrating with a Sparse Model The dense model presented in Section 2.1 is effective at capturing semantic topic similarity, but it is most effective when combined with other signals for entity linking. An important cue for resolving a mention is the use of link counts from hyperlinks in Wikipedia (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011), which tell us how often a given mention was linked to each article on Wikipedia. This information can serve as a useful prior, but only if we can leverage it effectively by targeting the most salient part of a mention. For example, we may have never observed President Barack Obama as a linked string on Wikipedia, even though we have seen the substring Barack Obama and it unambiguously indicates the correct answer. Following Durrett and Klein (2014), we introduce a latent variable q to capture which subset of a mention (known as a query) we reso"
N16-1150,P15-1026,0,0.0377226,"df (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods. In this work, we model semantic similarity between a mention’s source document context and its potential entity targets using convolutional neural networks (CNNs). CNNs have been shown to be effective for sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Iyyer et al., 2015) and for capturing similarity in models for entity linking (Sun et al., 2015) and other related tasks (Dong et al., 2015; Shen et al., 2014), so we expect them to be effective at isolating the relevant topic semantics for entity linking. We show that convolutions over multiple granularities of the input document are useful for providing different notions of semantic context. Finally, we show how to integrate these networks with a preexisting entity linking system (Durrett and Klein, 2014). Through a combination of these two distinct methods into a single system that leverages their complementary strengths, we achieve state-ofthe-art performance across several datasets. 2 Model Our model focuses on two core idea"
N16-1150,Q14-1037,1,0.701957,"that might refer to different entities in different contexts. We present a model that uses convolutional neural networks to capture semantic correspondence between a mention’s context and a proposed target entity. These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which n-grams characterize different topics. We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).1 1 Introduction One of the major challenges of entity linking is resolving contextually polysemous mentions. For example, Germany may refer to a nation, to that nation’s government, or even to a soccer team. Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013; Durrett and Klein, 2014). But an"
N16-1150,E14-1052,0,0.0588406,"mention, as well as standard lexical, POS, and named entity type features. fE mostly captures how likely the selected query is to correspond to a given entity based on factors like anchor text counts from Wikipedia, string match with proposed Wikipedia titles, and discretized cosine similarities of tf-idf vectors (Ratinov et al., 2011). Adding tf-idf indicators is the only modification we made to the features of Durrett and Klein (2014). 3 Experimental Results We performed experiments on 4 different entity linking datasets. • ACE (NIST, 2005; Bentivogli et al., 2010): This corpus was used in Fahrni and Strube (2014) and Durrett and Klein (2014). • CoNLL-YAGO (Hoffart et al., 2011): This corpus is based on the CoNLL 2003 dataset; the test set consists of 231 news articles and contains a number of rarer entities. • WP (Heath and Bizer, 2011): This dataset consists of short snippets from Wikipedia. • Wikipedia (Ratinov et al., 2011): This dataset consists of 10,000 randomly sampled Wikipedia articles, with the task being to resolve the links in each article.3 3 We do not compare to Ratinov et al. (2011) on this dataset because we do not have access to the original Wikipedia dump they used for their work and"
N16-1150,D13-1041,0,0.0395894,"g datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).1 1 Introduction One of the major challenges of entity linking is resolving contextually polysemous mentions. For example, Germany may refer to a nation, to that nation’s government, or even to a soccer team. Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013; Durrett and Klein, 2014). But an even simpler approach is to use context information from just the words in the source document itself to make sure the entity is being resolved sensibly in context. In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods. In this work, we model semantic similarity between a mention’s source document context and its"
N16-1150,D11-1072,0,0.786132,"Missing"
N16-1150,P15-1162,0,0.0617261,"Missing"
N16-1150,P11-1115,0,0.0504631,"layer (maintaining end-to-end inference and learning of the filters). When trained with backpropagation, the convolutional networks should learn to map text into vector spaces that are informative about whether the document and entity are related or not. 2.2 Integrating with a Sparse Model The dense model presented in Section 2.1 is effective at capturing semantic topic similarity, but it is most effective when combined with other signals for entity linking. An important cue for resolving a mention is the use of link counts from hyperlinks in Wikipedia (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011), which tell us how often a given mention was linked to each article on Wikipedia. This information can serve as a useful prior, but only if we can leverage it effectively by targeting the most salient part of a mention. For example, we may have never observed President Barack Obama as a linked string on Wikipedia, even though we have seen the substring Barack Obama and it unambiguously indicates the correct answer. Following Durrett and Klein (2014), we introduce a latent variable q to capture which subset of a mention (known as a query) we resolve. Query generation includes potentially remov"
N16-1150,P14-1062,0,0.0334107,"e document itself to make sure the entity is being resolved sensibly in context. In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods. In this work, we model semantic similarity between a mention’s source document context and its potential entity targets using convolutional neural networks (CNNs). CNNs have been shown to be effective for sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Iyyer et al., 2015) and for capturing similarity in models for entity linking (Sun et al., 2015) and other related tasks (Dong et al., 2015; Shen et al., 2014), so we expect them to be effective at isolating the relevant topic semantics for entity linking. We show that convolutions over multiple granularities of the input document are useful for providing different notions of semantic context. Finally, we show how to integrate these networks with a preexisting entity linking system (Durrett and Klein, 2014). Through a combination of these two distinct methods into a single system"
N16-1150,D14-1181,0,0.00426049,"ure the entity is being resolved sensibly in context. In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods. In this work, we model semantic similarity between a mention’s source document context and its potential entity targets using convolutional neural networks (CNNs). CNNs have been shown to be effective for sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Iyyer et al., 2015) and for capturing similarity in models for entity linking (Sun et al., 2015) and other related tasks (Dong et al., 2015; Shen et al., 2014), so we expect them to be effective at isolating the relevant topic semantics for entity linking. We show that convolutions over multiple granularities of the input document are useful for providing different notions of semantic context. Finally, we show how to integrate these networks with a preexisting entity linking system (Durrett and Klein, 2014). Through a combination of these two distinct methods into a single system that levera"
N16-1150,P14-2050,0,0.0529462,"ectors derived from Google News vs. Wikipedia on development sets for each corpus. 3.2 Embedding Vectors We also explored two different sources of embedding vectors for the convolutions. Table 4 shows that word vectors trained on Wikipedia outperformed Google News word vectors trained on a larger corpus. Further investigation revealed that the Google News vectors had much higher out-of-vocabulary rates. For learning the vectors, we use the standard word2vec toolkit (Mikolov et al., 2013) with vector length set to 300, window set to 21 (larger windows produce more semantically-focused vectors (Levy and Goldberg, 2014)), 10 negative samples and 10 iterations through Wikipedia. We do not fine-tune word vectors during training of our model, as that was not found to improve performance. 3.3 Analysis of Learned Convolutions One downside of our system compared to its purely indicator-based variant is that its operation is less interpretable. However, one way we can inspect the learned system is by examining what causes high activations of the various convolutional filters (rows of the matrices Mg from Equation 1). Table 3 shows the n-grams in the ACE dataset leading to maximal activations of three of the filters"
N16-1150,P11-1138,0,0.931799,"he-art performance on multiple entity linking datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).1 1 Introduction One of the major challenges of entity linking is resolving contextually polysemous mentions. For example, Germany may refer to a nation, to that nation’s government, or even to a soccer team. Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013; Durrett and Klein, 2014). But an even simpler approach is to use context information from just the words in the source document itself to make sure the entity is being resolved sensibly in context. In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods. In this work, we model semantic similarity between a men"
N16-1181,P13-2009,1,0.563292,"okup[Georgia])) In this paper, we present a model for learning to select such structures from a set of automatically generated candidates. We call this model a dynamic neural module network. 2 But note that unlike formal semantics, the behavior of the primitive functions here is itself unknown. 3 Related work There is an extensive literature on database question answering, in which strings are mapped to logical forms, then evaluated by a black-box execution model to produce answers. Supervision may be provided either by annotated logical forms (Wong and Mooney, 2007; Kwiatkowski et al., 2010; Andreas et al., 2013) or from (world, question, answer) triples alone (Liang et al., 2011; Pasupat and Liang, 2015). In general the set of primitive functions from which these logical forms can be assembled is fixed, but one recent line of work focuses on inducing new predicates functions automatically, either from perceptual features (Krishnamurthy and Kollar, 2013) or the underlying schema (Kwiatkowski et al., 2013). The model we describe in this paper has a unified framework for handling both the perceptual and schema cases, and differs from existing work primarily in learning a differentiable execution model w"
N16-1181,P14-1133,0,0.0158203,"le in each layout is a describe or exists module, the fully- instantiated network corresponds to a distribution over labels P conditioned on layouts. To train, we maximize (w,y,z) log pz (y|w; θe ) directly. This can be understood as a parameter-tying scheme, where the decisions about which parameters to tie are governed by the observed layouts z. 4.2 Assembling networks Next we describe the layout model p(z|x; θ` ). We first use a fixed syntactic parse to generate a small set of candidate layouts, analogously to the way a semantic grammar generates candidate semantic parses in previous work (Berant and Liang, 2014). A semantic parse differs from a syntactic parse in two primary ways. First, lexical items must be 1549 mapped onto a (possibly smaller) set of semantic primitives. Second, these semantic primitives must be combined into a structure that closely, but not exactly, parallels the structure provided by syntax. For example, state and province might need to be identified with the same field in a database schema, while all states have a capital might need to be identified with the correct (in situ) quantifier scope. While we cannot avoid the structure selection problem, continuous representations si"
N16-1181,D14-1067,0,0.00959861,"al features (Krishnamurthy and Kollar, 2013) or the underlying schema (Kwiatkowski et al., 2013). The model we describe in this paper has a unified framework for handling both the perceptual and schema cases, and differs from existing work primarily in learning a differentiable execution model with continuous evaluation results. Neural models for question answering are also a subject of current interest. These include approaches that model the task directly as a multiclass classification problem (Iyyer et al., 2014), models that attempt to embed questions and answers in a shared vector space (Bordes et al., 2014) and attentional models that select words from documents sources (Hermann et al., 2015). Such approaches generally require that answers can be retrieved directly based on surface linguistic features, without requiring intermediate computation. A more structured approach described by Yin et al. (2015) learns a query execution model for database tables without any natural language component. Previous efforts toward unifying formal logic and representation learning include those of Grefenstette (2013) and Krishnamurthy and Mitchell (2013). The visually-grounded component of this work relies on re"
N16-1181,W08-1301,0,0.0131897,"Missing"
N16-1181,S13-1001,0,0.0241993,"yer et al., 2014), models that attempt to embed questions and answers in a shared vector space (Bordes et al., 2014) and attentional models that select words from documents sources (Hermann et al., 2015). Such approaches generally require that answers can be retrieved directly based on surface linguistic features, without requiring intermediate computation. A more structured approach described by Yin et al. (2015) learns a query execution model for database tables without any natural language component. Previous efforts toward unifying formal logic and representation learning include those of Grefenstette (2013) and Krishnamurthy and Mitchell (2013). The visually-grounded component of this work relies on recent advances in convolutional networks for computer vision (Simonyan and Zisserman, 2014), and in particular the fact that late convolutional layers in networks trained for image recognition contain rich features useful for other downstream vision tasks, while preserving spatial information. These features have been used for both image captioning (Xu et al., 2015) and visual question answering (Yang et al., 2015). 1547 Most previous approaches to visual question answering either apply a recurrent"
N16-1181,D14-1070,0,0.0537942,"Missing"
N16-1181,Q13-1016,0,0.0265387,"ature on database question answering, in which strings are mapped to logical forms, then evaluated by a black-box execution model to produce answers. Supervision may be provided either by annotated logical forms (Wong and Mooney, 2007; Kwiatkowski et al., 2010; Andreas et al., 2013) or from (world, question, answer) triples alone (Liang et al., 2011; Pasupat and Liang, 2015). In general the set of primitive functions from which these logical forms can be assembled is fixed, but one recent line of work focuses on inducing new predicates functions automatically, either from perceptual features (Krishnamurthy and Kollar, 2013) or the underlying schema (Kwiatkowski et al., 2013). The model we describe in this paper has a unified framework for handling both the perceptual and schema cases, and differs from existing work primarily in learning a differentiable execution model with continuous evaluation results. Neural models for question answering are also a subject of current interest. These include approaches that model the task directly as a multiclass classification problem (Iyyer et al., 2014), models that attempt to embed questions and answers in a shared vector space (Bordes et al., 2014) and attentional models"
N16-1181,W13-3201,0,0.0138184,"s that attempt to embed questions and answers in a shared vector space (Bordes et al., 2014) and attentional models that select words from documents sources (Hermann et al., 2015). Such approaches generally require that answers can be retrieved directly based on surface linguistic features, without requiring intermediate computation. A more structured approach described by Yin et al. (2015) learns a query execution model for database tables without any natural language component. Previous efforts toward unifying formal logic and representation learning include those of Grefenstette (2013) and Krishnamurthy and Mitchell (2013). The visually-grounded component of this work relies on recent advances in convolutional networks for computer vision (Simonyan and Zisserman, 2014), and in particular the fact that late convolutional layers in networks trained for image recognition contain rich features useful for other downstream vision tasks, while preserving spatial information. These features have been used for both image captioning (Xu et al., 2015) and visual question answering (Yang et al., 2015). 1547 Most previous approaches to visual question answering either apply a recurrent model to deep representations of both"
N16-1181,D10-1119,0,0.00620017,"find[city] (relate[in] lookup[Georgia])) In this paper, we present a model for learning to select such structures from a set of automatically generated candidates. We call this model a dynamic neural module network. 2 But note that unlike formal semantics, the behavior of the primitive functions here is itself unknown. 3 Related work There is an extensive literature on database question answering, in which strings are mapped to logical forms, then evaluated by a black-box execution model to produce answers. Supervision may be provided either by annotated logical forms (Wong and Mooney, 2007; Kwiatkowski et al., 2010; Andreas et al., 2013) or from (world, question, answer) triples alone (Liang et al., 2011; Pasupat and Liang, 2015). In general the set of primitive functions from which these logical forms can be assembled is fixed, but one recent line of work focuses on inducing new predicates functions automatically, either from perceptual features (Krishnamurthy and Kollar, 2013) or the underlying schema (Kwiatkowski et al., 2013). The model we describe in this paper has a unified framework for handling both the perceptual and schema cases, and differs from existing work primarily in learning a different"
N16-1181,D13-1161,0,0.00883107,"mapped to logical forms, then evaluated by a black-box execution model to produce answers. Supervision may be provided either by annotated logical forms (Wong and Mooney, 2007; Kwiatkowski et al., 2010; Andreas et al., 2013) or from (world, question, answer) triples alone (Liang et al., 2011; Pasupat and Liang, 2015). In general the set of primitive functions from which these logical forms can be assembled is fixed, but one recent line of work focuses on inducing new predicates functions automatically, either from perceptual features (Krishnamurthy and Kollar, 2013) or the underlying schema (Kwiatkowski et al., 2013). The model we describe in this paper has a unified framework for handling both the perceptual and schema cases, and differs from existing work primarily in learning a differentiable execution model with continuous evaluation results. Neural models for question answering are also a subject of current interest. These include approaches that model the task directly as a multiclass classification problem (Iyyer et al., 2014), models that attempt to embed questions and answers in a shared vector space (Bordes et al., 2014) and attentional models that select words from documents sources (Hermann et"
N16-1181,P11-1060,1,0.843642,"e 2: Simple neural module networks, corresponding to the questions What color is the bird? and Are there any states? (a) A neural find module for computing an attention over pixels. (b) The same operation applied to a knowledge base. (c) Using an attention produced by a lower module to identify the color of the region of the image attended to. (d) Performing quantification by evaluating an attention directly. works are required. Thus our goal is to automatically induce variable-free, tree-structured computation descriptors. We can use a familiar functional notation from formal semantics (e.g. Liang et al., 2011) to represent these computations.2 We write the two examples in Figure 2 as (describe[color] find[bird]) and (exists find[state]) respectively. These are network layouts: they specify a structure for arranging modules (and their lexical parameters) into a complete network. Andreas et al. (2016) use hand-written rules to deterministically transform dependency trees into layouts, and are restricted to producing simple structures like the above for non-synthetic data. For full generality, we will need to solve harder problems, like transforming What cities are in Georgia? (Figure 1) into (and fin"
N16-1181,P15-1142,0,0.0531041,"om a set of automatically generated candidates. We call this model a dynamic neural module network. 2 But note that unlike formal semantics, the behavior of the primitive functions here is itself unknown. 3 Related work There is an extensive literature on database question answering, in which strings are mapped to logical forms, then evaluated by a black-box execution model to produce answers. Supervision may be provided either by annotated logical forms (Wong and Mooney, 2007; Kwiatkowski et al., 2010; Andreas et al., 2013) or from (world, question, answer) triples alone (Liang et al., 2011; Pasupat and Liang, 2015). In general the set of primitive functions from which these logical forms can be assembled is fixed, but one recent line of work focuses on inducing new predicates functions automatically, either from perceptual features (Krishnamurthy and Kollar, 2013) or the underlying schema (Kwiatkowski et al., 2013). The model we describe in this paper has a unified framework for handling both the perceptual and schema cases, and differs from existing work primarily in learning a differentiable execution model with continuous evaluation results. Neural models for question answering are also a subject of"
N16-1181,P13-1045,0,0.00900756,"015). All of these models assume that a fixed computation can be performed on the image and question to compute the answer, rather than adapting the structure of the computation to the question. As noted, Andreas et al. (2016) previously considered a simple generalization of these attentional approaches in which small variations in the network structure per-question were permitted, with the structure chosen by (deterministic) syntactic processing of questions. Other approaches in this general family include the “universal parser” sketched by Bottou (2014), and the recursive neural networks of Socher et al. (2013), which use a fixed tree structure to perform further linguistic analysis without any external world representation. We are unaware of previous work that succeeds in simultaneously learning both the parameters for and structures of instance-specific neural networks. 4 Model Recall that our goal is to map from questions and world representations to answers. This process involves the following variables: 1. 2. 3. 4. 5. w a world representation x a question y an answer z a network layout θ a collection of model parameters Our model is built around two distributions: a layout model p(z|x; θ` ) whi"
N16-1181,P07-1121,0,0.0105895,"a? (Figure 1) into (and find[city] (relate[in] lookup[Georgia])) In this paper, we present a model for learning to select such structures from a set of automatically generated candidates. We call this model a dynamic neural module network. 2 But note that unlike formal semantics, the behavior of the primitive functions here is itself unknown. 3 Related work There is an extensive literature on database question answering, in which strings are mapped to logical forms, then evaluated by a black-box execution model to produce answers. Supervision may be provided either by annotated logical forms (Wong and Mooney, 2007; Kwiatkowski et al., 2010; Andreas et al., 2013) or from (world, question, answer) triples alone (Liang et al., 2011; Pasupat and Liang, 2015). In general the set of primitive functions from which these logical forms can be assembled is fixed, but one recent line of work focuses on inducing new predicates functions automatically, either from perceptual features (Krishnamurthy and Kollar, 2013) or the underlying schema (Kwiatkowski et al., 2013). The model we describe in this paper has a unified framework for handling both the perceptual and schema cases, and differs from existing work primari"
N16-1181,S13-1002,0,\N,Missing
N16-1181,Q13-1015,0,\N,Missing
N18-1091,N16-1024,0,0.476304,"er of the paper is focused on analysis. In Section 3, we look at the decline of grammars and output correlations. Past work in constituency parsing used context-free grammars with production rules governing adjacent labels (or more generally production-factored scores) to propagate information and capture correlations between output decisions (Collins, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007; Hall et al., 2014). Many recent parsers no longer have explicit grammar production rules, but still use information about other predictions, allowing them to capture output correlations (Dyer et al., 2016; Choe and Charniak, 2016). Beyond this, there are some parsers that use no context for bracket scoring and only include mild output correlations in the form of tree constraints (Cross and Huang, 2016b; Stern et al., 2017). In our experiments, we find that we can accurately predict parents from the representation given to a child. Since a simple classifier can predict the information provided by parent-child relations, this explains why the information no longer needs to be specified explicitly. We also show that we can completely remove output correlations from our model with a variant of our"
N18-1091,P08-1109,0,0.289169,"Missing"
N18-1091,D15-1176,0,0.23044,"mance. Our parser is a natural extension of recent work in constituency parsing. We combine a common 999 Proceedings of NAACL-HLT 2018, pages 999–1010 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics tom lexical representations, such as word shape features, prefixes, suffixes, and special tokens for categories like numerals (Klein and Manning, 2003; Petrov and Klein, 2007; Finkel et al., 2008). Character-level models have shown promise in parsing and other NLP tasks as a way to remove the complexity of these lexical features (Ballesteros et al., 2015; Ling et al., 2015b; Kim et al., 2016; Coavoux and Crabb´e, 2017; Liu and Zhang, 2017). We compare the performance of characterlevel representations and externally predicted partof-speech tags and show that these two sources of information seem to fill a similar role. We also perform experiments showing that the representations learned with character-level models contain information that was hand-specified in some other models. Finally, in Section 5 we look at the surface context captured by recurrent neural networks. Many recent parsers use LSTMs, a popular type of recurrent neural network, to combine and summ"
N18-1091,P14-1022,1,0.954183,"ection 23 of the Penn Treebank, exceeding the performance of many other state-of-the-art models evaluated under comparable conditions. Section 2 describes our model in detail. The remainder of the paper is focused on analysis. In Section 3, we look at the decline of grammars and output correlations. Past work in constituency parsing used context-free grammars with production rules governing adjacent labels (or more generally production-factored scores) to propagate information and capture correlations between output decisions (Collins, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007; Hall et al., 2014). Many recent parsers no longer have explicit grammar production rules, but still use information about other predictions, allowing them to capture output correlations (Dyer et al., 2016; Choe and Charniak, 2016). Beyond this, there are some parsers that use no context for bracket scoring and only include mild output correlations in the form of tree constraints (Cross and Huang, 2016b; Stern et al., 2017). In our experiments, we find that we can accurately predict parents from the representation given to a child. Since a simple classifier can predict the information provided by parent-child re"
N18-1091,Q16-1037,0,0.0830984,"Missing"
N18-1091,Q17-1004,0,0.0765756,"tuency parsing. We combine a common 999 Proceedings of NAACL-HLT 2018, pages 999–1010 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics tom lexical representations, such as word shape features, prefixes, suffixes, and special tokens for categories like numerals (Klein and Manning, 2003; Petrov and Klein, 2007; Finkel et al., 2008). Character-level models have shown promise in parsing and other NLP tasks as a way to remove the complexity of these lexical features (Ballesteros et al., 2015; Ling et al., 2015b; Kim et al., 2016; Coavoux and Crabb´e, 2017; Liu and Zhang, 2017). We compare the performance of characterlevel representations and externally predicted partof-speech tags and show that these two sources of information seem to fill a similar role. We also perform experiments showing that the representations learned with character-level models contain information that was hand-specified in some other models. Finally, in Section 5 we look at the surface context captured by recurrent neural networks. Many recent parsers use LSTMs, a popular type of recurrent neural network, to combine and summarize context for making decisions (Choe and Charniak, 2016; Cross a"
N18-1091,J93-2004,0,0.0637763,"mputed by running a bidirectional LSTM over the input sentence and taking differences of the output vectors at the two endpoints. Here we illustrate the process for the span (1, 4) corresponding to “played soccer in” in the example sentence. Since ∆ decomposes over spans, the inner lossaugmented decode maxT [s(T ) + ∆(T, T ∗ )] can be performed efficiently using a slight modification of the dynamic program used for inference. In particular, we replace s(i, j, `) with s(i, j, `) + 1[` 6= `∗ij ], where `∗ij is the label of span (i, j) in the gold tree T ∗ . 2.7 Results We use the Penn Treebank (Marcus et al., 1993) for our experiments with the standard splits of sections 2-21 for training, section 22 for development, and section 23 for testing. Details about our model hyperparameters and training prodecure can be found in Appendix A. Across 10 trials, our model achieves an average development F1 score of 92.22 on section 22 of the Penn Treebank. We use this as our primary point of comparison in all subsequent analysis. The model with the best score on the development set achieves a test F1 score of 92.08 on section 23 of the Penn Treebank, exceeding the performance of other recent state-of-the-art discr"
N18-1091,P03-1054,1,0.0678126,"tures used in past models. To gain insight, we introduce a parser that is representative of recent trends and analyze its learned representations to determine what information it captures and what is important for its strong performance. Our parser is a natural extension of recent work in constituency parsing. We combine a common 999 Proceedings of NAACL-HLT 2018, pages 999–1010 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics tom lexical representations, such as word shape features, prefixes, suffixes, and special tokens for categories like numerals (Klein and Manning, 2003; Petrov and Klein, 2007; Finkel et al., 2008). Character-level models have shown promise in parsing and other NLP tasks as a way to remove the complexity of these lexical features (Ballesteros et al., 2015; Ling et al., 2015b; Kim et al., 2016; Coavoux and Crabb´e, 2017; Liu and Zhang, 2017). We compare the performance of characterlevel representations and externally predicted partof-speech tags and show that these two sources of information seem to fill a similar role. We also perform experiments showing that the representations learned with character-level models contain information that wa"
N18-1091,P10-1001,0,0.0137313,"e development set achieves a test F1 score of 92.08 on section 23 of the Penn Treebank, exceeding the performance of other recent state-of-the-art discriminative models which do not use external data or ensembling.1 3 Output Correlations Output correlations are information about compatibility between outputs in a structured prediction model. Since outputs are all a function of the input, output correlations are not necessary for prediction when a model has access to the entire input. In practice, however, many models throughout NLP have found them useful (Collins, 1997; Lafferty et al., 2001; Koo and Collins, 2010), and 1 Code for our parser is available at https://github. com/dgaddy/parser-analysis. Liang et al. (2008) provides theoretical results suggesting they may be useful for learning efficiently. In constituency parsing, there are two primary forms of output correlation typically captured by models. The first is correlations between label decisions, which often are captured by either production scores or the history in an incremental treecreation procedure. The second, more subtle correlation comes from the enforcement of tree constraints, since the inclusion of one bracket can affect whether or"
N18-1091,N16-1030,0,0.049707,"dition to word embeddings, character-level representations have also been gaining traction in recent years, with common choices including recurrent, convolutional, or bag-of-n-gram representations. These alleviate the unknown word problem by working with smaller, more frequent units, and readily capture morphological information not directly accessible through word embeddings. Character LSTMs in particular have proved useful in constituency parsing (Coavoux and Crabb´e, 2017), dependency parsing (Ballesteros et al., 2015), part-of-speech tagging (Ling et al., 2015a), named entity recognition (Lample et al., 2016), and machine translation (Ling et al., 2015b), making them a natural choice for our system. We obtain a character-level representation for a word by running it through a bidirectional character LSTM and concatenating the final forward and backward outputs. 1000 The complete representation of a given word is the concatenation of its word embedding and its character LSTM representation. While past work has also used sparse indicator features (Finkel et al., 2008) or part-of-speech tags predicted by an external system (Cross and Huang, 2016b) for additional word-level information, we find these"
N18-1091,N07-1051,1,0.758355,"t F1 score of 92.08 on section 23 of the Penn Treebank, exceeding the performance of many other state-of-the-art models evaluated under comparable conditions. Section 2 describes our model in detail. The remainder of the paper is focused on analysis. In Section 3, we look at the decline of grammars and output correlations. Past work in constituency parsing used context-free grammars with production rules governing adjacent labels (or more generally production-factored scores) to propagate information and capture correlations between output decisions (Collins, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007; Hall et al., 2014). Many recent parsers no longer have explicit grammar production rules, but still use information about other predictions, allowing them to capture output correlations (Dyer et al., 2016; Choe and Charniak, 2016). Beyond this, there are some parsers that use no context for bracket scoring and only include mild output correlations in the form of tree constraints (Cross and Huang, 2016b; Stern et al., 2017). In our experiments, we find that we can accurately predict parents from the representation given to a child. Since a simple classifier can predict the information provide"
N18-1091,D16-1159,0,0.0700171,"Missing"
N18-1091,P17-1076,1,0.846269,"ls (or more generally production-factored scores) to propagate information and capture correlations between output decisions (Collins, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007; Hall et al., 2014). Many recent parsers no longer have explicit grammar production rules, but still use information about other predictions, allowing them to capture output correlations (Dyer et al., 2016; Choe and Charniak, 2016). Beyond this, there are some parsers that use no context for bracket scoring and only include mild output correlations in the form of tree constraints (Cross and Huang, 2016b; Stern et al., 2017). In our experiments, we find that we can accurately predict parents from the representation given to a child. Since a simple classifier can predict the information provided by parent-child relations, this explains why the information no longer needs to be specified explicitly. We also show that we can completely remove output correlations from our model with a variant of our parser that makes independent span label decisions without any tree constraints while maintaining high F1 scores and mostly producing trees. In Section 4, we look at lexical representations. In the past, parsers used a va"
N18-1091,N03-1033,1,0.0682566,"h uses word embeddings and a character LSTM, with otherwise identical parsers that use other combinations of lexical representations. The results of these experiments are summarized in Table 1. First, we remove the character-level representations from our model, leaving only the word embeddings. We find that development performance drops from 92.22 F1 to 91.44 F1, showing that word embeddings alone do not capture sufficient information for state-of-the-art performance. Then, we replace the character-level representations with embeddings of part-of-speech tags predicted by the Stanford tagger (Toutanova et al., 2003). This model achieves a comparable development F1 score of 92.09, but unlike our base model relies on outputs from an external system. Next, we train a model which includes all three lexical representations: word embeddings, character LSTM representations, and part-of-speech tag embeddings. We find that development performance is nearly identical to the base model at 92.24 F1, suggesting that character representations and predicted part-of-speech tags provide much of the same information. Finally, we remove word embeddings and rely completely on character-level embeddings. After retuning the c"
N18-1091,P16-1218,0,0.0516039,"LSTM representation. While past work has also used sparse indicator features (Finkel et al., 2008) or part-of-speech tags predicted by an external system (Cross and Huang, 2016b) for additional word-level information, we find these to be unnecessary in the presence of a robust character-level representation. 2.3 Span Representation To build up to spans, we first run a bidirectional LSTM over the sequence of word representations for an input sentence to obtain context-sensitive forward and backward representations fi and bi for each fencepost i. We then follow past work in dependency parsing (Wang and Chang, 2016) and constituency parsing (Cross and Huang, 2016b; Stern et al., 2017) in representing the span (i, j) by the concatenation of the corresponding forward and backward span differences: rij = [fj − fi , bi − bj ]. See Figure 1 for an illustration. 2.4 Label Scoring Finally, we implement the label scoring function by feeding the span representation through a onelayer feedforward network whose output dimensionality equals the number of possible labels. The score of a specific label ` is the corresponding component of the output vector: s(i, j, `) = [W2 g(W1 rij + z1 ) + z2 ]` , sbest (i, i + 1) ="
N18-1091,E17-1063,0,0.0395559,"Missing"
N18-1091,W17-7534,0,\N,Missing
N18-1177,D10-1040,1,0.644454,"ch in this paper builds upon long lines of work in pragmatic modeling, instruction following, and instruction generation. 1 Source code is available at http://github.com/ dpfried/pragmatic-instructions 1952 Pragmatics Our approach to pragmatics (Grice, 1975) belongs to a general category of rational speech acts models (Frank and Goodman, 2012), in which the interaction between speakers and listeners is modeled as a probabilistic process with Bayesian actors (Goodman and Stuhlm¨uller, 2013). Alternative formulations (e.g. with bestresponse rather than probabilistic dynamics) are also possible (Golland et al., 2010). Inference in these models is challenging even when the space of listener actions is extremely simple (Smith et al., 2013), and one of our goals in the present work is to show how this inference problem can be solved even in much richer action spaces than previously considered in computational pragmatics. This family of pragmatic models captures a number of important linguistic phenomena, especially those involving conversational implicature (Monroe and Potts, 2015); we note that many other topics studied under the broad heading of “pragmatics,” including presupposition and indexicality, requ"
N18-1177,P17-1097,0,0.0921474,"ar inference procedure for a competitive negotiation task. The language learning model of Wang et al. (2016) also features a structured output space and uses pragmatics to improve online predictions for a semantic parsing model. Our approach in this paper performs both generation and interpretation, and investigates both structured and unstructured output representations. Instruction following Work on instruction following tasks includes models that parse commands into structured representations processed by a rich execution model (Tellex et al., 2011; Chen, 2012; Artzi and Zettlemoyer, 2013; Guu et al., 2017), and models that map directly from instructions to a policy over primitive actions (Branavan et al., 2009), possibly mediated by an intermediate alignment or attention variable (Andreas and Klein, 2015; Mei et al., 2016). We use a model similar to Mei et al. (2016) as our base listener in this paper, evaluating on the SAIL navigation task (MacMahon et al., 2006) as they did, as well as the SCONE context-dependent execution domains (Long et al., 2016). Instruction generation Previous work has also investigated the instruction generation task, in particular for navigational directions. The GIVE"
N18-1177,D14-1086,0,0.0299081,"of pragmatic models captures a number of important linguistic phenomena, especially those involving conversational implicature (Monroe and Potts, 2015); we note that many other topics studied under the broad heading of “pragmatics,” including presupposition and indexicality, require different machinery. Williams et al. (2015) use pragmatic reasoning with weighted inference rules to resolve ambiguity and generate clarification requests in a humanrobot dialog task. Other recent work on pragmatic models focuses on the referring expression generation or “contrastive captioning” task introduced by Kazemzadeh et al. (2014). In this family are approaches that model the listener at training time (Mao et al., 2016), at evaluation time (Andreas and Klein, 2016; Monroe et al., 2017; Vedantam et al., 2017; Su et al., 2017) or both (Yu et al., 2017b; Luo and Shakhnarovich, 2017). Other conditional sequence rescoring models that are structurally similar but motivated by concerns other than pragmatics include Li et al. (2016) and Yu et al. (2017a). Lewis et al. (2017) perform a similar inference procedure for a competitive negotiation task. The language learning model of Wang et al. (2016) also features a structured out"
N18-1177,W10-4233,0,0.0192443,"instructions to a policy over primitive actions (Branavan et al., 2009), possibly mediated by an intermediate alignment or attention variable (Andreas and Klein, 2015; Mei et al., 2016). We use a model similar to Mei et al. (2016) as our base listener in this paper, evaluating on the SAIL navigation task (MacMahon et al., 2006) as they did, as well as the SCONE context-dependent execution domains (Long et al., 2016). Instruction generation Previous work has also investigated the instruction generation task, in particular for navigational directions. The GIVE shared tasks (Byron et al., 2009; Koller et al., 2010; Striegnitz et al., 2011) have produced a large number of interactive direction-giving systems, both rule-based and learned. The work most immediately related to the generation task in this paper is that of Daniele et al. (2017), which also focuses on the SAIL dataset but requires substantial additional structured annotation for training, while both our base and pragmatic speaker models learn directly from strings and action sequences. Older work has studied the properties of effective human strategies for generating navigational directions (Anderson et al., 1991). Instructions of this kind c"
N18-1177,P02-1040,0,0.114507,"Missing"
N18-1177,D17-1259,0,0.0158321,"obot dialog task. Other recent work on pragmatic models focuses on the referring expression generation or “contrastive captioning” task introduced by Kazemzadeh et al. (2014). In this family are approaches that model the listener at training time (Mao et al., 2016), at evaluation time (Andreas and Klein, 2016; Monroe et al., 2017; Vedantam et al., 2017; Su et al., 2017) or both (Yu et al., 2017b; Luo and Shakhnarovich, 2017). Other conditional sequence rescoring models that are structurally similar but motivated by concerns other than pragmatics include Li et al. (2016) and Yu et al. (2017a). Lewis et al. (2017) perform a similar inference procedure for a competitive negotiation task. The language learning model of Wang et al. (2016) also features a structured output space and uses pragmatics to improve online predictions for a semantic parsing model. Our approach in this paper performs both generation and interpretation, and investigates both structured and unstructured output representations. Instruction following Work on instruction following tasks includes models that parse commands into structured representations processed by a rich execution model (Tellex et al., 2011; Chen, 2012; Artzi and Zet"
N18-1177,N16-1014,0,0.00569052,"erate clarification requests in a humanrobot dialog task. Other recent work on pragmatic models focuses on the referring expression generation or “contrastive captioning” task introduced by Kazemzadeh et al. (2014). In this family are approaches that model the listener at training time (Mao et al., 2016), at evaluation time (Andreas and Klein, 2016; Monroe et al., 2017; Vedantam et al., 2017; Su et al., 2017) or both (Yu et al., 2017b; Luo and Shakhnarovich, 2017). Other conditional sequence rescoring models that are structurally similar but motivated by concerns other than pragmatics include Li et al. (2016) and Yu et al. (2017a). Lewis et al. (2017) perform a similar inference procedure for a competitive negotiation task. The language learning model of Wang et al. (2016) also features a structured output space and uses pragmatics to improve online predictions for a semantic parsing model. Our approach in this paper performs both generation and interpretation, and investigates both structured and unstructured output representations. Instruction following Work on instruction following tasks includes models that parse commands into structured representations processed by a rich execution model (Tel"
N18-1177,P16-1138,0,0.0339462,"and must generate a sequence of direction sentences d1 , . . . dK describing the actions. The agent succeeds if a human listener is able to correctly follow those directions to the intended final state. We evaluate models for both tasks in four domains. The first domain is the SAIL corpus of virtual environments and navigational directions (MacMahon et al., 2006; Chen and Mooney, 2011), where an agent navigates through a twodimensional grid of hallways with patterned walls and floors and a discrete set of objects (Figure 1 shows a portion of one of these hallways). In the three SCONE domains (Long et al., 2016), the world contains a number of objects with various properties, such as colored beakers which an agent can combine, drain, and mix. Instructions describe how these objects should be manipulated. These domains were designed to elicit instructions with a variety of context-dependent language phenomena, including ellipsis and coreference (Long et al., 2016) which we might expect a model of pragmatics to help resolve (Potts, 2011). 3 Related Work The approach in this paper builds upon long lines of work in pragmatic modeling, instruction following, and instruction generation. 1 Source code is av"
N18-1177,W11-2845,0,0.0242438,"licy over primitive actions (Branavan et al., 2009), possibly mediated by an intermediate alignment or attention variable (Andreas and Klein, 2015; Mei et al., 2016). We use a model similar to Mei et al. (2016) as our base listener in this paper, evaluating on the SAIL navigation task (MacMahon et al., 2006) as they did, as well as the SCONE context-dependent execution domains (Long et al., 2016). Instruction generation Previous work has also investigated the instruction generation task, in particular for navigational directions. The GIVE shared tasks (Byron et al., 2009; Koller et al., 2010; Striegnitz et al., 2011) have produced a large number of interactive direction-giving systems, both rule-based and learned. The work most immediately related to the generation task in this paper is that of Daniele et al. (2017), which also focuses on the SAIL dataset but requires substantial additional structured annotation for training, while both our base and pragmatic speaker models learn directly from strings and action sequences. Older work has studied the properties of effective human strategies for generating navigational directions (Anderson et al., 1991). Instructions of this kind can be used to extract temp"
N18-1177,P16-1224,0,0.129739,"ning” task introduced by Kazemzadeh et al. (2014). In this family are approaches that model the listener at training time (Mao et al., 2016), at evaluation time (Andreas and Klein, 2016; Monroe et al., 2017; Vedantam et al., 2017; Su et al., 2017) or both (Yu et al., 2017b; Luo and Shakhnarovich, 2017). Other conditional sequence rescoring models that are structurally similar but motivated by concerns other than pragmatics include Li et al. (2016) and Yu et al. (2017a). Lewis et al. (2017) perform a similar inference procedure for a competitive negotiation task. The language learning model of Wang et al. (2016) also features a structured output space and uses pragmatics to improve online predictions for a semantic parsing model. Our approach in this paper performs both generation and interpretation, and investigates both structured and unstructured output representations. Instruction following Work on instruction following tasks includes models that parse commands into structured representations processed by a rich execution model (Tellex et al., 2011; Chen, 2012; Artzi and Zettlemoyer, 2013; Guu et al., 2017), and models that map directly from instructions to a policy over primitive actions (Branav"
N18-1197,D11-1140,0,0.0642625,"Missing"
N18-1197,P17-1015,0,0.0345304,"interesting and meaningful behaviors. 7 Other Related Work This is the first approach we are aware of to frame a general learning problem as optimization over a space of natural language strings. However, many closely related ideas have been explored in the literature. String-valued latent variables are widely used in language processing tasks ranging from morphological analysis (Dreyer and Eisner, 2009) to sentence compression (Miao and Blunsom, 2016). Natural language annotations have been used in conjunction with training examples to guide the discovery of logical descriptions of concepts (Ling et al., 2017; Srivastava et al., 2017), and used as an auxiliary loss for training (Frome et al., 2013), analogously to the Meta+Joint baseline in this paper. Structured language-like annotations have been used to improve learning of generalizable structured policies (Oh et al., 2017; Andreas et al., 2017; Denil et al., 2017). Finally, natural language instructions available at concept-learning time (rather than language-learning time) have been used to provide side information to reinforcement learners about high-level strategy (Branavan et al., 2011), environments (Narasimhan et al., 2017) and explorati"
N18-1197,D16-1031,0,0.0198,"that the model has used the structure provided by language to learn a better representation space for policies— one that facilitates sampling from a distribution over interesting and meaningful behaviors. 7 Other Related Work This is the first approach we are aware of to frame a general learning problem as optimization over a space of natural language strings. However, many closely related ideas have been explored in the literature. String-valued latent variables are widely used in language processing tasks ranging from morphological analysis (Dreyer and Eisner, 2009) to sentence compression (Miao and Blunsom, 2016). Natural language annotations have been used in conjunction with training examples to guide the discovery of logical descriptions of concepts (Ling et al., 2017; Srivastava et al., 2017), and used as an auxiliary loss for training (Frome et al., 2013), analogously to the Meta+Joint baseline in this paper. Structured language-like annotations have been used to improve learning of generalizable structured policies (Oh et al., 2017; Andreas et al., 2017; Denil et al., 2017). Finally, natural language instructions available at concept-learning time (rather than language-learning time) have been u"
N18-1197,Q14-1017,0,0.0494754,"guage provides a strong prior about the kinds of abstractions that are useful for natural learning problems. Concretely, we replace the pretraining phase above with a language-learning phase. We assume that at language-learning time we have access to natural-language descriptions w(`i) (Figure 2a, bottom). We use these w as parameters, in place of the task-specific parameters θ—that is, we learn a language interpretation model f (x; η, w) that uses shared parameters η to turn a description w into a function from inputs to outputs. For the example in Figure 2, f might be an image rating model (Socher et al., 2014) that outputs a scalar judgment y of how well an image x matches a caption w. Because these natural language parameters are observed at language-learning time, we need only learn the real-valued shared parameters η used for their interpretation (e.g. the weights of a neural network that implements the image rating model): arg min η ∈ Ra In this work, we are interested in developing a learning method that enjoys the benefits of both approaches. In particular, we seek an intermediate language of task representations that, like in program synthesis, is both expressive and compact, but like in mul"
N18-1197,D17-1161,0,0.0873683,"ningful behaviors. 7 Other Related Work This is the first approach we are aware of to frame a general learning problem as optimization over a space of natural language strings. However, many closely related ideas have been explored in the literature. String-valued latent variables are widely used in language processing tasks ranging from morphological analysis (Dreyer and Eisner, 2009) to sentence compression (Miao and Blunsom, 2016). Natural language annotations have been used in conjunction with training examples to guide the discovery of logical descriptions of concepts (Ling et al., 2017; Srivastava et al., 2017), and used as an auxiliary loss for training (Frome et al., 2013), analogously to the Meta+Joint baseline in this paper. Structured language-like annotations have been used to improve learning of generalizable structured policies (Oh et al., 2017; Andreas et al., 2017; Denil et al., 2017). Finally, natural language instructions available at concept-learning time (rather than language-learning time) have been used to provide side information to reinforcement learners about high-level strategy (Branavan et al., 2011), environments (Narasimhan et al., 2017) and exploration (Harrison et al., 2017)"
N18-1197,P17-2034,0,0.0430825,"d these techniques to reinforcement learning. 4 Few-shot Classification We begin by investigating whether natural language can be used to support high-dimensional few-shot classification. Our focus is on visual reasoning tasks like the one shown in Figure 3. In these problems, the learner is presented with four images, all positive examples of some visual concept like a blue shape near a yellow triangle, and must decide whether a fifth, held-out image matches the same concept. These kinds of reasoning problems have been well-studied in visual question answering settings (Johnson et al., 2017; Suhr et al., 2017). Our version of the problem, where the input and output feature no text data, but an explanation must be inferred, is similar to the visual reasoning problems proposed by Raven (1936) and Bongard (1968). To apply the recipe in Section 2, we need to specify an implementation of the interpretation model f and the proposal model q. We begin by computing representations of input images x. We start with a pre-trained 16-layer VGGNet (Simonyan and Zisserman, 2014). Because spatial information is important for these tasks, we extract a feature representation from the final convolutional layer of the"
N19-1410,N18-1150,0,0.0150674,"q. 1 (with weights tuned on the development data). Both of our pragmatic systems improve over the strong baseline S0 system on all five metrics, with the largest improvements (2.1 BLEU, 0.2 NIST, 0.8 METEOR , 1.5 ROUGE - L , and 0.1 CIDE r) from the S1R model. This S1R model outperforms the previous best results obtained by any system in the E2E challenge on BLEU, NIST, and CIDEr, with comparable performance on METEOR and ROUGE - L. Table 2: Test results for the non-anonymized CNN/Daily Mail summarization task. We compare to extractive baselines, and the best previous abstractive results of † Celikyilmaz et al. (2018), ‡ Paulus et al. (2018) and  Chen and Bansal (2018). We bold our highest performing model on each metric, as well as previous work if it outperforms all of our models. 4.2 Abstractive Summarization We evaluate on the CNN/Daily Mail summarization dataset (Hermann et al., 2015; Nallapati et al., 2016), using See et al.’s (2017) non-anonymized preprocessing. As in previous work (Chen and Bansal, 2018), we evaluate using ROUGE and ME TEOR . Table 2 compares our pragmatic systems to the base S0 model (with scores taken from Chen and Bansal (2018); we obtained comparable performance in our reprodu"
N19-1410,D10-1040,1,0.556817,"atic system (S1R ) Fitzbillies is a family friendly coffee shop that serves cheap English food in the riverside area. It has a customer rating of 5 out of 5. Figure 1: Example outputs of our systems on the E2E generation task. While a base sequence-to-sequence model (S0 , Sec. 2) fails to describe all attributes in the input meaning representation, both of our pragmatic systems (S1R , Sec. 3.1 and S1D , Sec. 3.2) and the human-written reference do. Introduction Computational approaches to pragmatics cast language generation and interpretation as gametheoretic or Bayesian inference procedures (Golland et al., 2010; Frank and Goodman, 2012). While such approaches are capable of modeling a variety of pragmatic phenomena, their main application in natural language processing has been to improve the informativeness of generated text in grounded language learning problems (Monroe et al., 2018). In this paper, we show that pragmatic reasoning can be similarly used to improve performance in more traditional language generation tasks like generation from structured meaning representations (Figure 1) and summarization. Our work builds on a line of learned Rational Speech Acts (RSA) models (Monroe and Potts, 201"
N19-1410,P18-1063,0,0.14831,"(Puzikov and Gurevych, 2018) that achieves comparable performance to the best published results in Duˇsek et al. (2018). Abstractive Summarization Our second task is multi-sentence document summarization. There is a vast amount of past work on summarization (Nenkova and McKeown, 2011); recent neural models have used large datasets (e.g., Hermann et al. (2015)) to train models in both the extractive (Cheng and Lapata, 2016; Nallapati et al., 2017) and abstractive (Rush et al., 2015; See et al., 2017) settings. Among these works, we build on the recent abstractive neural summarization system of Chen and Bansal (2018). First, this system uses a sentence-level extractive model RNN - EXT to identify a sequence of salient sentences i(1) , . . . i(P ) in each source document. Second, the system uses an abstractive model ABS to rewrite each i(p) into an output o(p) , which are then concatenated to produce the final summary. We rely on the fixed RNN EXT model to extract sentences as inputs in our pragmatic procedure, using ABS as our S0 model and applying pragmatics to the i(p) → o(p) abstractive step. 3 Pragmatic Models To produce informative outputs, we consider pragmatic methods that extend the base speaker m"
N19-1410,P16-1046,0,0.0272664,"rant with the specified attributes. We apply pragmatics to encourage output strings from which the input MR can be identified. For our S0 model, we use a publicly-released neural generation system (Puzikov and Gurevych, 2018) that achieves comparable performance to the best published results in Duˇsek et al. (2018). Abstractive Summarization Our second task is multi-sentence document summarization. There is a vast amount of past work on summarization (Nenkova and McKeown, 2011); recent neural models have used large datasets (e.g., Hermann et al. (2015)) to train models in both the extractive (Cheng and Lapata, 2016; Nallapati et al., 2017) and abstractive (Rush et al., 2015; See et al., 2017) settings. Among these works, we build on the recent abstractive neural summarization system of Chen and Bansal (2018). First, this system uses a sentence-level extractive model RNN - EXT to identify a sequence of salient sentences i(1) , . . . i(P ) in each source document. Second, the system uses an abstractive model ABS to rewrite each i(p) into an output o(p) , which are then concatenated to produce the final summary. We rely on the fixed RNN EXT model to extract sentences as inputs in our pragmatic procedure, u"
N19-1410,D14-1179,0,0.0145181,"Missing"
N19-1410,N18-2070,0,0.0519457,"fine LR (i |o) as the joint probability of predicting all input MR attributes in i from o. Summarization To construct LR for summarization, we train an ABS model (of the type we use for S0 , Chen and Bansal (2018)) but in reverse, i.e., taking as input a sentence in the summary and producing a sentence in the source document. We train LR on the same heuristically-extracted and aligned source document sentences used to train S0 (Chen and Bansal, 2018). 3.2 Distractor-Based Pragmatics Pragmatic approaches in this category (Frank and Goodman, 2012; Andreas and Klein, 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018) derive pragmatic behavior by producing outputs that distinguish the input i from an alternate distractor input (or inputs). We construct a distractor eı for a given input i in a task-dependent way.1 We follow the approach of Cohn-Gordon et al. (2018), outlined briefly here. The base speakers we build on produce outputs incrementally, where the probability of ot , the word output at time t, is conditioned on the input and the previously generated words: S0 (ot |i, o<t ). Since the output is generated incrementally and there is no separate 1 In tasks such as contrastive captioning or referring"
N19-1410,P16-2008,0,0.164084,"Missing"
N19-1410,N18-1014,0,0.0624822,".83 2.23 2.27• Extractive S0 S0 ×2 S1R S1D 66.52 65.93 68.60 67.76 8.55 8.31 8.73 8.72 44.45 43.52 45.25 44.59 69.34 69.58 70.82 69.41 2.23 2.12 2.37 2.27 R-2 R-L METEOR 40.34 38.93 17.70 18.23 36.57 35.90 22.21 24.66 Best Previous 41.69† 19.47† 39.08‡ 21.00 S0 S0 ×2 S1R S1D 40.88 40.76 41.23 41.39 17.80 17.88 18.07 18.30 38.54 38.46 38.76 38.78 20.38 19.88 20.57 21.70 Lead-3 Inputs Abstractive Table 1: Test results for the E2E generation task, in comparison to the T-Gen baseline (Duˇsek and Jurˇc´ıcˇ ek, 2016) and the best results from the E2E challenge, reported by Duˇsek et al. (2018): † Juraska et al. (2018), ‡ Puzikov and Gurevych (2018),  Zhang et al. (2018), and • Gong (2018). We bold our highest performing model on each metric, as well as previous work if it outperforms all of our models. 4.1 R-1 Meaning Representations We evaluate on the E2E task of generation from meaning representations containing restaurant attributes (Novikova et al., 2017). We report the task’s five automatic metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Lavie and Agarwal, 2007), ROUGE - L (Lin, 2004) and CIDEr (Vedantam et al., 2015). Table 1 compares the performance of our base S0 and pragma"
N19-1410,W07-0734,0,0.0472317,"seline (Duˇsek and Jurˇc´ıcˇ ek, 2016) and the best results from the E2E challenge, reported by Duˇsek et al. (2018): † Juraska et al. (2018), ‡ Puzikov and Gurevych (2018),  Zhang et al. (2018), and • Gong (2018). We bold our highest performing model on each metric, as well as previous work if it outperforms all of our models. 4.1 R-1 Meaning Representations We evaluate on the E2E task of generation from meaning representations containing restaurant attributes (Novikova et al., 2017). We report the task’s five automatic metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Lavie and Agarwal, 2007), ROUGE - L (Lin, 2004) and CIDEr (Vedantam et al., 2015). Table 1 compares the performance of our base S0 and pragmatic models to the baseline T-Gen system (Duˇsek and Jurˇc´ıcˇ ek, 2016) and the best previous result from the 20 primary systems evaluated in the E2E challenge (Duˇsek et al., 2018). The systems obtaining these results encompass a range of approaches: a template system (Puzikov and Gurevych, 2018), a neural model (Zhang et al., 2018), models trained with reinforcement learning (Gong, 2018), and systems using ensembling and reranking (Juraska et al., 2018). To ensure that the ben"
N19-1410,W04-1013,0,0.01764,") and the best results from the E2E challenge, reported by Duˇsek et al. (2018): † Juraska et al. (2018), ‡ Puzikov and Gurevych (2018),  Zhang et al. (2018), and • Gong (2018). We bold our highest performing model on each metric, as well as previous work if it outperforms all of our models. 4.1 R-1 Meaning Representations We evaluate on the E2E task of generation from meaning representations containing restaurant attributes (Novikova et al., 2017). We report the task’s five automatic metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Lavie and Agarwal, 2007), ROUGE - L (Lin, 2004) and CIDEr (Vedantam et al., 2015). Table 1 compares the performance of our base S0 and pragmatic models to the baseline T-Gen system (Duˇsek and Jurˇc´ıcˇ ek, 2016) and the best previous result from the 20 primary systems evaluated in the E2E challenge (Duˇsek et al., 2018). The systems obtaining these results encompass a range of approaches: a template system (Puzikov and Gurevych, 2018), a neural model (Zhang et al., 2018), models trained with reinforcement learning (Gong, 2018), and systems using ensembling and reranking (Juraska et al., 2018). To ensure that the benefit of the reconstruct"
N19-1410,N18-1196,0,0.0780322,"fails to describe all attributes in the input meaning representation, both of our pragmatic systems (S1R , Sec. 3.1 and S1D , Sec. 3.2) and the human-written reference do. Introduction Computational approaches to pragmatics cast language generation and interpretation as gametheoretic or Bayesian inference procedures (Golland et al., 2010; Frank and Goodman, 2012). While such approaches are capable of modeling a variety of pragmatic phenomena, their main application in natural language processing has been to improve the informativeness of generated text in grounded language learning problems (Monroe et al., 2018). In this paper, we show that pragmatic reasoning can be similarly used to improve performance in more traditional language generation tasks like generation from structured meaning representations (Figure 1) and summarization. Our work builds on a line of learned Rational Speech Acts (RSA) models (Monroe and Potts, 2015; Andreas and Klein, 2016), in which generated strings are selected to optimize the behavior of an embedded listener model. The canonical presentation of the RSA framework (Frank and Goodman, 2012) is grounded in reference resolution: models of speakers attempt to describe refer"
N19-1410,K16-1028,0,0.0199832,"t results obtained by any system in the E2E challenge on BLEU, NIST, and CIDEr, with comparable performance on METEOR and ROUGE - L. Table 2: Test results for the non-anonymized CNN/Daily Mail summarization task. We compare to extractive baselines, and the best previous abstractive results of † Celikyilmaz et al. (2018), ‡ Paulus et al. (2018) and  Chen and Bansal (2018). We bold our highest performing model on each metric, as well as previous work if it outperforms all of our models. 4.2 Abstractive Summarization We evaluate on the CNN/Daily Mail summarization dataset (Hermann et al., 2015; Nallapati et al., 2016), using See et al.’s (2017) non-anonymized preprocessing. As in previous work (Chen and Bansal, 2018), we evaluate using ROUGE and ME TEOR . Table 2 compares our pragmatic systems to the base S0 model (with scores taken from Chen and Bansal (2018); we obtained comparable performance in our reproduction3 ), an ensemble of two of these base models, and the best previous abstractive summarization result for each metric on this dataset (Celikyilmaz et al., 2018; Paulus et al., 2018; Chen and Bansal, 2018). We also report two extractive baselines: Lead-3, which uses the first three sentences of the"
N19-1410,P17-1099,0,0.228617,"from which the input MR can be identified. For our S0 model, we use a publicly-released neural generation system (Puzikov and Gurevych, 2018) that achieves comparable performance to the best published results in Duˇsek et al. (2018). Abstractive Summarization Our second task is multi-sentence document summarization. There is a vast amount of past work on summarization (Nenkova and McKeown, 2011); recent neural models have used large datasets (e.g., Hermann et al. (2015)) to train models in both the extractive (Cheng and Lapata, 2016; Nallapati et al., 2017) and abstractive (Rush et al., 2015; See et al., 2017) settings. Among these works, we build on the recent abstractive neural summarization system of Chen and Bansal (2018). First, this system uses a sentence-level extractive model RNN - EXT to identify a sequence of salient sentences i(1) , . . . i(P ) in each source document. Second, the system uses an abstractive model ABS to rewrite each i(p) into an output o(p) , which are then concatenated to produce the final summary. We rely on the fixed RNN EXT model to extract sentences as inputs in our pragmatic procedure, using ABS as our S0 model and applying pragmatics to the i(p) → o(p) abstractive"
N19-1410,W17-5525,0,0.123139,"Missing"
N19-1410,P15-1158,0,0.0634905,"Missing"
N19-1410,P02-1040,0,0.103698,"or the E2E generation task, in comparison to the T-Gen baseline (Duˇsek and Jurˇc´ıcˇ ek, 2016) and the best results from the E2E challenge, reported by Duˇsek et al. (2018): † Juraska et al. (2018), ‡ Puzikov and Gurevych (2018),  Zhang et al. (2018), and • Gong (2018). We bold our highest performing model on each metric, as well as previous work if it outperforms all of our models. 4.1 R-1 Meaning Representations We evaluate on the E2E task of generation from meaning representations containing restaurant attributes (Novikova et al., 2017). We report the task’s five automatic metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Lavie and Agarwal, 2007), ROUGE - L (Lin, 2004) and CIDEr (Vedantam et al., 2015). Table 1 compares the performance of our base S0 and pragmatic models to the baseline T-Gen system (Duˇsek and Jurˇc´ıcˇ ek, 2016) and the best previous result from the 20 primary systems evaluated in the E2E challenge (Duˇsek et al., 2018). The systems obtaining these results encompass a range of approaches: a template system (Puzikov and Gurevych, 2018), a neural model (Zhang et al., 2018), models trained with reinforcement learning (Gong, 2018), and systems using ensembling a"
N19-1410,W18-6557,0,0.302195,". For these S0 models we use systems from past work that are strong, but may still be underinformative relative to human reference outputs (e.g., Figure 1). Meaning Representations Our first task is generation from structured meaning representations (MRs) containing attribute-value pairs (Novikova et al., 2017). An example is shown in Figure 1, where systems must generate a description of the restaurant with the specified attributes. We apply pragmatics to encourage output strings from which the input MR can be identified. For our S0 model, we use a publicly-released neural generation system (Puzikov and Gurevych, 2018) that achieves comparable performance to the best published results in Duˇsek et al. (2018). Abstractive Summarization Our second task is multi-sentence document summarization. There is a vast amount of past work on summarization (Nenkova and McKeown, 2011); recent neural models have used large datasets (e.g., Hermann et al. (2015)) to train models in both the extractive (Cheng and Lapata, 2016; Nallapati et al., 2017) and abstractive (Rush et al., 2015; See et al., 2017) settings. Among these works, we build on the recent abstractive neural summarization system of Chen and Bansal (2018). Firs"
N19-1410,D15-1044,0,0.0475982,"age output strings from which the input MR can be identified. For our S0 model, we use a publicly-released neural generation system (Puzikov and Gurevych, 2018) that achieves comparable performance to the best published results in Duˇsek et al. (2018). Abstractive Summarization Our second task is multi-sentence document summarization. There is a vast amount of past work on summarization (Nenkova and McKeown, 2011); recent neural models have used large datasets (e.g., Hermann et al. (2015)) to train models in both the extractive (Cheng and Lapata, 2016; Nallapati et al., 2017) and abstractive (Rush et al., 2015; See et al., 2017) settings. Among these works, we build on the recent abstractive neural summarization system of Chen and Bansal (2018). First, this system uses a sentence-level extractive model RNN - EXT to identify a sequence of salient sentences i(1) , . . . i(P ) in each source document. Second, the system uses an abstractive model ABS to rewrite each i(p) into an output o(p) , which are then concatenated to produce the final summary. We rely on the fixed RNN EXT model to extract sentences as inputs in our pragmatic procedure, using ABS as our S0 model and applying pragmatics to the i(p)"
P01-1044,J93-2004,0,\N,Missing
P01-1044,P97-1003,0,\N,Missing
P02-1017,P93-1035,0,0.0742551,"Missing"
P02-1017,P95-1031,0,0.135505,"Missing"
P02-1017,W00-0717,0,0.171732,"treebank, a 1000 element vector was made by counting how often each co-occurred with each of the 500 most common words immediately to the left or right in Treebank text and additional 1994–96 WSJ newswire. These vectors were length-normalized, and then rank-reduced by an SVD, keeping the 50 largest singular vectors. The resulting vectors were clustered into 200 word classes by a weighted k-means algorithm, and then grammar induction operated over these classes. We do not believe that the quality of our tags matches that of the better methods of Sch¨utze (1995), much less the recent results of Clark (2000). Nevertheless, using these tags as input still gave induced structure substantially above right-branching. Figure 8 shows      80 70 60 50 40 30 20 10 0 0.35M  0.30M   0.25M 0.20M    0.15M   F1 log-likelihood 0.10M    0.05M   0.00M 0 4 8 12 16 20 24 28 32 36 40 Iterations Figure 10: F1 is non-decreasing until convergence. the performance with induced tags compared to correct tags. Overall F1 has dropped, but, interestingly, VP and S recall are higher. This seems to be due to a marked difference between the induced tags and the treebank tags: nouns are scattered among a"
P02-1017,W01-0713,0,0.886212,"induced and actual part-of-speech tags as input data, and examine extensions to the basic model. We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task. 1 Introduction The task of inducing hierarchical syntactic structure from observed yields alone has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), or to build better language models (Baker, 1979; Chen, 1995). In previous work, we presented a conditional model over trees which gave the best published results for unsupervised parsing of the ATIS corpus (Klein and Manning, 2001b). However, it suffered from several drawbacks, primarily stemming from the conditional model used for induction. Here, we improve on that model in several ways. First, we construct a generative model which utilizes the same features. Then, we extend the model to allow mul"
P02-1017,W01-0714,1,0.788755,"1968; Wolff, 1988). In the early 1990s, attempts were made to do grammar induction by parameter search, where the broad structure of the grammar is fixed in advance and only parameters are induced (Lari and Young, 1990; Carroll and Charniak, 1992). 1 However, this appeared unpromising and most recent work has returned to using structure search. Note that both approaches are local. Structure search requires ways of deciding locally which merges will produce a coherent, globally good grammar. To the extent that such approaches work, they work because good local heuristics have been engineered (Klein and Manning, 2001a; Clark, 2001). 1 On this approach, the question of which rules are included or excluded becomes the question of which parameters are zero. End S 0 VBD 3 End 4 5 0 1 2 3 4 5 0 0 0 1 1 1 2 PP 3 IN 4 5 NN 0 Factory 1 payrolls 2 fell 3 in 4 September 5 Start NNS Start NN VP 2 Start NP 1 2 3 2 3 4 4 5 5 Span End Label Constituent 0 h0,5i 1 2 3 S 4 5NN NNS VBD IN NN h0,2i NP NN NNS h2,5i VP VBD IN NN h3,5i PP IN NN h0,1i NN NN h1,2i NNS NNS h2,3i VBD VBD h3,4i IN IN h4,5i NN NNS Context –  – VBD NNS –  VBD –   – NNS NN – VBD NNS – IN VBD – NN IN –  (a) (b) (c) Figure 1: (a) Example parse tr"
P02-1017,P92-1017,0,0.399736,"Missing"
P02-1017,E95-1020,0,0.661882,"Missing"
P02-1017,C00-2139,0,0.426843,"Missing"
P02-1017,H93-1047,0,\N,Missing
P02-1017,P93-1034,0,\N,Missing
P03-1054,J98-2004,0,0.133778,"Missing"
P03-1054,W98-1115,0,0.155127,"Missing"
P03-1054,A00-2018,0,0.135229,"Missing"
P03-1054,P01-1017,0,0.134591,"Missing"
P03-1054,P96-1025,0,0.13596,"Missing"
P03-1054,P99-1059,0,0.15953,"Missing"
P03-1054,W01-0521,0,0.107575,"Missing"
P03-1054,J93-1005,0,0.128122,"Missing"
P03-1054,J98-4004,0,0.1401,"nd structural conditioning. 1 Experimental Setup To facilitate comparison with previous work, we trained our models on sections 2–21 of the WSJ section of the Penn treebank. We used the first 20 files (393 sentences) of section 22 as a development set (devset). This set is small enough that there is noticeable variance in individual results, but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill-climb. All of section 23 was used as a test set for the final model. For each model, input trees were annotated or transformed in some way, as in Johnson (1998). Given a set of transformed trees, we viewed the local trees as grammar rewrite rules in the standard way, and used (unsmoothed) maximum-likelihood estimates for rule probabilities.5 To parse the grammar, we used a simple array-based Java implementation of a generalized CKY parser, which, for our final best model, was able to exhaustively parse all sentences in section 23 in 1GB of memory, taking approximately 3 sec for average length sentences.6 5 The tagging probabilities were smoothed to accommodate unknown words. The quantity P(tag|wor d) was estimated as follows: words were split into on"
P03-1054,P01-1044,1,0.13755,"Missing"
P03-1054,P95-1037,0,0.127613,"Missing"
P03-1054,J03-4003,0,\N,Missing
P04-1061,P93-1035,0,0.0488106,"duct model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data. 1 Introduction The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved"
P04-1061,P95-1031,0,0.0254692,"tributional regularities that are salient in the data. 1 Introduction The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved models over observed structures (Baker, 1979; Chen, 1995), and work interested in the strong generative capacity of models, where the unobserved structure itself is evaluated (van Zaanen, 2000; Clark, 2001; Klein and Manning, 2002). This paper falls into the latter category; we will be inducing models of linguistic con"
P04-1061,W00-0717,0,0.0418312,"ic model, P(B) is uniform over binary trees. Then, for each hi, j i, the subspan and context pair (i s j , i−1 si ∼ j s j +1 ) is generated via a classconditional independence model: P(s, B) = P(B) Y P(i s j |bi j )P(i−1 si ∼ j s j +1 |bi j ) hi, j i data clustering methods. In the most common case, the items are words, and one uses distributions over adjacent words to induce word classes. Previous work has shown that even this quite simple representation allows the induction of quite high quality word classes, largely corresponding to traditional parts of speech (Finch, 1993; Sch¨utze, 1995; Clark, 2000). A typical pattern would be that stocks and treasuries both frequently occur before the words fell and rose, and might therefore be put into the same class. Clark (2001) and Klein and Manning (2002) show that this approach can be successfully used for discovering syntactic constituents as well. However, as one might expect, it is easier to cluster word sequences (or word class sequences) than to tell how to put them together into trees. In particular, if one is given all contiguous subsequences (subspans) from a corpus of sentences, most natural clusters will not represent valid constituents"
P04-1061,W01-0713,0,0.402317,"constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data. 1 Introduction The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved models over observed structures (Baker, 1979; Chen, 1995), and work interested in the strong generative capacity of models, where the unobserved structure itself is ev"
P04-1061,C96-1058,0,0.531625,". h and a are head and argument words, respectively, while i , j , and k are positions between words. attach to the verb. But then, given a NOUN NOUN VERB sequence, both nouns will attach to the verb – there is no way that the model can learn that verbs have exactly one subject. We now turn to an improved dependency model that addresses this problem. 3 An Improved Dependency Model The dependency models discussed above are distinct from dependency models used inside highperformance supervised probabilistic parsers in several ways. First, in supervised models, a head outward process is modeled (Eisner, 1996; Collins, 1999). In such processes, heads generate a sequence of arguments outward to the left or right, conditioning on not only the identity of the head and direction of the attachment, but also on some notion of distance or valence. Moreover, in a head-outward model, it is natural to model stop steps, where the final argument on each side of a head is always the special symbol STOP. Models like Paskin (2002) avoid modeling STOP by generating the graph skeleton G first, uniformly at random, then populating the words of s conditioned on G. Previous work (Collins, 1999) has stressed the impor"
P04-1061,P02-1017,1,0.666116,"age in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved models over observed structures (Baker, 1979; Chen, 1995), and work interested in the strong generative capacity of models, where the unobserved structure itself is evaluated (van Zaanen, 2000; Clark, 2001; Klein and Manning, 2002). This paper falls into the latter category; we will be inducing models of linguistic constituency and dependency with the goal of recovering linguistically plausible structures. We make no claims as to the cognitive plausibility of the induction mechanisms we present here; however, the ability of these systems to recover substantial linguistic patterns from surface yields alone does speak to the strength of support for these patterns in the data, and hence undermines arguments based on “the poverty of the stimulus” (Chomsky, 1965). 2 Unsupervised Dependency Parsing Most recent progress in uns"
P04-1061,P92-1017,0,0.283648,"inear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data. 1 Introduction The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads"
P04-1061,E95-1020,0,0.316771,"Missing"
P04-1061,C00-2139,0,0.120736,"Missing"
P04-1061,J03-4003,0,\N,Missing
P04-1061,H93-1047,0,\N,Missing
P04-1061,C80-1026,0,\N,Missing
P05-1046,N04-1015,0,0.0181962,"onditional Random Field (CRF) sequence models to the problem of parsing the headers of research papers. There has also been some previous work on unsupervised learning of field segmentation models in particular domains. Pasula et al. (2002) performs limited unsupervised segmentation of bibliographic citations as a small part of a larger probabilistic model of identity uncertainty. However, their system does not explicitly learn a field segmentation model for the citations, and encodes a large amount of hand-supplied information about name forms, abbreviation schemes, and so on. More recently, Barzilay and Lee (2004) defined content models, which can be viewed as field segmentation models occurring at the level of discourse. They perform unsupervised learning of these models from sets of news articles which describe similar events. The fields in that case are the topics discussed in those articles. They consider a very different set of applications from the present work, and show that the learned topic models improve performance on two discourse-related tasks: information ordering and extractive document summarization. Most importantly, their learning method differs significantly from ours; they use a com"
P05-1046,J97-1003,0,0.0212744,"ation ordering and extractive document summarization. Most importantly, their learning method differs significantly from ours; they use a complex and special purpose algorithm, which is difficult to adapt, while we see our contribution to be a demonstration of the interplay between model family and learned structure. Because the structure of the HMMs they learn is similar to ours it seems that their system could benefit from the techniques of this paper. Finally, Blei and Moreno (2001) use an HMM augmented by an aspect model to automatically segment documents, similar in goal to the system of Hearst (1997), but using techniques more similar to the present work. 7 Conclusions In this work, we have examined the task of learning field segmentation models using unsupervised learning. In two different domains, classified advertisements and bibliographic citations, we showed that by constraining the model class we were able to restrict the search space of EM to models of interest. We used unsupervised learning methods with 400 documents to yield field segmentation models of a similar quality to those learned using supervised learning with 50 documents. We demonstrated that further refinements of the"
P05-1046,N04-1042,0,0.0611895,"e cast as supervised learning of field segmentation models, using various model families and applied to various domains. McCallum et al. (1999) were the first to compare a number of supervised methods for learning HMMs for parsing bibliographic citations. The authors explicitly claim that the domain would be suitable for unsupervised learning, but they do not present experimental results. McCallum et al. (2000) applied supervised learning of Maximum Entropy Markov Models (MEMMs) to the domain of parsing Frequently Asked Question (FAQ) lists into their component field structure. More recently, Peng and McCallum (2004) applied supervised learning of Conditional Random Field (CRF) sequence models to the problem of parsing the headers of research papers. There has also been some previous work on unsupervised learning of field segmentation models in particular domains. Pasula et al. (2002) performs limited unsupervised segmentation of bibliographic citations as a small part of a larger probabilistic model of identity uncertainty. However, their system does not explicitly learn a field segmentation model for the citations, and encodes a large amount of hand-supplied information about name forms, abbreviation sc"
P06-1055,P05-1022,0,0.484383,"pages 433–440, c Sydney, July 2006. 2006 Association for Computational Linguistics (a) RB NP cursively: (b) FRAG ROOT FRAG . FRAG Not DT NN . this year RB NP PIN (r, t, Ax ) = . X β(Ax → By Cz ) ×PIN (r, s, By )PIN (s, t, Cz ) y,z . Not DT NN POUT (r, s, By ) = X β(Ax → By Cz ) ×POUT (r, t, Ax )PIN (s, t, Cz ) POUT (s, t, Cz ) = X β(Ax → By Cz ) ×POUT (r, t, Ax )PIN (r, s, By ) this year x,z Figure 1: (a) The original tree. (b) The X-bar tree. x,y mar in Matsuzaki et al. (2005). Our grammar’s accuracy was higher than fully lexicalized systems, including the maximum-entropy inspired parser of Charniak and Johnson (2005). 1.1 Experimental Setup We ran our experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank using the standard setup: we trained on sections 2 to 21, and we used section 1 as a validation set for tuning model hyperparameters. Section 22 was used as development set for intermediate results. All of section 23 was reserved for the final test. We used the EVALB parseval reference implementation, available from Sekine and Collins (1997), for scoring. All reported development set results are averages over four runs. For the final test we selected the grammar that performed best on"
P06-1055,A00-2018,0,0.0879868,"Missing"
P06-1055,C02-1126,0,0.0109507,"Missing"
P06-1055,P96-1024,0,0.152353,"Missing"
P06-1055,P04-1013,0,0.0165333,"e likelihood of the training trees, despite the fact that the original trees lack the latent annotations. The Expectation-Maximization (EM) algorithm allows us to do exactly that.2 Given a sentence w and its unannotated tree T , consider a nonterminal A spanning (r, t) and its children B and C spanning (r, s) and (s, t). Let Ax be a subsymbol of A, By of B, and Cz of C. Then the inside and def outside probabilities PIN (r, t, Ax ) = P (wr:t |Ax ) and def POUT (r, t, Ax ) = P (w1:r Ax wt:n ) can be computed reencourages sparsity) suggest a large reduction. 2 Other techniques are also possible; Henderson (2004) uses neural networks to induce latent left-corner parser states. 434 Although we show only the binary component here, of course there are both binary and unary productions that are included. In the Expectation step, one computes the posterior probability of each annotated rule and position in each training set tree T : P ((r, s, t, Ax → By Cz )|w, T ) ∝ POUT (r, t, Ax ) ×β(Ax → By Cz )PIN (r, s, By )PIN (s, t, Cz ) (1) In the Maximization step, one uses the above probabilities as weighted observations to update the rule probabilities: β(Ax → By Cz ) := P #{Ax → By Cz } #{Ax → By′ Cz′ } y ′ ,z"
P06-1055,J98-4004,0,0.0704583,"Missing"
P06-1055,P03-1054,1,0.417007,"with a Markov grammar and manually splitting symbols in response to observed linguistic trends in the data. For example, the symbol NP might be split into the subsymbol NPˆS in subject position and the subsymbol NPˆVP in object position. Recently, Matsuzaki et al. (2005) and also Prescher (2005) exhibited an automatic approach in which each symbol is split into a fixed number of subsymbols. For example, NP would be split into NP-1 through NP-8. Their exciting result was that, while grammars quickly grew too large to be managed, a 16-subsymbol induced grammar reached the parsing performance of Klein and Manning (2003)’s manual grammar. Other work has also investigated aspects of automatic grammar refinement; for example, Chiang and Bikel (2002) learn annotations such as head rules in a constrained declarative language for tree-adjoining grammars. We present a method that combines the strengths of both manual and automatic approaches while addressing some of their common shortcomings. Like Matsuzaki et al. (2005) and Prescher (2005), we induce splits in a fully automatic fashion. However, we use a more sophisticated split-and-merge approach that allocates subsymbols adaptively where they are most effective,"
P06-1055,P05-1010,0,0.430483,"ization and intricate smoothing (Collins, 1999; Charniak, 2000). In this paper, we investigate the learning of a grammar consistent with a treebank at the level of evaluation symbols (such as NP, VP, etc.) but split based on the likelihood of the training trees. Klein and Manning (2003) addressed this question from a linguistic perspective, starting with a Markov grammar and manually splitting symbols in response to observed linguistic trends in the data. For example, the symbol NP might be split into the subsymbol NPˆS in subject position and the subsymbol NPˆVP in object position. Recently, Matsuzaki et al. (2005) and also Prescher (2005) exhibited an automatic approach in which each symbol is split into a fixed number of subsymbols. For example, NP would be split into NP-1 through NP-8. Their exciting result was that, while grammars quickly grew too large to be managed, a 16-subsymbol induced grammar reached the parsing performance of Klein and Manning (2003)’s manual grammar. Other work has also investigated aspects of automatic grammar refinement; for example, Chiang and Bikel (2002) learn annotations such as head rules in a constrained declarative language for tree-adjoining grammars. We present a"
P06-1055,P92-1017,0,0.169459,"Missing"
P06-1055,J98-1004,0,0.116152,"ons). For example, plural common nouns (NNS) divide into the maximum number of categories (16). One category consists primarily of dates, whose typical parent is an NP subsymbol whose typical parent is a root S, essentially modeling the temporal noun annotation discussed in Klein and Manning (2003). Another category specializes in capitalized words, preferring as a parent an NP with an S parent (i.e. subject position). A third category specializes in monetary units, and so on. These kinds of syntactico-semantic categories are typical, and, given distributional clustering results like those of Schuetze (1998), unsurprising. The singular nouns are broadly similar, if slightly more homogenous, being dominated by categories for stocks and trading. The proper noun category (NNP, shown) also splits into the maximum 16 categories, including months, countries, variants of Co. and Inc., first names, last names, initials, and so on. Verbal categories are also heavily split. Verbal subcategories sometimes reflect syntactic selectional preferences, sometimes reflect semantic selectional preferences, and sometimes reflect other aspects of verbal syntax. For example, the present tense third person verb subsymb"
P06-1055,J98-2004,0,\N,Missing
P06-1055,J03-4003,0,\N,Missing
P06-1096,J92-4003,0,0.0353914,"ion in isolation. This apparent degradation causes no problems, because when des should actually be translated to of, these words are usually embedded in larger phrases, in which case the isolated translation probability plays no role. Another example of a related phenomenon is the following: Input B B+L ... ... ... pour cela que for that for this reason j ’ ai i have i vot´e favorablement voted in favour voted in favour . . . Counterintuitively, the phrase pair (j ’ ai, I have) ends up with a very negative weight. The reason behind this is that in French, 7 We also tried using word clusters (Brown et al., 1992) instead of POS but found that POS was more helpful. 766 Features B LANKET B LANKET +L EX B LANKET +L EX +POS , ce mˆ eme zero growth rate , that same croissance z´ ero secure refuge abri sˆ ur (a) (b) (c) +C ONST 32.2 32.5 32.5 Table 4: D EV BLEU score increase resulting from adding constellation features. Figure 3: Three constellation features with example phrase pairs. Constellations (a) and (b) have large positive weights and (c) has a large negative weight. good phrase pairs, we introduce an alignment constellation feature to indicate the presence of a particular alignment constellation.9"
P06-1096,P05-1033,0,0.200868,") the segmentation of the input sentence into phrases, (2) the segmentation of the output sentence into the same number of phrases, and (3) a bijection between the input and output phrases. The feature vector Φ(x, y, h) contains four components: the log probability of the output sentence y under a language model, the score of translating x into y based on a phrase table, a distortion score, and a length penalty.1 In Section 6, we vastly increase the number of features to take advantage of the full power of discriminative training. Another example of this framework is the hierarchical model of Chiang (2005). In this model the correspondence h is a synchronous parse tree 2.2 Perceptron-based training To tune the parameters w of the model, we use the averaged perceptron algorithm (Collins, 2002) because of its efficiency and past success on various NLP tasks (Collins and Roark, 2004; Roark et al., 2004). In principle, w could have been tuned by maximizing conditional probability or maximizing margin. However, these two options require either marginalization or numerical optimization, neither of which is tractable over the space of output sentences y and correspondences h. In contrast, the perceptr"
P06-1096,P04-1015,0,0.026227,"the output sentence y under a language model, the score of translating x into y based on a phrase table, a distortion score, and a length penalty.1 In Section 6, we vastly increase the number of features to take advantage of the full power of discriminative training. Another example of this framework is the hierarchical model of Chiang (2005). In this model the correspondence h is a synchronous parse tree 2.2 Perceptron-based training To tune the parameters w of the model, we use the averaged perceptron algorithm (Collins, 2002) because of its efficiency and past success on various NLP tasks (Collins and Roark, 2004; Roark et al., 2004). In principle, w could have been tuned by maximizing conditional probability or maximizing margin. However, these two options require either marginalization or numerical optimization, neither of which is tractable over the space of output sentences y and correspondences h. In contrast, the perceptron algorithm requires only a decoder that computes f (x; w). Recall the traditional perceptron update rule on an example (xi , yi ) is w ← w + Φ(xi , yt ) − Φ(xi , yp ), (2) where yt = yi is the target output and yp = f (xi ; w) = argmaxy w · Φ(xi , y) is the prediction using th"
P06-1096,W02-1001,0,0.752498,"The feature vector Φ(x, y, h) contains four components: the log probability of the output sentence y under a language model, the score of translating x into y based on a phrase table, a distortion score, and a length penalty.1 In Section 6, we vastly increase the number of features to take advantage of the full power of discriminative training. Another example of this framework is the hierarchical model of Chiang (2005). In this model the correspondence h is a synchronous parse tree 2.2 Perceptron-based training To tune the parameters w of the model, we use the averaged perceptron algorithm (Collins, 2002) because of its efficiency and past success on various NLP tasks (Collins and Roark, 2004; Roark et al., 2004). In principle, w could have been tuned by maximizing conditional probability or maximizing margin. However, these two options require either marginalization or numerical optimization, neither of which is tractable over the space of output sentences y and correspondences h. In contrast, the perceptron algorithm requires only a decoder that computes f (x; w). Recall the traditional perceptron update rule on an example (xi , yi ) is w ← w + Φ(xi , yt ) − Φ(xi , yp ), (2) where yt = yi is"
P06-1096,N03-1017,0,0.514782,"as the foundation for most of the work in statistical machine translation (Brown et al., 1994). At the same time, discriminative methods have provided substantial improvements over generative models on a wide range of NLP tasks. They allow one to easily encode domain knowledge in the form of features. Moreover, parameters are tuned to directly minimize error rather than to maximize joint likelihood, which may not correspond well to the task objective. In this paper, we present an end-to-end discriminative approach to machine translation. The proposed system is phrase-based, as in Koehn et al. (2003), but uses an online perceptron training scheme to learn model parameters. Unlike minimum error rate training (Och, 2003), our system is able to exploit large numbers of specific features in the same manner as static reranking systems (Shen et al., 2004; Och et al., 2004). However, unlike static rerankers, our system does not rely on a baseline translation system. Instead, it updates based on its own n-best lists. As parameter 761 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 761–768, c Sydney, July 2006. 2006 Associatio"
P06-1096,H05-1064,0,0.00570461,"system as we developed it. Note that the D EV set was not used to tune any parameters; tuning was done exclusively on T RAIN. At the end we ran our models once on T EST to get final numbers.2      Figure 1: Given the current prediction (a), there are two possible updates, local (b) and bold (c). Although the bold update (c) reaches the reference translation, a bad correspondence is used. The local update (b) does not reach the reference, but is more reasonable than (c). Discriminative training with hidden variables has been handled in this probabilistic framework (Quattoni et al., 2004; Koo and Collins, 2005), but we choose Equation 3 for efficiency. It turns out that using the Viterbi approximation (which we call bold updating) is not always the best strategy. To appreciate the difficulty, consider the example in Figure 1. Suppose we make the prediction (a) with the current set of parameters. There are often several acceptable output translations y, for example, (b) and (c). Since (c)’s output matches the reference translation, should we update towards (c)? In this case, the answer is negative. The problem with (c) is that the correspondence h contains an incorrect alignment (’, a). However, sinc"
P06-1096,P02-1038,0,0.825922,"Missing"
P06-1096,N04-1021,0,0.0488936,"domain knowledge in the form of features. Moreover, parameters are tuned to directly minimize error rather than to maximize joint likelihood, which may not correspond well to the task objective. In this paper, we present an end-to-end discriminative approach to machine translation. The proposed system is phrase-based, as in Koehn et al. (2003), but uses an online perceptron training scheme to learn model parameters. Unlike minimum error rate training (Och, 2003), our system is able to exploit large numbers of specific features in the same manner as static reranking systems (Shen et al., 2004; Och et al., 2004). However, unlike static rerankers, our system does not rely on a baseline translation system. Instead, it updates based on its own n-best lists. As parameter 761 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 761–768, c Sydney, July 2006. 2006 Association for Computational Linguistics over input and output sentences, and features include the scores of various productions used in the tree. Given features Φ and a corresponding set of parameters w, a standard classification rule f is to return the highest scoring output sen"
P06-1096,P03-1021,0,0.422148,"criminative methods have provided substantial improvements over generative models on a wide range of NLP tasks. They allow one to easily encode domain knowledge in the form of features. Moreover, parameters are tuned to directly minimize error rather than to maximize joint likelihood, which may not correspond well to the task objective. In this paper, we present an end-to-end discriminative approach to machine translation. The proposed system is phrase-based, as in Koehn et al. (2003), but uses an online perceptron training scheme to learn model parameters. Unlike minimum error rate training (Och, 2003), our system is able to exploit large numbers of specific features in the same manner as static reranking systems (Shen et al., 2004; Och et al., 2004). However, unlike static rerankers, our system does not rely on a baseline translation system. Instead, it updates based on its own n-best lists. As parameter 761 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 761–768, c Sydney, July 2006. 2006 Association for Computational Linguistics over input and output sentences, and features include the scores of various productions u"
P06-1096,W05-0908,0,0.0723205,"Missing"
P06-1096,P04-1007,0,0.353314,"er a language model, the score of translating x into y based on a phrase table, a distortion score, and a length penalty.1 In Section 6, we vastly increase the number of features to take advantage of the full power of discriminative training. Another example of this framework is the hierarchical model of Chiang (2005). In this model the correspondence h is a synchronous parse tree 2.2 Perceptron-based training To tune the parameters w of the model, we use the averaged perceptron algorithm (Collins, 2002) because of its efficiency and past success on various NLP tasks (Collins and Roark, 2004; Roark et al., 2004). In principle, w could have been tuned by maximizing conditional probability or maximizing margin. However, these two options require either marginalization or numerical optimization, neither of which is tractable over the space of output sentences y and correspondences h. In contrast, the perceptron algorithm requires only a decoder that computes f (x; w). Recall the traditional perceptron update rule on an example (xi , yi ) is w ← w + Φ(xi , yt ) − Φ(xi , yp ), (2) where yt = yi is the target output and yp = f (xi ; w) = argmaxy w · Φ(xi , y) is the prediction using the current parameters"
P06-1096,N04-1023,0,0.415602,"e to easily encode domain knowledge in the form of features. Moreover, parameters are tuned to directly minimize error rather than to maximize joint likelihood, which may not correspond well to the task objective. In this paper, we present an end-to-end discriminative approach to machine translation. The proposed system is phrase-based, as in Koehn et al. (2003), but uses an online perceptron training scheme to learn model parameters. Unlike minimum error rate training (Och, 2003), our system is able to exploit large numbers of specific features in the same manner as static reranking systems (Shen et al., 2004; Och et al., 2004). However, unlike static rerankers, our system does not rely on a baseline translation system. Instead, it updates based on its own n-best lists. As parameter 761 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 761–768, c Sydney, July 2006. 2006 Association for Computational Linguistics over input and output sentences, and features include the scores of various productions used in the tree. Given features Φ and a corresponding set of parameters w, a standard classification rule f is to return the highest"
P06-1096,P05-1069,0,0.0247617,"onal Linguistics over input and output sentences, and features include the scores of various productions used in the tree. Given features Φ and a corresponding set of parameters w, a standard classification rule f is to return the highest scoring output sentence y, maximizing over correspondences h: feature provides a potential way to initially extract phrases more aggressively and then later downweight undesirable patterns, essentially learning a weighted extraction heuristic. Finally, we use POS features to parameterize a distortion model in a limited distortion decoder (Zens and Ney, 2004; Tillmann and Zhang, 2005). We show that overall, BLEU score increases from 28.4 to 29.6 on French-English. f (x; w) = argmax w · Φ(x, y, h). 2 Approach 2.1 (1) y,h In the phrase-based model, computing the argmax exactly is intractable, so we approximate f with beam decoding. Translation as structured classification Machine translation can be seen as a structured classification task, in which the goal is to learn a mapping from an input (French) sentence x to an output (English) sentence y. Given this setup, discriminative methods allow us to define a broad class of features Φ that operate on (x, y). For example, some"
P06-1096,J93-2003,0,\N,Missing
P06-1096,N04-1033,0,\N,Missing
P06-1111,W01-0713,0,0.0590765,"g bracket locations. To improve the quality of the induced trees, we combine our PCFG induction with the CCM model of Klein and Manning (2002), which has complementary stengths: it identifies brackets but does not label them. Using only a handful of prototypes, we show substantial improvements over naive PCFG induction for English and Chinese grammar induction. 1 Introduction There has been a great deal of work on unsupervised grammar induction, with motivations ranging from scientific interest in language acquisition to engineering interest in parser construction (Carroll and Charniak, 1992; Clark, 2001). Recent work has successfully induced unlabeled grammatical structure, but has not successfully learned labeled tree structure (Klein and Manning, 2002; Klein and Manning, 2004; Smith and Eisner, 2004) . In this paper, our goal is to build a system capable of producing labeled parses in a target grammar with as little total effort as possible. We investigate a prototype-driven approach to grammar induction, in which one supplies canonical examples of each target concept. For example, we might specify that we are interested in trees which use the symbol NP and then list several examples of pro"
P06-1111,C02-1145,0,0.101159,"Missing"
P06-1111,P02-1017,1,0.21244,"rees using a combination of unlabeled data and sparse prototypes. We first affirm the well-known result that simple, unconstrained PCFG induction produces grammars of poor quality as measured against treebank structures. We then augment a PCFG with prototype features, and show that these features, when propagated to non-prototype sequences using distributional similarity, are effective at learning bracket labels on fixed unlabeled trees, but are still not enough to learn good tree structures without bracketing information. Finally, we intersect the feature-augmented PCFG with the CCM model of Klein and Manning (2002), a highquality bracketing model. The intersected model is able to learn trees with higher unlabeled F1 than those in Klein and Manning (2004). More imporWe investigate prototype-driven learning for primarily unsupervised grammar induction. Prior knowledge is specified declaratively, by providing a few canonical examples of each target phrase type. This sparse prototype information is then propagated across a corpus using distributional similarity features, which augment an otherwise standard PCFG model. We show that distributional features are effective at distinguishing bracket labels, but n"
P06-1111,P04-1061,1,0.514807,"berkeley.edu Abstract must be provided before they can begin the construction of a treebank. In principle, prototypedriven learning is just a kind of semi-supervised learning. However, in practice, the information we provide is on the order of dozens of total seed instances, instead of a handful of fully parsed trees, and is of a different nature. The prototype-driven approach has three strengths. First, since we provide a set of target symbols, we can evaluate induced trees using standard labeled parsing metrics, rather than the far more forgiving unlabeled metrics described in, for example, Klein and Manning (2004). Second, knowledge is declaratively specified in an interpretable way (see figure 1). If a user of the system is unhappy with its systematic behavior, they can alter it by altering the prototype information (see section 7.1 for examples). Third, and related to the first two, one does not confuse the ability of the system to learn a consistent grammar with its ability to learn the grammar a user has in mind. In this paper, we present a series of experiments in the induction of labeled context-free trees using a combination of unlabeled data and sparse prototypes. We first affirm the well-known"
P06-1111,P92-1017,0,0.217732,"t this can, and does, result in mapping multiple model symbols to the most frequent target symbols. This experiment, labeled PCFG × NONE in figure 4, resulted in an average labeled F1 of 26.3 and an unlabeled F1 of 45.7. The unlabeled F1 is better than randomly choosing a tree (34.7), but not better than always choosing a right branching structure (61.7). Klein and Manning (2002) suggest that the task of labeling constituents is significantly easier than identifying them. Perhaps it is too much to ask a PCFG induction algorithm to perform both of these tasks simultaneously. Along the lines of Pereira and Schabes (1992), we reran the insideoutside algorithm, but this time placed zero mass on all trees which did not respect the bracketing of the gold trees. This constraint does not fully Experiments in PCFG induction As an initial experiment, we used the insideoutside algorithm to induce a PCFG in the straightforward way (Lari and Young, 1990; Manning and Sch¨utze, 1999). For all the experiments in this paper, we considered binary PCFGs over the nonterminals and terminals occuring in WSJ10. The PCFG rules were of the following forms: • X → Y Z, for nonterminal types X, Y, and Z, with Y 6= X or Z 6= X • X → t"
P06-1111,E95-1020,0,0.144173,"Missing"
P06-1111,J93-2004,0,\N,Missing
P06-1111,W00-0717,0,\N,Missing
P07-1003,H05-1011,0,0.0267717,"yntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. This dependence runs deep; for example, Galley et al. (2006) requires word alignments to project trees from the target language to the source, while Chiang (2005) requires alignments to induce grammar rules. Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by the now much-maligned AER metric. A host of discriminative methods have been introduced (Taskar et al., 2005; Moore, 2005; Ayan 17 Dan Klein Computer Science Division University of California, Berkeley klein@cs.berkeley.edu and Dorr, 2006). However, few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them (Cherry and Lin, 2006; Daum´e III and Marcu, 2005; Lopez and Resnik, 2005). We are particularly motivated by systems like the one described in Galley et al. (2006), which constructs translations using tree-to-string transducer rules. These rules are extracted from a bitext annotated with both English (target side) parses and word alignments."
P07-1003,P06-1121,0,0.360131,"word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our model’s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments. 1 Introduction Syntactic methods are an increasingly promising approach to statistical machine translation, being both algorithmically appealing (Melamed, 2004; Wu, 1997) and empirically successful (Chiang, 2005; Galley et al., 2006). However, despite recent progress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. This dependence runs deep; for example, Galley et al. (2006) requires word alignments to project trees from the target language to the source, while Chiang (2005) requires alignments to induce grammar rules. Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by the now much-maligned AER metric. A host of discriminative met"
P07-1003,N06-1014,1,0.924763,"ppealing (Melamed, 2004; Wu, 1997) and empirically successful (Chiang, 2005; Galley et al., 2006). However, despite recent progress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. This dependence runs deep; for example, Galley et al. (2006) requires word alignments to project trees from the target language to the source, while Chiang (2005) requires alignments to induce grammar rules. Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by the now much-maligned AER metric. A host of discriminative methods have been introduced (Taskar et al., 2005; Moore, 2005; Ayan 17 Dan Klein Computer Science Division University of California, Berkeley klein@cs.berkeley.edu and Dorr, 2006). However, few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them (Cherry and Lin, 2006; Daum´e III and Marcu, 2005; Lopez and Resnik, 2005). We are particularly motivated by systems like the one described in Galley et al. (2006), which constructs translations using tree-"
P07-1003,W05-0812,0,0.228491,". Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by the now much-maligned AER metric. A host of discriminative methods have been introduced (Taskar et al., 2005; Moore, 2005; Ayan 17 Dan Klein Computer Science Division University of California, Berkeley klein@cs.berkeley.edu and Dorr, 2006). However, few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them (Cherry and Lin, 2006; Daum´e III and Marcu, 2005; Lopez and Resnik, 2005). We are particularly motivated by systems like the one described in Galley et al. (2006), which constructs translations using tree-to-string transducer rules. These rules are extracted from a bitext annotated with both English (target side) parses and word alignments. Rules are extracted from target side constituents that can be projected onto contiguous spans of the source sentence via the word alignment. Constituents that project onto non-contiguous spans of the source sentence do not yield transducer rules themselves, and can only be incorporated into larger transducer rules. Thus, if the"
P07-1003,2004.tmi-1.5,0,0.0160442,"ntactic correspondences. We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our model’s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments. 1 Introduction Syntactic methods are an increasingly promising approach to statistical machine translation, being both algorithmically appealing (Melamed, 2004; Wu, 1997) and empirically successful (Chiang, 2005; Galley et al., 2006). However, despite recent progress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. This dependence runs deep; for example, Galley et al. (2006) requires word alignments to project trees from the target language to the source, while Chiang (2005) requires alignments to induce grammar rules. Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as"
P07-1003,C96-2141,0,0.880353,"uents that can be projected onto contiguous spans of the source sentence via the word alignment. Constituents that project onto non-contiguous spans of the source sentence do not yield transducer rules themselves, and can only be incorporated into larger transducer rules. Thus, if the word alignment of a sentence pair does not respect the constituent structure of the target sentence, then the minimal translation units must span large tree fragments, which do not generalize well. We present and evaluate an unsupervised word alignment model similar in character and computation to the HMM model (Ney and Vogel, 1996), but which incorporates a novel, syntax-aware distortion component which conditions on target language parse trees. These trees, while automatically generated and therefore imperfect, are nonetheless (1) a useful source of structural bias and (2) the same trees which constrain future stages of processing anyway. In our model, the trees do not rule out any alignments, but rather softly influence the probability of transitioning between alignment positions. In particular, transition probabilities condition upon paths through the target parse tree, allowing the model to prefer distortions which"
P07-1003,J03-1002,0,0.0137066,"es. 5. Null alignments are treated just as in the HMM model, incurring a fixed cost from any position. This model can be simplified by removing all conditioning on node types. However, we found this variant to slightly underperform the full model described above. Intuitively, types carry information about cross-linguistic ordering preferences. 3.2 Training Approach Because our model largely mirrors the generative process and structure of the original HMM model, we apply a nearly identical training procedure to fit the parameters to the training data via the Expectation-Maximization algorithm. Och and Ney (2003) gives a detailed exposition of the technique. In the E-step, we employ the forward-backward algorithm and current parameters to find expected counts for each potential pair of links in each training pair. In this familiar dynamic programming approach, we must compute the distortion probabilities for each pair of English positions. The minimal path between two leaves in a tree can be computed efficiently by first finding the path from the root to each leaf, then comparing those paths to find the nearest common ancestor and a path through it – requiring time linear in the height of the tree. Co"
P07-1003,P06-2014,0,0.46099,"(2005) requires alignments to induce grammar rules. Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by the now much-maligned AER metric. A host of discriminative methods have been introduced (Taskar et al., 2005; Moore, 2005; Ayan 17 Dan Klein Computer Science Division University of California, Berkeley klein@cs.berkeley.edu and Dorr, 2006). However, few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them (Cherry and Lin, 2006; Daum´e III and Marcu, 2005; Lopez and Resnik, 2005). We are particularly motivated by systems like the one described in Galley et al. (2006), which constructs translations using tree-to-string transducer rules. These rules are extracted from a bitext annotated with both English (target side) parses and word alignments. Rules are extracted from target side constituents that can be projected onto contiguous spans of the source sentence via the word alignment. Constituents that project onto non-contiguous spans of the source sentence do not yield transducer rules themselves, and can only be inc"
P07-1003,H05-1010,1,0.42338,"rogress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. This dependence runs deep; for example, Galley et al. (2006) requires word alignments to project trees from the target language to the source, while Chiang (2005) requires alignments to induce grammar rules. Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by the now much-maligned AER metric. A host of discriminative methods have been introduced (Taskar et al., 2005; Moore, 2005; Ayan 17 Dan Klein Computer Science Division University of California, Berkeley klein@cs.berkeley.edu and Dorr, 2006). However, few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them (Cherry and Lin, 2006; Daum´e III and Marcu, 2005; Lopez and Resnik, 2005). We are particularly motivated by systems like the one described in Galley et al. (2006), which constructs translations using tree-to-string transducer rules. These rules are extracted from a bitext annotated with both English (target side) parses and wor"
P07-1003,P05-1033,0,0.0490835,"unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our model’s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments. 1 Introduction Syntactic methods are an increasingly promising approach to statistical machine translation, being both algorithmically appealing (Melamed, 2004; Wu, 1997) and empirically successful (Chiang, 2005; Galley et al., 2006). However, despite recent progress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. This dependence runs deep; for example, Galley et al. (2006) requires word alignments to project trees from the target language to the source, while Chiang (2005) requires alignments to induce grammar rules. Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by the now much-maligned AER metric. A host"
P07-1003,J05-4004,0,0.0137143,"Missing"
P07-1003,J97-3002,0,0.115131,"ondences. We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our model’s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments. 1 Introduction Syntactic methods are an increasingly promising approach to statistical machine translation, being both algorithmically appealing (Melamed, 2004; Wu, 1997) and empirically successful (Chiang, 2005; Galley et al., 2006). However, despite recent progress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. This dependence runs deep; for example, Galley et al. (2006) requires word alignments to project trees from the target language to the source, while Chiang (2005) requires alignments to induce grammar rules. Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by"
P07-1003,J93-2003,0,\N,Missing
P07-1003,D07-1111,0,\N,Missing
P07-1003,P06-1002,0,\N,Missing
P07-1107,W99-0611,0,0.597116,"h higher than others). However, it is not clear this is a comparable number, due to the apparent use of gold NER features, which provide a strong clue to coreference. Regardless, it is unsurprising that their system, which has many rich features, would outperform ours. 854 H EAD E NT T YPE G ENDER N UMBER Bush: 1.0 AP: 1.0 viacom: 0.64, company: 0.36 teamsters: 0.22, union: 0.78, PERS MALE SG ORG NEUTER PL ORG NEUTER SG MISC NEUTER PL Table 3: Frequent entities occurring across documents along with head distribution and mode of property distributions. closest comparable unsupervised system is Cardie and Wagstaff (1999) who use pairwise NP distances to cluster document mentions. They report a 53.6 F1 on MUC6 when tuning distance metric weights to maximize F1 on the development set. 5.2 ACE 2004 We also performed experiments on ACE 2004 data. Due to licensing restrictions, we did not have access to the ACE 2004 formal development and test sets, and so the results presented are on the training sets. We report results on the newswire section (NWIRE in table 2b) and the broadcast news section (BNEWS in table 2b). These datasets include the prenominal mention type, which is not present in the MUC6 data. We treate"
P07-1107,N07-1030,0,0.176512,"he ACE 2004 training sets. Our relatively higher performance on Chinese compared to English is perhaps due to the lack of prenominal mentions in the Chinese data, as well as the presence of fewer pronouns compared to English. Our ACE results are difficult to compare exactly to previous work because we did not have access to the restricted formal test set. However, we can perform a rough comparison between our results on the training data (without coreference annotation) to supervised work which has used the same training data (with coreference annotation) and evaluated on the formal test set. Denis and Baldridge (2007) report 67.1 F1 and 69.2 F1 on the English NWIRE and BNEWS respectively using true mention boundaries. While our system underperforms the supervised systems, its accuracy is nonetheless promising. 6 6.1 Discussion Error Analysis The largest source of error in our system is between coreferent proper and nominal mentions. The most common examples of this kind of error are appositive usages e.g. George W. Bush, president of the US, visited Idaho. Another error of this sort can be seen in figure 2, where the corporation mention is not labeled coreferent with the The Weir Group mention. Examples su"
P07-1107,W98-1119,0,0.0610698,"hat the mention type M is conditioned on S as shown in figure 4. We note that correctly sampling an entity now requires that we incorporate terms for how a change will affect all future salience values. This changes our sampling equation for existing entities: P (Zi,j = z|Z−i,j ) ∝ nz Adding Salience We would like our model to capture how mention types are generated for a given entity in a robust and somewhat language independent way. The choice of entities may reasonably be considered to be independent given the mixing weights β, but how we realize an entity is strongly dependent on context (Ge et al., 1998). In order to capture this in our model, we enrich it as shown in figure 4. As we proceed through a 852 P (Mi,j 0 |Si,j 0 , Z) (2) j 0 ≥j where the product ranges over future mentions in the document and Si,j 0 is the value of future salience feature given the setting of all entities, including setting the current entity Zi,j to z. A similar equation holds for sampling a new entity. Note that, as discussed below, this full product can be truncated as an approximation. This model gives a 71.5 F1 on our development data. Table 1 shows the posterior distribution of the mention type given the sali"
P07-1107,P04-1018,0,0.402118,"onably close to our best MUC-6 number of 70.3 F1 . McCallum and Wellner (2004) also report a much lower 91.6 F1 on only proper nouns mentions. Our system achieves a 89.8 F1 when evaluation is restricted to only proper mentions.11 The et al. (2004). A mention is proper if it is annotated with NER information. It is a pronoun if the head is on the list of English pronouns. Otherwise, it is a nominal mention. Note we do not use the NER information for any purpose but determining whether the mention is proper. 11 The best results we know on the MUC-6 test set using the standard setting are due to Luo et al. (2004) who report a 81.3 F1 (much higher than others). However, it is not clear this is a comparable number, due to the apparent use of gold NER features, which provide a strong clue to coreference. Regardless, it is unsurprising that their system, which has many rich features, would outperform ours. 854 H EAD E NT T YPE G ENDER N UMBER Bush: 1.0 AP: 1.0 viacom: 0.64, company: 0.36 teamsters: 0.22, union: 0.78, PERS MALE SG ORG NEUTER PL ORG NEUTER SG MISC NEUTER PL Table 3: Frequent entities occurring across documents along with head distribution and mode of property distributions. closest comparab"
P07-1107,P02-1014,0,0.231687,"ell as the sequential anaphoric structure within each document. While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state. Despite being unsupervised, our system achieves a 70.3 MUC F1 measure on the MUC-6 test set, broadly in the range of some recent supervised results. 1 Introduction applies discriminative learning methods to pairs of mentions, using features which encode properties such as distance, syntactic environment, and so on (Soon et al., 2001; Ng and Cardie, 2002). Although such approaches have been successful, they have several liabilities. First, rich features require plentiful labeled data, which we do not have for coreference tasks in most domains and languages. Second, coreference is inherently a clustering or partitioning task. Naive pairwise methods can and do fail to produce coherent partitions. One classic solution is to make greedy left-to-right linkage decisions. Recent work has addressed this issue in more global ways. McCallum and Wellner (2004) use graph partioning in order to reconcile pairwise scores into a final coherent clustering. No"
P07-1107,J01-4004,0,0.774485,"cross a corpus as well as the sequential anaphoric structure within each document. While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state. Despite being unsupervised, our system achieves a 70.3 MUC F1 measure on the MUC-6 test set, broadly in the range of some recent supervised results. 1 Introduction applies discriminative learning methods to pairs of mentions, using features which encode properties such as distance, syntactic environment, and so on (Soon et al., 2001; Ng and Cardie, 2002). Although such approaches have been successful, they have several liabilities. First, rich features require plentiful labeled data, which we do not have for coreference tasks in most domains and languages. Second, coreference is inherently a clustering or partitioning task. Naive pairwise methods can and do fail to produce coherent partitions. One classic solution is to make greedy left-to-right linkage decisions. Recent work has addressed this issue in more global ways. McCallum and Wellner (2004) use graph partioning in order to reconcile pairwise scores into a final c"
P07-1107,M95-1005,0,0.978408,"tmost mention token, and the mention type was automatically detected. We will not assume any other information to be present in the data beyond the text itself. In particular, unlike much related work, we do not assume gold named entity recognition (NER) labels; indeed we do not assume observed NER labels or POS tags at all. Our pri849 α β α φ K K Zi Hi β φ ∞ ∞ Zi Hi J I (a) J I (b) Figure 1: Graphical model depiction of document level entity models described in sections 3.1 and 3.2 respectively. The shaded nodes indicate observed variables. mary performance metric will be the MUC F1 measure (Vilain et al., 1995), commonly used to evaluate coreference systems on a within-document basis. Since our system relies on sampling, all results are averaged over five random runs. 3 Coreference Resolution Models In this section, we present a sequence of generative coreference resolution models for document corpora. All are essentially mixture models, where the mixture components correspond to entities. As far as notation, we assume a collection of I documents, each with Ji mentions. We use random variables Z to refer to (indices of) entities. We will use φz to denote the parameters for an entity z, and φ to refe"
P08-1088,N04-4038,0,0.00685771,"ogously. The increased accuracy may not be an accident: whether two words are translations is perhaps better characterized directly by how close their latent concepts are, whereas log-probability is more sensitive to perturbations in the source and target spaces. 774 Lexicon Note that the although the corpora here are derived from a parallel corpus, there are no parallel sentences. 8 LDC catalog # 2002E18. 9 LDC catalog # 2004E13. 10 These corpora contain no parallel sentences. 11 We use the Tree Tagger (Schmid, 1994) for all POS tagging except for Arabic, where we use the tagger described in Diab et al. (2004). Setting p0.1 p0.25 p0.33 p0.50 Best-F1 E DIT D IST O RTHO C ONTEXT MCCA 58.6 76.0 91.1 87.2 62.6 81.3 81.3 89.7 61.1 80.1 80.2 89.0 —52.3 65.3 89.7 47.4 55.0 58.0 72.0 1 EN-ES-P EN-ES-W 0.95 Precision 0.9 0.85 0.8 0.75 Table 1: Performance of E DIT D IST and our model with various features sets on EN-ES-W. See section 5. 0.7 0.65 0.6 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Figure 3: Example precision/recall curve of our system on EN-ES-P and EN-ES-W settings. See section 6.1. all languages pairs except English-Arabic, we extract evaluation lexicons from the Wiktionary online dictionary. As"
P08-1088,W95-0114,0,0.842841,"s. We present the highest confidence system predictions, where the only editing done is to ignore predictions which consist of identical source and target words. D and EN-FR-D, presumably due in part to the lack of orthographic features. However, MCCA still achieved surprising precision at lower recall levels. For instance, at p0.1 , MCCA yielded 60.1 and 70.0 on Chinese and Arabic, respectively. Figure 3 shows the highest-confidence outputs in several languages. 6.4 Comparison To Previous Work There has been previous work in extracting translation pairs from non-parallel corpora (Rapp, 1995; Fung, 1995; Koehn and Knight, 2002), but generally not in as extreme a setting as the one considered here. Due to unavailability of data and specificity in experimental conditions and evaluations, it is not possible to perform exact comparisons. How(a) Example Non-Cognate Pairs health traceability youth report advantages salud rastreabilidad juventud informe ventajas (b) Interesting Incorrect Pairs liberal Kirkhope action Albanians a.m. Netherlands partido Gorsel reacci´ on Bosnia horas Breta˜ na Table 4: System analysis on EN-ES-W: (a) non-cognate pairs proposed by our system, (b) hand-selected represe"
P08-1088,P06-1121,0,0.0217612,"Missing"
P08-1088,W02-0902,0,0.871975,"ihood weights with a simple proxy, the distances between the words’ mean latent concepts: wi,j = A − ||zi∗ − zj∗ ||2 , • EN-ES-P: 1st 100k sentences of text from the parallel English and Spanish Europarl corpus (Koehn, 2005). • EN-CH-D: English: 1st 50k sentences of Xinhua parallel news corpora;8 Chinese: 2nd 50k sentences. • EN-AR-D: English: 1st 50k sentences of 1994 proceedings of UN parallel corpora;9 Arabic: 2nd 50k sentences. • EN-ES-G: English: 100k sentences of English Gigaword; Spanish: 100k sentences of Spanish Gigaword.10 Each experiment requires a lexicon for evaluation. Following Koehn and Knight (2002), we consider lexicons over only noun word types, although this is not a fundamental limitation of our model. We consider a word type to be a noun if its most common tag is a noun in our monolingual corpus.11 For 7 (5) where A is a thresholding constant, zi∗ = E(zi,j |fS (si )) = P 1/2 US> fS (si ), and zj∗ is defined analogously. The increased accuracy may not be an accident: whether two words are translations is perhaps better characterized directly by how close their latent concepts are, whereas log-probability is more sensitive to perturbations in the source and target spaces. 774 Lexicon"
P08-1088,koen-2004-pharaoh,0,0.0112554,"Missing"
P08-1088,2005.mtsummit-papers.11,0,0.0928857,"plicit use is ever made of document or sentence-level alignments. In particular, our method is robust to permutations of the sentences in the corpora. 4 4.2 Experimental Setup In section 5, we present developmental experiments in English-Spanish lexicon induction; experiments 6 Empirically, we obtained much better efficiency and even increased accuracy by replacing these marginal likelihood weights with a simple proxy, the distances between the words’ mean latent concepts: wi,j = A − ||zi∗ − zj∗ ||2 , • EN-ES-P: 1st 100k sentences of text from the parallel English and Spanish Europarl corpus (Koehn, 2005). • EN-CH-D: English: 1st 50k sentences of Xinhua parallel news corpora;8 Chinese: 2nd 50k sentences. • EN-AR-D: English: 1st 50k sentences of 1994 proceedings of UN parallel corpora;9 Arabic: 2nd 50k sentences. • EN-ES-G: English: 100k sentences of English Gigaword; Spanish: 100k sentences of Spanish Gigaword.10 Each experiment requires a lexicon for evaluation. Following Koehn and Knight (2002), we consider lexicons over only noun word types, although this is not a fundamental limitation of our model. We consider a word type to be a noun if its most common tag is a noun in our monolingual co"
P08-1088,P95-1050,0,0.779727,"inese systems. We present the highest confidence system predictions, where the only editing done is to ignore predictions which consist of identical source and target words. D and EN-FR-D, presumably due in part to the lack of orthographic features. However, MCCA still achieved surprising precision at lower recall levels. For instance, at p0.1 , MCCA yielded 60.1 and 70.0 on Chinese and Arabic, respectively. Figure 3 shows the highest-confidence outputs in several languages. 6.4 Comparison To Previous Work There has been previous work in extracting translation pairs from non-parallel corpora (Rapp, 1995; Fung, 1995; Koehn and Knight, 2002), but generally not in as extreme a setting as the one considered here. Due to unavailability of data and specificity in experimental conditions and evaluations, it is not possible to perform exact comparisons. How(a) Example Non-Cognate Pairs health traceability youth report advantages salud rastreabilidad juventud informe ventajas (b) Interesting Incorrect Pairs liberal Kirkhope action Albanians a.m. Netherlands partido Gorsel reacci´ on Bosnia horas Breta˜ na Table 4: System analysis on EN-ES-W: (a) non-cognate pairs proposed by our system, (b) hand-sele"
P08-1088,P06-1072,0,0.0065989,"rd types. If too few word types are matched, learning will not progress quickly; if too many are matched, the model will be swamped with noise. We found that it was helpful to explicitly control the number of edges. Thus, we adopt a bootstrapping-style approach that only permits high confidence edges at first, and then slowly permits more over time. In particular, we compute the optimal full matching, but only retain the highest weighted edges. As we run EM, we gradually increase the number of edges to retain. In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995). • EN-ES(FR)-D: English: 1st 50k sentences of Europarl; Spanish (French): 2nd 50k sentences of Europarl.7 Note that even when corpora are derived from parallel sources, no explicit use is ever made of document or sentence-level alignments. In particular, our method is robust to permutations of the sentences in the corpora. 4 4.2 Experimental Setup In section 5, we present developmental exp"
P08-1088,P95-1026,0,0.225036,"Thus, we adopt a bootstrapping-style approach that only permits high confidence edges at first, and then slowly permits more over time. In particular, we compute the optimal full matching, but only retain the highest weighted edges. As we run EM, we gradually increase the number of edges to retain. In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995). • EN-ES(FR)-D: English: 1st 50k sentences of Europarl; Spanish (French): 2nd 50k sentences of Europarl.7 Note that even when corpora are derived from parallel sources, no explicit use is ever made of document or sentence-level alignments. In particular, our method is robust to permutations of the sentences in the corpora. 4 4.2 Experimental Setup In section 5, we present developmental experiments in English-Spanish lexicon induction; experiments 6 Empirically, we obtained much better efficiency and even increased accuracy by replacing these marginal likelihood weights with a simple proxy, th"
P08-1100,W01-0713,0,0.0668972,"ding language processing systems such as parsers from limited resources. Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsup"
P08-1100,P07-1094,0,0.0318791,"ency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsupervised induction systems in general. In this paper, we take a step back and present a more statistical view of unsupervised learning in the context of grammar induction. We identify four types of error that a system can make:"
P08-1100,P05-1046,1,0.824376,"(x) 6= pθ0 (x) for every θ, θ0 ∈ S where θ 6= θ0 .7 In general, identifiability error is incurred when the set of maximizers of E log pθ (x) is non-identifiable.8 Label symmetry is perhaps the most familiar example of non-identifiability and is intrinsic to models with hidden labels (HMM and PCFG, but not DMV). We can permute the hidden labels without changing the objective function or even the nature of the solution, so there is no reason to prefer one permutation over another. While seemingly benign, this symmetry actually presents a serious challenge in measuring discrepancy (Section 5.1). Grenager et al. (2005) augments an HMM to allow emission from a generic stopword distribution at any position with probability q. Their model would definitely not be identifiable if q were a free parameter, since we can set q to 0 and just mix in the stopword distribution with each of the other emission distributions to obtain a different parameter setting yielding the same overall distribution. This is a case where our notion of desired structure is absent in the likelihood, and a prior over parameters could help break ties. Permutation-invariant distance KL-divergence is a natural measure of discrepancy between t"
P08-1100,D07-1031,0,0.063917,"l optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsupervised induction systems in general. In this paper, we take a step back and present a more statistical view of unsupervised learning in the context of grammar induction. We identify four types of error that a system can make: approximation,"
P08-1100,P04-1061,1,0.965617,"processing systems such as parsers from limited resources. Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsupervised induction systems"
P08-1100,J94-2001,0,0.404717,"enerative training) or even conditional likelihood E log pθ (y |x) (discriminative training). In the remaining sections, we try to study each of the four errors in isolation. In practice, since it is difficult to work with some of the parameter settings that participate in the error decomposition, we use computationally feasible surrogates so that the error under study remains the dominant effect. 4 Approximation error We start by analyzing approximation error, the discrepancy between p∗ and pθ1∗ (the model found by optimizing likelihood), a point which has been dis881 cussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 θˆgen = ˆ log pθ (x, y), which acts as a surrogate argmaxθ E ∗ for p . As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly). We believe that the initial iterations of EM contain valuable information about the incorrect biases of these models. However, EM is changing hundr"
P08-1100,P92-1017,0,0.323103,"iated by increasing the number of training examples. We conduct our analyses on three models: the HMM, the PCFG, and a simple dependency model. 1 Introduction The unsupervised induction of linguistic structure from raw text is an important problem both for understanding language acquisition and for building language processing systems such as parsers from limited resources. Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induc"
P08-1100,P05-1044,0,0.344424,"s parsers from limited resources. Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsupervised induction systems in general. In this paper"
P08-1100,P06-1072,0,0.0348674,"ammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsupervised induction systems in general. In this paper, we take a step back and present a more statistical view o"
P08-2007,W07-0403,0,0.0660862,"finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient. 1 2 Phrase Alignment Problems Rather than focus on a particular model, we describe four problems that arise in training phrase alignment models. 2.1 Introduction Learning in phrase alignment models generally requires computing either Viterbi phrase alignments or expectations of alignment links. For some restricted combinatorial spaces of alignments—those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al., 2004)—inference can be accomplished using polynomial time dynamic programs. However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. Indeed, Marcu and Wong (2002) conjectures that none exist. In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard. On the other hand, we give a compact formulation of Viterbi infe"
P08-2007,W06-3105,1,0.812183,"hrase Alignment Problems Rather than focus on a particular model, we describe four problems that arise in training phrase alignment models. 2.1 Introduction Learning in phrase alignment models generally requires computing either Viterbi phrase alignments or expectations of alignment links. For some restricted combinatorial spaces of alignments—those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al., 2004)—inference can be accomplished using polynomial time dynamic programs. However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. Indeed, Marcu and Wong (2002) conjectures that none exist. In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard. On the other hand, we give a compact formulation of Viterbi inference as an integer linear program (ILP). Using this formulation, exact solutions to the Viterbi search problem can be found by highly optimized, general purpose ILP solvers. While ILP is of course also NP-ha"
P08-2007,N03-1017,0,0.024153,"ctive phrase alignments:     G G A= a: eij = e ; fkl = f   (eij ,fkl )∈a (eij ,fkl )∈a 1 As in parsing, the position between each word is assigned an index, where 0 is to the left of the first word. In this paper, we assume all phrases have length at least one: j &gt; i and l &gt; k. 25 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 25–28, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Both the conditional model of DeNero et al. (2006) and the joint model of Marcu and Wong (2002) operate in A, as does the phrase-based decoding framework of Koehn et al. (2003). 2.3 Problem Definitions For a weighted sentence pair (e, f, φ), let the score of an alignment be the product of its link scores: Y φ(eij , fkl ). φ(a) = (eij ,fkl )∈a Four related problems involving scored alignments arise when training phrase alignment models. O PTIMIZATION, O: Given (e, f, φ), find the highest scoring alignment a. D ECISION, D: Given (e, f, φ), decide if there is an alignment a with φ(a) ≥ 1. O arises in the popular Viterbi approximation to EM (Hard EM) that assumes probability mass is concentrated at the mode of the posterior distribution over alignments. D is the corresp"
P08-2007,P07-2045,0,0.00901026,"Missing"
P08-2007,P06-1096,1,0.224556,"Missing"
P08-2007,W02-1018,0,0.459285,"lly quite efficient. 1 2 Phrase Alignment Problems Rather than focus on a particular model, we describe four problems that arise in training phrase alignment models. 2.1 Introduction Learning in phrase alignment models generally requires computing either Viterbi phrase alignments or expectations of alignment links. For some restricted combinatorial spaces of alignments—those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al., 2004)—inference can be accomplished using polynomial time dynamic programs. However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. Indeed, Marcu and Wong (2002) conjectures that none exist. In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard. On the other hand, we give a compact formulation of Viterbi inference as an integer linear program (ILP). Using this formulation, exact solutions to the Viterbi search problem can be found by highly optimized, general purpose ILP solvers. While IL"
P08-2007,C04-1030,0,0.0539567,"teger linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient. 1 2 Phrase Alignment Problems Rather than focus on a particular model, we describe four problems that arise in training phrase alignment models. 2.1 Introduction Learning in phrase alignment models generally requires computing either Viterbi phrase alignments or expectations of alignment links. For some restricted combinatorial spaces of alignments—those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al., 2004)—inference can be accomplished using polynomial time dynamic programs. However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. Indeed, Marcu and Wong (2002) conjectures that none exist. In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard. On the other hand, we give a compact formulation of Viterbi inference as an integer linear program (ILP). Using"
P09-1011,D07-1071,0,0.0893892,"esent a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty—Robocup sportscasting, weather forecasts (a new domain), and NFL recaps. 1 Dan Klein UC Berkeley klein@cs.berkeley.edu Introduction Recent work in learning semantics has focused on mapping sentences to meaning representations (e.g., some logical form) given aligned sentence/meaning pairs as training data (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Lu et al., 2008). However, this degree of supervision is unrealistic for modeling human language acquisition and can be costly to obtain for building large-scale, broadcoverage language understanding systems. A more flexible direction is grounded language acquisition: learning the meaning of sentences in the context of an observed world state. The grounded approach has gained interest in various disciplines (Siskind, 1996; Yu and Ballard, 2004; Feldman and Narayanan, 2004; Gorniak and Roy, 2007). Some recent work in the NLP community has also moved in this direction by relaxing the amount of"
P09-1011,H05-1042,0,0.108796,"occur in multiple records, so there is still uncertainty about which record is referenced by a given sentence. NFL Recaps In this domain, each scenario represents a single NFL football game (see Figure 1(c) for an example). The world state (the things that happened during the game) is represented by database tables, e.g., scoring summary, team comparison, drive chart, play-by-play, etc. Each record is a database entry, for instance, the receiving statistics for a certain player. The text is the recap of the game— an article summarizing the game highlights. The dataset we used was collected by Barzilay and Lapata (2005). The data includes 466 games during the 2003–2004 NFL season. 78 of these games were annotated by Snyder and Barzilay (2007), who aligned each sentence to a set of records. This domain is by far the most complicated of the three. Many records corresponding to inconsequential game statistics are not mentioned. Conversely, the text contains many general remarks (e.g., it was just that type of game) which are not present in any of the records. Furthermore, the complexity of the language used in the recap is far greater than what we can represent us3 Generative Model To learn the correspondence b"
P09-1011,J93-2003,0,0.0224271,". To quantify the benefits of incorporating these two aspects, we compare our full model with two simpler variants. Table 2: Highest probability words for the categorical field skyCover.mode in the weather domain. It is interesting to note that skyCover=75-100 is so highly correlated with rain that the model learns to connect an overcast sky in the world to the indication of rain in the text. • Model 1 (no model of segmentation or coherence): Each record is chosen independently; each record generates one field, and each field generates one word. This model is similar in spirit to IBM model 1 (Brown et al., 1993). separate multinomial distribution over words from which w is drawn. An example of a categorical field is skyCover.mode in the weather domain, which has four values: 0-25, 25-50, 50-75, and 75-100. Table 2 shows the top words for each of these field values learned by our model. • Model 2 (models segmentation but not coherence): Records and fields are still generated independently, but each field can now generate multiple words. 4 • Model 3 (our full model of segmentation and coherence): Records and fields are generated according to the Markov chains described in Section 3. Learning and Infere"
P09-1011,D08-1033,1,0.619859,"Missing"
P09-1011,D08-1035,0,0.00830371,"consequential game statistics are not mentioned. Conversely, the text contains many general remarks (e.g., it was just that type of game) which are not present in any of the records. Furthermore, the complexity of the language used in the recap is far greater than what we can represent us3 Generative Model To learn the correspondence between a text w and a world state s, we propose a generative model p(w |s) with latent variables specifying this correspondence. Our model combines segmentation with alignment. The segmentation aspect of our model is similar to that of Grenager et al. (2005) and Eisenstein and Barzilay (2008), but in those two models, the segments are clustered into topics rather than grounded to a world state. The alignment aspect of our model is similar to the HMM model for word alignment (Ney and Vogel, 1996). DeNero et al. (2008) perform joint segmentation and word alignment for machine translation, but the nature of that task is different from ours. The model is defined by a generative process, 93 def which proceeds in three stages (Figure 2 shows the corresponding graphical model): where s(t) = {r ∈ s : r.t = t} and r0 .t is a dedicated START record type.2 We also model the transition of the"
P09-1011,W05-0602,0,0.0650037,"h degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty—Robocup sportscasting, weather forecasts (a new domain), and NFL recaps. 1 Dan Klein UC Berkeley klein@cs.berkeley.edu Introduction Recent work in learning semantics has focused on mapping sentences to meaning representations (e.g., some logical form) given aligned sentence/meaning pairs as training data (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Lu et al., 2008). However, this degree of supervision is unrealistic for modeling human language acquisition and can be costly to obtain for building large-scale, broadcoverage language understanding systems. A more flexible direction is grounded language acquisition: learning the meaning of sentences in the context of an observed world state. The grounded approach has gained interest in various disciplines (Siskind, 1996; Yu and Ballard, 2004; Feldman and Narayanan, 2004; Gorniak and Roy, 2007). Some recent work in the NLP commun"
P09-1011,P05-1046,1,0.205742,"records corresponding to inconsequential game statistics are not mentioned. Conversely, the text contains many general remarks (e.g., it was just that type of game) which are not present in any of the records. Furthermore, the complexity of the language used in the recap is far greater than what we can represent us3 Generative Model To learn the correspondence between a text w and a world state s, we propose a generative model p(w |s) with latent variables specifying this correspondence. Our model combines segmentation with alignment. The segmentation aspect of our model is similar to that of Grenager et al. (2005) and Eisenstein and Barzilay (2008), but in those two models, the segments are clustered into topics rather than grounded to a world state. The alignment aspect of our model is similar to the HMM model for word alignment (Ney and Vogel, 1996). DeNero et al. (2008) perform joint segmentation and word alignment for machine translation, but the nature of that task is different from ours. The model is defined by a generative process, 93 def which proceeds in three stages (Figure 2 shows the corresponding graphical model): where s(t) = {r ∈ s : r.t = t} and r0 .t is a dedicated START record type.2"
P09-1011,N06-1014,1,0.612603,"e predictions made by each of our three models for a particular example. Model 1 easily mistakes pink10 for the recipient of a pass record because decisions are made independently for each word. Model 2 chooses the correct record, but having no model of the field structure inside a record, it proposes an incorrect field segmentation (although our evaluation is insensitive to this). Equipped with the ability to prefer a coherent field sequence, Model 3 fixes these errors. Many of the remaining errors are due to the garbage collection phenomenon familiar from word alignment models (Moore, 2004; Liang et al., 2006). For example, the ballstopped record occurs frequently but is never mentioned in the text. At the same time, there is a correlation between ballstopped and utterances such as pink2 holds onto the ball, which are not aligned to any record in the annotation. As a result, our model incorrectly chooses to align the two. tialize the parameters of the latter.6 We also prohibited utterances in Models 2 and 3 from crossing punctuation during inference. Table 5 shows that performance improves substantially in the more sophisticated models, the gains being greater than in the Robocup domain. Figure 5 s"
P09-1011,D08-1082,0,0.166727,"imultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty—Robocup sportscasting, weather forecasts (a new domain), and NFL recaps. 1 Dan Klein UC Berkeley klein@cs.berkeley.edu Introduction Recent work in learning semantics has focused on mapping sentences to meaning representations (e.g., some logical form) given aligned sentence/meaning pairs as training data (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Lu et al., 2008). However, this degree of supervision is unrealistic for modeling human language acquisition and can be costly to obtain for building large-scale, broadcoverage language understanding systems. A more flexible direction is grounded language acquisition: learning the meaning of sentences in the context of an observed world state. The grounded approach has gained interest in various disciplines (Siskind, 1996; Yu and Ballard, 2004; Feldman and Narayanan, 2004; Gorniak and Roy, 2007). Some recent work in the NLP community has also moved in this direction by relaxing the amount of supervision to th"
P09-1011,P04-1066,0,0.00918886,"er look at the predictions made by each of our three models for a particular example. Model 1 easily mistakes pink10 for the recipient of a pass record because decisions are made independently for each word. Model 2 chooses the correct record, but having no model of the field structure inside a record, it proposes an incorrect field segmentation (although our evaluation is insensitive to this). Equipped with the ability to prefer a coherent field sequence, Model 3 fixes these errors. Many of the remaining errors are due to the garbage collection phenomenon familiar from word alignment models (Moore, 2004; Liang et al., 2006). For example, the ballstopped record occurs frequently but is never mentioned in the text. At the same time, there is a correlation between ballstopped and utterances such as pink2 holds onto the ball, which are not aligned to any record in the annotation. As a result, our model incorrectly chooses to align the two. tialize the parameters of the latter.6 We also prohibited utterances in Models 2 and 3 from crossing punctuation during inference. Table 5 shows that performance improves substantially in the more sophisticated models, the gains being greater than in the Roboc"
P09-1011,C96-2141,0,0.0633055,"language used in the recap is far greater than what we can represent us3 Generative Model To learn the correspondence between a text w and a world state s, we propose a generative model p(w |s) with latent variables specifying this correspondence. Our model combines segmentation with alignment. The segmentation aspect of our model is similar to that of Grenager et al. (2005) and Eisenstein and Barzilay (2008), but in those two models, the segments are clustered into topics rather than grounded to a world state. The alignment aspect of our model is similar to the HMM model for word alignment (Ney and Vogel, 1996). DeNero et al. (2008) perform joint segmentation and word alignment for machine translation, but the nature of that task is different from ours. The model is defined by a generative process, 93 def which proceeds in three stages (Figure 2 shows the corresponding graphical model): where s(t) = {r ∈ s : r.t = t} and r0 .t is a dedicated START record type.2 We also model the transition of the final record type to a designated STOP record type in order to capture regularities about the types of records which are described last. More sophisticated models of coherence could also be employed here (B"
P09-1011,J08-1001,0,\N,Missing
P09-1104,P06-1002,0,0.100247,"d or d d, which are rendered as English indefinite determiners. The right-hand three columns in Table 2 present supervised results on our Chinese English data set using block features. We note that almost all of our performance gains (relative to both the HMM and 1-1 matchings) come from BITG and block features. The maximum likelihood-trained normal form ITG model outperforms the HMM, even without including any features derived from the unlabeled data. Once we include the posteriors of the HMM as a feature, the AER decreases to 14.4. The previous best AER result on this data set is 15.9 from Ayan and Dorr (2006), who trained stacked neural networks based on GIZA++ alignments. Our results are not directly comparable (they used more labeled data, but did not have the HMM posteriors as an input feature). 6.3 Rec 84 77 80 83 Translations Rules BLEU 1.9M 23.22 4.0M 23.05 3.8M 24.28 4.2M 24.32 Table 3: Results on the NIST MT05 Chinese-English test set show that our ITG alignments yield improvements in translation quality. thresholding (DeNero and Klein, 2007). The ITG Viterbi alignments are the Viterbi output of the ITG model with all features, trained to maximize log likelihood. The ITG Posterior alignmen"
P09-1104,P06-2014,0,0.677855,"97) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG 923 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923–931, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP in O(n3 ) time using a bipartite matching algorithm (Kuhn, 1955).1 On the other hand, summing over A1-1 is #P -hard (Valiant, 1979). Initially, we consider heuristic alignment potentials given by Dice coefficients structured alignments (i.e. phrases), which general matchings cannot efficiently do. The need for block alignments is es"
P09-1104,W07-0403,0,0.759433,"tences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG 923 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923–9"
P09-1104,D08-1024,0,0.00973797,"as follows, log Pw (a|x) = w·φ(x,a)−log a∈A max a∗ In our data sets, many are not in A1-1 (and thus not in AIT G ), implying the minimum infamily loss must exceed 0. Since MIRA operates in an online fashion, this can cause severe stability problems. On the Hansards data, the simple averaging technique described by Collins (2002) yields a reasonable model. On the Chinese NIST data, however, where almost no alignment is in A1-1 , the update rule from Equation (2) is completely unstable, and even the averaged model does not yield high-quality results. We instead use a variant of MIRA similar to Chiang et al. (2008). First, rather than update towards the hand-labeled alignment a∗ , we update towards an alignment which achieves minimal loss within the family.4 We call this bestin-class alignment a∗p . Second, we perform lossˆ. This yields the augmented inference to obtain a modified QP, s.t. w · ˆ) + ≥ w · φ(x, a exp(w·φ(x,a0 )) where the log-denominator represents a sum over the alignment family A. This alignment probability only places mass on members of A. The likelihood objective is given by, (2) ˆ = arg max wt · φ(x, a) where a φ(x, a∗p ) X a0 ∈A ˆ) + L(ˆ s.t. w · φ(x, a∗ ) ≥ w · φ(x, a a, a∗ ) wt+1"
P09-1104,W02-1001,0,0.106614,"mmer et al., 2006). MIRA is an online procedure, where at each time step t + 1, we update our weights as follows: wt+1 = argminw ||w − wt ||22 4 An alternative to margin-based training is a likelihood objective, which learns a conditional alignment distribution Pw (a|x) parametrized as follows, log Pw (a|x) = w·φ(x,a)−log a∈A max a∗ In our data sets, many are not in A1-1 (and thus not in AIT G ), implying the minimum infamily loss must exceed 0. Since MIRA operates in an online fashion, this can cause severe stability problems. On the Hansards data, the simple averaging technique described by Collins (2002) yields a reasonable model. On the Chinese NIST data, however, where almost no alignment is in A1-1 , the update rule from Equation (2) is completely unstable, and even the averaged model does not yield high-quality results. We instead use a variant of MIRA similar to Chiang et al. (2008). First, rather than update towards the hand-labeled alignment a∗ , we update towards an alignment which achieves minimal loss within the family.4 We call this bestin-class alignment a∗p . Second, we perform lossˆ. This yields the augmented inference to obtain a modified QP, s.t. w · ˆ) + ≥ w · φ(x, a exp(w·φ("
P09-1104,P07-1003,1,0.743967,"s our best model without external alignment models and the second row includes features from the jointly trained HMM. Under likelihood, BITG-S uses the simple grammar (Section 2.2). BITG-N uses the normal form grammar (Section 4.1). 6.2 (2005); we compute external features from the same unlabeled data, 1.1 million sentence pairs. Our second is the Chinese-English hand-aligned portion of the 2002 NIST MT evaluation set. This dataset has 491 sentences, which we split into a training set of 150 and a test set of 191. When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary. For likelihood based models, we set the L2 regularization parameter, σ 2 , to 100 and the threshold for posterior decoding to 0.33. We report results using the simple ITG grammar (ITG-S, Section 2.2) where summing over derivations double counts alignments, as well as the normal form ITG grammar (ITG-N,Section 4.1) which does not double count. We ran our annealed lossaugmented MIRA for 15 iterations, beginning with λ at 0 and increasing it linearly to 0.5. We compute Viterbi alignments using the averaged weight vector from this procedure. 6.1 Chinese NIST R"
P09-1104,P08-2007,1,0.0414573,"ng the same heuristic Dice potentials on the Hansards test set, the maximal scoring alignment from AIT G yields 28.4 AER—2.4 better than A1-1 —indicating that ITG can be beneficial as a constraint on heuristic alignments. 2.3 Block ITG An important alignment pattern disallowed by A1-1 is the many-to-one alignment block. While not prevalent in our hand-aligned French Hansards dataset, blocks occur frequently in our handaligned Chinese-English NIST data. Figure 1 contains an example. Extending A1-1 to include blocks is problematic, because finding a maximal 1-1 matching over phrases is NP-hard (DeNero and Klein, 2008). With ITG, it is relatively easy to allow contiguous many-to-one alignment blocks without added complexity.3 This is accomplished by adding additional unary terminal productions aligning a foreign phrase to a single English terminal or vice versa. We will use BITG to refer to this block ITG variant and ABIT G to refer to the alignment family, which is neither contained in nor contains A1-1 . For this alignment family, we expand the alignment potential decomposition in Equation (1) to incorporate block potentials sef and sef which represent English and foreign many-to-one alignment blocks, res"
P09-1104,N09-1026,1,0.810211,"ate more than 8 of these high-precision alignments. Our second pruning technique is to prune all one-by-one (word-to-word) bitext cells that have a posterior below 10−4 in both HMM models. Pruning a one-by-one cell also indirectly prunes larger cells containing it. To take maximal advantage of this indirect pruning, we avoid explicitly attempting to build each cell in the dynamic program. Instead, we track bounds on the spans for which we have successfully built ITG cells, and we only iterate over larger spans that fall within those bounds. The details of a similar bounding approach appear in DeNero et al. (2009). In all, pruning reduces MIRA iteration time from 175 to 5 minutes on the NIST ChineseEnglish dataset with negligible performance loss. Likelihood training time is reduced by nearly two orders of magnitude. Se ne est pas suﬃsant d gh ou en o go t no is at Th Figure 3: Often, the gold alignment a∗ isn’t in our alignment family, here ABIT G . For the likelihood objective (Section 4.2), we maximize the probability of the set M(a∗ ) consisting of alignments ABIT G which achieve minimal loss relative to a∗ . In this example, the minimal loss is 1, and we have a choice of removing either of the sur"
P09-1104,P08-1112,0,0.0283754,"ord side of a block is. The final block feature type consists of phrase shape features. These are designed as follows: For each word in a potential many-to-one block alignment, we map an individual word to X if it is not one of the 25 most frequent words. Some example features of this type are, French Hansards Results The French Hansards data are well-studied data sets for discriminative word alignment (Taskar et al., 2005; Cherry and Lin, 2006; Lacoste-Julien et al., 2006). For this data set, it is not clear that improving alignment error rate beyond that of GIZA++ is useful for translation (Ganchev et al., 2008). Table 1 illustrates results for the Hansards data set. The first row uses dice and the same distance features as Taskar et al. (2005). The first two rows repeat the experiments of Taskar et al. (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. The last row includes the posterior of the jointly-trained HMM of Liang et al. (2006) as a feature. This model alone achieves an AER of 5.4. No model significantly improves over the HMM alone, which is consistent with the results of Taskar et al. (2005). 929 Alignments Model Prec GIZA++ 62 Join"
P09-1104,N06-1015,1,0.455258,"l as pointwise mutual information statistics for the multi-word parts of many-to-one blocks. These features capture roughly how “coherent” the multi-word side of a block is. The final block feature type consists of phrase shape features. These are designed as follows: For each word in a potential many-to-one block alignment, we map an individual word to X if it is not one of the 25 most frequent words. Some example features of this type are, French Hansards Results The French Hansards data are well-studied data sets for discriminative word alignment (Taskar et al., 2005; Cherry and Lin, 2006; Lacoste-Julien et al., 2006). For this data set, it is not clear that improving alignment error rate beyond that of GIZA++ is useful for translation (Ganchev et al., 2008). Table 1 illustrates results for the Hansards data set. The first row uses dice and the same distance features as Taskar et al. (2005). The first two rows repeat the experiments of Taskar et al. (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. The last row includes the posterior of the jointly-trained HMM of Liang et al. (2006) as a feature. This model alone achieves an AER of 5.4. No model si"
P09-1104,W08-0402,0,0.0139488,"aligners under simple and normal form grammars. We showed that through the combination of relaxed learning objectives, many-to-one block alignment potential, and efficient pruning, ITG models can yield state-of-the art word alignments, even when the underlying gold alignments are highly nonITG. Our models yielded the lowest published error for Chinese-English alignment and an increase in downstream translation performance. End-To-End MT Experiments We further evaluated our alignments in an end-toend Chinese to English translation task using the publicly available hierarchical pipeline JosHUa (Li and Khudanpur, 2008). The pipeline extracts a Hiero-style synchronous context-free grammar (Chiang, 2007), employs suffix-array based rule extraction (Lopez, 2007), and tunes model parameters with minimum error rate training (Och, 2003). We trained on the FBIS corpus using sentences up to length 40, which includes 2.7 million English words. We used a 5-gram language model trained on 126 million words of the Xinhua section of the English Gigaword corpus, estimated with SRILM (Stolcke, 2002). We tuned on 300 sentences of the NIST MT04 test set. Results on the NIST MT05 test set appear in Table 3. We compared four s"
P09-1104,N06-1014,1,0.70755,"grammar (Section 4.1). For MIRA (Viterbi inference), the highest-scoring alignment is the same, regardless of grammar. ∗ Target Alignments M(a ) Gold Alignment a∗ lihood uses the inside-outside algorithm for computing cell posteriors. Exhaustive computation of these quantities requires an O(n6 ) dynamic program that is prohibitively slow even on small supervised training sets. However, most of the search space can safely be pruned using posterior predictions from a simpler alignment models. We use posteriors from two jointly estimated HMM models to make pruning decisions during ITG inference (Liang et al., 2006). Our first pruning technique is broadly similar to Cherry and Lin (2007a). We select high-precision alignment links from the HMM models: those word pairs that have a posterior greater than 0.9 in either model. Then, we prune all bitext cells that would invalidate more than 8 of these high-precision alignments. Our second pruning technique is to prune all one-by-one (word-to-word) bitext cells that have a posterior below 10−4 in both HMM models. Pruning a one-by-one cell also indirectly prunes larger cells containing it. To take maximal advantage of this indirect pruning, we avoid explicitly a"
P09-1104,D07-1104,0,0.0187893,"al, and efficient pruning, ITG models can yield state-of-the art word alignments, even when the underlying gold alignments are highly nonITG. Our models yielded the lowest published error for Chinese-English alignment and an increase in downstream translation performance. End-To-End MT Experiments We further evaluated our alignments in an end-toend Chinese to English translation task using the publicly available hierarchical pipeline JosHUa (Li and Khudanpur, 2008). The pipeline extracts a Hiero-style synchronous context-free grammar (Chiang, 2007), employs suffix-array based rule extraction (Lopez, 2007), and tunes model parameters with minimum error rate training (Och, 2003). We trained on the FBIS corpus using sentences up to length 40, which includes 2.7 million English words. We used a 5-gram language model trained on 126 million words of the Xinhua section of the English Gigaword corpus, estimated with SRILM (Stolcke, 2002). We tuned on 300 sentences of the NIST MT04 test set. Results on the NIST MT05 test set appear in Table 3. We compared four sets of alignments. The GIZA++ alignments7 are combined across directions with the grow-diag-final heuristic, which outperformed the union. The"
P09-1104,W03-0301,0,0.0226162,"d-based objective is that we can obtain posteriors over individual alignment cells, Pw ((i, j)|x) = X Pw (a|x) a∈A:(i,j)∈a We obtain posterior ITG alignments by including all alignment cells (i, j) such that Pw ((i, j)|x) exceeds a fixed threshold t. Posterior thresholding allows us to easily trade-off precision and recall in our alignments by raising or lowering t. 5 6 Dynamic Program Pruning Alignment Quality Experiments We present results which measure the quality of our models on two hand-aligned data sets. Our first is the English-French Hansards data set from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). Here we use the same 337/100 train/test split of the labeled data as Taskar et al. Both discriminative methods require repeated model inference: MIRA depends upon lossaugmented Viterbi parsing, while conditional like6 Note that alignments that achieve the minimal loss would not introduce any alignments not either sure or possible, so it suffices to keep track only of the number of sure recall errors. 928 Features Dice, dist, blcks, dict, lex +HMM P 1-1 R AER P 85.7 90.5 63.7 69.4 26.8 21.2 86.2 91.2 MIRA ITG R AER 65.8 70.1 25.2 20.3 BITG R AER P 85.0 90.2 73.3 80.1 21.1 15.0 P 85.7 87.3 Lik"
P09-1104,P06-1065,0,0.452034,"Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG 923 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923–931, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP in O(n3 ) time using a bipartite matching algorithm (Kuhn, 1955).1 On the other hand, summing over A1-1 is #P -hard (Valiant, 1979). Initially, we consider heuristic alignment potentials given by Dice coefficients structured alignments (i.e. phrases), which general matchings"
P09-1104,P03-1021,0,0.0204384,"ments, even when the underlying gold alignments are highly nonITG. Our models yielded the lowest published error for Chinese-English alignment and an increase in downstream translation performance. End-To-End MT Experiments We further evaluated our alignments in an end-toend Chinese to English translation task using the publicly available hierarchical pipeline JosHUa (Li and Khudanpur, 2008). The pipeline extracts a Hiero-style synchronous context-free grammar (Chiang, 2007), employs suffix-array based rule extraction (Lopez, 2007), and tunes model parameters with minimum error rate training (Och, 2003). We trained on the FBIS corpus using sentences up to length 40, which includes 2.7 million English words. We used a 5-gram language model trained on 126 million words of the Xinhua section of the English Gigaword corpus, estimated with SRILM (Stolcke, 2002). We tuned on 300 sentences of the NIST MT04 test set. Results on the NIST MT05 test set appear in Table 3. We compared four sets of alignments. The GIZA++ alignments7 are combined across directions with the grow-diag-final heuristic, which outperformed the union. The joint HMM alignments are generated from competitive posterior References"
P09-1104,D08-1012,1,0.831177,"ne block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG 923 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923–931, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP in O(n3 ) time using a bipartite matchi"
P09-1104,H05-1010,1,0.548785,"ments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG 923 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923–931, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP in O(n3 ) time using a bipartite matching algorithm (Kuhn, 1955).1 On the other hand, summing over A1-1 is #P -hard (Valiant, 1979). Initially, we consider heuristic alignment potentials given by Dice coefficients structured alignments (i.e. phrases), wh"
P09-1104,J97-3002,0,0.871532,"lihood models. A major challenge in both cases is that our training alignments are often not one-to-one ITG alignments. Under such conditions, directly training to maximize margin is unstable, and training to maximize likelihood is ill-defined, since the target alignment derivations don’t exist in our hypothesis class. We show how to adapt both margin and likelihood objectives to learn good ITG aligners. In the case of likelihood training, two innovations are presented. The simple, two-rule ITG grammar exponentially over-counts certain alignment structures relative to others. Because of this, Wu (1997) and Zens and Ney (2003) introduced a normal form ITG which avoids this over-counting. We extend this normal form to null productions and give the first extensive empirical comparison of simple and normal form ITGs, for posterior decoding under our likelihood models. Additionally, we show how to deal with training instances where the gold alignments are outside of the hypothesis class by instead optimizing the likelihood of a set of minimum-loss alignments. Perhaps the greatest advantage of ITG models is that they straightforwardly permit blockThis work investigates supervised word alignment m"
P09-1104,P03-1019,0,0.717345,"A major challenge in both cases is that our training alignments are often not one-to-one ITG alignments. Under such conditions, directly training to maximize margin is unstable, and training to maximize likelihood is ill-defined, since the target alignment derivations don’t exist in our hypothesis class. We show how to adapt both margin and likelihood objectives to learn good ITG aligners. In the case of likelihood training, two innovations are presented. The simple, two-rule ITG grammar exponentially over-counts certain alignment structures relative to others. Because of this, Wu (1997) and Zens and Ney (2003) introduced a normal form ITG which avoids this over-counting. We extend this normal form to null productions and give the first extensive empirical comparison of simple and normal form ITGs, for posterior decoding under our likelihood models. Additionally, we show how to deal with training instances where the gold alignments are outside of the hypothesis class by instead optimizing the likelihood of a set of minimum-loss alignments. Perhaps the greatest advantage of ITG models is that they straightforwardly permit blockThis work investigates supervised word alignment methods that exploit inve"
P09-1104,P05-1059,0,0.319588,"er allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG 923 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of"
P09-1104,P08-1012,0,0.381786,"gnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG 923 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923–931, c Suntec, Singapor"
P09-1104,J00-2004,0,\N,Missing
P09-1104,J07-2003,0,\N,Missing
P09-1108,J93-2004,0,0.0310236,"This triggering is similar to the “lazy frontier” used by Huang and Chiang (2005). All of our experiments use this lazy representation. 3 Experiments 3.1 State-Split Grammars We performed our first experiments with the grammars of Petrov et al. (2006). The training procedure for these grammars produces a hierarchy of increasingly refined grammars through statesplitting. We followed Pauls and Klein (2009) in computing heuristics for the most refined grammar from outside scores for less-split grammars. We used the Berkeley Parser4 to learn such grammars from Sections 2-21 of the Penn Treebank (Marcus et al., 1993). We trained with 6 split-merge cycles, producing 7 grammars. We tested these grammars on 100 sentences of length at most 30 of Section 23 of the Treebank. Our “target grammar” was in all cases the most split grammar. 4 963 http://berkeleyparser.googlecode.com 25000 EXH 15000 K Best Bottom-up 0 5000 Items pushed (millions) 5000 15000 K Best Bottom-up Heuristic 0 Items pushed (millions) 25000 KA* 0 2000 4000 6000 8000 10000 0 2000 4000 6000 k 8000 10000 k Figure 3: The cost of k-best extraction as a function of k for state-split grammars, for both KA∗ and EXH. The amount of time spent in the k-"
P09-1108,N06-1020,0,0.0298459,"e giving the speed-ups of A∗ methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A∗ parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types. 1 Introduction Many situations call for a parser to return the kbest parses rather than only the 1-best. Uses for k-best lists include minimum Bayes risk decoding (Goodman, 1998; Kumar and Byrne, 2004), discriminative reranking (Collins, 2000; Charniak and Johnson, 2005), and discriminative training (Och, 2003; McClosky et al., 2006). The most efficient known algorithm for k-best parsing (Jim´enez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005). In this paper, we propose an extension of A∗ parsing which integrates k-best search with an A∗ based exploration of the 1-best chart. A∗ parsing can avoid significant amounts of computation by guiding 1-best search with heuristic estimates of parse completion costs, and has been applied successfully in several"
P09-1108,J03-1006,0,0.171136,"stimates of parse completion costs, and has been applied successfully in several domains (Klein and Manning, 2002; Klein and Manning, 2003c; Haghighi et al., 2007). Our algorithm extends the speedups achieved in the 1-best case to the k-best case and is optimal under the same conditions as a stan2 A k-Best A∗ Parsing Algorithm We build up to our full algorithm in several stages, beginning with standard 1-best A∗ parsing and making incremental modifications. 2.1 Parsing as Weighted Deduction Our algorithm can be formulated in terms of prioritized weighted deduction rules (Shieber et al., 1995; Nederhof, 2003; Felzenszwalb and McAllester, 2007). A prioritized weighted deduction rule has the form p(w1 ,...,wn ) φ1 : w1 , . . . , φn : wn −−−−−−−−→ φ0 : g(w1 , . . . , wn ) where φ1 , . . . , φn are the antecedent items of the deduction rule and φ0 is the conclusion item. A deduction rule states that, given the antecedents φ1 , . . . , φn with weights w1 , . . . , wn , the conclusion φ0 can be formed with weight g(w1 , . . . , wn ) and priority p(w1 , . . . , wn ). 958 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 958–966, c Suntec, Singapore, 2-7 August 2009"
P09-1108,P03-1021,0,0.00332833,"action while giving the speed-ups of A∗ methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A∗ parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types. 1 Introduction Many situations call for a parser to return the kbest parses rather than only the 1-best. Uses for k-best lists include minimum Bayes risk decoding (Goodman, 1998; Kumar and Byrne, 2004), discriminative reranking (Collins, 2000; Charniak and Johnson, 2005), and discriminative training (Och, 2003; McClosky et al., 2006). The most efficient known algorithm for k-best parsing (Jim´enez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005). In this paper, we propose an extension of A∗ parsing which integrates k-best search with an A∗ based exploration of the 1-best chart. A∗ parsing can avoid significant amounts of computation by guiding 1-best search with heuristic estimates of parse completion costs, and has been applied"
P09-1108,N09-1063,1,0.914386,"computed priority than D(A, i, j, l, r, u − 1, v) and D(A, i, j, l, r, u, v − 1) (Jim´enez and Marzal, 2000). So, we can wait until one of the latter two is built before “triggering” the construction of the former. This triggering is similar to the “lazy frontier” used by Huang and Chiang (2005). All of our experiments use this lazy representation. 3 Experiments 3.1 State-Split Grammars We performed our first experiments with the grammars of Petrov et al. (2006). The training procedure for these grammars produces a hierarchy of increasingly refined grammars through statesplitting. We followed Pauls and Klein (2009) in computing heuristics for the most refined grammar from outside scores for less-split grammars. We used the Berkeley Parser4 to learn such grammars from Sections 2-21 of the Penn Treebank (Marcus et al., 1993). We trained with 6 split-merge cycles, producing 7 grammars. We tested these grammars on 100 sentences of length at most 30 of Section 23 of the Treebank. Our “target grammar” was in all cases the most split grammar. 4 963 http://berkeleyparser.googlecode.com 25000 EXH 15000 K Best Bottom-up 0 5000 Items pushed (millions) 5000 15000 K Best Bottom-up Heuristic 0 Items pushed (millions)"
P09-1108,P06-1055,1,0.650413,"k done by the algorithm of Huang and Chiang (2005). Our algorithm is also equivalent to standard A∗ parsing (up to ties) if it is terminated after the 1-best derivation is found. Finally, our algorithm can be written down in terms of deduction rules, and thus falls into the well-understood view of parsing as weighted deduction (Shieber et al., 1995; Goodman, 1998; Nederhof, 2003). In addition to presenting the algorithm, we show experiments in which we extract k-best lists for three different kinds of grammars: the lexicalized grammars of Klein and Manning (2003b), the state-split grammars of Petrov et al. (2006), and the tree transducer grammars of Galley et al. (2006). We demonstrate that optimal k-best lists can be extracted significantly faster using our algorithm than with previous methods. Abstract A∗ parsing makes 1-best search efficient by suppressing unlikely 1-best items. Existing kbest extraction methods can efficiently search for top derivations, but only after an exhaustive 1-best pass. We present a unified algorithm for k-best A∗ parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A∗ methods. Our algorithm produces optimal k-best parses under the sam"
P09-1108,P05-1022,0,0.0617403,"t A∗ parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A∗ methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A∗ parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types. 1 Introduction Many situations call for a parser to return the kbest parses rather than only the 1-best. Uses for k-best lists include minimum Bayes risk decoding (Goodman, 1998; Kumar and Byrne, 2004), discriminative reranking (Collins, 2000; Charniak and Johnson, 2005), and discriminative training (Och, 2003; McClosky et al., 2006). The most efficient known algorithm for k-best parsing (Jim´enez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005). In this paper, we propose an extension of A∗ parsing which integrates k-best search with an A∗ based exploration of the 1-best chart. A∗ parsing can avoid significant amounts of computation by guiding 1-best search with heuristic estimates of pars"
P09-1108,N04-1035,0,0.0104349,"ical to the number popped by EXH in our experiments (both algorithms have the same ordering bounds on which derivation items are popped). The only real difference between the algorithms in this limited case is that EXH places k-best items on local priority queues per edge, while KA∗ makes use of one global queue. Thus, in addition to providing a method for speeding up k-best extraction with A∗ , our algorithm also provides an alternate form of Huang and Chiang (2005)’s k-best extraction that can be phrased in a weighted deduction system. Tree Transducer Grammars Syntactic machine translation (Galley et al., 2004) uses tree transducer grammars to translate sentences. Transducer rules are synchronous contextfree productions that have both a source and a target side. We examine the cost of k-best parsing in the source side of such grammars with KA∗ , which can be a first step in translation. We extracted a grammar from 220 million words of Arabic-English bitext using the approach of Galley et al. (2006), extracting rules with at most 3 non-terminals. These rules are highly lexicalized. About 300K rules are applicable for a typical 30-word sentence; we filter the rest. We tested on 100 sentences of length"
P09-1108,P06-1121,0,0.0891466,"gorithm is also equivalent to standard A∗ parsing (up to ties) if it is terminated after the 1-best derivation is found. Finally, our algorithm can be written down in terms of deduction rules, and thus falls into the well-understood view of parsing as weighted deduction (Shieber et al., 1995; Goodman, 1998; Nederhof, 2003). In addition to presenting the algorithm, we show experiments in which we extract k-best lists for three different kinds of grammars: the lexicalized grammars of Klein and Manning (2003b), the state-split grammars of Petrov et al. (2006), and the tree transducer grammars of Galley et al. (2006). We demonstrate that optimal k-best lists can be extracted significantly faster using our algorithm than with previous methods. Abstract A∗ parsing makes 1-best search efficient by suppressing unlikely 1-best items. Existing kbest extraction methods can efficiently search for top derivations, but only after an exhaustive 1-best pass. We present a unified algorithm for k-best A∗ parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A∗ methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A∗ parser"
P09-1108,N07-1052,1,0.843921,"enez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005). In this paper, we propose an extension of A∗ parsing which integrates k-best search with an A∗ based exploration of the 1-best chart. A∗ parsing can avoid significant amounts of computation by guiding 1-best search with heuristic estimates of parse completion costs, and has been applied successfully in several domains (Klein and Manning, 2002; Klein and Manning, 2003c; Haghighi et al., 2007). Our algorithm extends the speedups achieved in the 1-best case to the k-best case and is optimal under the same conditions as a stan2 A k-Best A∗ Parsing Algorithm We build up to our full algorithm in several stages, beginning with standard 1-best A∗ parsing and making incremental modifications. 2.1 Parsing as Weighted Deduction Our algorithm can be formulated in terms of prioritized weighted deduction rules (Shieber et al., 1995; Nederhof, 2003; Felzenszwalb and McAllester, 2007). A prioritized weighted deduction rule has the form p(w1 ,...,wn ) φ1 : w1 , . . . , φn : wn −−−−−−−−→ φ0 : g(w1"
P09-1108,W05-1506,0,0.797958,"ns required for optimality in a 1-best A∗ parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types. 1 Introduction Many situations call for a parser to return the kbest parses rather than only the 1-best. Uses for k-best lists include minimum Bayes risk decoding (Goodman, 1998; Kumar and Byrne, 2004), discriminative reranking (Collins, 2000; Charniak and Johnson, 2005), and discriminative training (Och, 2003; McClosky et al., 2006). The most efficient known algorithm for k-best parsing (Jim´enez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005). In this paper, we propose an extension of A∗ parsing which integrates k-best search with an A∗ based exploration of the 1-best chart. A∗ parsing can avoid significant amounts of computation by guiding 1-best search with heuristic estimates of parse completion costs, and has been applied successfully in several domains (Klein and Manning, 2002; Klein and Manning, 2003c; Haghighi et al., 2007). Our algorithm extends"
P09-1108,W01-1812,1,0.813667,"plied by consistency is admissibility, which states that the heuristic never overestimates the true Viterbi outside score for an edge, i.e. h(e) ≤ α(e). For the remainder of this paper, we will assume our heuristics are consistent. 2.3 A Naive k-Best A∗ Algorithm Due to the optimal substructure of 1-best PCFG derivations, a 1-best parser searches over the space of edges; this is the essence of 1-best dynamic programming. Although most edges can be built 1 While we present the algorithm specialized to parsing with a PCFG, it generalizes to a wide range of hypergraph search problems as shown in Klein and Manning (2001). 959 Inside Edge Deductions (Used in A∗ and KA∗ ) IN: I(B, i, l) : w1 w1 +w2 +wr +h(A,i,j) −−−−−−−−−−−−−→ I(C, l, j) : w2 I(A, i, j) w1 + w2 + wr : Table 1: The deduction schema (IN) for building inside edge items, using a supplied heuristic. This schema is sufficient on its own for 1-best A∗ , and it is used in KA∗ . Here, r is the rule A → B C. Inside Derivation Deductions (Used in NAIVE) DERIV: D(TB , i, l) : w1 D(TC , l, j) : w2 w1 +w2 +wr +h(A,i,j) −−−−−−−−−−−−−→ ! A , i, j D TB : w1 + w2 + wr TC Table 2: The deduction schema for building derivations, using a supplied heuristic. TB and T"
P09-1108,P03-1054,1,0.404224,"e in the k-best phase is no more than the amount of work done by the algorithm of Huang and Chiang (2005). Our algorithm is also equivalent to standard A∗ parsing (up to ties) if it is terminated after the 1-best derivation is found. Finally, our algorithm can be written down in terms of deduction rules, and thus falls into the well-understood view of parsing as weighted deduction (Shieber et al., 1995; Goodman, 1998; Nederhof, 2003). In addition to presenting the algorithm, we show experiments in which we extract k-best lists for three different kinds of grammars: the lexicalized grammars of Klein and Manning (2003b), the state-split grammars of Petrov et al. (2006), and the tree transducer grammars of Galley et al. (2006). We demonstrate that optimal k-best lists can be extracted significantly faster using our algorithm than with previous methods. Abstract A∗ parsing makes 1-best search efficient by suppressing unlikely 1-best items. Existing kbest extraction methods can efficiently search for top derivations, but only after an exhaustive 1-best pass. We present a unified algorithm for k-best A∗ parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A∗ methods. Our al"
P09-1108,N03-1016,1,0.540153,"e in the k-best phase is no more than the amount of work done by the algorithm of Huang and Chiang (2005). Our algorithm is also equivalent to standard A∗ parsing (up to ties) if it is terminated after the 1-best derivation is found. Finally, our algorithm can be written down in terms of deduction rules, and thus falls into the well-understood view of parsing as weighted deduction (Shieber et al., 1995; Goodman, 1998; Nederhof, 2003). In addition to presenting the algorithm, we show experiments in which we extract k-best lists for three different kinds of grammars: the lexicalized grammars of Klein and Manning (2003b), the state-split grammars of Petrov et al. (2006), and the tree transducer grammars of Galley et al. (2006). We demonstrate that optimal k-best lists can be extracted significantly faster using our algorithm than with previous methods. Abstract A∗ parsing makes 1-best search efficient by suppressing unlikely 1-best items. Existing kbest extraction methods can efficiently search for top derivations, but only after an exhaustive 1-best pass. We present a unified algorithm for k-best A∗ parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A∗ methods. Our al"
P09-1108,N04-1022,0,\N,Missing
P09-2036,N09-1026,1,0.882842,"inarization, we transform the translation forest into n-ary form. In the n-ary forest, each hyperedge corresponds to an original ! correspond to original grammar rule, and all nodes grammar symbols, rather than those introduced during binarizaiton. Transforming the entire forest to n-ary form is intractable, however, because the number of hyperedges would be exponential in n. Instead, we include only the top k n-ary backtraces for each forest node. These backtraces can be enumerated efficiently from the binary forest. Figure 2(b) illustrates the result. For efficiency, we follow DeNero et al. (2009) in pruning low-scoring nodes in the n-ary forest under the weighted translation grammar. We use a max-marginal threshold to prune unlikely nodes, which can be computed through a maxsum semiring variant of inside-outside (Goodman, 1996; Petrov and Klein, 2007). Forest reranking with a language model can be performed over this n-ary forest using the cube growing algorithm of Huang and Chiang (2007). Cube growing lazily builds k-best lists of derivations at each node in the forest by filling a nodespecific priority queue upon request from the parent. N -ary forest reranking serves as our baselin"
P09-2036,N06-1033,0,0.123236,"al. (2009) describe normal forms particularly suited to transducer grammars, demonstrating that well-chosen binarizations admit cubic-time parsing algorithms while introducing very few intermediate grammar symbols. Binarization choice can also improve monolingual parsing efficiency (Song et al., 2008). The parsing stage of our decoder proceeds by first converting the source-side projection of the translation grammar into lexical normal form (DeNero et al., 2009), which allows each rule to be applied to any span in linear time, then build2.3 Reranking with Target-Side Binarization Zhang et al. (2006) demonstrate that reranking over binarized derivations improves search accuracy by better exploring the space of translations within the strict confines of beam search. Binarizing the forest during reranking permits pairs of adjacent non-terminals in the target-side projection of rules to be rescored at intermediate forest nodes. This target-side binarization can be performed onthe-fly: when a node Pij is queried for its k-best list, we binarize its n-ary backtraces. Suppose Pij can be constructed from a rule r with target-side projection P → `0 C1 `1 C2 `2 . . . Cn `n where C1 , . . . , Cn ar"
P09-2036,N07-1051,1,\N,Missing
P09-2036,D08-1018,0,\N,Missing
P09-2036,J09-4009,0,\N,Missing
P09-2036,W07-0405,0,\N,Missing
P09-2036,J97-3002,0,\N,Missing
P09-2036,P07-1019,0,\N,Missing
P09-2036,P06-1121,0,\N,Missing
P09-2036,P96-1024,0,\N,Missing
P10-1105,D07-1093,1,0.740234,"Missing"
P10-1105,P07-3005,0,0.171353,"ly differing in only a single character. However, no linguist would posit that these two words are related. To correctly learn that they are not related, linguists typically rely on two kinds of evidence. First, because sound change is largely regular, we would need to commonly see /i/ in Greek wherever we see /a/ in Malay (Ross, 1950). Second, we should look at languages closely related to Greek and Malay, to see if similar patterns hold there, too. Some authors have attempted to automatically detect cognate words (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 2001; Mulloni, 2007), but these methods 1030 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1030–1039, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics typically work on language pairs rather than on larger language families. To fully automate the comparative method, it is necessary to consider multiple languages, and to do so in a model which couples cognate detection with similarity learning. In this paper, we present a new generative model for the automatic induction of cognate groups given only (1) a known family tree of languag"
P10-1105,N09-1008,1,0.730519,"Missing"
P10-1105,P07-1009,0,0.095837,"Missing"
P10-1105,N09-1067,0,0.0597064,"Missing"
P10-1105,D09-1011,0,0.131221,"transition types (insertion, substitution/match, deletion) are normalized for each ancestral phoneme. 5 Transducers and Automata In our model, it is not just the edit distances that are finite state machines. Indeed, the words themselves are string-valued random variables that have, in principle, an infinite domain. To represent distributions and messages over these variables, we chose weighted finite state automata, which can compactly represent functions over strings. Unfortunately, while initially compact, these automata become unwieldy during inference, and so approximations must be used (Dreyer and Eisner, 2009). In this section, we summarize the standard algorithms and representations used for weighted finite state transducers. For more detailed treatment of the general transducer operations, we direct readers to Mohri (2009). A weighted automaton (resp. transducer) encodes a function over strings (resp. pairs of strings) as weighted paths through a directed graph. Each edge in the graph has a real-valued weight4 and a label, which is a single phoneme in some alphabet Σ or the empty phoneme ε (resp. pair of labels in some alphabet Σ×∆). The weight of a string is then the sum of all paths through the"
P10-1105,P02-1001,0,0.0167819,". With this distribution, we calculate the expected “alignment unigrams.” That is, for each pair of phonemes x and y (or empty phoneme ε), we need to find the quantity: Ep(wa ,wd ) [#(x, y; z)] = X X #(x,y; z)p(z|wa , wd )p(wa , wd ) wa ,wd z∈ align(wa ,wd ) where we denote #(x, y; z) to be the number of times the pair of phonemes (x, y) are aligned in alignment z. The exact method for computing these counts is to use an expectation semiring (Eisner, 2001). Given the expected counts, we now need to normalize them to ensure that the transducer represents a conditional probability distribution (Eisner, 2002; Oncina and Sebban, 2006). We have that, for each phoneme x in the ancestor language: E[#(ε, y; z)] E[#(·, ·; z)] X E[#(x, y; z)] σ(x, y) = (1 − ηy 0 ) E[#(x, ·; z)] 0 ηy = y δx = (1 − X y0 ηy 0 ) E[#(x, ε; z)] E[#(x, ·; z)] P Here, we haveP#(·, ·; z) = x,y #(x, y; Pz) and #(x, ·; z) = #(x, y; z). The (1 − y y 0 ηy 0 ) term P ensure P that for any ancestral phoneme x, η + y y y σ(x, y)+δx = 1. These equations ensure that the three transition types (insertion, substitution/match, deletion) are normalized for each ancestral phoneme. 5 Transducers and Automata In our model, it is not just the ed"
P10-1105,N03-2016,0,0.350374,"of languages, averaged across pairs of words. The other is accuExact Match 48.1 35.4 37.2 43.0 68.6 0.1 38.7 90.3 26.2 26.5 56.8 0.0 11.3 86.6 Table 1: Accuracies for reconstructing cognate groups. Levenshtein refers to fixed parameter edit distance transducer. Learned refers to automatically learned edit distances. Pairwise Accuracy means averaged on each word pair; Exact Match refers to percentage of completely and accurately reconstructed groups. For a description of the baseline, see Section 7.1. (3) Dice’s coefficients are commonly used in bilingual detection of cognates (Kondrak, 2001; Kondrak et al., 2003). We follow prior work and use sets of bigrams within words. In our case, during bipartite matching the set X is the set of bigrams in the language being re-permuted, and Y is the union of bigrams in the other languages. 7.2 Transducers Levenshtein Levenshtein Levenshtein Learned Learned Learned Heuristic Baseline Model Messages Unigrams Bigrams Anch. Unigrams Unigrams Bigrams Anch. Unigrams Pairwise Acc. Transducers Levenshtein Learned Prec. Heuristic Baseline 49.0 Model Messages Anch. Unigrams 86.5 Anch. Unigrams 66.9 Recall F1 43.5 46.1 36.1 82.0 50.9 73.6 Table 2: Accuracies for reconstruc"
P10-1105,D09-1005,0,0.019087,"Wd = wd and that p(WdP= wd |wa ) = P 1 for all P ancestral words wa . Then, clearly wd µ(wd ) = p(Wd = wd wd |wa ) = ∞ whenever there are an infinite number of possible ancestral strings wa . 6 This approach is reminiscent of Expectation Propagation (Minka, 2001). 1035 as follows. Given a deterministic prior automaton τ , and a deterministic automaton topology µ ˜∗ , we create the composed unweighted automaton τ ◦µ ˜∗ , and calculate arc transitions weights to minimize the KL divergence between that composed transducer and τ ◦ µ. The procedure for calculating these statistics is described in Li and Eisner (2009), which amounts to using an expectation semiring (Eisner, 2001) to compute expected transitions in τ ◦ µ ˜∗ under the probability distribution τ ◦ µ. From there, we need to create the automaton −1 τ ◦τ ◦µ ˜. That is, we need to divide out the influence of τ (w). Since we know the topology and arc weights for τ ahead of time, this is often as simple as dividing arc weights in τ ◦ µ ˜ by the corresponding arc weight in τ (w). For example, if τ encodes a geometric distribution over word lengths and a uniform distribution over phonemes (that is, τ (w) ∝ p|w |), then computing µ ˜ is as sim7 ple as"
P10-1105,J94-3004,0,0.264111,"tch as cognates: they are nearly identical, essentially differing in only a single character. However, no linguist would posit that these two words are related. To correctly learn that they are not related, linguists typically rely on two kinds of evidence. First, because sound change is largely regular, we would need to commonly see /i/ in Greek wherever we see /a/ in Malay (Ross, 1950). Second, we should look at languages closely related to Greek and Malay, to see if similar patterns hold there, too. Some authors have attempted to automatically detect cognate words (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 2001; Mulloni, 2007), but these methods 1030 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1030–1039, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics typically work on language pairs rather than on larger language families. To fully automate the comparative method, it is necessary to consider multiple languages, and to do so in a model which couples cognate detection with similarity learning. In this paper, we present a new generative model for the automatic induction of cognate groups g"
P10-1105,N01-1020,0,0.138089,"be a highly attractive match as cognates: they are nearly identical, essentially differing in only a single character. However, no linguist would posit that these two words are related. To correctly learn that they are not related, linguists typically rely on two kinds of evidence. First, because sound change is largely regular, we would need to commonly see /i/ in Greek wherever we see /a/ in Malay (Ross, 1950). Second, we should look at languages closely related to Greek and Malay, to see if similar patterns hold there, too. Some authors have attempted to automatically detect cognate words (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 2001; Mulloni, 2007), but these methods 1030 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1030–1039, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics typically work on language pairs rather than on larger language families. To fully automate the comparative method, it is necessary to consider multiple languages, and to do so in a model which couples cognate detection with similarity learning. In this paper, we present a new generative model for the automatic indu"
P10-1105,H01-1035,0,\N,Missing
P10-1105,N01-1014,0,\N,Missing
P10-1112,E93-1006,0,0.901614,"tantially more complicated state-of-theart lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding. 1 Introduction Modern NLP systems have increasingly used dataintensive models that capture many or even all substructures from the training data. In the domain of syntactic parsing, the idea that all training fragments1 might be relevant to parsing has a long history, including tree-substitution grammar (data-oriented parsing) approaches (Scha, 1990; Bod, 1993; Goodman, 1996a; Chiang, 2003) and tree kernel approaches (Collins and Duffy, 2002). For machine translation, the key modern advancement has been the ability to represent and memorize large training substructures, be it in contiguous phrases (Koehn et al., 2003) or syntactic trees 1 In this paper, a fragment means an elementary tree in a tree-substitution grammar, while a subtree means a fragment that bottoms out in terminals. (Galley et al., 2004; Chiang, 2005; Deneefe and Knight, 2009). In all such systems, a central challenge is efficiency: there are generally a combinatorial number of sub"
P10-1112,P01-1010,0,0.925161,"avoid selection of fragments, and work with all fragments. Of course, having a grammar that includes all training substructures is only desirable to the extent that those structures can be appropriately weighted. Implicit representations like those used here do not allow arbitrary weightings of fragments. However, we use a simple weighting scheme which does decompose appropriately over the implicit encoding, and which is flexible enough to allow weights to depend not only on frequency but also on fragment size, node patterns, and certain lexical properties. Similar ideas have been explored in Bod (2001), Collins and Duffy (2002), and Goodman (2003). Our model empirically affirms the effectiveness of such a flexible weighting scheme in full-scale experiments. We also investigate parsing without an explicit lexicon. The all-fragments approach has the advantage that parsing down to the character level requires no special treatment; we show that an explicit lexicon is not needed when sentences are considered as strings of characters rather than words. This avoids the need for complex unknown word models and other specialized lexical resources. The main contribution of this work is to show practi"
P10-1112,P05-1022,0,0.121447,"Missing"
P10-1112,W98-1115,0,0.0767748,"d (PT) -13 Figure 5: Effect of coarse-pass pruning on parsing accuracy (WSJ, training ≤ 20 words, tested on dev-set ≤ 20 words). This graph shows that the fortuitous improvement due to pruning is very small and that the peak accuracy is almost equal to the accuracy without pruning (the dotted line). from no pruning to pruning with a −6.2 log posterior threshold.10 Figure 4 depicts the variation in parsing accuracies in response to the amount of pruning done by the coarse-pass. Higher posterior pruning thresholds induce more aggressive pruning. Here, we observe an effect seen in previous work (Charniak et al. (1998), Petrov and Klein (2007), Petrov et al. (2008)), that a certain amount of pruning helps accuracy, perhaps by promoting agreement between the coarse and full grammars (model intersection). However, these ‘fortuitous’ search errors give only a small improvement and the peak accuracy is almost equal to the parsing accuracy without any pruning (as seen in Figure 5).11 This outcome suggests that the coarsepass pruning is critical for tractability but not for performance. 10 Unpruned experiments could not be run for 40-word test sentences even with 50GB of memory, therefore we calculated the improv"
P10-1112,N06-1022,0,0.135273,"Missing"
P10-1112,A00-2018,0,0.228742,"Missing"
P10-1112,J98-4004,0,0.583695,"Missing"
P10-1112,J02-1005,0,0.0898002,"by chored local binary tree counts, which are easily directly optimizing parsing F1 on our developcomputed from P (dI |s) and equivalent to those ment set. Because this objective is not easily diffrom P (d|s). Therefore, no additional approximaferentiated, we simply perform a grid search on tions are made in GI over G. the three hyperparameters. The tuned values are As shown in Table 1, our model (an allωBODY = 0.35, ωLEX = 0.25 and asp = 0.018. fragments grammar with the weighting scheme For generalization to a larger parameter space, we would of course need to switch to a learning aptent by Johnson (2002). Later, Zollmann and Sima’an (2005) proach that scales more gracefully in the number presented a statistically consistent estimator, with the basic of tunable hyperparameters.8 insight of optimizing on a held-out set. Our estimator is not 8 Note that there has been a long history of DOP estimators. The generative DOP1 model was shown to be inconsisintended to be viewed as a generative model of trees at all, but simply a loss-minimizing conditional distribution within our parametric family. 1102 88.2 88.0 87.8 Coarse-to-Fine Inference -4.0 Coarse-to-fine inference is a well-established way to"
P10-1112,P03-1054,1,0.223084,"Missing"
P10-1112,N03-1017,0,0.00134302,"oduction Modern NLP systems have increasingly used dataintensive models that capture many or even all substructures from the training data. In the domain of syntactic parsing, the idea that all training fragments1 might be relevant to parsing has a long history, including tree-substitution grammar (data-oriented parsing) approaches (Scha, 1990; Bod, 1993; Goodman, 1996a; Chiang, 2003) and tree kernel approaches (Collins and Duffy, 2002). For machine translation, the key modern advancement has been the ability to represent and memorize large training substructures, be it in contiguous phrases (Koehn et al., 2003) or syntactic trees 1 In this paper, a fragment means an elementary tree in a tree-substitution grammar, while a subtree means a fragment that bottoms out in terminals. (Galley et al., 2004; Chiang, 2005; Deneefe and Knight, 2009). In all such systems, a central challenge is efficiency: there are generally a combinatorial number of substructures in the training data, and it is impractical to explicitly extract them all. On both efficiency and statistical grounds, much recent TSG work has focused on fragment selection (Zuidema, 2007; Cohn et al., 2009; Post and Gildea, 2009). At the same time,"
P10-1112,P05-1010,0,0.170065,"Missing"
P10-1112,N07-1051,1,0.748561,",i,k,j) P r I(rootr ,0,n) tmax = argmax P r(A→B C,i,k,j) x O(Ax ,i,j)I(Ax ,i,j) tmax = argmax t t P q(c) c∈t P q(e) e∈t Q q(e) e∈t Figure 3: Inference: Different objectives for parsing with posteriors. A, B, C are base symbols, Ax , By , Cz are indexed symbols and i,j,k are between-word indices. Hence, (Ax , i, j) represents a constituent labeled with Ax spanning words i to j. I(Ax , i, j) and O(Ax , i, j) denote the inside and outside scores of this constituent, respectively. For brevity, we write c ≡ (A, i, j) and e ≡ (A → B C, i, k, j). Also, tmax is the highest scoring parse. Adapted from Petrov and Klein (2007). constant ωLEX (see Figure 2). Fractional values of these parameters allow the weight of a fragment to depend on its size and lexical properties. Another parameter we introduce is a ‘switching-penalty’ csp for the END rules (Figure 2). The DOP1 model uses binary values (0 if symbol is intermediate, 1 otherwise) as the END rule weight, which is equivalent to prohibiting fragment switching at intermediate symbols. We learn a fractional constant asp that allows (but penalizes) switching between fragments at annotated symbols through the formulation csp (Xintermediate ) = 1 − asp and csp (Xnon−in"
P10-1112,D08-1091,1,0.651745,"Missing"
P10-1112,P05-1033,0,0.0133267,"Missing"
P10-1112,P06-1055,1,0.842051,"Missing"
P10-1112,N09-1062,0,0.298064,"Missing"
P10-1112,D08-1012,1,0.0957222,"g on parsing accuracy (WSJ, training ≤ 20 words, tested on dev-set ≤ 20 words). This graph shows that the fortuitous improvement due to pruning is very small and that the peak accuracy is almost equal to the accuracy without pruning (the dotted line). from no pruning to pruning with a −6.2 log posterior threshold.10 Figure 4 depicts the variation in parsing accuracies in response to the amount of pruning done by the coarse-pass. Higher posterior pruning thresholds induce more aggressive pruning. Here, we observe an effect seen in previous work (Charniak et al. (1998), Petrov and Klein (2007), Petrov et al. (2008)), that a certain amount of pruning helps accuracy, perhaps by promoting agreement between the coarse and full grammars (model intersection). However, these ‘fortuitous’ search errors give only a small improvement and the peak accuracy is almost equal to the parsing accuracy without any pruning (as seen in Figure 5).11 This outcome suggests that the coarsepass pruning is critical for tractability but not for performance. 10 Unpruned experiments could not be run for 40-word test sentences even with 50GB of memory, therefore we calculated the improvement factors using a smaller experiment with f"
P10-1112,P02-1034,0,0.558039,"iable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding. 1 Introduction Modern NLP systems have increasingly used dataintensive models that capture many or even all substructures from the training data. In the domain of syntactic parsing, the idea that all training fragments1 might be relevant to parsing has a long history, including tree-substitution grammar (data-oriented parsing) approaches (Scha, 1990; Bod, 1993; Goodman, 1996a; Chiang, 2003) and tree kernel approaches (Collins and Duffy, 2002). For machine translation, the key modern advancement has been the ability to represent and memorize large training substructures, be it in contiguous phrases (Koehn et al., 2003) or syntactic trees 1 In this paper, a fragment means an elementary tree in a tree-substitution grammar, while a subtree means a fragment that bottoms out in terminals. (Galley et al., 2004; Chiang, 2005; Deneefe and Knight, 2009). In all such systems, a central challenge is efficiency: there are generally a combinatorial number of substructures in the training data, and it is impractical to explicitly extract them al"
P10-1112,P09-2012,0,0.513536,"Missing"
P10-1112,C92-2065,0,0.100557,"ariable modeling, no smoothing, and even no explicit lexicon (hence negligible training overall). These techniques, however, are not limited to the case of monolingual parsing, offering extensions to models of machine translation, semantic interpretation, and other areas in which a similar tension exists between the desire to extract many large structures and the computational cost of doing so. 2 Representation of Implicit Grammars 2.1 All-Fragments Grammars We consider an all-fragments grammar G (see Figure 1(a)) derived from a binarized treebank B. G is formally a tree-substitution grammar (Resnik, 1992; Bod, 1993) wherein each subgraph of each training tree in B is an elementary tree, or fragment f , in G. In G, each derivation d is a tree (multiset) of fragments (Figure 1(c)), and the weight of the derivation is the Q product of the weights of the fragments: ω(d) = f ∈d ω(f ). In the following, the derivation weights, when normalized over a given sentence s, are interpretable as conditional probabilities, so G induces distributions of the form P (d|s). In models like G, many derivations will generally correspond to the same unsegmented tree, and the parsing task is to find the tree whose s"
P10-1112,D09-1076,0,0.0351793,"Missing"
P10-1112,N04-1035,0,0.0110723,"Missing"
P10-1112,C96-2215,0,0.222042,"Missing"
P10-1112,W96-0214,0,0.919889,"ore complicated state-of-theart lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding. 1 Introduction Modern NLP systems have increasingly used dataintensive models that capture many or even all substructures from the training data. In the domain of syntactic parsing, the idea that all training fragments1 might be relevant to parsing has a long history, including tree-substitution grammar (data-oriented parsing) approaches (Scha, 1990; Bod, 1993; Goodman, 1996a; Chiang, 2003) and tree kernel approaches (Collins and Duffy, 2002). For machine translation, the key modern advancement has been the ability to represent and memorize large training substructures, be it in contiguous phrases (Koehn et al., 2003) or syntactic trees 1 In this paper, a fragment means an elementary tree in a tree-substitution grammar, while a subtree means a fragment that bottoms out in terminals. (Galley et al., 2004; Chiang, 2005; Deneefe and Knight, 2009). In all such systems, a central challenge is efficiency: there are generally a combinatorial number of substructures in t"
P10-1112,P00-1008,0,0.441448,"Missing"
P10-1112,P96-1024,0,0.234224,"ore complicated state-of-theart lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding. 1 Introduction Modern NLP systems have increasingly used dataintensive models that capture many or even all substructures from the training data. In the domain of syntactic parsing, the idea that all training fragments1 might be relevant to parsing has a long history, including tree-substitution grammar (data-oriented parsing) approaches (Scha, 1990; Bod, 1993; Goodman, 1996a; Chiang, 2003) and tree kernel approaches (Collins and Duffy, 2002). For machine translation, the key modern advancement has been the ability to represent and memorize large training substructures, be it in contiguous phrases (Koehn et al., 2003) or syntactic trees 1 In this paper, a fragment means an elementary tree in a tree-substitution grammar, while a subtree means a fragment that bottoms out in terminals. (Galley et al., 2004; Chiang, 2005; Deneefe and Knight, 2009). In all such systems, a central challenge is efficiency: there are generally a combinatorial number of substructures in t"
P10-1112,P04-1013,0,0.0153987,"Missing"
P10-1112,D07-1058,0,0.742542,"Missing"
P10-1112,J03-4003,0,\N,Missing
P10-1112,P00-1058,0,\N,Missing
P10-1131,N10-1083,1,0.230173,"Missing"
P10-1131,W00-1201,0,0.0104417,"eech tags (except for Dutch, which is automatically tagged) and provide dependency parses, which are either themselves hand-labeled or have been converted from hand-labeled parses of other kinds. For English and Chinese we use sections 2-21 of the Penn Treebank (PTB) (Marcus et al., 1993) and sections 1-270 of the Chinese Treebank (CTB) (Xue et al., 2002) respectively. Similarly, these sections were used for both training and testing. The English and Chinese data sets have hand-labeled constituency parses and part-ofspeech tags, but no dependency parses. We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. None of the corpora are bitexts. For all languages, we ran experiments on all sentences of length 10 or less after punctuation has been removed. When constructing phylogenies over the languages we made use of their linguistic classifications. English and Dutch are part of the West Ger1291 share a common parent node in the prior, meaning that global regularities that are consistent across all languages can be captured. We refer to this structure as G LOBAL. While the globa"
P10-1131,D07-1093,1,0.714797,"Missing"
P10-1131,D08-1092,1,0.430261,"Missing"
P10-1131,N09-1009,0,0.612839,"nly available for limited languages and domains. In this work, we consider unsupervised grammar induction without bitexts or multitexts. Without translation examples, multilingual constraints cannot be exploited at the sentence token level. Rather, we capture multilingual constraints at a parameter level, using a phylogeny-structured prior to tie together the various individual languages’ learning problems. Our joint, hierarchical prior couples model parameters for different languages in a way that respects knowledge about how the languages evolved. Aspects of this work are closely related to Cohen and Smith (2009) and Bouchard-Cˆot´e et al. (2007). Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts. In their work, structurally constrained covariance in a logistic normal prior is used to couple parameters between the two languages. Our work, though also different in technical approach, differs most centrally in the extension to multiple languages and the use of a phylogeny. Bouchard-Cˆot´e et al. (2007) considers an entirely different problem, phonological reconstruction, but shares with this work both the use of a phylogenetic structure a"
P10-1131,J93-2004,0,0.0355398,"utch, Danish, Swedish, Spanish, Portuguese, Slovene, and Chinese. For all languages but English and Chinese, we used corpora from the 2006 CoNLL-X Shared Task dependency parsing data set (Buchholz and Marsi, 2006). We used the shared task training set to both train and test our models. These corpora provide hand-labeled partof-speech tags (except for Dutch, which is automatically tagged) and provide dependency parses, which are either themselves hand-labeled or have been converted from hand-labeled parses of other kinds. For English and Chinese we use sections 2-21 of the Penn Treebank (PTB) (Marcus et al., 1993) and sections 1-270 of the Chinese Treebank (CTB) (Xue et al., 2002) respectively. Similarly, these sections were used for both training and testing. The English and Chinese data sets have hand-labeled constituency parses and part-ofspeech tags, but no dependency parses. We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. None of the corpora are bitexts. For all languages, we ran experiments on all sentences of length 10 or less after punctuation has been remove"
P10-1131,D09-1086,0,0.0711954,"Missing"
P10-1131,P09-1009,0,0.28893,"Missing"
P10-1131,N09-1010,0,0.227813,"Missing"
P10-1131,C02-1145,0,0.00871453,"ll languages but English and Chinese, we used corpora from the 2006 CoNLL-X Shared Task dependency parsing data set (Buchholz and Marsi, 2006). We used the shared task training set to both train and test our models. These corpora provide hand-labeled partof-speech tags (except for Dutch, which is automatically tagged) and provide dependency parses, which are either themselves hand-labeled or have been converted from hand-labeled parses of other kinds. For English and Chinese we use sections 2-21 of the Penn Treebank (PTB) (Marcus et al., 1993) and sections 1-270 of the Chinese Treebank (CTB) (Xue et al., 2002) respectively. Similarly, these sections were used for both training and testing. The English and Chinese data sets have hand-labeled constituency parses and part-ofspeech tags, but no dependency parses. We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. None of the corpora are bitexts. For all languages, we ran experiments on all sentences of length 10 or less after punctuation has been removed. When constructing phylogenies over the languages we made use of t"
P10-1131,P07-1033,0,0.140506,"Missing"
P10-1131,P02-1001,0,0.0338299,"Germanic English Dutch North Germanic IberoRomance Danish Swedish Spanish Portuguese Slavic Slovene Chinese Figure 1: An example of a linguistically-plausible phylogenetic tree over the languages in our training data. Leaves correspond to (observed) modern languages, while internal nodes represent (unobserved) ancestral languages. actually word classes. 2.1.1 Log-Linear Parameterization The DMV’s local conditional distributions were originally given as simple multinomial distributions with one parameter per outcome. However, they can be re-parameterized to give the following log-linear form (Eisner, 2002; Bouchard-Cˆot´e et al., 2007; Berg-Kirkpatrick et al., 2010): P CONTINUE (c|h, dir, adj; θℓ ) = ˆ ˜ exp θℓ T f CONTINUE (c, h, dir, adj) ˆ T ˜ P ′ c′ exp θℓ f CONTINUE (c , h, dir, adj) P ATTACH (a|h, dir; θℓ ) = ˆ ˜ exp θℓ T f ATTACH (a, h, dir) ˆ T ˜ P ′ a′ exp θℓ f ATTACH (a , h, dir) The parameters are weights θℓ with one weight vector per language. In the case where the vector of feature functions f has an indicator for each possible conjunction of outcome and conditions, the original multinomial distributions are recovered. We refer to these full indicator features as the set of S PECI"
P10-1131,N09-1068,0,0.0206512,"retation, but which provides very similar capacity for parameter coupling. 3.2.1 Phylogenetic Models The first phylogenetic model uses the shallow phylogeny shown in Figure 2(a), in which only languages within the same family have a shared parent node. We refer to this structure as FAMILIES. Under this prior, the learning task decouples into independent subtasks for each family, but no regularities across families can be captured. The family-level model misses the constraints between distant languages. Figure 2(b) shows another simple configuration, wherein all languages Daum´e III (2007) and Finkel and Manning (2009) consider a formally similar Gaussian hierarchy for domain adaptation. As pointed out in Finkel and Manning (2009), there is a simple equivalence between hierarchical regularization as described here and the addition of new tied features in a “flat” model with zero-meaned Gaussian regularization on all parameters. In particular, instead of parameterizing the objective in Section 2.4 in terms of multiple sets of weights, one at each node in the phylogeny (the hierarchical parameterization, described in Section 2.4), it is equivalent to parameterize this same objective in terms of a single set o"
P10-1131,P04-1061,1,0.564388,"le languages and the use of a phylogeny. Bouchard-Cˆot´e et al. (2007) considers an entirely different problem, phonological reconstruction, but shares with this work both the use of a phylogenetic structure as well as the use of log-linear parameterization of local model components. Our work differs from theirs primarily in the task (syntax vs. phonology) and the variables governed by the phylogeny: in our model it is the grammar parameters that drift (in the prior) rather than individual word forms (in the likelihood model). Specifically, we consider dependency induction in the DMV model of Klein and Manning (2004). Our data is a collection of standard dependency data sets in eight languages: English, Dutch, Danish, Swedish, Spanish, Portuguese, Slovene, and Chinese. Our focus is not the DMV model itself, which is well-studied, but rather the prior which couples the various languages’ parameters. While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods. In our experiments, joint multili"
P10-1131,P04-1060,0,0.379192,"Missing"
P10-1131,P09-1042,0,0.135264,"Missing"
P10-1131,W06-2920,0,\N,Missing
P10-1131,J03-4003,0,\N,Missing
P10-1147,P06-1002,0,0.192022,"ze is a closed form function of the loss and feature vectors: τ =   L(Am ; Ag ) − θ · (φ(Ag ) − φ(Am )) min C, ||φ(Ag ) − φ(Am )||22 We train the model for 30 iterations over the training set, shuffling the order each time, and we average the weight vectors observed after each iteration to estimate our final model. 3.1 Extraction Set Loss Function In order to focus learning on predicting the right bispans, we use an extraction-level loss L(Am ; Ag ): an F-measure of the overlap between bispans in Rn (Am ) and Rn (Ag ). This measure has been proposed previously to evaluate alignment systems (Ayan and Dorr, 2006). Based on preliminary translation results during development, we chose bispan F5 as our loss: Pr(Am ) = |Rn (Am ) ∩ Rn (Ag )|/|Rn (Am )| Rc(Am ) = |Rn (Am ) ∩ Rn (Ag )|/|Rn (Ag )| (1 + 52 ) · Pr(Am ) · Rc(Am ) 52 · Pr(Am ) + Rc(Am ) L(Am ; Ag ) = 1 − F5 (Am ; Ag ) F5 (Am ; Ag ) = F5 favors recall over precision. Previous alignment work has shown improvements from adjusting the F-measure parameter (Fraser and Marcu, 2006). In particular, Lacoste-Julien et al. (2006) also chose a recall-biased objective. Optimizing for a bispan F-measure penalizes alignment mistakes in proportion to their rule"
P10-1147,H05-1009,0,0.0856986,"Missing"
P10-1147,W06-3123,0,0.0684454,"l errors eventually. A* search with this heuristic makes no errors, and the time required to compute pseudo-gold alignments is negligible. 5 Relationship to Previous Work Our model is certainly not the first alignment approach to include structures larger than words. Model-based phrase-to-phrase alignment was proposed early in the history of phrase-based translation as a method for training translation models (Marcu and Wong, 2002). A variety of unsupervised models refined this initial work with priors (DeNero et al., 2008; Blunsom et al., 2009) and inference constraints (DeNero et al., 2006; Birch et al., 2006; Cherry and Lin, 2007; Zhang et al., 2008). These models fundamentally differ from ours in that they stipulate a segmentation of the sentence pair into phrases, and only align the minimal phrases in that segmentation. Our model scores the larger overlapping phrases that result from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word alignments via partial matching inference algorithms (Melamed, 2000; Taskar et al., 2005; Moore, 2005; Lacoste-Julien et al., 2006). Work in semi-supervised estimation has also contributed"
P10-1147,P09-1088,0,0.111641,"t j ∈ / [k, `), as depicted in Figure 7. These links will become recall errors eventually. A* search with this heuristic makes no errors, and the time required to compute pseudo-gold alignments is negligible. 5 Relationship to Previous Work Our model is certainly not the first alignment approach to include structures larger than words. Model-based phrase-to-phrase alignment was proposed early in the history of phrase-based translation as a method for training translation models (Marcu and Wong, 2002). A variety of unsupervised models refined this initial work with priors (DeNero et al., 2008; Blunsom et al., 2009) and inference constraints (DeNero et al., 2006; Birch et al., 2006; Cherry and Lin, 2007; Zhang et al., 2008). These models fundamentally differ from ours in that they stipulate a segmentation of the sentence pair into phrases, and only align the minimal phrases in that segmentation. Our model scores the larger overlapping phrases that result from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word alignments via partial matching inference algorithms (Melamed, 2000; Taskar et al., 2005; Moore, 2005; Lacoste-Julien et"
P10-1147,J98-2004,0,0.021304,"small to large, where we 后spe- [after] appears in each edge column or row, but the define the size of a bispan as the total number of cific location of edge links is not required. words contained within it. For each size, we main[I] tain a separate agenda. Only when the agenda for 我 l =4 size k is exhausted does the parser proceed to proIn order to gcompute I(AhL=3 , AR ), we need 睡 cer- [sleep] =1 cess the agenda for size k + 1. tain information about the alignment configuraWe also employ coarse-to-fine search to speed tions of AL and AR where they adjoin at a corner. 了 (past) up inference (Charniak and Caraballo, 1998). In The state must represent (a) the specific alignment the coarse pass, we search over the space of ITG links in the nAfter − 1 deep cornerI of each dinner sleptA, and (b) alignments, but score only features on alignment whether any sure alignments appear in the rows or links and bispans that are local to terminal blocks. columns extending from those corners.6 With this This simplification eliminates the need to augment information, we can infer the bispans licensed by grammar symbols, and so we can exhaustively exadjoining AL and AR , as in Figure 6. plore the (pruned) space. We then comput"
P10-1147,P06-2014,0,0.0201374,"00; Taskar et al., 2005; Moore, 2005; Lacoste-Julien et al., 2006). Work in semi-supervised estimation has also contributed evidence that hand-annotations are useful for training alignment models (Fraser and Marcu, 2006; Fraser and Marcu, 2007). The ITG grammar formalism, the corresponding word alignment class, and inference procedures for the class have also been explored extensively (Wu, 1997; Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008). At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al., 2009). Our model directly extends this research agenda with first-class possible links, overlapping phrasal rule features, and an extraction-level loss function. K¨aa¨ ri¨ainen (2009) trains a translation model discriminatively using features on overlapping phrase pairs. That work differs from ours in that it uses fixed word alignments and focuses on translation model estimation, while we focus on alignment and translate using standard relative frequency estimators. Deng and Zhou (2009) present an alignment combination technique that uses phrasal features. O"
P10-1147,W07-0403,0,0.706688,"ut loss of information, although we emphasize that A is a set of sure and possible alignments, and φ(A) does not decompose as a sum of vectors on individual word-level alignment links. Our model is parameterized by a weight vector θ, which scores an extraction set Rn (A) as θ · φ(A). To further limit the space of extraction sets we are willing to consider, we restrict A to block inverse transduction grammar (ITG) alignments, a space that allows many-to-many alignments through phrasal terminal productions, but otherwise enforces at-most-one-to-one phrase matchings with ITG reordering patterns (Cherry and Lin, 2007; Zhang et al., 2008). The ITG constraint 1455 On February n] 饭 15 2010 被 [passive marker] 发现 [discover] was discovered model class. Hence, we update toward the extraction set for a pseudo-gold alignment Ag ∈ σ(e ) ITG (e, f) with minimal1distance from the true reference alignment At . Ag = arg minA∈ITG(e,f) |A ∪ At − A ∩ At |(3) [after] [dinner] [after] Figure 4: Above, we show a representative subset of the block alignment patterns that serve as terminal productions of the ITG that restricts the output space of our model. These terminal productions cover up to n = 3 words in each sentence an"
P10-1147,D08-1024,0,0.00554395,"summary, our model scores all Rn (A) for A ∈ ITG(e, f) where A can include block terminals of size up to n. In our experiments, n = 3. Unlike previous work, we allow possible alignment links to appear in the block terminals, as depicted in Figure 4. 3 Model Estimation We estimate the weights θ of our extraction set model discriminatively using the margin-infused relaxed algorithm (MIRA) of Crammer and Singer (2003)—a large-margin, perceptron-style, online learning algorithm. MIRA has been used successfully in MT to estimate both alignment models (Haghighi et al., 2009) and translation models (Chiang et al., 2008). For each training example, MIRA requires that we find the alignment Am corresponding to the highest scoring extraction set Rn (Am ) under the current model, Am = arg maxA∈ITG(e,f) θ · φ(A) (2) Section 4 describes our approach to solving this search problem for model inference. MIRA updates away from Rn (Am ) and toward a gold extraction set Rn (Ag ). Some handannotated alignments are outside of the block ITG Inference details appear in Section 4.3. σ(f 2) Given Ag and Am , we update the model parameters away from Am and toward Ag . θ ← θ + τ · (φ(Ag ) − φ(Am )) where τ is the minimal size th"
P10-1147,P07-1003,1,0.949486,"rs included with the Moses training script (Koehn et al., 2007). The designated regimen concludes by Viterbi aligning under Model 4 in both directions. We combined these alignments with 1459 On Febru the grow-diag heuristic (Koehn et al., 2003). Unsupervised Baseline: Joint HMM. We trained and combined two HMM alignment models (Ney and Vogel, 1996) using the Berkeley Aligner.7 We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al., 2006), combined word-toword posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-ofthe-art unsupervised baseline. Supervised Baseline: Block ITG. We discriminatively trained a block ITG aligner with only sure links, using block terminal productions up to 3 words by 3 words in size. This supervised baseline is a reimplementation of the MIRA-trained model of Haghighi et al. (2009). We use the same features and parser implementation for this model as we do for our extraction set model to ensure a clean comparison. To remain within the alignment class, MIRA updates this model toward a pseudogold alignment with only sure links. This model does not score any o"
P10-1147,D08-1033,1,0.665858,"ch that i ∈ [g, h) but j ∈ / [k, `), as depicted in Figure 7. These links will become recall errors eventually. A* search with this heuristic makes no errors, and the time required to compute pseudo-gold alignments is negligible. 5 Relationship to Previous Work Our model is certainly not the first alignment approach to include structures larger than words. Model-based phrase-to-phrase alignment was proposed early in the history of phrase-based translation as a method for training translation models (Marcu and Wong, 2002). A variety of unsupervised models refined this initial work with priors (DeNero et al., 2008; Blunsom et al., 2009) and inference constraints (DeNero et al., 2006; Birch et al., 2006; Cherry and Lin, 2007; Zhang et al., 2008). These models fundamentally differ from ours in that they stipulate a segmentation of the sentence pair into phrases, and only align the minimal phrases in that segmentation. Our model scores the larger overlapping phrases that result from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word alignments via partial matching inference algorithms (Melamed, 2000; Taskar et al., 2005; Moore, 2"
P10-1147,P09-2058,0,0.0625,"models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al., 2009). Our model directly extends this research agenda with first-class possible links, overlapping phrasal rule features, and an extraction-level loss function. K¨aa¨ ri¨ainen (2009) trains a translation model discriminatively using features on overlapping phrase pairs. That work differs from ours in that it uses fixed word alignments and focuses on translation model estimation, while we focus on alignment and translate using standard relative frequency estimators. Deng and Zhou (2009) present an alignment combination technique that uses phrasal features. Our approach differs in two ways. First, their approach is tightly coupled to the input alignments, while we perform a full search over the space of ITG alignments. Also, their approach uses greedy search, while our search is optimal aside from pruning and beaming. Despite these differences, their strong results reinforce our claim that phraselevel information is useful for alignment. 6 Experiments We evaluate our extraction set model by the bispans it predicts, the word alignments it generates, and the translations genera"
P10-1147,P06-1097,0,0.183362,"extraction-level loss L(Am ; Ag ): an F-measure of the overlap between bispans in Rn (Am ) and Rn (Ag ). This measure has been proposed previously to evaluate alignment systems (Ayan and Dorr, 2006). Based on preliminary translation results during development, we chose bispan F5 as our loss: Pr(Am ) = |Rn (Am ) ∩ Rn (Ag )|/|Rn (Am )| Rc(Am ) = |Rn (Am ) ∩ Rn (Ag )|/|Rn (Ag )| (1 + 52 ) · Pr(Am ) · Rc(Am ) 52 · Pr(Am ) + Rc(Am ) L(Am ; Ag ) = 1 − F5 (Am ; Ag ) F5 (Am ; Ag ) = F5 favors recall over precision. Previous alignment work has shown improvements from adjusting the F-measure parameter (Fraser and Marcu, 2006). In particular, Lacoste-Julien et al. (2006) also chose a recall-biased objective. Optimizing for a bispan F-measure penalizes alignment mistakes in proportion to their rule extraction consequences. That is, adding a word link that prevents the extraction of many correct phrasal rules, or which licenses many incorrect rules, is strongly discouraged by this loss. 1456 2010年 2月 15日 3.2 σ(ei ) Features on Extraction Sets The discriminative power of our model is driven by the features on sure word alignment links φa (i, j) and bispans φb (g, h, k, `). In both cases, the most important features co"
P10-1147,D07-1006,0,0.0207186,"hey stipulate a segmentation of the sentence pair into phrases, and only align the minimal phrases in that segmentation. Our model scores the larger overlapping phrases that result from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word alignments via partial matching inference algorithms (Melamed, 2000; Taskar et al., 2005; Moore, 2005; Lacoste-Julien et al., 2006). Work in semi-supervised estimation has also contributed evidence that hand-annotations are useful for training alignment models (Fraser and Marcu, 2006; Fraser and Marcu, 2007). The ITG grammar formalism, the corresponding word alignment class, and inference procedures for the class have also been explored extensively (Wu, 1997; Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008). At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al., 2009). Our model directly extends this research agenda with first-class possible links, overlapping phrasal rule features, and an extraction-level loss function. K¨aa¨ ri¨ainen (2009) tra"
P10-1147,P06-1121,0,0.0354551,"odel is an unaligned sentence pair, and the output is an extraction set of phrasal translation rules. Word-level alignments are generated as a byproduct of inference. We first specify the relationship between word alignments and extraction sets, then define our model. 2.1 Extraction Sets from Word Alignments Rule extraction is a standard concept in machine translation: word alignment constellations license particular sets of overlapping rules, from which subsets are selected according to limits on phrase length (Koehn et al., 2003), number of gaps (Chiang, 2007), count of internal tree nodes (Galley et al., 2006), etc. In this paper, we focus on phrasal rule extraction (i.e., phrase pair extraction), upon which most other extraction procedures are based. Given a sentence pair (e, f), phrasal rule extraction defines a mapping from a set of word-to-word and likewise each word fj projects to a span of e. Then, Rn (A) includes a bispan [g, h) ⇔ [k, `) iff σ(ei ) ⊆ [k, `) ∀i ∈ [g, h) σ(fj ) ⊆ [g, h) ∀j ∈ [k, `) That is, every word in one of the phrasal spans must project within the other. This mapping is deterministic, and so we can interpret a word-level alignment A as also specifying the phrasal rules th"
P10-1147,P96-1024,0,0.0148137,"rI of each dinner sleptA, and (b) alignments, but score only features on alignment whether any sure alignments appear in the rows or links and bispans that are local to terminal blocks. columns extending from those corners.6 With this This simplification eliminates the need to augment information, we can infer the bispans licensed by grammar symbols, and so we can exhaustively exadjoining AL and AR , as in Figure 6. plore the (pruned) space. We then compute outApplying our score recurrence yields a side scores for bispans under a max-sum semirpolynomial-time dynamic program. This dynamic ing (Goodman, 1996). In the fine pass with the program is an instance of ITG bitext parsing, full extraction set model, we impose a maximum where the grammar uses symbols to encode size of 10,000 for each agenda. We order states on the alignment contexts described above. This agendas by the sum of their inside score under the context-as-symbol augmentation of the grammar full model and the outside score computed in the is similar in character to augmenting symbols with coarse pass, pruning all states not within the fixed lexical items to score language models during agenda beam size. hierarchical decoding (Chian"
P10-1147,P09-1104,1,0.816047,"e show a representative subset of the block alignment patterns that serve as terminal productions of the ITG that restricts the output space of our model. These terminal productions cover up to n = 3 words in each sentence and include a mixture of sure (filled) and possible (striped) word-level alignment links. [I] is more computationally convenient than arbitrarily ordered phrase matchings (Wu, 1997; DeNero and Klein, 2008). However, the space of block [sleep] ITG alignments is expressive enough to include [past tense] the vast majority of patterns observed in handannotated parallel corpora (Haghighi et al., 2009). In summary, our model scores all Rn (A) for A ∈ ITG(e, f) where A can include block terminals of size up to n. In our experiments, n = 3. Unlike previous work, we allow possible alignment links to appear in the block terminals, as depicted in Figure 4. 3 Model Estimation We estimate the weights θ of our extraction set model discriminatively using the margin-infused relaxed algorithm (MIRA) of Crammer and Singer (2003)—a large-margin, perceptron-style, online learning algorithm. MIRA has been used successfully in MT to estimate both alignment models (Haghighi et al., 2009) and translation mod"
P10-1147,P07-1019,0,0.00436211,"res O(k 6 ) ner and size combination, built states are maintime in sentence length k, and is prohibitively slow tained in sorted order according to their inside when there is no sparsity in the grammar. Mainscore. This ordering allows us to stop combintaining the context necessary to score non-local ing states early when the results are falling off the bispans further increases running time. That is, agenda beams. Similar search and beaming strateITG inference is organized around search states gies appear in many decoders for machine transassociated with a grammar symbol and a bispan; lation (Huang and Chiang, 2007; Koehn and Hadaugmenting grammar symbols also augments this dow, 2009; Moore and Quirk, 2007). state space. To parse quickly, we prune away search states 4.3 Finding Pseudo-Gold ITG Alignments using predictions from the more efficient HMM Equation 3 asks for the block ITG alignment 6 The number of configuration states does not depend on Ag that is closest to a reference alignment At , the size of A because corners have fixed size, and because the position of links within rows or columns is not needed. which may not lie in ITG(e,f). We search for 1458 σ l σ(f2 ) 在 k =1 饭 l =4 g =0 After h =3 d"
P10-1147,D09-1107,0,0.177678,"Missing"
P10-1147,N03-1016,1,0.375845,"of A because corners have fixed size, and because the position of links within rows or columns is not needed. which may not lie in ITG(e,f). We search for 1458 σ l σ(f2 ) 在 k =1 饭 l =4 g =0 After h =3 dinner I [after] [dinner] 后 [after] 我 [I] 睡 [sleep] 了 [past tense] slept Figure 7: A* search for pseudo-gold ITG alignments uses an admissible heuristic for bispans that counts the number of gold links outside of [k, `) but within [g, h). Above, the heuristic is 1, which is also the minimal number of alignment errors that an ITG alignment will incur using this bispan. Ag using A* bitext parsing (Klein and Manning, 2003). Search states, which correspond to bispans [g, h) ⇔ [k, `), are scored by the number of errors within the bispan plus the number of (i, j) ∈ At such that j ∈ [k, `) but i ∈ / [g, h) (recall errors). As an admissible heuristic for the future cost of a bispan [g, h) ⇔ [k, `), we count the number of (i, j) ∈ At such that i ∈ [g, h) but j ∈ / [k, `), as depicted in Figure 7. These links will become recall errors eventually. A* search with this heuristic makes no errors, and the time required to compute pseudo-gold alignments is negligible. 5 Relationship to Previous Work Our model is certainly n"
P10-1147,P08-2007,1,0.304937,"a pseudo-gold alignment Ag ∈ σ(e ) ITG (e, f) with minimal1distance from the true reference alignment At . Ag = arg minA∈ITG(e,f) |A ∪ At − A ∩ At |(3) [after] [dinner] [after] Figure 4: Above, we show a representative subset of the block alignment patterns that serve as terminal productions of the ITG that restricts the output space of our model. These terminal productions cover up to n = 3 words in each sentence and include a mixture of sure (filled) and possible (striped) word-level alignment links. [I] is more computationally convenient than arbitrarily ordered phrase matchings (Wu, 1997; DeNero and Klein, 2008). However, the space of block [sleep] ITG alignments is expressive enough to include [past tense] the vast majority of patterns observed in handannotated parallel corpora (Haghighi et al., 2009). In summary, our model scores all Rn (A) for A ∈ ITG(e, f) where A can include block terminals of size up to n. In our experiments, n = 3. Unlike previous work, we allow possible alignment links to appear in the block terminals, as depicted in Figure 4. 3 Model Estimation We estimate the weights θ of our extraction set model discriminatively using the margin-infused relaxed algorithm (MIRA) of Crammer"
P10-1147,W09-0429,0,0.0367767,"Missing"
P10-1147,W06-3105,1,0.181167,"nks will become recall errors eventually. A* search with this heuristic makes no errors, and the time required to compute pseudo-gold alignments is negligible. 5 Relationship to Previous Work Our model is certainly not the first alignment approach to include structures larger than words. Model-based phrase-to-phrase alignment was proposed early in the history of phrase-based translation as a method for training translation models (Marcu and Wong, 2002). A variety of unsupervised models refined this initial work with priors (DeNero et al., 2008; Blunsom et al., 2009) and inference constraints (DeNero et al., 2006; Birch et al., 2006; Cherry and Lin, 2007; Zhang et al., 2008). These models fundamentally differ from ours in that they stipulate a segmentation of the sentence pair into phrases, and only align the minimal phrases in that segmentation. Our model scores the larger overlapping phrases that result from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word alignments via partial matching inference algorithms (Melamed, 2000; Taskar et al., 2005; Moore, 2005; Lacoste-Julien et al., 2006). Work in semi-supervised estimation"
P10-1147,N03-1017,0,0.353293,"that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments. 1 Introduction In the last decade, the field of statistical machine translation has shifted from generating sentences word by word to systems that recycle whole fragments of training examples, expressed as translation rules. This general paradigm was first pursued using contiguous phrases (Och et al., 1999; Koehn et al., 2003), and has since been generalized to a wide variety of hierarchical and syntactic formalisms. The training stage of statistical systems focuses primarily on discovering translation rules in parallel corpora. Most systems discover translation rules via a two-stage pipeline: a parallel corpus is aligned at the word level, and then a second procedure extracts fragment-level rules from word-aligned sentence pairs. This paper offers a model-based alternative to phrasal rule extraction, which merges this two-stage pipeline into a single step. We present a discriminative model that directly predicts w"
P10-1147,P07-2045,0,0.0628644,"m phrase length n ensures that also demonstrate that extraction sets are useful for max(h − g, ` − k) ≤ n. end-to-end machine translation. Our model imFebruarythis 15 mapping 2010 PDT We canOndescribe via word-toproves translation quality relative to state-of-thephrase projections, as illustrated in Figure 1. Let art Chinese-to-English baselines across two pubword ei project to the phrasal span σ(ei ), where licly available systems, providing total BLEU im  provements of 1.2 in Moses, a phrase-based sysσ(ei ) = min j , max j + 1 (1) tem, and 1.4 in a Joshua, a hierarchical system j∈Ji j∈Ji (Koehn et al., 2007; Li et al., 2009) Ji = {j : (i, j) ∈ A} 2 Extraction Set Models The input to our model is an unaligned sentence pair, and the output is an extraction set of phrasal translation rules. Word-level alignments are generated as a byproduct of inference. We first specify the relationship between word alignments and extraction sets, then define our model. 2.1 Extraction Sets from Word Alignments Rule extraction is a standard concept in machine translation: word alignment constellations license particular sets of overlapping rules, from which subsets are selected according to limits on phrase length"
P10-1147,P02-1040,0,0.10707,"h the HMM caused parse failures, which in turn caused training sentences to be skipped. To account for these issues, we added counts of phrasal rules extracted from the baseline HMM to the counts produced by supervised aligners. In Moses, our extraction set model predicts the set of phrases extracted by the system, and so the estimation techniques for the alignment model and translation model both share a common underlying representation: extraction sets. Empirically, we observe a BLEU score improvement of 1.2 over the best unsupervised baseline and 0.8 over the block ITG supervised baseline (Papineni et al., 2002). In Joshua, hierarchical rule extraction is based upon phrasal rule extraction, but abstracts away sub-phrases to create a grammar. Hence, the extraction sets we predict are closely linked to the representation that this system uses to translate. The extraction model again outperformed both unsupervised and supervised baselines, by 1.4 BLEU and 1.2 BLEU respectively. 7 Conclusion Our extraction set model serves to coordinate the alignment and translation model components of a statistical translation system by unifying their representations. Moreover, our model provides an effective alternativ"
P10-1147,N06-1015,1,0.93101,"measure of the overlap between bispans in Rn (Am ) and Rn (Ag ). This measure has been proposed previously to evaluate alignment systems (Ayan and Dorr, 2006). Based on preliminary translation results during development, we chose bispan F5 as our loss: Pr(Am ) = |Rn (Am ) ∩ Rn (Ag )|/|Rn (Am )| Rc(Am ) = |Rn (Am ) ∩ Rn (Ag )|/|Rn (Ag )| (1 + 52 ) · Pr(Am ) · Rc(Am ) 52 · Pr(Am ) + Rc(Am ) L(Am ; Ag ) = 1 − F5 (Am ; Ag ) F5 (Am ; Ag ) = F5 favors recall over precision. Previous alignment work has shown improvements from adjusting the F-measure parameter (Fraser and Marcu, 2006). In particular, Lacoste-Julien et al. (2006) also chose a recall-biased objective. Optimizing for a bispan F-measure penalizes alignment mistakes in proportion to their rule extraction consequences. That is, adding a word link that prevents the extraction of many correct phrasal rules, or which licenses many incorrect rules, is strongly discouraged by this loss. 1456 2010年 2月 15日 3.2 σ(ei ) Features on Extraction Sets The discriminative power of our model is driven by the features on sure word alignment links φa (i, j) and bispans φb (g, h, k, `). In both cases, the most important features come from the predictions of unsupervised model"
P10-1147,H05-1010,1,0.85326,"h priors (DeNero et al., 2008; Blunsom et al., 2009) and inference constraints (DeNero et al., 2006; Birch et al., 2006; Cherry and Lin, 2007; Zhang et al., 2008). These models fundamentally differ from ours in that they stipulate a segmentation of the sentence pair into phrases, and only align the minimal phrases in that segmentation. Our model scores the larger overlapping phrases that result from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word alignments via partial matching inference algorithms (Melamed, 2000; Taskar et al., 2005; Moore, 2005; Lacoste-Julien et al., 2006). Work in semi-supervised estimation has also contributed evidence that hand-annotations are useful for training alignment models (Fraser and Marcu, 2006; Fraser and Marcu, 2007). The ITG grammar formalism, the corresponding word alignment class, and inference procedures for the class have also been explored extensively (Wu, 1997; Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008). At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) a"
P10-1147,W09-0424,0,0.0785326,"sures that also demonstrate that extraction sets are useful for max(h − g, ` − k) ≤ n. end-to-end machine translation. Our model imFebruarythis 15 mapping 2010 PDT We canOndescribe via word-toproves translation quality relative to state-of-thephrase projections, as illustrated in Figure 1. Let art Chinese-to-English baselines across two pubword ei project to the phrasal span σ(ei ), where licly available systems, providing total BLEU im  provements of 1.2 in Moses, a phrase-based sysσ(ei ) = min j , max j + 1 (1) tem, and 1.4 in a Joshua, a hierarchical system j∈Ji j∈Ji (Koehn et al., 2007; Li et al., 2009) Ji = {j : (i, j) ∈ A} 2 Extraction Set Models The input to our model is an unaligned sentence pair, and the output is an extraction set of phrasal translation rules. Word-level alignments are generated as a byproduct of inference. We first specify the relationship between word alignments and extraction sets, then define our model. 2.1 Extraction Sets from Word Alignments Rule extraction is a standard concept in machine translation: word alignment constellations license particular sets of overlapping rules, from which subsets are selected according to limits on phrase length (Koehn et al., 200"
P10-1147,J97-3002,0,0.853276,"function during training affects model performance. We optimize for a phrase-level F-measure in order to focus learning on the task of predicting phrasal rules rather than word alignment links. Third, our discriminative approach requires that we perform inference in the space of extraction sets. Our model does not factor over disjoint wordto-word links or minimal phrase pairs, and so existing inference procedures do not directly apply. However, we show that the dynamic program for a block ITG aligner can be augmented to score extraction sets that are indexed by underlying ITG word alignments (Wu, 1997). We also describe a 1453 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1453–1463, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics σ(ei ) 2010年2010年 过去 [past] ast] σ(fj ) wo] σ(fj ) 2月 Distribution [go over] Distribution over over 过 [go过over] possible link types possible link types 地球 [Earth] 地球 [Earth] 2月 [two] over Earth the Earth over the 2: Role-equivalent word pairs Type 2:Type Role-equivalent pairs that are equivalents not lexical equivalents are not that lexical 15日 15日 年 ear] [year] n]中 [in] 饭 (a) 1:Typ"
P10-1147,P05-1059,0,0.0752464,"lt from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word alignments via partial matching inference algorithms (Melamed, 2000; Taskar et al., 2005; Moore, 2005; Lacoste-Julien et al., 2006). Work in semi-supervised estimation has also contributed evidence that hand-annotations are useful for training alignment models (Fraser and Marcu, 2006; Fraser and Marcu, 2007). The ITG grammar formalism, the corresponding word alignment class, and inference procedures for the class have also been explored extensively (Wu, 1997; Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008). At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al., 2009). Our model directly extends this research agenda with first-class possible links, overlapping phrasal rule features, and an extraction-level loss function. K¨aa¨ ri¨ainen (2009) trains a translation model discriminatively using features on overlapping phrase pairs. That work differs from ours in that it uses fixed word alignments and focuses on translation"
P10-1147,N06-1014,1,0.872769,"s the extraction of many correct phrasal rules, or which licenses many incorrect rules, is strongly discouraged by this loss. 1456 2010年 2月 15日 3.2 σ(ei ) Features on Extraction Sets The discriminative power of our model is driven by the features on sure word alignment links φa (i, j) and bispans φb (g, h, k, `). In both cases, the most important features come from the predictions of unsupervised models trained on large parallel corpora, which provide frequency and cooccurrence information. To score word-to-word links, we use the posterior predictions of a jointly trained HMM alignment model (Liang et al., 2006). The remaining features include a dictionary feature, an identical word feature, an absolute position distortion feature, and features for numbers and punctuation. To score phrasal translation rules in an extraction set, we use a mixture of feature types. Extraction set models allow us to incorporate the same phrasal relative frequency statistics that drive phrase-based translation performance (Koehn et al., 2003). To implement these frequency features, we extract a phrase table from the alignment predictions of a jointly trained unsupervised HMM model using Moses (Koehn et al., 2007), and sc"
P10-1147,W06-1627,0,0.0510636,"Missing"
P10-1147,D07-1104,0,0.00646831,"er methods did. In the BLEU evaluation, all systems used a bilingual dictionary included in the training corpus. The BLEU evaluation of supervised systems also included rule counts from the Joint HMM to compensate for parse failures. 6.3 Translation Experiments We evaluate the alignments predicted by our model using two publicly available, open-source, state-of-the-art translation systems. Moses is a phrase-based system with lexicalized reordering (Koehn et al., 2007). Joshua (Li et al., 2009) is an implementation of Hiero (Chiang, 2007) using a suffix-array-based grammar extraction approach (Lopez, 2007). Both of these systems take word alignments as input, and neither of these systems accepts possible links in the alignments they consume. To interface with our extraction set models, we produced three sets of sure-only alignments from our model predictions: one that omitted possible links, one that converted all possible links to sure links, and one that includes each possible link with 0.5 probability. These three sets were aggregated and rules were extracted from all three. The training set we used for MT experiments is quite heterogenous and noisy compared to our alignment test sets, and t"
P10-1147,P08-1012,0,0.558386,", although we emphasize that A is a set of sure and possible alignments, and φ(A) does not decompose as a sum of vectors on individual word-level alignment links. Our model is parameterized by a weight vector θ, which scores an extraction set Rn (A) as θ · φ(A). To further limit the space of extraction sets we are willing to consider, we restrict A to block inverse transduction grammar (ITG) alignments, a space that allows many-to-many alignments through phrasal terminal productions, but otherwise enforces at-most-one-to-one phrase matchings with ITG reordering patterns (Cherry and Lin, 2007; Zhang et al., 2008). The ITG constraint 1455 On February n] 饭 15 2010 被 [passive marker] 发现 [discover] was discovered model class. Hence, we update toward the extraction set for a pseudo-gold alignment Ag ∈ σ(e ) ITG (e, f) with minimal1distance from the true reference alignment At . Ag = arg minA∈ITG(e,f) |A ∪ At − A ∩ At |(3) [after] [dinner] [after] Figure 4: Above, we show a representative subset of the block alignment patterns that serve as terminal productions of the ITG that restricts the output space of our model. These terminal productions cover up to n = 3 words in each sentence and include a mixture o"
P10-1147,W02-1018,0,0.188876,"euristic for the future cost of a bispan [g, h) ⇔ [k, `), we count the number of (i, j) ∈ At such that i ∈ [g, h) but j ∈ / [k, `), as depicted in Figure 7. These links will become recall errors eventually. A* search with this heuristic makes no errors, and the time required to compute pseudo-gold alignments is negligible. 5 Relationship to Previous Work Our model is certainly not the first alignment approach to include structures larger than words. Model-based phrase-to-phrase alignment was proposed early in the history of phrase-based translation as a method for training translation models (Marcu and Wong, 2002). A variety of unsupervised models refined this initial work with priors (DeNero et al., 2008; Blunsom et al., 2009) and inference constraints (DeNero et al., 2006; Birch et al., 2006; Cherry and Lin, 2007; Zhang et al., 2008). These models fundamentally differ from ours in that they stipulate a segmentation of the sentence pair into phrases, and only align the minimal phrases in that segmentation. Our model scores the larger overlapping phrases that result from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word align"
P10-1147,2007.mtsummit-papers.43,0,0.0084729,"rohibitively slow tained in sorted order according to their inside when there is no sparsity in the grammar. Mainscore. This ordering allows us to stop combintaining the context necessary to score non-local ing states early when the results are falling off the bispans further increases running time. That is, agenda beams. Similar search and beaming strateITG inference is organized around search states gies appear in many decoders for machine transassociated with a grammar symbol and a bispan; lation (Huang and Chiang, 2007; Koehn and Hadaugmenting grammar symbols also augments this dow, 2009; Moore and Quirk, 2007). state space. To parse quickly, we prune away search states 4.3 Finding Pseudo-Gold ITG Alignments using predictions from the more efficient HMM Equation 3 asks for the block ITG alignment 6 The number of configuration states does not depend on Ag that is closest to a reference alignment At , the size of A because corners have fixed size, and because the position of links within rows or columns is not needed. which may not lie in ITG(e,f). We search for 1458 σ l σ(f2 ) 在 k =1 饭 l =4 g =0 After h =3 dinner I [after] [dinner] 后 [after] 我 [I] 睡 [sleep] 了 [past tense] slept Figure 7: A* search fo"
P10-1147,H05-1011,0,0.0650786,"l., 2008; Blunsom et al., 2009) and inference constraints (DeNero et al., 2006; Birch et al., 2006; Cherry and Lin, 2007; Zhang et al., 2008). These models fundamentally differ from ours in that they stipulate a segmentation of the sentence pair into phrases, and only align the minimal phrases in that segmentation. Our model scores the larger overlapping phrases that result from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word alignments via partial matching inference algorithms (Melamed, 2000; Taskar et al., 2005; Moore, 2005; Lacoste-Julien et al., 2006). Work in semi-supervised estimation has also contributed evidence that hand-annotations are useful for training alignment models (Fraser and Marcu, 2006; Fraser and Marcu, 2007). The ITG grammar formalism, the corresponding word alignment class, and inference procedures for the class have also been explored extensively (Wu, 1997; Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008). At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block mode"
P10-1147,C96-2141,0,0.354347,"ions generated by two end-to-end systems. Table 1 compares the five systems described below, including three baselines. All supervised aligners were optimized for bispan F5 . Unsupervised Baseline: GIZA++. We trained GIZA++ (Och and Ney, 2003) using the default parameters included with the Moses training script (Koehn et al., 2007). The designated regimen concludes by Viterbi aligning under Model 4 in both directions. We combined these alignments with 1459 On Febru the grow-diag heuristic (Koehn et al., 2003). Unsupervised Baseline: Joint HMM. We trained and combined two HMM alignment models (Ney and Vogel, 1996) using the Berkeley Aligner.7 We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al., 2006), combined word-toword posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-ofthe-art unsupervised baseline. Supervised Baseline: Block ITG. We discriminatively trained a block ITG aligner with only sure links, using block terminal productions up to 3 words by 3 words in size. This supervised baseline is a reimplementation of the MIRA-trained model of Haghighi et al. (2009). We use"
P10-1147,J03-1002,0,0.00697973,"ace of ITG alignments. Also, their approach uses greedy search, while our search is optimal aside from pruning and beaming. Despite these differences, their strong results reinforce our claim that phraselevel information is useful for alignment. 6 Experiments We evaluate our extraction set model by the bispans it predicts, the word alignments it generates, and the translations generated by two end-to-end systems. Table 1 compares the five systems described below, including three baselines. All supervised aligners were optimized for bispan F5 . Unsupervised Baseline: GIZA++. We trained GIZA++ (Och and Ney, 2003) using the default parameters included with the Moses training script (Koehn et al., 2007). The designated regimen concludes by Viterbi aligning under Model 4 in both directions. We combined these alignments with 1459 On Febru the grow-diag heuristic (Koehn et al., 2003). Unsupervised Baseline: Joint HMM. We trained and combined two HMM alignment models (Ney and Vogel, 1996) using the Berkeley Aligner.7 We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al., 2006), combined word-toword posteriors by averaging (soft union), and decoded with the competitive"
P10-1147,W99-0604,0,0.178065,"ased loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments. 1 Introduction In the last decade, the field of statistical machine translation has shifted from generating sentences word by word to systems that recycle whole fragments of training examples, expressed as translation rules. This general paradigm was first pursued using contiguous phrases (Och et al., 1999; Koehn et al., 2003), and has since been generalized to a wide variety of hierarchical and syntactic formalisms. The training stage of statistical systems focuses primarily on discovering translation rules in parallel corpora. Most systems discover translation rules via a two-stage pipeline: a parallel corpus is aligned at the word level, and then a second procedure extracts fragment-level rules from word-aligned sentence pairs. This paper offers a model-based alternative to phrasal rule extraction, which merges this two-stage pipeline into a single step. We present a discriminative model tha"
P10-1147,2009.eamt-smart.4,0,\N,Missing
P10-1147,J00-2004,0,\N,Missing
P10-1147,2006.amta-papers.2,0,\N,Missing
P10-1147,J07-2003,0,\N,Missing
P10-2037,W05-1506,0,0.646769,". . sn−1 is called the Viterbi outside score α(e). The goal of a kbest parsing algorithm is to compute the k best (minimum weight) inside derivations of the edge (G, 0, n). We formulate the algorithms in this paper in terms of prioritized weighted deduction rules (Shieber et al., 1995; Nederhof, 2003). A prioritized weighted deduction rule has the form Introduction Many situations call for a parser to return a kbest list of parses instead of a single best hypothesis.1 Currently, there are two efficient approaches known in the literature. The k-best algorithm of Jim´enez and Marzal (2000) and Huang and Chiang (2005), referred to hereafter as L AZY, operates by first performing an exhaustive Viterbi inside pass and then lazily extracting k-best lists in top-down manner. The k-best A∗ algorithm of Pauls and Klein (2009), hereafter KA∗ , computes Viterbi inside and outside scores before extracting k-best lists bottom up. Because these additional passes are only partial, KA∗ can be significantly faster than L AZY, especially when a heuristic is used (Pauls and Klein, 2009). In this paper, we propose TKA∗ , a topdown variant of KA∗ that, like L AZY, performs only an inside pass before extracting k-best lists"
P10-2037,W01-1812,1,0.776118,"on items called inside edge items I(A, i, j), which represent the many possible inside derivations of an edge (A, i, j). Inside edge items are constructed according to the IN deduction rule of Table 1. This deduction rule constructs inside edge items in a bottom-up fashion, combining items representing smaller edges I(B, i, k) and I(C, k, j) with a grammar rule r = A → B C to form a larger item I(A, i, j). The weight of a newly constructed item is given by the sum of the weights of the antecedent items and the grammar rule r, and its priority is given by hypergraph search problems as shown in Klein and Manning (2001). 201 IN∗† : IN-D† : OUT-L† : OUT-R† : OUT-D∗ : O(A, i, j) : w1 O(A, i, j) : w1 O(A, i, j) : w1 I(B, i, l) : w1 D(T B , i, l) : w2 I(B, i, l) : w2 I(B, i, l) : w2 I(C, l, j) : w2 D(T C , l, j) : w3 I(C, l, j) : w3 I(C, l, j) : w3 Q(TAG , i, j, F) : w1 I(B, i, l) : w2 I(C, l, j) : w3 w1 +w2 +wr +h(A,i,j) −−−−−−−−−−−−−−→ w +w3 +wr +w1 −−2−−− −−−−−→ w +w3 +wr +w2 −−1−−− −−−−−→ w +w2 +wr +w3 −−1−−− −−−−−→ w1 +wr +w2 +w3 +β(F ) −−−−−−−−−−−−−−−→ I(A, i, j) : w1 + w2 + wr D(T A , i, j) : w2 + w3 + wr O(B, i, l) : w1 + w3 + wr O(C, l, j) : w1 + w2 + wr Q(TBG , i, l, FC ) : w1 + wr Table 1: The deducti"
P10-2037,N03-1016,1,0.920129,"Abstract 2 We propose a top-down algorithm for extracting k-best lists from a parser. Our algorithm, TKA∗ is a variant of the kbest A∗ (KA∗ ) algorithm of Pauls and Klein (2009). In contrast to KA∗ , which performs an inside and outside pass before performing k-best extraction bottom up, TKA∗ performs only the inside pass before extracting k-best lists top down. TKA∗ maintains the same optimality and efficiency guarantees of KA∗ , but is simpler to both specify and implement. 1 Because our algorithm is very similar to KA∗ , which is in turn an extension of the (1-best) A∗ parsing algorithm of Klein and Manning (2003), we first introduce notation and review those two algorithms before presenting our new algorithm. 2.1 Notation Assume we have a PCFG2 G and an input sentence s0 . . . sn−1 of length n. The grammar G has a set of symbols denoted by capital letters, including a distinguished goal (root) symbol G. Without loss of generality, we assume Chomsky normal form: each non-terminal rule r in G has the form r = A → B C with weight wr . Edges are labeled spans e = (A, i, j). Inside derivations of an edge (A, i, j) are trees with root nonterminal A, spanning si . . . sj−1 . The weight (negative log-probabil"
P10-2037,J03-1006,0,0.0252999,"= (A, i, j). Inside derivations of an edge (A, i, j) are trees with root nonterminal A, spanning si . . . sj−1 . The weight (negative log-probability) of the best (minimum) inside derivation for an edge e is called the Viterbi inside score β(e), and the weight of the best derivation of G → s0 . . . si−1 A sj . . . sn−1 is called the Viterbi outside score α(e). The goal of a kbest parsing algorithm is to compute the k best (minimum weight) inside derivations of the edge (G, 0, n). We formulate the algorithms in this paper in terms of prioritized weighted deduction rules (Shieber et al., 1995; Nederhof, 2003). A prioritized weighted deduction rule has the form Introduction Many situations call for a parser to return a kbest list of parses instead of a single best hypothesis.1 Currently, there are two efficient approaches known in the literature. The k-best algorithm of Jim´enez and Marzal (2000) and Huang and Chiang (2005), referred to hereafter as L AZY, operates by first performing an exhaustive Viterbi inside pass and then lazily extracting k-best lists in top-down manner. The k-best A∗ algorithm of Pauls and Klein (2009), hereafter KA∗ , computes Viterbi inside and outside scores before extrac"
P10-2037,P09-1108,1,0.935694,"is paper in terms of prioritized weighted deduction rules (Shieber et al., 1995; Nederhof, 2003). A prioritized weighted deduction rule has the form Introduction Many situations call for a parser to return a kbest list of parses instead of a single best hypothesis.1 Currently, there are two efficient approaches known in the literature. The k-best algorithm of Jim´enez and Marzal (2000) and Huang and Chiang (2005), referred to hereafter as L AZY, operates by first performing an exhaustive Viterbi inside pass and then lazily extracting k-best lists in top-down manner. The k-best A∗ algorithm of Pauls and Klein (2009), hereafter KA∗ , computes Viterbi inside and outside scores before extracting k-best lists bottom up. Because these additional passes are only partial, KA∗ can be significantly faster than L AZY, especially when a heuristic is used (Pauls and Klein, 2009). In this paper, we propose TKA∗ , a topdown variant of KA∗ that, like L AZY, performs only an inside pass before extracting k-best lists top-down, but maintains the same optimality and efficiency guarantees as KA∗ . This algorithm can be seen as a generalization of the lattice k-best algorithm of Soong and Huang (1991) to parsing. Because TK"
P10-2037,P06-1055,1,0.677118,"Missing"
P10-2054,de-marneffe-etal-2006-generating,0,0.023899,"Missing"
P10-2054,N10-1061,1,0.873528,"h do not fill a template role (see Section 5.2). Each role R is represented as a mapping between properties r and pairs of multinomials (θr , fr ). θr is a unigram distribution of words for property r that are semantically licensed for the role (e.g., being the subject of “acquired” for the ACQUIRED role). fr is a “fertility” distribution over the integers that characterizes entity list lengths. Together, these distributions control the lists Lr for entities which instantiate the role. We describe our generative model for a document, which has many similarities to the coreferenceonly model of Haghighi and Klein (2010), but which integrally models template role-fillers. We briefly describe the key abstractions of our model. Mentions: A mention is an observed textual reference to a latent real-world entity. Mentions are associated with nodes in a parse tree and are typically realized as NPs. There are three basic forms of mentions: proper (NAM), nominal (NOM), and pronominal (PRO). Each mention M is represented as collection of key-value pairs. The keys are called properties and the values are words. The set of properties utilized here, denoted R, are the same as in Haghighi and Klein (2010) and consist of t"
P10-2054,D07-1075,0,0.0385445,"mentions sharing the same role label. Note that pronoun mentions provide direct clues to entity roles. Introduction can naturally incorporate unannotated data, which further increases accuracy. Template-filling information extraction (IE) systems must merge information across multiple sentences to identify all role fillers of interest. For instance, in the MUC4 terrorism event extraction task, the entity filling the individual perpetrator role often occurs multiple times, variously as proper, nominal, or pronominal mentions. However, most template-filling systems (Freitag and McCallum, 2000; Patwardhan and Riloff, 2007) assign roles to individual textual mentions using only local context as evidence, leaving aggregation for post-processing. While prior work has acknowledged that coreference resolution and discourse analysis are integral to accurate role identification, to our knowledge no model has been proposed which jointly models these phenomena. In this work, we describe an entity-centered approach to template-filling IE problems. Our model jointly merges surface mentions into underlying entities (coreference resolution) and assigns roles to those discovered entities. In the generative process proposed h"
P10-2054,D09-1016,0,0.078679,"Missing"
P10-2054,P06-1055,1,0.578415,"occur. 4 Sentence parse trees are merged into a right-branching document parse tree. This allows us to extend tree distance to inter-sentence nodes. qi ← argmin KL(Q(E, Z)|P (E, Z|M, R, φ) qi ∝ exp{EQ/qi ln P (E, Z|M, R, φ))} 5 293 The sole parameter γ is fixed at 0.1. INDEP JOINT JOINT+PRO Ment Acc. 60.0 64.6 68.2 Ent. Acc. 43.7 54.2 57.8 documents, proper and (usually) nominal mentions are annotated with roles, while pronouns are not. We preprocess each document identically to Haghighi and Klein (2010): we sentence-segment using the OpenNLP toolkit, parse sentences with the Berkeley Parser (Petrov et al., 2006), and extract mention properties from parse trees and the Stanford Dependency Extractor (de Marneffe et al., 2006). Table 1: Results on corporate acquisition tasks with given role mention boundaries. We report mention role accuracy and entity role accuracy (correctly labeling all entity mentions). For example, the update for a non-pronominal entity indicator component rj (·) is given by:6 5.1 ln rj (z) ∝ EQ/rj ln P (E, Z, M|R, φ) We first consider the simplified task where role mention boundaries are given. We map each labeled token span in training and test data to a parse tree node that shar"
P10-2064,P06-1055,1,0.6271,"ft and coarse inside item on the right. builds both inside and bridge outside items under the target grammar, where HA∗ only builds inside items. It is an empirical, grammar- and hierarchydependent question whether the increased tightness of the outside estimates outweighs the additional cost needed to compute them. We demonstrate empirically in this section that for hierarchies with very loosely approximating coarse grammars, BHA∗ can outperform HA∗ , while for hierarchies with good approximations, performance of the two algorithms is comparable. We performed experiments with the grammars of Petrov et al. (2006). The training procedure for these grammars produces a hierarchy of increasingly refined grammars through state-splitting, so a natural projection function πt is given. We used the Berkeley Parser2 to learn such grammars from Sections 2-21 of the Penn Treebank (Marcus et al., 1993). We trained with 6 split-merge cycles, producing 7 grammars. We tested these grammars on 300 sentences of length ≤ 25 of Section 23 of the Treebank. Our “target grammar” was in all cases the most split grammar. Experiments The performance of BHA∗ is determined by the efficiency guarantee given in the previous sectio"
P10-2064,N03-1016,1,0.712356,"combines two inside items over smaller spans with a grammar rule to form an inside item over larger spans. The weight of the resulting item is the sum of the weights of the smaller inside items and the grammar rule. However, the IN rule also requires that an outside score in the coarse grammar1 be computed before an inside item is built. Once constructed, this coarse outside score is added to the weight of the conclusion item to form the priority of the resulting item. In other words, the coarse outside score computed by the algorithm plays the same role as a heuristic in standard A∗ parsing (Klein and Manning, 2003). Outside scores are computed by the OUT-L and OUT-R deduction rules. These rules combine an outside item over a large span and inside items over smaller spans to form outside items over smaller spans. Unlike the IN deduction, the OUT deductions only involve items from the same level of the hierarchy. That is, whereas inside scores wait on coarse outside scores to be constructed, outside scores wait on inside scores at the same level in the hierarchy. Conceptually, these deduction rules operate by Deduction Rules HA∗ and our modification BHA∗ can be formulated in terms of prioritized weighted"
P10-2064,J93-2004,0,0.0330201,"tional cost needed to compute them. We demonstrate empirically in this section that for hierarchies with very loosely approximating coarse grammars, BHA∗ can outperform HA∗ , while for hierarchies with good approximations, performance of the two algorithms is comparable. We performed experiments with the grammars of Petrov et al. (2006). The training procedure for these grammars produces a hierarchy of increasingly refined grammars through state-splitting, so a natural projection function πt is given. We used the Berkeley Parser2 to learn such grammars from Sections 2-21 of the Penn Treebank (Marcus et al., 1993). We trained with 6 split-merge cycles, producing 7 grammars. We tested these grammars on 300 sentences of length ≤ 25 of Section 23 of the Treebank. Our “target grammar” was in all cases the most split grammar. Experiments The performance of BHA∗ is determined by the efficiency guarantee given in the previous section. However, we cannot determine in advance whether BHA∗ will be faster than HA∗ . In fact, BHA∗ has the potential to be slower – BHA∗ 2 351 http://berkeleyparser.googlecode.com In our first experiment, we construct 2-level hierarchies consisting of one coarse grammar and the target"
P10-2064,N09-1063,1,0.86308,"costs computed in coarser grammars. We present Bridge Hierarchical A∗ (BHA∗ ), a modified Hierarchial A∗ algorithm which computes a novel outside cost called a bridge outside cost. These bridge costs mix finer outside scores with coarser inside scores, and thus constitute tighter heuristics than entirely coarse scores. We show that BHA∗ substantially outperforms HA∗ when the hierarchy contains only very coarse grammars, while achieving comparable performance on more refined hierarchies. 1 2 Previous Work In this section, we introduce notation and review HA∗ . Our presentation closely follows Pauls and Klein (2009), and we refer the reader to that work for a more detailed presentation. Introduction The Hierarchical A∗ (HA∗ ) algorithm of Felzenszwalb and McAllester (2007) allows the use of a hierarchy of coarse grammars to speed up parsing without sacrificing optimality. Pauls and Klein (2009) showed that a hierarchy of coarse grammars outperforms standard A∗ parsing for a range of grammars. HA∗ operates by computing Viterbi inside and outside scores in an agendabased way, using outside scores computed under coarse grammars as heuristics which guide the search in finer grammars. The outside scores compu"
P11-1027,D07-1090,0,0.262475,"Missing"
P11-1027,P05-1033,0,0.0879203,"Missing"
P11-1027,D07-1021,0,0.0672126,"Missing"
P11-1027,W07-0712,0,0.0122706,"probabilities and back-offs in the same way, allowing us to be agnostic to whether 1 http://code.google.com/p/berkeleylm/ 259 we encode counts, probabilities and/or back-off weights in our model. In general, the number of bits per value required to encode all value ranks for a given language model will vary – we will refer to this variable as v . 2.2 Trie-Based Language Models The data structure of choice for the majority of modern language model implementations is a trie (Fredkin, 1960). Tries or variants thereof are implemented in many LM tool kits, including SRILM (Stolcke, 2002), IRSTLM (Federico and Cettolo, 2007), CMU SLM (Whittaker and Raj, 2001), and MIT LM (Hsu and Glass, 2008). Tries represent collections of n-grams using a tree. Each node in the tree encodes a word, and paths in the tree correspond to n-grams in the collection. Tries ensure that each n-gram prefix is represented only once, and are very efficient when n-grams share common prefixes. Values can also be stored in a trie by placing them in the appropriate nodes. Conceptually, trie nodes can be implemented as records that contain two entries: one for the word in the node, and one for either a pointer to the parent of the node or a list"
P11-1027,W09-1505,0,0.720803,"duction For modern statistical machine translation systems, language models must be both fast and compact. The largest language models (LMs) can contain as many as several hundred billion n-grams (Brants et al., 2007), so storage is a challenge. At the same time, decoding a single sentence can trigger hundreds of thousands of queries to the language model, so speed is also critical. As always, trade-offs exist between time, space, and accuracy, with many recent papers considering smallbut-approximate noisy LMs (Chazelle et al., 2004; Guthrie and Hepple, 2010) or small-but-slow compressed LMs (Germann et al., 2009). In this paper, we present several lossless methods for compactly but efficiently storing large LMs in memory. As in much previous work (Whittaker and Raj, 2001; Hsu and Glass, 2008), our methods are conceptually based on tabular trie encodings wherein each n-gram key is stored as the concatenation of one word (here, the last) and an offset encoding the remaining words (here, the context). After presenting a bit-conscious basic system that typifies such approaches, we improve on it in several ways. First, we show how the last word of each entry can be implicitly encoded, almost entirely elimi"
P11-1027,D10-1026,0,0.229983,"x of the original query. This setup substantially accelerates the scrolling queries issued by decoders, and also exploits language model state equivalence (Li and Khudanpur, 2008). Overall, we are able to store the 4 billion n-grams of the Google Web1T (Brants and Franz, 2006) cor258 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 258–267, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics pus, with associated counts, in 10 GB of memory, which is smaller than state-of-the-art lossy language model implementations (Guthrie and Hepple, 2010), and significantly smaller than the best published lossless implementation (Germann et al., 2009). We are also able to simultaneously outperform SRILM in both total size and speed. Our LM toolkit, which is implemented in Java and compatible with the standard ARPA file formats, is available on the web.1 2 Preliminaries Our goal in this paper is to provide data structures that map n-gram keys to values, i.e. probabilities or counts. Maps are fundamental data structures and generic implementations of mapping data structures are readily available. However, because of the sheer number of keys and"
P11-1027,D09-1079,0,0.0322867,"Missing"
P11-1027,W08-0402,0,0.0177293,"hen, rather than represent the LM context in the decoder as an explicit list of words, we can simply store context offsets. When we query the language model, we get back both a language model score and context offset ˆ 1n−1 is the the longest suffix of c(w ˆ 1n−1 ), where w w1n−1 contained in the language model. We can then quickly form the context encoding of the next query by simply concatenating the new word with the offset c(w ˆ 1n−1 ) returned from the previous query. In addition to speeding up language model queries, this approach also automatically supports an equivalence of LM states (Li and Khudanpur, 2008): in standard back-off schemes, whenever we compute the probability for an n-gram (wn , c(wn−1 )) when 1 wn−1 is not in the language model, the result will be 1 the same as the result of the query (wn , c(w ˆ 1n−1 ). It is therefore only necessary to store as much of the context as the language model contains instead of all n − 1 words in the context. If a decoder maintains LM states using the context offsets returned by our language model, then the decoder will automatically exploit this equivalence and the size of the search space will be reduced. This same effect is exploited explicitly by"
P11-1027,W09-0424,0,0.0120118,"ublished lossless compression to date. 4 Speeding up Decoding In the previous section, we provided compact and efficient implementations of associative arrays that allow us to query a value for an arbitrary n-gram. However, decoders do not issue language model requests at random. In this section, we show that language model requests issued by a standard decoder exhibit two patterns we can exploit: they are highly repetitive, and also exhibit a scrolling effect. 4.1 Exploiting Repetitive Queries In a simple experiment, we recorded all of the language model queries issued by the Joshua decoder (Li et al., 2009) on a 100 sentence test set. Of the 31 million queries, only about 1 million were unique. Therefore, we expect that keeping the results of language model queries in a cache should be effective at reducing overall language model latency. To this end, we added a very simple cache to our language model. Our cache uses an array of key/value pairs with size fixed to 2b − 1 for some integer b (we used 24). We use a b-bit hash function to compute the address in an array where we will always place a given n-gram and its fully computed language model score. Querying the cache is straightforward: we che"
P11-1027,J04-4002,0,0.0169683,"Missing"
P11-1049,D08-1024,0,0.00885439,"T: 1(docCount(b) = ·) where docCount(b) is the number of documents containing b. S TOP: 1(isStop(b1 ) = ·, isStop(b2 ) = ·) where isStop(w) indicates a stop word. P OSITION: 1(docPosition(b) = ·) where docPosition(b) is the earliest position in a document of any sentence containing b, buckets earliest positions ≥ 4. C ONJ: All two- and three-way conjunctions of C OUNT, S TOP, and P OSITION features. B IAS: Bias feature, active on all bigrams. Table 1: Bigram features: component feature functions in g(b, x) that we use to characterize the bigram b in both the extractive and compressive models. Chiang et al., 2008). Previous work has referred to the lack of extracted, compressed data sets as an obstacle to joint learning for summarizaiton (Daum´e III, 2006; Martins and Smith, 2009). We collected joint data via a Mechanical Turk task. To make the joint annotation task more feasible, we adopted an approximate approach that closely matches our fast approximate prediction procedure. Annotators were shown a 150-word maximum bigram recall extractions from the full document set and instructed to form a compressed summary by deleting words until 100 or fewer words remained. Each task was performed by two annota"
P11-1049,W09-1802,1,0.726218,"s certainly simple and does guarantee some minimal readability, Lin (2003) showed that sentence compression (Knight and Marcu, 2001; McDonald, 2006; Clarke and Lapata, 2008) has the potential to improve the resulting summaries. However, attempts to incorporate compression into a summarization system have largely failed to realize large gains. For example, Zajic et al (2006) use a pipeline approach, pre-processing to yield additional candidates for extraction by applying heuristic sentence compressions, but their system does not outperform state-of-the-art purely extractive systems. Similarly, Gillick and Favre (2009), though not learning weights, do a limited form of compression jointly with extraction. They report a marginal increase in the automatic wordoverlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). A second contribution of the current work is to show a system for jointly learning to jointly compress and extract that exhibits gains in both ROUGE and content metrics over purely extractive systems. Both Martins and Smith (2009) and Woodsend and Lapata (2010) build models that jointly extract and compress, but learn scores for sentences (or phrases) using"
P11-1049,W10-0722,1,0.343109,"four words) are given by the official ROUGE toolkit with the standard options (Lin, 2004). Pyramid (Nenkova and Passonneau, 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries. It is designed to help annotators distinguish information content from linguistic quality. Two annotators performed the entire evaluation without overlap by splitting the set of problems in half. To evaluate linguistic quality, we sent all the summaries to Mechanical Turk (with two times redun488 dancy), using the template and instructions designed by Gillick and Liu (2010). They report that Turkers can faithfully reproduce experts’ rankings of average system linguistic quality (though their judgements of content are poorer). The table shows average linguistic quality. All the content-based metrics show substantial improvement for learned systems over unlearned ones, and we see an extremely large improvement for the learned joint extractive and compressive system over the previous state-of-the-art E XTRACTIVE BASELINE. The ROUGE scores for the learned joint system, L EARNED C OMPRESSIVE, are, to our knowledge, the highest reported on this task. We cannot compare"
P11-1049,P06-1096,1,0.262422,"Missing"
P11-1049,W03-1101,0,0.0232796,"2007) learn predictors of individual words’ appearance in the references, but in isolation from the sentence selection procedure. Exceptions are Li et al. (2009) who take a max-margin approach to learning sentence values jointly, but still have ad hoc constraints to handle redundancy. One main contribution of the current paper is the direct optimization of summary quality in a single model; we find that our learned systems substantially outperform unlearned counterparts on both automatic and manual metrics. While pure extraction is certainly simple and does guarantee some minimal readability, Lin (2003) showed that sentence compression (Knight and Marcu, 2001; McDonald, 2006; Clarke and Lapata, 2008) has the potential to improve the resulting summaries. However, attempts to incorporate compression into a summarization system have largely failed to realize large gains. For example, Zajic et al (2006) use a pipeline approach, pre-processing to yield additional candidates for extraction by applying heuristic sentence compressions, but their system does not outperform state-of-the-art purely extractive systems. Similarly, Gillick and Favre (2009), though not learning weights, do a limited form o"
P11-1049,W04-1013,0,0.357697,"ial to improve the resulting summaries. However, attempts to incorporate compression into a summarization system have largely failed to realize large gains. For example, Zajic et al (2006) use a pipeline approach, pre-processing to yield additional candidates for extraction by applying heuristic sentence compressions, but their system does not outperform state-of-the-art purely extractive systems. Similarly, Gillick and Favre (2009), though not learning weights, do a limited form of compression jointly with extraction. They report a marginal increase in the automatic wordoverlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). A second contribution of the current work is to show a system for jointly learning to jointly compress and extract that exhibits gains in both ROUGE and content metrics over purely extractive systems. Both Martins and Smith (2009) and Woodsend and Lapata (2010) build models that jointly extract and compress, but learn scores for sentences (or phrases) using independent classifiers. Daum´e III (2006) Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 481–490, c Portland, Oregon, June 19"
P11-1049,W09-1801,0,0.871248,"ying heuristic sentence compressions, but their system does not outperform state-of-the-art purely extractive systems. Similarly, Gillick and Favre (2009), though not learning weights, do a limited form of compression jointly with extraction. They report a marginal increase in the automatic wordoverlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). A second contribution of the current work is to show a system for jointly learning to jointly compress and extract that exhibits gains in both ROUGE and content metrics over purely extractive systems. Both Martins and Smith (2009) and Woodsend and Lapata (2010) build models that jointly extract and compress, but learn scores for sentences (or phrases) using independent classifiers. Daum´e III (2006) Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 481–490, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics learns parameters for compression and extraction jointly using an approximate training procedure, but his results are not competitive with state-of-the-art extractive systems, and he does not report improvements on manual content or quali"
P11-1049,E06-1038,0,0.132293,"ces, but in isolation from the sentence selection procedure. Exceptions are Li et al. (2009) who take a max-margin approach to learning sentence values jointly, but still have ad hoc constraints to handle redundancy. One main contribution of the current paper is the direct optimization of summary quality in a single model; we find that our learned systems substantially outperform unlearned counterparts on both automatic and manual metrics. While pure extraction is certainly simple and does guarantee some minimal readability, Lin (2003) showed that sentence compression (Knight and Marcu, 2001; McDonald, 2006; Clarke and Lapata, 2008) has the potential to improve the resulting summaries. However, attempts to incorporate compression into a summarization system have largely failed to realize large gains. For example, Zajic et al (2006) use a pipeline approach, pre-processing to yield additional candidates for extraction by applying heuristic sentence compressions, but their system does not outperform state-of-the-art purely extractive systems. Similarly, Gillick and Favre (2009), though not learning weights, do a limited form of compression jointly with extraction. They report a marginal increase in"
P11-1049,N04-1019,0,0.614718,"er, attempts to incorporate compression into a summarization system have largely failed to realize large gains. For example, Zajic et al (2006) use a pipeline approach, pre-processing to yield additional candidates for extraction by applying heuristic sentence compressions, but their system does not outperform state-of-the-art purely extractive systems. Similarly, Gillick and Favre (2009), though not learning weights, do a limited form of compression jointly with extraction. They report a marginal increase in the automatic wordoverlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). A second contribution of the current work is to show a system for jointly learning to jointly compress and extract that exhibits gains in both ROUGE and content metrics over purely extractive systems. Both Martins and Smith (2009) and Woodsend and Lapata (2010) build models that jointly extract and compress, but learn scores for sentences (or phrases) using independent classifiers. Daum´e III (2006) Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 481–490, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics learns"
P11-1049,P08-2052,0,0.0136493,"out a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set. 1 Introduction Applications of machine learning to automatic summarization have met with limited success, and, as a result, many top-performing systems remain largely ad-hoc. One reason learning may have provided limited gains is that typical models do not learn to optimize end summary quality directly, but rather learn intermediate quantities in isolation. For example, many models learn to score each input sentence independently (Teufel and Moens, 1997; Shen et al., 2007; Schilder and Kondadadi, 2008), and then assemble extractive summaries from the top-ranked sentences in a way not incorporated into the learning process. This extraction is often done in the 481 presence of a heuristic that limits redundancy. As another example, Yih et al. (2007) learn predictors of individual words’ appearance in the references, but in isolation from the sentence selection procedure. Exceptions are Li et al. (2009) who take a max-margin approach to learning sentence values jointly, but still have ad hoc constraints to handle redundancy. One main contribution of the current paper is the direct optimization"
P11-1049,W04-3201,1,0.160476,"nly O( N ) constraints are added before constraint induction finds a C-optimal solution. Loss-augmented prediction is not always tractable. Luckily, our choice of loss function, bigram recall, factors over bigrams. Thus, we can easily perform loss-augmented prediction using the same procedure we use to perform Viterbi prediction (described in Section 4). We simply modify each bigram value vb to include bigram b’s contribution to the total loss. We solve the intermediate partially-constrained max-margin problems using the factored sequential minimal optimization (SMO) algorithm (Platt, 1999; Taskar et al., 2004). In practice, for  = 10−4 , the cutting-plane algorithm converges after only three passes through the training set when applied to our summarization task. 3.3 Loss function In the simplest case, 0-1 loss, the system only receives credit for exactly identifying the label summary. Since there are many reasonable summaries we are less interested in exactly matching any specific training instance, and more interested in the degree to which a predicted summary deviates from a label. The standard method for automatically evaluating a summary against a reference is ROUGE, which we simplify slightly"
P11-1049,W97-0710,0,0.209424,"only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set. 1 Introduction Applications of machine learning to automatic summarization have met with limited success, and, as a result, many top-performing systems remain largely ad-hoc. One reason learning may have provided limited gains is that typical models do not learn to optimize end summary quality directly, but rather learn intermediate quantities in isolation. For example, many models learn to score each input sentence independently (Teufel and Moens, 1997; Shen et al., 2007; Schilder and Kondadadi, 2008), and then assemble extractive summaries from the top-ranked sentences in a way not incorporated into the learning process. This extraction is often done in the 481 presence of a heuristic that limits redundancy. As another example, Yih et al. (2007) learn predictors of individual words’ appearance in the references, but in isolation from the sentence selection procedure. Exceptions are Li et al. (2009) who take a max-margin approach to learning sentence values jointly, but still have ad hoc constraints to handle redundancy. One main contributi"
P11-1049,P10-1058,0,0.14535,"essions, but their system does not outperform state-of-the-art purely extractive systems. Similarly, Gillick and Favre (2009), though not learning weights, do a limited form of compression jointly with extraction. They report a marginal increase in the automatic wordoverlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). A second contribution of the current work is to show a system for jointly learning to jointly compress and extract that exhibits gains in both ROUGE and content metrics over purely extractive systems. Both Martins and Smith (2009) and Woodsend and Lapata (2010) build models that jointly extract and compress, but learn scores for sentences (or phrases) using independent classifiers. Daum´e III (2006) Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 481–490, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics learns parameters for compression and extraction jointly using an approximate training procedure, but his results are not competitive with state-of-the-art extractive systems, and he does not report improvements on manual content or quality metrics. In our approach, we"
P11-1060,P09-1010,0,0.521693,"ng problem. Piantadosi et al. (2008) induces first-order formulae using CCG in a small domain assuming observed lexical semantics. The closest work to ours is Clarke et al. (2010), which we discussed earlier. The integration of natural language with denotations computed against a world (grounding) is becoming increasingly popular. Feedback from the world has been used to guide both syntactic parsing (Schuler, 2003) and semantic parsing (Popescu et al., 2003; Clarke et al., 2010). Past work has also focused on aligning text to a world (Liang et al., 2009), using text in reinforcement learning (Branavan et al., 2009; Branavan et al., 2010), and many others. Our work pushes the grounded language agenda towards deeper representations of language—think grounded compositional semantics. 6 Conclusion We built a system that interprets natural language utterances much more accurately than existing systems, despite using no annotated logical forms. Our system is based on a new semantic representation, DCS, which offers a simple and expressive alternative to lambda calculus. Free from the burden of annotating logical forms, we hope to use our techniques in developing even more accurate and broader-coverage langua"
P11-1060,P10-1129,0,0.285074,"et al. (2008) induces first-order formulae using CCG in a small domain assuming observed lexical semantics. The closest work to ours is Clarke et al. (2010), which we discussed earlier. The integration of natural language with denotations computed against a world (grounding) is becoming increasingly popular. Feedback from the world has been used to guide both syntactic parsing (Schuler, 2003) and semantic parsing (Popescu et al., 2003; Clarke et al., 2010). Past work has also focused on aligning text to a world (Liang et al., 2009), using text in reinforcement learning (Branavan et al., 2009; Branavan et al., 2010), and many others. Our work pushes the grounded language agenda towards deeper representations of language—think grounded compositional semantics. 6 Conclusion We built a system that interprets natural language utterances much more accurately than existing systems, despite using no annotated logical forms. Our system is based on a new semantic representation, DCS, which offers a simple and expressive alternative to lambda calculus. Free from the burden of annotating logical forms, we hope to use our techniques in developing even more accurate and broader-coverage language understanding systems"
P11-1060,W10-2903,0,0.815625,"e types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language. Figure 1 shows our probabilistic model: 590 We want to induce latent logical forms z (and parameters θ) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs. The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answe"
P11-1060,D09-1100,0,0.0135121,"s in programs (DCS trees) which are much simpler than the logically-equivalent lambda calculus formulae. The idea of using CSPs to represent semantics is inspired by Discourse Representation Theory (DRT) (Kamp and Reyle, 1993; Kamp et al., 2005), where variables are discourse referents. The restriction to trees is similar to economical DRT (Bos, 2009). The other major focus of this work is program induction—inferring logical forms from their denotations. There has been a fair amount of past work on this topic: Liang et al. (2010) induces combinatory logic programs in a non-linguistic setting. Eisenstein et al. (2009) induces conjunctive formulae and uses them as features in another learning problem. Piantadosi et al. (2008) induces first-order formulae using CCG in a small domain assuming observed lexical semantics. The closest work to ours is Clarke et al. (2010), which we discussed earlier. The integration of natural language with denotations computed against a world (grounding) is becoming increasingly popular. Feedback from the world has been used to guide both syntactic parsing (Schuler, 2003) and semantic parsing (Popescu et al., 2003; Clarke et al., 2010). Past work has also focused on aligning tex"
P11-1060,W05-0602,0,0.0374606,"1 z ∼ pθ (z |x) area c argmax y = JzKw Figure 1: Our probabilistic model: a question x is mapped to a latent logical form z, which is then evaluated with respect to a world w (database of facts), producing an answer y. We represent logical forms z as labeled trees, induced automatically from (x, y) pairs. Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to captu"
P11-1060,D10-1119,0,0.86269,"ch is then evaluated with respect to a world w (database of facts), producing an answer y. We represent logical forms z as labeled trees, induced automatically from (x, y) pairs. Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language. Figure 1 shows our probabilistic model: 590 We want to induce latent logical forms z (and parameters θ"
P11-1060,P09-1011,1,0.536083,"nctive formulae and uses them as features in another learning problem. Piantadosi et al. (2008) induces first-order formulae using CCG in a small domain assuming observed lexical semantics. The closest work to ours is Clarke et al. (2010), which we discussed earlier. The integration of natural language with denotations computed against a world (grounding) is becoming increasingly popular. Feedback from the world has been used to guide both syntactic parsing (Schuler, 2003) and semantic parsing (Popescu et al., 2003; Clarke et al., 2010). Past work has also focused on aligning text to a world (Liang et al., 2009), using text in reinforcement learning (Branavan et al., 2009; Branavan et al., 2010), and many others. Our work pushes the grounded language agenda towards deeper representations of language—think grounded compositional semantics. 6 Conclusion We built a system that interprets natural language utterances much more accurately than existing systems, despite using no annotated logical forms. Our system is based on a new semantic representation, DCS, which offers a simple and expressive alternative to lambda calculus. Free from the burden of annotating logical forms, we hope to use our techniques"
P11-1060,P06-1055,1,0.413012,"Missing"
P11-1060,D09-1001,0,0.303397,"omatically from (x, y) pairs. Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language. Figure 1 shows our probabilistic model: 590 We want to induce latent logical forms z (and parameters θ) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs. The core problem that arises in this setting is progra"
P11-1060,P03-1067,0,0.0381539,"on this topic: Liang et al. (2010) induces combinatory logic programs in a non-linguistic setting. Eisenstein et al. (2009) induces conjunctive formulae and uses them as features in another learning problem. Piantadosi et al. (2008) induces first-order formulae using CCG in a small domain assuming observed lexical semantics. The closest work to ours is Clarke et al. (2010), which we discussed earlier. The integration of natural language with denotations computed against a world (grounding) is becoming increasingly popular. Feedback from the world has been used to guide both syntactic parsing (Schuler, 2003) and semantic parsing (Popescu et al., 2003; Clarke et al., 2010). Past work has also focused on aligning text to a world (Liang et al., 2009), using text in reinforcement learning (Branavan et al., 2009; Branavan et al., 2010), and many others. Our work pushes the grounded language agenda towards deeper representations of language—think grounded compositional semantics. 6 Conclusion We built a system that interprets natural language utterances much more accurately than existing systems, despite using no annotated logical forms. Our system is based on a new semantic representation, DCS, which"
P11-1060,P07-1121,0,0.913929,"ent logical form z, which is then evaluated with respect to a world w (database of facts), producing an answer y. We represent logical forms z as labeled trees, induced automatically from (x, y) pairs. Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language. Figure 1 shows our probabilistic model: 590 We want to induce latent logica"
P11-1060,D07-1071,0,0.91835,"a question x is mapped to a latent logical form z, which is then evaluated with respect to a world w (database of facts), producing an answer y. We represent logical forms z as labeled trees, induced automatically from (x, y) pairs. Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language. Figure 1 shows our probabilistic model: 590 We want"
P11-1060,W11-0103,0,\N,Missing
P11-1060,J93-2004,0,\N,Missing
P11-1060,C04-1180,0,\N,Missing
P11-1060,J03-4003,0,\N,Missing
P11-1060,D11-1039,0,\N,Missing
P11-1060,N06-1056,0,\N,Missing
P11-1060,P10-1083,0,\N,Missing
P11-1060,P02-1041,0,\N,Missing
P11-1060,D08-1082,0,\N,Missing
P11-1060,P06-1063,0,\N,Missing
P11-1060,P06-1115,0,\N,Missing
P11-1060,P96-1008,0,\N,Missing
P11-1060,J82-3002,0,\N,Missing
P11-1060,P11-1149,0,\N,Missing
P11-1060,D11-1140,0,\N,Missing
P11-1060,P11-1028,0,\N,Missing
P11-1060,N09-1069,1,\N,Missing
P11-1070,J07-4002,0,0.0225942,"auer (1995), for example, would be to take an ambiguous noun sequence like hydrogen ion exchange and compare the various counts (or associated conditional probabilities) of n-grams like hydrogen ion and hydrogen exchange. The attachment with the greater score is chosen. More recently, Pitler et al. (2010) use web-scale n-grams to compute similar association statistics for longer sequences of nouns. Our affinity features closely follow this basic idea of association statistics. However, because a real parser will not have access to gold-standard knowledge of the competing attachment sites (see Atterer and Schutze (2007)’s criticism of previous work), we must instead compute features for all possible head-argument pairs from our web corpus. Moreover, when there are only two competing attachment options, one can do things like directly compare two count-based heuristics and choose the larger. Integration into a parser requires features to be functions of single attachments, not pairwise comparisons between alternatives. A learning algorithm can then weight features so that they compare appropriately 695 across parses. We employ a collection of affinity features of varying specificity. The basic feature is the"
P11-1070,P05-1022,0,0.142616,"Missing"
P11-1070,J05-1003,0,0.0577981,"Missing"
P11-1070,W02-1001,0,0.108147,"Missing"
P11-1070,P08-1109,0,0.0639056,"Missing"
P11-1070,P08-1067,0,0.0383059,"Missing"
P11-1070,P10-1001,0,0.0108779,"ocal) features. Our baseline system is the Berkeley parser, from which we obtain k-best lists for the development set (WSJ section 22) and test set (WSJ section 23) using a grammar trained on all the training data (WSJ sections 2-21).8 To get k-best lists for the training set, we use 3-fold jackknifing where we train a grammar 6 Their README specifies ‘training-k:5 iters:10 losstype:nopunc decode-type:proj’, which we used for all final experiments; we used the faster ‘training-k:1 iters:5’ setting for most development experiments. 7 Work such as Smith and Eisner (2008), Martins et al. (2009), Koo and Collins (2010) has been exploring more nonlocal features for dependency parsing. It will be interesting to see how these features interact with our web features. 8 Settings: 6 iterations of split and merge with smoothing. 698 k = 1 k = 2 k = 10 k = 25 k = 50 k = 100 Dev 90.6 92.3 95.1 95.8 96.2 96.5 Test 90.2 91.8 94.7 95.6 96.1 96.4 Table 2: Oracle F1-scores for k-best lists output by Berkeley parser for English WSJ parsing (Dev is section 22 and Test is section 23, all lengths). on 2 folds to get parses for the third fold.9 The oracle scores of the k-best lists (for different values of k) for the developm"
P11-1070,P08-1068,0,0.185751,"consider how to efficiently mine the Google n-grams corpus. Given the success of Web counts for isolated ambiguities, there is relatively little previous research in this direction. The most similar work is Pitler et al. (2010), which use Web-scale n-gram counts for multi-way noun bracketing decisions, though that work considers only sequences of nouns and uses only affinity-based web features. Yates et al. (2006) use Web counts to filter out certain ‘semantically bad’ parses from extraction candidate sets but are not concerned with distinguishing amongst top parses. In an important contrast, Koo et al. (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using clusterbased word-senses as intermediate abstractions in 694 addition to POS tags (also see Finkel et al. (2008)). Their work also gives a way to tap into corpora beyond the training data, through cluster membership rather than explicit corpus counts and paraphrases. This work uses a large web-scale corpus (Google n-grams) to compute features for the full parsing task. To show end-to-end effectiveness, we incorporate our features into state-of-the-art dependency and constituent parsers. For the dependency"
P11-1070,N04-1016,0,0.395696,"ld edges are in dashed gold and edges common in guess and gold parses are in black. attachment ambiguity where by yesterday afternoon should attach to had already, (b) is an NP-internal ambiguity where half a should attach to dozen and not to newspapers, and (c) is an adverb attachment ambiguity, where just should modify fine and not the verb ’s. Resolving many of these errors requires information that is simply not present in the approximately 1M words on which the parser was trained. One way to access more information is to exploit surface counts from large corpora like the web (Volk, 2001; Lapata and Keller, 2004). For example, the phrase raising from is much more frequent on the Web than $ x billion from. While this ‘affinity’ is only a surface correlation, Volk (2001) showed that comparing such counts can often correctly resolve tricky PP attachments. This basic idea has led to a good deal of successful work on disambiguating isolated, binary PP attachments. For example, Nakov and Hearst (2005b) showed that looking for paraphrase counts can further improve PP resolution. In this case, the existence of reworded phrases like raising it from on the Web also imply a verbal atProceedings of the 49th Annua"
P11-1070,P95-1007,0,0.0762287,"ead ?(raising ?($ from) raising arg) $ from) from debt Figure 3: Features factored over head-argument pairs. pairs, as is standard in the dependency parsing literature (see Figure 3). Here, we discuss which webcount based features φ(h, a) should fire over a given head-argument pair (we consider the words h and a to be indexed, and so features can be sensitive to their order and distance, as is also standard). 2.1 Affinity Features Affinity statistics, such as lexical co-occurrence counts from large corpora, have been used previously for resolving individual attachments at least as far back as Lauer (1995) for noun-compound bracketing, and later for PP attachment (Volk, 2001; Lapata and Keller, 2004) and coordination ambiguity (Nakov and Hearst, 2005b). The approach of Lauer (1995), for example, would be to take an ambiguous noun sequence like hydrogen ion exchange and compare the various counts (or associated conditional probabilities) of n-grams like hydrogen ion and hydrogen exchange. The attachment with the greater score is chosen. More recently, Pitler et al. (2010) use web-scale n-grams to compute similar association statistics for longer sequences of nouns. Our affinity features closely"
P11-1070,P09-1039,0,0.00841255,"using (generally non-local) features. Our baseline system is the Berkeley parser, from which we obtain k-best lists for the development set (WSJ section 22) and test set (WSJ section 23) using a grammar trained on all the training data (WSJ sections 2-21).8 To get k-best lists for the training set, we use 3-fold jackknifing where we train a grammar 6 Their README specifies ‘training-k:5 iters:10 losstype:nopunc decode-type:proj’, which we used for all final experiments; we used the faster ‘training-k:1 iters:5’ setting for most development experiments. 7 Work such as Smith and Eisner (2008), Martins et al. (2009), Koo and Collins (2010) has been exploring more nonlocal features for dependency parsing. It will be interesting to see how these features interact with our web features. 8 Settings: 6 iterations of split and merge with smoothing. 698 k = 1 k = 2 k = 10 k = 25 k = 50 k = 100 Dev 90.6 92.3 95.1 95.8 96.2 96.5 Test 90.2 91.8 94.7 95.6 96.1 96.4 Table 2: Oracle F1-scores for k-best lists output by Berkeley parser for English WSJ parsing (Dev is section 22 and Test is section 23, all lengths). on 2 folds to get parses for the third fold.9 The oracle scores of the k-best lists (for different value"
P11-1070,E06-1011,0,0.366453,"et al., 2010). For example, in (b), half dozen is more frequent than half newspapers. In this paper, we show how to apply these ideas to all attachments in full-scale parsing. Doing so requires three main issues to be addressed. First, we show how features can be generated for arbitrary head-argument configurations. Affinity features are relatively straightforward, but paraphrase features, which have been hand-developed in the past, are more complex. Second, we integrate our features into full-scale parsing systems. For dependency parsing, we augment the features in the second-order parser of McDonald and Pereira (2006). For constituent parsing, we rerank the output of the Berkeley parser (Petrov et al., 2006). Third, past systems have usually gotten their counts from web search APIs, which does not scale to quadratically-many attachments in each sentence. Instead, we consider how to efficiently mine the Google n-grams corpus. Given the success of Web counts for isolated ambiguities, there is relatively little previous research in this direction. The most similar work is Pitler et al. (2010), which use Web-scale n-gram counts for multi-way noun bracketing decisions, though that work considers only sequences"
P11-1070,P05-1012,0,0.126243,"riments Our features are designed to be used in full-sentence parsing rather than for limited decisions about isolated ambiguities. We first integrate our features into a dependency parser, where the integration is more natural and pushes all the way into the underlying dynamic program. We then add them to a constituent parser in a reranking approach. We also verify that our features contribute on top of standard reranking features.3 4.1 Dependency Parsing For dependency parsing, we use the discriminatively-trained MSTParser4 , an implementation of first and second order MST parsing models of McDonald et al. (2005) and McDonald and Pereira (2006). We use the standard splits of Penn Treebank into training (sections 2-21), development (section 22) and test (section 23). We used the ‘pennconverter’5 tool to convert Penn trees from constituent format to dependency format. Following Koo et al. (2008), we used the MXPOST tagger (Ratnaparkhi, 1996) trained on the full training data to provide part-of-speech tags for the development 3 All reported experiments are run on all sentences, i.e. without any length limit. 4 http://sourceforge.net/projects/mstparser 5 This supersedes ‘Penn2Malt’ and is available at htt"
P11-1070,W05-0603,0,0.268752,"formation that is simply not present in the approximately 1M words on which the parser was trained. One way to access more information is to exploit surface counts from large corpora like the web (Volk, 2001; Lapata and Keller, 2004). For example, the phrase raising from is much more frequent on the Web than $ x billion from. While this ‘affinity’ is only a surface correlation, Volk (2001) showed that comparing such counts can often correctly resolve tricky PP attachments. This basic idea has led to a good deal of successful work on disambiguating isolated, binary PP attachments. For example, Nakov and Hearst (2005b) showed that looking for paraphrase counts can further improve PP resolution. In this case, the existence of reworded phrases like raising it from on the Web also imply a verbal atProceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 693–702, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics (a) NP NP (b) S QP VP PP had already … PDT … half …Lehman Hutton Inc. (c) NP DT VBZ PDT PDT … ´s dozen newspapers a VP ADVP ADJP ADJP RB JJ just fine by yesterday afternoon Figure 2: Different kinds of attachment errors in the pa"
P11-1070,H05-1105,0,0.325504,"formation that is simply not present in the approximately 1M words on which the parser was trained. One way to access more information is to exploit surface counts from large corpora like the web (Volk, 2001; Lapata and Keller, 2004). For example, the phrase raising from is much more frequent on the Web than $ x billion from. While this ‘affinity’ is only a surface correlation, Volk (2001) showed that comparing such counts can often correctly resolve tricky PP attachments. This basic idea has led to a good deal of successful work on disambiguating isolated, binary PP attachments. For example, Nakov and Hearst (2005b) showed that looking for paraphrase counts can further improve PP resolution. In this case, the existence of reworded phrases like raising it from on the Web also imply a verbal atProceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 693–702, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics (a) NP NP (b) S QP VP PP had already … PDT … half …Lehman Hutton Inc. (c) NP DT VBZ PDT PDT … ´s dozen newspapers a VP ADVP ADJP ADJP RB JJ just fine by yesterday afternoon Figure 2: Different kinds of attachment errors in the pa"
P11-1070,P08-1052,0,0.0189483,"stracted versions of these paraphrase features where the context words c are collapsed to their parts-of-speech POS(c), obtained using a unigram-tagger trained on the parser training set. As discussed in Section 5, the top features learned by our learning algorithm duplicate the hand-crafted configurations used in previous work (Nakov and Hearst, 2005b) but also add numerous others, and, of course, apply to many more attachment types. Working with Web n-Grams 3 Previous approaches have generally used search engines to collect count statistics (Lapata and Keller, 2004; Nakov and Hearst, 2005b; Nakov and Hearst, 2008). Lapata and Keller (2004) uses the number of page hits as the web-count of the queried ngram (which is problematic according to Kilgarriff (2007)). Nakov and Hearst (2008) post-processes the first 1000 result snippets. One challenge with this approach is that an external search API is now embedded into the parser, raising issues of both speed and daily query limits, especially if all possible attachments trigger queries. Such methods also create a dependence on the quality and postprocessing of the search results, limitations of the query process (for instance, search engines can ignore punct"
P11-1070,P06-1055,1,0.0718814,", we show how to apply these ideas to all attachments in full-scale parsing. Doing so requires three main issues to be addressed. First, we show how features can be generated for arbitrary head-argument configurations. Affinity features are relatively straightforward, but paraphrase features, which have been hand-developed in the past, are more complex. Second, we integrate our features into full-scale parsing systems. For dependency parsing, we augment the features in the second-order parser of McDonald and Pereira (2006). For constituent parsing, we rerank the output of the Berkeley parser (Petrov et al., 2006). Third, past systems have usually gotten their counts from web search APIs, which does not scale to quadratically-many attachments in each sentence. Instead, we consider how to efficiently mine the Google n-grams corpus. Given the success of Web counts for isolated ambiguities, there is relatively little previous research in this direction. The most similar work is Pitler et al. (2010), which use Web-scale n-gram counts for multi-way noun bracketing decisions, though that work considers only sequences of nouns and uses only affinity-based web features. Yates et al. (2006) use Web counts to fi"
P11-1070,C10-1100,0,0.434752,"inguistics (a) NP NP (b) S QP VP PP had already … PDT … half …Lehman Hutton Inc. (c) NP DT VBZ PDT PDT … ´s dozen newspapers a VP ADVP ADJP ADJP RB JJ just fine by yesterday afternoon Figure 2: Different kinds of attachment errors in the parse output of the Berkeley parser (on Penn Treebank). Guess edges are in solid blue, gold edges are in dashed gold and edges common in guess and gold parses are in black. tachment. Still other work has exploited Web counts for other isolated ambiguities, such as NP coordination (Nakov and Hearst, 2005b) and noun-sequence bracketing (Nakov and Hearst, 2005a; Pitler et al., 2010). For example, in (b), half dozen is more frequent than half newspapers. In this paper, we show how to apply these ideas to all attachments in full-scale parsing. Doing so requires three main issues to be addressed. First, we show how features can be generated for arbitrary head-argument configurations. Affinity features are relatively straightforward, but paraphrase features, which have been hand-developed in the past, are more complex. Second, we integrate our features into full-scale parsing systems. For dependency parsing, we augment the features in the second-order parser of McDonald and"
P11-1070,W96-0213,0,0.194628,"Missing"
P11-1070,D08-1016,0,0.0212834,"n rerank this k-best list using (generally non-local) features. Our baseline system is the Berkeley parser, from which we obtain k-best lists for the development set (WSJ section 22) and test set (WSJ section 23) using a grammar trained on all the training data (WSJ sections 2-21).8 To get k-best lists for the training set, we use 3-fold jackknifing where we train a grammar 6 Their README specifies ‘training-k:5 iters:10 losstype:nopunc decode-type:proj’, which we used for all final experiments; we used the faster ‘training-k:1 iters:5’ setting for most development experiments. 7 Work such as Smith and Eisner (2008), Martins et al. (2009), Koo and Collins (2010) has been exploring more nonlocal features for dependency parsing. It will be interesting to see how these features interact with our web features. 8 Settings: 6 iterations of split and merge with smoothing. 698 k = 1 k = 2 k = 10 k = 25 k = 50 k = 100 Dev 90.6 92.3 95.1 95.8 96.2 96.5 Test 90.2 91.8 94.7 95.6 96.1 96.4 Table 2: Oracle F1-scores for k-best lists output by Berkeley parser for English WSJ parsing (Dev is section 22 and Test is section 23, all lengths). on 2 folds to get parses for the third fold.9 The oracle scores of the k-best lis"
P11-1070,P07-1031,0,0.0115344,"Missing"
P11-1070,W06-1604,0,0.0237835,"he Berkeley parser (Petrov et al., 2006). Third, past systems have usually gotten their counts from web search APIs, which does not scale to quadratically-many attachments in each sentence. Instead, we consider how to efficiently mine the Google n-grams corpus. Given the success of Web counts for isolated ambiguities, there is relatively little previous research in this direction. The most similar work is Pitler et al. (2010), which use Web-scale n-gram counts for multi-way noun bracketing decisions, though that work considers only sequences of nouns and uses only affinity-based web features. Yates et al. (2006) use Web counts to filter out certain ‘semantically bad’ parses from extraction candidate sets but are not concerned with distinguishing amongst top parses. In an important contrast, Koo et al. (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using clusterbased word-senses as intermediate abstractions in 694 addition to POS tags (also see Finkel et al. (2008)). Their work also gives a way to tap into corpora beyond the training data, through cluster membership rather than explicit corpus counts and paraphrases. This work uses a large web-scale corpus (G"
P11-1070,J03-4003,0,\N,Missing
P11-2005,D08-1087,0,0.0593334,"Missing"
P11-2005,P10-2041,0,0.0409051,"racting from the count of each n-gram, is one of the core aspects of Kneser-Ney language modeling (Kneser and Ney, 1995). For all but the smallest n-gram counts, Kneser-Ney uses a single discount, one that does not grow with the ngram count, because such constant-discounting was seen in early experiments on held-out data (Church and Gale, 1991). However, due to increasing computational power and corpus sizes, language modeling today presents a different set of challenges than it did 20 years ago. In particular, modeling crossdomain effects has become increasingly more important (Klakow, 2000; Moore and Lewis, 2010), and deployed systems must frequently process data that is out-of-domain from the standpoint of the language model. In this work, we perform experiments on heldout data to evaluate how discounting behaves in the Discount Analysis Underlying discounting is the idea that n-grams will occur fewer times in test data than they do in training data. We investigate this quantitatively by conducting experiments similar in spirit to those of Church and Gale (1991). Suppose that we have collected counts on two corpora of the same size, which we will call our train and test corpora. For an n-gram w = (w1"
P11-2005,P09-2088,0,0.0309829,"m types exhibit similar discount relationships. from different years, and between the NYT and AFP newswire, discounts grow even more quickly. We observed these trends continuing steadily up into ngram counts in the hundreds, beyond which point it becomes difficult to robustly estimate discounts due to fewer n-gram types in this count range. This result is surprising in light of the constant discounts observed for the NYT95/NYT950 pair. Goodman (2001) proposes that discounts arise from document-level “burstiness” in a corpus, because language often repeats itself locally within a document, and Moore and Quirk (2009) suggest that discounting also corrects for quantization error due to estimating a continuous distribution using a discrete maximum likelihood estimator (MLE). Both of these factors are at play in the NYT95/NYT950 experiment, and yet only a small, constant discount is observed. Our growing discounts must therefore be caused by other, larger-scale phenomena, such as shifts in the subjects of news articles over time or in the style of the writing between newswire sources. The increasing rate of discount growth as the source changes and temporal divergence increases lends credence to this hypothe"
P11-2005,P06-1124,0,0.105458,"Missing"
P11-2127,P10-1112,1,0.871391,"P The all-fragments grammar (AFG) for a (binarized) treebank is formally the tree-substitution grammar (TSG) (Resnik, 1992; Bod, 1993) that consists of all fragments (elementary trees) of all training trees in the treebank, with some weighting on each fragment. AFGs are too large to fully extract explicitly; researchers therefore either work with a tractable subset of the fragments (Sima’an, 2000; Bod, 2001; Post and Gildea, 2009; Cohn and Blunsom, 2010) or use a PCFG reduction like that of Goodman (1996a), in which each treebank node token Xi is given its own unique grammar symbol. We follow Bansal and Klein (2010) in choosing the latter, both to permit comparison to their results and because SDP is easily phrased as a PCFG reduction. Bansal and Klein (2010) use a carefully paOne guiding intuition in parsing, and data-driven NLP more generally, is that, all else equal, it is advantageous to memorize large fragments of training examples. Taken to the extreme, this intuition suggests shortest derivation parsing (SDP), wherein a test sentence is analyzed in a way which uses as few training fragments as possible (Bod, 2000; Goodman, 2003). SDP certainly has appealing properties: it is simple and parameter f"
P11-2127,E93-1006,0,0.213648,"Missing"
P11-2127,C00-1011,0,0.0817071,"Missing"
P11-2127,P01-1010,0,0.0607119,"Missing"
P11-2127,W98-1115,0,0.0767635,"Missing"
P11-2127,N06-1022,0,0.0487217,"Missing"
P11-2127,P10-2042,0,0.0614055,"Missing"
P11-2127,P05-1039,0,0.0222265,"ns more pruning. These are results without the coarseposterior tie-breaking to illustrate the sole effect of pruning. of Java code, including I/O.7 5.1 Other Treebanks One nice property of the parameter-free, allfragments SDP approach is that we can easily transfer it to any new domain with a treebank, or any new annotation of an existing treebank. Table 3 shows domain adaptation performance by the results for training and testing on the Brown and German datasets.8 On Brown, we perform better than the relatively complex lexicalized Model 1 of Collins (1999). For German, our parser outperforms Dubey (2005) and we are not far behind latentvariable parsers, for which parsing is substantially 7 6 PCFG + SDP SDP 20 # of fragments SDP PCFG PCFG+SDP test (≤ 40) F1 EX 66.9 18.4 84.0 21.6 86.9 31.5 F1 Model dev (≤ 40) F1 EX 66.2 18.0 83.8 20.0 86.4 30.6 These statistics can be further improved with standard parsing micro-optimization. 8 See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. test (≤ 40) F1 EX BROWN Gildea (2001) 84.1 – This Paper (PCFG+SDP) 84.7 34.6 GERMAN Dubey (2005) 76.3 – Petrov and Klein (2007) 80.8 40.8 This Paper (PCFG+SDP) 78.1 39."
P11-2127,W01-0521,0,0.012192,"ank. Table 3 shows domain adaptation performance by the results for training and testing on the Brown and German datasets.8 On Brown, we perform better than the relatively complex lexicalized Model 1 of Collins (1999). For German, our parser outperforms Dubey (2005) and we are not far behind latentvariable parsers, for which parsing is substantially 7 6 PCFG + SDP SDP 20 # of fragments SDP PCFG PCFG+SDP test (≤ 40) F1 EX 66.9 18.4 84.0 21.6 86.9 31.5 F1 Model dev (≤ 40) F1 EX 66.2 18.0 83.8 20.0 86.4 30.6 These statistics can be further improved with standard parsing micro-optimization. 8 See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. test (≤ 40) F1 EX BROWN Gildea (2001) 84.1 – This Paper (PCFG+SDP) 84.7 34.6 GERMAN Dubey (2005) 76.3 – Petrov and Klein (2007) 80.8 40.8 This Paper (PCFG+SDP) 78.1 39.3 Model test (all) F1 EX – 83.1 – 32.6 – 80.1 77.1 – 39.1 38.2 Table 3: Results for training and testing on the Brown and German treebanks. Gildea (2001) uses the lexicalized Collins’ Model 1 (Collins, 1999). Annotation S TAN -A NNOTATION B ERK -A NNOTATION test (≤ 40) F1 EX 88.1 34.3 90.0 38.9 test (all) F1 EX 87.4 32.2 89.5 36.8 Table 4: Resul"
P11-2127,W96-0214,0,0.0701869,"test-derivation parsing (after coarse-pruning), if two derivations have the same cost (i.e., the number of switches), then we break the tie between them by choosing the derivation which has a higher sum of coarse posteriors (i.e., the sum of the coarse PCFG chart-cell posteriors P (X, i, j|s) used to build the derivation).4 The coarse PCFG has an extremely beneficial interaction with the fine all-fragments SDP grammar, wherein the accuracy of the combined grammars is significantly higher than either individually (see 4 This is similar to the maximum recall objective for approximate inference (Goodman, 1996b). The product of posteriors also works equally well. 722 Model B&K2010 pruned B&K2010 unpruned dev (≤ 40) F1 EX 88.4 33.7 87.9 32.4 test (≤ 40) F1 EX 88.5 33.0 88.1 31.9 Table 1: Accuracy (F1) and exact match (EX) for Bansal and Klein (2010). The pruned row shows their original results with coarse-to-fine pruning. The unpruned row shows new results for an unpruned version of their parser; these accuracies are very similar to their pruned counterparts. Table 2). In addition, the speed of parsing and memory-requirements improve by more than an order of magnitude over the exact SDP pass alone."
P11-2127,P96-1024,0,0.0646017,"test-derivation parsing (after coarse-pruning), if two derivations have the same cost (i.e., the number of switches), then we break the tie between them by choosing the derivation which has a higher sum of coarse posteriors (i.e., the sum of the coarse PCFG chart-cell posteriors P (X, i, j|s) used to build the derivation).4 The coarse PCFG has an extremely beneficial interaction with the fine all-fragments SDP grammar, wherein the accuracy of the combined grammars is significantly higher than either individually (see 4 This is similar to the maximum recall objective for approximate inference (Goodman, 1996b). The product of posteriors also works equally well. 722 Model B&K2010 pruned B&K2010 unpruned dev (≤ 40) F1 EX 88.4 33.7 87.9 32.4 test (≤ 40) F1 EX 88.5 33.0 88.1 31.9 Table 1: Accuracy (F1) and exact match (EX) for Bansal and Klein (2010). The pruned row shows their original results with coarse-to-fine pruning. The unpruned row shows new results for an unpruned version of their parser; these accuracies are very similar to their pruned counterparts. Table 2). In addition, the speed of parsing and memory-requirements improve by more than an order of magnitude over the exact SDP pass alone."
P11-2127,J98-4004,0,0.0768119,"Missing"
P11-2127,P03-1054,1,0.107655,"Missing"
P11-2127,N07-1051,1,0.819401,"Missing"
P11-2127,P06-1055,1,0.874824,"Missing"
P11-2127,P09-2012,0,0.168671,"Missing"
P11-2127,C92-2065,0,0.0715887,"Missing"
P11-2127,P00-1008,0,0.64147,"Missing"
P11-2127,J03-4003,0,\N,Missing
P12-1041,P11-1070,1,0.881287,"mentioned otherwise. example, the head of the Palestinian territories is the word territories). Next, we take each headword pair (h1 , h2 ) and compute various Web-count functions on it that can signal whether or not this mention pair is coreferent. As the source of Web information, we use the Google n-grams corpus (Brants and Franz, 2006) which contains English n-grams (n = 1 to 5) and their Web frequency counts, derived from nearly 1 trillion word tokens and 95 billion sentences. Because we have many queries that must be run against this corpus, we apply the trie-based hashing algorithm of Bansal and Klein (2011) to efficiently answer all of them in one pass over it. The features that require word clusters (Section 3.4) use the output of Lin et al. (2010).2 We describe our five types of features in turn. The first four types are most intuitive for mention pairs where both members are non-pronominal, but, aside from the general co-occurrence group, helped for all mention pair types. The fifth feature group applies only to pairs in which the anaphor is a pronoun but the antecedent is a non-pronoun. Related work for each feature category is discussed inline. 3.1 General co-occurrence These features captu"
P12-1041,D08-1031,0,0.31621,"mention systems, their current form was most directly applicable to the mention-pair approach, making Reconcile a particularly well-suited platform for this investigation. The Reconcile system provides baseline features, learning mechanisms, and resolution procedures that already achieve near state-of-the-art results on multiple popular datasets using multiple standard metrics. It includes over 80 core features that exploit 390 various automatically generated annotations such as named entity tags, syntactic parses, and WordNet classes, inspired by Soon et al. (2001), Ng and Cardie (2002), and Bengtson and Roth (2008). The Reconcile system also facilitates standardized empirical evaluation to past work.1 In this paper, we develop a suite of simple semantic Web features based on pairs of mention headwords which stack with the default Reconcile features to surpass past state-of-the-art results. 2.2 Decision Tree Classifier Among the various learning algorithms that Reconcile supports, we chose the decision tree classifier, available in Weka (Hall et al., 2009) as J48, an open source Java implementation of the C4.5 algorithm of Quinlan (1993). The C4.5 algorithm builds decision trees by incrementally maximizi"
P12-1041,P06-1005,0,0.833709,"His election campaign is expected to [...] For resolving coreference in this example, a system would benefit from the world knowledge that Obama is the president. Also, to resolve the pronoun his to the correct antecedent Obama, we can use the knowledge that Obama has an election campaign while Jobs does not. Such ambiguities are difficult to resolve on purely syntactic or configurational grounds. There have been multiple previous systems that incorporate some form of world knowledge in coreference resolution tasks. Most work (Poesio et al., 2004; Markert and Nissim, 2005; Yang et al., 2005; Bergsma and Lin, 2006) addresses special cases and subtasks such as bridging anaphora, In this paper, we look to the Web for broader if shallower sources of semantics. In order to harness the information on the Web without presupposing a deep understanding of all Web text, we instead turn to a diverse collection of Web n-gram counts (Brants and Franz, 2006) which, in aggregate, contain diffuse and indirect, but often robust, cues to reference. For example, we can collect the cooccurrence statistics of an anaphor with various candidate antecedents to judge relative surface affinities (i.e., (Obama, president) versus"
P12-1041,P10-1089,0,0.0107131,"and Hanks, 1989)).3 This normalized value is quantized by taking its log10 and binning. The actual feature that fires is an indicator of which quantized bin the query produced. As a real example from our development set, the cooccurrence count c12 for the headword pair (leader, president) is 11383, while it is only 95 for the headword pair (voter, president); after normalization and log10 , the values are -10.9 and -12.0, respectively. These kinds of general Web co-occurrence statistics have been used previously for other supervised NLP tasks such as spelling correction and syntactic parsing (Bergsma et al., 2010; Bansal and Klein, 2011). In coreference, similar word-association scores were used by Kobdani et al. (2011), but from Wikipedia and for self-training. 3.2 Hearst co-occurrence These features capture templated co-occurrence of the two headwords h1 and h2 in the Web-corpus. Here, we only collect statistics of the headwords cooccurring with a generalized Hearst pattern (Hearst, 1992) in between. Hearst patterns capture various lexical semantic relations between items. For example, seeing X is a Y or X and other Y indicates hypernymy and also tends to cue coreference. The specific patterns we us"
P12-1041,P89-1010,0,0.0434437,"not. Using the n-grams corpus (for n = 1 to 5), we collect co-occurrence Web-counts by allowing a varying number of wildcards between h1 and h2 in the query. The co-occurrence value is:    c12 bin log10 c1 · c2 where c12 = count(“h1 ? h2 ”) + count(“h1 ? ? h2 ”) + count(“h1 ? ? ? h2 ”), c1 = count(“h1 ”), and c2 = count(“h2 ”). We normalize the overall co-occurrence count of the headword pair c12 by the unigram counts of the individual headwords c1 and c2 , so that high-frequency headwords do not unfairly get a high feature value (this is similar to computing scaled mutual information MI (Church and Hanks, 1989)).3 This normalized value is quantized by taking its log10 and binning. The actual feature that fires is an indicator of which quantized bin the query produced. As a real example from our development set, the cooccurrence count c12 for the headword pair (leader, president) is 11383, while it is only 95 for the headword pair (voter, president); after normalization and log10 , the values are -10.9 and -12.0, respectively. These kinds of general Web co-occurrence statistics have been used previously for other supervised NLP tasks such as spelling correction and syntactic parsing (Bergsma et al.,"
P12-1041,H05-1013,0,0.587923,"Missing"
P12-1041,1993.eamt-1.1,0,0.160596,"7.5 73.7 (p &lt; 0.005) 76.5 73.2 77.0 77.0 78.8 80.0 1.3 82.1 63.9 83.2 68.4 85.0 65.5 89.4 64.2 81.1 70.8 (p &lt; 0.1) 73.7 71.8 75.1 73.9 74.7 75.6 0.9 54.4 70.5 53.2 73.1 55.4 74.8 80.6 60.5 85.1 60.4 80.7 65.9 (p &lt; 0.001) 61.4 61.6 63.8 69.1 70.6 72.5 1.9 Table 3: Primary test results on the ACE04, ACE05, and ACE05-ALL datasets. All systems reported here use automatically extracted system mentions. B3 here is the B3 All version of Stoyanov et al. (2009). We also report statistical significance of the improvements from the Web features on the DT baseline, using the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1993). The perceptron baseline in this work (Reconcile settings: 15 iterations, threshold = 0.45, SIG for ACE04 and AP for ACE05, ACE05ALL) has different results from Stoyanov et al. (2009) because their current publicly available code is different from that used in their paper (p.c.). Also, the B3 variant used by Rahman and Ng (2009) is slightly different from other systems (they remove all and only the singleton twinless system mentions, so it is neither B3 All nor B3 None). For completeness, our (untuned) B3 None results (DT + Web) on the ACE05-ALL dataset are P=69.9|R=65.9|F1=67.8. class / sens"
P12-1041,D09-1120,1,0.967,"nt amount of noise. Also, we do not constrain the order of h1 and h2 because these patterns can hold for either direction of coreference.4 As a real example from our development set, the c12 count for the headword pair (leader, president) is 752, while for (voter, president), it is 0. Hypernymic semantic compatibility for coreference is intuitive and has been explored in varying forms by previous work. Poesio et al. (2004) and Markert and Nissim (2005) employ a subset of our Hearst patterns and Web-hits for the subtasks of bridging anaphora, other-anaphora, and definite NP resolution. Others (Haghighi and Klein, 2009; Rahman and Ng, 2011; Daum´e III and Marcu, 2005) use similar relations to extract compatibility statistics from Wikipedia, YAGO, and noun-similarity lists. Yang and Su (2007) use Wikipedia to automatically extract semantic patterns, which are then used as features in a learning setup. Instead of extracting patterns from the training data, we use all the above patterns, which helps us generalize to new datasets for end-to-end coreference resolution (see Section 4.3). 3.3 Entity-based context For each headword h, we first collect context seeds y using the pattern h {is |are |was |were} {a |an"
P12-1041,N10-1061,1,0.570643,"d Dan Klein Computer Science Division University of California, Berkeley {mbansal, klein}@cs.berkeley.edu Abstract other anaphora, definite NP reference, and pronoun resolution, computing semantic compatibility via Web-hits and counts from large corpora. There is also work on end-to-end coreference resolution that uses large noun-similarity lists (Daum´e III and Marcu, 2005) or structured knowledge bases such as Wikipedia (Yang and Su, 2007; Haghighi and Klein, 2009; Kobdani et al., 2011) and YAGO (Rahman and Ng, 2011). However, such structured knowledge bases are of limited scope, and, while Haghighi and Klein (2010) self-acquires knowledge about coreference, it does so only via reference constructions and on a limited scale. To address semantic ambiguities in coreference resolution, we use Web n-gram features that capture a range of world knowledge in a diffuse but robust way. Specifically, we exploit short-distance cues to hypernymy, semantic compatibility, and semantic context, as well as general lexical co-occurrence. When added to a state-of-the-art coreference baseline, our Web features give significant gains on multiple datasets (ACE 2004 and ACE 2005) and metrics (MUC and B3 ), resulting in the be"
P12-1041,C92-2082,0,0.0237996,", the values are -10.9 and -12.0, respectively. These kinds of general Web co-occurrence statistics have been used previously for other supervised NLP tasks such as spelling correction and syntactic parsing (Bergsma et al., 2010; Bansal and Klein, 2011). In coreference, similar word-association scores were used by Kobdani et al. (2011), but from Wikipedia and for self-training. 3.2 Hearst co-occurrence These features capture templated co-occurrence of the two headwords h1 and h2 in the Web-corpus. Here, we only collect statistics of the headwords cooccurring with a generalized Hearst pattern (Hearst, 1992) in between. Hearst patterns capture various lexical semantic relations between items. For example, seeing X is a Y or X and other Y indicates hypernymy and also tends to cue coreference. The specific patterns we use are: • h1 {is |are |was |were} {a |an |the}? h2 • h1 {and |or} {other |the other |another} h2 2 These clusters are derived form the V2 Google n-grams corpus. The V2 corpus itself is not publicly available, but the clusters are available at http://www.clsp.jhu.edu/ ˜sbergsma/PhrasalClusters 391 • h1 other than {a |an |the}? h2 3 We also tried adding count(“h1 h2”) to c12 but this d"
P12-1041,P11-1079,0,0.40519,"hat fires is an indicator of which quantized bin the query produced. As a real example from our development set, the cooccurrence count c12 for the headword pair (leader, president) is 11383, while it is only 95 for the headword pair (voter, president); after normalization and log10 , the values are -10.9 and -12.0, respectively. These kinds of general Web co-occurrence statistics have been used previously for other supervised NLP tasks such as spelling correction and syntactic parsing (Bergsma et al., 2010; Bansal and Klein, 2011). In coreference, similar word-association scores were used by Kobdani et al. (2011), but from Wikipedia and for self-training. 3.2 Hearst co-occurrence These features capture templated co-occurrence of the two headwords h1 and h2 in the Web-corpus. Here, we only collect statistics of the headwords cooccurring with a generalized Hearst pattern (Hearst, 1992) in between. Hearst patterns capture various lexical semantic relations between items. For example, seeing X is a Y or X and other Y indicates hypernymy and also tends to cue coreference. The specific patterns we use are: • h1 {is |are |was |were} {a |an |the}? h2 • h1 {and |or} {other |the other |another} h2 2 These clust"
P12-1041,P08-1068,0,0.0391799,"in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. However, we also assume that higher-ranked matches tend to imply closer meanings. To this end, we fire a feature indicating the value bin(i+j), where i and j are the earliest match positions in the cluster id lists of h1 and h2 . Binning here means that match positions in a close range generally trigger the same feature. Recent previous work has used clustering information to improve the performance of supervised NLP tasks such as NER and dependency parsing (Koo et al., 2008; Lin and Wu, 2009). However, in coreference, the only related work to our knowledge is from Daum´e III and Marcu (2005), who use word class features derived from a Web-scale corpus via a process described in Ravichandran et al. (2005). 3.5 Pronoun context Our last feature category specifically addresses pronoun reference, for cases when the anaphoric mention N P2 (and hence its headword h2 ) is a pronoun, while the candidate antecedent mention N P1 (and hence its headword h1 ) is not. For such a headword pair (h1 , h2 ), the idea is to substitute the nonpronoun h1 into h2 ’s position and see"
P12-1041,P09-1116,0,0.0168332,"tributional K-Means clustering (with K = 1000) on phrases, using the n-gram context as features. The cluster data contains almost 10 million phrases and their soft cluster memberships. Up to twenty cluster ids with the highest centroid similarities are included for each phrase in this dataset (Lin et al., 2010). Our cluster-based features assume that if the headwords of the two mentions have matches in their cluster id lists, then they are more compatible for coreference. We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. However, we also assume that higher-ranked matches tend to imply closer meanings. To this end, we fire a feature indicating the value bin(i+j), where i and j are the earliest match positions in the cluster id lists of h1 and h2 . Binning here means that match positions in a close range generally trigger the same feature. Recent previous work has used clustering information to improve the performance of supervised NLP tasks such as NER and dependency parsing (Koo et al., 2008; Lin and Wu, 2009). However, in coreference, the"
P12-1041,lin-etal-2010-new,0,0.0412494,"e various Web-count functions on it that can signal whether or not this mention pair is coreferent. As the source of Web information, we use the Google n-grams corpus (Brants and Franz, 2006) which contains English n-grams (n = 1 to 5) and their Web frequency counts, derived from nearly 1 trillion word tokens and 95 billion sentences. Because we have many queries that must be run against this corpus, we apply the trie-based hashing algorithm of Bansal and Klein (2011) to efficiently answer all of them in one pass over it. The features that require word clusters (Section 3.4) use the output of Lin et al. (2010).2 We describe our five types of features in turn. The first four types are most intuitive for mention pairs where both members are non-pronominal, but, aside from the general co-occurrence group, helped for all mention pair types. The fifth feature group applies only to pairs in which the anaphor is a pronoun but the antecedent is a non-pronoun. Related work for each feature category is discussed inline. 3.1 General co-occurrence These features capture co-occurrence statistics of the two headwords, i.e., how often h1 and h2 are seen adjacent or nearly adjacent on the Web. This count can be a"
P12-1041,J05-3004,0,0.15927,"ussed the economy, technology, and education. His election campaign is expected to [...] For resolving coreference in this example, a system would benefit from the world knowledge that Obama is the president. Also, to resolve the pronoun his to the correct antecedent Obama, we can use the knowledge that Obama has an election campaign while Jobs does not. Such ambiguities are difficult to resolve on purely syntactic or configurational grounds. There have been multiple previous systems that incorporate some form of world knowledge in coreference resolution tasks. Most work (Poesio et al., 2004; Markert and Nissim, 2005; Yang et al., 2005; Bergsma and Lin, 2006) addresses special cases and subtasks such as bridging anaphora, In this paper, we look to the Web for broader if shallower sources of semantics. In order to harness the information on the Web without presupposing a deep understanding of all Web text, we instead turn to a diverse collection of Web n-gram counts (Brants and Franz, 2006) which, in aggregate, contain diffuse and indirect, but often robust, cues to reference. For example, we can collect the cooccurrence statistics of an anaphor with various candidate antecedents to judge relative surface"
P12-1041,P02-1014,0,0.174008,"ould be adapted to entity-mention systems, their current form was most directly applicable to the mention-pair approach, making Reconcile a particularly well-suited platform for this investigation. The Reconcile system provides baseline features, learning mechanisms, and resolution procedures that already achieve near state-of-the-art results on multiple popular datasets using multiple standard metrics. It includes over 80 core features that exploit 390 various automatically generated annotations such as named entity tags, syntactic parses, and WordNet classes, inspired by Soon et al. (2001), Ng and Cardie (2002), and Bengtson and Roth (2008). The Reconcile system also facilitates standardized empirical evaluation to past work.1 In this paper, we develop a suite of simple semantic Web features based on pairs of mention headwords which stack with the default Reconcile features to surpass past state-of-the-art results. 2.2 Decision Tree Classifier Among the various learning algorithms that Reconcile supports, we chose the decision tree classifier, available in Weka (Hall et al., 2009) as J48, an open source Java implementation of the C4.5 algorithm of Quinlan (1993). The C4.5 algorithm builds decision t"
P12-1041,P10-1142,0,0.0915512,"3.2 63.6 64.3 F1 65.9 69.5 69.8 70.0 70.4 70.7 71.3 P 82.2 89.5 88.7 89.1 88.1 87.9 88.0 B3 R 69.9 69.0 69.8 70.1 70.9 71.2 71.6 F1 75.5 77.9 78.1 78.5 78.6 78.6 79.0 Table 2: Incremental results for the Web features on the ACE04 development set. AvgPerc is the averaged perceptron baseline, DecTree is the decision tree baseline, and the +Feature rows show the effect of adding a particular feature incrementally (not in isolation) to the DecTree baseline. The feature categories correspond to those described in Section 3. and gold cluster, respectively. It is well known (Recasens and Hovy, 2010; Ng, 2010; Kobdani et al., 2011) that MUC is biased towards large clusters (chains) whereas B3 is biased towards singleton clusters. Therefore, for a more balanced evaluation, we show improvements on both metrics simultaneously. 4.3 Results We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al. (2009).9 Table 2 compares the baseline perceptron results to the DT results and then shows the incremental addition of the Web features to the DT baseline (on the AC"
P12-1041,P04-1019,0,0.453558,"s, the president discussed the economy, technology, and education. His election campaign is expected to [...] For resolving coreference in this example, a system would benefit from the world knowledge that Obama is the president. Also, to resolve the pronoun his to the correct antecedent Obama, we can use the knowledge that Obama has an election campaign while Jobs does not. Such ambiguities are difficult to resolve on purely syntactic or configurational grounds. There have been multiple previous systems that incorporate some form of world knowledge in coreference resolution tasks. Most work (Poesio et al., 2004; Markert and Nissim, 2005; Yang et al., 2005; Bergsma and Lin, 2006) addresses special cases and subtasks such as bridging anaphora, In this paper, we look to the Web for broader if shallower sources of semantics. In order to harness the information on the Web without presupposing a deep understanding of all Web text, we instead turn to a diverse collection of Web n-gram counts (Brants and Franz, 2006) which, in aggregate, contain diffuse and indirect, but often robust, cues to reference. For example, we can collect the cooccurrence statistics of an anaphor with various candidate antecedents"
P12-1041,D09-1101,0,0.637989,"r of gold mentions in the test split; chn: the number of coreference chains in the test split. 4 Experiments 4.1 Data We show results on three popular and comparatively larger coreference resolution data sets – the ACE04, ACE05, and ACE05-ALL datasets from the ACE Program (NIST, 2004). In ACE04 and ACE05, we have only the newswire portion (of the original ACE 2004 and 2005 training sets) and use the standard train/test splits reported in Stoyanov et al. (2009) and Haghighi and Klein (2010). In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). Note that most previous work does not report (or need) a standard development set; hence, for tuning our features and its hyper-parameters, we randomly split the original training data into a training and development set with a 70/30 ratio (and then use the full original training set during testing). Details of the corpora are shown in Table 1.7 Details of the Web-scale corpora used for extracting features are discussed in Section 3. 4.2 Evaluation Metrics We evaluated our work on both MUC (Vilain et al., 1995) and B3 (Bagga and Baldwin, 1998). Both scorers are"
P12-1041,P11-1082,0,0.278685,"we do not constrain the order of h1 and h2 because these patterns can hold for either direction of coreference.4 As a real example from our development set, the c12 count for the headword pair (leader, president) is 752, while for (voter, president), it is 0. Hypernymic semantic compatibility for coreference is intuitive and has been explored in varying forms by previous work. Poesio et al. (2004) and Markert and Nissim (2005) employ a subset of our Hearst patterns and Web-hits for the subtasks of bridging anaphora, other-anaphora, and definite NP resolution. Others (Haghighi and Klein, 2009; Rahman and Ng, 2011; Daum´e III and Marcu, 2005) use similar relations to extract compatibility statistics from Wikipedia, YAGO, and noun-similarity lists. Yang and Su (2007) use Wikipedia to automatically extract semantic patterns, which are then used as features in a learning setup. Instead of extracting patterns from the training data, we use all the above patterns, which helps us generalize to new datasets for end-to-end coreference resolution (see Section 4.3). 3.3 Entity-based context For each headword h, we first collect context seeds y using the pattern h {is |are |was |were} {a |an |the}? y taking seeds"
P12-1041,P05-1077,0,0.017262,"s end, we fire a feature indicating the value bin(i+j), where i and j are the earliest match positions in the cluster id lists of h1 and h2 . Binning here means that match positions in a close range generally trigger the same feature. Recent previous work has used clustering information to improve the performance of supervised NLP tasks such as NER and dependency parsing (Koo et al., 2008; Lin and Wu, 2009). However, in coreference, the only related work to our knowledge is from Daum´e III and Marcu (2005), who use word class features derived from a Web-scale corpus via a process described in Ravichandran et al. (2005). 3.5 Pronoun context Our last feature category specifically addresses pronoun reference, for cases when the anaphoric mention N P2 (and hence its headword h2 ) is a pronoun, while the candidate antecedent mention N P1 (and hence its headword h1 ) is not. For such a headword pair (h1 , h2 ), the idea is to substitute the nonpronoun h1 into h2 ’s position and see whether the result is attested on the Web. If the anaphoric pronominal mention is h2 and its sentential context is l’ l h2 r r’, then the substituted phrase will be l’ l h1 r r’.5 High Web counts of substituted phrases tend to indicate"
P12-1041,P10-1144,0,0.199359,"C R 63.1 61.0 62.1 62.3 63.2 63.6 64.3 F1 65.9 69.5 69.8 70.0 70.4 70.7 71.3 P 82.2 89.5 88.7 89.1 88.1 87.9 88.0 B3 R 69.9 69.0 69.8 70.1 70.9 71.2 71.6 F1 75.5 77.9 78.1 78.5 78.6 78.6 79.0 Table 2: Incremental results for the Web features on the ACE04 development set. AvgPerc is the averaged perceptron baseline, DecTree is the decision tree baseline, and the +Feature rows show the effect of adding a particular feature incrementally (not in isolation) to the DecTree baseline. The feature categories correspond to those described in Section 3. and gold cluster, respectively. It is well known (Recasens and Hovy, 2010; Ng, 2010; Kobdani et al., 2011) that MUC is biased towards large clusters (chains) whereas B3 is biased towards singleton clusters. Therefore, for a more balanced evaluation, we show improvements on both metrics simultaneously. 4.3 Results We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al. (2009).9 Table 2 compares the baseline perceptron results to the DT results and then shows the incremental addition of the Web features to the DT baseline"
P12-1041,J01-4004,0,0.573195,"significant improvements over the Reconcile DT baseline and over the state-of-the-art results of Haghighi and Klein (2010). 2 Baseline System Before describing our semantic Web features, we first describe our baseline. The core inference and features come from the Reconcile package (Stoyanov et al., 2009; Stoyanov et al., 2010), with modifications described below. Our baseline differs most substantially from Stoyanov et al. (2009) in using a decision tree classifier rather than an averaged linear perceptron. 2.1 Reconcile Reconcile is one of the best implementations of the mention-pair model (Soon et al., 2001) of coreference resolution. The mention-pair model relies on a pairwise function to determine whether or not two mentions are coreferent. Pairwise predictions are then consolidated by transitive closure (or some other clustering method) to form the final set of coreference clusters (chains). While our Web features could be adapted to entity-mention systems, their current form was most directly applicable to the mention-pair approach, making Reconcile a particularly well-suited platform for this investigation. The Reconcile system provides baseline features, learning mechanisms, and resolution"
P12-1041,P09-1074,0,0.134032,"(2) lexical relations (via Hearst-style hypernymy patterns), (3) similarity of entity-based context (e.g., common values of y for 389 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 389–398, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics which h is a y is attested), (4) matches of distributional soft cluster ids, and (5) attested substitutions of candidate antecedents in the context of a pronominal anaphor. We first describe a strong baseline consisting of the mention-pair model of the Reconcile system (Stoyanov et al., 2009; Stoyanov et al., 2010) using a decision tree (DT) as its pairwise classifier. To this baseline system, we add our suite of features in turn, each class of features providing substantial gains. Altogether, our final system produces the best numbers reported to date on end-to-end coreference resolution (with automatically detected system mentions) on multiple data sets (ACE 2004 and ACE 2005) and metrics (MUC and B3 ), achieving significant improvements over the Reconcile DT baseline and over the state-of-the-art results of Haghighi and Klein (2010). 2 Baseline System Before describing our sem"
P12-1041,M95-1005,0,0.612371,"and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). Note that most previous work does not report (or need) a standard development set; hence, for tuning our features and its hyper-parameters, we randomly split the original training data into a training and development set with a 70/30 ratio (and then use the full original training set during testing). Details of the corpora are shown in Table 1.7 Details of the Web-scale corpora used for extracting features are discussed in Section 3. 4.2 Evaluation Metrics We evaluated our work on both MUC (Vilain et al., 1995) and B3 (Bagga and Baldwin, 1998). Both scorers are available in the Reconcile infrastructure.8 MUC measures how many predicted clusters need to be merged to cover the true gold clusters. B3 computes precision and recall for each mention by computing the intersection of its predicted and gold cluster and dividing by the size of the predicted 7 Note that the development set is used only for ACE04, because for ACE05, and ACE05-ALL, we directly test using the features tuned on ACE04. 8 Note that B3 has two versions which handle twinless (spurious) mentions in different ways (see Stoyanov et al. ("
P12-1041,P07-1067,0,0.117233,"the c12 count for the headword pair (leader, president) is 752, while for (voter, president), it is 0. Hypernymic semantic compatibility for coreference is intuitive and has been explored in varying forms by previous work. Poesio et al. (2004) and Markert and Nissim (2005) employ a subset of our Hearst patterns and Web-hits for the subtasks of bridging anaphora, other-anaphora, and definite NP resolution. Others (Haghighi and Klein, 2009; Rahman and Ng, 2011; Daum´e III and Marcu, 2005) use similar relations to extract compatibility statistics from Wikipedia, YAGO, and noun-similarity lists. Yang and Su (2007) use Wikipedia to automatically extract semantic patterns, which are then used as features in a learning setup. Instead of extracting patterns from the training data, we use all the above patterns, which helps us generalize to new datasets for end-to-end coreference resolution (see Section 4.3). 3.3 Entity-based context For each headword h, we first collect context seeds y using the pattern h {is |are |was |were} {a |an |the}? y taking seeds y in order of decreasing Web count. The corresponding ordered seed list Y = {y} gives us useful information about the headword’s entity type. For example,"
P12-1041,P05-1021,0,0.70583,"ogy, and education. His election campaign is expected to [...] For resolving coreference in this example, a system would benefit from the world knowledge that Obama is the president. Also, to resolve the pronoun his to the correct antecedent Obama, we can use the knowledge that Obama has an election campaign while Jobs does not. Such ambiguities are difficult to resolve on purely syntactic or configurational grounds. There have been multiple previous systems that incorporate some form of world knowledge in coreference resolution tasks. Most work (Poesio et al., 2004; Markert and Nissim, 2005; Yang et al., 2005; Bergsma and Lin, 2006) addresses special cases and subtasks such as bridging anaphora, In this paper, we look to the Web for broader if shallower sources of semantics. In order to harness the information on the Web without presupposing a deep understanding of all Web text, we instead turn to a diverse collection of Web n-gram counts (Brants and Franz, 2006) which, in aggregate, contain diffuse and indirect, but often robust, cues to reference. For example, we can collect the cooccurrence statistics of an anaphor with various candidate antecedents to judge relative surface affinities (i.e., ("
P12-1041,J90-1003,0,\N,Missing
P12-1041,D08-1067,0,\N,Missing
P12-1101,D07-1090,0,0.0913137,"al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper, we describe a generative, syntactic language model that conditions on local context treelets1 in a parse tree, backing off to smaller treelets as necessary. Our model can be trained simply by collecting counts and using the same smoothing techniques normally applied to n-gram models (Kneser and Ney, 1995), enabling us to apply techniques developed for scaling n-gram models out of the box (Brants et al., 2007; Pauls and Klein, 2011). The simplicity of our training procedure allows us to train a model on a billion tokens of data in a matter of hours on a single machine, which compares favorably to the more involved training algorithm of Tan et al. (2011), who use a two-pass EM training algorithm that takes several days on several hundred CPUs using similar amounts of data. The simplicity of our approach also contrasts with recent work on language modeling with tree substitution grammars (Post and Gildea, 2009), where larger treelet contexts are incorporated by using sophisticated priors to learn a"
P12-1101,P05-1022,0,0.0271534,". We parsed the 50K positive training examples of Post (2011) with the Berkeley Parser and used the resulting treebank to train a treelet language model. We set an SLR threshold for each model on the 6K positive and negative development sentences. Results are shown in Table 4. In addition to our generative baselines, we show results for the discriminative models reported in Cherry and Quirk (2008) and Post (2011). The former train a latent PCFG support vector machine for binary classification (L SVM). The latter report results for two binary classifiers: R ERANK uses the reranking features of Charniak and Johnson (2005), and T SG uses 8 Och et al. (2004) also report using a parser probability normalized by the unigram probability (but not length), and did not find it effective. We assume this is either because the lengthnormalization is important, or because their choice of syntactic language model was poor. Generative BLLIP P CFG 81.5 T REELET-T RANS 87.7 T REELET-RULE 89.8 T REELET 88.9 P CFG - LA 87.1* H EAD L EX 87.6 5- GRAM 67.9 Discriminative BLLIP L SVM 81.42** T SG 89.9 R ERANK 93.0 Model 1B 81.8 90.1 94.1 93.3 – 92.0 87.5 P CFG T REELET-RULE T REELET 5- GRAM H EAD L EX P CFG - LA Foster et al. (2008"
P12-1101,A00-2018,0,0.0815094,"of treelet tuples in the same way that we require counts of word tuples for estimating n-gram language models. There is one additional hurdle in the estimation of our model: while there exist corpora with humanannotated constituency parses like the Penn Treebank (Marcus et al., 1993), these corpora are quite small – on the order of millions of tokens – and we cannot gather nearly as many counts as we can for ngrams, for which billions or even trillions (Brants et al., 2007) of tokens are available on the Web. However, we can use one of several high-quality constituency parsers (Collins, 1997; Charniak, 2000; Petrov et al., 2006) to automatically generate parses. These parses may contain errors, but not all parsing errors are problematic for our model, since we only care about the sentences generated by our model and not the parses themselves. We show in our experiments that the addition of data with automatic parses does improve the performance of our language models across a range of tasks. 3 λ d Y i=1 i−1 p(Ci |Ci−3 , P ) + (1 − λ) d Y i−1 p(Ci |Ci−3 ) i=1 where λ = 0.9 is an interpolation constant. For terminal (i.e lexical) productions, we first remove lexical context, backing off from 961 E"
P12-1101,P01-1017,0,0.0549324,"speech recognition and machine translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), efficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper, we describe a generative, syntactic language model that conditions on local context treelets1 in a parse tree, backing off to smaller treelets as necessary. Our model can be trained simply by collecting counts and using the same smoothing techniques normally applied to n-gram models (Kneser and Ney, 1995), enabling us to apply techniques developed for scaling n-gram models out of the box (Brants et al., 2007; Pauls"
P12-1101,P97-1064,0,0.0461998,"are a central component of all speech recognition and machine translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), efficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper, we describe a generative, syntactic language model that conditions on local context treelets1 in a parse tree, backing off to smaller treelets as necessary. Our model can be trained simply by collecting counts and using the same smoothing techniques normally applied to n-gram models (Kneser and Ney, 1995), enabling us to apply techniques developed for scaling n-gram models out of th"
P12-1101,2008.amta-papers.4,0,0.0642977,"-best approximation for the sum over trees, all perplexities (except for P CFG - LA and 5- GRAM) are pessimistic bounds. 5.3 Classification of Pseudo-Negative Sentences We make use of three kinds of automatically generated pseudo-negative sentences previously proposed in the literature: Okanohara and Tsujii (2007) proposed generating pseudo-negative examples from a trigram language model; Foster et al. (2008) create “noisy” sentences by automatically inserting a single error into grammatical sentences with a script that randomly deletes, inserts, or misspells a word; and Och et al. (2004) and Cherry and Quirk (2008) both use the 1-best output of a machine translation system. Examples of these three types of pseudonegative data are shown in Table 3. We evaluate our model’s ability to distinguish positive from pseudonegative data, and compare against generative baselines and state-of-the-art discriminative methods. 7 We use signatures generated by the Berkeley Parser. These signatures capture surface features such as capitalization, presents of digits, and common suffixes. For example, the word vexing would be replaced with the signature UNK-ing. 964 We would like to use our model to make grammaticality ju"
P12-1101,P05-1033,0,0.202486,"r than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment. 1 Introduction N -gram language models are a central component of all speech recognition and machine translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), efficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper, we describe a generative, syntactic language model that co"
P12-1101,P97-1003,0,0.0949932,"pirical counts of treelet tuples in the same way that we require counts of word tuples for estimating n-gram language models. There is one additional hurdle in the estimation of our model: while there exist corpora with humanannotated constituency parses like the Penn Treebank (Marcus et al., 1993), these corpora are quite small – on the order of millions of tokens – and we cannot gather nearly as many counts as we can for ngrams, for which billions or even trillions (Brants et al., 2007) of tokens are available on the Web. However, we can use one of several high-quality constituency parsers (Collins, 1997; Charniak, 2000; Petrov et al., 2006) to automatically generate parses. These parses may contain errors, but not all parsing errors are problematic for our model, since we only care about the sentences generated by our model and not the parses themselves. We show in our experiments that the addition of data with automatic parses does improve the performance of our language models across a range of tasks. 3 λ d Y i=1 i−1 p(Ci |Ci−3 , P ) + (1 − λ) d Y i−1 p(Ci |Ci−3 ) i=1 where λ = 0.9 is an interpolation constant. For terminal (i.e lexical) productions, we first remove lexical context, backin"
P12-1101,P08-2056,0,0.0478806,"Missing"
P12-1101,P06-1121,0,0.0472874,"Missing"
P12-1101,W11-2123,0,0.0209079,"ammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment. 1 Introduction N -gram language models are a central component of all speech recognition and machine translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), efficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper,"
P12-1101,P07-2045,0,0.00738303,"sify semantically coherent but ungrammatical sentences. For pairwise comparisons, where semantic coherence is effectively held constant, such sentences are not problematic. 5.3.3 Machine Translation Classification We follow Och et al. (2004) and Cherry and Quirk (2008) in evaluating our language models on their ability to distinguish the 1-best output of a machine translation system from a reference translation in a pairwise fashion. Unfortunately, we do not have access to the data used in those papers, so a direct comparison is not possible. Instead, we collected the English output of Moses (Hoang et al., 2007), using both French and German as source language, trained on the Europarl corpus used by WMT 2009.10 We also collected the output of Joshua (Li et al., 2009) trained on 500K sentences of GALE Chinese-English parallel newswire. We trained both our T REELET model and a 5- GRAM model on the union of our 1B corpus and the English sides of our parallel corpora. In Table 6, we show the pairwise comparison accuracy (using SLR) on these three corpora. We see that our system prefers the reference much more often than the 5- GRAM language model.11 However, we also note that the easiness of the task is"
P12-1101,J98-4004,0,0.147324,"tence using a black box parser, and capture several important dependencies. First, it robust and easy to implement, requiring only the col0 summing over our model’s probabilities of the 1000captures both P and its parent P , which predicts lection of counts from data. 5.2 Perplexity 4 Note that the bottleneck in this case best Weparses. would like to apply the same smoothing tech- the distribution over child symbols far better than Perplexity is the standard intrinsic evaluation metric is the parser, so our model niques to distributions over can ruleessentially yields in score a con-a just P (Johnson, 1998). Second, it captures posifor language models. It measures the inverse of the sentence attree, the speed of a parser. tional effects. For example, subject and object noun stituency conditioned on contexts consisting per-word probability a model assigns to some heldphrases (NPs) have different distributions (Klein and of previously generated treelets (rules, nodes, etc.). out set of grammatical English (so lower is better). 5 Experiments Manning, 2003), and the position of an NP relative Formally, let T be a constituency tree consisting of For training data, we constructed a large treebank by t"
P12-1101,P03-1054,1,0.168847,"e to empty subjects and/or objects. VP-VBˆS VB reset . NP-NNS PP-for JJ NNS IN-for NNT opening arguments for today . Figure 2: A sample parse from the Penn Treebank after the tree transformations described in Section 3. Note that we have not shown head tag annotations on preterminals because in that case, the head tag is the preterminal itself. number of transformations of Treebank constituency parses that allow us to capture such dependencies. We list the annotations and deletions in the order in which they are performed. A sample transformed tree is shown in Figure 2. Temporal NPs Following Klein and Manning (2003), we attempt to annotate temporal noun phrases. Although the Penn Treebank annotates temporal NPs, most off-theshelf parsers do not retain these tags, and we do not assume their presence. Instead, we mark any noun that is the head of a NP-TMP constituent at least once in the Treebank as a temporal noun, so for example today would be tagged as NNT and months would be tagged as NNTS. Head Annotations We annotate every non-terminal or preterminal with its head word if the head is a closedclass word3 and with its head tag otherwise. Klein and Manning (2003) used head tag annotation extensively, th"
P12-1101,koen-2004-pharaoh,0,0.0189039,"well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment. 1 Introduction N -gram language models are a central component of all speech recognition and machine translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), efficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper, we describe a generative, syntactic languag"
P12-1101,W09-0424,0,0.0259517,"roblematic. 5.3.3 Machine Translation Classification We follow Och et al. (2004) and Cherry and Quirk (2008) in evaluating our language models on their ability to distinguish the 1-best output of a machine translation system from a reference translation in a pairwise fashion. Unfortunately, we do not have access to the data used in those papers, so a direct comparison is not possible. Instead, we collected the English output of Moses (Hoang et al., 2007), using both French and German as source language, trained on the Europarl corpus used by WMT 2009.10 We also collected the output of Joshua (Li et al., 2009) trained on 500K sentences of GALE Chinese-English parallel newswire. We trained both our T REELET model and a 5- GRAM model on the union of our 1B corpus and the English sides of our parallel corpora. In Table 6, we show the pairwise comparison accuracy (using SLR) on these three corpora. We see that our system prefers the reference much more often than the 5- GRAM language model.11 However, we also note that the easiness of the task is correlated with the quality of translations (as measured in BLEU score). This is not surprising – high-quality translations are often grammatical and even a p"
P12-1101,J93-2004,0,0.0397539,"probabilities in our model can be done very simply using the same techniques (in fact, the same code) used to estimate n-gram language models. Our model requires estimates of four distributions: p(C1d |P, P 0 , r0 ), p(w|P, R, r0 , w−1 , w−2 ), i−1 i−1 p(Ci |Ci−n+1 , P ), and p(Ci |Ci−n+1 ). In each case, we require empirical counts of treelet tuples in the same way that we require counts of word tuples for estimating n-gram language models. There is one additional hurdle in the estimation of our model: while there exist corpora with humanannotated constituency parses like the Penn Treebank (Marcus et al., 1993), these corpora are quite small – on the order of millions of tokens – and we cannot gather nearly as many counts as we can for ngrams, for which billions or even trillions (Brants et al., 2007) of tokens are available on the Web. However, we can use one of several high-quality constituency parsers (Collins, 1997; Charniak, 2000; Petrov et al., 2006) to automatically generate parses. These parses may contain errors, but not all parsing errors are problematic for our model, since we only care about the sentences generated by our model and not the parses themselves. We show in our experiments th"
P12-1101,P07-1010,0,0.0140649,"ted, or pseudonegative,5 English. We report machine translation reranking results in Section 5.4. 5.1 Generating Samples Because our model is generative, we can qualitatively assess it by generating samples and verifying that they are more syntactically coherent than other approaches. In Table 1, we show the first four samples of length between 15 and 20 generated from our model and a 5-gram model trained on the Penn Treebank. 4 We found that using the 1-best worked just as well as the 1000-best on our grammaticality tasks, but significantly overestimated our model’s perplexities. 5 We follow Okanohara and Tsujii (2007) in using the term pseudo-negative to highlight the fact that automatically generated negative examples might not actually be ungrammatical. 963 5.2 Perplexity Perplexity is the standard intrinsic evaluation metric for language models. It measures the inverse of the per-word probability a model assigns to some heldout set of grammatical English (so lower is better). For training data, we constructed a large treebank by concatenating the WSJ and Brown portions of the Penn Treebank, the 50K BLLIP training sentences from Post (2011), and the AFP and APW portions of English Gigaword version 3 (Gra"
P12-1101,P11-1027,1,0.865409,"exity and a range of grammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment. 1 Introduction N -gram language models are a central component of all speech recognition and machine translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), efficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011"
P12-1101,P06-1055,1,0.545143,"s in the same way that we require counts of word tuples for estimating n-gram language models. There is one additional hurdle in the estimation of our model: while there exist corpora with humanannotated constituency parses like the Penn Treebank (Marcus et al., 1993), these corpora are quite small – on the order of millions of tokens – and we cannot gather nearly as many counts as we can for ngrams, for which billions or even trillions (Brants et al., 2007) of tokens are available on the Web. However, we can use one of several high-quality constituency parsers (Collins, 1997; Charniak, 2000; Petrov et al., 2006) to automatically generate parses. These parses may contain errors, but not all parsing errors are problematic for our model, since we only care about the sentences generated by our model and not the parses themselves. We show in our experiments that the addition of data with automatic parses does improve the performance of our language models across a range of tasks. 3 λ d Y i=1 i−1 p(Ci |Ci−3 , P ) + (1 − λ) d Y i−1 p(Ci |Ci−3 ) i=1 where λ = 0.9 is an interpolation constant. For terminal (i.e lexical) productions, we first remove lexical context, backing off from 961 Estimation Tree Transfo"
P12-1101,P11-2038,0,0.181016,"restimated our model’s perplexities. 5 We follow Okanohara and Tsujii (2007) in using the term pseudo-negative to highlight the fact that automatically generated negative examples might not actually be ungrammatical. 963 5.2 Perplexity Perplexity is the standard intrinsic evaluation metric for language models. It measures the inverse of the per-word probability a model assigns to some heldout set of grammatical English (so lower is better). For training data, we constructed a large treebank by concatenating the WSJ and Brown portions of the Penn Treebank, the 50K BLLIP training sentences from Post (2011), and the AFP and APW portions of English Gigaword version 3 (Graff, 2003), totaling about 1.3 billion tokens. We used the humanannotated parses for the sentences in the Penn Treebank, but parsed the Gigaword and BLLIP sentences with the Berkeley Parser. Hereafter, we refer to this training data as our 1B corpus. We used Section 0 of the WSJ as our test corpus. Results are shown in Table 2. In addition to our T REELET model, we also show results for the following baselines: 5- GRAM A 5-gram interpolated Kneser-Ney model. P CFG - LA The Berkeley Parser in language model mode. H EAD L EX A head-"
P12-1101,P05-1034,0,0.00933531,"elet contexts are incorporated by using sophisticated priors to learn a segmentation of parse trees. Such an approach implicitly assumes that a “correct” segmentation exists, but it is not clear that this is true in practice. Instead, we build upon the success of n-gram language models, which do not assume a segmentation and instead score all overlapping contexts. We evaluate our model in terms of perplexity, and show that we achieve the same performance as a state-of-the-art n-gram model. We also evaluate our model on several grammaticality tasks proposed in 1 We borrow the term treelet from Quirk et al. (2005), who use it to refer to an arbitrary connected subgraph of a tree. 959 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 959–968, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics (a) The index fell 109.85 Monday . (b) (c) ROOT ROOT S-VBDˆROOT NP-NN S-VBDˆROOT VP-VBDˆS . DT-the NN VBD CD-DC NNTP The index fell 109.85 Monday . NP-NN VP-VBDˆS . DT-the NN VBD CD-DC NNTP The index fell 109.85 Monday . 5- GRAM Figure 1: The Conditioning contexts andfeasible back-off, from strategies for Markov models. The bolded i"
P12-1101,P11-1021,0,0.0165783,"and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper, we describe a generative, syntactic language model that conditions on local context treelets1 in a parse tree, backing off to smaller treelets as necessary. Our model can be trained simply by collecting counts and using the same smoothing techniques normally applied to n-gram models (Kneser and Ney, 1995), enabling us to apply techniques developed for scaling n-gram models out of the box (Brants et al., 2007; Pauls and Klein, 2011). The simplicity of our training procedure allows us to train a model on a billion tokens of data in a matter of hours on a single machine, which co"
P12-1101,P11-1086,0,0.0128906,"y tasks, but significantly overesA distribution over trees also induces a distribution over P ` ` timated ourwmodel’s perplexities. sentences given by p(w ) = p(T ), where ` 1 1 T :s(T )=w 1 960 pseudo-negative to highlight the fact that automatically generated examples might s(T )negative is the terminal yield of Tnot . actually be ungrammatical. to use back-off-based smoothing for syntactic language modeling – such techniques have been applied to models that condition on head-word contexts (Charniak, 2001; Roark, 2004; Zhang, 2009). Parent rule context has also been employed in translation (Vaswani et al., 2011). However, to our knowledge, we are the first to apply these techniques for language modeling on large amounts of data. 2.1 Lexical context Although it is tempting to think that we can replace the left-to-right generation of n-gram models with the purely top-down generation of typical PCFGs, in practice, words are often highly predictive of the words that follow them – indeed, n-gram models would be terrible language models if this were not the case. To capture linear effects, we extend the context for terminal (lexical) productions to include the previous two words w−2 and w−1 in the sentence"
P12-1101,P02-1025,0,0.0273193,"component of all speech recognition and machine translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), efficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper, we describe a generative, syntactic language model that conditions on local context treelets1 in a parse tree, backing off to smaller treelets as necessary. Our model can be trained simply by collecting counts and using the same smoothing techniques normally applied to n-gram models (Kneser and Ney, 1995), enabling us to apply techniques developed for scaling n-gram models out of the box (Brants et"
P12-1101,J03-4003,0,\N,Missing
P12-1101,J01-2004,0,\N,Missing
P12-1101,N04-1021,0,\N,Missing
P12-2021,H91-1060,0,0.0702437,"Missing"
P12-2021,P11-1048,0,0.0449345,"Missing"
P12-2021,J08-1003,0,0.0563225,"Missing"
P12-2021,P05-1022,0,0.10656,"Missing"
P12-2021,P00-1058,0,0.0132874,"se errors are inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics S NP JJ VP NNS VBD S NP PRP$ NP NN NN DT Italian magistrates labeled his death"
P12-2021,J07-4004,1,0.831639,"Missing"
P12-2021,P09-2014,1,0.921488,"re not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics S NP JJ VP NNS VBD S NP PRP$ NP NN NN DT Italian magistrates labeled his death a suicide N /N N ((S [dcl ]NP )/NP )/NP NP [nb]/N N NP [nb]/N N &gt; N NP &gt; NP &gt;"
P12-2021,P97-1003,0,0.0734998,"Missing"
P12-2021,P10-1035,0,0.0303575,"Missing"
P12-2021,P03-1054,1,0.0215858,"Missing"
P12-2021,J93-2004,0,0.0443419,"Missing"
P12-2021,C08-1069,0,0.386976,"ch as QPs, NXs, and NACs. Many of these errors are inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics S NP JJ VP NNS VBD S NP PRP$ NP NN NN DT Italian magistr"
P12-2021,N07-1051,1,0.802091,"Missing"
P12-2021,N01-1023,0,0.0298084,"inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics S NP JJ VP NNS VBD S NP PRP$ NP NN NN DT Italian magistrates labeled his death a suicide N /N"
P12-2021,H01-1014,0,0.0421712,"he most common errors our approach makes involve nodes for clauses and rare spans such as QPs, NXs, and NACs. Many of these errors are inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Associa"
P13-1012,P08-2012,0,0.139183,"our model: mentions manage their partial membership in various coreference chains, so that information about entity-level properties is decentralized and propagated across individual mentions, and we never need to explicitly instantiate entities. Exact inference in this factor graph is intractable, but efficient approximate inference can be carried out with belief propagation. Our model is the first discriminatively-trained model that both makes joint decisions over an entire document and models specific entity-level properties, rather than simply enforcing transitivity of pairwise decisions (Finkel and Manning, 2008; Song et al., 2012). We evaluate our system on the dataset from the CoNLL 2011 shared task using three different types of properties: synthetic oracle properties, entity phi features (number, gender, animacy, and NER type), and properties derived from unsupervised clusters targeting semantic type information. In all cases, our transitive model of entity properties equals or outperforms our pairwise system and our reimplementation of a previous entity-level system (Rahman and Ng, 2009). Our final system is competitive with the winner of the CoNLL 2011 shared task (Lee et al., 2011). Efficientl"
P13-1012,P12-1041,1,0.844664,"ARG1:approved ... ... ARG1:cause ARG2:following ARG1:reported ARG1:filed prices shares index rates ... ... ... ARG1:rose ARG1:fell ARG1:cut ARG1:closed ... ... Figure 4: Examples of clusters produced by the NAIVE BAYES model on SRL-tagged data with pronouns discarded. types of fine-grained semantic class information (Hendrickx and Daelemans, 2007; Ng, 2007; Rahman and Ng, 2010). Other approaches incorporate information from other sources (Ponzetto and Strube, 2006) or compute heuristic scores for realvalued features based on a large corpus or the web (Dagan and Itai, 1990; Yang et al., 2005; Bansal and Klein, 2012). We use four different clusterings in our experiments, each with twenty clusters: dependency-parse-derived NAIVE BAYES clusters, semantic-role-derived C ONDITIONAL clusters, SRL-derived NAIVE BAYES clusters generating a N OV ERB token when r cannot be determined, and SRL-derived NAIVE BAYES clusters with all pronoun tuples discarded. Examples of the latter clusters are shown in Figure 4. Each clustering is learned for 30 iterations of EM over English Gigaword (Graff et al., 2007), parsed with the Berkeley Parser (Petrov et al., 2006) and with SRL determined by Senna (Collobert et al., 2011)."
P13-1012,N10-1112,0,0.122795,"chi et al., 2011); we found this to be faster and give higher performance than L-BFGS using L2 regularization (Liu and Nocedal, 1989). Note that because of the marginalization over A(C i ), even the objective for the BASIC model is not convex. 5 l(a, C) = Inference Inference in the BASIC model is straightforward. Given a set of weights w, we can predict a∈A(C i ) where (xi , C i ) is the ith labeled training example. This optimizes for the 0-1 loss; however, we are much more interested in optimizing with respect to a coreference-specific loss function. To this end, we will use softmax-margin (Gimpel and Smith, 2010), which augments the probability of each example with a term proportional to its loss, pushing the model to assign less mass to highly incorrect examples. We modify Equation 1 to use a new probability distribution P 0 instead of P , where P 0 (a|xi ) ∝ P (a|xi ) exp (l(a, C)) and l(a, C) is a loss function. In order to perform inference efficiently, l(a, C) must decompose linearly across mentions: l(a, C) = Pn i=1 l(ai , C). Commonly-used coreference metrics such as MUC (Vilain et al., 1995) and B 3 (Bagga and Baldwin, 1998) do not have this property, so we instead make use of a parameterized"
P13-1012,D08-1031,0,0.348235,"s feature function can include pairwise features based on mention i and the chosen antecedent ai , since information about each mention is contained in x. Because the model factors completely over the individual ai , these feature functions fA can be expressed as unary factors Ai (see Figure 1), with Ai (j) ∝ exp wT fA (i, j, x) . Given a setting of w, we can determine a ˆ = arg maxa P (a|x) and then deterministically compute C(a), the final set of coreference chains. While the features of this model factor over coreference links, this approach differs from classical pairwise systems such as Bengtson and Roth (2008) or Stoyanov et al. (2010). Because potential antecedents compete with each other and with the non-anaphoric hypothesis, the choice of ai actually represents a joint decision about i−1 pairwise links, as opposed to systems that use a pairwise binary classifier and a separate agglomeration step, which consider one link at a time during learning. This approach is similar to the mentionranking model of Rahman and Ng (2009). Models We will first present our BASIC model (Section 3.1) and describe the features it incorporates (Section 3.2), then explain how to extend it to use transitive features (S"
P13-1012,P06-1005,0,0.0589705,"rameter settings give fairly weak oracle information: a document may have hundreds of clusters, so even in the absence of noise these oracle properties do not have high dis7.3 Phi Features As we have seen, our T RANSITIVE model can exploit high-quality entity-level features. How does it perform using real features that have been proposed for entity-level coreference? Here, we use hard phi feature determinations extracted from the system of Lee et al. (2011). Named-entity type and animacy are both computed based on the output of a named-entity tagger, while number and gender use the dataset of Bergsma and Lin (2006). Once this information is determined, the PAIR P ROPERTY and L EFTT O R IGHT systems can compute features over it directly. In the T RANSITIVE model, each of the Ri factors places 34 of its mass on the determined label and distributes the remainder uniformly among the possible options. Table 2 shows results when adding entity-level phi features on top of our BASIC pairwise system (which already contains pairwise features) and on top of an ablated BASIC system without pairwise 6 Using gold entities for training as in Rahman and Ng (2009) resulted in a lower-performing system. 7 We even do this"
P13-1012,N10-1061,1,0.85998,"nd-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We show that our method can use entity-level information to outperform a basic pairwise system. 1 Introduction The inclusion of entity-level features has been a driving force behind the development of many coreference resolution systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Lee et al., 2011). There is no polynomial-time dynamic program for inference in a model with arbitrary entity-level features, so systems that use such features typically rely on making decisions in a pipelined manner and sticking with them, operating greedily in a left-to-right fashion (Rahman and Ng, 2009) or in a multi-pass, sieve-like manner (Raghunathan et al., 2010). However, such systems may be locked into bad coreference decisions and are difficult to directly optimize for standard evaluation metrics. In this work, we present a new structured model of entity-level information designed"
P13-1012,N12-1004,1,0.837515,"t takes the standard form of the gradient of a log-linear model, a difference of expected feature counts under the gold annotation and under no annotation. This requires computing marginals P 0 (ai |x) for each mention i, but because the model already factors this way, this step is easy. The T RANSITIVE model is more complex. Exact inference is intractable due to the E factors that couple all of the ai by way of the pi nodes. However, we can compute approximate marginals for the ai , pi , and ri using belief propagation. BP has been effectively used on other NLP tasks (Smith and Eisner, 2008; Burkett and Klein, 2012), and is effective in cases such as this where the model is largely driven by non-loopy factors (here, the Ai ). From marginals over each node, we can compute the necessary gradient and decode as before: [c1 I(K1 (ai , C)) + c2 I(K2 (ai , C)) a ˆ = arg max Pˆ (a|x) i=1 a + c3 I(K3 (ai , C))] 3 One could use ILP-based decoding in the style of Finkel and Manning (2008) and Song et al. (2012) to attempt to explicitly find the optimal C with choice of a marginalized out, but we did not explore this option. where c1 , c2 , and c3 are real-valued weights, K1 denotes the event that ai is falsely anap"
P13-1012,N06-2015,0,0.07396,"e, we use our BA SIC model to prune antecedent choices for each ai in order to reduce the size of the factor graph that we must instantiate. Specifically, we prune links between pairs of mentions that are of mention distance more than 100, as well as values for ai that fall below a particular odds ratio threshold with respect to the best setting of that ai in the BASIC model; that is, those for which   PBASIC (ai |x) log maxj PBASIC (ai = j|x) 7 We use the datasets, experimental setup, and scoring program from the CoNLL 2011 shared task (Pradhan et al., 2011), based on the OntoNotes corpus (Hovy et al., 2006). We use the standard automatic parses and NER tags for each document. Our mentions are those output by the system of Lee et al. (2011); we also use their postprocessing to remove appositives, predicate nominatives, and singletons before evaluation. For each experiment, we report MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average. is below a cutoff γ. 6 Experiments Related Work Parameter settings. We take the regularization constant λ = 0.001 and the parameters of our surrogate loss (c1 , c2 , c3 ) = (0.15, 2.5, 1) for all models.4 All mod"
P13-1012,W11-1902,0,0.470878,"ons (Finkel and Manning, 2008; Song et al., 2012). We evaluate our system on the dataset from the CoNLL 2011 shared task using three different types of properties: synthetic oracle properties, entity phi features (number, gender, animacy, and NER type), and properties derived from unsupervised clusters targeting semantic type information. In all cases, our transitive model of entity properties equals or outperforms our pairwise system and our reimplementation of a previous entity-level system (Rahman and Ng, 2009). Our final system is competitive with the winner of the CoNLL 2011 shared task (Lee et al., 2011). Efficiently incorporating entity-level information is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions. We describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We show that our method can use entity-level information to outperform a basic pairwise system. 1 In"
P13-1012,C90-3063,0,0.0628431,"nd agreement ARG1:set ARG0:announced plan ARG1:approved ... ... ARG1:cause ARG2:following ARG1:reported ARG1:filed prices shares index rates ... ... ... ARG1:rose ARG1:fell ARG1:cut ARG1:closed ... ... Figure 4: Examples of clusters produced by the NAIVE BAYES model on SRL-tagged data with pronouns discarded. types of fine-grained semantic class information (Hendrickx and Daelemans, 2007; Ng, 2007; Rahman and Ng, 2010). Other approaches incorporate information from other sources (Ponzetto and Strube, 2006) or compute heuristic scores for realvalued features based on a large corpus or the web (Dagan and Itai, 1990; Yang et al., 2005; Bansal and Klein, 2012). We use four different clusterings in our experiments, each with twenty clusters: dependency-parse-derived NAIVE BAYES clusters, semantic-role-derived C ONDITIONAL clusters, SRL-derived NAIVE BAYES clusters generating a N OV ERB token when r cannot be determined, and SRL-derived NAIVE BAYES clusters with all pronoun tuples discarded. Examples of the latter clusters are shown in Figure 4. Each clustering is learned for 30 iterations of EM over English Gigaword (Graff et al., 2007), parsed with the Berkeley Parser (Petrov et al., 2006) and with SRL de"
P13-1012,N07-1030,0,0.043178,"n constant λ = 0.001 and the parameters of our surrogate loss (c1 , c2 , c3 ) = (0.15, 2.5, 1) for all models.4 All models are trained for 20 iterations. We take the pruning threshold γ = −2. Our BASIC model is a mention-ranking approach resembling models used by Denis and Baldridge (2008) and Rahman and Ng (2009), though it is trained using a novel parameterized loss function. It is also similar to the MLN-J OINT (BF) model of Song et al. (2012), but we enforce the singleparent constraint at a deeper structural level, allowing us to treat non-anaphoricity symmetrically with coreference as in Denis and Baldridge (2007) and Stoyanov and Eisner (2012). The model of Fernandes et al. (2012) also uses the single-parent constraint structurally, but with learning via latent perceptron and ILP-based one-best decoding rather than logistic regression and BP-based marginal computation. Our T RANSITIVE model is novel; while McCallum and Wellner (2004) proposed the idea of using attributes for mentions, they do not actually implement a model that does so. Other systems include entity-level information via handwritten rules (Raghunathan et al., 2010), induced rules (Yang et al., 2008), or features with learned weights (L"
P13-1012,P04-1018,0,0.553537,"rence over partitions. We describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We show that our method can use entity-level information to outperform a basic pairwise system. 1 Introduction The inclusion of entity-level features has been a driving force behind the development of many coreference resolution systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Lee et al., 2011). There is no polynomial-time dynamic program for inference in a model with arbitrary entity-level features, so systems that use such features typically rely on making decisions in a pipelined manner and sticking with them, operating greedily in a left-to-right fashion (Rahman and Ng, 2009) or in a multi-pass, sieve-like manner (Raghunathan et al., 2010). However, such systems may be locked into bad coreference decisions and are difficult to directly optimize for standard evaluation metrics. In this work, we present a new struct"
P13-1012,D08-1069,0,0.594988,"use their postprocessing to remove appositives, predicate nominatives, and singletons before evaluation. For each experiment, we report MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average. is below a cutoff γ. 6 Experiments Related Work Parameter settings. We take the regularization constant λ = 0.001 and the parameters of our surrogate loss (c1 , c2 , c3 ) = (0.15, 2.5, 1) for all models.4 All models are trained for 20 iterations. We take the pruning threshold γ = −2. Our BASIC model is a mention-ranking approach resembling models used by Denis and Baldridge (2008) and Rahman and Ng (2009), though it is trained using a novel parameterized loss function. It is also similar to the MLN-J OINT (BF) model of Song et al. (2012), but we enforce the singleparent constraint at a deeper structural level, allowing us to treat non-anaphoricity symmetrically with coreference as in Denis and Baldridge (2007) and Stoyanov and Eisner (2012). The model of Fernandes et al. (2012) also uses the single-parent constraint structurally, but with learning via latent perceptron and ILP-based one-best decoding rather than logistic regression and BP-based marginal computation. Ou"
P13-1012,H05-1004,0,0.417385,"SIC model; that is, those for which   PBASIC (ai |x) log maxj PBASIC (ai = j|x) 7 We use the datasets, experimental setup, and scoring program from the CoNLL 2011 shared task (Pradhan et al., 2011), based on the OntoNotes corpus (Hovy et al., 2006). We use the standard automatic parses and NER tags for each document. Our mentions are those output by the system of Lee et al. (2011); we also use their postprocessing to remove appositives, predicate nominatives, and singletons before evaluation. For each experiment, we report MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average. is below a cutoff γ. 6 Experiments Related Work Parameter settings. We take the regularization constant λ = 0.001 and the parameters of our surrogate loss (c1 , c2 , c3 ) = (0.15, 2.5, 1) for all models.4 All models are trained for 20 iterations. We take the pruning threshold γ = −2. Our BASIC model is a mention-ranking approach resembling models used by Denis and Baldridge (2008) and Rahman and Ng (2009), though it is trained using a novel parameterized loss function. It is also similar to the MLN-J OINT (BF) model of Song et al. (2012), but we enforce the singlepa"
P13-1012,P07-1068,0,0.0115929,"has been noted previously (Luo et al., 2004). 7.4 C LUSTERS MUC B 3 CEAFe 61.96 70.66 47.30 62.88 70.71 47.45 61.98 70.19 45.77 63.34 70.89 46.88 ARG0:said way ARG1:signed ARG0:say law ARG1:announced ARG0:found agreement ARG1:set ARG0:announced plan ARG1:approved ... ... ARG1:cause ARG2:following ARG1:reported ARG1:filed prices shares index rates ... ... ... ARG1:rose ARG1:fell ARG1:cut ARG1:closed ... ... Figure 4: Examples of clusters produced by the NAIVE BAYES model on SRL-tagged data with pronouns discarded. types of fine-grained semantic class information (Hendrickx and Daelemans, 2007; Ng, 2007; Rahman and Ng, 2010). Other approaches incorporate information from other sources (Ponzetto and Strube, 2006) or compute heuristic scores for realvalued features based on a large corpus or the web (Dagan and Itai, 1990; Yang et al., 2005; Bansal and Klein, 2012). We use four different clusterings in our experiments, each with twenty clusters: dependency-parse-derived NAIVE BAYES clusters, semantic-role-derived C ONDITIONAL clusters, SRL-derived NAIVE BAYES clusters generating a N OV ERB token when r cannot be determined, and SRL-derived NAIVE BAYES clusters with all pronoun tuples discarded."
P13-1012,P06-1055,1,0.0610045,"pus or the web (Dagan and Itai, 1990; Yang et al., 2005; Bansal and Klein, 2012). We use four different clusterings in our experiments, each with twenty clusters: dependency-parse-derived NAIVE BAYES clusters, semantic-role-derived C ONDITIONAL clusters, SRL-derived NAIVE BAYES clusters generating a N OV ERB token when r cannot be determined, and SRL-derived NAIVE BAYES clusters with all pronoun tuples discarded. Examples of the latter clusters are shown in Figure 4. Each clustering is learned for 30 iterations of EM over English Gigaword (Graff et al., 2007), parsed with the Berkeley Parser (Petrov et al., 2006) and with SRL determined by Senna (Collobert et al., 2011). Table 3 shows results of modeling these cluster properties. As in the case of oracle features, the PAIR P ROPERTY and L EFT T O R IGHT systems use the modes of the cluster posteriors, and the T RAN SITIVE system uses the posteriors directly as the Ri . We see comparable performance from incorporating features in both an entity-level framework and a pairwise framework, though the T RANSI - Clustering Features Finally, we consider mention properties derived from unsupervised clusterings; these properties are designed to target semantic"
P13-1012,M95-1005,0,0.929899,"izing with respect to a coreference-specific loss function. To this end, we will use softmax-margin (Gimpel and Smith, 2010), which augments the probability of each example with a term proportional to its loss, pushing the model to assign less mass to highly incorrect examples. We modify Equation 1 to use a new probability distribution P 0 instead of P , where P 0 (a|xi ) ∝ P (a|xi ) exp (l(a, C)) and l(a, C) is a loss function. In order to perform inference efficiently, l(a, C) must decompose linearly across mentions: l(a, C) = Pn i=1 l(ai , C). Commonly-used coreference metrics such as MUC (Vilain et al., 1995) and B 3 (Bagga and Baldwin, 1998) do not have this property, so we instead make use of a parameterized loss function that does and fit the parameters to give good performance. Specifically, we take n X a∈A(C i ) a ˆ = arg max P (a|x) a We then report the corresponding chains C(a) as the system output.3 For learning, the gradient takes the standard form of the gradient of a log-linear model, a difference of expected feature counts under the gold annotation and under no annotation. This requires computing marginals P 0 (ai |x) for each mention i, but because the model already factors this way,"
P13-1012,N06-1025,0,0.081196,"2.88 70.71 47.45 61.98 70.19 45.77 63.34 70.89 46.88 ARG0:said way ARG1:signed ARG0:say law ARG1:announced ARG0:found agreement ARG1:set ARG0:announced plan ARG1:approved ... ... ARG1:cause ARG2:following ARG1:reported ARG1:filed prices shares index rates ... ... ... ARG1:rose ARG1:fell ARG1:cut ARG1:closed ... ... Figure 4: Examples of clusters produced by the NAIVE BAYES model on SRL-tagged data with pronouns discarded. types of fine-grained semantic class information (Hendrickx and Daelemans, 2007; Ng, 2007; Rahman and Ng, 2010). Other approaches incorporate information from other sources (Ponzetto and Strube, 2006) or compute heuristic scores for realvalued features based on a large corpus or the web (Dagan and Itai, 1990; Yang et al., 2005; Bansal and Klein, 2012). We use four different clusterings in our experiments, each with twenty clusters: dependency-parse-derived NAIVE BAYES clusters, semantic-role-derived C ONDITIONAL clusters, SRL-derived NAIVE BAYES clusters generating a N OV ERB token when r cannot be determined, and SRL-derived NAIVE BAYES clusters with all pronoun tuples discarded. Examples of the latter clusters are shown in Figure 4. Each clustering is learned for 30 iterations of EM over"
P13-1012,P05-1021,0,0.0314974,"ARG0:announced plan ARG1:approved ... ... ARG1:cause ARG2:following ARG1:reported ARG1:filed prices shares index rates ... ... ... ARG1:rose ARG1:fell ARG1:cut ARG1:closed ... ... Figure 4: Examples of clusters produced by the NAIVE BAYES model on SRL-tagged data with pronouns discarded. types of fine-grained semantic class information (Hendrickx and Daelemans, 2007; Ng, 2007; Rahman and Ng, 2010). Other approaches incorporate information from other sources (Ponzetto and Strube, 2006) or compute heuristic scores for realvalued features based on a large corpus or the web (Dagan and Itai, 1990; Yang et al., 2005; Bansal and Klein, 2012). We use four different clusterings in our experiments, each with twenty clusters: dependency-parse-derived NAIVE BAYES clusters, semantic-role-derived C ONDITIONAL clusters, SRL-derived NAIVE BAYES clusters generating a N OV ERB token when r cannot be determined, and SRL-derived NAIVE BAYES clusters with all pronoun tuples discarded. Examples of the latter clusters are shown in Figure 4. Each clustering is learned for 30 iterations of EM over English Gigaword (Graff et al., 2007), parsed with the Berkeley Parser (Petrov et al., 2006) and with SRL determined by Senna ("
P13-1012,W11-1901,0,0.171711,"w when a document contains over 200 mentions. Therefore, we use our BA SIC model to prune antecedent choices for each ai in order to reduce the size of the factor graph that we must instantiate. Specifically, we prune links between pairs of mentions that are of mention distance more than 100, as well as values for ai that fall below a particular odds ratio threshold with respect to the best setting of that ai in the BASIC model; that is, those for which   PBASIC (ai |x) log maxj PBASIC (ai = j|x) 7 We use the datasets, experimental setup, and scoring program from the CoNLL 2011 shared task (Pradhan et al., 2011), based on the OntoNotes corpus (Hovy et al., 2006). We use the standard automatic parses and NER tags for each document. Our mentions are those output by the system of Lee et al. (2011); we also use their postprocessing to remove appositives, predicate nominatives, and singletons before evaluation. For each experiment, we report MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average. is below a cutoff γ. 6 Experiments Related Work Parameter settings. We take the regularization constant λ = 0.001 and the parameters of our surrogate loss (c1 ,"
P13-1012,P08-1096,0,0.0325606,"cally with coreference as in Denis and Baldridge (2007) and Stoyanov and Eisner (2012). The model of Fernandes et al. (2012) also uses the single-parent constraint structurally, but with learning via latent perceptron and ILP-based one-best decoding rather than logistic regression and BP-based marginal computation. Our T RANSITIVE model is novel; while McCallum and Wellner (2004) proposed the idea of using attributes for mentions, they do not actually implement a model that does so. Other systems include entity-level information via handwritten rules (Raghunathan et al., 2010), induced rules (Yang et al., 2008), or features with learned weights (Luo et al., 2004; Rahman and Ng, 2011), but all of these systems freeze past coreference decisions in order to compute their entities. Most similar to our entity-level approach is the system of Haghighi and Klein (2010), which 7.1 Systems Besides our BASIC and T RANSITIVE systems, we evaluate a strictly pairwise system that incorporates property information by way of indicator features on the current mention’s most likely property value and the proposed antecedent’s most likely property value. We call this system PAIR P ROP ERTY ; it is simply the BASIC syst"
P13-1012,D10-1048,0,0.288912,"ation to outperform a basic pairwise system. 1 Introduction The inclusion of entity-level features has been a driving force behind the development of many coreference resolution systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Lee et al., 2011). There is no polynomial-time dynamic program for inference in a model with arbitrary entity-level features, so systems that use such features typically rely on making decisions in a pipelined manner and sticking with them, operating greedily in a left-to-right fashion (Rahman and Ng, 2009) or in a multi-pass, sieve-like manner (Raghunathan et al., 2010). However, such systems may be locked into bad coreference decisions and are difficult to directly optimize for standard evaluation metrics. In this work, we present a new structured model of entity-level information designed to allow efficient inference. We use a log-linear model that can be expressed as a factor graph. Pairwise features appear in the model as unary factors, adjacent to nodes representing a choice of antecedent (or none) for each mention. Additional nodes model entity-level properties on a per-mention basis, and 2 Example We begin with an example motivating our use of entity-"
P13-1012,D11-1135,0,0.0273976,"ns the semantic role of n (or some approximation thereof) conjoined with its governor. Two different algorithms are used to cluster these pairs: a NAIVE BAYES model, where c generates n and r, and a C ONDITIONAL model, where c is generated conditioned on r and then n is generated from c. Parameters for each can be learned with the expectation maximization (EM) algorithm (Dempster et al., 1977), with symmetry broken by a small amount of random noise at initialization. Similar models have been used to learn subcategorization information (Rooth et al., 1999) or properties of verb argument slots (Yao et al., 2011). We choose this kind of clustering for its relative simplicity and because it allows pronouns to have more informed properties (from their verbal context) than would be possible using a model that makes type-level decisions about nominals only. Though these specific cluster features are novel to coreference, previous work has used similar 121 BASIC S TANFORD Prec. 69.99 61.49 MUC Rec. 55.59 59.59 PAIR P ROPERTY L EFT T O R IGHT T RANSITIVE 76.49 76.92 76.48 58.53 58.55 60.20 L EFT T O R IGHT T RANSITIVE 69.77 70.27 54.73 56.54 BASIC -P HI PAIR P ROPERTY L EFT T O R IGHT T RANSITIVE 67.04 70.2"
P13-1012,D09-1101,0,0.527408,"models specific entity-level properties, rather than simply enforcing transitivity of pairwise decisions (Finkel and Manning, 2008; Song et al., 2012). We evaluate our system on the dataset from the CoNLL 2011 shared task using three different types of properties: synthetic oracle properties, entity phi features (number, gender, animacy, and NER type), and properties derived from unsupervised clusters targeting semantic type information. In all cases, our transitive model of entity properties equals or outperforms our pairwise system and our reimplementation of a previous entity-level system (Rahman and Ng, 2009). Our final system is competitive with the winner of the CoNLL 2011 shared task (Lee et al., 2011). Efficiently incorporating entity-level information is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions. We describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We"
P13-1012,C10-1105,0,0.0107721,"oted previously (Luo et al., 2004). 7.4 C LUSTERS MUC B 3 CEAFe 61.96 70.66 47.30 62.88 70.71 47.45 61.98 70.19 45.77 63.34 70.89 46.88 ARG0:said way ARG1:signed ARG0:say law ARG1:announced ARG0:found agreement ARG1:set ARG0:announced plan ARG1:approved ... ... ARG1:cause ARG2:following ARG1:reported ARG1:filed prices shares index rates ... ... ... ARG1:rose ARG1:fell ARG1:cut ARG1:closed ... ... Figure 4: Examples of clusters produced by the NAIVE BAYES model on SRL-tagged data with pronouns discarded. types of fine-grained semantic class information (Hendrickx and Daelemans, 2007; Ng, 2007; Rahman and Ng, 2010). Other approaches incorporate information from other sources (Ponzetto and Strube, 2006) or compute heuristic scores for realvalued features based on a large corpus or the web (Dagan and Itai, 1990; Yang et al., 2005; Bansal and Klein, 2012). We use four different clusterings in our experiments, each with twenty clusters: dependency-parse-derived NAIVE BAYES clusters, semantic-role-derived C ONDITIONAL clusters, SRL-derived NAIVE BAYES clusters generating a N OV ERB token when r cannot be determined, and SRL-derived NAIVE BAYES clusters with all pronoun tuples discarded. Examples of the latte"
P13-1012,P99-1014,0,0.0200981,"pairs (n, r) of a noun head n and a string r which contains the semantic role of n (or some approximation thereof) conjoined with its governor. Two different algorithms are used to cluster these pairs: a NAIVE BAYES model, where c generates n and r, and a C ONDITIONAL model, where c is generated conditioned on r and then n is generated from c. Parameters for each can be learned with the expectation maximization (EM) algorithm (Dempster et al., 1977), with symmetry broken by a small amount of random noise at initialization. Similar models have been used to learn subcategorization information (Rooth et al., 1999) or properties of verb argument slots (Yao et al., 2011). We choose this kind of clustering for its relative simplicity and because it allows pronouns to have more informed properties (from their verbal context) than would be possible using a model that makes type-level decisions about nominals only. Though these specific cluster features are novel to coreference, previous work has used similar 121 BASIC S TANFORD Prec. 69.99 61.49 MUC Rec. 55.59 59.59 PAIR P ROPERTY L EFT T O R IGHT T RANSITIVE 76.49 76.92 76.48 58.53 58.55 60.20 L EFT T O R IGHT T RANSITIVE 69.77 70.27 54.73 56.54 BASIC -P H"
P13-1012,D08-1016,0,0.016988,"or learning, the gradient takes the standard form of the gradient of a log-linear model, a difference of expected feature counts under the gold annotation and under no annotation. This requires computing marginals P 0 (ai |x) for each mention i, but because the model already factors this way, this step is easy. The T RANSITIVE model is more complex. Exact inference is intractable due to the E factors that couple all of the ai by way of the pi nodes. However, we can compute approximate marginals for the ai , pi , and ri using belief propagation. BP has been effectively used on other NLP tasks (Smith and Eisner, 2008; Burkett and Klein, 2012), and is effective in cases such as this where the model is largely driven by non-loopy factors (here, the Ai ). From marginals over each node, we can compute the necessary gradient and decode as before: [c1 I(K1 (ai , C)) + c2 I(K2 (ai , C)) a ˆ = arg max Pˆ (a|x) i=1 a + c3 I(K3 (ai , C))] 3 One could use ILP-based decoding in the style of Finkel and Manning (2008) and Song et al. (2012) to attempt to explicitly find the optimal C with choice of a marginalized out, but we did not explore this option. where c1 , c2 , and c3 are real-valued weights, K1 denotes the eve"
P13-1012,D12-1114,0,0.183478,"their partial membership in various coreference chains, so that information about entity-level properties is decentralized and propagated across individual mentions, and we never need to explicitly instantiate entities. Exact inference in this factor graph is intractable, but efficient approximate inference can be carried out with belief propagation. Our model is the first discriminatively-trained model that both makes joint decisions over an entire document and models specific entity-level properties, rather than simply enforcing transitivity of pairwise decisions (Finkel and Manning, 2008; Song et al., 2012). We evaluate our system on the dataset from the CoNLL 2011 shared task using three different types of properties: synthetic oracle properties, entity phi features (number, gender, animacy, and NER type), and properties derived from unsupervised clusters targeting semantic type information. In all cases, our transitive model of entity properties equals or outperforms our pairwise system and our reimplementation of a previous entity-level system (Rahman and Ng, 2009). Our final system is competitive with the winner of the CoNLL 2011 shared task (Lee et al., 2011). Efficiently incorporating enti"
P13-1012,C12-1154,0,0.0766171,"rameters of our surrogate loss (c1 , c2 , c3 ) = (0.15, 2.5, 1) for all models.4 All models are trained for 20 iterations. We take the pruning threshold γ = −2. Our BASIC model is a mention-ranking approach resembling models used by Denis and Baldridge (2008) and Rahman and Ng (2009), though it is trained using a novel parameterized loss function. It is also similar to the MLN-J OINT (BF) model of Song et al. (2012), but we enforce the singleparent constraint at a deeper structural level, allowing us to treat non-anaphoricity symmetrically with coreference as in Denis and Baldridge (2007) and Stoyanov and Eisner (2012). The model of Fernandes et al. (2012) also uses the single-parent constraint structurally, but with learning via latent perceptron and ILP-based one-best decoding rather than logistic regression and BP-based marginal computation. Our T RANSITIVE model is novel; while McCallum and Wellner (2004) proposed the idea of using attributes for mentions, they do not actually implement a model that does so. Other systems include entity-level information via handwritten rules (Raghunathan et al., 2010), induced rules (Yang et al., 2008), or features with learned weights (Luo et al., 2004; Rahman and Ng,"
P13-1012,W12-4502,0,\N,Missing
P13-1012,P10-2029,0,\N,Missing
P13-1012,D08-1067,0,\N,Missing
P13-1021,koen-2004-pharaoh,0,0.0151054,"to each result. Bernoulli parameter for this pixel, we can write:  [XiGLYPH ]jk ∼ Bernoulli θ PIXEL (j, k, gi ; φei ) The interpolation process for a single row is depicted in Figure 5. We define a constant interpolation vector µ(gi , k) that is specific to the glyph box width gi and glyph box column k. Each µ(gi , k) is shaped according to a Gaussian centered at the relative column position in φei . The glyph pixel Bernoulli parameters are defined as follows: 4.2 The number of states in the dynamic programming lattice grows exponentially with the order of the language model (Jelinek, 1998; Koehn, 2004). As a result, inference can become slow when the language model order n is large. To remedy this, we take a coarse-to-fine approach to both learning and inference. On each iteration of EM, we perform two passes: a coarse pass using a low-order language model, and a fine pass using a high-order language model (Petrov et al., 2008; Zhang and Gildea, 2008). We use the marginals4 from the coarse pass to prune states from the dynamic program of the fine pass. θ PIXEL (j, k,gi ; φei ) = w h X i logistic µ(gi , k)k0 · [φei ]jk0 k0 =1 The fact that the parameterization is log-linear will ensure tha"
P13-1021,D11-1029,1,0.848221,"fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and in our problem we face a grid of pixels for which the segmentation into symbols is unknown. In contrast, decipherment typically deals only with discrete symbols. P (E, T, R, X) = [Language model] P (E) [Ty"
P13-1021,N03-1018,0,0.0344936,"Missing"
P13-1021,N10-1083,1,0.467591,"Missing"
P13-1021,J04-4002,0,0.02125,"Missing"
P13-1021,D08-1012,1,0.899245,"Missing"
P13-1021,D08-1085,0,0.0125897,"rical documents has done so using a pipelined approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and in our problem we face a grid of pixels for which the segmentation into symbols is unknown. In contrast, decipherment typicall"
P13-1021,P11-1025,0,0.01315,"d approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and in our problem we face a grid of pixels for which the segmentation into symbols is unknown. In contrast, decipherment typically deals only with discrete symbols. P (E, T,"
P13-1021,P10-1107,0,0.0205837,"e so using a pipelined approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and in our problem we face a grid of pixels for which the segmentation into symbols is unknown. In contrast, decipherment typically deals only with dis"
P13-1021,P08-1025,0,0.0542564,"a Gaussian centered at the relative column position in φei . The glyph pixel Bernoulli parameters are defined as follows: 4.2 The number of states in the dynamic programming lattice grows exponentially with the order of the language model (Jelinek, 1998; Koehn, 2004). As a result, inference can become slow when the language model order n is large. To remedy this, we take a coarse-to-fine approach to both learning and inference. On each iteration of EM, we perform two passes: a coarse pass using a low-order language model, and a fine pass using a high-order language model (Petrov et al., 2008; Zhang and Gildea, 2008). We use the marginals4 from the coarse pass to prune states from the dynamic program of the fine pass. θ PIXEL (j, k,gi ; φei ) = w h X i logistic µ(gi , k)k0 · [φei ]jk0 k0 =1 The fact that the parameterization is log-linear will ensure that, during the unsupervised learning process, updating the shape parameters φc is simple and feasible. By varying the magnitude of µ we can change the level of smoothing in the logistic model and cause it to permit areas that are over-inked. This is the effect that di controls. By offsetting the rows of φc that we interpolate weights from, we change the v"
P13-2018,E09-1031,0,0.17099,"h error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges. 1 James R. Curran‡ Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modiﬁcations to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the PCTB. However, their manual analysis was limited in scope, covering only part of th"
P13-2018,D07-1027,0,0.0730843,"Missing"
P13-2018,P09-1059,0,0.08056,"lso investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges. 1 James R. Curran‡ Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modiﬁcations to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the PCTB. However, their manual analysis was limited in scope, c"
P13-2018,P03-1054,1,0.0191192,"g some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the PCTB. However, their manual analysis was limited in scope, covering only part of the parser output, and was unable to characterize the relative impact of the issues they uncovered. 2 Background The closest previous work is the detailed manual analysis performed by Levy and Manning (2003). While their focus was on issues faced by their factored PCFG parser (Klein and Manning, 2003b), the error types they identiﬁed are general issues presented by Chinese syntax in the PCTB. They presented several Chinese error types that are rare or absent in English, including noun/verb ambiguity, NP-internal structure and coordination ambiguity due to pro-drop, suggesting that closing the English-Chinese parsing gap demands techniques 1 The system described in this paper is available from http://code.google.com/p/berkeley-parser-analyser/ 98 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 98–103, c Sofia, Bulgaria, August 4-9 2013. 2013 A"
P13-2018,D12-1096,1,0.875836,"Missing"
P13-2018,P03-1056,0,0.0337462,"only addresses certain error types, leaving substantial outstanding challenges. 1 James R. Curran‡ Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modiﬁcations to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the PCTB. However, their manual analysis was limited in scope, covering only part of the parser output, and was unable to characterize the relative impact of the issues they uncovered. 2 Bac"
P13-2018,J93-2004,0,0.0433648,"Missing"
P13-2018,W09-3825,0,0.0367848,"ach Span Label Sense Edge Attach Attach Attach Other 0.76 0.72 0.21 0.30 0.05 0.21 0.26 0.22 0.18 1.87 1.48 1.68 1.06 1.02 0.88 0.55 0.50 0.44 0.44 4.11 Table 2: Error breakdown for the development set of PCTB 6. The area ﬁlled in for each bar indicates the average number of bracket errors per sentence attributed to that error type, where an empty bar is no errors and a full bar has the value indicated in the bottom row. The parsers are: the Berkeley parser with gold POS tags as input (Berk-G), the Berkeley product parser with two grammars (Berk-2), the Berkeley parser (Berk-1), the parser of Zhang and Clark (2009) (ZPAR), the Bikel parser (Bikel), the Stanford Factored parser (Stan-F), and the Stanford Unlexicalized PCFG parser (Stan-P). two categories (e.g. between Verb taking wrong args and NP Attachment). Differences in treebank annotations also present a challenge for cross-language error comparison. The most common error type in Chinese, NPinternal structure, is rare in the results of Kummerfeld et al. (2012), but the datasets are not comparable because the PTB has very limited NP-internal structure annotated. Further characterization of the impact of annotation differences on errors is beyond the"
P13-2018,P06-1055,1,0.686979,"Missing"
P13-2018,D12-1046,0,0.0984765,"se parsers, covering a broad range of error types for large sets of sentences, enabling the ﬁrst empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges. 1 James R. Curran‡ Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modiﬁcations to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis t"
P13-2018,N12-1030,1,0.873947,"as been a major part of several recent papers (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009). The Berk-G row of Table 2 shows the performance of the Berkeley parser when given gold POS tags.5 While the F1 improvement is unsurprising, for the ﬁrst time we can clearly show that the gains are only in a subset of the error types. In particular, tagging improvement will not help for two of the most signiﬁcant challenges: coordination scope errors, and verb argument selection. To see which tagging confusions contribute to which error reductions, we adapt the POS ablation approach of Tse and Curran (2012). We consider the POS tag pairs shown in Table 3. To isolate the effects of each confusion we start from the gold tags and introduce the output of the Stanford tagger whenever it returns one of the two tags being considered.6 We then feed these “semi-gold” tags Cross-parser analysis The previous section described the error types and their distribution for a single Chinese parser. Here we conﬁrm that these are general trends, by showing that the same pattern is observed for several different parsers on the PCTB 6 dev set.3 We include results for a transition-based parser (ZPAR; Zhang and Clark,"
P13-2018,I05-1007,0,0.0330528,"Missing"
P13-2018,N07-1051,1,\N,Missing
P13-2018,W00-1201,0,\N,Missing
P13-2018,N10-1003,0,\N,Missing
P14-1020,D13-1195,1,0.152689,"ley.edu Abstract classic single-core processors and main memory architectures are no longer getting substantially faster over time, so speed gains must now come from parallelism within a single machine. Second, compared to CPUs, GPUs devote a much larger fraction of their computational power to actual arithmetic. Since tasks like parsing boil down to repeated read-multiply-write loops, GPUs should be many times more efficient in time, power, or cost. The challenge is that GPUs are not a good fit for the kinds of sparse computations that most current CPU-based NLP algorithms rely on. Recently, Canny et al. (2013) proposed a GPU implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that GPUs can provide. Their system uses a grammar based on the Berkeley parser (Petrov and Klein, 2007) (which is particularly amenable to GPU processing), “compiling” the grammar into a sequence of GPU kernels that are applied densely to every item in the parse chart. Together these kernels implement the Viterbi inside algorithm. On a mid-range GPU, their system can compute Viterbi derivations at 164 sentences per second on sentences of length 40 or less (see timing detai"
P14-1020,N06-1022,0,0.0738887,"Missing"
P14-1020,N13-1033,0,0.188699,"Missing"
P14-1020,U11-1006,0,0.0944752,"Missing"
P14-1020,J93-2004,0,0.0455682,"U implementation, resulting in overall speeds of up to 404 sentences per second. For comparison, the publicly available CPU implementation of Petrov and Klein (2007) parses approximately 7 sentences per second per core on a modern CPU. A further drawback of the dense approach in Canny et al. (2013) is that it only computes Viterbi parses. As with other grammars with a parse/derivation distinction, the grammars of Petrov and Klein (2007) only achieve their full accuracy using minimum-Bayes-risk parsing, with improvements of over 1.5 F1 over best-derivation Viterbi parsing on the Penn Treebank (Marcus et al., 1993). To that end, we extend our coarse-tofine GPU approach to computing marginals, along the way proposing a new way to exploit the coarse pass to avoid expensive log-domain computations in the fine pass. We then implement minimumBayes-risk parsing via the max recall algorithm of Goodman (1996). Without the coarse pass, the dense marginal computation is not efficient on a GPU, processing only 32 sentences per second. However, our approach allows us to process over 190 sentences per second, almost a 6x speedup. 2 use two NVIDIA GeForce GTX 690s—each of which is essentially a repackaging of two 680"
P14-1020,P05-1010,0,0.0375439,"ecause parsing with these grammars is still quite fast, we tried using them as the coarse pass instead. As shown in Table 1, using a 1-split grammar as a coarse pass allows us to produce over 400 sentences per second, a full 2x improvement over our original system. Conducting a coarse pass with a 2-split grammar is somewhat slower, at a “mere” 343 sentences per second. 9 Minimum Bayes risk parsing The Viterbi algorithm is a reasonably effective method for parsing. However, many authors have noted that parsers benefit substantially from minimum Bayes risk decoding (Goodman, 1996; Simaan, 2003; Matsuzaki et al., 2005; Titov and Henderson, 2006; Petrov and Klein, 2007). MBR algorithms for parsing do not compute the best derivation, as in Viterbi parsing, but instead the parse tree that maximizes the expected count of some figure of merit. For instance, one might want to maximize the expected number of correct constituents (Goodman, 1996), or the expected rule counts (Simaan, 2003; Petrov and Klein, 2007). MBR parsing has proven especially useful in latent variable grammars. Petrov and Klein (2007) showed that MBR trees substantially improved performance over Viterbi parses for latent variable grammars, ear"
P14-1020,N07-1051,1,0.909398,"CPUs, GPUs devote a much larger fraction of their computational power to actual arithmetic. Since tasks like parsing boil down to repeated read-multiply-write loops, GPUs should be many times more efficient in time, power, or cost. The challenge is that GPUs are not a good fit for the kinds of sparse computations that most current CPU-based NLP algorithms rely on. Recently, Canny et al. (2013) proposed a GPU implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that GPUs can provide. Their system uses a grammar based on the Berkeley parser (Petrov and Klein, 2007) (which is particularly amenable to GPU processing), “compiling” the grammar into a sequence of GPU kernels that are applied densely to every item in the parse chart. Together these kernels implement the Viterbi inside algorithm. On a mid-range GPU, their system can compute Viterbi derivations at 164 sentences per second on sentences of length 40 or less (see timing details below). In this paper, we develop algorithms that can exploit sparsity on a GPU by adapting coarse-tofine pruning to a GPU setting. On a CPU, pruning methods can give speedups of up to 100x. Such extreme speedups over a den"
P14-1020,W03-3021,0,0.0193841,"per second. Because parsing with these grammars is still quite fast, we tried using them as the coarse pass instead. As shown in Table 1, using a 1-split grammar as a coarse pass allows us to produce over 400 sentences per second, a full 2x improvement over our original system. Conducting a coarse pass with a 2-split grammar is somewhat slower, at a “mere” 343 sentences per second. 9 Minimum Bayes risk parsing The Viterbi algorithm is a reasonably effective method for parsing. However, many authors have noted that parsers benefit substantially from minimum Bayes risk decoding (Goodman, 1996; Simaan, 2003; Matsuzaki et al., 2005; Titov and Henderson, 2006; Petrov and Klein, 2007). MBR algorithms for parsing do not compute the best derivation, as in Viterbi parsing, but instead the parse tree that maximizes the expected count of some figure of merit. For instance, one might want to maximize the expected number of correct constituents (Goodman, 1996), or the expected rule counts (Simaan, 2003; Petrov and Klein, 2007). MBR parsing has proven especially useful in latent variable grammars. Petrov and Klein (2007) showed that MBR trees substantially improved performance over Viterbi parses for laten"
P14-1020,W06-1666,0,0.0263958,"e grammars is still quite fast, we tried using them as the coarse pass instead. As shown in Table 1, using a 1-split grammar as a coarse pass allows us to produce over 400 sentences per second, a full 2x improvement over our original system. Conducting a coarse pass with a 2-split grammar is somewhat slower, at a “mere” 343 sentences per second. 9 Minimum Bayes risk parsing The Viterbi algorithm is a reasonably effective method for parsing. However, many authors have noted that parsers benefit substantially from minimum Bayes risk decoding (Goodman, 1996; Simaan, 2003; Matsuzaki et al., 2005; Titov and Henderson, 2006; Petrov and Klein, 2007). MBR algorithms for parsing do not compute the best derivation, as in Viterbi parsing, but instead the parse tree that maximizes the expected count of some figure of merit. For instance, one might want to maximize the expected number of correct constituents (Goodman, 1996), or the expected rule counts (Simaan, 2003; Petrov and Klein, 2007). MBR parsing has proven especially useful in latent variable grammars. Petrov and Klein (2007) showed that MBR trees substantially improved performance over Viterbi parses for latent variable grammars, earning up to 1.5F1. Here, we"
P14-1020,W11-2921,0,0.287847,"Missing"
P14-1020,P96-1024,0,\N,Missing
P14-1022,W13-4916,0,0.0731857,"Missing"
P14-1022,E93-1006,0,0.190874,"ead lexicalization (Eisner, 1996; Collins, 1997; Charniak, 1997), structural annotation (Johnson, 1998; Klein and Manning, 2003), and state-splitting (Matsuzaki et al., 2005; Petrov et al., 2006) are all designed to take coarse symbols like PP and decorate them with additional context. The underlying reason that such propagation is even needed is that PCFG parsers score trees based on local configurations only, and any information that is not threaded through the tree becomes inaccessible to the scoring function. There have been non-local approaches as well, such as tree-substitution parsers (Bod, 1993; Sima’an, 2000), neural net parsers (Henderson, 2003), and rerankers (Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008). These non-local approaches can actually go even further in enriching the grammar’s structural complexity by coupling larger domains in various ways, though their non-locality generally complicates inference. In this work, we instead try to minimize the structural complexity of the grammar by moving as much context as possible onto local surface features. We examine the position that grammars should not propagate any information that is available from surface s"
P14-1022,J92-4003,0,0.04521,"consistently head final. Structural contexts like those captured by parent annotation (Johnson, 1998) are more subtle. Parent annotation can capture, for instance, the difference in distribution in NPs that have S as a parent (that is, subjects) and NPs under VPs (objects). We try to capture some of this same intuition by introducing a feature on the length of a span. For instance, VPs embedded in NPs tend to be short, usually as embedded gerund phrases. Because constituents in the treebank can be quite long, we bin our length features into 8 buckets, of 1 Experiments with the Brown clusters (Brown et al., 1992) provided by Turian et al. (2010) in lieu of suffixes were not promising. Moreover, lowering this threshold did not improve performance. 231 PRN VP ( CEO of Enron ) said , “ Too bad , ” (XxX) x,“Xx,” VP → no VBP NNS VP VBP NNS no read messages in his inbox Figure 2: An example showing the utility of span context. The ambiguity about whether read is an adjective or a verb is resolved when we construct a VP and notice that the word proceeding it is unlikely. Figure 4: Computation of span shape features on two examples. Parentheticals, quotes, and other punctuation-heavy, short constituents benef"
P14-1022,P05-1022,0,0.0838469,"Klein and Manning, 2003), and state-splitting (Matsuzaki et al., 2005; Petrov et al., 2006) are all designed to take coarse symbols like PP and decorate them with additional context. The underlying reason that such propagation is even needed is that PCFG parsers score trees based on local configurations only, and any information that is not threaded through the tree becomes inaccessible to the scoring function. There have been non-local approaches as well, such as tree-substitution parsers (Bod, 1993; Sima’an, 2000), neural net parsers (Henderson, 2003), and rerankers (Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008). These non-local approaches can actually go even further in enriching the grammar’s structural complexity by coupling larger domains in various ways, though their non-locality generally complicates inference. In this work, we instead try to minimize the structural complexity of the grammar by moving as much context as possible onto local surface features. We examine the position that grammars should not propagate any information that is available from surface strings, since a discriminative parser can access that information directly. We therefore begin with a minimal grammar an"
P14-1022,J05-1003,0,0.0405921,"otation (Johnson, 1998; Klein and Manning, 2003), and state-splitting (Matsuzaki et al., 2005; Petrov et al., 2006) are all designed to take coarse symbols like PP and decorate them with additional context. The underlying reason that such propagation is even needed is that PCFG parsers score trees based on local configurations only, and any information that is not threaded through the tree becomes inaccessible to the scoring function. There have been non-local approaches as well, such as tree-substitution parsers (Bod, 1993; Sima’an, 2000), neural net parsers (Henderson, 2003), and rerankers (Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008). These non-local approaches can actually go even further in enriching the grammar’s structural complexity by coupling larger domains in various ways, though their non-locality generally complicates inference. In this work, we instead try to minimize the structural complexity of the grammar by moving as much context as possible onto local surface features. We examine the position that grammars should not propagate any information that is available from surface strings, since a discriminative parser can access that information directly. We therefore beg"
P14-1022,P97-1003,0,0.271872,"longer spans, we only use words sufficiently close to the span’s beginning and end. 232 Annotation v = 0, h = 0 v = 1, h = 0 v = 0, h = 1 v = 1, h = 1 Lexicalized Dev, len ≤ 40 90.1 90.5 90.2 90.9 90.3 Berkeley This work Test all 90.1 89.2 Table 3: Final Parseval results for the v = 1, h = 0 parser on Section 23 of the Penn Treebank. Table 2: Results for the Penn Treebank development set, sentences of length ≤ 40, for different annotation schemes implemented on top of the Xbar grammar. 5.2 Lexical Annotation Another commonly-used kind of structural annotation is lexicalization (Eisner, 1996; Collins, 1997; Charniak, 1997). By annotating grammar nonterminals with their headwords, the idea is to better model phenomena that depend heavily on the semantics of the words involved, such as coordination and PP attachment. Table 2 shows results from lexicalizing the Xbar grammar; it provides meager improvements. One probable reason for this is that our parser already includes monolexical features that inspect the first and last words of each span, which captures the syntactic or the semantic head in many cases or can otherwise provide information about what the constituent’s type may be and how it is l"
P14-1022,C96-1058,0,0.151043,"of one in2 For longer spans, we only use words sufficiently close to the span’s beginning and end. 232 Annotation v = 0, h = 0 v = 1, h = 0 v = 0, h = 1 v = 1, h = 1 Lexicalized Dev, len ≤ 40 90.1 90.5 90.2 90.9 90.3 Berkeley This work Test all 90.1 89.2 Table 3: Final Parseval results for the v = 1, h = 0 parser on Section 23 of the Penn Treebank. Table 2: Results for the Penn Treebank development set, sentences of length ≤ 40, for different annotation schemes implemented on top of the Xbar grammar. 5.2 Lexical Annotation Another commonly-used kind of structural annotation is lexicalization (Eisner, 1996; Collins, 1997; Charniak, 1997). By annotating grammar nonterminals with their headwords, the idea is to better model phenomena that depend heavily on the semantics of the words involved, such as coordination and PP attachment. Table 2 shows results from lexicalizing the Xbar grammar; it provides meager improvements. One probable reason for this is that our parser already includes monolexical features that inspect the first and last words of each span, which captures the syntactic or the semantic head in many cases or can otherwise provide information about what the constituent’s type may be"
P14-1022,P08-1109,0,0.0650238,"Missing"
P14-1022,W01-0521,0,0.0593252,"PP attachment. Table 2 shows results from lexicalizing the Xbar grammar; it provides meager improvements. One probable reason for this is that our parser already includes monolexical features that inspect the first and last words of each span, which captures the syntactic or the semantic head in many cases or can otherwise provide information about what the constituent’s type may be and how it is likely to combine. Lexicalization allows us to capture bilexical relationships along dependency arcs, but it has been previously shown that these add only marginal benefit to Collins’s model anyway (Gildea, 2001). Recall from Section 3 that every span feature is conjoined with indicators over rules and rule parents to produce features over anchored rule productions; when we consider adding an annotation layer to the grammar, what that does is refine the rule indicators that are conjoined with every span feature. While this is a powerful way of refining features, we show that common successful annotation schemes provide at best modest benefit on top of the base parser. 5.1 Test ≤ 40 90.6 89.9 Structural Annotation The most basic, well-understood kind of annotation on top of an X-bar grammar is structur"
P14-1022,D12-1105,1,0.819196,"across tasks to an extent. For example, Socher et al. (2013) demonstrates that sentiment analysis, which is usually approached as a flat classification task, can be viewed as tree-structured. In their work, they propagate real-valued vectors up a tree using neural tensor nets and see gains from their recursive approach. Our parser can be easily adapted to this task by replacing the X-bar grammar over treebank symbols with a grammar over the sentiment values to encode the output variables and then adding n-gram indicators to our feature set to capture the bulk of the lexical effects. When 229 Hall and Klein (2012) employed both kinds of annotations, along with lexicalized head word annotation. All of these past CRF parsers do also exploit span features, as did the structured margin parser of Taskar et al. (2004); the current work primarily differs in shifting the work from the grammar to the surface features. The problem with rich annotations is that they increase the state space of the grammar substantially. For example, adding parent annotation can square the number of symbols, and each subsequent annotation causes a multiplicative increase in the size of the state space. Hall and Klein (2012) attemp"
P14-1022,P08-1067,0,0.0124476,"nd state-splitting (Matsuzaki et al., 2005; Petrov et al., 2006) are all designed to take coarse symbols like PP and decorate them with additional context. The underlying reason that such propagation is even needed is that PCFG parsers score trees based on local configurations only, and any information that is not threaded through the tree becomes inaccessible to the scoring function. There have been non-local approaches as well, such as tree-substitution parsers (Bod, 1993; Sima’an, 2000), neural net parsers (Henderson, 2003), and rerankers (Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008). These non-local approaches can actually go even further in enriching the grammar’s structural complexity by coupling larger domains in various ways, though their non-locality generally complicates inference. In this work, we instead try to minimize the structural complexity of the grammar by moving as much context as possible onto local surface features. We examine the position that grammars should not propagate any information that is available from surface strings, since a discriminative parser can access that information directly. We therefore begin with a minimal grammar and iteratively"
P14-1022,P00-1008,0,0.0493641,"Missing"
P14-1022,J98-4004,0,0.0581003,"ble to us, namely, the identity of the first and last words of a span. Because heads of constituents are often at the beginning or the end of a span, these feature templates can (noisily) capture monolexical properties of heads without having to incur the inferential cost of lexicalized annotations. For example, in English, the syntactic head of a verb phrase is typically at the beginning of the span, while the head of a simple noun phrase is the last word. Other languages, like Korean or Japanese, are more consistently head final. Structural contexts like those captured by parent annotation (Johnson, 1998) are more subtle. Parent annotation can capture, for instance, the difference in distribution in NPs that have S as a parent (that is, subjects) and NPs under VPs (objects). We try to capture some of this same intuition by introducing a feature on the length of a span. For instance, VPs embedded in NPs tend to be short, usually as embedded gerund phrases. Because constituents in the treebank can be quite long, we bin our length features into 8 buckets, of 1 Experiments with the Brown clusters (Brown et al., 1992) provided by Turian et al. (2010) in lieu of suffixes were not promising. Moreover"
P14-1022,P03-1054,1,0.142646,"other annotations are less clearly beneficial. 5.3 English Evaluation Finally, Table 3 shows our final evaluation on Section 23 of the Penn Treebank. We use the v = 1, h = 0 grammar. While we do not do as well as the Berkeley parser, we will see in Section 6 that our parser does a substantially better job of generalizing to other languages. 6 Other Languages Historically, many annotation schemes for parsers have required language-specific engineering: for example, lexicalized parsers require a set of head rules and manually-annotated grammars require detailed analysis of the treebank itself (Klein and Manning, 2003). A key strength of a parser that does not rely heavily on an annotated grammar is that it may be more portable to other languages. We show that this is indeed the case: on nine languages, our system is competitive with or better than the Berkeley parser, which is the best single 3 We use v = 0 to indicate no annotation, diverging from the notation in Klein and Manning (2003). 233 Arabic Basque French German Berkeley Berkeley-Rep Our work 78.24 78.70 78.89 69.17 84.33 83.74 79.74 79.68 79.40 81.74 82.74 83.28 Berkeley Berkeley-Tags Our work 79.19 78.66 78.75 70.50 74.74 83.39 80.38 79.76 79.70"
P14-1022,D13-1170,0,0.376465,"s have surface reflexes, our system can still parse accurately with context-free backbones as minimal as Xbar grammars. Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks. On the SPMRL 2013 multilingual constituency parsing shared task (Seddah et al., 2013), our system outperforms the top single parser system of Bj¨orkelund et al. (2013) on a range of languages. In addition, despite being designed for syntactic analysis, our system also achieves stateof-the-art numbers on the structural sentiment task of Socher et al. (2013). Finally, we show that, in both syntactic parsing and sentiment analysis, many broad linguistic trends can be captured via surface features. 1 Introduction Na¨ıve context-free grammars, such as those embodied by standard treebank annotations, do not parse well because their symbols have too little context to constrain their syntactic behavior. For example, to PPs usually attach to verbs and of PPs usually attach to nouns, but a context-free PP 228 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 228–237, c Baltimore, Maryland, USA, June 23-25 2014"
P14-1022,W04-3201,1,0.874709,"s inference. In this work, we instead try to minimize the structural complexity of the grammar by moving as much context as possible onto local surface features. We examine the position that grammars should not propagate any information that is available from surface strings, since a discriminative parser can access that information directly. We therefore begin with a minimal grammar and iteratively augment it with rich input features that do not enrich the context-free backbone. Previous work has also used surface features in their parsers, but the focus has been on machine learning methods (Taskar et al., 2004), latent annotations (Petrov and Klein, 2008a; Petrov and Klein, 2008b), or implementation (Finkel et al., 2008). By contrast, we investigate the extent to which We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure. For example, instead of creating separate grammar symbols to mark the definiteness of an NP, our parser might instead capture the same information from the first word of the NP. Moving context out of the grammar and onto surface features can greatly simplify the stru"
P14-1022,P05-1010,0,0.142664,"Missing"
P14-1022,N03-1033,1,0.0577354,"Linguistics applied to this task, our system generally matches their accuracy overall and is able to outperform it on the overall sentence-level subtask. we need a grammar at all. As a thought experiment, consider a parser with no grammar, which functions by independently classifying each span (i, j) of a sentence as an NP, VP, and so on, or null if that span is a non-constituent. For example, spans that begin with the might tend to be NPs, while spans that end with of might tend to be non-constituents. An independent classification approach is actually very viable for part-of-speech tagging (Toutanova et al., 2003), but is problematic for parsing – if nothing else, parsing comes with a structural requirement that the output be a well-formed, nested tree. Our parser uses a minimal PCFG backbone grammar to ensure a basic level of structural well-formedness, but relies mostly on features of surface spans to drive accuracy. Formally, our model is a CRF where the features factor over anchored rules of a small backbone grammar, as shown in Figure 1. 2 Parsing Model In order to exploit non-independent surface features of the input, we use a discriminative formulation. Our model is a conditional random field (L"
P14-1022,P05-1015,0,0.0881148,"nford Sentiment Treebank which shows the utility of our span features for this task. The presence of “While” under this kind of rule tells us that the sentiment of the constituent to the right dominates the sentiment to the left. 7 Adapting to Sentiment Sentiment Analysis Finally, because the system is, at its core, a classifier of spans, it can be used equally well for tasks that do not normally use parsing algorithms. One example is sentiment analysis. While approaches to sentiment analysis often simply classify the sentence monolithically, treating it as a bag of ngrams (Pang et al., 2002; Pang and Lee, 2005; Wang and Manning, 2012), the recent dataset of Socher et al. (2013) imposes a layer of structure on the problem that we can exploit. They annotate every constituent in a number of training trees with an integer sentiment value from 1 (very negative) to 5 (very positive), opening the door for models such as ours to learn how syntax can structurally affect sentiment.7 Figure 5 shows an example that requires some analysis of sentence structure to correctly understand. The first constituent conveys positive sentiment with never lethargic and the second conveys negative sentiment with hindered, b"
P14-1022,W02-1011,0,0.0141368,"ntence from the Stanford Sentiment Treebank which shows the utility of our span features for this task. The presence of “While” under this kind of rule tells us that the sentiment of the constituent to the right dominates the sentiment to the left. 7 Adapting to Sentiment Sentiment Analysis Finally, because the system is, at its core, a classifier of spans, it can be used equally well for tasks that do not normally use parsing algorithms. One example is sentiment analysis. While approaches to sentiment analysis often simply classify the sentence monolithically, treating it as a bag of ngrams (Pang et al., 2002; Pang and Lee, 2005; Wang and Manning, 2012), the recent dataset of Socher et al. (2013) imposes a layer of structure on the problem that we can exploit. They annotate every constituent in a number of training trees with an integer sentiment value from 1 (very negative) to 5 (very positive), opening the door for models such as ours to learn how syntax can structurally affect sentiment.7 Figure 5 shows an example that requires some analysis of sentence structure to correctly understand. The first constituent conveys positive sentiment with never lethargic and the second conveys negative sentim"
P14-1022,P10-1040,0,0.0551624,"ral contexts like those captured by parent annotation (Johnson, 1998) are more subtle. Parent annotation can capture, for instance, the difference in distribution in NPs that have S as a parent (that is, subjects) and NPs under VPs (objects). We try to capture some of this same intuition by introducing a feature on the length of a span. For instance, VPs embedded in NPs tend to be short, usually as embedded gerund phrases. Because constituents in the treebank can be quite long, we bin our length features into 8 buckets, of 1 Experiments with the Brown clusters (Brown et al., 1992) provided by Turian et al. (2010) in lieu of suffixes were not promising. Moreover, lowering this threshold did not improve performance. 231 PRN VP ( CEO of Enron ) said , “ Too bad , ” (XxX) x,“Xx,” VP → no VBP NNS VP VBP NNS no read messages in his inbox Figure 2: An example showing the utility of span context. The ambiguity about whether read is an adjective or a verb is resolved when we construct a VP and notice that the word proceeding it is unlikely. Figure 4: Computation of span shape features on two examples. Parentheticals, quotes, and other punctuation-heavy, short constituents benefit from being explicitly modeled"
P14-1022,N07-1051,1,0.929923,"elopment set because neither the system nor test set values are publicly available. Berkeley-Tags is a version of the Berkeley parser run by the task organizers where tags are provided to the model, and is the best single parser submitted to the official task. In both cases, we match or outperform the baseline parsers in aggregate and on the majority of individual languages. parser4 for the majority of cases we consider. We evaluate on the constituency treebanks from the Statistical Parsing of Morphologically Rich Languages Shared Task (Seddah et al., 2013). We compare to the Berkeley parser (Petrov and Klein, 2007) as well as two variants. First, we use the “Replaced” system of Bj¨orkelund et al. (2013) (Berkeley-Rep), which is their best single parser.5 The “Replaced” system modifies the Berkeley parser by replacing rare words with morphological descriptors of those words computed using language-specific modules, which have been hand-crafted for individual languages or are trained with additional annotation layers in the treebanks that we do not exploit. Unfortunately, Bj¨orkelund et al. (2013) only report results on the development set for the Berkeley-Rep model; however, the task organizers also use"
P14-1022,P12-2018,0,0.0458306,"ll find that parent annotation is effective and otherwise additional annotation layers are not useful. One structural difference between sentiment analysis and syntactic parsing lies in where the relevant information is present in a span. Syntax is often driven by heads of constituents, which tend to be located at the beginning or the end, whereas sentiment is more likely to depend on modifiers such as adjectives, which are typically present in the middle of spans. Therefore, we augment our existing model with standard sentiment analysis features that look at unigrams and bigrams in the span (Wang and Manning, 2012). Moreover, the Stanford Sentiment Treebank is unique in that each constituent was annotated in isolation, meaning that context never affects sentiment and that every word always has the same tag. We exploit this by adding an additional feature template similar to our span shape feature from Section 4.4 which uses the (deterministic) tag for each word as its descriptor. 2 4 1 While “ Gangs ” is never lethargic , it is hindered by its plot . Figure 5: An example of a sentence from the Stanford Sentiment Treebank which shows the utility of our span features for this task. The presence of “While”"
P14-1022,D08-1091,1,0.84872,"to minimize the structural complexity of the grammar by moving as much context as possible onto local surface features. We examine the position that grammars should not propagate any information that is available from surface strings, since a discriminative parser can access that information directly. We therefore begin with a minimal grammar and iteratively augment it with rich input features that do not enrich the context-free backbone. Previous work has also used surface features in their parsers, but the focus has been on machine learning methods (Taskar et al., 2004), latent annotations (Petrov and Klein, 2008a; Petrov and Klein, 2008b), or implementation (Finkel et al., 2008). By contrast, we investigate the extent to which We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure. For example, instead of creating separate grammar symbols to mark the definiteness of an NP, our parser might instead capture the same information from the first word of the NP. Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser: because so m"
P14-1022,P06-1055,1,0.750388,"Missing"
P14-1022,W13-4917,0,0.0218132,"Missing"
P14-1022,N03-1014,0,\N,Missing
P14-1098,P11-1070,1,0.486021,"Missing"
P14-1098,P06-1038,0,0.0161687,"mmetric (in the sense that Sijk is redundant to Sikj ) and hence the patterns are undirected. Therefore, for each term pair, we first symmetrize the collected Web n-grams and Wikipedia patterns by accumulating the counts of symmetric patterns like rats or squirrels and squirrels or rats.8 4 Related Work In our work, we assume a known term set and do not address the problem of extracting related terms from text. However, a great deal of past work has considered automating this process, typically taking one of two major approaches. The clustering-based approach (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Yamada et al., 2009) discovers relations based on the assumption that similar concepts appear in sim7 One can also add features on the full triple (xi , xj , xk ) but most such features will be sparse. 8 All the patterns and counts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one co"
P14-1098,N12-1051,0,0.0313556,"nal probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words (Cimiano et al., 2005; Cimiano and Staab, 2005; Poon and Domingos, 2010; Fountain and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related previous systems, Kozareva and Hovy (2010) and Navigli et al. (2011), that build full taxonomies from scratch. Both of these systems use a process that starts by finding basic level terms (leaves of the final taxonomy tree, typically) and then using relational patterns (hand-selected ones in the case of Kozareva and Hovy (2010), and ones learned separately by a pairwise classifier on manually annotated co-occurrence patterns for Navigli and Velardi"
P14-1098,N03-1011,0,0.0146092,"ring only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most"
P14-1098,P96-1024,0,0.045358,"o et al., 2007), because multi-root spanning ‘forests’ are not applicable to our task. Also, note that we currently assume one node per term. We are following the task description from previous work where the goal is to create a taxonomy for a specific domain (e.g., animals). Within a specific domain, terms typically just have a single sense. However, our algorithms could certainly be adapted to the case of multiple term senses (by treating the different senses as unique nodes in the tree) in future work. 5 The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). 1044 ties or syntactic word classes, which are primary drivers for dependency parsing, are mostly uninformative for taxonomy induction. Instead, inducing taxonomies requires world knowledge to capture the semantic relations between various unseen terms. For this, we use semantic cues to hypernymy and siblinghood via features on simple surface patterns and statistics in large text corpora. We fire features on both the edge and the sibling factors. We first describe all the edge features in detail (Section 3.1 and Section 3.2), and th"
P14-1098,C92-2082,0,0.253921,"gest common substring of xi and xj , and create indicator features for rounded-off and binned values of |LCS|/((|xi |+ |xj |)/2). Length difference: We compute the signed length difference between xj and xi , and create indicator features for rounded-off and binned values of (|xj |− |xi |)/((|xi |+ |xj |)/2). Yang and Callan (2009) use a similar feature. 3.2 3.2.1 Semantic Features Web n-gram Features Patterns and counts: Hypernymy for a term pair (P=xi , C=xj ) is often signaled by the presence of surface patterns like C is a P, P such as C in large text corpora, an observation going back to Hearst (1992). For each potential parent-child edge (P=xi , C=xj ), we mine the top k strings (based on count) in which both xi and xj occur (we use k=200). We collect patterns in both directions, which allows us to judge the correct direction of an edge (e.g., C is a P is a positive signal for hypernymy whereas P is a C is a negative signal).6 Next, for each pattern in this top-k list, we compute its normalized pattern count c, and fire an indicator feature on the tuple (pattern, t), for all thresholds t (in a fixed set) s.t. c ≥ t. Our supervised model then automatically learns which patterns are good in"
P14-1098,D09-1099,0,0.0559525,"et’s vertebrates taxonomy. Introduction Many tasks in natural language understanding, such as question answering, information extraction, and textual entailment, benefit from lexical semantic information in the form of types and hypernyms. A recent example is IBM’s Jeopardy! system Watson (Ferrucci et al., 2010), which used type information to restrict the set of answer candidates. Information of this sort is present in term taxonomies (e.g., Figure 1), ontologies, and thesauri. However, currently available taxonomies such as WordNet are incomplete in coverage (Pennacchiotti and Pantel, 2006; Hovy et al., 2009), unavailable in many domains and languages, and time-intensive to create or extend manually. There has thus been considerable interest in building lexical taxonomies automatically. In this work, we focus on the task of taking collections of terms as input and predicting a complete taxonomy structure over them as output. Our model takes a loglinear form and is represented using a factor graph that includes both 1st-order scoring factors on directed hypernymy edges (a parent and child in the taxonomy) and 2nd-order scoring factors on sibling edge pairs (pairs of hypernym edges with a shared par"
P14-1098,D07-1015,0,0.0118216,". Hence, at decoding time, we instead start out by once more using belief propagation to find marginal beliefs, and then set the score of each edge to be its belief odds ratio: 3 bYij (ON) 5 bYij (OFF) . Features While spanning trees are familiar from nonprojective dependency parsing, features based on the linear order of the words or on lexical identi4 See Georgiadis (2003) for a detailed algorithmic proof, and McDonald et al. (2005) for an illustrative example. Also, we constrain the Chu-Liu-Edmonds MST algorithm to output only single-root MSTs, where the (dummy) root has exactly one child (Koo et al., 2007), because multi-root spanning ‘forests’ are not applicable to our task. Also, note that we currently assume one node per term. We are following the task description from previous work where the goal is to create a taxonomy for a specific domain (e.g., animals). Within a specific domain, terms typically just have a single sense. However, our algorithms could certainly be adapted to the case of multiple term senses (by treating the different senses as unique nodes in the tree) in future work. 5 The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996)"
P14-1098,D10-1108,0,0.852751,"signal of the link rodent → rat. Moreover, sibling or coordination cues like either rats or squirrels suggest that rat is a sibling of squirrel and adds evidence for the links rodent → rat and rodent → squirrel. Our supervised model captures exactly these types of intuitions by automatically discovering such heterogeneous relational patterns as features (and learning their weights) on edges and on sibling edge pairs, respectively. There have been several previous studies on taxonomy induction. e.g., the incremental taxonomy induction system of Snow et al. (2006), the longest path approach of Kozareva and Hovy (2010), and the maximum spanning tree (MST) approach of Navigli et al. (2011) (see Section 4 for a more detailed overview). The main contribution of this work is that we present the first discriminatively trained, structured probabilistic model over the full space of taxonomy trees, using a structured inference procedure through both the learning and decoding phases. Our model is also the first to directly learn relational patterns as part of the process of training an end-to-end taxonomic induction system, rather than using patterns that were hand-selected or learned via pairwise classifiers on man"
P14-1098,P08-1119,0,0.0317587,"ts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction"
P14-1098,D12-1093,0,0.0195349,"et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words (Cimiano et al., 2005; Cimiano and Staab, 2005; Poon and Domingos, 2010; Fountain and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related"
P14-1098,C02-1144,0,0.0345728,"sibling factors are symmetric (in the sense that Sijk is redundant to Sikj ) and hence the patterns are undirected. Therefore, for each term pair, we first symmetrize the collected Web n-grams and Wikipedia patterns by accumulating the counts of symmetric patterns like rats or squirrels and squirrels or rats.8 4 Related Work In our work, we assume a known term set and do not address the problem of extracting related terms from text. However, a great deal of past work has considered automating this process, typically taking one of two major approaches. The clustering-based approach (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Yamada et al., 2009) discovers relations based on the assumption that similar concepts appear in sim7 One can also add features on the full triple (xi , xj , xk ) but most such features will be sparse. 8 All the patterns and counts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our e"
P14-1098,P98-2127,0,0.0361892,"s that the sibling factors are symmetric (in the sense that Sijk is redundant to Sikj ) and hence the patterns are undirected. Therefore, for each term pair, we first symmetrize the collected Web n-grams and Wikipedia patterns by accumulating the counts of symmetric patterns like rats or squirrels and squirrels or rats.8 4 Related Work In our work, we assume a known term set and do not address the problem of extracting related terms from text. However, a great deal of past work has considered automating this process, typically taking one of two major approaches. The clustering-based approach (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Yamada et al., 2009) discovers relations based on the assumption that similar concepts appear in sim7 One can also add features on the full triple (xi , xj , xk ) but most such features will be sparse. 8 All the patterns and counts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the trainin"
P14-1098,H05-1066,0,0.0473065,"Missing"
P14-1098,P10-1134,0,0.0105973,"in and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related previous systems, Kozareva and Hovy (2010) and Navigli et al. (2011), that build full taxonomies from scratch. Both of these systems use a process that starts by finding basic level terms (leaves of the final taxonomy tree, typically) and then using relational patterns (hand-selected ones in the case of Kozareva and Hovy (2010), and ones learned separately by a pairwise classifier on manually annotated co-occurrence patterns for Navigli and Velardi (2010), Navigli et al. (2011)) to find intermediate terms and all the attested hypernymy links between them.10 To prune down the resulting tax9 Determining the set of input terms is orthogonal to our work, and our method can be used in conjunction with various term extraction approaches described above. 10 Unlike our system, which assumes a complete set of terms and only attempts to induce the taxonomic structure, 1046 onomy graph, Kozareva and Hovy (2010) use a procedure that iteratively retains the longest paths between root and leaf terms, removing conflicting graph edges as they go. The end resu"
P14-1098,P06-1015,0,0.01721,"fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors."
P14-1098,P06-1100,0,0.0219366,"1 Figure 1: An excerpt of WordNet’s vertebrates taxonomy. Introduction Many tasks in natural language understanding, such as question answering, information extraction, and textual entailment, benefit from lexical semantic information in the form of types and hypernyms. A recent example is IBM’s Jeopardy! system Watson (Ferrucci et al., 2010), which used type information to restrict the set of answer candidates. Information of this sort is present in term taxonomies (e.g., Figure 1), ontologies, and thesauri. However, currently available taxonomies such as WordNet are incomplete in coverage (Pennacchiotti and Pantel, 2006; Hovy et al., 2009), unavailable in many domains and languages, and time-intensive to create or extend manually. There has thus been considerable interest in building lexical taxonomies automatically. In this work, we focus on the task of taking collections of terms as input and predicting a complete taxonomy structure over them as output. Our model takes a loglinear form and is represented using a factor graph that includes both 1st-order scoring factors on directed hypernymy edges (a parent and child in the taxonomy) and 2nd-order scoring factors on sibling edge pairs (pairs of hypernym edg"
P14-1098,W02-1017,0,0.0512144,"full triple (xi , xj , xk ) but most such features will be sparse. 8 All the patterns and counts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzio"
P14-1098,P10-1031,0,0.0146464,"y maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words (Cimiano et al., 2005; Cimiano and Staab, 2005; Poon and Domingos, 2010; Fountain and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related previous systems, Kozareva and Hovy (2010) and Navigli et al. (2011), that build full taxonomies from scratch. Both of these systems use a process that starts by finding basic level terms (leaves of the final taxonomy tree, typically) and then using relational patterns (hand-selected ones in the case of Kozareva and Hovy (2010), and ones learned separately by a pairwise classifier on manually annotated co-occurrence patte"
P14-1098,W97-0313,0,0.0581972,"otstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster t"
P14-1098,D08-1016,0,0.0246291,"we wish to accomplish: computing expected feature counts and selecting a particular taxonomy tree for a given set of input terms (decoding). As an initial step to each of these procedures, we wish to compute the marginal probabilities of particular edges (and pairs of edges) being on. In a factor graph, the natural inference procedure for computing marginals is belief propagation. Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008). Therefore, we will only briefly sketch the procedure here. Belief propagation is a general-purpose inference method that computes marginals via directed messages passed from variables to adjacent factors (and vice versa) in the factor graph. These messages take the form of (possibly unnormalized) distributions over values of the variable. The two types of messages (variable to factor or factor to variable) have mutually recursive definitions. The message from a factor F to an adjacent variable V involves a sum over all possible values of every other variable that F touches. While the E DGE a"
P14-1098,P06-1101,0,0.389767,"like rat is a rodent in large corpora is a strong signal of the link rodent → rat. Moreover, sibling or coordination cues like either rats or squirrels suggest that rat is a sibling of squirrel and adds evidence for the links rodent → rat and rodent → squirrel. Our supervised model captures exactly these types of intuitions by automatically discovering such heterogeneous relational patterns as features (and learning their weights) on edges and on sibling edge pairs, respectively. There have been several previous studies on taxonomy induction. e.g., the incremental taxonomy induction system of Snow et al. (2006), the longest path approach of Kozareva and Hovy (2010), and the maximum spanning tree (MST) approach of Navigli et al. (2011) (see Section 4 for a more detailed overview). The main contribution of this work is that we present the first discriminatively trained, structured probabilistic model over the full space of taxonomy trees, using a structured inference procedure through both the learning and decoding phases. Our model is also the first to directly learn relational patterns as part of the process of training an end-to-end taxonomic induction system, rather than using patterns that were h"
P14-1098,D08-1061,0,0.0139227,"Missing"
P14-1098,N03-1036,0,0.0418323,"relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task o"
P14-1098,D09-1097,0,0.0196102,"Missing"
P14-1098,P09-1031,0,0.555984,"or not. This captures pairs such as (fish, bony fish) in our data. Contains: Checks if xj contains xi , or not. This captures pairs such as (bird, bird of prey). Suffix match: Checks whether the k-length suffixes of xi and xj match, or not, for k = 1, 2, . . . , 7. LCS: We compute the longest common substring of xi and xj , and create indicator features for rounded-off and binned values of |LCS|/((|xi |+ |xj |)/2). Length difference: We compute the signed length difference between xj and xi , and create indicator features for rounded-off and binned values of (|xj |− |xi |)/((|xi |+ |xj |)/2). Yang and Callan (2009) use a similar feature. 3.2 3.2.1 Semantic Features Web n-gram Features Patterns and counts: Hypernymy for a term pair (P=xi , C=xj ) is often signaled by the presence of surface patterns like C is a P, P such as C in large text corpora, an observation going back to Hearst (1992). For each potential parent-child edge (P=xi , C=xj ), we mine the top k strings (based on count) in which both xi and xj occur (we use k=200). We collect patterns in both directions, which allows us to judge the correct direction of an edge (e.g., C is a P is a positive signal for hypernymy whereas P is a C is a negat"
P14-1098,C98-2122,0,\N,Missing
P14-2020,P13-1021,1,0.803406,"that generates vi conditioned on the previous vertical offset vi−1 (labeled Slow-vary) and an extension that generates a sequence of font styles fi (labeled Italic). with a forward-cost-augmented beaming scheme. Our method is over 25x faster on a typical document, yet actually yields improved transcriptions. 2 system they are independent. Finally, the pixels in the ith glyph bounding box XiGLYPH are generated conditioned on the character ei , width gi , and vertical offset vi , and the pixels in the ith pad bounding box XiPAD are generated conditioned on the width pi . We refer the reader to Berg-Kirkpatrick et al. (2013) for the details of the pixel generation process. We have omitted the token-level inking random variables for the purpose of brevity. These can be treated as part of the pixel generation process. Let X denote the matrix of pixels for the entire line, V = (v1 , . . . , vn ), P = (p1 , . . . , pn ), and G = (g1 , . . . , gn ). The joint distribution is written: Model We first describe the generative model used by the ‘Ocular’ historical OCR system of BergKirkpatrick et al. (2013)1 and then describe our extensions. The graphical model corresponding to their basic generative process for a single l"
P14-2020,koen-2004-pharaoh,0,0.0761304,"pus (Graff et al., 2007).3 tive, but slow. For example, while transcribing a typical document consisting of 30 lines of text, their system spends 63 minutes computing expected sufficient statistics and decoding when run on a 4.5GHz 4-core CPU. We instead use hard counts of the sufficient statistics for learning (i.e. perform hard-EM). As a result, we are free to use inference procedures that are specialized for Viterbi computation. Specifically, we use beam-search with estimated forward costs. Because the model is semi-Markov, our beam-search procedure is very similar the one used by Pharaoh (Koehn, 2004) for phrasebased machine translation, only without a distortion model. We use a beam of size 20, and estimate forward costs using a character bigram language model. On the machine mentioned above, transcribing the same document, our simplified system that uses hard-EM and beam-search spends only 2.4 minutes computing sufficient statistics and decoding. This represents a 26x speedup. 4 4.1 Document Recognition Performance We evaluate predicted transcriptions using both character error rate (CER) and word error rate (WER). CER is the edit distance between the guessed transcription and the gold t"
P14-2020,D08-1012,1,0.893269,"Missing"
P14-2020,P08-1025,0,0.0314965,"that the if vi differs substantially from vi−1 , a large penalty is incurred. As a result, the model should prefer sequences of vi that vary slowly. In experiments, we set σ 2 = 0.05. 2.2 Streamlined Inference Inference in our extended typesetting models is costly because the state space is large; we propose an new inference procedure that is fast and simple. Berg-Kirkpatrick et al. (2013) used EM to learn the font parameters Φ, and therefore required expected sufficient statistics (indicators on (ei , gi , vi ) tuples), which they computed using coarse-tofine inference (Petrov et al., 2008; Zhang and Gildea, 2008) with a semi-Markov dynamic program (Levinson, 1986). This approach is effecItalic Font Styles Many of the documents in the Old Bailey corpus contain both italic and non-italic font styles (Shoemaker, 2005). The way that italic fonts are used depends on the year the document was printed, but generally italics are reserved for proper nouns, 120 Learned typesetting: Document image: Figure 3: This first line depicts the Viterbi typesetting layout predicted by the O CULAR -B EAM -IT model. Pad boxes are shown in blue. Glyphs boxes are shown in white and display the Bernoulli template probabilities"
P14-2133,N07-1051,1,0.646584,"artof-speech tagging (Sch¨utze, 1995), and chunking 822 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 822–827, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics which shows embeddings for a variety of English determiners, projected onto their first two principal components. We can see that the quantifiers each and every cluster together, as do few and most. These are precisely the kinds of distinctions between determiners that state-splitting in the Berkeley parser has shown to be useful (Petrov and Klein, 2007), and existing work (Mikolov et al., 2013b) has observed that such regular embedding structure extends to many other parts of speech. But we don’t know how prevalent or important such “syntactic axes” are in practice. Thus we have two questions: Are such groupings (learned on large data sets but from less syntactically rich models) better than the ones the parser finds on its own? How much data is needed to learn them without word embeddings? We consider three general hypotheses about how embeddings might interact with a parser: 0.8 few most 0.6 0.4 0.2 that the 0 a each this every −0.2 −0.4 0"
P14-2133,W13-3211,1,0.825458,"Missing"
P14-2133,E95-1020,0,0.213589,"Missing"
P14-2133,P14-2131,0,0.206208,"embeddings encode about syntax? Jacob Andreas and Dan Klein Computer Science Division University of California, Berkeley {jda,klein}@cs.berkeley.edu Abstract (Turian et al., 2010) have been shown to benefit from the inclusion of word embeddings as features. In the other direction, access to a syntactic parse has been shown to be useful for constructing word embeddings for phrases compositionally (Hermann and Blunsom, 2013; Andreas and Ghahramani, 2013). Dependency parsers have seen gains from distributional statistics in the form of discrete word clusters (Koo et al., 2008), and recent work (Bansal et al., 2014) suggests that similar gains can be derived from embeddings like the ones used in this paper. It has been less clear how (and indeed whether) word embeddings in and of themselves are useful for constituency parsing. There certainly exist competitive parsers that internally represent lexical items as real-valued vectors, such as the neural network-based parser of Henderson (2004), and even parsers which use pre-trained word embeddings to represent the lexicon, such as Socher et al. (2013). In these parsers, however, use of word vectors is a structural choice, rather than an added feature, and i"
P14-2133,P13-1045,0,0.020897,"s from distributional statistics in the form of discrete word clusters (Koo et al., 2008), and recent work (Bansal et al., 2014) suggests that similar gains can be derived from embeddings like the ones used in this paper. It has been less clear how (and indeed whether) word embeddings in and of themselves are useful for constituency parsing. There certainly exist competitive parsers that internally represent lexical items as real-valued vectors, such as the neural network-based parser of Henderson (2004), and even parsers which use pre-trained word embeddings to represent the lexicon, such as Socher et al. (2013). In these parsers, however, use of word vectors is a structural choice, rather than an added feature, and it is difficult to disentangle whether vector-space lexicons are actually more powerful than their discrete analogs—perhaps the performance of neural network parsers comes entirely from the model’s extra-lexical syntactic structure. In order to isolate the contribution from word embeddings, it is useful to demonstrate improvement over a parser that already achieves state-of-the-art performance without vector representations. The fundamental question we want to explore is whether embedding"
P14-2133,P10-1040,0,0.329117,"Missing"
P14-2133,W04-3234,0,0.0358537,"tations of lexical items as points in a real vector space—have a long history in natural language processing, going back at least as far as work on latent semantic analysis (LSA) for information retrieval (Deerwester et al., 1990). While word embeddings can be constructed directly from surface distributional statistics, as in LSA, more sophisticated tools for unsupervised extraction of word representations have recently gained popularity (Collobert et al., 2011; Mikolov et al., 2013a). Semi-supervised and unsupervised models for a variety of core NLP tasks, including named-entity recognition (Freitag, 2004), partof-speech tagging (Sch¨utze, 1995), and chunking 822 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 822–827, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics which shows embeddings for a variety of English determiners, projected onto their first two principal components. We can see that the quantifiers each and every cluster together, as do few and most. These are precisely the kinds of distinctions between determiners that state-splitting in the Berkeley parser has shown to be usefu"
P14-2133,P04-1013,0,0.185961,"ompositionally (Hermann and Blunsom, 2013; Andreas and Ghahramani, 2013). Dependency parsers have seen gains from distributional statistics in the form of discrete word clusters (Koo et al., 2008), and recent work (Bansal et al., 2014) suggests that similar gains can be derived from embeddings like the ones used in this paper. It has been less clear how (and indeed whether) word embeddings in and of themselves are useful for constituency parsing. There certainly exist competitive parsers that internally represent lexical items as real-valued vectors, such as the neural network-based parser of Henderson (2004), and even parsers which use pre-trained word embeddings to represent the lexicon, such as Socher et al. (2013). In these parsers, however, use of word vectors is a structural choice, rather than an added feature, and it is difficult to disentangle whether vector-space lexicons are actually more powerful than their discrete analogs—perhaps the performance of neural network parsers comes entirely from the model’s extra-lexical syntactic structure. In order to isolate the contribution from word embeddings, it is useful to demonstrate improvement over a parser that already achieves state-of-the-a"
P14-2133,P13-1088,0,0.0220342,"Missing"
P14-2133,I11-1025,0,0.0686956,"f its weight with a small number of close neighbors, and almost none with words farther away. To exploit this, we pre-compute the k-nearest-neighbor graph of points in the embedding space, and take the sum in Equation 1 only over this set of nearest neighbors. Empirically, taking k = 20 gives adequate performance, and increasing it does not seem to alter the behavior of the parser. As in the OOV model, we also need to worry about how to handle words for which we have no For the experiments in this paper, we will use the Berkeley parser (Petrov and Klein, 2007) and the related Maryland parser (Huang and Harper, 2011). The Berkeley parser induces a latent, statesplit PCFG in which each symbol V of the (observed) X-bar grammar is refined into a set of more specific symbols {V1 , V2 , . . .} which capture more detailed grammatical behavior. This allows the parser to distinguish between words which share the same tag but exhibit very different syntactic behavior—for example, between articles and demonstrative pronouns. The Maryland parser builds on the state-splitting parser, replacing its basic word emission model with a featurerich, log-linear representation of the lexicon. The choice of this parser family"
P14-2133,P08-1068,0,0.345207,"Missing"
P14-2133,N13-1090,0,0.152431,"a variety of ways in which word embeddings might augment a constituency parser with a discrete state space. Word embeddings—representations of lexical items as points in a real vector space—have a long history in natural language processing, going back at least as far as work on latent semantic analysis (LSA) for information retrieval (Deerwester et al., 1990). While word embeddings can be constructed directly from surface distributional statistics, as in LSA, more sophisticated tools for unsupervised extraction of word representations have recently gained popularity (Collobert et al., 2011; Mikolov et al., 2013a). Semi-supervised and unsupervised models for a variety of core NLP tasks, including named-entity recognition (Freitag, 2004), partof-speech tagging (Sch¨utze, 1995), and chunking 822 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 822–827, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics which shows embeddings for a variety of English determiners, projected onto their first two principal components. We can see that the quantifiers each and every cluster together, as do few and most. Thes"
P15-1030,W13-4916,0,0.0441349,"Missing"
P15-1030,W14-6110,0,0.198064,"Missing"
P15-1030,P05-1022,0,0.430767,"Missing"
P15-1030,W08-2102,0,0.0281083,"s in a conventional CRF, with the gradient of the network parameters naturally computed by backpropagating a difference of expected anchored rule counts through the network for each span and split point. Using dense learned features alone, the neural CRF model obtains high performance, outperforming the CRF parser of Hall et al. (2014). When sparse indicators are used in addition, the resulting model gets 91.1 F1 on section 23 of the Penn Treebank, outperforming the parser of Socher et al. (2013) as well as the Berkeley Parser (Petrov and Klein, 2007) and matching the discriminative parser of Carreras et al. (2008). The model also obtains the best single parser results on nine other languages, again outperforming the system of Hall et al. (2014). 2 PP j reflected the flip side of the Stoltzman personality . DT NNP VBZ NP The Fed issued … h r = NP ! NP PP 2.1 Anchored Rules The fundamental units that our parsing models consider are anchored rules. As shown in Figure 2, we define an anchored rule as a tuple (r, s), where r is an indicator of the rule’s identity and s = (i, j, k) indicates the span (i, k) and split point j of the rule.3 A tree T is simply a collection of anchored rules subject to the const"
P15-1030,D14-1082,0,0.690947,"split point. As input, it takes vector representations of words at the split point and span boundaries; it then outputs scores for anchored rules applied to that span and split point. These scores can be thought of as nonlinear potentials analogous to linear potentials in conventional CRFs. Crucially, while the network replicates are connected in a unified model, their computations factor along the same substructures as in standard CRFs. Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al., 2013; Le and Zuidema, 2014); by contrast, our framework permits exact inference via CKY, since the model’s structured interactions are purely discrete and do not involve continuous hidden state. Therefore, we can exploit a neural net’s capacity to learn nonlinear features without modifying This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of lin"
P15-1030,C14-1078,0,0.017442,"ters and a single feature extractor f that jointly inspects the surface and the rule. However, when the feature representation conjoins each rule r with surface properties of the sentence in a systematic way (an assumption that holds in our case as well as for standard CRF models for POS tagging and NER), this is equivalent to our formalism. 5 Embedding words allows us to use standard pre-trained vectors more easily and tying embeddings across word positions substantially reduces the number of model parameters. However, embedding features rather than words has also been shown to be effective (Chen et al., 2014). 304 more discriminating power. Also note that it is possible to use deeper networks or more sophisticated architectures here; we will return to this in Section 6. Our two models can be easily combined: increases the ability to overfit. Following Hall et al. (2014), we use grammars with very little annotation: we use no horizontal Markovization for any of experiments, and all of our English experiments with the neural CRF use no vertical Markovization (V = 0). This also has the benefit of making the system much faster, due to the smaller state space for dynamic programming. We do find that us"
P15-1030,P14-2133,1,0.856453,"continuous word representations themselves, we also experimented with vectors trained on just the text from the training set of the Penn Treebank using the skip-gram model with a window size of 1. While these vectors are somewhat lower performing on their own (f), they still provide a surprising and noticeable gain when stacked on top of sparse features (h), again suggesting that dense and sparse representations have complementary strengths. This result also reinforces the notion that the utility of word vectors does not come primarily from importing information about out-of-vocabulary words (Andreas and Klein, 2014). Since the neural features incorporate information from unlabeled data, we should provide the 6 Design Choices The neural net design space is large, so we wish to analyze the particular design choices we made for this system by examining the performance of several variants of the neural net architecture used in our system. Table 2 shows development results from potential alternate architectural choices, which we now discuss. Choice of nonlinearity The choice of nonlinearity g has been frequently discussed in the neural network literature. Our choice g(x) = max(x, 0), a rectified linear unit,"
P15-1030,P14-2131,0,0.174074,", and split point of the anchored rule (as shown in Figure 2) as well as on two other span properties, span length and span shape (an indicator of where capitalized words, numbers, and punctuation occur in the span). For our neural model, we take fw for all productions (preterminal and nonterminal) to be the words surrounding the beginning and end of a span and the split point, as shown in Figure 2; in particular, we look two words in either direction around each point of interest, meaning the neural net takes 12 words as input.7 For our word embeddings v, we use pre-trained word vectors from Bansal et al. (2014). We compare with other sources of word vectors in Section 5. Contrary to standard practice, we do not update these vectors during training; we found that doing so did not provide an accuracy benefit and slowed down training considerably. 2.4 Learning L(H, W ) = D X log P (Ti∗ |wi ; H, W ) i=1 Because we are using rectified linear units as our nonlinearity, our objective is not everywhere differentiable. The interaction of the parameters and the nonlinearity also makes the objective nonconvex. However, in spite of this, we can still follow subgradients to optimize this objective, as is standar"
P15-1030,Q14-1043,0,0.0115935,"tures, our system1 achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages. 1 Introduction Neural network-based approaches to structured NLP tasks have both strengths and weaknesses when compared to more conventional models, such conditional random fields (CRFs). A key strength of neural approaches is their ability to learn nonlinear interactions between underlying features. In the case of unstructured output spaces, this capability has led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchbrenner et al., 2014; Kim, 2014). Neural methods are also powerful tools in the case of structured 1 System available at http://nlp.cs.berkeley.edu 302 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 302–312, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics NP S NP NP VP i fo W fw v(fw ) Structured inference (discrete) fw k reflected the side of personality . v(fw ) Feature extraction (continuous) fs [[PreviousWor"
P15-1030,P08-1109,0,0.133233,"Missing"
P15-1030,N07-1051,1,0.899028,"arse model. Our model can be trained by gradient descent exactly as in a conventional CRF, with the gradient of the network parameters naturally computed by backpropagating a difference of expected anchored rule counts through the network for each span and split point. Using dense learned features alone, the neural CRF model obtains high performance, outperforming the CRF parser of Hall et al. (2014). When sparse indicators are used in addition, the resulting model gets 91.1 F1 on section 23 of the Penn Treebank, outperforming the parser of Socher et al. (2013) as well as the Berkeley Parser (Petrov and Klein, 2007) and matching the discriminative parser of Carreras et al. (2008). The model also obtains the best single parser results on nine other languages, again outperforming the system of Hall et al. (2014). 2 PP j reflected the flip side of the Stoltzman personality . DT NNP VBZ NP The Fed issued … h r = NP ! NP PP 2.1 Anchored Rules The fundamental units that our parsing models consider are anchored rules. As shown in Figure 2, we define an anchored rule as a tuple (r, s), where r is an indicator of the rule’s identity and s = (i, j, k) indicates the span (i, k) and split point j of the rule.3 A tre"
P15-1030,P14-1022,1,0.115909,"zation of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system1 achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages. 1 Introduction Neural network-based approaches to structured NLP tasks have both strengths and weaknesses when compared to more conventional models, such conditional random fields (CRFs). A key strength of neural approaches is their ability to learn nonlinear interactions between underlying features. In the case of unstructured output spaces, this capability has led to gains in problems ranging from"
P15-1030,N03-1014,0,0.25862,"Missing"
P15-1030,D08-1091,1,0.843811,"kes the standard form of log-linear models:   X ∂L = h(w, s; H)fo (r)>  − ∂W (r,s)∈T ∗   X X  P (T |w; H, W ) h(w, s; H)fo (r)>  T (r,s)∈T Note that the outer products give matrices of feature counts isomorphic to W . The second expression can be simplified to be in terms of expected feature counts. To update H, we use standard backpropagation by first computing:   X ∂L  = W fo (r) − ∂h ∗ (r,s)∈T   X X  P (T |w; H, W ) W fo (r) Grammar Refinements A recurring issue in discriminative constituency parsing is the granularity of annotation in the base grammar (Finkel et al., 2008; Petrov and Klein, 2008; Hall et al., 2014). Using finer-grained symbols in our rules r gives the model greater capacity, but also introduces more parameters into W and 6 The model actually uses the longest suffix of each word occurring at least 100 times in the training set, up to the entire word. Removing this abstraction of rare words harms performance. 7 The sparse model did not benefit from using this larger neighborhood, so improvements from the neural net are not simply due to considering more lexical context. T (r,s)∈T Since h is the output of the neural network, we can then apply the chain rule to compute g"
P15-1030,N10-1003,0,0.0196019,"unlabeled data. We further compare to the shiftreduce parser of Zhu et al. (2013), which uses unlabeled data in the form of Brown clusters. Our method achieves performance close to that of their parser. We also compare to the compositional vector grammar (CVG) parser of Socher et al. (2013) as well as the LSTM-based parser of Vinyals et al. (2014). The conditions these parsers are operating under are slightly different: the former is a reranker on top of the Stanford Parser (Klein and Manning, 2003) and the latter trains on much larger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010). Regardless, we outperform the CVG parser as well as the single parser results from Vinyals et al. (2014). F1 all Single model, PTB only Hall et al. (2014) Berkeley Carreras et al. (2008) Shindo et al. (2012) single 89.2 90.1 91.1 91.1 Single model, PTB + vectors/clusters Zhu et al. (2013) This work* 91.3 91.1 Extended conditions Charniak and Johnson (2005) Socher et al. (2013) Vinyals et al. (2014) single Vinyals et al. (2014) ensemble Shindo et al. (2012) ensemble 91.5 90.4 90.5 91.6 92.4 Table 4: Test results on section 23 of the Penn Treebank. We compare to several categories of parsers f"
P15-1030,D14-1080,0,0.0135229,"Missing"
P15-1030,W13-4917,0,0.0183787,"Missing"
P15-1030,P14-1062,0,0.0135426,"n 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages. 1 Introduction Neural network-based approaches to structured NLP tasks have both strengths and weaknesses when compared to more conventional models, such conditional random fields (CRFs). A key strength of neural approaches is their ability to learn nonlinear interactions between underlying features. In the case of unstructured output spaces, this capability has led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchbrenner et al., 2014; Kim, 2014). Neural methods are also powerful tools in the case of structured 1 System available at http://nlp.cs.berkeley.edu 302 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 302–312, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics NP S NP NP VP i fo W fw v(fw ) Structured inference (discrete) fw k reflected the side of personality . v(fw ) Feature extraction (continuous) fs [[PreviousWord = reflected]], [[SpanLength = 7]], … Figure 1:"
P15-1030,D14-1181,0,0.0886277,"and more generally outperforms the best prior single parser results on a range of languages. 1 Introduction Neural network-based approaches to structured NLP tasks have both strengths and weaknesses when compared to more conventional models, such conditional random fields (CRFs). A key strength of neural approaches is their ability to learn nonlinear interactions between underlying features. In the case of unstructured output spaces, this capability has led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchbrenner et al., 2014; Kim, 2014). Neural methods are also powerful tools in the case of structured 1 System available at http://nlp.cs.berkeley.edu 302 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 302–312, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics NP S NP NP VP i fo W fw v(fw ) Structured inference (discrete) fw k reflected the side of personality . v(fw ) Feature extraction (continuous) fs [[PreviousWord = reflected]], [[SpanLength = 7]], … Figure 1: Neural CRF"
P15-1030,P03-1054,1,0.0651747,"ly, single parser data condition; we match their performance at 91.1 F1 , though we also use word vectors computed from unlabeled data. We further compare to the shiftreduce parser of Zhu et al. (2013), which uses unlabeled data in the form of Brown clusters. Our method achieves performance close to that of their parser. We also compare to the compositional vector grammar (CVG) parser of Socher et al. (2013) as well as the LSTM-based parser of Vinyals et al. (2014). The conditions these parsers are operating under are slightly different: the former is a reranker on top of the Stanford Parser (Klein and Manning, 2003) and the latter trains on much larger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010). Regardless, we outperform the CVG parser as well as the single parser results from Vinyals et al. (2014). F1 all Single model, PTB only Hall et al. (2014) Berkeley Carreras et al. (2008) Shindo et al. (2012) single 89.2 90.1 91.1 91.1 Single model, PTB + vectors/clusters Zhu et al. (2013) This work* 91.3 91.1 Extended conditions Charniak and Johnson (2005) Socher et al. (2013) Vinyals et al. (2014) single Vinyals et al. (2014) ensemble Shindo et al. (2012) ensemble 91.5 90.4 90.5 91.6"
P15-1030,W14-6111,0,0.0334686,"Missing"
P15-1030,P08-1068,0,0.0446488,"lusters (c), and even word representations learned just on the Penn Treebank are surprisingly effective (f, h). F1 len ≤ 40 Table 2: Exploration of other implementation choices in the feedforward neural network on sentences of length ≤ 40 from section 22 of the Penn Treebank. Rectified linear units perform better than tanh or cubic units, a network with one hidden layer performs best, and embedding the output feature vector gives worse performance. sparse model with similar information for a true apples-to-apples comparison. Brown clusters have been shown to be effective vehicles in the past (Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014). We can incorporate Brown clusters into the baseline CRF model in an analogous way to how embedding features are used in the dense model: surface features are fired on Brown cluster identities (we use prefixes of length 4 and 10) of key words. We use the Brown clusters from Koo et al. (2008), which are trained on the same data as the vectors of Bansal et al. (2014). However, Table 1 shows that these features provide no benefit to the baseline model, which suggests either that it is difficult to learn reliable weights for these as sparse features or t"
P15-1030,P12-1046,0,0.498905,"in the SPMRL 2013/2014 Shared Tasks; all values are F-scores for sentences of all lengths using the version of evalb distributed with the shared task. Our parser substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabb´e and Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the best published numbers on this dataset (Bj¨orkelund et al., 2013; Bj¨orkelund et al., 2014). the single TSG parser of Shindo et al. (2012). To our knowledge, the latter two systems are the highest performing in this PTB-only, single parser data condition; we match their performance at 91.1 F1 , though we also use word vectors computed from unlabeled data. We further compare to the shiftreduce parser of Zhu et al. (2013), which uses unlabeled data in the form of Brown clusters. Our method achieves performance close to that of their parser. We also compare to the compositional vector grammar (CVG) parser of Socher et al. (2013) as well as the LSTM-based parser of Vinyals et al. (2014). The conditions these parsers are operating un"
P15-1030,P13-1045,0,0.749596,"rds at the split point and span boundaries; it then outputs scores for anchored rules applied to that span and split point. These scores can be thought of as nonlinear potentials analogous to linear potentials in conventional CRFs. Crucially, while the network replicates are connected in a unified model, their computations factor along the same substructures as in standard CRFs. Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al., 2013; Le and Zuidema, 2014); by contrast, our framework permits exact inference via CKY, since the model’s structured interactions are purely discrete and do not involve continuous hidden state. Therefore, we can exploit a neural net’s capacity to learn nonlinear features without modifying This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use n"
P15-1030,D14-1081,0,0.0210564,"pan boundaries; it then outputs scores for anchored rules applied to that span and split point. These scores can be thought of as nonlinear potentials analogous to linear potentials in conventional CRFs. Crucially, while the network replicates are connected in a unified model, their computations factor along the same substructures as in standard CRFs. Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al., 2013; Le and Zuidema, 2014); by contrast, our framework permits exact inference via CKY, since the model’s structured interactions are purely discrete and do not involve continuous hidden state. Therefore, we can exploit a neural net’s capacity to learn nonlinear features without modifying This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials com"
P15-1030,P14-1130,0,0.0178065,"ing substantially over linear features of the continuous input. We can use the embedding vector of an anchored span v(fw ) directly as input to a basic linear CRF, as shown in Figure 4a. Table 1 shows that the purely linear architecture (0 HL) performs surprisingly well, but is still less effective than the network with one hidden layer. This agrees with the results of Wang and Manning (2013), who noted that dense features typically benefit from nonlinear modeling. We also compare against a two-layer neural network, but find that this also performs worse than the one-layer architecture. work (Lei et al., 2014). This approach saves us from having to learn a separate row of W for every rule in the grammar; if rules are given similar embeddings, then they will behave similarly according to the model. We experimented with noe = 20 and show the results in Table 2. Unfortunately, this approach does not seem to work well for parsing. Learning the output representation was empirically very unstable, and it also required careful initialization. We tried Gaussian initialization (as in the rest of our model) and initializing the model by clustering rules either randomly or according to their parent symbol. Th"
P15-1030,D14-1101,0,0.00492609,"it takes vector representations of words at the split point and span boundaries; it then outputs scores for anchored rules applied to that span and split point. These scores can be thought of as nonlinear potentials analogous to linear potentials in conventional CRFs. Crucially, while the network replicates are connected in a unified model, their computations factor along the same substructures as in standard CRFs. Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al., 2013; Le and Zuidema, 2014); by contrast, our framework permits exact inference via CKY, since the model’s structured interactions are purely discrete and do not involve continuous hidden state. Therefore, we can exploit a neural net’s capacity to learn nonlinear features without modifying This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential f"
P15-1030,P14-2050,0,0.00645301,"hology for predicting tags of unknown words, which typically have regular inflection patterns. By contrast, the neural model must rely on the geometry of the vector space exposing useful regularities. At the same time, the strong performance of the combination of the two systems (g) indicates that not only are both featurization approaches highperforming on their own, but that they have complementary strengths. Unlabeled data Much attention has been paid to the choice of word vectors for various NLP tasks, notably whether they capture more syntactic or semantic phenomena (Bansal et al., 2014; Levy and Goldberg, 2014). We primarily use vectors from Bansal et al. (2014), who train the skipgram model of Mikolov et al. (2013) using contexts from dependency links; a similar approach was also suggested by Levy and Goldberg (2014). 8 One reason we did not choose to include the rule identity fo as an input to the network is that it requires computing an even larger number of network activations, since we cannot reuse them across rules over the same span and split point. 306 Sparse Neural V a b c Hall et al. (2014), V = 1 90.5 X X X 89.22 90.13 90.17 d e f g h Word Reps F1 len ≤ 40 F1 all X X 0 1 1 Brown 89.89 90."
P15-1030,P10-1040,0,0.0103985,"ven word representations learned just on the Penn Treebank are surprisingly effective (f, h). F1 len ≤ 40 Table 2: Exploration of other implementation choices in the feedforward neural network on sentences of length ≤ 40 from section 22 of the Penn Treebank. Rectified linear units perform better than tanh or cubic units, a network with one hidden layer performs best, and embedding the output feature vector gives worse performance. sparse model with similar information for a true apples-to-apples comparison. Brown clusters have been shown to be effective vehicles in the past (Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014). We can incorporate Brown clusters into the baseline CRF model in an analogous way to how embedding features are used in the dense model: surface features are fired on Brown cluster identities (we use prefixes of length 4 and 10) of key words. We use the Brown clusters from Koo et al. (2008), which are trained on the same data as the vectors of Bansal et al. (2014). However, Table 1 shows that these features provide no benefit to the baseline model, which suggests either that it is difficult to learn reliable weights for these as sparse features or that different regular"
P15-1030,J93-2004,0,0.0507879,"important. 4 expensive. However, because only a small number of rules can apply to a given span and split point, fo is sparse and we can selectively compute the terms necessary for the final bilinear product. Our combined sparse and neural model trains on the Penn Treebank in 24 hours on a single machine with a parallelized CPU implementation. For reference, the purely sparse model with a parentannotated grammar (necessary for the best results) takes around 15 hours on the same machine. 5 System Ablations Table 1 shows results on section 22 (the development set) of the English Penn Treebank (Marcus et al., 1993), computed using evalb. Full test results and comparisons to other systems are shown in Table 4. We compare variants of our system along two axes: whether they use standard linear sparse features, nonlinear dense features from the neural net, or both, and whether any word representations (vectors or clusters) are used. Inference Our baseline and neural model both score anchored rule productions. We can use CKY in the standard fashion to compute either expected anchored rule counts EP (T |w) [(r, s)] or the Viterbi tree arg maxT P (T |w). We speed up inference by using a coarse pruning pass. We"
P15-1030,I13-1183,0,0.114938,"RFs that decompose over anchored rule productions and place a probability distribution over trees conditioned on a sentence w as follows:   X P (T |w) ∝ exp  φ(w, r, s) (r,s)∈T 2 Throughout this work, we will primarily consider two potential functions: linear functions of sparse indicators and nonlinear neural networks over dense, continuous features. Although other modeling choices are possible, these two points in the design space reflect common choices in NLP, and past work has suggested that nonlinear functions of indicators or linear functions of dense features may perform less well (Wang and Manning, 2013). 3 For simplicity of exposition, we ignore unary rules; however, they are easily supported in this framework by simply specifying a null value for the split point. Model Figure 1 shows our neural CRF model. The model decomposes over anchored rules, and it scores each of these with a potential function; in a standard CRF, these potentials are typically linear functions of sparse indicator features, whereas 303 where φ is a scoring function that considers the input sentence and the anchored rule in question. Figure 1 shows this scoring process schematically. As we will see, the module on the le"
P15-1030,P13-1043,0,0.0428738,"Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the best published numbers on this dataset (Bj¨orkelund et al., 2013; Bj¨orkelund et al., 2014). the single TSG parser of Shindo et al. (2012). To our knowledge, the latter two systems are the highest performing in this PTB-only, single parser data condition; we match their performance at 91.1 F1 , though we also use word vectors computed from unlabeled data. We further compare to the shiftreduce parser of Zhu et al. (2013), which uses unlabeled data in the form of Brown clusters. Our method achieves performance close to that of their parser. We also compare to the compositional vector grammar (CVG) parser of Socher et al. (2013) as well as the LSTM-based parser of Vinyals et al. (2014). The conditions these parsers are operating under are slightly different: the former is a reranker on top of the Stanford Parser (Klein and Manning, 2003) and the latter trains on much larger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010). Regardless, we outperform the CVG parser as well as the single pars"
P16-1188,P13-1020,0,0.0223492,"its antecedent. These pronoun rewrites are scored in the objective and introduced into the length constraint to make sure they do not cause our summary to be too long. Finally, constraints on these variables control when they are used and also require the model to include antecedents of pronouns when the model is not confident enough to rewrite them. 2.1 Grammaticality Constraints Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (2011). RST compressions Figure 2a shows how to derive compressions from Rhetorical Structure Theory (Mann and Thompson, 1988; Carlson et al., 2001). We show a sentence broken into"
P16-1188,J08-1001,0,0.0195037,"ROUGE score than either method. These results indicate that our model has the expressive capacity to extract important content, but is sufficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in Figure 1. Broadly, our ILP takes a set of textual units u = (u1 , . . . , un ) from a document and finds the highest-scoring extracti"
P16-1188,N04-1015,0,0.0350411,"bjective and are incorporated into the length constraint. Yoshida et al. (2014) and approaching the clarity of a sentence-extractive baseline—and still achieves substantially higher ROUGE score than either method. These results indicate that our model has the expressive capacity to extract important content, but is sufficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with e"
P16-1188,H01-1065,0,0.162869,"Missing"
P16-1188,P11-1049,1,0.934644,"damental units of extraction in our model. For a sentence-extractive model, these would be entire sentences, but for our compressive models we will have more fine-grained units, as shown in Figure 2 and described in Section 2.1. Textual units are scored according to features f and model parameters w learned on training data. Finally, the extraction process is subject to a length constraint of k words. This approach is similar in spirit to ILP formulations of multi-document summarization systems, though in those systems content is typically modeled in terms of bigrams (Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Hong and Nenkova, 2014; Li et al., 2015). For our model, type-level n-gram scoring only arises when we compute our loss function in maxmargin training (see Section 3). In Section 2.1, we discuss grammaticality constraints, which take the form of introducing dependencies between textual units, as shown in Figure 2. If one textual unit requires another, it cannot be included unless its prerequisite is. We will show that different sets of requirements can capture both syntactic and discourse-based compression schemes. Furthermore, we introduce anaphora constraints (Section 2.2) via a new set of"
P16-1188,W01-1605,0,0.0722609,"Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (2011). RST compressions Figure 2a shows how to derive compressions from Rhetorical Structure Theory (Mann and Thompson, 1988; Carlson et al., 2001). We show a sentence broken into elemen2 The features in our model are actually rich enough to learn a sophisticated compression model, but the data we have (abstractive summaries) does not directly provide examples of correct compressions; past work has gotten around this with multi-task learning (Almeida and Martins, 2013), but we simply treat grammaticality as a constraint from upstream models. tary discourse units (EDUs) with RST relations between them. Units marked as S AME -U NIT must both be kept or both be deleted, but other nodes in the tree structure can be deleted as long as we do n"
P16-1188,N13-1136,0,0.0110846,"ate that our model has the expressive capacity to extract important content, but is sufficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in Figure 1. Broadly, our ILP takes a set of textual units u = (u1 , . . . , un ) from a document and finds the highest-scoring extractive summary by optimizing over variables xUNIT = x1UN"
P16-1188,J10-3005,0,0.0598346,"ll trees that are not contained in other trees that are used in the augmentation process. This is broadly similar to the combined compression scheme in Kikuchi et al. (2014) but we use a different set of constraints that more strictly enforce grammaticality.3 2.2 Anaphora Constraints What kind of cross-sentential coherence do we need to ensure for the kinds of summaries our system produces? Many notions of coherence are useful, including centering theory (Grosz et al., 1995) and lexical cohesion (Nishikawa et al., 2014), but one of the most pressing phenomena to deal with is pronoun anaphora (Clarke and Lapata, 2010). Cases of pronouns being “orphaned” during extraction (their antecedents are deleted) are 3 We also differ from past work in that we do not use crosssentential RST constraints (Hirao et al., 2013; Yoshida et al., 2014). We experimented with these and found no improvement from using them, possibly because we have a featurebased model rather than a heuristic content selection procedure, and possibly because automatic discourse parsers are less good at recovering cross-sentence relations. u1 This hasn’t been Kellogg’s year . it No replacement necessary u2 Kellogg p1 p3 p2 it year The oat-bran cr"
P16-1188,P02-1057,0,0.146913,"Missing"
P16-1188,E14-4040,0,0.0180116,"as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sandhaus, 2008) which contains around 100,000 news articles with abstractive summaries—learning to select important content with lexical features. This corpus has been explored in related contexts (Dunietz and Gillick, 2014; Hong and Nenkova, 2014), but to our knowledge it has not been directly used for singledocument summarization. To increase the expressive capacity of our model we allow more aggressive compression of individual sentences by combining two different formalisms—one syntactic and the other discursive. Additionally, we incorporate a model of anaphora resolution and give our system the ability rewrite pronominal mentions, further increasing expressivity. In order to guide the model, we incorporate (1) constraints from coreference ensuring that critical pronoun references are clear in the final summ"
P16-1188,D13-1203,1,0.744439,"is Kellogg. We explore two types of constraints for dealing with this: rewriting the pronoun explicitly, or constraining the summary to include the pronoun’s antecedent. 2.2.1 Pronoun Replacement One way of dealing with these pronoun reference issues is to explicitly replace the pronoun with what it refers to. This replacement allows us to maintain maximal extraction flexibility, since we 4 We focus on pronoun coreference because it is the most pressing manifestation of this problem and because existing coreference systems perform well on pronouns compared to harder instances of coreference (Durrett and Klein, 2013). 2001 can make an isolated textual unit meaningful even if it contains a pronoun. Figure 3 shows how this process works. We run the Berkeley Entity Resolution System (Durrett and Klein, 2014) and compute posteriors over possible links for the pronoun. If the coreference system is sufficiently confident in its prediction (i.e. maxi pi &gt; α for a specified threshold α &gt; 12 ), we allow ourselves to replace the pronoun with the first mention of the entity corresponding to the pronoun’s most likely antecedent. In Figure 3, if the system correctly determines that Kellogg is the correct antecedent wi"
P16-1188,Q14-1037,1,0.910185,"ment One way of dealing with these pronoun reference issues is to explicitly replace the pronoun with what it refers to. This replacement allows us to maintain maximal extraction flexibility, since we 4 We focus on pronoun coreference because it is the most pressing manifestation of this problem and because existing coreference systems perform well on pronouns compared to harder instances of coreference (Durrett and Klein, 2013). 2001 can make an isolated textual unit meaningful even if it contains a pronoun. Figure 3 shows how this process works. We run the Berkeley Entity Resolution System (Durrett and Klein, 2014) and compute posteriors over possible links for the pronoun. If the coreference system is sufficiently confident in its prediction (i.e. maxi pi &gt; α for a specified threshold α &gt; 12 ), we allow ourselves to replace the pronoun with the first mention of the entity corresponding to the pronoun’s most likely antecedent. In Figure 3, if the system correctly determines that Kellogg is the correct antecedent with high probability, we enable the first replacement shown there, which is used if u2 is included the summary without u1 .5 As shown in the ILP in Figure 1, we instantiate corresponding pronou"
P16-1188,W09-1802,0,0.312645,"n is licensed by compression rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun’s antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system1 outperforms prior work on both ROUGE as well as on human judgments of linguistic quality. 1 Introduction While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Time"
P16-1188,W10-0722,0,0.076163,"hat removing either syntactic or EDU-based compressions decreases ROUGE. ument prefix summary. One reason for this is that many of the articles are longer-form pieces that begin with a relatively content-free lede of several sentences, which should be identifiable with lexicosyntactic indicators as are used in our discriminative model. 4.3 New York Times Results We evaluate our system along two axes: first, on content selection, using ROUGE9 (Lin and Hovy, 2003), and second, on clarity of language and referential structure, using annotators from Amazon Mechanical Turk. We follow the method of Gillick and Liu (2010) for this evaluation and ask Turkers to rate a summary on how grammatical it is using a 10-point Likert scale. Furthermore, we ask how many unclear pronouns references there were in the text. The Turkers do not see the original document or the reference summary, and rate each summary in isolation. Gillick and Liu (2010) showed that for linguistic quality judgments (as opposed to content judgments), Turkers reproduced the ranking of systems according to expert judgments. To speed up preprocessing and training time 9 We use the ROUGE 1.5.5 script with the following command line arguments: -n 2 -"
P16-1188,D13-1158,0,0.0503894,"then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun’s antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system1 outperforms prior work on both ROUGE as well as on human judgments of linguistic quality. 1 Introduction While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sandhaus, 2008) which contains around 100,000 news articles with abstractive summaries—learning to select important content with"
P16-1188,E14-1075,0,0.0534042,"selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sandhaus, 2008) which contains around 100,000 news articles with abstractive summaries—learning to select important content with lexical features. This corpus has been explored in related contexts (Dunietz and Gillick, 2014; Hong and Nenkova, 2014), but to our knowledge it has not been directly used for singledocument summarization. To increase the expressive capacity of our model we allow more aggressive compression of individual sentences by combining two different formalisms—one syntactic and the other discursive. Additionally, we incorporate a model of anaphora resolution and give our system the ability rewrite pronominal mentions, further increasing expressivity. In order to guide the model, we incorporate (1) constraints from coreference ensuring that critical pronoun references are clear in the final summary and (2) constraints f"
P16-1188,P13-1048,0,0.015369,"Missing"
P16-1188,P14-2052,0,0.0168089,": Tcomb = (S ∪ Ssyn (kl) ∪ {(i, k), (l, j)}, πrst ∪ πsyn(kl) ∪ {(i, k) → (l, j), (l, j) → (i, k), (k, l) → (i, k)}) That is, we maintain the existing tree structure except for the EDU (i, j), which is broken into three parts: the outer two depend on each other (is a claims adjuster and . from Figure 2d) and the inner one depends on the others and preserves the tree structure from Tsyn . We augment Trst with all maximal subtrees of Tsyn , i.e. all trees that are not contained in other trees that are used in the augmentation process. This is broadly similar to the combined compression scheme in Kikuchi et al. (2014) but we use a different set of constraints that more strictly enforce grammaticality.3 2.2 Anaphora Constraints What kind of cross-sentential coherence do we need to ensure for the kinds of summaries our system produces? Many notions of coherence are useful, including centering theory (Grosz et al., 1995) and lexical cohesion (Nishikawa et al., 2014), but one of the most pressing phenomena to deal with is pronoun anaphora (Clarke and Lapata, 2010). Cases of pronouns being “orphaned” during extraction (their antecedents are deleted) are 3 We also differ from past work in that we do not use cros"
P16-1188,D15-1032,1,0.437982,"y that in general cannot be produced by our model. Specifically, we take: `(xNGRAM , y) = maxx∗ ∗ ROUGE -1(x , y) − ROUGE -1(xNGRAM , y) i.e. the gap between the hypothesis’s ROUGE score and the oracle ROUGE score achievable under the model (including constraints). Here xNGRAM are indicator variables that track, for each n-gram type in the reference summary, whether that n-gram is present in the system summary. These are the sufficient statistics for computing ROUGE. We train the model via stochastic subgradient descent on the primal form of the structured SVM objective (Ratliff et al., 2007; Kummerfeld et al., 2015). In order to compute the subgradient for a given training example, we need to find the most violated constraint on the given instance through a loss-augmented decode, which for a linear model takes the form arg maxx w&gt; f (x) + `(x, y). To do this decode at training time in the context of our model, we use an extended version of our ILP in Figure 1 that is augmented to explicitly track typelevel n-grams: &quot; Xh max xUNIT ,xREF ,xNGRAM UNIT xi i (w f (ui )) &gt; i  + Xh REF &gt; i xij (w f (rij )) − `(x NGRAM , y) (i,j) subject to all constraints from Figure 1, and xiNGRAM = 1 iff an included textual"
P16-1188,N15-1079,0,0.012553,"ce-extractive model, these would be entire sentences, but for our compressive models we will have more fine-grained units, as shown in Figure 2 and described in Section 2.1. Textual units are scored according to features f and model parameters w learned on training data. Finally, the extraction process is subject to a length constraint of k words. This approach is similar in spirit to ILP formulations of multi-document summarization systems, though in those systems content is typically modeled in terms of bigrams (Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Hong and Nenkova, 2014; Li et al., 2015). For our model, type-level n-gram scoring only arises when we compute our loss function in maxmargin training (see Section 3). In Section 2.1, we discuss grammaticality constraints, which take the form of introducing dependencies between textual units, as shown in Figure 2. If one textual unit requires another, it cannot be included unless its prerequisite is. We will show that different sets of requirements can capture both syntactic and discourse-based compression schemes. Furthermore, we introduce anaphora constraints (Section 2.2) via a new set of variables that capture the process of rew"
P16-1188,P11-1052,0,0.0300998,"ion rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun’s antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system1 outperforms prior work on both ROUGE as well as on human judgments of linguistic quality. 1 Introduction While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sa"
P16-1188,J95-2003,0,0.469362,"Missing"
P16-1188,N03-1020,0,0.193323,"ally significant gains compared to No Anaphoricity and Tree Knapsack (respectively) with p &lt; 0.05 according to a bootstrap resampling test. We also see that removing either syntactic or EDU-based compressions decreases ROUGE. ument prefix summary. One reason for this is that many of the articles are longer-form pieces that begin with a relatively content-free lede of several sentences, which should be identifiable with lexicosyntactic indicators as are used in our discriminative model. 4.3 New York Times Results We evaluate our system along two axes: first, on content selection, using ROUGE9 (Lin and Hovy, 2003), and second, on clarity of language and referential structure, using annotators from Amazon Mechanical Turk. We follow the method of Gillick and Liu (2010) for this evaluation and ask Turkers to rate a summary on how grammatical it is using a 10-point Likert scale. Furthermore, we ask how many unclear pronouns references there were in the text. The Turkers do not see the original document or the reference summary, and rate each summary in isolation. Gillick and Liu (2010) showed that for linguistic quality judgments (as opposed to content judgments), Turkers reproduced the ranking of systems"
P16-1188,P14-1020,1,0.819066,"(Sandhaus, 2008). We also investigate its performance on the RST Discourse Treebank (Carlson et al., 2001), but because this dataset is only 30 documents it provides much less robust estimates of performance.8 Throughout this section, when we decode a document, we set the word budget for our summarizer to be the same as the number of words in the corresponding reference summary, following previous work (Hirao et al., 2013; Yoshida et al., 2014). 4.1 Preprocessing We preprocess all data using the Berkeley Parser (Petrov et al., 2006), specifically the GPUaccelerated version of the parser from Hall et al. (2014), and the Berkeley Entity Resolution System (Durrett and Klein, 2014). For RST discourse analysis, we segment text into EDUs using a semiMarkov CRF trained on the RST treebank with features on boundaries similar to those of Hernault et al. (2010), plus novel features on spans including span length and span identity for short spans. To follow the conditions of Yoshida et al. (2014) as closely as possible, we also build a discourse parser in the style of Hirao et al. (2013), since their parser is not publicly available. Specifically, 8 Tasks like DUC and TAC have focused on multidocument summari"
P16-1188,W03-1101,0,0.0302607,"explicit mentions. That is, xij = 1 if we should rewrite the jth pronoun in the ith unit with its antecedent. These pronoun rewrites are scored in the objective and introduced into the length constraint to make sure they do not cause our summary to be too long. Finally, constraints on these variables control when they are used and also require the model to include antecedents of pronouns when the model is not confident enough to rewrite them. 2.1 Grammaticality Constraints Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (2011). RST compressions Figure 2a shows how to derive compressions fr"
P16-1188,D12-1106,0,0.016443,"thod. These results indicate that our model has the expressive capacity to extract important content, but is sufficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in Figure 1. Broadly, our ILP takes a set of textual units u = (u1 , . . . , un ) from a document and finds the highest-scoring extractive summary by optimizing"
P16-1188,W10-4327,0,0.0273313,"Missing"
P16-1188,W98-1124,0,0.0373498,"Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun’s antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system1 outperforms prior work on both ROUGE as well as on human judgments of linguistic quality. 1 Introduction While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sandhaus, 2008) which contains around 100,000 news articles with abstractive summaries—learning"
P16-1188,W09-1801,0,0.0266717,"entions. That is, xij = 1 if we should rewrite the jth pronoun in the ith unit with its antecedent. These pronoun rewrites are scored in the objective and introduced into the length constraint to make sure they do not cause our summary to be too long. Finally, constraints on these variables control when they are used and also require the model to include antecedents of pronouns when the model is not confident enough to rewrite them. 2.1 Grammaticality Constraints Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (2011). RST compressions Figure 2a shows how to derive compressions from Rhetorical Structure T"
P16-1188,P05-1012,0,0.0434139,"Missing"
P16-1188,E06-1038,0,0.0215508,"d) Process of augmenting a textual unit with syntactic compressions. REF explicit mentions. That is, xij = 1 if we should rewrite the jth pronoun in the ith unit with its antecedent. These pronoun rewrites are scored in the objective and introduced into the length constraint to make sure they do not cause our summary to be too long. Finally, constraints on these variables control when they are used and also require the model to include antecedents of pronouns when the model is not confident enough to rewrite them. 2.1 Grammaticality Constraints Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (20"
P16-1188,R11-1066,0,0.0174782,"nstraint. Yoshida et al. (2014) and approaching the clarity of a sentence-extractive baseline—and still achieves substantially higher ROUGE score than either method. These results indicate that our model has the expressive capacity to extract important content, but is sufficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in"
P16-1188,D15-1182,0,0.0301045,"Missing"
P16-1188,P14-1084,0,0.0402219,"nsure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in Figure 1. Broadly, our ILP takes a set of textual units u = (u1 , . . . , un ) from a document and finds the highest-scoring extractive summary by optimizing over variables xUNIT = x1UNIT , . . . , xnUNIT , which are binary indicators of whether each unit is included. Textual units are contig"
P16-1188,N03-1030,0,0.042861,"Missing"
P16-1188,I13-1198,0,0.0176762,"fficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model’s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning. 2 Model Our model is shown in Figure 1. Broadly, our ILP takes a set of textual units u = (u1 , . . . , un ) from a document and finds the highest-scoring extractive summary by optimizing over variables xUNIT = x1UNIT , . . . , xnUNIT , which are binary indicators of whether each unit is included. Te"
P16-1188,D12-1022,0,0.0100531,"ronoun in the ith unit with its antecedent. These pronoun rewrites are scored in the objective and introduced into the length constraint to make sure they do not cause our summary to be too long. Finally, constraints on these variables control when they are used and also require the model to include antecedents of pronouns when the model is not confident enough to rewrite them. 2.1 Grammaticality Constraints Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (2011). RST compressions Figure 2a shows how to derive compressions from Rhetorical Structure Theory (Mann and Thompson, 1988; Carlson et al., 2001). We"
P16-1188,D14-1196,0,0.0482556,"By training our full system endto-end on a large-scale dataset, we are able to learn a high-capacity structured model of the summarization process, contrasting with past approaches to the single-document task which have typically been heuristic in nature (Daum´e and Marcu, 2002; Hirao et al., 2013). We focus our evaluation on the New York Times Annotated corpus (Sandhaus, 2008). According to ROUGE, our system outperforms a document prefix baseline, a bigram coverage baseline adapted from a strong multi-document system (Gillick and Favre, 2009), and a discourse-informed method from prior work (Yoshida et al., 2014). Imposing discursive and referential constraints improves human judgments of linguistic clarity and referential structure—outperforming the method of 1998 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1998–2008, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics max unit ref x ,x subject to h X ⇥ Extraction Score xunit (w&gt; f (ui )) + i i Grammaticality Constraints (Section 2.1) Anaphora Score &gt; xref ij (w f (rij )) (i,j) i ⇤ Length adjustment for explicit mention Length Constraint X xunit  xunit if ui requires"
P16-1188,C14-1156,0,0.0799128,"reserves the tree structure from Tsyn . We augment Trst with all maximal subtrees of Tsyn , i.e. all trees that are not contained in other trees that are used in the augmentation process. This is broadly similar to the combined compression scheme in Kikuchi et al. (2014) but we use a different set of constraints that more strictly enforce grammaticality.3 2.2 Anaphora Constraints What kind of cross-sentential coherence do we need to ensure for the kinds of summaries our system produces? Many notions of coherence are useful, including centering theory (Grosz et al., 1995) and lexical cohesion (Nishikawa et al., 2014), but one of the most pressing phenomena to deal with is pronoun anaphora (Clarke and Lapata, 2010). Cases of pronouns being “orphaned” during extraction (their antecedents are deleted) are 3 We also differ from past work in that we do not use crosssentential RST constraints (Hirao et al., 2013; Yoshida et al., 2014). We experimented with these and found no improvement from using them, possibly because we have a featurebased model rather than a heuristic content selection procedure, and possibly because automatic discourse parsers are less good at recovering cross-sentence relations. u1 This h"
P16-1188,P08-1054,0,0.0219105,"r work on both ROUGE as well as on human judgments of linguistic quality. 1 Introduction While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat1 Available at http://nlp.cs.berkeley.edu urally occurring corpus—the New York Times Annotated Corpus (Sandhaus, 2008) which contains around 100,000 news articles with abstractive summaries—learning to select important content with lexical features. This corpus has been explored in related contexts (Dunietz and Gillick, 2014; Hong and Nenkova, 2014), but to our knowledge it has not been directly used for singledocument summarization. To increase the expressive capacity of our model"
P16-1188,P06-1055,1,0.129979,"n a roughly 3000-document evaluation set from the New York Times Annotated Corpus (Sandhaus, 2008). We also investigate its performance on the RST Discourse Treebank (Carlson et al., 2001), but because this dataset is only 30 documents it provides much less robust estimates of performance.8 Throughout this section, when we decode a document, we set the word budget for our summarizer to be the same as the number of words in the corresponding reference summary, following previous work (Hirao et al., 2013; Yoshida et al., 2014). 4.1 Preprocessing We preprocess all data using the Berkeley Parser (Petrov et al., 2006), specifically the GPUaccelerated version of the parser from Hall et al. (2014), and the Berkeley Entity Resolution System (Durrett and Klein, 2014). For RST discourse analysis, we segment text into EDUs using a semiMarkov CRF trained on the RST treebank with features on boundaries similar to those of Hernault et al. (2010), plus novel features on spans including span length and span identity for short spans. To follow the conditions of Yoshida et al. (2014) as closely as possible, we also build a discourse parser in the style of Hirao et al. (2013), since their parser is not publicly availabl"
P16-1188,W01-0100,0,\N,Missing
P17-1022,D16-1125,1,0.839691,"nd even communication using natural language messages (Vogel et al., 2013b). All of these approaches employ structured communication schemes with manually engineered messaging protocols; these are, in some sense, automatically interpretable, but at the cost of introducing considerable complexity into both training and inference. Our evaluation in this paper investigates communication strategies that arise in a number of different games, including reference games and an extended-horizon driving game. Communication strategies for reference games were previously explored by Vogel et al. (2013a), Andreas and Klein (2016) and Kazemzadeh et al. (2014), and reference games specifically featuring end-to-end communication protocols by Yu et al. (2016). On the control side, a long line of work considers nonverbal communication strategies in multiagent policies (Dragan and Srinivasa, 2013). Another group of related approaches focuses on the development of more general machinery for interpreting deep models in which messages have no explicit semantics. This includes both visualization techniques (Zeiler and Fergus, 2014; Strobelt et al., 2016), and approaches focused on generating explanations in the form of natural"
P17-1022,W14-4012,0,0.0172249,"Missing"
P17-1022,D14-1086,0,0.0208833,"atural language messages (Vogel et al., 2013b). All of these approaches employ structured communication schemes with manually engineered messaging protocols; these are, in some sense, automatically interpretable, but at the cost of introducing considerable complexity into both training and inference. Our evaluation in this paper investigates communication strategies that arise in a number of different games, including reference games and an extended-horizon driving game. Communication strategies for reference games were previously explored by Vogel et al. (2013a), Andreas and Klein (2016) and Kazemzadeh et al. (2014), and reference games specifically featuring end-to-end communication protocols by Yu et al. (2016). On the control side, a long line of work considers nonverbal communication strategies in multiagent policies (Dragan and Srinivasa, 2013). Another group of related approaches focuses on the development of more general machinery for interpreting deep models in which messages have no explicit semantics. This includes both visualization techniques (Zeiler and Fergus, 2014; Strobelt et al., 2016), and approaches focused on generating explanations in the form of natural language (Hendricks et al., 2"
P17-1022,Q15-1008,0,0.0145022,"ation. Our evaluation considers two kinds of tasks: reference games and navigation games. In a reference game (e.g. Figure 6a), both players observe a pair of candidate referents. A speaker is assigned a target referent; it must communicate this target to a listener, who then performs a choice action corresponding to its belief about the true target. In this paper we consider two variants on the reference game: a simple color-naming task, and a more complex task involving natural images of birds. For examples of human communication strategies for these tasks, we obtain the XKCD color dataset (McMahan and Stone, 2015; Monroe et al., 2016) and the Caltech Birds dataset (Welinder et al., 2010) with accom(b) (a) (c) Figure 6: Tasks used to evaluate the translation model. (a–b) Reference games: both players observe a pair of reference candidates (colors or images); Player a is assigned a target (marked with a star), which player b must guess based on a message from a. (c) Driving game: each car attempts to navigate to its goal (marked with a star). The cars cannot see each other, and must communicate to avoid a collision. panying natural language descriptions (Reed et al., 2016). We use standard train / valid"
P17-1022,D16-1243,0,0.0113975,"siders two kinds of tasks: reference games and navigation games. In a reference game (e.g. Figure 6a), both players observe a pair of candidate referents. A speaker is assigned a target referent; it must communicate this target to a listener, who then performs a choice action corresponding to its belief about the true target. In this paper we consider two variants on the reference game: a simple color-naming task, and a more complex task involving natural images of birds. For examples of human communication strategies for these tasks, we obtain the XKCD color dataset (McMahan and Stone, 2015; Monroe et al., 2016) and the Caltech Birds dataset (Welinder et al., 2010) with accom(b) (a) (c) Figure 6: Tasks used to evaluate the translation model. (a–b) Reference games: both players observe a pair of reference candidates (colors or images); Player a is assigned a target (marked with a star), which player b must guess based on a message from a. (c) Driving game: each car attempts to navigate to its goal (marked with a star). The cars cannot see each other, and must communicate to avoid a collision. panying natural language descriptions (Reed et al., 2016). We use standard train / validation / test splits fo"
P17-1022,P16-1003,0,0.0231119,"sage whose meaning is most similar. The key question is then what form this grounded meaning representation should take. The existing literature suggests two broad approaches: Semantic representation The meaning of a message za is given by its denotations: that is, by the set of world states of which za may be felicitously predicated, given the existing context available to a listener. In probabilistic terms, this says that the meaning of a message za is represented by the distribution p(xa |za , xb ) it induces over speaker states. Examples of this approach include Guerin and Pitt (2001) and Pasupat and Liang (2016). These two approaches can give rise to rather different behaviors. Consider the following example: square few hexagon circle many many The top language (in blue) has a unique name for every kind of shape, while the bottom language (in red) only distinguishes between shapes with few sides and shapes with many sides. Now imagine a simple reference game with the following form: player a is covertly assigned one of these three shapes as a reference target, and communicates that reference to b; b must then pull a lever labeled large or small depending on the size of the target shape. Blue language"
P17-1022,N16-3020,0,0.00784252,"pose to understand neuralese messages by translating them. In this work, we present a simple technique for inducing a dictionary that maps between neuralese message vectors and short natural language strings, given only examples of DCP agents interacting with other agents, and humans interacting with other humans. Natural language already provides a rich set of tools for describing beliefs, observations, and plans—our thesis is that these tools provide a useful complement to the visualization and ablation techniques used in previous work on understanding complex models (Strobelt et al., 2016; Ribeiro et al., 2016). While structurally quite similar to the task of machine translation between pairs of human languages, interpretation of neuralese poses a number of novel challenges. First, there is no natural source of parallel data: there are no bilingual “speakers” of both neuralese and natural language. Second, there may not be a direct correspondence between the strategy employed by humans and DCP agents: even if it were constrained to communicate using natural language, an automated agent might choose to produce a different message from humans in a given state. We tackle both of these challenges by app"
P17-1022,N13-1127,0,0.176177,"tions to learned messages; however, this approach relies on supervised cluster labels and is targeted specifically towards referring expression games. Here we attempt to develop an approach that can handle general multiagent interactions without assuming a prior discrete structure in space of observations. 233 The literature on learning decentralized multiagent policies in general is considerably larger (Bernstein et al., 2002; Dibangoye et al., 2016). This includes work focused on communication in multiagent settings (Roth et al., 2005) and even communication using natural language messages (Vogel et al., 2013b). All of these approaches employ structured communication schemes with manually engineered messaging protocols; these are, in some sense, automatically interpretable, but at the cost of introducing considerable complexity into both training and inference. Our evaluation in this paper investigates communication strategies that arise in a number of different games, including reference games and an extended-horizon driving game. Communication strategies for reference games were previously explored by Vogel et al. (2013a), Andreas and Klein (2016) and Kazemzadeh et al. (2014), and reference game"
P17-1022,P13-2014,0,0.0998908,"tions to learned messages; however, this approach relies on supervised cluster labels and is targeted specifically towards referring expression games. Here we attempt to develop an approach that can handle general multiagent interactions without assuming a prior discrete structure in space of observations. 233 The literature on learning decentralized multiagent policies in general is considerably larger (Bernstein et al., 2002; Dibangoye et al., 2016). This includes work focused on communication in multiagent settings (Roth et al., 2005) and even communication using natural language messages (Vogel et al., 2013b). All of these approaches employ structured communication schemes with manually engineered messaging protocols; these are, in some sense, automatically interpretable, but at the cost of introducing considerable complexity into both training and inference. Our evaluation in this paper investigates communication strategies that arise in a number of different games, including reference games and an extended-horizon driving game. Communication strategies for reference games were previously explored by Vogel et al. (2013a), Andreas and Klein (2016) and Kazemzadeh et al. (2014), and reference game"
P17-1076,Q13-1033,0,0.0155865,", and which easily accommodates both sparse log-linear scoring models (Hall et al., 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016). The best-performing constituency parsers in the last two years have largely been transition-based rather than global; examples include the models of Dyer et al. (2016), Cross and Huang (2016) and Liu and Zhang (2016). The present work takes many of the insights developed in these models (e.g. the recurrent representation of spans (Kiperwasser and Goldberg, 2016), and the use of a dynamic oracle and exploration policy during training (Goldberg and Nivre, 2013)) and extends these insights to span-oriented models, which support a wider range of decoding procedures. Our approach differs from other recent chart-based neural models (e.g. Durrett and Klein (2015)) in the use of a recurrent input representation, structured loss function, and comparatively simple parameterization of the scoring function. In addition to the globally optimal decoding procedures for which these models were designed, and in contrast to the left-to-right decoder typically employed by transition-based models, our model admits an additional greedy top-to-bottom inference procedur"
P17-1076,P08-1109,0,0.319203,"versity of California, Berkeley {mitchell,jda,klein}@cs.berkeley.edu Abstract outputs. However, transition-based models do not admit fast dynamic programs and require careful feature engineering to support exact search-based inference (Thang et al., 2015). Moreover, models with recurrent state require complex training procedures to benefit from anything other than greedy decoding (Wiseman and Rush, 2016). An alternative line of work focuses on chart parsers, which use log-linear or neural scoring potentials to parameterize a tree-structured dynamic program for maximization or marginalization (Finkel et al., 2008; Durrett and Klein, 2015). These models enjoy a number of appealing formal properties, including support for exact inference and structured loss functions. However, previous chart-based approaches have required considerable scaffolding beyond a simple well-formedness potential, e.g. pre-specification of a complete context-free grammar for generating output structures and initial pruning of the output space with a weaker model (Hall et al., 2014). Additionally, we are unaware of any recent chartbased models that achieve results competitive with the best transition-based models. In this work, w"
P17-1076,P04-1013,0,0.0224012,"in the input space, by using rich feature representations to parameterize local potentials that interact with a comparatively unconstrained structured decoder. As noted in the introduction, this class of feature-based tree scoring functions can be implemented with either a linear transition system (Chen and Manning, 2014) or a global decoder (Finkel et al., 2008). Kiperwasser and Goldberg (2016) describe an approach closely related to ours but targeted at dependency formalisms, and which easily accommodates both sparse log-linear scoring models (Hall et al., 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016). The best-performing constituency parsers in the last two years have largely been transition-based rather than global; examples include the models of Dyer et al. (2016), Cross and Huang (2016) and Liu and Zhang (2016). The present work takes many of the insights developed in these models (e.g. the recurrent representation of spans (Kiperwasser and Goldberg, 2016), and the use of a dynamic oracle and exploration policy during training (Goldberg and Nivre, 2013)) and extends these insights to span-oriented models, which support a wider range of decoding procedures. Ou"
P17-1076,D16-1211,0,0.0254732,"ce, by using rich feature representations to parameterize local potentials that interact with a comparatively unconstrained structured decoder. As noted in the introduction, this class of feature-based tree scoring functions can be implemented with either a linear transition system (Chen and Manning, 2014) or a global decoder (Finkel et al., 2008). Kiperwasser and Goldberg (2016) describe an approach closely related to ours but targeted at dependency formalisms, and which easily accommodates both sparse log-linear scoring models (Hall et al., 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016). The best-performing constituency parsers in the last two years have largely been transition-based rather than global; examples include the models of Dyer et al. (2016), Cross and Huang (2016) and Liu and Zhang (2016). The present work takes many of the insights developed in these models (e.g. the recurrent representation of spans (Kiperwasser and Goldberg, 2016), and the use of a dynamic oracle and exploration policy during training (Goldberg and Nivre, 2013)) and extends these insights to span-oriented models, which support a wider range of decoding procedures. Our approach differs from oth"
P17-1076,Q16-1023,0,0.0168907,"e long-distance dependencies and lexical phenomena (Collins, 2003; Klein and Manning, 2003; Petrov and Klein, 2007). By contrast, the approach we have described here continues a recent line of work on direct modeling of correlations in the input space, by using rich feature representations to parameterize local potentials that interact with a comparatively unconstrained structured decoder. As noted in the introduction, this class of feature-based tree scoring functions can be implemented with either a linear transition system (Chen and Manning, 2014) or a global decoder (Finkel et al., 2008). Kiperwasser and Goldberg (2016) describe an approach closely related to ours but targeted at dependency formalisms, and which easily accommodates both sparse log-linear scoring models (Hall et al., 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016). The best-performing constituency parsers in the last two years have largely been transition-based rather than global; examples include the models of Dyer et al. (2016), Cross and Huang (2016) and Liu and Zhang (2016). The present work takes many of the insights developed in these models (e.g. the recurrent representation of spans (Kiperwasser and Goldbe"
P17-1076,W14-6110,0,0.0412147,"Missing"
P17-1076,P03-1054,1,0.0790107,", 824 Final Parsing Results on Penn Treebank Parser LR LP F1 Durrett and Klein (2015) – – 91.1 Vinyals et al. (2015) – – 88.3 Dyer et al. (2016) – – 89.8 Cross and Huang (2016) 90.5 92.1 91.3 Liu and Zhang (2016) 91.3 92.1 91.7 Best Chart Parser 90.63 92.98 91.79 Best Top-Down Parser 90.35 93.23 91.77 7 Related Work Many early successful approaches to constituency parsing focused on rich modeling of correlations in the output space, typically by engineering proabilistic context-free grammars with state spaces enriched to capture long-distance dependencies and lexical phenomena (Collins, 2003; Klein and Manning, 2003; Petrov and Klein, 2007). By contrast, the approach we have described here continues a recent line of work on direct modeling of correlations in the input space, by using rich feature representations to parameterize local potentials that interact with a comparatively unconstrained structured decoder. As noted in the introduction, this class of feature-based tree scoring functions can be implemented with either a linear transition system (Chen and Manning, 2014) or a global decoder (Finkel et al., 2008). Kiperwasser and Goldberg (2016) describe an approach closely related to ours but targeted"
P17-1076,D14-1082,0,0.248614,"paper presents a minimal but surprisingly effective span-based neural model for constituency parsing. Recent years have seen a great deal of interest in parsing architectures that make use of recurrent neural network (RNN) representations of input sentences (Vinyals et al., 2015). Despite evidence that linear RNN decoders are implicitly able to respect some nontrivial well-formedness constraints on structured outputs (Graves, 2013), researchers have consistently found that the best performance is achieved by systems that explicitly require the decoder to generate well-formed tree structures (Chen and Manning, 2014). There are two general approaches to ensuring this structural consistency. The most common is to encode the output as a sequence of operations within a transition system which constructs trees incrementally. This transforms the parsing problem back into a sequence-to-sequence problem, while making it easy to force the decoder to take only actions guaranteed to produce well-formed 818 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 818–827 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org"
P17-1076,J03-4003,0,0.0304987,"vident, however, 824 Final Parsing Results on Penn Treebank Parser LR LP F1 Durrett and Klein (2015) – – 91.1 Vinyals et al. (2015) – – 88.3 Dyer et al. (2016) – – 89.8 Cross and Huang (2016) 90.5 92.1 91.3 Liu and Zhang (2016) 91.3 92.1 91.7 Best Chart Parser 90.63 92.98 91.79 Best Top-Down Parser 90.35 93.23 91.77 7 Related Work Many early successful approaches to constituency parsing focused on rich modeling of correlations in the output space, typically by engineering proabilistic context-free grammars with state spaces enriched to capture long-distance dependencies and lexical phenomena (Collins, 2003; Klein and Manning, 2003; Petrov and Klein, 2007). By contrast, the approach we have described here continues a recent line of work on direct modeling of correlations in the input space, by using rich feature representations to parameterize local potentials that interact with a comparatively unconstrained structured decoder. As noted in the introduction, this class of feature-based tree scoring functions can be implemented with either a linear transition system (Chen and Manning, 2014) or a global decoder (Finkel et al., 2008). Kiperwasser and Goldberg (2016) describe an approach closely rela"
P17-1076,J93-2004,0,0.0615263,"Missing"
P17-1076,D16-1001,0,0.171489,"mple chart-based neural parser based on independent scoring of labels and spans, and show how this model can be adapted to support a greedy topdown decoding procedure. Our goal is to preserve the basic algorithmic properties of span-oriented (rather than transition-oriented) parse representations, while exploring the extent to which neural representational machinery can replace the additional structure required by existing chart parsers. On the Penn Treebank, our approach outperforms a number of recent models for chart-based and transition-based parsing—including the state-ofthe-art models of Cross and Huang (2016) and Liu and Zhang (2016)—achieving an F1 score of 91.79. We additionally obtain a strong F1 score of 82.23 on the French Treebank. In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring"
P17-1076,D16-1137,0,0.022034,"Missing"
P17-1076,N07-1051,1,0.404147,"ts on Penn Treebank Parser LR LP F1 Durrett and Klein (2015) – – 91.1 Vinyals et al. (2015) – – 88.3 Dyer et al. (2016) – – 89.8 Cross and Huang (2016) 90.5 92.1 91.3 Liu and Zhang (2016) 91.3 92.1 91.7 Best Chart Parser 90.63 92.98 91.79 Best Top-Down Parser 90.35 93.23 91.77 7 Related Work Many early successful approaches to constituency parsing focused on rich modeling of correlations in the output space, typically by engineering proabilistic context-free grammars with state spaces enriched to capture long-distance dependencies and lexical phenomena (Collins, 2003; Klein and Manning, 2003; Petrov and Klein, 2007). By contrast, the approach we have described here continues a recent line of work on direct modeling of correlations in the input space, by using rich feature representations to parameterize local potentials that interact with a comparatively unconstrained structured decoder. As noted in the introduction, this class of feature-based tree scoring functions can be implemented with either a linear transition system (Chen and Manning, 2014) or a global decoder (Finkel et al., 2008). Kiperwasser and Goldberg (2016) describe an approach closely related to ours but targeted at dependency formalisms,"
P17-1076,W14-6111,0,0.0196274,"Missing"
P17-1076,P15-1148,0,0.0946059,"Missing"
P17-1076,N03-1033,1,0.16234,"Missing"
P17-1076,P16-1218,0,0.124071,"recurrent neural networks as a starting point, since they have previously been shown to capture contextual information suitable for use in a variety of natural language applications (Bahdanau et al., 2014; Wang et al., 2015) In particular, we run a bidirectional LSTM over the input to obtain context-sensitive forward and backward encodings for each position i, denoted by fi and bi , respectively. Our representation of the span (i, j) is then the concatenatation the vector differences fj − fi and bi − bj . This corresponds to a bidirectional version of the LSTMMinus features first proposed by Wang and Chang (2016). On top of this base, our label and span scoring functions are implemented as one-layer feedforward networks, taking as input the concatenated span difference and producing as output either a vector of label scores or a single span score. More formally, letting sij denote the vector representation of span (i, j), we define 3 Chart Parsing Our basic model is compatible with traditional chart-based dynamic programming. Representing a constituency tree T by its labeled spans, T := {(`t , (it , jt )) : t = 1, . . . , |T |}, we define the score of a tree to be the sum of its constituent label and"
P17-1105,J84-2007,0,0.84649,"Missing"
P17-1105,D14-1082,0,0.0695921,"Missing"
P17-1105,D16-1001,0,0.0328479,"Missing"
P17-1105,P16-1004,0,0.639209,"( &quot;Dire Wolf Alpha&quot;, 2, CHARACTER_CLASS.ALL, CARD_RARITY.COMMON, minion_type=MINION_TYPE.BEAST) def create_minion(self, player): return Minion(2, 2, auras=[ Aura(ChangeAttack(1), MinionSelector(Adjacent())) ]) Figure 1: Example code for the “Dire Wolf Alpha” Hearthstone card. Introduction show me the fare from ci0 to ci1 Tasks like semantic parsing and code generation are challenging in part because they are structured (the output must be well-formed) but not synchronous (the output structure diverges from the input structure). Sequence-to-sequence models have proven effective for both tasks (Dong and Lapata, 2016; Ling et al., 2016), using encoder-decoder frameworks to exploit the sequential structure on both the input and output side. Yet these approaches do not account for much richer structural constraints on outputs—including well-formedness, well-typedness, and executability. The wellformedness case is of particular interest, since it can readily be enforced by representing outputs as abstract syntax trees (ASTs) (Aho et al., 2006), an approach that can be seen as a much lighter weight ∗ Equal contribution. lambda $0 e ( exists $1 ( and ( from $1 ci0 ) ( to $1 ci1 ) ( = ( fare $1 ) $0 ) ) ) Figur"
P17-1105,N16-1024,0,0.0873672,"Missing"
P17-1105,D13-1161,0,0.224481,"Missing"
P17-1105,J13-2005,1,0.830509,"Missing"
P17-1105,P16-1057,0,0.480811,"Missing"
P17-1105,D14-1135,0,0.469969,"Missing"
P17-1105,D07-1071,0,0.786515,"Missing"
P17-1105,N15-1162,0,0.203946,"Missing"
P17-1105,P11-1060,1,\N,Missing
P17-2025,J93-2004,0,0.0605723,"10 × Kw , with Kw ranging from 10 to 100. Table 1 shows F1 for decoding in both generative models on the development set, using the top-scoring parse found for a sentence when searching with the given beam size. RG has comparatively larger gains in performance between the larger beam sizes, while still underperforming LM, suggesting that more search is necessary in this model. 3 Experiments Using the above decoding procedures, we attempt to separate reranking effects from model combination effects through a set of reranking experiments. Our base experiments are performed on the Penn Treebank (Marcus et al., 1993), using sections 2-21 for training, section 22 for development, and section 23 for testing. For the LSTM generative model (LM), we use the pre-trained model released by Choe and Charniak (2016). We train RNNG discriminative (RD) and generative (RG) models, following Dyer et al. (2016) by using the same hyperparameter settings, and using pretrained word embeddings from Ling et al. (2015) for the discriminative model. The automaticallypredicted part-of-speech tags we use as input for RD are the same as those used by Cross and Huang (2016). In each experiment, we obtain a set of candidate parses"
P17-2025,N10-1003,0,0.0196099,"1.73 93.29 93.64 93.75 94.18 94.27 94.63 94.33 94.66 92.72 94.09 93.97 94.25 92.53 94.22 94.56 94.62 Table 3: Test F1 scores on section 23 of the PTB, by treebank training data conditions: either using only the training sections of the PTB, or using additional silver data (+S). Semi-supervised silver data Choe and Charniak (2016) found a substantial increase in performance by training on external data in addition to trees from the Penn Treebank. This silver dataset was obtained by parsing the entire New York Times section of the fifth Gigaword corpus using a product of eight Berkeley parsers (Petrov, 2010) and ZPar (Zhu et al., 2013), then retaining 24 million sentences on which both parsers agreed. For our experiments we train RD and RG using the same silver dataset.3 The +S column in Table 3 shows these results, where we observe gains over the PTB models in nearly every case. As in the PTB training data setting, using all models for candidates and score combinations is best, achieving 94.66 F1 (row 9). Ensembling Finally, we compare to another commonly used model combination method: ensembling multiple instances of the same model type trained from different random initializations. We train en"
P17-2025,P15-2142,0,0.0612446,"RG in two ways: 1) LM has separate reduce actions R EDUCE (X ) for each nonterminal X, and 2) LM allows any action to have non-zero probability at all times, even those that may be structurally invalid. 162 model RG LM Word-synchronous beam size, Kw 10 20 40 60 80 100 74.1 80.1 85.3 87.5 88.7 89.6 83.7 88.6 90.9 91.6 92.0 92.2 Table 1: F1 on the development set for word-synchronous beam search when searching in the RNNG generative (RG) and LSTM generative (LM) models. Ka is set to 10 × Kw . decoding procedures developed for other generative models (Henderson, 2003; Titov and Henderson, 2010; Buys and Blunsom, 2015) and can be viewed as a simplified version of the procedure used in the generative top-down parsers of Roark (2001) and Charniak (2010). In word-synchronous search, we augment the beam state space, identifying beams by tuples (|W |, |Aw |), where |W |is the number of words that have been produced so far in the sentence, and |Aw |is the number of structural actions that have been taken since the last word was produced. Intuitively, we want candidates with the same |W |= w to compete against each other. For a beam of partial parses in the state (|W |= w, |Aw |= a), we generate a beam of successo"
P17-2025,J01-2004,0,0.404585,"n-zero probability at all times, even those that may be structurally invalid. 162 model RG LM Word-synchronous beam size, Kw 10 20 40 60 80 100 74.1 80.1 85.3 87.5 88.7 89.6 83.7 88.6 90.9 91.6 92.0 92.2 Table 1: F1 on the development set for word-synchronous beam search when searching in the RNNG generative (RG) and LSTM generative (LM) models. Ka is set to 10 × Kw . decoding procedures developed for other generative models (Henderson, 2003; Titov and Henderson, 2010; Buys and Blunsom, 2015) and can be viewed as a simplified version of the procedure used in the generative top-down parsers of Roark (2001) and Charniak (2010). In word-synchronous search, we augment the beam state space, identifying beams by tuples (|W |, |Aw |), where |W |is the number of words that have been produced so far in the sentence, and |Aw |is the number of structural actions that have been taken since the last word was produced. Intuitively, we want candidates with the same |W |= w to compete against each other. For a beam of partial parses in the state (|W |= w, |Aw |= a), we generate a beam of successors by taking all of the next possible actions for each partial parse in the beam. If the action is NT(X) or R EDUCE"
P17-2025,A00-2018,0,0.708762,"100 for RD and word-synchronous beam (Section 2.2) with Kw = 100 and Ka = 1000 for the generative models RG and LM. In the case that we are using only the scores from a single generative model to rescore candidates taken from the discriminative parser, this setup is close to the reranking procedures originally proposed for these generative models. For RG, the original work also used RD to produce candidates, but drew samples from it, whereas we use a beam search to approximate its k-best list. The LM generative model was originally used to rerank a 50-best list taken from the Charniak parser (Charniak, 2000). In comparison, we found higher performance for the LM model when using a candidate list from the RD parser: 93.66 F1 versus 92.79 F1 on the development data. This may be attributable to having a stronger set of candidates: with beam size 100, RD has an oracle F1 of 98.2, compared to 95.9 for the 50-best list from the Charniak parser. 3.1 Augmenting the candidate set We first experiment with combining the candidate lists from multiple models, which allows us to look for potential model errors and model combination effects. Consider the standard reranking setup B → A, where we search in B to g"
P17-2025,D10-1066,0,0.373935,"y at all times, even those that may be structurally invalid. 162 model RG LM Word-synchronous beam size, Kw 10 20 40 60 80 100 74.1 80.1 85.3 87.5 88.7 89.6 83.7 88.6 90.9 91.6 92.0 92.2 Table 1: F1 on the development set for word-synchronous beam search when searching in the RNNG generative (RG) and LSTM generative (LM) models. Ka is set to 10 × Kw . decoding procedures developed for other generative models (Henderson, 2003; Titov and Henderson, 2010; Buys and Blunsom, 2015) and can be viewed as a simplified version of the procedure used in the generative top-down parsers of Roark (2001) and Charniak (2010). In word-synchronous search, we augment the beam state space, identifying beams by tuples (|W |, |Aw |), where |W |is the number of words that have been produced so far in the sentence, and |Aw |is the number of structural actions that have been taken since the last word was produced. Intuitively, we want candidates with the same |W |= w to compete against each other. For a beam of partial parses in the state (|W |= w, |Aw |= a), we generate a beam of successors by taking all of the next possible actions for each partial parse in the beam. If the action is NT(X) or R EDUCE, we place the resul"
P17-2025,P13-1043,0,0.0455343,"4.18 94.27 94.63 94.33 94.66 92.72 94.09 93.97 94.25 92.53 94.22 94.56 94.62 Table 3: Test F1 scores on section 23 of the PTB, by treebank training data conditions: either using only the training sections of the PTB, or using additional silver data (+S). Semi-supervised silver data Choe and Charniak (2016) found a substantial increase in performance by training on external data in addition to trees from the Penn Treebank. This silver dataset was obtained by parsing the entire New York Times section of the fifth Gigaword corpus using a product of eight Berkeley parsers (Petrov, 2010) and ZPar (Zhu et al., 2013), then retaining 24 million sentences on which both parsers agreed. For our experiments we train RD and RG using the same silver dataset.3 The +S column in Table 3 shows these results, where we observe gains over the PTB models in nearly every case. As in the PTB training data setting, using all models for candidates and score combinations is best, achieving 94.66 F1 (row 9). Ensembling Finally, we compare to another commonly used model combination method: ensembling multiple instances of the same model type trained from different random initializations. We train ensembles of 8 copies each of"
P17-2025,D16-1257,0,0.468843,"coring candidates using some combined score A + B would be even better, which we would characterize as a model combination gain. It might even be the case that B is a better parser overall (i.e. B → B outperforms A → A). Of course, many real hybrids will exhibit both reranking and model combination gains. In this paper, we present experiments to isolate the degree to which each gain occurs for each of two state-of-the-art generative neural parsing models: the Recurrent Neural Network Grammar generative parser (RG) of Dyer et al. (2016), and the LSTM language modeling generative parser (LM) of Choe and Charniak (2016). In particular, we present and use a beam-based search procedure with an augmented state space that can search directly in the generative models, allowing us to explore A → A for these generative parsers A independent of any base parsers. Our findings suggest the presence of model combination effects in both generative parsers: when parses found by searching directly in the generative parser are added to a list of candidates from a strong base parser (the RNNG discriminative parser, RD (Dyer et al., 2016)), performance decreases when compared to using just candidates from the base parser, i.e"
P17-2025,D16-1001,0,0.198124,"s. Our base experiments are performed on the Penn Treebank (Marcus et al., 1993), using sections 2-21 for training, section 22 for development, and section 23 for testing. For the LSTM generative model (LM), we use the pre-trained model released by Choe and Charniak (2016). We train RNNG discriminative (RD) and generative (RG) models, following Dyer et al. (2016) by using the same hyperparameter settings, and using pretrained word embeddings from Ling et al. (2015) for the discriminative model. The automaticallypredicted part-of-speech tags we use as input for RD are the same as those used by Cross and Huang (2016). In each experiment, we obtain a set of candidate parses for each sentence by performing beam search in one or more parsers. We use actionsynchronous beam search (Section 2.1) with beam size K = 100 for RD and word-synchronous beam (Section 2.2) with Kw = 100 and Ka = 1000 for the generative models RG and LM. In the case that we are using only the scores from a single generative model to rescore candidates taken from the discriminative parser, this setup is close to the reranking procedures originally proposed for these generative models. For RG, the original work also used RD to produce cand"
P17-2025,N16-1024,0,0.271359,"stem B → A shows gains merely from subtle model combination effects. If so, scoring candidates using some combined score A + B would be even better, which we would characterize as a model combination gain. It might even be the case that B is a better parser overall (i.e. B → B outperforms A → A). Of course, many real hybrids will exhibit both reranking and model combination gains. In this paper, we present experiments to isolate the degree to which each gain occurs for each of two state-of-the-art generative neural parsing models: the Recurrent Neural Network Grammar generative parser (RG) of Dyer et al. (2016), and the LSTM language modeling generative parser (LM) of Choe and Charniak (2016). In particular, we present and use a beam-based search procedure with an augmented state space that can search directly in the generative models, allowing us to explore A → A for these generative parsers A independent of any base parsers. Our findings suggest the presence of model combination effects in both generative parsers: when parses found by searching directly in the generative parser are added to a list of candidates from a strong base parser (the RNNG discriminative parser, RD (Dyer et al., 2016)), per"
P17-2025,Q13-1012,0,0.018979,"a larger contributor to the success of LM, as this model achieves stronger performance on its own for the described search setting. Scoring models LM RD + LM 93.66 93.99 92.20 93.07 93.47 94.15 3.2 Score combination If the cross-scoring setup exhibits an implicit model combination effect, where strong performance results from searching in one model and scoring with the other, we might expect substantial further improvements in performance by explicitly combining the scores of both models. To do so, we score each parse by taking a weighted sum of the log-probabilities assigned by both models (Hayashi et al., 2013), using an interpolation parameter which we tune to maximize F1 on the development set. These results are given in columns RD + RG and RD + LM in Table 2. We find that combining the scores of both models improves on using the score of either model alone, regardless of the source of candidates. These improvements are statistically significant in all cases. Score combination also more than compensates for the decrease in performance we saw previously when adding in candidates from the generative model: RD ∪ RG → RD + RG improves upon both RD → RG and RD ∪ RG → RG, and the same effect holds for L"
P17-2025,N03-1014,0,0.231913,"imate 1 The action space for LM differs from RG in two ways: 1) LM has separate reduce actions R EDUCE (X ) for each nonterminal X, and 2) LM allows any action to have non-zero probability at all times, even those that may be structurally invalid. 162 model RG LM Word-synchronous beam size, Kw 10 20 40 60 80 100 74.1 80.1 85.3 87.5 88.7 89.6 83.7 88.6 90.9 91.6 92.0 92.2 Table 1: F1 on the development set for word-synchronous beam search when searching in the RNNG generative (RG) and LSTM generative (LM) models. Ka is set to 10 × Kw . decoding procedures developed for other generative models (Henderson, 2003; Titov and Henderson, 2010; Buys and Blunsom, 2015) and can be viewed as a simplified version of the procedure used in the generative top-down parsers of Roark (2001) and Charniak (2010). In word-synchronous search, we augment the beam state space, identifying beams by tuples (|W |, |Aw |), where |W |is the number of words that have been produced so far in the sentence, and |Aw |is the number of structural actions that have been taken since the last word was produced. Intuitively, we want candidates with the same |W |= w to compete against each other. For a beam of partial parses in the state"
P17-2025,N15-1142,0,0.0393842,"ts Using the above decoding procedures, we attempt to separate reranking effects from model combination effects through a set of reranking experiments. Our base experiments are performed on the Penn Treebank (Marcus et al., 1993), using sections 2-21 for training, section 22 for development, and section 23 for testing. For the LSTM generative model (LM), we use the pre-trained model released by Choe and Charniak (2016). We train RNNG discriminative (RD) and generative (RG) models, following Dyer et al. (2016) by using the same hyperparameter settings, and using pretrained word embeddings from Ling et al. (2015) for the discriminative model. The automaticallypredicted part-of-speech tags we use as input for RD are the same as those used by Cross and Huang (2016). In each experiment, we obtain a set of candidate parses for each sentence by performing beam search in one or more parsers. We use actionsynchronous beam search (Section 2.1) with beam size K = 100 for RD and word-synchronous beam (Section 2.2) with Kw = 100 and Ka = 1000 for the generative models RG and LM. In the case that we are using only the scores from a single generative model to rescore candidates taken from the discriminative parser"
P17-2025,Q17-1004,0,\N,Missing
P17-2052,D15-1103,0,0.116134,"Missing"
P17-2052,Q14-1037,1,0.84888,"plotted by type frequency in the training corpus. Corpus head of the child. We conclude by projecting each type down to its syntactic head. Our corpus construction methodology involves three key stages: mention identification, type system construction, and type assignment.1 We explain each of these in turn. Type assignment. The type set for an entity is obtained by taking its Wikipedia category assignments, augmenting these with their ancestors in the category graph above, and then projecting these down to their syntactic heads. Mention identification. We follow prior work on entity linking (Durrett and Klein, 2014) and take all mentions that occur as anchor text. We filter the resulting collection of mentions down to those that pass a heuristic filter that removes mentions of common nouns, as well as spurious sentences representing Wikipedia formatting. 4 Experiments We evaluate our method on the dataset described in Section 3. For these experiments, we restrict to the 100 most frequent types and downsample to 750K mentions. We use a baseline that closely replicates the F IGER system (Ling and Weld, 2012). Within our framework, this can be thought of as a model that sets all type pair features in (2) to"
P17-2052,D15-1032,1,0.802889,"rams and bigrams. Indicators on all uni- and bigrams within a certain window of the entity mention. set–e.g. a parent-child type pair, sibling type pairs, and so on, abstracting away the specific types. 2. Dependency parse features. Indicators on the lexical parent of the entity mention head, as well as the corresponding dependency type. Separately, indicators on the lexical children of the entity mention head and their dependency types. 2.2 Learning and Inference We train our system using structured maxmargin (Tsochantaridis et al., 2005). Optimization is performed via AdaGrad on the primal (Kummerfeld et al., 2015). We use set-F1 as our loss function. Inference, for both prediction and lossaugmented decoding, poses a greater challenge, as solving the maximization problem (1) exactly requires iterating over all subsets of the type system. Fortunately, we find a simple greedy algorithm is effective. Our decoder begins by choosing the type that scores highest individually, taking only single-type features into account. It then proceeds by iteratively adding new types into the set until doing so would decrease the score. At the cost of restricting the permissible type sets slightly, we can speed up the gree"
P17-2052,W16-1313,0,0.0702447,"input sentence-entity pair, the set of types associated with that entity. We take the commonly-used linear model approach to this structured prediction problem. Given a featurizer ϕ that takes an input sentence x and entity e, we seek to learn a weight vector w such that has mostly followed this lead (Yaghoobzadeh and Sch¨utze, 2016; Yogatama et al., 2015), although types based on WordNet have recently also been investigated (Corro et al., 2015). Most prior work has focused on unstructured predictors using some form of multiclass logistic regression (Ling and Weld, 2012; Gillick et al., 2014; Shimaoka et al., 2016; Yaghoobzadeh and Sch¨utze, 2016; Yogatama et al., 2015). Some of these approaches implicitly incorporate structure during decoding by enforcing hierarchy constraints (Gillick et al., 2014), while neural approaches can encode correlations in a soft manner via shared hidden layers (Shimaoka et al., 2016; Yaghoobzadeh and Sch¨utze, 2016). Our work differs from these lines of work in two respects: its use of a corpus exhibiting high type multiplicity with types derived from a semi-open inventory and its use of a fully structured model and decoding procedure, one that can in principle be integrat"
P17-2052,P15-2048,0,0.214085,"12 13 14 15 Figure 1: Comparison of type set size CDFs for the our Wikipedia corpus and the prior F IGER corpus (Ling and Weld, 2012). The figure illustrates that our corpus exhibits much greater type assignment multiplicity. Introduction Motivated by potential applications to information retrieval, coreference resolution, question answering, and other downstream tasks, recent work on entity typing has moved beyond coarse-grained systems towards richer ontologies with much more detailed information, and therefore correspondingly more specific types (Ling and Weld, 2012; Gillick et al., 2014; Yogatama et al., 2015). As types become more specific, entities will tend to belong to more types (i.e. there will tend to be higher type multiplicity). However, most data used in previous work exhibits an extremely low degree of multiplicity. In this paper, we focus on the high multiplicity case, which we argue naturally arises in largescale knowledge resources. To illustrate this point, we construct a corpus of entity mentions paired with higher-multiplicity type assignments. Our corpus is based on mentions and categories drawn from Wikipedia, but we generalize and denoise the raw Wikipedia categories to provide"
P18-1249,W14-6110,0,0.0986118,"Missing"
P18-1249,W13-4916,0,0.0738451,"Missing"
P18-1249,D16-1257,0,0.134948,"trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset. 1 …(VP(VBD fled)(NP(DT the)(NN market))… Decoder Encoder Input and CC fled VBD the DT market NN in IN Figure 1: Our parser combines a chart decoder with a sentence encoder based on self-attention. Introduction In recent years, neural network approaches have led to improvements in constituency parsing (Dyer et al., 2016; Cross and Huang, 2016; Choe and Charniak, 2016; Stern et al., 2017a; Fried et al., 2017). Many of these parsers can broadly be characterized as following an encoder-decoder design: an encoder reads the input sentence and summarizes it into a vector or set of vectors (e.g. one for each word or span in the sentence), and then a decoder uses these vector summaries to incrementally build up a labeled parse tree. In contrast to the large variety of decoder architectures investigated in recent work, the encoders in recent parsers have predominantly been built using recurrent neural networks (RNNs), and in particular Long Short-Term Memory netwo"
P18-1249,E17-2053,0,0.218273,"Missing"
P18-1249,D16-1001,0,0.318253,"sults for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset. 1 …(VP(VBD fled)(NP(DT the)(NN market))… Decoder Encoder Input and CC fled VBD the DT market NN in IN Figure 1: Our parser combines a chart decoder with a sentence encoder based on self-attention. Introduction In recent years, neural network approaches have led to improvements in constituency parsing (Dyer et al., 2016; Cross and Huang, 2016; Choe and Charniak, 2016; Stern et al., 2017a; Fried et al., 2017). Many of these parsers can broadly be characterized as following an encoder-decoder design: an encoder reads the input sentence and summarizes it into a vector or set of vectors (e.g. one for each word or span in the sentence), and then a decoder uses these vector summaries to incrementally build up a labeled parse tree. In contrast to the large variety of decoder architectures investigated in recent work, the encoders in recent parsers have predominantly been built using recurrent neural networks (RNNs), and in particular Lon"
P18-1249,P15-1030,1,0.951225,"g an encoder-decoder design: an encoder reads the input sentence and summarizes it into a vector or set of vectors (e.g. one for each word or span in the sentence), and then a decoder uses these vector summaries to incrementally build up a labeled parse tree. In contrast to the large variety of decoder architectures investigated in recent work, the encoders in recent parsers have predominantly been built using recurrent neural networks (RNNs), and in particular Long Short-Term Memory networks (LSTMs). RNNs have largely replaced approaches such as the fixed-window-size feed-forward networks of Durrett and Klein (2015) in part due to their ability to capture global context. However, RNNs are not the only architecture capable of summarizing large global contexts: recent work by Vaswani et al. (2017) presented a new state-of-the-art approach to machine translation with an architecture that entirely eliminates recurrent connections and relies instead on a repeated neural attention mechanism. In this paper, we introduce a parser that combines an encoder built using this kind of self-attentive architecture with a decoder customized for parsing (Figure 1). In Section 2 of this paper, we describe the architecture"
P18-1249,N16-1024,0,0.15173,"state-ofthe-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset. 1 …(VP(VBD fled)(NP(DT the)(NN market))… Decoder Encoder Input and CC fled VBD the DT market NN in IN Figure 1: Our parser combines a chart decoder with a sentence encoder based on self-attention. Introduction In recent years, neural network approaches have led to improvements in constituency parsing (Dyer et al., 2016; Cross and Huang, 2016; Choe and Charniak, 2016; Stern et al., 2017a; Fried et al., 2017). Many of these parsers can broadly be characterized as following an encoder-decoder design: an encoder reads the input sentence and summarizes it into a vector or set of vectors (e.g. one for each word or span in the sentence), and then a decoder uses these vector summaries to incrementally build up a labeled parse tree. In contrast to the large variety of decoder architectures investigated in recent work, the encoders in recent parsers have predominantly been built using recurrent neural networks (RNNs)"
P18-1249,P17-2025,1,0.869112,"t the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset. 1 …(VP(VBD fled)(NP(DT the)(NN market))… Decoder Encoder Input and CC fled VBD the DT market NN in IN Figure 1: Our parser combines a chart decoder with a sentence encoder based on self-attention. Introduction In recent years, neural network approaches have led to improvements in constituency parsing (Dyer et al., 2016; Cross and Huang, 2016; Choe and Charniak, 2016; Stern et al., 2017a; Fried et al., 2017). Many of these parsers can broadly be characterized as following an encoder-decoder design: an encoder reads the input sentence and summarizes it into a vector or set of vectors (e.g. one for each word or span in the sentence), and then a decoder uses these vector summaries to incrementally build up a labeled parse tree. In contrast to the large variety of decoder architectures investigated in recent work, the encoders in recent parsers have predominantly been built using recurrent neural networks (RNNs), and in particular Long Short-Term Memory networks (LSTMs). RNNs have largely replaced ap"
P18-1249,N18-1091,1,0.79425,"Missing"
P18-1249,P14-1022,1,0.871805,"own in Table 4. We first evaluate an approach (C HAR LSTM) that independently runs a bidirectional LSTM over the characters in each word and uses the LSTM outputs in place of part-of-speech tag embeddings. We find that this approach performs better than using predicted part-of-speech tags. We can further remove the word embeddings (leaving the character LSTMs only), which does not seem to hurt and can actually help increase parsing accuracy. Next we examine the importance of recurrent connections by constructing and evaluating a simpler alternative. Our approach (C HAR C ONCAT) is inspired by Hall et al. (2014), who found it effective to replace words with frequently-occurring suffixes, and the observation that our original tag embeddings are rather high-dimensional. To represent a word, we extract its first 8 letters and last 8 letters, embed each letter, and concatenate the results. If we use 32-dimensional embeddings, the 16 letters can be packed into a 512-dimensional vector – the same size as the inputs to our model. This size for the inputs in our model was chosen to simplify the use of residual connections (by matching vector dimensions), even though the inputs themselves could have been enco"
P18-1249,Q17-1029,0,0.28889,"Missing"
P18-1249,N18-1202,0,0.327902,"ords or inflections. In Section 5.1, we demonstrate that a simple scheme based on concatenating character embeddings of word prefixes/suffixes can outperform using part-of-speech tags from an external system. We also present a version of our model that uses a character LSTM, which performs better than other lexical representations – even if word embeddings are removed from the model. In Section 5.2, we explore an alternative approach for lexical representations that makes use of pre-training on a large unsupervised corpus. We find that using the deep contextualized representations proposed by Peters et al. (2018) can boost parsing accuracy. Our parser achieves 93.55 F1 on the Penn Treebank WSJ test set when not using external word representations, outperforming all previous singlesystem constituency parsers trained only on the WSJ training set. The addition of pre-trained word representations following Peters et al. (2018) increases parsing accuracy to 95.13 F1, a new stateof-the-art for this dataset. Our model also outperforms previous best published results on 8 of the 9 languages in the SPMRL 2013/2014 shared tasks. Code and trained English models are publicly available.1 2 Base Model Our parser fo"
P18-1249,P17-1076,1,0.732315,"bank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset. 1 …(VP(VBD fled)(NP(DT the)(NN market))… Decoder Encoder Input and CC fled VBD the DT market NN in IN Figure 1: Our parser combines a chart decoder with a sentence encoder based on self-attention. Introduction In recent years, neural network approaches have led to improvements in constituency parsing (Dyer et al., 2016; Cross and Huang, 2016; Choe and Charniak, 2016; Stern et al., 2017a; Fried et al., 2017). Many of these parsers can broadly be characterized as following an encoder-decoder design: an encoder reads the input sentence and summarizes it into a vector or set of vectors (e.g. one for each word or span in the sentence), and then a decoder uses these vector summaries to incrementally build up a labeled parse tree. In contrast to the large variety of decoder architectures investigated in recent work, the encoders in recent parsers have predominantly been built using recurrent neural networks (RNNs), and in particular Long Short-Term Memory networks (LSTMs). RNNs ha"
P18-1249,D17-1178,1,0.855067,"bank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset. 1 …(VP(VBD fled)(NP(DT the)(NN market))… Decoder Encoder Input and CC fled VBD the DT market NN in IN Figure 1: Our parser combines a chart decoder with a sentence encoder based on self-attention. Introduction In recent years, neural network approaches have led to improvements in constituency parsing (Dyer et al., 2016; Cross and Huang, 2016; Choe and Charniak, 2016; Stern et al., 2017a; Fried et al., 2017). Many of these parsers can broadly be characterized as following an encoder-decoder design: an encoder reads the input sentence and summarizes it into a vector or set of vectors (e.g. one for each word or span in the sentence), and then a decoder uses these vector summaries to incrementally build up a labeled parse tree. In contrast to the large variety of decoder architectures investigated in recent work, the encoders in recent parsers have predominantly been built using recurrent neural networks (RNNs), and in particular Long Short-Term Memory networks (LSTMs). RNNs ha"
P18-2075,D16-1001,0,0.0726186,"vel evaluation metric, for example F1. A large family of techniques address the exposure bias problem by allowing the model to make mistakes and explore incorrect states during training, supervising actions at the resulting states using an expert policy (Daum´e III et al., 2009; Ross et al., 2011; Choi and Palmer, 2011; Chang et al., 2015); these expert policies are typically referred to as dynamic oracles in parsing (Goldberg and Nivre, 2012; Ballesteros et al., 2016). While dynamic oracles have produced substantial improvements in constituency parsing performance (Coavoux and Crabb´e, 2016; Cross and Huang, 2016; Stern et al., 2017; Gonz´alez and G´omez-Rodr´ıguez, 2018), they must be custom designed for each transition system. To address the loss mismatch problem, another line of work has directly optimized for structurelevel cost functions (Goodman, 1996; Och, 2003). Recent methods applied to models that produce output sequentially commonly use policy gradient (Auli and Gao, 2014; Ranzato et al., 2016; Shen et al., 2016) or beam search (Xu et al., 2016; Wiseman and Rush, 2016; Edunov et al., 2017) at training time to minimize a structured cost. These methods also reduce exposure bias through explor"
P18-2075,P16-1231,0,0.0383048,"define for the transition system of Dyer et al. (2016)), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle. 1 Introduction Many recent state-of-the-art models for constituency parsing are transition based, decomposing production of each parse tree into a sequence of action decisions (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017), building on a long line of work in transition-based parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Henderson, 2004; Zhang and Clark, 2011; Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016). However, models of this type, which decompose structure prediction into sequential decisions, can be prone to two issues (Ranzato et al., 2016; Wiseman and Rush, 2016). The first is exposure bias: if, at training time, the model only observes 469 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 469–476 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Dyer et al. (2016). We find that while policy gradient usually outperforms standard likelihood training, it t"
P18-2075,N16-1024,0,0.508022,"arser’s transition system. We explore using a policy gradient method as a parser-agnostic alternative. In addition to directly optimizing for a tree-level metric such as F1, policy gradient has the potential to reduce exposure bias by allowing exploration during training; moreover, it does not require a dynamic oracle for supervision. On four constituency parsers in three languages, the method substantially outperforms static oracle likelihood training in almost all settings. For parsers where a dynamic oracle is available (including a novel oracle which we define for the transition system of Dyer et al. (2016)), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle. 1 Introduction Many recent state-of-the-art models for constituency parsing are transition based, decomposing production of each parse tree into a sequence of action decisions (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017), building on a long line of work in transition-based parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Henderson, 2004; Zhang and Clark, 2011; Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016). H"
P18-2075,P14-2023,0,0.0171467,"eferred to as dynamic oracles in parsing (Goldberg and Nivre, 2012; Ballesteros et al., 2016). While dynamic oracles have produced substantial improvements in constituency parsing performance (Coavoux and Crabb´e, 2016; Cross and Huang, 2016; Stern et al., 2017; Gonz´alez and G´omez-Rodr´ıguez, 2018), they must be custom designed for each transition system. To address the loss mismatch problem, another line of work has directly optimized for structurelevel cost functions (Goodman, 1996; Och, 2003). Recent methods applied to models that produce output sequentially commonly use policy gradient (Auli and Gao, 2014; Ranzato et al., 2016; Shen et al., 2016) or beam search (Xu et al., 2016; Wiseman and Rush, 2016; Edunov et al., 2017) at training time to minimize a structured cost. These methods also reduce exposure bias through exploration but do not require an expert policy for supervision. In this work, we apply a simple policy gradient method to train four different state-of-theart transition-based constituency parsers to maximize expected F1. We compare against training with a dynamic oracle (both to supervise exploration and provide loss-augmentation) where one is available, including a novel dynami"
P18-2075,P17-1138,0,0.0237914,"ave produced substantial improvements in constituency parsing performance (Coavoux and Crabb´e, 2016; Cross and Huang, 2016; Stern et al., 2017; Gonz´alez and G´omez-Rodr´ıguez, 2018), they must be custom designed for each transition system. To address the loss mismatch problem, another line of work has directly optimized for structurelevel cost functions (Goodman, 1996; Och, 2003). Recent methods applied to models that produce output sequentially commonly use policy gradient (Auli and Gao, 2014; Ranzato et al., 2016; Shen et al., 2016) or beam search (Xu et al., 2016; Wiseman and Rush, 2016; Edunov et al., 2017) at training time to minimize a structured cost. These methods also reduce exposure bias through exploration but do not require an expert policy for supervision. In this work, we apply a simple policy gradient method to train four different state-of-theart transition-based constituency parsers to maximize expected F1. We compare against training with a dynamic oracle (both to supervise exploration and provide loss-augmentation) where one is available, including a novel dynamic oracle that we define for the top-down transition system of Dynamic oracles provide strong supervision for training co"
P18-2075,D11-1031,0,0.0166683,"there are any open constituents on the stack which can be closed (i.e. have had a word shifted since being opened), check the topmost of these (the one that has been opened most recently). If closing it would produce a constituent from the the gold tree that has not yet been produced (which is determined by the constituent’s label, span beginning position, and the number of words currently shifted), or if the constituent could not be closed at a later position in the sentence to produce a constituent in the gold tree, return C LOSE. Softmax Margin Softmax margin loss (Gimpel and Smith, 2010; Auli and Lopez, 2011) addresses loss mismatch by incorporating task cost into the training loss. Since trees are decomposed into a sequence of local action predictions, we cannot use a global cost, such as F1, directly. As a proxy, we rely on the dynamic oracles’ action-level supervision. In all models we consider, action probabilities (Eq. 1) are parameterized by a softmax function pM L (a |st ; θ) ∝ exp(z(a, st , θ)) for some state–action scoring function z. The softmax-margin objective replaces this by the estimate of the risk objective’s gradient; however since in the parsing tasks we consider, the gold tree h"
P18-2075,P17-2025,1,0.731015,"exploration). Finally, we observe that policy gradient also provides large improvements for the In-Order parser, where a dynamic oracle has not been defined. We note that although some of these results (92.6 for English, 83.5 for French, 87.0 for Chinese) are state-of-the-art for single model, discriminative transition-based parsers, other work on constituency parsing achieves better performance through other methods. Techniques that combine multiple models or add semi-supervised data (Vinyals et al., 2015; Dyer et al., 2016; Choe and Charniak, 2016; Kuncoro et al., 2017; Liu and Zhang, 2017; Fried et al., 2017) are orthogonal to, and could be combined with, the singlemodel, fixed training data methods we explore. Other recent work (Gaddy et al., 2018; Kitaev and Klein, 2018) obtains comparable or stronger performance with global chart decoders, where training uses loss augmentation provided by an oracle. By performing model-optimal global inference, these parsers likely avoid the exposure bias problem of the sequential transition-based parsers we investigate, at the cost of requiring a chart decoding procedure for inference. Overall, we find that although optimizing for F1 in a model-agnostic fashio"
P18-2075,N18-1091,1,0.729461,"been defined. We note that although some of these results (92.6 for English, 83.5 for French, 87.0 for Chinese) are state-of-the-art for single model, discriminative transition-based parsers, other work on constituency parsing achieves better performance through other methods. Techniques that combine multiple models or add semi-supervised data (Vinyals et al., 2015; Dyer et al., 2016; Choe and Charniak, 2016; Kuncoro et al., 2017; Liu and Zhang, 2017; Fried et al., 2017) are orthogonal to, and could be combined with, the singlemodel, fixed training data methods we explore. Other recent work (Gaddy et al., 2018; Kitaev and Klein, 2018) obtains comparable or stronger performance with global chart decoders, where training uses loss augmentation provided by an oracle. By performing model-optimal global inference, these parsers likely avoid the exposure bias problem of the sequential transition-based parsers we investigate, at the cost of requiring a chart decoding procedure for inference. Overall, we find that although optimizing for F1 in a model-agnostic fashion with policy gradient typically underperforms the model-aware expert supervision given by the dynamic oracle training methods, it provides a"
P18-2075,D16-1211,0,0.26875,"ared to recover from its own mistakes during prediction. Second is the loss mismatch between the action-level loss used at training and any structure-level evaluation metric, for example F1. A large family of techniques address the exposure bias problem by allowing the model to make mistakes and explore incorrect states during training, supervising actions at the resulting states using an expert policy (Daum´e III et al., 2009; Ross et al., 2011; Choi and Palmer, 2011; Chang et al., 2015); these expert policies are typically referred to as dynamic oracles in parsing (Goldberg and Nivre, 2012; Ballesteros et al., 2016). While dynamic oracles have produced substantial improvements in constituency parsing performance (Coavoux and Crabb´e, 2016; Cross and Huang, 2016; Stern et al., 2017; Gonz´alez and G´omez-Rodr´ıguez, 2018), they must be custom designed for each transition system. To address the loss mismatch problem, another line of work has directly optimized for structurelevel cost functions (Goodman, 1996; Och, 2003). Recent methods applied to models that produce output sequentially commonly use policy gradient (Auli and Gao, 2014; Ranzato et al., 2016; Shen et al., 2016) or beam search (Xu et al., 2016;"
P18-2075,N10-1112,0,0.159371,"twork Grammars (RNNG) parser of Dyer et al. (2016), (2) the In-Order parser of Liu and Zhang (2017), (3) the Span-Based parser of Cross and Huang (2016), and (4) the Top-Down parser of Stern et al. (2017).1 We refer to the original papers for descriptions of the transition systems and model parameterizations. 3 y which measures the model’s expected cost over possible outputs y for each of the training examples (x(1) , y(1) ), . . . , (x(N ) , y(N ) ). Minimizing a risk objective has a long history in structured prediction (Povey and Woodland, 2002; Smith and Eisner, 2006; Li and Eisner, 2009; Gimpel and Smith, 2010) but often relies on the cost function decomposing according to the output structure. However, we can avoid any restrictions on the cost using reinforcement learning-style approaches (Xu et al., 2016; Shen et al., 2016; Edunov et al., 2017) where cost is ascribed to the entire output structure – albeit at the expense of introducing a potentially difficult credit assignment problem. The policy gradient method we apply is a simple variant of REINFORCE (Williams, 1992). We perform mini-batch gradient descent on the gradient of the risk objective: The transition-based parsers we use all decompose"
P18-2075,C12-1059,0,0.0624615,"sions, it will not be prepared to recover from its own mistakes during prediction. Second is the loss mismatch between the action-level loss used at training and any structure-level evaluation metric, for example F1. A large family of techniques address the exposure bias problem by allowing the model to make mistakes and explore incorrect states during training, supervising actions at the resulting states using an expert policy (Daum´e III et al., 2009; Ross et al., 2011; Choi and Palmer, 2011; Chang et al., 2015); these expert policies are typically referred to as dynamic oracles in parsing (Goldberg and Nivre, 2012; Ballesteros et al., 2016). While dynamic oracles have produced substantial improvements in constituency parsing performance (Coavoux and Crabb´e, 2016; Cross and Huang, 2016; Stern et al., 2017; Gonz´alez and G´omez-Rodr´ıguez, 2018), they must be custom designed for each transition system. To address the loss mismatch problem, another line of work has directly optimized for structurelevel cost functions (Goodman, 1996; Och, 2003). Recent methods applied to models that produce output sequentially commonly use policy gradient (Auli and Gao, 2014; Ranzato et al., 2016; Shen et al., 2016) or be"
P18-2075,D14-1082,0,0.0434819,"a novel oracle which we define for the transition system of Dyer et al. (2016)), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle. 1 Introduction Many recent state-of-the-art models for constituency parsing are transition based, decomposing production of each parse tree into a sequence of action decisions (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017), building on a long line of work in transition-based parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Henderson, 2004; Zhang and Clark, 2011; Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016). However, models of this type, which decompose structure prediction into sequential decisions, can be prone to two issues (Ranzato et al., 2016; Wiseman and Rush, 2016). The first is exposure bias: if, at training time, the model only observes 469 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 469–476 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Dyer et al. (2016). We find that while policy gradient usually outperforms standard likel"
P18-2075,D16-1257,0,0.147388,"he smaller model on French (including 83.5 with softmax margin with exploration). Finally, we observe that policy gradient also provides large improvements for the In-Order parser, where a dynamic oracle has not been defined. We note that although some of these results (92.6 for English, 83.5 for French, 87.0 for Chinese) are state-of-the-art for single model, discriminative transition-based parsers, other work on constituency parsing achieves better performance through other methods. Techniques that combine multiple models or add semi-supervised data (Vinyals et al., 2015; Dyer et al., 2016; Choe and Charniak, 2016; Kuncoro et al., 2017; Liu and Zhang, 2017; Fried et al., 2017) are orthogonal to, and could be combined with, the singlemodel, fixed training data methods we explore. Other recent work (Gaddy et al., 2018; Kitaev and Klein, 2018) obtains comparable or stronger performance with global chart decoders, where training uses loss augmentation provided by an oracle. By performing model-optimal global inference, these parsers likely avoid the exposure bias problem of the sequential transition-based parsers we investigate, at the cost of requiring a chart decoding procedure for inference. Overall, we"
P18-2075,P96-1024,0,0.638045,"um´e III et al., 2009; Ross et al., 2011; Choi and Palmer, 2011; Chang et al., 2015); these expert policies are typically referred to as dynamic oracles in parsing (Goldberg and Nivre, 2012; Ballesteros et al., 2016). While dynamic oracles have produced substantial improvements in constituency parsing performance (Coavoux and Crabb´e, 2016; Cross and Huang, 2016; Stern et al., 2017; Gonz´alez and G´omez-Rodr´ıguez, 2018), they must be custom designed for each transition system. To address the loss mismatch problem, another line of work has directly optimized for structurelevel cost functions (Goodman, 1996; Och, 2003). Recent methods applied to models that produce output sequentially commonly use policy gradient (Auli and Gao, 2014; Ranzato et al., 2016; Shen et al., 2016) or beam search (Xu et al., 2016; Wiseman and Rush, 2016; Edunov et al., 2017) at training time to minimize a structured cost. These methods also reduce exposure bias through exploration but do not require an expert policy for supervision. In this work, we apply a simple policy gradient method to train four different state-of-theart transition-based constituency parsers to maximize expected F1. We compare against training with"
P18-2075,P11-2121,0,0.0276711,"Division University of California, Berkeley {dfried,klein}@cs.berkeley.edu Abstract states resulting from correct past decisions, it will not be prepared to recover from its own mistakes during prediction. Second is the loss mismatch between the action-level loss used at training and any structure-level evaluation metric, for example F1. A large family of techniques address the exposure bias problem by allowing the model to make mistakes and explore incorrect states during training, supervising actions at the resulting states using an expert policy (Daum´e III et al., 2009; Ross et al., 2011; Choi and Palmer, 2011; Chang et al., 2015); these expert policies are typically referred to as dynamic oracles in parsing (Goldberg and Nivre, 2012; Ballesteros et al., 2016). While dynamic oracles have produced substantial improvements in constituency parsing performance (Coavoux and Crabb´e, 2016; Cross and Huang, 2016; Stern et al., 2017; Gonz´alez and G´omez-Rodr´ıguez, 2018), they must be custom designed for each transition system. To address the loss mismatch problem, another line of work has directly optimized for structurelevel cost functions (Goodman, 1996; Och, 2003). Recent methods applied to models tha"
P18-2075,P04-1013,0,0.0425899,"dynamic oracle is available (including a novel oracle which we define for the transition system of Dyer et al. (2016)), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle. 1 Introduction Many recent state-of-the-art models for constituency parsing are transition based, decomposing production of each parse tree into a sequence of action decisions (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017), building on a long line of work in transition-based parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Henderson, 2004; Zhang and Clark, 2011; Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016). However, models of this type, which decompose structure prediction into sequential decisions, can be prone to two issues (Ranzato et al., 2016; Wiseman and Rush, 2016). The first is exposure bias: if, at training time, the model only observes 469 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 469–476 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Dyer et al. (2016). We find that while pol"
P18-2075,P16-1017,0,0.341103,"Missing"
P18-2075,Q16-1023,0,0.0306084,"ition system of Dyer et al. (2016)), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle. 1 Introduction Many recent state-of-the-art models for constituency parsing are transition based, decomposing production of each parse tree into a sequence of action decisions (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017), building on a long line of work in transition-based parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Henderson, 2004; Zhang and Clark, 2011; Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016). However, models of this type, which decompose structure prediction into sequential decisions, can be prone to two issues (Ranzato et al., 2016; Wiseman and Rush, 2016). The first is exposure bias: if, at training time, the model only observes 469 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 469–476 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Dyer et al. (2016). We find that while policy gradient usually outperforms standard likelihood training, it typically underperforms the dynami"
P18-2075,E17-1117,0,0.151083,"Missing"
P18-2075,W14-6111,0,0.0951098,"Missing"
P18-2075,D09-1005,0,0.0383667,"e Recurrent Neural Network Grammars (RNNG) parser of Dyer et al. (2016), (2) the In-Order parser of Liu and Zhang (2017), (3) the Span-Based parser of Cross and Huang (2016), and (4) the Top-Down parser of Stern et al. (2017).1 We refer to the original papers for descriptions of the transition systems and model parameterizations. 3 y which measures the model’s expected cost over possible outputs y for each of the training examples (x(1) , y(1) ), . . . , (x(N ) , y(N ) ). Minimizing a risk objective has a long history in structured prediction (Povey and Woodland, 2002; Smith and Eisner, 2006; Li and Eisner, 2009; Gimpel and Smith, 2010) but often relies on the cost function decomposing according to the output structure. However, we can avoid any restrictions on the cost using reinforcement learning-style approaches (Xu et al., 2016; Shen et al., 2016; Edunov et al., 2017) where cost is ascribed to the entire output structure – albeit at the expense of introducing a potentially difficult credit assignment problem. The policy gradient method we apply is a simple variant of REINFORCE (Williams, 1992). We perform mini-batch gradient descent on the gradient of the risk objective: The transition-based pars"
P18-2075,P16-1159,0,0.207393,"Goldberg and Nivre, 2012; Ballesteros et al., 2016). While dynamic oracles have produced substantial improvements in constituency parsing performance (Coavoux and Crabb´e, 2016; Cross and Huang, 2016; Stern et al., 2017; Gonz´alez and G´omez-Rodr´ıguez, 2018), they must be custom designed for each transition system. To address the loss mismatch problem, another line of work has directly optimized for structurelevel cost functions (Goodman, 1996; Och, 2003). Recent methods applied to models that produce output sequentially commonly use policy gradient (Auli and Gao, 2014; Ranzato et al., 2016; Shen et al., 2016) or beam search (Xu et al., 2016; Wiseman and Rush, 2016; Edunov et al., 2017) at training time to minimize a structured cost. These methods also reduce exposure bias through exploration but do not require an expert policy for supervision. In this work, we apply a simple policy gradient method to train four different state-of-theart transition-based constituency parsers to maximize expected F1. We compare against training with a dynamic oracle (both to supervise exploration and provide loss-augmentation) where one is available, including a novel dynamic oracle that we define for the top-down t"
P18-2075,P06-2101,0,0.0585402,"e: (1) the discriminative Recurrent Neural Network Grammars (RNNG) parser of Dyer et al. (2016), (2) the In-Order parser of Liu and Zhang (2017), (3) the Span-Based parser of Cross and Huang (2016), and (4) the Top-Down parser of Stern et al. (2017).1 We refer to the original papers for descriptions of the transition systems and model parameterizations. 3 y which measures the model’s expected cost over possible outputs y for each of the training examples (x(1) , y(1) ), . . . , (x(N ) , y(N ) ). Minimizing a risk objective has a long history in structured prediction (Povey and Woodland, 2002; Smith and Eisner, 2006; Li and Eisner, 2009; Gimpel and Smith, 2010) but often relies on the cost function decomposing according to the output structure. However, we can avoid any restrictions on the cost using reinforcement learning-style approaches (Xu et al., 2016; Shen et al., 2016; Edunov et al., 2017) where cost is ascribed to the entire output structure – albeit at the expense of introducing a potentially difficult credit assignment problem. The policy gradient method we apply is a simple variant of REINFORCE (Williams, 1992). We perform mini-batch gradient descent on the gradient of the risk objective: The"
P18-2075,N15-1142,0,0.0320573,"utlined by Cross and Huang (2016) and Stern et al. (2017) for representing morphological features as learned embeddings, and use the same dimensions for these embeddings as in their papers. For RNNG and In-Order, we similarly use 10-dimensional learned embeddings for each morphological feature, feeding them as LSTM inputs for each word alongside the word and part-of-speech tag embeddings. For RNNG and the In-Order parser, we use the same word embeddings as the original papers for English and Chinese, and train 100-dimensional word embeddings for French using the structured skip-gram method of Ling et al. (2015) on French Wikipedia. Figure 1: English development set F1 by training epoch, comparing likelihood training with two exploration variants for the Top-Down parser. Softmax Margin with Exploration Finally, we train using a combination of softmax margin loss augmentation and exploration. We perform the same sample-based candidate generation as for policy gradient and likelihood training with exploration, but use Eq. 2 to compute the training loss for candidate states. For those parsers that have a dynamic oracle, this provides a means of training that more directly provides both exploration and c"
P18-2075,P17-1076,1,0.0658021,"for example F1. A large family of techniques address the exposure bias problem by allowing the model to make mistakes and explore incorrect states during training, supervising actions at the resulting states using an expert policy (Daum´e III et al., 2009; Ross et al., 2011; Choi and Palmer, 2011; Chang et al., 2015); these expert policies are typically referred to as dynamic oracles in parsing (Goldberg and Nivre, 2012; Ballesteros et al., 2016). While dynamic oracles have produced substantial improvements in constituency parsing performance (Coavoux and Crabb´e, 2016; Cross and Huang, 2016; Stern et al., 2017; Gonz´alez and G´omez-Rodr´ıguez, 2018), they must be custom designed for each transition system. To address the loss mismatch problem, another line of work has directly optimized for structurelevel cost functions (Goodman, 1996; Och, 2003). Recent methods applied to models that produce output sequentially commonly use policy gradient (Auli and Gao, 2014; Ranzato et al., 2016; Shen et al., 2016) or beam search (Xu et al., 2016; Wiseman and Rush, 2016; Edunov et al., 2017) at training time to minimize a structured cost. These methods also reduce exposure bias through exploration but do not req"
P18-2075,Q17-1029,0,0.3905,"y parsers in three languages, the method substantially outperforms static oracle likelihood training in almost all settings. For parsers where a dynamic oracle is available (including a novel oracle which we define for the transition system of Dyer et al. (2016)), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle. 1 Introduction Many recent state-of-the-art models for constituency parsing are transition based, decomposing production of each parse tree into a sequence of action decisions (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017), building on a long line of work in transition-based parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Henderson, 2004; Zhang and Clark, 2011; Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016). However, models of this type, which decompose structure prediction into sequential decisions, can be prone to two issues (Ranzato et al., 2016; Wiseman and Rush, 2016). The first is exposure bias: if, at training time, the model only observes 469 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 4"
P18-2075,N03-1033,1,0.11853,", 5, 10} (where k also counts the sentence’s gold tree, included in the candidate set). See Appendix A in the supplemental material for the values of k used. likelihood likelihood + explore, k=10 policy gradient, k=10 30 40 50 Tags, Embeddings, and Morphology We largely follow previous work for each parser in our use of predicted part-of-speech tags, pretrained word embeddings, and morphological features. All parsers use predicted part-of-speech tags as part of their sentence representations. For English and Chinese, we follow the setup of Cross and Huang (2016): training the Stanford tagger (Toutanova et al., 2003) on the training set of each parsing corpus to predict development and test set tags, and using 10-way jackknifing to predict tags for the training set. For French, we use the predicted tags and morphological features provided with the SPMRL dataset (Seddah et al., 2014). We modified the publicly released code for all parsers to use predicted morphological features for French. We follow the approach outlined by Cross and Huang (2016) and Stern et al. (2017) for representing morphological features as learned embeddings, and use the same dimensions for these embeddings as in their papers. For RN"
P18-2075,J93-2004,0,0.06164,"er. Softmax Margin with Exploration Finally, we train using a combination of softmax margin loss augmentation and exploration. We perform the same sample-based candidate generation as for policy gradient and likelihood training with exploration, but use Eq. 2 to compute the training loss for candidate states. For those parsers that have a dynamic oracle, this provides a means of training that more directly provides both exploration and cost-aware losses. 4 Experiments We compare the constituency parsers listed in Section 2 using the above training methods. Our experiments use the English PTB (Marcus et al., 1993), French Treebank (Abeill´e et al., 2003), and Penn Chinese Treebank (CTB) Version 5.1 (Xue et al., 2005). Training To compare the training procedures as closely as possible, we train all models for a given parser in a given language from the same randomly-initialized parameter values. We train two different versions of the RNNG model: one model using size 128 for the LSTMs and hidden states (following the original work), and a larger model with size 256. We perform evaluation using greedy search in the Span-Based and Top-Down parsers, and beam search with beam size 10 for the RNNG and In-Orde"
P18-2075,W03-3017,0,0.0667967,"almost all settings. For parsers where a dynamic oracle is available (including a novel oracle which we define for the transition system of Dyer et al. (2016)), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle. 1 Introduction Many recent state-of-the-art models for constituency parsing are transition based, decomposing production of each parse tree into a sequence of action decisions (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017), building on a long line of work in transition-based parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Henderson, 2004; Zhang and Clark, 2011; Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016). However, models of this type, which decompose structure prediction into sequential decisions, can be prone to two issues (Ranzato et al., 2016; Wiseman and Rush, 2016). The first is exposure bias: if, at training time, the model only observes 469 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 469–476 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguisti"
P18-2075,P03-1021,0,0.0777745,", 2009; Ross et al., 2011; Choi and Palmer, 2011; Chang et al., 2015); these expert policies are typically referred to as dynamic oracles in parsing (Goldberg and Nivre, 2012; Ballesteros et al., 2016). While dynamic oracles have produced substantial improvements in constituency parsing performance (Coavoux and Crabb´e, 2016; Cross and Huang, 2016; Stern et al., 2017; Gonz´alez and G´omez-Rodr´ıguez, 2018), they must be custom designed for each transition system. To address the loss mismatch problem, another line of work has directly optimized for structurelevel cost functions (Goodman, 1996; Och, 2003). Recent methods applied to models that produce output sequentially commonly use policy gradient (Auli and Gao, 2014; Ranzato et al., 2016; Shen et al., 2016) or beam search (Xu et al., 2016; Wiseman and Rush, 2016; Edunov et al., 2017) at training time to minimize a structured cost. These methods also reduce exposure bias through exploration but do not require an expert policy for supervision. In this work, we apply a simple policy gradient method to train four different state-of-theart transition-based constituency parsers to maximize expected F1. We compare against training with a dynamic o"
P18-2075,D16-1137,0,0.0436488,"Missing"
P18-2075,N16-1025,0,0.253102,"os et al., 2016). While dynamic oracles have produced substantial improvements in constituency parsing performance (Coavoux and Crabb´e, 2016; Cross and Huang, 2016; Stern et al., 2017; Gonz´alez and G´omez-Rodr´ıguez, 2018), they must be custom designed for each transition system. To address the loss mismatch problem, another line of work has directly optimized for structurelevel cost functions (Goodman, 1996; Och, 2003). Recent methods applied to models that produce output sequentially commonly use policy gradient (Auli and Gao, 2014; Ranzato et al., 2016; Shen et al., 2016) or beam search (Xu et al., 2016; Wiseman and Rush, 2016; Edunov et al., 2017) at training time to minimize a structured cost. These methods also reduce exposure bias through exploration but do not require an expert policy for supervision. In this work, we apply a simple policy gradient method to train four different state-of-theart transition-based constituency parsers to maximize expected F1. We compare against training with a dynamic oracle (both to supervise exploration and provide loss-augmentation) where one is available, including a novel dynamic oracle that we define for the top-down transition system of Dynamic orac"
P18-2075,W03-3023,0,0.0584001,"ettings. For parsers where a dynamic oracle is available (including a novel oracle which we define for the transition system of Dyer et al. (2016)), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle. 1 Introduction Many recent state-of-the-art models for constituency parsing are transition based, decomposing production of each parse tree into a sequence of action decisions (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017), building on a long line of work in transition-based parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Henderson, 2004; Zhang and Clark, 2011; Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016). However, models of this type, which decompose structure prediction into sequential decisions, can be prone to two issues (Ranzato et al., 2016; Wiseman and Rush, 2016). The first is exposure bias: if, at training time, the model only observes 469 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 469–476 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Dyer et al. (2016). We fi"
P18-2075,J11-1005,0,0.0117657,"s available (including a novel oracle which we define for the transition system of Dyer et al. (2016)), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle. 1 Introduction Many recent state-of-the-art models for constituency parsing are transition based, decomposing production of each parse tree into a sequence of action decisions (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017), building on a long line of work in transition-based parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Henderson, 2004; Zhang and Clark, 2011; Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016). However, models of this type, which decompose structure prediction into sequential decisions, can be prone to two issues (Ranzato et al., 2016; Wiseman and Rush, 2016). The first is exposure bias: if, at training time, the model only observes 469 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 469–476 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Dyer et al. (2016). We find that while policy gradient usually ou"
P19-1031,A00-2018,0,0.383445,"g section of the Penn Chinese Treebank (CTB) v5.1 (Xue et al., 2005), consisting primarily of newswire. For out-of-domain evaluation on Chinese, we use treebank domains introduced in CTB versions 7 and 8: broadcast conversations (B. Conv), broadcast news (B. News), web discussion forums (Forums) and weblogs (Blogs). Experimental setup We compare the generalization of strong nonneural parsers against recent state-of-the-art neural parsers on English and Chinese corpora. Non-neural models We use publicly released code and models for the Berkeley Parser (Petrov and Klein, 2007) and BLLIP Parser (Charniak, 2000; Charniak and Johnson, 2005) for English; and ZPar (Zhang and Clark, 2011) for Chinese. Neural models We use two state-of-the-art neural models: the Chart model of Kitaev and Klein (2018), and In-Order shift-reduce model of Liu and Zhang (2017). These parsers differ in their modeling both of input sentences and output structures. The Chart model uses a self-attentive encoder over the input sentence, and does not explicitly model output structure correlations, predicting tree span labels independently conditioned 1 The only joint constraint on span predictions is to ensure they constitute a va"
P19-1031,P05-1022,0,0.0655123,"Penn Chinese Treebank (CTB) v5.1 (Xue et al., 2005), consisting primarily of newswire. For out-of-domain evaluation on Chinese, we use treebank domains introduced in CTB versions 7 and 8: broadcast conversations (B. Conv), broadcast news (B. News), web discussion forums (Forums) and weblogs (Blogs). Experimental setup We compare the generalization of strong nonneural parsers against recent state-of-the-art neural parsers on English and Chinese corpora. Non-neural models We use publicly released code and models for the Berkeley Parser (Petrov and Klein, 2007) and BLLIP Parser (Charniak, 2000; Charniak and Johnson, 2005) for English; and ZPar (Zhang and Clark, 2011) for Chinese. Neural models We use two state-of-the-art neural models: the Chart model of Kitaev and Klein (2018), and In-Order shift-reduce model of Liu and Zhang (2017). These parsers differ in their modeling both of input sentences and output structures. The Chart model uses a self-attentive encoder over the input sentence, and does not explicitly model output structure correlations, predicting tree span labels independently conditioned 1 The only joint constraint on span predictions is to ensure they constitute a valid tree. 324 F1 ZPar ∆ Err."
P19-1031,D16-1257,0,0.0752079,"bracketing F1 versus minimum span length for the English corpora. F1 scores for the In-Order parser with BERT (orange) and the Chart parser with BERT (cyan) start to diverge for longer spans. Brown Test Genia Test EWT Test prior work Chart +BERT In-Order +BERT 87.7 (C+’15) 79.4 (C+’15) 83.5 (L+’12) 93.16 86.11 89.13 93.66 86.45 89.62 We note that these systems from prior work (Choe et al., 2015; Petrov and McDonald, 2012; Le Roux et al., 2012) use additional ensembling or selftraining techniques, which have also been shown to be compatible with neural constituency parsers (Dyer et al., 2016; Choe and Charniak, 2016; Fried et al., 2017; Kitaev et al., 2019) and may provide benefits orthogonal to the pre-trained representations and structured models we analyze here. Encouragingly, parser improvements on the WSJ and CTB treebanks still transfer out-of-domain, indicating that improving results on these benchmarks may still continue to yield benefits in broader domains. Table 6: Comparison of F1 scores for neural models with BERT pretraining to past state-of-the art results on transfer to the out-of-domain treebanks: (C+’15: Choe et al. 2015, L+’12: Le Roux et al. 2012).6 EWT scores are averaged across the 3"
P19-1031,P04-1013,0,0.159257,"Missing"
P19-1031,D15-1160,0,0.0160466,"0.9 0.8 Labelled span F1 Labelled span F1 0.99 Labelled span F1 chart inorder 1.00 0.70 40 60 80 Minimum span length 100 120 140 (c) EWT All 0 20 40 60 80 Minimum span length 100 120 140 (d) Genia All Figure 1: Labelled bracketing F1 versus minimum span length for the English corpora. F1 scores for the In-Order parser with BERT (orange) and the Chart parser with BERT (cyan) start to diverge for longer spans. Brown Test Genia Test EWT Test prior work Chart +BERT In-Order +BERT 87.7 (C+’15) 79.4 (C+’15) 83.5 (L+’12) 93.16 86.11 89.13 93.66 86.45 89.62 We note that these systems from prior work (Choe et al., 2015; Petrov and McDonald, 2012; Le Roux et al., 2012) use additional ensembling or selftraining techniques, which have also been shown to be compatible with neural constituency parsers (Dyer et al., 2016; Choe and Charniak, 2016; Fried et al., 2017; Kitaev et al., 2019) and may provide benefits orthogonal to the pre-trained representations and structured models we analyze here. Encouragingly, parser improvements on the WSJ and CTB treebanks still transfer out-of-domain, indicating that improving results on these benchmarks may still continue to yield benefits in broader domains. Table 6: Comparis"
P19-1031,P18-1110,0,0.256969,"Missing"
P19-1031,D16-1001,0,0.0317943,"pically generalizes better to longer spans and out-of-domain treebanks, and has higher exact match accuracies in all domains. on the encoded input.1 The In-Order shift-reduce model of Liu and Zhang (2017) uses a simpler LSTM-based encoding of the input sentence but a decoder that explicitly conditions on previously constructed structure of the output tree, obtaining the best performance among similarly structured models (Dyer et al., 2016; Kuncoro et al., 2017). The In-Order model conditions on predicted part-of-speech tags; we use tags predicted by the Stanford tagger (following the setup of Cross and Huang (2016)). At test time, we use Viterbi decoding for the Chart model and beam search with beam size 10 for the In-Order model. To control for randomness in the training procedure of the neural parsers, all scores reported in the remainder of the paper for the Chart and InOrder parsers are averaged across five copies of each model trained from separate random initializations. 2 Corpora The English parsers are trained on the WSJ training section of the Penn Treebank. We perform in-domain evaluation of these parsers on the WSJ test section, and out-of-domain evaluation using the Brown, Genia, and English"
P19-1031,P19-1340,1,0.895954,"ally provide a larger error reduction out-of-domain than in-domain. 4 Exact Match Chart In-Order +BERT +BERT WSJ Test Brown All EWT All Genia All 95.64 93.10 88.72 87.54 95.71 93.54 89.27 87.75 55.11 49.23 41.83 17.46 57.05 51.98 43.98 18.03 CTB Test B. News Forums Blogs B. Conv. 92.14 88.21 86.72 84.28 76.35 91.81 88.41 87.04 84.29 75.88 44.42 15.91 20.00 17.14 17.24 44.94 17.29 21.95 18.85 18.99 Table 5: F1 and exact match accuracies comparing the Chart (unstructured) and In-Order (structured) parsers with BERT pretraining on English (top) and Chinese (bottom) corpora. trained BERT encoder (Kitaev et al., 2019), using the publicly-released code3 to train and evaluate both models. For the In-Order parser, we introduce a novel integration of a BERT encoder with the parser’s structured tree decoder. These architectures represent the best-performing types of encoder and decoder, respectively, from past work on constituency parsing, but have not been previously combined. We replace the word embeddings and predicted part-of-speech tags in the InOrder parser’s stack and buffer representations with BERT’s contextual embeddings. See Appendix A.1 for details on the architecture. Code and trained models for th"
P19-1031,N19-1423,0,0.16963,"lization than neural parsers? We might expect neural systems to generalize poorly because they are highly-parameterized, and may overfit to their training domain. We find that neural and non-neural parsers generalize similarly, and, encouragingly, improvements on indomain treebanks still transfer to out-of-domain. Second, does pre-training particularly improve out-of-domain performance, or does it just generally improve test accuracies? Neural parsers incorporate rich representations of language that can easily be pre-trained on large unlabeled corpora (Ling et al., 2015; Peters et al., 2018; Devlin et al., 2019) and improve accuracies in new domains (Joshi et al., 2018). Past work has shown that lexical supervision on an out-of-domain treebank can substantially improve parser performance (Rimell and Clark, 2009). Similarly, we might expect pre-trained language representations to give the largest improvements on out-of-domain treebanks, by providing representations of language disparate from the training domains. Surprisingly, however, we find that pre-trained representations give similar error reductions across domains. Neural parsers obtain state-of-the-art results on benchmark treebanks for constit"
P19-1031,P18-1249,1,0.862589,"performance on WSJ (BLLIP and In-Order) have comparable generalization out-of-domain, despite one being neural and one non-neural. Finally, how much does structured prediction help neural parsers? While neural models with rich modeling of syntactic structure have obtained strong performance on parsing (Dyer et al., 2016; Liu and Zhang, 2017) and a range of related tasks (Kuncoro et al., 2018; Hale et al., 2018), recent neural parsers obtain state-of-the-art F1 on benchmark datasets using rich input encoders without any explicit modeling of correlations in output structure (Shen et al., 2018; Kitaev and Klein, 2018). Does structural modeling still improve parsing performance even with these strong encoder representations? We find that, yes, while structured and unstructured neural models (using the same encoder representations) obtain similar F1 on in-domain datasets, the structured model typically generalizes better to longer spans and out-of-domain treebanks, and has higher exact match accuracies in all domains. on the encoded input.1 The In-Order shift-reduce model of Liu and Zhang (2017) uses a simpler LSTM-based encoding of the input sentence but a decoder that explicitly conditions on previously co"
P19-1031,E17-1117,0,0.0512556,"Missing"
P19-1031,N16-1024,0,0.251007,"erformance and relative increase in error (both given by F1) on English corpora as parsers are evaluated out-of-domain, relative to performance on the in-domain WSJ Test set. Improved performance on WSJ Test translates to improved performance out-of-domain. The two parsers with similar absolute performance on WSJ (BLLIP and In-Order) have comparable generalization out-of-domain, despite one being neural and one non-neural. Finally, how much does structured prediction help neural parsers? While neural models with rich modeling of syntactic structure have obtained strong performance on parsing (Dyer et al., 2016; Liu and Zhang, 2017) and a range of related tasks (Kuncoro et al., 2018; Hale et al., 2018), recent neural parsers obtain state-of-the-art F1 on benchmark datasets using rich input encoders without any explicit modeling of correlations in output structure (Shen et al., 2018; Kitaev and Klein, 2018). Does structural modeling still improve parsing performance even with these strong encoder representations? We find that, yes, while structured and unstructured neural models (using the same encoder representations) obtain similar F1 on in-domain datasets, the structured model typically generalize"
P19-1031,P18-1132,0,0.0237445,"Missing"
P19-1031,P17-2025,1,0.880769,"imum span length for the English corpora. F1 scores for the In-Order parser with BERT (orange) and the Chart parser with BERT (cyan) start to diverge for longer spans. Brown Test Genia Test EWT Test prior work Chart +BERT In-Order +BERT 87.7 (C+’15) 79.4 (C+’15) 83.5 (L+’12) 93.16 86.11 89.13 93.66 86.45 89.62 We note that these systems from prior work (Choe et al., 2015; Petrov and McDonald, 2012; Le Roux et al., 2012) use additional ensembling or selftraining techniques, which have also been shown to be compatible with neural constituency parsers (Dyer et al., 2016; Choe and Charniak, 2016; Fried et al., 2017; Kitaev et al., 2019) and may provide benefits orthogonal to the pre-trained representations and structured models we analyze here. Encouragingly, parser improvements on the WSJ and CTB treebanks still transfer out-of-domain, indicating that improving results on these benchmarks may still continue to yield benefits in broader domains. Table 6: Comparison of F1 scores for neural models with BERT pretraining to past state-of-the art results on transfer to the out-of-domain treebanks: (C+’15: Choe et al. 2015, L+’12: Le Roux et al. 2012).6 EWT scores are averaged across the 3 SANCL’12 test sets,"
P19-1031,W01-0521,0,0.580394,"Missing"
P19-1031,N15-1142,0,0.0918926,"parsers have better out-ofdomain generalization than neural parsers? We might expect neural systems to generalize poorly because they are highly-parameterized, and may overfit to their training domain. We find that neural and non-neural parsers generalize similarly, and, encouragingly, improvements on indomain treebanks still transfer to out-of-domain. Second, does pre-training particularly improve out-of-domain performance, or does it just generally improve test accuracies? Neural parsers incorporate rich representations of language that can easily be pre-trained on large unlabeled corpora (Ling et al., 2015; Peters et al., 2018; Devlin et al., 2019) and improve accuracies in new domains (Joshi et al., 2018). Past work has shown that lexical supervision on an out-of-domain treebank can substantially improve parser performance (Rimell and Clark, 2009). Similarly, we might expect pre-trained language representations to give the largest improvements on out-of-domain treebanks, by providing representations of language disparate from the training domains. Surprisingly, however, we find that pre-trained representations give similar error reductions across domains. Neural parsers obtain state-of-the-art"
P19-1031,P18-1254,0,0.014094,"re evaluated out-of-domain, relative to performance on the in-domain WSJ Test set. Improved performance on WSJ Test translates to improved performance out-of-domain. The two parsers with similar absolute performance on WSJ (BLLIP and In-Order) have comparable generalization out-of-domain, despite one being neural and one non-neural. Finally, how much does structured prediction help neural parsers? While neural models with rich modeling of syntactic structure have obtained strong performance on parsing (Dyer et al., 2016; Liu and Zhang, 2017) and a range of related tasks (Kuncoro et al., 2018; Hale et al., 2018), recent neural parsers obtain state-of-the-art F1 on benchmark datasets using rich input encoders without any explicit modeling of correlations in output structure (Shen et al., 2018; Kitaev and Klein, 2018). Does structural modeling still improve parsing performance even with these strong encoder representations? We find that, yes, while structured and unstructured neural models (using the same encoder representations) obtain similar F1 on in-domain datasets, the structured model typically generalizes better to longer spans and out-of-domain treebanks, and has higher exact match accuracies i"
P19-1031,Q17-1029,0,0.0437398,"tive increase in error (both given by F1) on English corpora as parsers are evaluated out-of-domain, relative to performance on the in-domain WSJ Test set. Improved performance on WSJ Test translates to improved performance out-of-domain. The two parsers with similar absolute performance on WSJ (BLLIP and In-Order) have comparable generalization out-of-domain, despite one being neural and one non-neural. Finally, how much does structured prediction help neural parsers? While neural models with rich modeling of syntactic structure have obtained strong performance on parsing (Dyer et al., 2016; Liu and Zhang, 2017) and a range of related tasks (Kuncoro et al., 2018; Hale et al., 2018), recent neural parsers obtain state-of-the-art F1 on benchmark datasets using rich input encoders without any explicit modeling of correlations in output structure (Shen et al., 2018; Kitaev and Klein, 2018). Does structural modeling still improve parsing performance even with these strong encoder representations? We find that, yes, while structured and unstructured neural models (using the same encoder representations) obtain similar F1 on in-domain datasets, the structured model typically generalizes better to longer spa"
P19-1031,P17-1076,1,0.920164,"Missing"
P19-1031,J93-2004,0,0.0653806,"ly, despite the rich input representations they learn, neural parsers still benefit from structured output prediction of output trees, yielding higher exact match accuracy and stronger generalization both to larger text spans and to out-of-domain corpora. We analyze generalization on English and Chinese corpora, and in the process obtain state-of-the-art parsing results for the Brown, Genia, and English Web treebanks. 1 Introduction Neural constituency parsers have obtained increasingly high performance when measured by F1 scores on in-domain benchmarks, such as the Wall Street Journal (WSJ) (Marcus et al., 1993) and Penn Chinese Treebank (CTB) (Xue et al., 2005). However, in order to construct systems useful for cross-domain NLP, we seek parsers that generalize well to domains other than the ones they were trained on. While classical, non-neural parsers are known to perform better in their training domains than on out-of-domain corpora, their out-ofdomain performance degrades in well-understood ways (Gildea, 2001; Petrov and Klein, 2007), and improvements in performance on in-domain ∗ Equal contribution. 323 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, page"
P19-1031,P06-1043,0,0.0747211,"ter out-of-domain than in-domain. How well do neural parsers generalize? Table 1 compares the generalization performance of the English parsers, both non-neural (Berkeley, BLLIP) and neural (Chart, In-Order). None of these parsers use additional data beyond the WSJ training section of the PTB: we use the version of the BLLIP parser without self-training on unlabeled data, and use the In-Order parser without external pre-trained word embeddings. Across all parsers, higher performance on the WSJ Test set corresponds to higher performance on each outof-domain corpus, showing that the findings of McClosky et al. (2006) extend to recent neural parsers. In particular, the Chart parser has highest performance in all four domains. The ∆ Err. column shows the generalization gap for each parser on each corpus: the parser’s relative increase in error (with error defined by 100 − F1) from the WSJ Test set (lower values are better). Improved performance on the WSJ Test set corresponds to increased generalization gaps, indicating that to some extent parser improvements on WSJ have come at the expense of out-ofdomain generalization. However, the two parsers with similar absolute performance on WSJ—the BLLIP parser and"
P19-1031,I05-2038,0,0.176412,"in}@cs.berkeley.edu Abstract treebanks still transfer to out-of-domain improvements (McClosky et al., 2006). Is the success of neural constituency parsers (Henderson 2004; Vinyals et al. 2015; Dyer et al. 2016; Cross and Huang 2016; Choe and Charniak 2016; Stern et al. 2017; Liu and Zhang 2017; Kitaev and Klein 2018, inter alia) similarly transferable to out-of-domain treebanks? In this work, we focus on zero-shot generalization: training parsers on a single treebank (e.g. WSJ) and evaluating on a range of broad-coverage, out-of-domain treebanks (e.g. Brown (Francis and Kuˇcera, 1979), Genia (Tateisi et al., 2005), the English Web Treebank (Petrov and McDonald, 2012)). We ask three questions about zero-shot generalization properties of state-of-the-art neural constituency parsers: First, do non-neural parsers have better out-ofdomain generalization than neural parsers? We might expect neural systems to generalize poorly because they are highly-parameterized, and may overfit to their training domain. We find that neural and non-neural parsers generalize similarly, and, encouragingly, improvements on indomain treebanks still transfer to out-of-domain. Second, does pre-training particularly improve out-of"
P19-1031,J11-1005,0,0.0205144,"), consisting primarily of newswire. For out-of-domain evaluation on Chinese, we use treebank domains introduced in CTB versions 7 and 8: broadcast conversations (B. Conv), broadcast news (B. News), web discussion forums (Forums) and weblogs (Blogs). Experimental setup We compare the generalization of strong nonneural parsers against recent state-of-the-art neural parsers on English and Chinese corpora. Non-neural models We use publicly released code and models for the Berkeley Parser (Petrov and Klein, 2007) and BLLIP Parser (Charniak, 2000; Charniak and Johnson, 2005) for English; and ZPar (Zhang and Clark, 2011) for Chinese. Neural models We use two state-of-the-art neural models: the Chart model of Kitaev and Klein (2018), and In-Order shift-reduce model of Liu and Zhang (2017). These parsers differ in their modeling both of input sentences and output structures. The Chart model uses a self-attentive encoder over the input sentence, and does not explicitly model output structure correlations, predicting tree span labels independently conditioned 1 The only joint constraint on span predictions is to ensure they constitute a valid tree. 324 F1 ZPar ∆ Err. In-Order F1 In-Order F1 ∆ Err. CTB Test 83.01"
P19-1031,N18-1202,0,0.0313336,"r out-ofdomain generalization than neural parsers? We might expect neural systems to generalize poorly because they are highly-parameterized, and may overfit to their training domain. We find that neural and non-neural parsers generalize similarly, and, encouragingly, improvements on indomain treebanks still transfer to out-of-domain. Second, does pre-training particularly improve out-of-domain performance, or does it just generally improve test accuracies? Neural parsers incorporate rich representations of language that can easily be pre-trained on large unlabeled corpora (Ling et al., 2015; Peters et al., 2018; Devlin et al., 2019) and improve accuracies in new domains (Joshi et al., 2018). Past work has shown that lexical supervision on an out-of-domain treebank can substantially improve parser performance (Rimell and Clark, 2009). Similarly, we might expect pre-trained language representations to give the largest improvements on out-of-domain treebanks, by providing representations of language disparate from the training domains. Surprisingly, however, we find that pre-trained representations give similar error reductions across domains. Neural parsers obtain state-of-the-art results on benchmark"
P19-1031,N07-1051,1,0.564412,"Chinese parsers are trained on the training section of the Penn Chinese Treebank (CTB) v5.1 (Xue et al., 2005), consisting primarily of newswire. For out-of-domain evaluation on Chinese, we use treebank domains introduced in CTB versions 7 and 8: broadcast conversations (B. Conv), broadcast news (B. News), web discussion forums (Forums) and weblogs (Blogs). Experimental setup We compare the generalization of strong nonneural parsers against recent state-of-the-art neural parsers on English and Chinese corpora. Non-neural models We use publicly released code and models for the Berkeley Parser (Petrov and Klein, 2007) and BLLIP Parser (Charniak, 2000; Charniak and Johnson, 2005) for English; and ZPar (Zhang and Clark, 2011) for Chinese. Neural models We use two state-of-the-art neural models: the Chart model of Kitaev and Klein (2018), and In-Order shift-reduce model of Liu and Zhang (2017). These parsers differ in their modeling both of input sentences and output structures. The Chart model uses a self-attentive encoder over the input sentence, and does not explicitly model output structure correlations, predicting tree span labels independently conditioned 1 The only joint constraint on span predictions"
P19-1031,P18-1108,0,0.0313873,"th similar absolute performance on WSJ (BLLIP and In-Order) have comparable generalization out-of-domain, despite one being neural and one non-neural. Finally, how much does structured prediction help neural parsers? While neural models with rich modeling of syntactic structure have obtained strong performance on parsing (Dyer et al., 2016; Liu and Zhang, 2017) and a range of related tasks (Kuncoro et al., 2018; Hale et al., 2018), recent neural parsers obtain state-of-the-art F1 on benchmark datasets using rich input encoders without any explicit modeling of correlations in output structure (Shen et al., 2018; Kitaev and Klein, 2018). Does structural modeling still improve parsing performance even with these strong encoder representations? We find that, yes, while structured and unstructured neural models (using the same encoder representations) obtain similar F1 on in-domain datasets, the structured model typically generalizes better to longer spans and out-of-domain treebanks, and has higher exact match accuracies in all domains. on the encoded input.1 The In-Order shift-reduce model of Liu and Zhang (2017) uses a simpler LSTM-based encoding of the input sentence but a decoder that explicitly co"
P19-1188,D17-1311,1,0.89784,"Missing"
P19-1188,N18-1197,1,0.882986,"Missing"
P19-1188,Q13-1005,0,0.218333,"Missing"
P19-1188,P18-1175,0,0.022776,"Missing"
P19-1188,D16-1116,0,0.0334801,"Missing"
P19-1188,D17-1321,0,0.0286117,"Missing"
P19-1188,D18-1287,0,0.0767588,"Missing"
P19-1188,D17-1161,0,0.0463294,"Missing"
P19-1188,P16-1224,0,0.136199,"mation may not be easily extractable from a prior. For example, a prior can tell you that stacking red blocks on orange blocks is likely across a range of initial configurations, but our pre-training method may also choose to represent all of these transitions with the same vector a. Finding this underly1948 ing structure is key to the generalization improvements seen with our procedure. 4 Experiments We evaluate our method in two different environments, as described below in sections 4.1 and 4.2.1 4.1 Block Stacking For our first test environment, we use the block stacking task introduced by Wang et al. (2016) and depicted in Figure 1. This environment consists of a series of levels (tasks), where each level requires adding or removing blocks to get from a start configuration to a goal configuration. Human annotators were told to give the computer step by step instructions on how to move blocks from one configuration to the other. After each instruction, the annotator selected the desired resulting state from a list. Following the original work for this dataset (Wang et al., 2016), we adopt an online learning setup and metric. The data is broken up into a number of sessions, one for each human anno"
P19-1340,D12-1091,1,0.874317,"Missing"
P19-1340,W14-6110,0,0.0753304,"Missing"
P19-1340,W13-4916,0,0.0781728,"Missing"
P19-1340,Q17-1010,0,0.0179738,"parsing, our earlier results only consider the LSTM-based ELMo representations (Peters et al., 2018), and only for the English language. In this work, we study a broader range of pre-training conditions and experiment over a variety of languages, both jointly and individually. First, we consider the impact on parsing of using different methods for pre-training initial network layers on a large collection of un-annotated text. Here, we see that pre-training provides benefits for all languages evaluated, and that BERT (Devlin et al., 2018a) outperforms ELMo, which in turn outperforms fastText (Bojanowski et al., 2017; Mikolov et al., 2018), which performs slightly better than the non pre-trained baselines. Pre-training with a larger model capacity typically leads to higher parsing accuracies. Second, we consider various schemes for the parser fine-tuning that is required after pretraining. While BERT itself can be pre-trained jointly on many languages, successfully applying it, e.g. to parsing, requires task-specific adaptation via fine-tuning (Devlin et al., 2018a). Therefore, the obvious approach to parsing ten languages is to fine-tune ten times, producing ten variants of the parameter-heavy BERT layer"
P19-1340,D16-1257,0,0.0531862,"Missing"
P19-1340,E17-2053,0,0.108502,"Missing"
P19-1340,N16-1024,0,0.122583,"Missing"
P19-1340,P18-2075,1,0.898978,"Missing"
P19-1340,P17-2025,1,0.921306,"Missing"
P19-1340,P18-1031,0,0.0528554,"Missing"
P19-1340,P18-1110,0,0.109405,"Missing"
P19-1340,P18-1249,1,0.738326,"error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-ofthe-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1). 1 Introduction There has recently been rapid progress in developing contextual word representations that improve accuracy across a range of natural language tasks (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018a). While we have shown in previous work (Kitaev and Klein, 2018) that such representations are beneficial for constituency parsing, our earlier results only consider the LSTM-based ELMo representations (Peters et al., 2018), and only for the English language. In this work, we study a broader range of pre-training conditions and experiment over a variety of languages, both jointly and individually. First, we consider the impact on parsing of using different methods for pre-training initial network layers on a large collection of un-annotated text. Here, we see that pre-training provides benefits for all languages evaluated, and that BERT (Devlin et al., 201"
P19-1340,Q17-1029,0,0.114844,"Missing"
P19-1340,L18-1008,0,0.039165,"ults only consider the LSTM-based ELMo representations (Peters et al., 2018), and only for the English language. In this work, we study a broader range of pre-training conditions and experiment over a variety of languages, both jointly and individually. First, we consider the impact on parsing of using different methods for pre-training initial network layers on a large collection of un-annotated text. Here, we see that pre-training provides benefits for all languages evaluated, and that BERT (Devlin et al., 2018a) outperforms ELMo, which in turn outperforms fastText (Bojanowski et al., 2017; Mikolov et al., 2018), which performs slightly better than the non pre-trained baselines. Pre-training with a larger model capacity typically leads to higher parsing accuracies. Second, we consider various schemes for the parser fine-tuning that is required after pretraining. While BERT itself can be pre-trained jointly on many languages, successfully applying it, e.g. to parsing, requires task-specific adaptation via fine-tuning (Devlin et al., 2018a). Therefore, the obvious approach to parsing ten languages is to fine-tune ten times, producing ten variants of the parameter-heavy BERT layers. In this work, we com"
P19-1340,N18-1202,0,0.241482,"Missing"
P19-1340,C18-1011,0,0.0471703,"Missing"
P19-1655,D15-1138,1,0.899681,"Missing"
P19-1655,Q13-1005,0,0.14692,"Missing"
P19-1655,P15-2017,0,0.0325102,"valuated by metrics such as success rate. However, it is unclear where the high performance comes from. In this paper, we find that agents without any visual input can achieve competitive performance, matching or even outperforming their vision-based counterparts under two state-of-theart model models (Fried et al., 2018b; Ma et al., 2019). We also explore two approaches to make the agents better utilize their visual inputs. The role of vision in vision-and-language tasks. In several vision-and-language tasks, high performance can be achieved without effective modeling of the visual modality. Devlin et al. (2015) find that image captioning models can exploit regularity in the captions, showing that a nearestneighbor matching approach can achieve competitive performance to sophisticated language generation models. Hendricks et al. (2018) and Rohrbach et al. (2018) find that neural captioning models often ground object mentions into incorrect objects due to correlations in the training data, and can hallucinate non-existing objects. Recent work has also investigated singlemodality performance in vision-and-language embodiment tasks. Anand et al. (2018) find that stateof-the-art results can be achieved o"
P19-1655,N18-1177,1,0.393695,"proach. Introduction The Vision-and-Language Navigation (VLN) task (Anderson et al., 2018) requires an agent to navigate to a particular location in a real-world environment, following complex, context-dependent instructions written by humans (e.g. go down the second hallway on the left, enter the bedroom and stop by the mirror). The agent must navigate through the environment, conditioning on the instruction as well as the visual imagery that it observes along the route, to stop at the location specified by the instruction (e.g. the mirror). Recent state-of-the-art models (Wang et al., 2018; Fried et al., 2018b; Ma et al., 2019) have demonstrated large gains in accuracy on the VLN task. However, it is unclear which modality these substantial increases in task metrics can be attributed to, and, in particular, whether the gains in performance are due to stronger grounding into visual context or e.g. simply into the discrete, geometric structure of possible routes, such as turning left or moving forward (see Fig. 1, top vs. middle). First, we analyze to what extent VLN models ground language into visual appearance and route structure by training versions of two state-ofthe-art models without visual fe"
P19-1655,W14-4012,0,0.0744407,"Missing"
P19-1655,D18-1287,0,0.110569,"Missing"
P19-1655,D14-1162,0,0.0933918,"on with a set of visual features {ximg,i }, where ximg,i is a vector extracted from an image patch at a particular orientation i using a CNN. Both models also use a visual attention mechanism to extract an attended visual feature ximg,att from {ximg,i }. For our objectbased representation, we use a Faster R-CNN (Ren et al., 2015) object detector trained on the Visual Genome dataset (Krishna et al., 2017). We construct a set of vectors {xobj,j } representing detected objects and their attributes. Each vector xobj,j (j-th detected object in the scene) is a concatenation of summed GloVe vectors (Pennington et al., 2014) for the detected object label (e.g. door) and attribute labels (e.g. white) and a location vector from the object’s bounding box coordinates. We then use the same visual attention mechanism as in Fried et al. (2018b) and Ma et al. (2019) to obtain an attended object representation xobj,att over these {xobj,j } vectors. We either substitute the ResNet CNN features ximg,att (“RN”) with our object representation xobj,att (“Obj”), or concatenate ximg,att and xobj,att (“RN+Obj”). Then we train the SF model or the SM model using this object representation, with results shown in Table 2.3 For SF (li"
P19-1655,D18-1437,1,0.780937,"unterparts under two state-of-theart model models (Fried et al., 2018b; Ma et al., 2019). We also explore two approaches to make the agents better utilize their visual inputs. The role of vision in vision-and-language tasks. In several vision-and-language tasks, high performance can be achieved without effective modeling of the visual modality. Devlin et al. (2015) find that image captioning models can exploit regularity in the captions, showing that a nearestneighbor matching approach can achieve competitive performance to sophisticated language generation models. Hendricks et al. (2018) and Rohrbach et al. (2018) find that neural captioning models often ground object mentions into incorrect objects due to correlations in the training data, and can hallucinate non-existing objects. Recent work has also investigated singlemodality performance in vision-and-language embodiment tasks. Anand et al. (2018) find that stateof-the-art results can be achieved on the EmbodiedQA task (Das et al., 2018) using an agent without visual inputs. Work concurrent to ours evaluates the performance of single-modality models for several embodied tasks including VLN (Thomason et al., 2019), finding that high performance can"
P19-1655,N19-1197,0,0.0457544,"models. Hendricks et al. (2018) and Rohrbach et al. (2018) find that neural captioning models often ground object mentions into incorrect objects due to correlations in the training data, and can hallucinate non-existing objects. Recent work has also investigated singlemodality performance in vision-and-language embodiment tasks. Anand et al. (2018) find that stateof-the-art results can be achieved on the EmbodiedQA task (Das et al., 2018) using an agent without visual inputs. Work concurrent to ours evaluates the performance of single-modality models for several embodied tasks including VLN (Thomason et al., 2019), finding that high performance can be achieved on the R2R dataset using a non-visual version of the baseline model (Anderson et al., 2018). In this paper, we show that the same trends hold for two recent state-of-the-art architectures (Ma et al., 2019; Fried et al., 2018b) for the VLN task; we also analyze to what extent object-based representations and mixture-ofexperts methods can address these issues. 3 State-of-the-art VLN models do not use vision effectively We experiment with the benchmark Room-toRoom (R2R) dataset (Anderson et al., 2018) for the Vision-and-Language navigation task, whi"
P19-1655,P10-1083,0,0.189945,"Missing"
Q14-1037,P12-1041,1,0.831013,"al., 2012). This corpus does not contain gold-standard entity links, so we cannot evaluate this portion of our model, though the model still exploits the information from Wikipedia to make coreference and named entity decisions. We will compare to prior coreference and named entity work in the system mentions setting. 6.1 ACE Evaluation We tokenize and sentence-split the ACE dataset using the tools bundled with Reconcile (Stoyanov et al., 2010) and parse it using the Berkeley Parser (Petrov et al., 2006). We use the train/test split from Stoyanov et al. (2009), Haghighi and Klein (2010), and Bansal and Klein (2012). FAHRNI I NDEP. J OINT ∆ over I NDEP. Prec. 81.15 80.26 83.26 +3.00 Non-NILS Rec. F1 78.10 79.60 76.30 78.23 77.67 80.37 +1.37 +2.14 NILS Prec. 41.25 33.39 35.19 +1.80 Rec. 61.10 54.47 65.42 +10.95 F1 49.25 41.40 45.77 +3.37 Accuracy 76.87 74.71 76.78 +2.07 Table 2: Detailed entity linking results on the ACE 2005 test set. We evaluate both our I NDEP. (task-specific factors only) and J OINT models and compare to the results of the FAHRNI model, a state-of-the-art entity linking system. We compare overall accuracy as well as performance at predicting NILS (mentions not in the knowledge base) a"
Q14-1037,P14-1005,0,0.243887,"Missing"
Q14-1037,N12-1004,1,0.849804,"Missing"
Q14-1037,D13-1057,0,0.0400117,"r I N DEP. and J OINT models compared to three strong systems: Durrett and Klein (2013), Fernandes et al. (2012) (the winner of the CoNLL shared task), and Bj¨orkelund and Kuhn (2014) (the best reported results on the dataset). Our J OINT method outperforms all three as well as the I NDEP. system.12 Next, we report results on named entity recognition. We use the same OntoNotes splits as for the coreference data; however, the New Testament (NT) 11 The NER-coreference portion of the model now resembles the skip-chain CRF from Finkel et al. (2005), though with soft coreference. 12 The systems of Chang et al. (2013) and Webster and Curran (2014) perform similarly to the F ERNANDES system; changes in the reference implementation of the metrics make exact comparison to printed numbers difficult. 486 I LLINOIS PASSOS I NDEP. J OINT ∆ over I NDEP. Prec. 82.00 − 83.79 85.22 +1.43 Rec. 84.95 − 81.53 82.89 +1.36 F1 83.45 82.24 82.64 84.04 +1.40 Table 5: Results for NER tagging on the OntoNotes 5.0 / CoNLL 2011 test set. We compare our systems to the Illinois system (Ratinov and Roth, 2009) and the system of Passos et al. (2014). Our model outperforms both other systems in terms of F1 , and once again joint mode"
Q14-1037,D13-1184,0,0.63446,"to the method of Durrett et al. (2013). Figure 1 shows an example of the effects such factors can capture. The non-locality of coreference factors make exact inference intractable, but we find that belief propagation is a suitable approximation technique and performs well. Our joint modeling of these three tasks is motivated by their heavy interdependencies, which have been noted in previous work (discussed more in Section 7). Entity linking has been employed for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012) and coreference for entity linking (Cheng and Roth, 2013) as part of pipelined systems. Past work has 477 Transactions of the Association for Computational Linguistics, vol. 2, pp. 477–490, 2014. Action Editor: Jason Eisner. c Submitted 8/2014; Revised 10/2014; Published November 1, 2014. 2014 Association for Computational Linguistics. en.wikipedia.org/wiki/Dell Infobox type: company en.wikipedia.org/wiki/Michael_Dell Infobox type: person ORGANIZATION PERSON Revenues of $14.5 billion were posted by Dell1. The company1 ... Figure 1: Coreference can help resolve ambiguous cases of semantic types or entity links: propagating information across corefere"
Q14-1037,D07-1074,0,0.511529,"How do we characterize the collection of entities present in a document? Two broad threads exist in the literature. The first is coreference resolution (Soon et al., 2001; Ng, 2010; Pradhan et al., 2011), which identifies clusters of mentions in a document referring to the same entity. This process gives us access to useful information about the referents of pronouns and nominal expressions, but because clusters are local to each document, it is often hard to situate document entities in a broader context. A separate line of work has considered the problem of entity linking or “Wikification” (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011), where mentions are linked to entries in a given knowledge 1 System available at http://nlp.cs.berkeley.edu In this paper, we describe a joint model of coreference, entity linking, and semantic typing (named entity recognition) using a structured conditional random field. Variables in the model capture decisions about antecedence, semantic type, and entity links for each mention. Unary factors on these variables incorporate features that are commonly employed when solving each task in isolation. Binary and higher-order factors capture interactio"
Q14-1037,H05-1013,0,0.129353,"Missing"
Q14-1037,D08-1069,0,0.0764432,"mplement high-performing models for each task. State-of-the-art approaches to coreference (Durrett and Klein, 2013) and entity linking (Ratinov et al., 2011) already have this independent structure and Ratinov and Roth (2009) note that it is a reasonable assumption to make for NER as well.6 In this section, we describe the features present in the task-specific factors of each type (which also serve as our three separate baseline systems). 3.1.1 Coreference Our modeling of the coreference output space (as antecedents chosen for each mention) follows the mention-ranking approach to coreference (Denis and Baldridge, 2008; Durrett and Klein, 2013). Our feature set is that of Durrett and Klein, targeting surface properties of mentions: for each mention, we examine the first word, head word, last word, context words, the mention’s length, and whether the mention is nominal, proper or pronominal. Anaphoricity features examine each of these properties in turn; coreference features conjoin various properties between mention pairs and also use properties of the mention pair itself, such as the distance between the mentions and whether their heads match. Note that this baseline does not rely on having access to named"
Q14-1037,C10-1032,0,0.056639,"e clusters from Koo et al. (2008), and 6) common bigrams of word shape and word identity. 6 Pairwise potentials in sequence-based NER are useful for producing coherent output (e.g. prohibiting configurations like O I - PER ), but since we have so far defined the task as operating over fixed mentions, this structural constraint does not come into play for our system. 480 3.1.3 Entity Linking Our entity linking system diverges more substantially from past work than the coreference or NER systems. Most entity linking systems operate in two distinct phases (Cucerzan, 2007; Milne and Witten, 2008; Dredze et al., 2010; Ratinov et al., 2011). First, in the candidate generation phase, a system generates a ranked set of possible candidates for a given mention by querying Wikipedia. The standard approach for doing so is to collect all hyperlinks in Wikipedia and associate each hyperlinked span of text (e.g. Michael Jordan) with a distribution over titles of Wikipedia articles it is observed to link to (Michael Jordan, Michael I. Jordan, etc.). Second, in the disambiguation phase, a learned model selects the correct candidate from the set of possibilities. As noted by Hachey et al. (2013) and Guo et al. (2013),"
Q14-1037,D13-1203,1,0.830037,"with standard factor graph notation; features over a particular set of output variables (and x) are associated with factors connected to those variables. Figure 3 shows the task-specific factors in the model, discussed next in Section 3.1. Higher-order factors coupling variables between tasks are discussed in Section 3.2. 3.1 Independent Model Figure 3 shows a version of the model with only task-specific factors. Though this framework is structurally simple, it is nevertheless powerful enough for us to implement high-performing models for each task. State-of-the-art approaches to coreference (Durrett and Klein, 2013) and entity linking (Ratinov et al., 2011) already have this independent structure and Ratinov and Roth (2009) note that it is a reasonable assumption to make for NER as well.6 In this section, we describe the features present in the task-specific factors of each type (which also serve as our three separate baseline systems). 3.1.1 Coreference Our modeling of the coreference output space (as antecedents chosen for each mention) follows the mention-ranking approach to coreference (Denis and Baldridge, 2008; Durrett and Klein, 2013). Our feature set is that of Durrett and Klein, targeting surfac"
Q14-1037,P13-1012,1,0.864729,"type, and entity links for each mention. Unary factors on these variables incorporate features that are commonly employed when solving each task in isolation. Binary and higher-order factors capture interactions between pairs of tasks. For entity linking and NER, factors capture a mapping between NER’s semantic types and Wikipedia’s semantics as described by infoboxes, categories, and article text. Coreference interacts with the other tasks in a more complex way, via factors that softly encourage consistency of semantic types and entity links across coreference arcs, similar to the method of Durrett et al. (2013). Figure 1 shows an example of the effects such factors can capture. The non-locality of coreference factors make exact inference intractable, but we find that belief propagation is a suitable approximation technique and performs well. Our joint modeling of these three tasks is motivated by their heavy interdependencies, which have been noted in previous work (discussed more in Section 7). Entity linking has been employed for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012) and coreference for entity linking (Cheng and Roth, 2013) as part of pipel"
Q14-1037,E14-1052,0,0.169282,"addressed in past joint modeling efforts (Daum´e and Marcu, 2005; Li and Ji, 2014), but that is outside the scope of the current work. 478 models. Finally, as a structured CRF, it is conceptually no more complex than its component models and its behavior can be understood using the same intuition. We apply our model to two datasets, ACE 2005 and OntoNotes, with different mention standards and layers of annotation. In both settings, our joint model outperforms our independent baseline models. On ACE, we achieve state-of-the-art entity linking results, matching the performance of the system of Fahrni and Strube (2014). On OntoNotes, we match the performance of the best published coreference system (Bj¨orkelund and Kuhn, 2014) and outperform two strong NER systems (Ratinov and Roth, 2009; Passos et al., 2014). 2 Motivating Examples We first present two examples to motivate our approach. Figure 1 shows an example of a case where coreference is beneficial for named entity recognition and entity linking. The company is clearly coreferent to Dell by virtue of the lack of other possible antecedents; this in turn indicates that Dell refers to the corporation rather than to Michael Dell. This effect can be capture"
Q14-1037,N09-1037,0,0.0137602,"s (2013) jointly model NER and entity linking in such a way that they maintain uncertainty over mention boundaries, allowing information from Wikipedia to inform segmentation choices. We could strengthen our model by integrating this capability; however, the primary cause of errors for mention detection on OntoNotes is parsing ambiguities rather than named entity ambiguities, so we would be unlikely to see improvements in the experiments presented here. Beyond maintaining uncertainty over mention boundaries, we might also consider maintaining uncertainty over the entire parse structure, as in Finkel and Manning (2009), who consider parsing and named entity recognition together with a PCFG. 8 Conclusion We return to our initial motivation for joint modeling, namely that the three tasks we address have the potential to influence one another. Table 3 shows 487 that failing to exploit any of the pairwise interactions between the tasks causes lower performance on at least one of them. Therefore, any pipelined system would necessarily underperform a joint model on whatever task came first in the pipeline, which is undesirable given the importance of these tasks. The trend towards broader and deeper NLP pipelines"
Q14-1037,P05-1045,0,0.187866,"ATION PERSON Revenues of $14.5 billion were posted by Dell1. The company1 ... Figure 1: Coreference can help resolve ambiguous cases of semantic types or entity links: propagating information across coreference arcs can inform us that, in this context, Dell is an organization and should therefore link to the article on Dell in Wikipedia. shown that tighter integration of coreference and entity linking is promising (Hajishirzi et al., 2013; Zheng et al., 2013); we extend these approaches and model the entire process more holistically. Named entity recognition is improved by simple coreference (Finkel et al., 2005; Ratinov and Roth, 2009) and knowledge from Wikipedia (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Nothman et al., 2013; Sil and Yates, 2013). Joint models of coreference and NER have been proposed in Haghighi and Klein (2010) and Durrett et al. (2013), but in neither case was supervised data used for both tasks. Technically, our model is most closely related to that of Singh et al. (2013), who handle coreference, named entity recognition, and relation extraction.2 Our system is novel in three ways: the choice of tasks to model jointly, the fact that we maintain uncertainty about all d"
Q14-1037,N10-1112,0,0.039656,"latent structure has been employed in prior work as well (Fernandes et al., 2012; Durrett and Klein, 2013). We adapt this objective to exploit parameterized loss functions for each task by modifying the distribution as follows: p0 (a, t, e|x; θ) ∝ p(a, t, e, x) exp [αc `c (a, C ∗ ) +αt `t (t, t∗ ) + αe `e (e, e∗ )] where `c , `t , and `e are task-specific loss functions with weight parameters α. This technique, softmaxmargin, allows us to shape the distribution learned by the model and encourage the model to move probability mass away from outputs that are bad according to our loss functions (Gimpel and Smith, 2010). As in Durrett and Klein (2013), we take αc = 1 and use `c as defined there, penalizing the model by αc,FA = 0.1 for linking up a mention that should have been nonanaphoric, by αc,FN = 3 for calling nonanaphoric a mention that should have an antecedent, and by αc,WL = 1 for picking the wrong antecedent for an anaphoric mention. `t and `e are simply Hamming distance, with αt = 3 and αe = 0 for all experiments. We found that the outcome of learning was not particularly sensitive to these parameters.7 We optimize our objective using AdaGrad (Duchi et al., 2011) with L1 regularization and λ = 0.0"
Q14-1037,D09-1120,1,0.347203,"ities. Integration with the rest of the model, learning, and inference would remain unchanged. However, while such features have been employed in past entity linking systems (Ratinov et al., 2011; Hoffart et al., 2011), Ratinov et al. found them to be of limited utility, so we omit them from the present work. 3.2 Cross-task Interaction Factors We now add factors that tie the predictions of multiple output variables in a feature-based way. Figure 4 shows the general structure of these factors. Each 481 • Copula in the first sentence (is a British politician); used for coreference previously in Haghighi and Klein (2009) We fire features that conjoin the information from the selected Wikipedia article with the selected NER type. Because these types of information from Wikipedia are of a moderate granularity, we should be able to learn a mapping between them and NER types and exploit Wikipedia as a soft gazetteer. 3.2.2 Coreference and NER Coreference can improve NER by ensuring consistent semantic type predictions across coreferent mentions; likewise, NER can help coreference by encouraging the system to link up mentions of the same type. The factors we implement for these purposes closely resemble the factor"
Q14-1037,N10-1061,1,0.736533,"s that, in this context, Dell is an organization and should therefore link to the article on Dell in Wikipedia. shown that tighter integration of coreference and entity linking is promising (Hajishirzi et al., 2013; Zheng et al., 2013); we extend these approaches and model the entire process more holistically. Named entity recognition is improved by simple coreference (Finkel et al., 2005; Ratinov and Roth, 2009) and knowledge from Wikipedia (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Nothman et al., 2013; Sil and Yates, 2013). Joint models of coreference and NER have been proposed in Haghighi and Klein (2010) and Durrett et al. (2013), but in neither case was supervised data used for both tasks. Technically, our model is most closely related to that of Singh et al. (2013), who handle coreference, named entity recognition, and relation extraction.2 Our system is novel in three ways: the choice of tasks to model jointly, the fact that we maintain uncertainty about all decisions throughout inference (rather than using a greedy approach), and the feature sets we deploy for cross-task interactions. In designing a joint model, we would like to preserve the modularity, efficiency, and structural simplici"
Q14-1037,D13-1029,0,0.607243,"ed November 1, 2014. 2014 Association for Computational Linguistics. en.wikipedia.org/wiki/Dell Infobox type: company en.wikipedia.org/wiki/Michael_Dell Infobox type: person ORGANIZATION PERSON Revenues of $14.5 billion were posted by Dell1. The company1 ... Figure 1: Coreference can help resolve ambiguous cases of semantic types or entity links: propagating information across coreference arcs can inform us that, in this context, Dell is an organization and should therefore link to the article on Dell in Wikipedia. shown that tighter integration of coreference and entity linking is promising (Hajishirzi et al., 2013; Zheng et al., 2013); we extend these approaches and model the entire process more holistically. Named entity recognition is improved by simple coreference (Finkel et al., 2005; Ratinov and Roth, 2009) and knowledge from Wikipedia (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Nothman et al., 2013; Sil and Yates, 2013). Joint models of coreference and NER have been proposed in Haghighi and Klein (2010) and Durrett et al. (2013), but in neither case was supervised data used for both tasks. Technically, our model is most closely related to that of Singh et al. (2013), who handle coreferenc"
Q14-1037,D11-1072,0,0.661577,"Missing"
Q14-1037,N06-2015,0,0.289643,"the latter is much more computationally difficult to find and would be largely the same, since the posterior distributions of the ai are quite peaked. 6 Experiments We present results on two corpora. First, we use the ACE 2005 corpus (NIST, 2005): this corpus annotates mentions complete with coreference, semantic types (per mention), and entity links (also per mention) later added by Bentivogli et al. (2010). We evaluate on gold mentions in this setting for comparability with prior work on entity linking; we lift this restriction in Section 6.3. Second, we evaluate on the OntoNotes 5 corpus (Hovy et al., 2006) as used in the CoNLL 2012 coreference shared task (Pradhan et al., 2012). This corpus does not contain gold-standard entity links, so we cannot evaluate this portion of our model, though the model still exploits the information from Wikipedia to make coreference and named entity decisions. We will compare to prior coreference and named entity work in the system mentions setting. 6.1 ACE Evaluation We tokenize and sentence-split the ACE dataset using the tools bundled with Reconcile (Stoyanov et al., 2010) and parse it using the Berkeley Parser (Petrov et al., 2006). We use the train/test spli"
Q14-1037,P11-1115,0,0.00946756,"entities present in a document? Two broad threads exist in the literature. The first is coreference resolution (Soon et al., 2001; Ng, 2010; Pradhan et al., 2011), which identifies clusters of mentions in a document referring to the same entity. This process gives us access to useful information about the referents of pronouns and nominal expressions, but because clusters are local to each document, it is often hard to situate document entities in a broader context. A separate line of work has considered the problem of entity linking or “Wikification” (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011), where mentions are linked to entries in a given knowledge 1 System available at http://nlp.cs.berkeley.edu In this paper, we describe a joint model of coreference, entity linking, and semantic typing (named entity recognition) using a structured conditional random field. Variables in the model capture decisions about antecedence, semantic type, and entity links for each mention. Unary factors on these variables incorporate features that are commonly employed when solving each task in isolation. Binary and higher-order factors capture interactions between pairs of tasks. For entity linking an"
Q14-1037,D07-1073,0,0.183991,".. Figure 1: Coreference can help resolve ambiguous cases of semantic types or entity links: propagating information across coreference arcs can inform us that, in this context, Dell is an organization and should therefore link to the article on Dell in Wikipedia. shown that tighter integration of coreference and entity linking is promising (Hajishirzi et al., 2013; Zheng et al., 2013); we extend these approaches and model the entire process more holistically. Named entity recognition is improved by simple coreference (Finkel et al., 2005; Ratinov and Roth, 2009) and knowledge from Wikipedia (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Nothman et al., 2013; Sil and Yates, 2013). Joint models of coreference and NER have been proposed in Haghighi and Klein (2010) and Durrett et al. (2013), but in neither case was supervised data used for both tasks. Technically, our model is most closely related to that of Singh et al. (2013), who handle coreference, named entity recognition, and relation extraction.2 Our system is novel in three ways: the choice of tasks to model jointly, the fact that we maintain uncertainty about all decisions throughout inference (rather than using a greedy approach), and the feat"
Q14-1037,P08-1068,0,0.00800239,"concatenation of standard NER surface features associated with each token in that chunk. We use surface token features similar to those from previous work (Zhang and Johnson, 2003; Ratinov and Roth, 2009; Passos et al., 2014): for tokens at offsets of {−2, −1, 0, 1, 2} from the current token, we fire features based on 1) word identity, 2) POS tag, 3) word class (based on capitalization, presence of numbers, suffixes, etc.), 4) word shape (based on the pattern of uppercase and lowercase letters, digits, and punctuation), 5) Brown cluster prefixes of length 4, 6, 10, 20 using the clusters from Koo et al. (2008), and 6) common bigrams of word shape and word identity. 6 Pairwise potentials in sequence-based NER are useful for producing coherent output (e.g. prohibiting configurations like O I - PER ), but since we have so far defined the task as operating over fixed mentions, this structural constraint does not come into play for our system. 480 3.1.3 Entity Linking Our entity linking system diverges more substantially from past work than the coreference or NER systems. Most entity linking systems operate in two distinct phases (Cucerzan, 2007; Milne and Witten, 2008; Dredze et al., 2010; Ratinov et a"
Q14-1037,W11-1902,0,0.0242695,"Missing"
Q14-1037,P14-1038,0,0.0214376,"ke to preserve the modularity, efficiency, and structural simplicity of pipelined approaches. Our model’s feature-based structure permits improvement of features specific to a particular task or to a pair of tasks. By pruning variable domains with a coarse model and using approximate inference via belief propagation, we maintain efficiency and our model is only a factor of two slower than the union of the individual 2 Our model could potentially be extended to handle relation extraction or mention detection, which has also been addressed in past joint modeling efforts (Daum´e and Marcu, 2005; Li and Ji, 2014), but that is outside the scope of the current work. 478 models. Finally, as a structured CRF, it is conceptually no more complex than its component models and its behavior can be understood using the same intuition. We apply our model to two datasets, ACE 2005 and OntoNotes, with different mention standards and layers of annotation. In both settings, our joint model outperforms our independent baseline models. On ACE, we achieve state-of-the-art entity linking results, matching the performance of the system of Fahrni and Strube (2014). On OntoNotes, we match the performance of the best publis"
Q14-1037,H05-1004,0,0.380812,".07 Table 2: Detailed entity linking results on the ACE 2005 test set. We evaluate both our I NDEP. (task-specific factors only) and J OINT models and compare to the results of the FAHRNI model, a state-of-the-art entity linking system. We compare overall accuracy as well as performance at predicting NILS (mentions not in the knowledge base) and nonNILS . The J OINT model roughly matches the performance of FAHRNI and gives strong gains over the I NDEP. system. Table 1 shows our results. Coreference results are reported using MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric, all computed from the reference implementation of the CoNLL scorer (Pradhan et al., 2014). We see that the joint model improves all three tasks compared to the individual task models in the baseline. More in-depth entity linking results are shown in Table 2. We both evaluate on overall accuracy (how many mentions are correctly linked) as well as two more specific criteria: precision/recall/F1 of nonNIL9 predictions, and precision/recall/F1 of NIL predictions. This latter measure may be important if a system designer is trying to identify new entiti"
Q14-1037,P10-1142,0,0.0120988,"ry factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-theart results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.1 1 Introduction How do we characterize the collection of entities present in a document? Two broad threads exist in the literature. The first is coreference resolution (Soon et al., 2001; Ng, 2010; Pradhan et al., 2011), which identifies clusters of mentions in a document referring to the same entity. This process gives us access to useful information about the referents of pronouns and nominal expressions, but because clusters are local to each document, it is often hard to situate document entities in a broader context. A separate line of work has considered the problem of entity linking or “Wikification” (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011), where mentions are linked to entries in a given knowledge 1 System available at http://nlp.cs.berkeley.edu In this p"
Q14-1037,W14-1609,0,0.0617995,"o more complex than its component models and its behavior can be understood using the same intuition. We apply our model to two datasets, ACE 2005 and OntoNotes, with different mention standards and layers of annotation. In both settings, our joint model outperforms our independent baseline models. On ACE, we achieve state-of-the-art entity linking results, matching the performance of the system of Fahrni and Strube (2014). On OntoNotes, we match the performance of the best published coreference system (Bj¨orkelund and Kuhn, 2014) and outperform two strong NER systems (Ratinov and Roth, 2009; Passos et al., 2014). 2 Motivating Examples We first present two examples to motivate our approach. Figure 1 shows an example of a case where coreference is beneficial for named entity recognition and entity linking. The company is clearly coreferent to Dell by virtue of the lack of other possible antecedents; this in turn indicates that Dell refers to the corporation rather than to Michael Dell. This effect can be captured for entity linking by a feature tying the lexical item company to the fact that C OMPANY is in the Wikipedia infobox for Dell,3 thereby helping the linker make the correct decision. This would"
Q14-1037,P06-1055,1,0.154267,"te on the OntoNotes 5 corpus (Hovy et al., 2006) as used in the CoNLL 2012 coreference shared task (Pradhan et al., 2012). This corpus does not contain gold-standard entity links, so we cannot evaluate this portion of our model, though the model still exploits the information from Wikipedia to make coreference and named entity decisions. We will compare to prior coreference and named entity work in the system mentions setting. 6.1 ACE Evaluation We tokenize and sentence-split the ACE dataset using the tools bundled with Reconcile (Stoyanov et al., 2010) and parse it using the Berkeley Parser (Petrov et al., 2006). We use the train/test split from Stoyanov et al. (2009), Haghighi and Klein (2010), and Bansal and Klein (2012). FAHRNI I NDEP. J OINT ∆ over I NDEP. Prec. 81.15 80.26 83.26 +3.00 Non-NILS Rec. F1 78.10 79.60 76.30 78.23 77.67 80.37 +1.37 +2.14 NILS Prec. 41.25 33.39 35.19 +1.80 Rec. 61.10 54.47 65.42 +10.95 F1 49.25 41.40 45.77 +3.37 Accuracy 76.87 74.71 76.78 +2.07 Table 2: Detailed entity linking results on the ACE 2005 test set. We evaluate both our I NDEP. (task-specific factors only) and J OINT models and compare to the results of the FAHRNI model, a state-of-the-art entity linking sys"
Q14-1037,N06-1025,0,0.731424,"ctors that softly encourage consistency of semantic types and entity links across coreference arcs, similar to the method of Durrett et al. (2013). Figure 1 shows an example of the effects such factors can capture. The non-locality of coreference factors make exact inference intractable, but we find that belief propagation is a suitable approximation technique and performs well. Our joint modeling of these three tasks is motivated by their heavy interdependencies, which have been noted in previous work (discussed more in Section 7). Entity linking has been employed for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012) and coreference for entity linking (Cheng and Roth, 2013) as part of pipelined systems. Past work has 477 Transactions of the Association for Computational Linguistics, vol. 2, pp. 477–490, 2014. Action Editor: Jason Eisner. c Submitted 8/2014; Revised 10/2014; Published November 1, 2014. 2014 Association for Computational Linguistics. en.wikipedia.org/wiki/Dell Infobox type: company en.wikipedia.org/wiki/Michael_Dell Infobox type: person ORGANIZATION PERSON Revenues of $14.5 billion were posted by Dell1. The company1 ... Figure 1: Coreference can"
Q14-1037,W11-1901,0,0.0278341,"encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-theart results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.1 1 Introduction How do we characterize the collection of entities present in a document? Two broad threads exist in the literature. The first is coreference resolution (Soon et al., 2001; Ng, 2010; Pradhan et al., 2011), which identifies clusters of mentions in a document referring to the same entity. This process gives us access to useful information about the referents of pronouns and nominal expressions, but because clusters are local to each document, it is often hard to situate document entities in a broader context. A separate line of work has considered the problem of entity linking or “Wikification” (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011), where mentions are linked to entries in a given knowledge 1 System available at http://nlp.cs.berkeley.edu In this paper, we describe a joi"
Q14-1037,W12-4501,0,0.593459,"e largely the same, since the posterior distributions of the ai are quite peaked. 6 Experiments We present results on two corpora. First, we use the ACE 2005 corpus (NIST, 2005): this corpus annotates mentions complete with coreference, semantic types (per mention), and entity links (also per mention) later added by Bentivogli et al. (2010). We evaluate on gold mentions in this setting for comparability with prior work on entity linking; we lift this restriction in Section 6.3. Second, we evaluate on the OntoNotes 5 corpus (Hovy et al., 2006) as used in the CoNLL 2012 coreference shared task (Pradhan et al., 2012). This corpus does not contain gold-standard entity links, so we cannot evaluate this portion of our model, though the model still exploits the information from Wikipedia to make coreference and named entity decisions. We will compare to prior coreference and named entity work in the system mentions setting. 6.1 ACE Evaluation We tokenize and sentence-split the ACE dataset using the tools bundled with Reconcile (Stoyanov et al., 2010) and parse it using the Berkeley Parser (Petrov et al., 2006). We use the train/test split from Stoyanov et al. (2009), Haghighi and Klein (2010), and Bansal and"
Q14-1037,P14-2006,0,0.14136,"bjective from the different tasks, addressing Singh et al. (2013)’s objection to single objectives for joint models. Dev MUC I NDEP. J OINT ∆ 77.95 79.41 +1.46 B3 74.81 75.56 +0.75 CEAFe 71.84 73.34 +1.50 Avg. 74.87 76.10 +1.23 NER 83.04 85.94 +2.90 Link 73.07 75.69 +2.62 Test MUC 81.03 81.41 +0.42 B3 74.89 74.70 −0.19 CEAFe 72.56 72.93 +0.37 Avg. 76.16 76.35 +0.19 NER 82.35 85.60 +3.25 Link 74.71 76.78 +2.07 Table 1: Results on the ACE 2005 dev and test sets for the I NDEP. (task-specific factors only) and J OINT models. Coreference metrics are computed using their reference implementations (Pradhan et al., 2014). We report accuracy on NER because the set of mentions is fixed and all mentions have named entity types. Coreference and NER are compared to prior work in a more standard setting in Section 6.3. Finally, we also report accuracy of our entity linker (including links to NIL); entity linking is analyzed more thoroughly in Table 2. Bolded values represent statistically significant improvements with p < 0.05 according to a bootstrap resampling test. 5 Inference For both learning and decoding, inference consists of computing marginals for individual variables or for sets of variables adjacent to a"
Q14-1037,D10-1048,0,0.0112298,"ing baselines from the literature: the Illinois NER system of Ratinov and Roth (2009) and the results of Passos et al. (2014). Table 5 shows that we outperform both prior systems in terms of F1 , though the I LLINOIS system features higher recall while our system features higher precision. 7 Related Work There are two closely related threads of prior work: those that address the tasks we consider in a different way and those that propose joint models for other related sets of tasks. In the first category, Hajishirzi et al. (2013) integrate entity linking into a sieve-based coreference system (Raghunathan et al., 2010), the aim being to propagate link decisions throughout coreference chains, block coreference links between different entities, and use semantic information to make additional coreference links. Zheng et al. (2013) build coreference clusters greedily left-to-right and maintain entity link information for each cluster, namely a list of possible targets in the knowledge base as well as a current best link target that is used to extract features (though that might not be the target that is chosen by the end of inference). Cheng and Roth (2013) use coreference as a preprocessing step for entity lin"
Q14-1037,P11-1082,0,0.0414084,"consistency of semantic types and entity links across coreference arcs, similar to the method of Durrett et al. (2013). Figure 1 shows an example of the effects such factors can capture. The non-locality of coreference factors make exact inference intractable, but we find that belief propagation is a suitable approximation technique and performs well. Our joint modeling of these three tasks is motivated by their heavy interdependencies, which have been noted in previous work (discussed more in Section 7). Entity linking has been employed for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012) and coreference for entity linking (Cheng and Roth, 2013) as part of pipelined systems. Past work has 477 Transactions of the Association for Computational Linguistics, vol. 2, pp. 477–490, 2014. Action Editor: Jason Eisner. c Submitted 8/2014; Revised 10/2014; Published November 1, 2014. 2014 Association for Computational Linguistics. en.wikipedia.org/wiki/Dell Infobox type: company en.wikipedia.org/wiki/Michael_Dell Infobox type: person ORGANIZATION PERSON Revenues of $14.5 billion were posted by Dell1. The company1 ... Figure 1: Coreference can help resolve ambiguo"
Q14-1037,W09-1119,0,0.677451,"of $14.5 billion were posted by Dell1. The company1 ... Figure 1: Coreference can help resolve ambiguous cases of semantic types or entity links: propagating information across coreference arcs can inform us that, in this context, Dell is an organization and should therefore link to the article on Dell in Wikipedia. shown that tighter integration of coreference and entity linking is promising (Hajishirzi et al., 2013; Zheng et al., 2013); we extend these approaches and model the entire process more holistically. Named entity recognition is improved by simple coreference (Finkel et al., 2005; Ratinov and Roth, 2009) and knowledge from Wikipedia (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Nothman et al., 2013; Sil and Yates, 2013). Joint models of coreference and NER have been proposed in Haghighi and Klein (2010) and Durrett et al. (2013), but in neither case was supervised data used for both tasks. Technically, our model is most closely related to that of Singh et al. (2013), who handle coreference, named entity recognition, and relation extraction.2 Our system is novel in three ways: the choice of tasks to model jointly, the fact that we maintain uncertainty about all decisions throughout infer"
Q14-1037,D12-1113,0,0.229189,"tic types and entity links across coreference arcs, similar to the method of Durrett et al. (2013). Figure 1 shows an example of the effects such factors can capture. The non-locality of coreference factors make exact inference intractable, but we find that belief propagation is a suitable approximation technique and performs well. Our joint modeling of these three tasks is motivated by their heavy interdependencies, which have been noted in previous work (discussed more in Section 7). Entity linking has been employed for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012) and coreference for entity linking (Cheng and Roth, 2013) as part of pipelined systems. Past work has 477 Transactions of the Association for Computational Linguistics, vol. 2, pp. 477–490, 2014. Action Editor: Jason Eisner. c Submitted 8/2014; Revised 10/2014; Published November 1, 2014. 2014 Association for Computational Linguistics. en.wikipedia.org/wiki/Dell Infobox type: company en.wikipedia.org/wiki/Michael_Dell Infobox type: person ORGANIZATION PERSON Revenues of $14.5 billion were posted by Dell1. The company1 ... Figure 1: Coreference can help resolve ambiguous cases of semantic type"
Q14-1037,P11-1138,0,0.476003,"over a particular set of output variables (and x) are associated with factors connected to those variables. Figure 3 shows the task-specific factors in the model, discussed next in Section 3.1. Higher-order factors coupling variables between tasks are discussed in Section 3.2. 3.1 Independent Model Figure 3 shows a version of the model with only task-specific factors. Though this framework is structurally simple, it is nevertheless powerful enough for us to implement high-performing models for each task. State-of-the-art approaches to coreference (Durrett and Klein, 2013) and entity linking (Ratinov et al., 2011) already have this independent structure and Ratinov and Roth (2009) note that it is a reasonable assumption to make for NER as well.6 In this section, we describe the features present in the task-specific factors of each type (which also serve as our three separate baseline systems). 3.1.1 Coreference Our modeling of the coreference output space (as antecedents chosen for each mention) follows the mention-ranking approach to coreference (Denis and Baldridge, 2008; Durrett and Klein, 2013). Our feature set is that of Durrett and Klein, targeting surface properties of mentions: for each mention"
Q14-1037,D08-1016,0,0.00918921,"Missing"
Q14-1037,J01-4004,0,0.574465,"l random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-theart results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.1 1 Introduction How do we characterize the collection of entities present in a document? Two broad threads exist in the literature. The first is coreference resolution (Soon et al., 2001; Ng, 2010; Pradhan et al., 2011), which identifies clusters of mentions in a document referring to the same entity. This process gives us access to useful information about the referents of pronouns and nominal expressions, but because clusters are local to each document, it is often hard to situate document entities in a broader context. A separate line of work has considered the problem of entity linking or “Wikification” (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011), where mentions are linked to entries in a given knowledge 1 System available at http://nlp.cs.berkeley.edu"
Q14-1037,P09-1074,0,0.0101264,"d in the CoNLL 2012 coreference shared task (Pradhan et al., 2012). This corpus does not contain gold-standard entity links, so we cannot evaluate this portion of our model, though the model still exploits the information from Wikipedia to make coreference and named entity decisions. We will compare to prior coreference and named entity work in the system mentions setting. 6.1 ACE Evaluation We tokenize and sentence-split the ACE dataset using the tools bundled with Reconcile (Stoyanov et al., 2010) and parse it using the Berkeley Parser (Petrov et al., 2006). We use the train/test split from Stoyanov et al. (2009), Haghighi and Klein (2010), and Bansal and Klein (2012). FAHRNI I NDEP. J OINT ∆ over I NDEP. Prec. 81.15 80.26 83.26 +3.00 Non-NILS Rec. F1 78.10 79.60 76.30 78.23 77.67 80.37 +1.37 +2.14 NILS Prec. 41.25 33.39 35.19 +1.80 Rec. 61.10 54.47 65.42 +10.95 F1 49.25 41.40 45.77 +3.37 Accuracy 76.87 74.71 76.78 +2.07 Table 2: Detailed entity linking results on the ACE 2005 test set. We evaluate both our I NDEP. (task-specific factors only) and J OINT models and compare to the results of the FAHRNI model, a state-of-the-art entity linking system. We compare overall accuracy as well as performance a"
Q14-1037,P10-2029,0,0.00810508,"ing; we lift this restriction in Section 6.3. Second, we evaluate on the OntoNotes 5 corpus (Hovy et al., 2006) as used in the CoNLL 2012 coreference shared task (Pradhan et al., 2012). This corpus does not contain gold-standard entity links, so we cannot evaluate this portion of our model, though the model still exploits the information from Wikipedia to make coreference and named entity decisions. We will compare to prior coreference and named entity work in the system mentions setting. 6.1 ACE Evaluation We tokenize and sentence-split the ACE dataset using the tools bundled with Reconcile (Stoyanov et al., 2010) and parse it using the Berkeley Parser (Petrov et al., 2006). We use the train/test split from Stoyanov et al. (2009), Haghighi and Klein (2010), and Bansal and Klein (2012). FAHRNI I NDEP. J OINT ∆ over I NDEP. Prec. 81.15 80.26 83.26 +3.00 Non-NILS Rec. F1 78.10 79.60 76.30 78.23 77.67 80.37 +1.37 +2.14 NILS Prec. 41.25 33.39 35.19 +1.80 Rec. 61.10 54.47 65.42 +10.95 F1 49.25 41.40 45.77 +3.37 Accuracy 76.87 74.71 76.78 +2.07 Table 2: Detailed entity linking results on the ACE 2005 test set. We evaluate both our I NDEP. (task-specific factors only) and J OINT models and compare to the resul"
Q14-1037,M95-1005,0,0.733428,"+10.95 F1 49.25 41.40 45.77 +3.37 Accuracy 76.87 74.71 76.78 +2.07 Table 2: Detailed entity linking results on the ACE 2005 test set. We evaluate both our I NDEP. (task-specific factors only) and J OINT models and compare to the results of the FAHRNI model, a state-of-the-art entity linking system. We compare overall accuracy as well as performance at predicting NILS (mentions not in the knowledge base) and nonNILS . The J OINT model roughly matches the performance of FAHRNI and gives strong gains over the I NDEP. system. Table 1 shows our results. Coreference results are reported using MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric, all computed from the reference implementation of the CoNLL scorer (Pradhan et al., 2014). We see that the joint model improves all three tasks compared to the individual task models in the baseline. More in-depth entity linking results are shown in Table 2. We both evaluate on overall accuracy (how many mentions are correctly linked) as well as two more specific criteria: precision/recall/F1 of nonNIL9 predictions, and precision/recall/F1 of NIL predictions. This latter measure may be important"
Q14-1037,C14-1201,0,0.0160604,"dels compared to three strong systems: Durrett and Klein (2013), Fernandes et al. (2012) (the winner of the CoNLL shared task), and Bj¨orkelund and Kuhn (2014) (the best reported results on the dataset). Our J OINT method outperforms all three as well as the I NDEP. system.12 Next, we report results on named entity recognition. We use the same OntoNotes splits as for the coreference data; however, the New Testament (NT) 11 The NER-coreference portion of the model now resembles the skip-chain CRF from Finkel et al. (2005), though with soft coreference. 12 The systems of Chang et al. (2013) and Webster and Curran (2014) perform similarly to the F ERNANDES system; changes in the reference implementation of the metrics make exact comparison to printed numbers difficult. 486 I LLINOIS PASSOS I NDEP. J OINT ∆ over I NDEP. Prec. 82.00 − 83.79 85.22 +1.43 Rec. 84.95 − 81.53 82.89 +1.36 F1 83.45 82.24 82.64 84.04 +1.40 Table 5: Results for NER tagging on the OntoNotes 5.0 / CoNLL 2011 test set. We compare our systems to the Illinois system (Ratinov and Roth, 2009) and the system of Passos et al. (2014). Our model outperforms both other systems in terms of F1 , and once again joint modeling gives substantial improve"
Q14-1037,W03-0434,0,0.0128739,"perties between mention pairs and also use properties of the mention pair itself, such as the distance between the mentions and whether their heads match. Note that this baseline does not rely on having access to named entity chunks. 3.1.2 Named Entity Recognition Our NER model places a distribution over possible semantic types for each mention, which corresponds to a fixed span of the input text. We define the features of a span to be the concatenation of standard NER surface features associated with each token in that chunk. We use surface token features similar to those from previous work (Zhang and Johnson, 2003; Ratinov and Roth, 2009; Passos et al., 2014): for tokens at offsets of {−2, −1, 0, 1, 2} from the current token, we fire features based on 1) word identity, 2) POS tag, 3) word class (based on capitalization, presence of numbers, suffixes, etc.), 4) word shape (based on the pattern of uppercase and lowercase letters, digits, and punctuation), 5) Brown cluster prefixes of length 4, 6, 10, 20 using the clusters from Koo et al. (2008), and 6) common bigrams of word shape and word identity. 6 Pairwise potentials in sequence-based NER are useful for producing coherent output (e.g. prohibiting con"
Q14-1037,W13-3517,0,0.227496,"Association for Computational Linguistics. en.wikipedia.org/wiki/Dell Infobox type: company en.wikipedia.org/wiki/Michael_Dell Infobox type: person ORGANIZATION PERSON Revenues of $14.5 billion were posted by Dell1. The company1 ... Figure 1: Coreference can help resolve ambiguous cases of semantic types or entity links: propagating information across coreference arcs can inform us that, in this context, Dell is an organization and should therefore link to the article on Dell in Wikipedia. shown that tighter integration of coreference and entity linking is promising (Hajishirzi et al., 2013; Zheng et al., 2013); we extend these approaches and model the entire process more holistically. Named entity recognition is improved by simple coreference (Finkel et al., 2005; Ratinov and Roth, 2009) and knowledge from Wikipedia (Kazama and Torisawa, 2007; Ratinov and Roth, 2009; Nothman et al., 2013; Sil and Yates, 2013). Joint models of coreference and NER have been proposed in Haghighi and Klein (2010) and Durrett et al. (2013), but in neither case was supervised data used for both tasks. Technically, our model is most closely related to that of Singh et al. (2013), who handle coreference, named entity recog"
Q14-1037,W12-4502,0,\N,Missing
Q14-1037,W10-3503,0,\N,Missing
Q17-1031,P11-2037,0,0.180279,"sponds to the 1-EC pagenumber-2 space, a property that applies to this work as well2 . Parsing with Null Elements in the PTB has taken two general approaches. The first broadly effective system was Johnson (2002), which post-processed the output of a parser, inserting extra elements. This was effective for some types of structure, such as null complementizers, but had difficulty with long distance dependencies. The other common approach has been to thread a trace through the tree structure on the non-terminal symbols. Collins (1997)’s third model used this approach to recover wh-traces, while Cai et al. (2011) used it to recover null pronouns, and others have used it for a range of movement types (Dienes and Dubey 2003; Schmid 2006). These approaches have the disadvantage that each 1 To produce directed edges, their parser treats the direction as part of the edge label. 2 This is a topological space with two half-planes sharing a boundary. All edges are drawn on one of the two half-planes and each half-plane contains no crossings. 442 additional trace dramatically expands the grammar. Our representation is similar to LTAG-Spinal (Shen et al. 2007) but has the advantage that it can be converted back"
Q17-1031,P04-1082,0,0.296388,"Missing"
Q17-1031,P17-1193,0,0.0332959,"the space of all non-projective graphs is intractable. Fortunately, in practice almost all parses are covered by well-defined subsets of this space. For dependency parsing, recent work has defined algorithms for inference within various subspaces (G´omezRodr´ıguez and Nivre 2010; Pitler et al. 2013). We build upon these algorithms and adapt them to constituency parsing. For constituency parsing, a range of formalisms have been developed that are mildlycontext sensitive, such as CCG (Steedman 2000), LFG (Kaplan and Bresnan 1982), and LTAG (Joshi and Schabes 1997). Concurrently with this work, Cao et al. (2017) also proposed a graph version of Pitler et al. (2013)’s One-Endpoint Crossing (1-EC) algorithm. However, Cao’s algorithm does not consider the direction of edges1 and so it could produce cycles, or graphs with multiple root nodes. Their algorithm also has spurious ambiguity, with multiple derivations of the same parse structure permitted. One advantage of their algorithm is that by introducing a new item type it can handle some cases of the Locked-Chain we define below (specifically, when N is even), though in practise they also restrict their algorithm to ignore such cases. They also show th"
Q17-1031,W08-2102,0,0.229054,"k word labels in the state, we need to adjust every n by a factor of S, leading to O(S 4 n4 + ES 2 n2 ). 4 Parse Representation Our algorithm relies on the assumption that we can process the dependents to the left and right of a word independently and then combine the two halves. This means we need lexicalized structures, which the PTB does not provide. We define a new representation in which each non-terminal symbol is associated with a specific word (the head). Unlike dependency parsing, we retain all the information required to reconstruct the constituency parse. Our approach is related to Carreras et al. (2008) and Hayashi and Nagata (2016), with three key differences: (1) we encode non-terminals explicitly, rather than implicitly through adjunction operations, which can cause ambiguity, (2) we add representations of null elements and co-indexation, (3) we modify head rules to avoid problematic structures. Figure 3 shows a comparison of the PTB representation and ours. We add lexicalization, assigning each non-terminal to a word. The only other changes are visual notation, with non-terminals moved to be directly above the words to more clearly show the distinction between spines and edges. Spines: E"
Q17-1031,J07-2003,0,0.00752945,"bining items). Almost every template in Algorithm 1 generates some unnecessary rules, and no items of type B are needed. 4 One alternative is to count half of it on each end, removing the need for subtraction later. Another is to add it during the combination step. The remaining rules still have high coverage of the development set, missing only 15 rules, each applied once (out of 78,692 rule applications). By pruning in this way, we are considering the intersection of 1-EC graphs and the true space of structures used in language. Chart Pruning: To improve speed we use beams and cube pruning (Chiang 2007), discarding items based on their Viterbi inside score. We divide each beam into sub-beams based on aspects of the state. This ensures diversity and enables consideration of only compatible items during binary and ternary compositions. Coarse to Fine Pruning: Rather than parsing immediately with the full model we use several passes with progressively richer structure (Goodman 1997): (1) Projective parsing without traces or spines, and simultaneously a trace classifier, (2) Non-projective parsing without spines, and simultaneously a spine classifier, (3) Full structure parsing. Each pass prunes"
Q17-1031,P97-1003,0,0.199574,"nore such cases. They also show that the class of graphs they generate corresponds to the 1-EC pagenumber-2 space, a property that applies to this work as well2 . Parsing with Null Elements in the PTB has taken two general approaches. The first broadly effective system was Johnson (2002), which post-processed the output of a parser, inserting extra elements. This was effective for some types of structure, such as null complementizers, but had difficulty with long distance dependencies. The other common approach has been to thread a trace through the tree structure on the non-terminal symbols. Collins (1997)’s third model used this approach to recover wh-traces, while Cai et al. (2011) used it to recover null pronouns, and others have used it for a range of movement types (Dienes and Dubey 2003; Schmid 2006). These approaches have the disadvantage that each 1 To produce directed edges, their parser treats the direction as part of the edge label. 2 This is a topological space with two half-planes sharing a boundary. All edges are drawn on one of the two half-planes and each half-plane contains no crossings. 442 additional trace dramatically expands the grammar. Our representation is similar to LTA"
Q17-1031,P03-1055,0,0.600072,"ce edges that represent control structures, wh-movement and more. However, most parsers and the standard evaluation metric ignore these edges and all null elements. By leaving out parts of the structure, they fail to provide key relations to downstream tasks such as question answering. While there has been work on capturing some parts of this extra structure, it has generally either been through post-processing on trees (Johnson 2002; Jijkoun 2003; Campbell 2004; Levy and Manning 2004; Gabbard et al. 2006) or has only captured a limited set of phenomena via grammar augmentation (Collins 1997; Dienes and Dubey 2003; Schmid 2006; Cai et al. 2011). We propose a new general-purpose parsing algorithm that can efficiently search over a wide range of syntactic phenomena. Our algorithm extends a non-projective tree parsing algorithm (Pitler et al. 2013; Pitler 2014) to graph structures, with improvements to avoid derivational ambiguity while maintaining an O(n4 ) runtime. Our algorithm also includes an optional extension to ensure parses contain a directed projective tree of non-trace edges. Our algorithm cannot apply directly to constituency parses–it requires lexicalized structures similar to dependency pars"
Q17-1031,C96-1058,0,0.551031,"007; Hall and Nivre 2008; Fern´andezGonz´alez and Martins 2015; Kong et al. 2015). Kato and Matsubara (2016) described a new approach, modifying a transition-based parser to recover null elements and traces, with strong results, but using heuristics to determine trace referents. 3 Algorithm Our algorithm is a dynamic program, similar at a high level to CKY (Kasami 1966; Younger 1967; Cocke 1969). The states of our dynamic program (items) represent partial parses. Usually in CKY, items are defined as covering the n words in a sentence, starting and ending at the spaces between words. We follow Eisner (1996), defining items as covering the n−1 spaces in a sentence, starting and ending on words, as shown in Figure 1. This means that we process each word’s left and right dependents separately, then combine the two halves. We use three types of items: (1) a single edge, linking two words, (2) a continuous span, going from one word to another, representing all edges linking pairs of words within the span, (3) a span (as defined in 2) plus an additional word outside the span, enabling the inclusion of edges between that word and words in the span. Within the CKY framework, the key to defining our algo"
Q17-1031,P15-1147,0,0.0695872,"Missing"
Q17-1031,N06-1024,0,0.874605,"Missing"
Q17-1031,P10-1151,0,0.042706,"Missing"
Q17-1031,W97-0302,0,0.329732,"78,692 rule applications). By pruning in this way, we are considering the intersection of 1-EC graphs and the true space of structures used in language. Chart Pruning: To improve speed we use beams and cube pruning (Chiang 2007), discarding items based on their Viterbi inside score. We divide each beam into sub-beams based on aspects of the state. This ensures diversity and enables consideration of only compatible items during binary and ternary compositions. Coarse to Fine Pruning: Rather than parsing immediately with the full model we use several passes with progressively richer structure (Goodman 1997): (1) Projective parsing without traces or spines, and simultaneously a trace classifier, (2) Non-projective parsing without spines, and simultaneously a spine classifier, (3) Full structure parsing. Each pass prunes using parse max-marginals and classifier scores, tuned on the development set. The third pass also prunes spines that are not consistent with any unpruned edge from the second pass. For the spine classifier we use a bidirectional LSTM tagger, implemented in DyNet (Neubig et al. 2017). Speed: Parsing took an average of 8.6 seconds per sentence for graph parsing and 0.5 seconds when"
Q17-1031,W08-1007,0,0.0380596,"drawn on one of the two half-planes and each half-plane contains no crossings. 442 additional trace dramatically expands the grammar. Our representation is similar to LTAG-Spinal (Shen et al. 2007) but has the advantage that it can be converted back into the PTB representation. Hayashi and Nagata (2016) also incorporated null elements into a spinal structure but did not include a representation of co-indexation. In related work, dependency parsers have been used to assist in constituency parsing, with varying degrees of representation design, but only for trees (Hall, Nivre, and Nilsson 2007; Hall and Nivre 2008; Fern´andezGonz´alez and Martins 2015; Kong et al. 2015). Kato and Matsubara (2016) described a new approach, modifying a transition-based parser to recover null elements and traces, with strong results, but using heuristics to determine trace referents. 3 Algorithm Our algorithm is a dynamic program, similar at a high level to CKY (Kasami 1966; Younger 1967; Cocke 1969). The states of our dynamic program (items) represent partial parses. Usually in CKY, items are defined as covering the n words in a sentence, starting and ending at the spaces between words. We follow Eisner (1996), defining"
Q17-1031,W07-2444,0,0.0606674,"Missing"
Q17-1031,P16-2016,0,0.452837,"uns, and others have used it for a range of movement types (Dienes and Dubey 2003; Schmid 2006). These approaches have the disadvantage that each 1 To produce directed edges, their parser treats the direction as part of the edge label. 2 This is a topological space with two half-planes sharing a boundary. All edges are drawn on one of the two half-planes and each half-plane contains no crossings. 442 additional trace dramatically expands the grammar. Our representation is similar to LTAG-Spinal (Shen et al. 2007) but has the advantage that it can be converted back into the PTB representation. Hayashi and Nagata (2016) also incorporated null elements into a spinal structure but did not include a representation of co-indexation. In related work, dependency parsers have been used to assist in constituency parsing, with varying degrees of representation design, but only for trees (Hall, Nivre, and Nilsson 2007; Hall and Nivre 2008; Fern´andezGonz´alez and Martins 2015; Kong et al. 2015). Kato and Matsubara (2016) described a new approach, modifying a transition-based parser to recover null elements and traces, with strong results, but using heuristics to determine trace referents. 3 Algorithm Our algorithm is"
Q17-1031,P03-2006,0,0.126092,"Missing"
Q17-1031,P02-1018,0,0.768512,"gorithm also has spurious ambiguity, with multiple derivations of the same parse structure permitted. One advantage of their algorithm is that by introducing a new item type it can handle some cases of the Locked-Chain we define below (specifically, when N is even), though in practise they also restrict their algorithm to ignore such cases. They also show that the class of graphs they generate corresponds to the 1-EC pagenumber-2 space, a property that applies to this work as well2 . Parsing with Null Elements in the PTB has taken two general approaches. The first broadly effective system was Johnson (2002), which post-processed the output of a parser, inserting extra elements. This was effective for some types of structure, such as null complementizers, but had difficulty with long distance dependencies. The other common approach has been to thread a trace through the tree structure on the non-terminal symbols. Collins (1997)’s third model used this approach to recover wh-traces, while Cai et al. (2011) used it to recover null pronouns, and others have used it for a range of movement types (Dienes and Dubey 2003; Schmid 2006). These approaches have the disadvantage that each 1 To produce direct"
Q17-1031,P04-1042,0,0.350116,"Missing"
Q17-1031,P16-1088,0,0.585559,"442 additional trace dramatically expands the grammar. Our representation is similar to LTAG-Spinal (Shen et al. 2007) but has the advantage that it can be converted back into the PTB representation. Hayashi and Nagata (2016) also incorporated null elements into a spinal structure but did not include a representation of co-indexation. In related work, dependency parsers have been used to assist in constituency parsing, with varying degrees of representation design, but only for trees (Hall, Nivre, and Nilsson 2007; Hall and Nivre 2008; Fern´andezGonz´alez and Martins 2015; Kong et al. 2015). Kato and Matsubara (2016) described a new approach, modifying a transition-based parser to recover null elements and traces, with strong results, but using heuristics to determine trace referents. 3 Algorithm Our algorithm is a dynamic program, similar at a high level to CKY (Kasami 1966; Younger 1967; Cocke 1969). The states of our dynamic program (items) represent partial parses. Usually in CKY, items are defined as covering the n words in a sentence, starting and ending at the spaces between words. We follow Eisner (1996), defining items as covering the n−1 spaces in a sentence, starting and ending on words, as sho"
Q17-1031,N15-1080,0,0.0134479,"tains no crossings. 442 additional trace dramatically expands the grammar. Our representation is similar to LTAG-Spinal (Shen et al. 2007) but has the advantage that it can be converted back into the PTB representation. Hayashi and Nagata (2016) also incorporated null elements into a spinal structure but did not include a representation of co-indexation. In related work, dependency parsers have been used to assist in constituency parsing, with varying degrees of representation design, but only for trees (Hall, Nivre, and Nilsson 2007; Hall and Nivre 2008; Fern´andezGonz´alez and Martins 2015; Kong et al. 2015). Kato and Matsubara (2016) described a new approach, modifying a transition-based parser to recover null elements and traces, with strong results, but using heuristics to determine trace referents. 3 Algorithm Our algorithm is a dynamic program, similar at a high level to CKY (Kasami 1966; Younger 1967; Cocke 1969). The states of our dynamic program (items) represent partial parses. Usually in CKY, items are defined as covering the n words in a sentence, starting and ending at the spaces between words. We follow Eisner (1996), defining items as covering the n−1 spaces in a sentence, starting"
Q17-1031,D15-1032,1,0.90542,"Missing"
Q17-1031,J93-2004,0,0.061933,"Missing"
Q17-1031,P05-1012,0,0.132057,"Missing"
Q17-1031,Q14-1004,0,0.167306,"Missing"
Q17-1031,Q13-1002,0,0.374895,"to downstream tasks such as question answering. While there has been work on capturing some parts of this extra structure, it has generally either been through post-processing on trees (Johnson 2002; Jijkoun 2003; Campbell 2004; Levy and Manning 2004; Gabbard et al. 2006) or has only captured a limited set of phenomena via grammar augmentation (Collins 1997; Dienes and Dubey 2003; Schmid 2006; Cai et al. 2011). We propose a new general-purpose parsing algorithm that can efficiently search over a wide range of syntactic phenomena. Our algorithm extends a non-projective tree parsing algorithm (Pitler et al. 2013; Pitler 2014) to graph structures, with improvements to avoid derivational ambiguity while maintaining an O(n4 ) runtime. Our algorithm also includes an optional extension to ensure parses contain a directed projective tree of non-trace edges. Our algorithm cannot apply directly to constituency parses–it requires lexicalized structures similar to dependency parses. We extend and improve previous work on lexicalized constituent representations (Shen et al. 2007; Carreras et al. 2008; Hayashi and Nagata 2016) to handle traces. In this form, traces can create problematic structures such as direc"
Q17-1031,P06-1023,0,0.78761,"control structures, wh-movement and more. However, most parsers and the standard evaluation metric ignore these edges and all null elements. By leaving out parts of the structure, they fail to provide key relations to downstream tasks such as question answering. While there has been work on capturing some parts of this extra structure, it has generally either been through post-processing on trees (Johnson 2002; Jijkoun 2003; Campbell 2004; Levy and Manning 2004; Gabbard et al. 2006) or has only captured a limited set of phenomena via grammar augmentation (Collins 1997; Dienes and Dubey 2003; Schmid 2006; Cai et al. 2011). We propose a new general-purpose parsing algorithm that can efficiently search over a wide range of syntactic phenomena. Our algorithm extends a non-projective tree parsing algorithm (Pitler et al. 2013; Pitler 2014) to graph structures, with improvements to avoid derivational ambiguity while maintaining an O(n4 ) runtime. Our algorithm also includes an optional extension to ensure parses contain a directed projective tree of non-trace edges. Our algorithm cannot apply directly to constituency parses–it requires lexicalized structures similar to dependency parses. We extend"
Q17-1031,C12-1147,0,0.0517144,"Missing"
Q17-1031,Q14-1026,0,\N,Missing
S01-1021,J96-1002,0,0.00227623,"Missing"
S01-1021,W96-0208,0,0.0412505,"Missing"
W01-0714,A00-2018,0,\N,Missing
W01-0714,J97-4005,0,\N,Missing
W01-0714,P97-1003,0,\N,Missing
W01-0714,E95-1020,0,\N,Missing
W01-1812,J98-2004,0,0.0152326,"ot handle empty elements, cyclic unary productions, or n-ary rules. Stolcke (1995) presents a top-down parser for arbitrary PCFGs, which incorporates elements of ( ) 7 This ( ) has the theoretical – but not clearly useful – advantage of allowing the score combination function to vary per production. (1998) provides an insightful presentation unifying many categorical and probabilistic parsing algorithms in terms of the problem’s semiring structure, but he merely notes this problem (p. 172), and on this basis puts probabilistic agenda-based chart parsers aside. The agenda-based chart parser of Caraballo and Charniak (1998) (used for determining inside probabilities) suffers from exactly this problem: In Appendix A (p. 293), they note that such updates “can be quite expensive in terms of CPU time”, but merely suggest a method of thresholding which delays probability propagation until the amount of unpropagated probability mass has become significant, and suggest that this thresholding allows them to keep the performance of the parser “as O (n3 ) empirically.” We do not present an inside probability algorithm here, but the hypergraphical view of parsing can be developed to give an inside parsing algorithm, as dis"
W01-1812,P81-1022,0,0.15259,". The choice of which formalism to base our work on is thus more aesthetic than substantive, but we believe that the hypergraph presentation allows easier access to a greater variety of algorithmic tools, and presents a clearer, more visually appealing intuition. At any rate, the practical issues described above, and their solutions, which form the bulk of this paper, would be unchanged under either framework. 3 Viterbi Parsing Algorithm Agenda-based active chart parsing (Kay 1980, Pereira and Shieber 1987) is an attractive presentation of the central ideas of tabular methods for CFG parsing. Earley (1970)-style dotted items combine via deduction steps (“the fundamental rule”) in an order-independent manner, such that the same basic algorithm supports top-down, bottom-up, and left-corner parsing, and the parser deals naturally and correctly with the difficult cases of left-recursive rules, empty elements, and unary rules. However, while O n3 methods for parsing PCFGs are well known (Baker 1979, Jelinek et al. 1992, Stolcke 1995), a O n3 probabilistic parser corresponding to active chart parsing for categorical CFGs, has not yet been provided. Producing a probabilistic version of an agenda-drive"
W01-1812,P01-1044,1,0.922282,"ibe an extension of Dijkstra’s algorithm to B-graphs, which runs in time linear in the size of the graph.6 2.4 Practical Issues At this point, one might wonder what is left to be done. We have a reduction which, given a grammar G and a lattice L, allows us to build and score the induced graph. From this graph, we can use reachability algorithms to decide parse existence, and we can use shortest-path algorithms to find best parses. Furthermore, this view can be extended to other problems of parsing. For example, algorithms for summing paths can be adapted to calculate inside probabilities (see Klein and Manning (2001a)). However, there are two primary issues which remain. First, there is the issue of efficiency. Reachability and shortest-path algorithms, such as those cited above, generally run in time linear in the size of the induced graph. However, the size of the induced graph, while polynomial in the size of the lattice L, is exponential in the arity of the grammar G, having a term of jLjarity(G)+1 in its size. The implicit binarization of the grammar done by chart parsers is responsible for their cubic bounds, and we wish to preserve this bound for our Viterbi parsing. Second, one does not, in gener"
W01-1812,H91-1046,0,0.0282073,"terbi chart parser, if we later find a better way to form the NP, we will have to update not only the score of that NP, but also the score of any edge whose current score depends on that NP’s score. This can potentially lead to an extremely inefficient upward propagation of scores every time a new traversal is explored.8 Most exhaustive PCFG parsing work has used the bottom-up CKY algorithm (Kasami 1965, Younger 1967) with Chomsky Normal Form (CNF) Grammars (Baker 1979, Jelinek et al. 1992) or extended CKY parsers that work with n-ary branching grammars, but still not with empty constituents (Kupiec 1991, Chappelier and Rajman 1998). Such bottom-up parsers straightforwardly avoid the above problem, by always building all edges over shorter spans before building edges over longer spans which make use of them. However, such methods do not allow top-down grammar filtering, and often do not handle empty elements, cyclic unary productions, or n-ary rules. Stolcke (1995) presents a top-down parser for arbitrary PCFGs, which incorporates elements of ( ) 7 This ( ) has the theoretical – but not clearly useful – advantage of allowing the score combination function to vary per production. (1998) provid"
W01-1812,J97-2003,0,0.0181335,"he initial states of rules and are introduced in accordance with the grammar strategy (top-down, bottom-up, etc.). To hold the traversals or edges which have not yet been processed, a CP has a data structure called an agenda, which holds both traversals and introduction edges. Items from this agenda can be processed in any order whatsoever, even arbitrarily or randomly, without affecting the final chart contents. In our probabilistic chart parser (PCP), the central data structures are augmented with scores. Grammar rules, which were previously encoded as symbolic DFSAs are scored DFSAs, as in Mohri (1997), with a score for entering the initial state, a score on each transition, and, for each accepting state, a score for accepting in that state. Each edge e is also scored at all times. This value, s ore e (or s ore e; t at a time t), is the best estimate to date of that edge’s true best score,  e . In our algorithm, the estimate will always be conservative: s ore e will always be worse than or equal to  e . The full algorithm is shown in pseudocode in figure 6. It is broadly similar to a standard categorical chart parsing algorithm. However, in order to solve the problem of entering edges int"
W01-1812,P83-1021,0,0.0876085,"parser with an O(n3 ) time bound for arbitrary PCFGs, while preserving as much of the flexibility of symbolic chart parsers as allowed by the inherent ordering of probabilistic dependencies. 1 Introduction An influential view of parsing is as a process of logical deduction, in which a parser is presented as a set of parsing schemata. The grammar rules are the logical axioms, and the question of whether or not a certain category can be constructed over a certain span becomes the question of whether that category can be derived over that span, treating the initial words as starting assumptions (Pereira and Warren 1983, Shieber et al. 1995, Sikkel and Nijholt 1997). But such a viewpoint is less natural when we turn to probabilistic parsers, since probabilities, or, generalizing, scores, are not an organic part of logical systems.1 There is also a deep connection between logic, in particular propositional satisfiability, and directed hypergraphs (Gallo et al. 1993). In this paper, we develop and exploit the third side of this triangle, directly connecting parsing with directed hypergraph algorithms. The advantage of doing this is that scored arcs are a central and well-studied concept of graph theory, and we"
W01-1812,J95-2002,0,0.135611,"Algorithm Agenda-based active chart parsing (Kay 1980, Pereira and Shieber 1987) is an attractive presentation of the central ideas of tabular methods for CFG parsing. Earley (1970)-style dotted items combine via deduction steps (“the fundamental rule”) in an order-independent manner, such that the same basic algorithm supports top-down, bottom-up, and left-corner parsing, and the parser deals naturally and correctly with the difficult cases of left-recursive rules, empty elements, and unary rules. However, while O n3 methods for parsing PCFGs are well known (Baker 1979, Jelinek et al. 1992, Stolcke 1995), a O n3 probabilistic parser corresponding to active chart parsing for categorical CFGs, has not yet been provided. Producing a probabilistic version of an agenda-driven chart parser is not trivial. A central idea of such parsers is that the algorithm is correct and complete regardless of the order in which items on the agenda are processed. Achieving this is straightforward for categorical parsers, but problematic for probabilistic parsers. For example, consider extending an active edge VP!V. NP PP :[1,2] with an NP :[2,5] to form an edge VP ! V NP. PP over [1,5]. In a categorical chart pars"
W02-0811,J96-1002,0,0.00125762,"each (classifier, chosen sense, correct sense) triple. However, most senses are rarely chosen and rarely correct, so most features had zero or singleton support. f i (s, s1 , . . . , sk ) = 1 ⇐⇒ s = si v(s) = P i λi δ(si = s) The indicators δ are true for exactly one sense, and correspond to the simple f i defined above.4 The sense with the largest vote v(s) will be the sense with the highest posterior probability P(s|s 1 , . . . sk ) and will be chosen. For the maximum entropy classifier, we estimate the weights by maximizing the likelihood of a heldout set, using the standard IIS algorithm (Berger et al., 1996). For both weighted schemes, we found that stopping the iterative procedures before convergence gave better results. IIS was halted after 50 rounds, while EM was halted after a single round. Both methods were initialized to uniform starting weights. More importantly than changing the exact weight estimates, moving from method to method triggers broad qualitative changes in what kind of weights are allowed. With majority voting, classifiers all have equal, positive weights. With weighted voting, the weights are no longer required to be equal, but are still non-negative. With maximum entropy wei"
W02-0811,W96-0208,0,0.00775637,"scoring teams’ systems, the combination achieves high performance. We discuss trade-offs and empirical performance. Finally, we present an analysis of the combination, examining how ensemble performance depends on error independence and task difficulty. 1 Introduction The problem of supervised word sense disambiguation (WSD) has been approached using many different classification algorithms, including naive-Bayes, decision trees, decision lists, and memory-based learners. While it is unquestionable that certain algorithms are better suited to the WSD problem than others (for a comparison, see Mooney (1996)), it seems that, given similar input features, various algorithms exhibit roughly similar accuracies. 1 This was supported by the S ENSEVAL -2 results, where a This paper is based on work supported in part by the National Science Foundation under Grants IIS-0085896 and IIS9982226, by an NSF Graduate Fellowship, and by the Research Collaboration between NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation and CSLI, Stanford University. 1 In fact, we have observed that differences between implementations of a single classifier type, such as smoothing or window size"
W02-0811,S01-1040,0,\N,Missing
W02-0811,W00-1304,0,\N,Missing
W02-0811,W02-1005,0,\N,Missing
W02-0811,P00-1027,0,\N,Missing
W02-0811,P00-1035,0,\N,Missing
W02-0811,N01-1006,0,\N,Missing
W02-0811,J01-3001,0,\N,Missing
W02-0811,J95-4004,0,\N,Missing
W02-0811,P98-1081,0,\N,Missing
W02-0811,C98-1078,0,\N,Missing
W02-0811,W02-1004,0,\N,Missing
W02-0811,P98-1029,0,\N,Missing
W02-0811,C98-1029,0,\N,Missing
W02-0811,A00-2009,0,\N,Missing
W02-1002,J96-1002,0,0.00745968,"Missing"
W02-1002,A00-1031,0,0.0596498,"Missing"
W02-1002,P96-1024,0,0.0255577,"Missing"
W02-1002,P01-1042,0,0.0925924,"Missing"
W02-1002,J98-1006,0,0.0312336,"Missing"
W02-1002,P99-1023,0,0.061857,"Missing"
W02-1002,W00-1308,1,0.410612,"Missing"
W03-0428,J97-3003,0,\N,Missing
W03-0428,W96-0213,0,\N,Missing
W03-0428,A97-1030,0,\N,Missing
W03-0428,W99-0612,0,\N,Missing
W03-0428,A97-1029,0,\N,Missing
W04-3201,P04-1014,0,0.091323,"Missing"
W04-3201,P02-1036,0,0.0719483,"Missing"
W04-3201,P99-1069,0,0.179281,"Missing"
W04-3201,P01-1042,0,0.410478,"regression and support vector machines (SVMs) in flat classification tasks like text categorization, word-sense disambiguation, and relevance routing has been repeatedly demonstrated. For sequence tasks like part-of-speech tagging or named-entity extraction, recent top-performing systems have also generally been based on discriminative sequence models, like conditional Markov models (Toutanova et al., 2003) or conditional random fields (Lafferty et al., 2001). A number of recent papers have considered discriminative approaches for natural language parsing (Johnson et al., 1999; Collins, 2000; Johnson, 2001; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004; Kaplan et al., 2004; Collins, 2004). Broadly speaking, these approaches fall into two categories, reranking and dynamic programming approaches. In reranking methods (Johnson et al., 1999; Collins, 2000; Shen et al., 2003), an initial parser is used to generate a number of candidate parses. A discriminative model is then used to choose between these candidates. In dynamic programming methods, a large number of candidate parse trees are represented compactly in a parse tree forest or chart. Given sufficiently “local” feat"
W04-3201,N04-1013,0,0.0494885,"Missing"
W04-3201,P03-1054,1,0.0643329,"ant fact about this kind of training is that, similar to the basic perceptron approach, it only requires picking up sentences one at a time, checking what the best parse is according to the current primal and dual weights, and adjusting the weights. each model and setting trained and tested on only the sentences of length ≤ 15 words. Aside from the length restriction, we used the standard splits: sections 2-21 for training (9753 sentences), 22 for development (603 sentences), and 23 for final testing (421 sentences). As a baseline, we trained a CNF transformation of the unlexicalized model of Klein and Manning (2003) on this data. The resulting grammar had 3975 non-terminal symbols and contained two kinds of productions: binary nonterminal rewrites and tag-word rewrites.5 The scores for the binary rewrites were estimated using unsmoothed relative frequency estimators. The tagging rewrites were estimated with a smoothed model of P (w|t), also using the model from Klein and Manning (2003). Figure 3 shows the performance of this model (generative): 87.99 F1 on the test set. For the basic max-margin model, we used exactly the same set of allowed rewrites (and therefore the same set of candidate parses) as in"
W04-3201,W03-1012,0,0.0604909,"Missing"
W04-3201,N03-1033,1,0.110006,"generative baseline; this feature added little information, but made the learning phase faster. The second feature was the output of a flat classifier which was trained to predict whether single spans, in isolation, were constituents or not, based on a bundle of features including the list above, but also the following: the preceding, first, last, and following tag in the span, pairs of tags such as preceding-first, last-following, preceding-following, first-last, and the entire tag sequence. Tag features on the test sets were taken from a pretagging of the sentence by the tagger described in Toutanova et al. (2003). While the flat classifier alone was quite poor (P 78.77 / R 63.94 / F1 70.58), the resulting max-margin model (lexical+aux) scored 89.12 F1 . To situate these numbers with respect to other models, the parser in Collins (1999), which is generative, lexicalized, and intricately smoothed scores 88.69 over the same train/test configuration. It is worth considering the cost of this kind of method. At training time, discriminative methods are inherently expensive, since they all involve iteratively checking current model performance on the training set, which means parsing the training set (usuall"
W04-3201,J03-4003,0,\N,Missing
W04-3201,W01-1802,0,\N,Missing
W05-0104,A00-1031,0,0.0360656,"Missing"
W05-0104,J90-2002,0,0.349077,"Missing"
W05-0104,J00-4006,0,0.00363808,"ound linguistic topics rather than mathematical methods. However, given the degree to which the course focused on such foundational methods, this order was perhaps a mistake. For example, it meant that simple word alignment models like IBM models 1 and 2 (Brown et 24 al., 1990) and the HMM model (Vogel et al., 1996) came many weeks after HMMs were introduced in the context of part-of-speech tagging. I also separated unsupervised learning into its own sub-sequence, where I now wish I had presented the unsupervised approaches to each task along with the supervised ones. I assigned readings from Jurafsky and Martin (2000) and Manning and Sch¨ utze (1999) for the first half of the course, but the second half was almost entirely based on papers from the research literature. This reflected both increasing sophistication on the part of the students and insufficient coverage of the latter topics in the textbooks. 4 Assignments The key component which characterized this course was the assignments. Each assignment is described below. They are available for use by other instructors. While licensing issues with the data make it impossible to put the entirety of the assignment materials on the web, some materials will b"
W05-0104,P03-1054,1,0.0244012,"Missing"
W05-0104,W96-0213,0,0.0285019,"Missing"
W05-0104,J01-2004,0,0.0321858,"ce for PCFGs Grammar Representations Lexicalized Dependency Models Other Parsing Models Word-to-Word Alignment Models Decoding Word-to-Word Models Syntactic Translation Models Document Clustering Word-Level Clustering Grammar Induction Lectures 2 2 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 Figure 1: Topics Covered. Each lecture was 80 minutes. the support harness. First, perplexity on heldout WSJ text was calculated. In this evaluation, reserving the correct mass for unknown words was important. Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001)). Finally, random sentences were generatively sampled from their models, giving students concrete feedback on how their models did (or did not) capture information about English. The support code intially provided an unsmoothed unigram model to get students started. They were then asked to build several more complex language models, including at least one higher-order interpolated model, and at least one model using GoodTuring or held-out smoothing. Beyond these requirements, students were encouraged to acheive the best possible word error rate and perplexity figures by whatever means they ch"
W05-0104,C96-2141,0,0.0125488,"retation) and their weaknesses (many interpretations arise from just a few rules, ambiguity poorly handled). From there, I discussed statistical approaches to problems of increasing complexity, spending a large amount of time on tree and sequence models. As mentioned above, I organized the lectures around linguistic topics rather than mathematical methods. However, given the degree to which the course focused on such foundational methods, this order was perhaps a mistake. For example, it meant that simple word alignment models like IBM models 1 and 2 (Brown et 24 al., 1990) and the HMM model (Vogel et al., 1996) came many weeks after HMMs were introduced in the context of part-of-speech tagging. I also separated unsupervised learning into its own sub-sequence, where I now wish I had presented the unsupervised approaches to each task along with the supervised ones. I assigned readings from Jurafsky and Martin (2000) and Manning and Sch¨ utze (1999) for the first half of the course, but the second half was almost entirely based on papers from the research literature. This reflected both increasing sophistication on the part of the students and insufficient coverage of the latter topics in the textbooks"
W06-2903,A00-2018,0,0.0276932,"nt comes from the learning of specialized grammars that capture non-local correlations. 1 Introduction The probabilistic context-free grammar (PCFG) formalism is the basis of most modern statistical parsers. The symbols in a PCFG encode contextfreedom assumptions about statistical dependencies in the derivations of sentences, and the relative conditional probabilities of the grammar rules induce scores on trees. Compared to a basic treebank grammar (Charniak, 1996), the grammars of highaccuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical (Charniak, 2000; Collins, 1999) or nonlexical (Klein and Manning, 2003; Matsuzaki et al., 2005) conditioning information. While such splitting, or conditioning, can cause problems for statistical estimation, it can dramatically improve the accuracy of a parser. However, the configurations exploited in PCFG parsers are quite local: rules’ probabilities may depend on parents or head words, but do not depend on arbitrarily distant tree configurations. For example, it is generally not modeled that if one quantifier phrase (QP in the Penn Treebank) appears in a sentence, the likelihood of finding another QP in th"
W06-2903,W01-0521,0,0.122497,"PRN → -LRB- NP -RRBADJP → QP PP → IN NP ADVP NP → NP PRN VP → VBN PP PP PP ADVP → NP RBR Score 131.6 77.1 33.7 28.4 17.3 13.3 12.3 12.3 11.6 10.1 Figure 1: Self-triggering: QP → # CD CD. If one British financial occurs in the sentence, the probability of seeing a second one in the same sentence is highly inreased. There is also a similar, but weaker, correlation for the American financial ($). On the right hand side we show the ten rules whose likelihoods are most increased in a sentence containing this rule. strong and that weakening them results in better models of language (Johnson, 1998; Gildea, 2001; Klein and Manning, 2003). In particular, certain grammar productions often cooccur with other productions, which may be either near or distant in the parse tree. In general, there exist three types of correlations: (i) local (e.g. parent-child), (ii) non-local, and (iii) self correlations (which may be local or non-local). In order to quantify the strength of a correlation, we use a likelihood ratio (LR). For two rules X → α and Y → β, we compute LR(X → α, Y → β) = P(α, β|X, Y ) P(α|X, Y )P(β|X, Y ) This measures how much more often the rules occur together than they would in the case of ind"
W06-2903,J98-4004,0,0.0343584,"→ VBD NP NP PP PRN → -LRB- NP -RRBADJP → QP PP → IN NP ADVP NP → NP PRN VP → VBN PP PP PP ADVP → NP RBR Score 131.6 77.1 33.7 28.4 17.3 13.3 12.3 12.3 11.6 10.1 Figure 1: Self-triggering: QP → # CD CD. If one British financial occurs in the sentence, the probability of seeing a second one in the same sentence is highly inreased. There is also a similar, but weaker, correlation for the American financial ($). On the right hand side we show the ten rules whose likelihoods are most increased in a sentence containing this rule. strong and that weakening them results in better models of language (Johnson, 1998; Gildea, 2001; Klein and Manning, 2003). In particular, certain grammar productions often cooccur with other productions, which may be either near or distant in the parse tree. In general, there exist three types of correlations: (i) local (e.g. parent-child), (ii) non-local, and (iii) self correlations (which may be local or non-local). In order to quantify the strength of a correlation, we use a likelihood ratio (LR). For two rules X → α and Y → β, we compute LR(X → α, Y → β) = P(α, β|X, Y ) P(α|X, Y )P(β|X, Y ) This measures how much more often the rules occur together than they would in t"
W06-2903,P03-1054,1,0.397849,"mars that capture non-local correlations. 1 Introduction The probabilistic context-free grammar (PCFG) formalism is the basis of most modern statistical parsers. The symbols in a PCFG encode contextfreedom assumptions about statistical dependencies in the derivations of sentences, and the relative conditional probabilities of the grammar rules induce scores on trees. Compared to a basic treebank grammar (Charniak, 1996), the grammars of highaccuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical (Charniak, 2000; Collins, 1999) or nonlexical (Klein and Manning, 2003; Matsuzaki et al., 2005) conditioning information. While such splitting, or conditioning, can cause problems for statistical estimation, it can dramatically improve the accuracy of a parser. However, the configurations exploited in PCFG parsers are quite local: rules’ probabilities may depend on parents or head words, but do not depend on arbitrarily distant tree configurations. For example, it is generally not modeled that if one quantifier phrase (QP in the Penn Treebank) appears in a sentence, the likelihood of finding another QP in that same sentence is greatly increased. This kind of eff"
W06-2903,P05-1010,0,0.0587569,"al correlations. 1 Introduction The probabilistic context-free grammar (PCFG) formalism is the basis of most modern statistical parsers. The symbols in a PCFG encode contextfreedom assumptions about statistical dependencies in the derivations of sentences, and the relative conditional probabilities of the grammar rules induce scores on trees. Compared to a basic treebank grammar (Charniak, 1996), the grammars of highaccuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical (Charniak, 2000; Collins, 1999) or nonlexical (Klein and Manning, 2003; Matsuzaki et al., 2005) conditioning information. While such splitting, or conditioning, can cause problems for statistical estimation, it can dramatically improve the accuracy of a parser. However, the configurations exploited in PCFG parsers are quite local: rules’ probabilities may depend on parents or head words, but do not depend on arbitrarily distant tree configurations. For example, it is generally not modeled that if one quantifier phrase (QP in the Penn Treebank) appears in a sentence, the likelihood of finding another QP in that same sentence is greatly increased. This kind of effect is neither surprising"
W06-2903,P85-1011,0,0.375937,"Missing"
W06-2903,H86-1020,0,\N,Missing
W06-2903,J03-4003,0,\N,Missing
W06-2903,C92-3126,0,\N,Missing
W06-3105,N03-1017,0,0.0553999,"al difference is that distinct word alignments cannot all be correct, while distinct segmentations can. Alternate segmentations rather than alternate alignments compete, resulting in increased determinization of the phrase table, decreased generalization, and decreased final BLEU score. We also show that interpolation of the two methods can result in a modest increase in BLEU score. 1 Introduction At the core of a phrase-based statistical machine translation system is a phrase table containing pairs of source and target language phrases, each weighted by a conditional translation probability. Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data. One particularly surprising result is that a simple heuristic extraction algorithm based on surface statistics of a word-aligned training set outperformed the phrase-based generative model proposed by Marcu and Wong (2002). This result is surprising in light of the reverse situation for word-based statistical translation. Specifically, in the task of word alignment, heuristic approaches such as the Dice coefficient consistently underperform their re-estimated counterparts, such as the IB"
W06-3105,N03-2036,0,0.0294915,"Missing"
W06-3105,J03-1002,0,0.0185038,"Missing"
W06-3105,W99-0604,0,0.573162,"Missing"
W06-3105,2002.tmi-tutorials.2,0,0.0640769,"result in a modest increase in BLEU score. 1 Introduction At the core of a phrase-based statistical machine translation system is a phrase table containing pairs of source and target language phrases, each weighted by a conditional translation probability. Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data. One particularly surprising result is that a simple heuristic extraction algorithm based on surface statistics of a word-aligned training set outperformed the phrase-based generative model proposed by Marcu and Wong (2002). This result is surprising in light of the reverse situation for word-based statistical translation. Specifically, in the task of word alignment, heuristic approaches such as the Dice coefficient consistently underperform their re-estimated counterparts, such as the IBM word alignment models (Brown et al., 1993). This well-known result is unsurprising: reestimation introduces an element of competition into the learning process. The key virtue of competition in word alignment is that, to a first approximation, only one source word should generate each target word. If a good alignment for a wor"
W06-3105,koen-2004-pharaoh,0,\N,Missing
W06-3105,J93-2003,0,\N,Missing
W08-1005,P05-1022,0,0.747949,"nitial grammar is hierarchically refined using an adaptive split-and-merge EM procedure, giving compact, accurate grammars. The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language specific or linguistically constrained features. Nonetheless, the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most high-performance parsers in one way or another (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005). However, as demonstrated in Charniak (1996) and Klein and Manning (2003), a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well. This naive grammar is a poor one because its contextfreedom assumptions are too strong in some ways (e.g. it assumes that subject and object NPs share the same distribution) and too weak in others (e.g. it assumes that long rewrites do not decompose into smaller steps). Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from simple tree annotation and symb"
W08-1005,A00-2018,0,0.0517578,"hod, a minimal initial grammar is hierarchically refined using an adaptive split-and-merge EM procedure, giving compact, accurate grammars. The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language specific or linguistically constrained features. Nonetheless, the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most high-performance parsers in one way or another (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005). However, as demonstrated in Charniak (1996) and Klein and Manning (2003), a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well. This naive grammar is a poor one because its contextfreedom assumptions are too strong in some ways (e.g. it assumes that subject and object NPs share the same distribution) and too weak in others (e.g. it assumes that long rewrites do not decompose into smaller steps). Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from si"
W08-1005,C02-1126,0,0.0501696,"Missing"
W08-1005,P05-1039,0,0.0663587,"Missing"
W08-1005,P96-1024,0,0.0581366,"nguistic like a parent annotation context, but which is formally just an integer. G therefore induces a derivation distribution over trees labeled with split symbols. This distribution in turn induces a parse distribution over (projected) trees with unsplit evaluation symbols. We have several choices of how to select a tree given these posterior distributions over trees. Since computing the most likely parse tree is NP-complete (Sima’an, 1992), we settle for an approximation that allows us to (partially) sum out the latent annotation. In Petrov and Klein (2007) we relate this approximation to Goodman (1996)’s labeled brackets algorithm applied to rules and to Matsuzaki et al. (2005)’s sentence specific variational approximation. This procedure is substantially superior to simply erasing the latent annotations from the the Viterbi derivation. 2.3 Results In Petrov and Klein (2007) we trained models for English, Chinese and German using the standard corpora and setups. We applied our latent variable model directly to each of the treebanks, without any 35 Table 1: Our split-and-merge latent variable approach produces the best published parsing performance on many languages. language dependent modif"
W08-1005,J98-4004,0,0.125561,"demonstrated in Charniak (1996) and Klein and Manning (2003), a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well. This naive grammar is a poor one because its contextfreedom assumptions are too strong in some ways (e.g. it assumes that subject and object NPs share the same distribution) and too weak in others (e.g. it assumes that long rewrites do not decompose into smaller steps). Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from simple tree annotation and symbol splitting (Johnson, 1998; Klein and Manning, 2003) to full lexicalization and intricate smoothing (Collins, 1999; Charniak, 2000). We view treebank parsing as the search for an optimally refined grammar consistent with a coarse training treebank. As a result, we begin with the provided evaluation symbols (such as NP, VP, etc.) but split them based on the statistical patterns in the training trees. A manual approach might take the symbol NP and subdivide it into one subsymbol NPˆS for subjects and another subsymbol NPˆVP for objects. However, rather than devising linguistically motivated features or splits, we take a"
W08-1005,P03-1054,1,0.132934,"procedure, giving compact, accurate grammars. The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language specific or linguistically constrained features. Nonetheless, the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most high-performance parsers in one way or another (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005). However, as demonstrated in Charniak (1996) and Klein and Manning (2003), a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well. This naive grammar is a poor one because its contextfreedom assumptions are too strong in some ways (e.g. it assumes that subject and object NPs share the same distribution) and too weak in others (e.g. it assumes that long rewrites do not decompose into smaller steps). Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from simple tree annotation and symbol splitting (Johnson, 1998; Klein and Manning, 2003) to full lexicalizati"
W08-1005,P05-1010,0,0.244311,"atent variable approach recovers linguistically interpretable phenomena. In our analysis, we pay particular attention to similarities and differences between 33 Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 33–39, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics FRAG ROOT NP FRAG-x RB Not . DT this NN year (a) . FRAG-x .-x RB-x NP-x Not DT-x NN-x this . year (b) Figure 1: (a) The original tree. (b) The binarized tree with latent variables. grammars learned from the two treebanks. 2 Latent Variable Parsing In latent variable parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006), we learn rule probabilities on latent annotations that, when marginalized out, maximize the likelihood of the unannotated training trees. We use an automatic approach in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of the training treebank. In this section we briefly review the main ideas in latent variable parsing. This work has been previously published and we therefore provide only a short overview. For a more detailed exposition of the learning algorithm the reader is referred to Petrov et al. (2006). Th"
W08-1005,N07-1051,1,0.948011,"ities on latent annotations that, when marginalized out, maximize the likelihood of the unannotated training trees. We use an automatic approach in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of the training treebank. In this section we briefly review the main ideas in latent variable parsing. This work has been previously published and we therefore provide only a short overview. For a more detailed exposition of the learning algorithm the reader is referred to Petrov et al. (2006). The corresponding inference procedure is described in detail in Petrov and Klein (2007). The parser, code, and trained models are available for download at http://nlp.cs.berkeley.edu. 2.1 Learning Starting with a simple X-bar grammar, we use the Expectation-Maximization (EM) algorithm to learn a new grammar whose nonterminals are subsymbols of the original evaluation nonterminals. The X-bar grammar is created by binarizing the treebank trees; for each local tree rooted at an evaluation nonterminal X, we introduce a cascade of new nodes labeled X so that each node has at most two children, see Figure 1. This initialization is the absolute minimum starting grammar that distinguish"
W08-1005,P06-1055,1,0.589636,"tically interpretable phenomena. In our analysis, we pay particular attention to similarities and differences between 33 Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 33–39, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics FRAG ROOT NP FRAG-x RB Not . DT this NN year (a) . FRAG-x .-x RB-x NP-x Not DT-x NN-x this . year (b) Figure 1: (a) The original tree. (b) The binarized tree with latent variables. grammars learned from the two treebanks. 2 Latent Variable Parsing In latent variable parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006), we learn rule probabilities on latent annotations that, when marginalized out, maximize the likelihood of the unannotated training trees. We use an automatic approach in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of the training treebank. In this section we briefly review the main ideas in latent variable parsing. This work has been previously published and we therefore provide only a short overview. For a more detailed exposition of the learning algorithm the reader is referred to Petrov et al. (2006). The corresponding inference procedure is"
W08-1005,J03-4003,0,\N,Missing
W10-2906,P07-2045,0,0.00225424,"or retraining, we used the same data, but weighted it to match the sizes of the original monolingual treebanks. We tested on the standard Chinese treebank development set, which also includes English translations. 9.1 Machine Translation Experiments Although we don’t have hand-labeled data for our largest Chinese-English parallel corpora, we can still evaluate our parsing results via our performance on a downstream machine translation (MT) task. Our experimental setup is as follows: first, we used the first 100,000 sentences of the EnglishChinese bitext from Wang et al. (2007) to train Moses (Koehn et al., 2007), a phrase-based MT system that we use as a baseline. We then used the same sentences to extract tree-to-string transducer rules from target-side (English) trees (Galley et al., 2004). We compare the single-reference BLEU scores of syntactic MT systems that result from using different parsers to generate these trees. Table 4 gives results for syntactic parsing. For comparison, we also show results for the supervised bilingual model of Burkett and Klein (2008). This model uses the same features at prediction time as the multiview trained “Bilingual w/ Full” model, but it is trained on hand-anno"
W10-2906,J93-2004,0,0.0453072,"st time. The texts used for retraining overlapped with the bitexts used for training the bilingual model, but both sets were disjoint from the test sets. 8 NER Experiments We demonstrate the utility of multiview learning for named entity recognition (NER) on English/German sentence pairs. We built both our full and weakened monolingual English and German models from the CoNLL 2003 shared task 9 Parsing Experiments Our next set of experiments are on syntactic parsing of English and Chinese. We trained both our full and weakened monolingual English models on the Penn Wall Street Journal corpus (Marcus et al., 1993), as described in Section 4. Our full and weakened Chinese models were trained on 2 Of course, unannotated monolingual data is even more plentiful, but as we will show, with the same amount of data, our method is more effective than simple monolingual selftraining. 51 Eng Parliament Prec Rec F1 Eng Newswire Ger Parliament Prec Rec F1 Prec Rec F1 Monolingual Models (Baseline) 58.5 67.7 83.0 74.6 71.3 36.4 48.2 68.4 80.1 88.7 84.2 69.8 44.0 54.0 Multiview Trained Bilingual Models 62.7 71.4 86.2 78.1 70.1 66.3 68.2 68.7 80.6 88.7 84.4 70.1 70.1 70.1 Retrained Monolingual Models 72.9 79.9 87.4 83."
W10-2906,E03-1035,0,0.158748,"Missing"
W10-2906,N07-1051,1,0.365502,"Figure 1 contains sample values for each of these features. Another natural setting where bilingual constraints can be exploited is syntactic parsing. Figure 2 shows an example English prepositional phrase attachment ambiguity that can be resolved bilingually by exploiting Chinese. The English monolingual parse mistakenly attaches to to the verb increased. In Chinese, however, this ambiguity does not exist. Instead, the word 对, which aligns to to, has strong selectional preference for attaching to a noun on the left. In our parsing experiments, we use the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), a split-merge latent variable parser, for our monolingual models. Our full model is the result of training the parser with five split-merge phases. Our weakened model uses only two. For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). Table 2 gives some examples, but does not exhaustively enumerate those features. 5 Our training algorithm is summarized in Figure 3. For each unlabeled point x = (x1 , x2 ), let yˆM be the joint label which has the highest score from the independent monolingual models (line ˆ λ ˆ1, λ ˆ2 1). We then find bilingual parameter"
W10-2906,D08-1092,1,0.701008,"odel consists of two views, which we will refer to as monolingual and bilingual. The monolingual view estimates the joint probability as the product of independent marginal distributions over each language, pM (y|x) = p1 (y1 |x1 )p2 (y2 |x2 ). In our applications, these marginal distributions will be computed by stateof-the-art statistical taggers and parsers trained on large monolingual corpora. This work focuses on learning parameters for the bilingual view of the data. We parameterize the bilingual view using at most one-to-one matchings between nodes of structured labels in each language (Burkett and Klein, 2008). In this work, we use the term node to indicate a particular component of a label, such as a single (multi-word) named entity or a node in a parse tree. In Figure 2(a), for example, the nodes labeled NP1 in both the Chinese and English trees are matched. Since we don’t know a priori how the components relate to one another, we treat these matchings as hidden. For each matching a and pair of labels y, we define a feature vector φ(y1 , a, y2 ) which factors on edges in the matching. Our model is a conditional exponential family distribution over matchings and labels: h i pθ (y, a|x) = exp θ > φ"
W10-2906,P06-1055,1,0.1235,"has the same label. Figure 1 contains sample values for each of these features. Another natural setting where bilingual constraints can be exploited is syntactic parsing. Figure 2 shows an example English prepositional phrase attachment ambiguity that can be resolved bilingually by exploiting Chinese. The English monolingual parse mistakenly attaches to to the verb increased. In Chinese, however, this ambiguity does not exist. Instead, the word 对, which aligns to to, has strong selectional preference for attaching to a noun on the left. In our parsing experiments, we use the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), a split-merge latent variable parser, for our monolingual models. Our full model is the result of training the parser with five split-merge phases. Our weakened model uses only two. For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). Table 2 gives some examples, but does not exhaustively enumerate those features. 5 Our training algorithm is summarized in Figure 3. For each unlabeled point x = (x1 , x2 ), let yˆM be the joint label which has the highest score from the independent monolingual models (line ˆ λ ˆ1, λ ˆ2 1). We then"
W10-2906,P05-1022,0,0.00423527,"able 1: Sample features used for named entity recognition for the ORG entity in Figure 1. Berichte des Europäischen Rechnungshofes ORG1 rate predictions, but ensure that bilingual features will be required to optimize the training objective. W W W Let `W 1 = log p1 (y1 |x1 ), `2 = log p2 (y2 |x2 ) be the log-probability scores from the weakened models. Our final approximation to the marginal distribution over labels y is: h Figure 1: An example where English NER can be used to disambiguate German NER. We further simplify inference in our model by working in a reranking setting (Collins, 2000; Charniak and Johnson, 2005), where we only consider the top k outputs from monolingual models in both languages, for a total of k 2 labels y. In practice, k 2 ≤ 10, 000 for our largest problem. 3.1 Examples I NSIDE B OTH=3 I N E N O NLY=0 L BL M ATCH=true B IAS=true def W qλ1 ,λ2 ,θ (y|x) = max exp λ1 `W 1 + λ2 `2 + a i ˜ 1 , λ2 , θ; x) . θ > φ(y1 , a, y2 ) − A(λ (1) Where ˜ 1 , λ2 , θ; x) = A(λ h i X W > log max exp λ1 `W 1 + λ2 `2 + θ φ(y1 , a, y2 ) Including Weakened Models Now that we have defined our bilingual model, we could train it to agree with the output of the monolingual model (Collins and Singer, 1999; Ganc"
W10-2906,W99-0613,0,0.739657,"0; Charniak and Johnson, 2005), where we only consider the top k outputs from monolingual models in both languages, for a total of k 2 labels y. In practice, k 2 ≤ 10, 000 for our largest problem. 3.1 Examples I NSIDE B OTH=3 I N E N O NLY=0 L BL M ATCH=true B IAS=true def W qλ1 ,λ2 ,θ (y|x) = max exp λ1 `W 1 + λ2 `2 + a i ˜ 1 , λ2 , θ; x) . θ > φ(y1 , a, y2 ) − A(λ (1) Where ˜ 1 , λ2 , θ; x) = A(λ h i X W > log max exp λ1 `W 1 + λ2 `2 + θ φ(y1 , a, y2 ) Including Weakened Models Now that we have defined our bilingual model, we could train it to agree with the output of the monolingual model (Collins and Singer, 1999; Ganchev et al., 2008). As we will see in Section 4, however, the feature functions φ(y1 , a, y2 ) make no reference to the input sentences x, other than through a fixed word alignment. With such limited monolingual information, it is impossible for the bilingual model to adequately capture all of the information necessary for NER or parsing. As a simple example, a bilingual NER model will be perfectly happy to label two aligned person names as ORG instead of PER: both labelings agree equally well. We briefly illustrate how poorly such a basic bilingual model performs in Section 10. One way t"
W10-2906,W04-3207,0,0.293071,"llel text (after machine translation) (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009). They assume the existence of a good, monolingual model for one language but little or no information about the second language. Given a parallel sentence pair, they use the annotations for one language to heavily constrain the set of possible annotations for the other. Our work falls into the final category: We wish to use bilingual data to improve monolingual models which are already trained on large amounts of data and effective on their own (Huang and Vogel, 2002; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Procedurally, our work is most closely related to that of Burkett and Klein (2008). They used an annotated bitext to learn parse reranking models for English and Chinese, exploiting features that examine pieces of parse trees in both languages. Our method can be thought of as the semi-supervised counterpart to their supervised model. Indeed, we achieve nearly the same results, but without annotated bitexts. Smith and Smith (2004) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they con"
W10-2906,P05-1045,0,0.00791798,"Missing"
W10-2906,P09-1009,0,0.0824724,"r each matching a and pair of labels y, we define a feature vector φ(y1 , a, y2 ) which factors on edges in the matching. Our model is a conditional exponential family distribution over matchings and labels: h i pθ (y, a|x) = exp θ > φ(y1 , a, y2 ) − A(θ; x) , Prior Work on Learning from Bilingual Text Prior work in learning monolingual models from bitexts falls roughly into three categories: Unsupervised induction, cross-lingual projection, and bilingual constraints for supervised monolingual models. Two recent, successful unsupervised induction methods are those of Blunsom et al. (2009) and Snyder et al. (2009). Both of them estimate hierarchical Bayesian models and employ bilingual data to constrain the types of models that can be derived. Projection methods, on the other hand, were among the first applications of parallel text (after machine translation) (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009). They assume the existence of a good, monolingual model for one language but little or no information about the second language. Given a parallel sentence pair, they use the annotations for one language to heavily constrain the set of possible annotations for"
W10-2906,N04-1035,0,0.00650347,"includes English translations. 9.1 Machine Translation Experiments Although we don’t have hand-labeled data for our largest Chinese-English parallel corpora, we can still evaluate our parsing results via our performance on a downstream machine translation (MT) task. Our experimental setup is as follows: first, we used the first 100,000 sentences of the EnglishChinese bitext from Wang et al. (2007) to train Moses (Koehn et al., 2007), a phrase-based MT system that we use as a baseline. We then used the same sentences to extract tree-to-string transducer rules from target-side (English) trees (Galley et al., 2004). We compare the single-reference BLEU scores of syntactic MT systems that result from using different parsers to generate these trees. Table 4 gives results for syntactic parsing. For comparison, we also show results for the supervised bilingual model of Burkett and Klein (2008). This model uses the same features at prediction time as the multiview trained “Bilingual w/ Full” model, but it is trained on hand-annotated parses. We first examine the first four rows of Table 4. The “Bilingual w/ Full” model significantly improves performance in both English and Chinese relative to the monolingual"
W10-2906,C02-1145,0,0.0106436,"the Penn Treebank, are larger than those in Chinese, which is in domain. We also emphasize that, unlike our NER data, this bitext was fairly small relative to the annotated monolingual data. Therefore, while we still learn good bilingual model parameters which give a sizable agreement-based boost when doing bilingual prediction, we don’t expect retraining to result in a coverage-based boost in monolingual performance. Table 4: Parsing results. Rows are grouped by data condition. We bold entries that are best in their group and beat the the Full Monolingual baseline. the Penn Chinese treebank (Xue et al., 2002) (articles 400-1151), excluding the bilingual portion. The bilingual data consists of the parallel part of the Chinese treebank (articles 1-270), which also includes manually parsed English translations of each Chinese sentence (Bies et al., 2007). Only the Chinese sentences and their English translations were used to train the bilingual models – the gold trees were ignored. For retraining, we used the same data, but weighted it to match the sizes of the original monolingual treebanks. We tested on the standard Chinese treebank development set, which also includes English translations. 9.1 Mac"
W10-2906,P09-1042,0,0.431985,"rsed text. We achieve nearly identical improvements using a purely unlabeled bitext. These results carry over to machine translation, where we can achieve slightly better BLEU improvements than the supervised model of Burkett and Klein (2008) since we are able to train our model directly on the parallel data where we perform rule extraction. Introduction Natural language analysis in one language can be improved by exploiting translations in another language. This observation has formed the basis for important work on syntax projection across languages (Yarowsky et al., 2001; Hwa et al., 2005; Ganchev et al., 2009) and unsupervised syntax induction in multiple languages (Snyder et al., 2009), as well as other tasks, such as cross-lingual named entity recognition (Huang and Vogel, 2002; Moore, 2003) and information retrieval (Si and Callan, 2005). In all of these cases, multilingual models yield increased accuracy because different languages present different ambiguities and therefore offer complementary constraints on the shared underlying labels. In the present work, we consider a setting where we already possess supervised monolingual models, and wish to improve these models using unannotated bilingua"
W10-2906,N01-1026,0,0.756885,"ual Text Prior work in learning monolingual models from bitexts falls roughly into three categories: Unsupervised induction, cross-lingual projection, and bilingual constraints for supervised monolingual models. Two recent, successful unsupervised induction methods are those of Blunsom et al. (2009) and Snyder et al. (2009). Both of them estimate hierarchical Bayesian models and employ bilingual data to constrain the types of models that can be derived. Projection methods, on the other hand, were among the first applications of parallel text (after machine translation) (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009). They assume the existence of a good, monolingual model for one language but little or no information about the second language. Given a parallel sentence pair, they use the annotations for one language to heavily constrain the set of possible annotations for the other. Our work falls into the final category: We wish to use bilingual data to improve monolingual models which are already trained on large amounts of data and effective on their own (Huang and Vogel, 2002; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Procedura"
W10-2906,H01-1035,0,0.415326,"nolingual parsers using parallel, hand-parsed text. We achieve nearly identical improvements using a purely unlabeled bitext. These results carry over to machine translation, where we can achieve slightly better BLEU improvements than the supervised model of Burkett and Klein (2008) since we are able to train our model directly on the parallel data where we perform rule extraction. Introduction Natural language analysis in one language can be improved by exploiting translations in another language. This observation has formed the basis for important work on syntax projection across languages (Yarowsky et al., 2001; Hwa et al., 2005; Ganchev et al., 2009) and unsupervised syntax induction in multiple languages (Snyder et al., 2009), as well as other tasks, such as cross-lingual named entity recognition (Huang and Vogel, 2002; Moore, 2003) and information retrieval (Si and Callan, 2005). In all of these cases, multilingual models yield increased accuracy because different languages present different ambiguities and therefore offer complementary constraints on the shared underlying labels. In the present work, we consider a setting where we already possess supervised monolingual models, and wish to improv"
W11-1916,N10-1061,1,0.916509,"n Coreference resolution is concerned with identifying mentions of entities in text and determining which mentions are referring to the same entity. Previously the focus in the field has been on the latter task. Typically, mentions were considered correct if their span was within the true span of a gold mention, and contained the head word. This task (Pradhan et al., 2011) has set a harder challenge by only considering exact matches to be correct. Our system uses an unsupervised approach based on a generative model. Unlike previous work, we did not use the Bllip or Wikipedia data described in Haghighi and Klein (2010). This was necessary for the system to be eligible for the closed task. The system detects mentions by finding the maximal projection of every noun and pronoun. For the OntoNotes corpus this approach posed several problems. First, the annotation scheme explicitly rejects noun phrases in certain constructions. And second, it includes coreference for events as well as things. In preliminary experiments on the development set, we found that spurious mentions were our primary source of error. Using an oracle to exclude all spurious mentions at evaluation time yielded improvements ranging from five"
W11-1916,P06-1055,1,0.631097,"a spurious mention, or because it is not co-referent. Without manually annotating the singletons in the data, these two cases cannot be easily separated. 3.1 Baseline mention detection The standard approach used in the system to detect mentions is to consider each word and its maximal projection, accepting it only if the span is an NP or the word is a pronoun. This approach will introduce spurious mentions if the parser makes a mistake, or if the NP is not considered a mention in the OntoNotes corpus. In this work, we considered the provided parses and parses produced by the Berkeley parser (Petrov et al., 2006) trained on the provided training data. We added a set of filters based on the annotation scheme described by Pradhan et al. (2007). Some filters are applied before coreference resolution and others afterward, as described below. Data Set Dev Test Filters None Pre Post All All P 37.59 39.49 59.05 58.69 56.97 R 76.93 76.83 68.08 67.98 69.77 F 50.50 52.17 63.24 63.00 62.72 Filters None Pre Post All Table 1: Mention detection performance with various subsets of the filters. 3.2 Before Coreference Resolution The pre-resolution filters were based on three reliable features of spurious mentions: • A"
W11-1916,W11-1901,0,0.0603164,"system behavior on the development set. These changes led to improvements in coreference F-score of 10.06, 5.71, 6.78, 6.63 and 3.09 on the MUC, B3 , Ceaf-e, Ceaf-m and Blanc, metrics, respectively, and a final task score of 47.10. 1 Introduction Coreference resolution is concerned with identifying mentions of entities in text and determining which mentions are referring to the same entity. Previously the focus in the field has been on the latter task. Typically, mentions were considered correct if their span was within the true span of a gold mention, and contained the head word. This task (Pradhan et al., 2011) has set a harder challenge by only considering exact matches to be correct. Our system uses an unsupervised approach based on a generative model. Unlike previous work, we did not use the Bllip or Wikipedia data described in Haghighi and Klein (2010). This was necessary for the system to be eligible for the closed task. The system detects mentions by finding the maximal projection of every noun and pronoun. For the OntoNotes corpus this approach posed several problems. First, the annotation scheme explicitly rejects noun phrases in certain constructions. And second, it includes coreference for"
W11-1916,J03-4003,0,\N,Missing
W14-1607,J13-2005,1,0.795549,"k, we consider language that deIntroduction This paper introduces a probabilistic model for predicting grounded, real-valued trajectories from natural language text. A long tradition of research in compositional semantics has focused on discrete representations of meaning. The original focus of such work was on logical translation: mapping statements of natural language to a formal language like first-order logic (Zettlemoyer and Collins, 2005) or database queries (Zelle and Mooney, 1996). Subsequent work has integrated this logical translation with interpretation against a symbolic database (Liang et al., 2013). There has been a recent increase in interest in perceptual grounding, where lexical semantics anchor in perceptual variables (points, distances, etc.) derived from images or video. Bruni et al. (2014) describe a procedure for constructing word representations using text- and image-based dis58 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 58–67, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics scribes time-evolving trajectories, and unlike Yu and Siskind (2013), we allow these trajectories to have event substructur"
W14-1607,Q13-1005,0,0.0197985,"te complex. Head59 considerably less formal than the language found in the Wall Street Journal examples, involving disfluency, redundancy and occasionally errors. Nevertheless the underlying structure of this problem and the stock problem are fundamentally similar. In addition to Vogel and Jurafsky, Tellex et al. (2011) give a weakly-supervised model for mapping single sentences to commands, and Branavan et al. (2009) give an alternative reinforcementlearning approach for following long command sequences. An intermediate between this approach and ours is the work of Chen and Mooney (2011) and Artzi and Zettlemoyer (2013), which bootstrap a semantic parser to generate logical forms specifying the output path, rather than predicting the path directly. right round the white water [. . . ] but stay quite close ’cause you don’t otherwise you’re going to be in that stone creek Figure 3: Example map data: a portion of a map, and a single line from a dialog which describes navigation relative to the two visible landmarks. Between them, these tasks span a wide range of linguistic phenomena relevant to grounded semantics, and provide a demonstration of the usefulness and general applicability of our model. While develo"
W14-1607,N07-1051,1,0.547524,"ng model: approx. to V (stocks only) Cartesian prod. of ϕt (T ) with: ϕa (T, Ai , Ai−1 ) ■ I[Ai is aligned] ■ I[Ai−1 is aligned] ■ A1 − Ai−1 (if both aligned) Table 1: Features used for linear parameterization of the grounding model. simplify notation by writing Ti = ϕt (Ti ) and Vi = ϕv (Vi ). As the ultimate prediction task is to produce paths, and not their featurized representations, we will assume that it is also straightforward to compute ϕ−1 v , which projects path features back into the original grounding domain. All parse trees are predicted from input text using the Berkeley Parser (Petrov and Klein, 2007). Feature representations for both trees and paths are simple and largely domain-independent; they are explicitly enumerated in Table 1. The general framework presented here leaves one significant problem unaddressed: given a large state vector encoding properties of multiple objects, how do we resolve an utterance about a single object to the correct subset of indices in the vector? While none of the tasks considered in this paper require an argument resolution step of this kind, interpretation of noun phrases is one of the better-studied problems in compositional semantics (Zelle and Mooney"
W14-1607,P09-1010,0,0.36793,"rization—in this case just a single number describing the total value of the index—but as shown by the headline example in Figure 1, the language used to describe changes in the stock market can be quite complex. Head59 considerably less formal than the language found in the Wall Street Journal examples, involving disfluency, redundancy and occasionally errors. Nevertheless the underlying structure of this problem and the stock problem are fundamentally similar. In addition to Vogel and Jurafsky, Tellex et al. (2011) give a weakly-supervised model for mapping single sentences to commands, and Branavan et al. (2009) give an alternative reinforcementlearning approach for following long command sequences. An intermediate between this approach and ours is the work of Chen and Mooney (2011) and Artzi and Zettlemoyer (2013), which bootstrap a semantic parser to generate logical forms specifying the output path, rather than predicting the path directly. right round the white water [. . . ] but stay quite close ’cause you don’t otherwise you’re going to be in that stone creek Figure 3: Example map data: a portion of a map, and a single line from a dialog which describes navigation relative to the two visible la"
W14-1607,P10-1083,0,0.41456,"nstructing word representations using text- and image-based dis58 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 58–67, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics scribes time-evolving trajectories, and unlike Yu and Siskind (2013), we allow these trajectories to have event substructure, and model temporal ordering. Our class of models generalizes to a variety of different domains: a new color-picking task, a new financial news task, and a more challenging variant of the direction-following task established by Vogel and Jurafsky (2010). As an example of the kinds of phenomena we want to model, consider Figure 1, which shows the value of the Dow Jones Industrial Average over June 3rd and 4th 2008, along with a financial news headline from June 4th. There are several effects of interest here. One phenomenon we want to capture is that the lexical semantics of individual words must be combined: swoon roughly describes a drop while bruising indicates that the drop was severe. We isolate this lexical combination in Section 4, where we consider a limited model of color descriptions (Figure 2). A second phenomenon is that the descr"
W14-1607,P13-1006,0,0.173813,"slation with interpretation against a symbolic database (Liang et al., 2013). There has been a recent increase in interest in perceptual grounding, where lexical semantics anchor in perceptual variables (points, distances, etc.) derived from images or video. Bruni et al. (2014) describe a procedure for constructing word representations using text- and image-based dis58 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 58–67, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics scribes time-evolving trajectories, and unlike Yu and Siskind (2013), we allow these trajectories to have event substructure, and model temporal ordering. Our class of models generalizes to a variety of different domains: a new color-picking task, a new financial news task, and a more challenging variant of the direction-following task established by Vogel and Jurafsky (2010). As an example of the kinds of phenomena we want to model, consider Figure 1, which shows the value of the Dow Jones Industrial Average over June 3rd and 4th 2008, along with a financial news headline from June 4th. There are several effects of interest here. One phenomenon we want to cap"
W14-1607,D10-1040,1,0.900242,"Missing"
W14-1607,N09-1031,0,0.0135557,"ning or end of the trading day. Along with temporal structure, the problem requires a more sophisticated treatment of syntax than the colors case— now we have to identify which subspans of the sentence are associated with each event observed, and determine the correspondence between surface order and actual order in time. The learning of correspondences between text and time series has attracted more interest in natural language generation than in semantics (Yu et al., 2007). Research on natural language processing and stock data, meanwhile, has largely focused on prediction of future events (Kogan et al., 2009). 3 Preliminaries In the experiments that follow, each training example will consist of: – Natural language text, consisting of a constituency parse tree or trees. For a given example, we will denote the associated trees (T1 , T2 , . . .). These are also observed at test time, and used to predict new groundings. – A vector-valued, grounded observation, or a sequence of observations (a path), which we will denote V for a given example. We will further assume that each of these paths has been pre-segmented (discussed in detail in Section 5) into a sequence (V1 , V2 , . . .). These are only obser"
W14-1607,P11-1060,1,\N,Missing
W14-1607,Q13-1016,0,\N,Missing
