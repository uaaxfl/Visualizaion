2021.semeval-1.37,{R}o{M}a at {S}em{E}val-2021 Task 7: A Transformer-based Approach for Detecting and Rating Humor and Offense,2021,-1,-1,4,0,1719,roberto labadie,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"In this paper we describe the systems used by the RoMa team in the shared task on Detecting and Rating Humor and Offense (HaHackathon) at SemEval 2021. Our systems rely on data representations learned through fine-tuned neural language models. Particularly, we explore two distinct architectures. The first one is based on a Siamese Neural Network (SNN) combined with a graph-based clustering method. The SNN model is used for learning a latent space where instances of humor and non-humor can be distinguished. The clustering method is applied to build prototypes of both classes which are used for training and classifying new messages. The second one combines neural language model representations with a linear regression model which makes the final ratings. Our systems achieved the best results for humor classification using model one, whereas for offensive and humor rating the second model obtained better performance. In the case of the controversial humor prediction, the most significant improvement was achieved by a fine-tuning of the neural language model. In general, the results achieved are encouraging and give us a starting point for further improvements."
2021.eacl-main.56,{F}ake{F}low: Fake News Detection by Modeling the Flow of Affective Information,2021,-1,-1,3,1,524,bilal ghanem,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Fake news articles often stir the readers{'} attention by means of emotional appeals that arouse their feelings. Unlike in short news texts, authors of longer articles can exploit such affective factors to manipulate readers by adding exaggerations or fabricating events, in order to affect the readers{'} emotions. To capture this, we propose in this paper to model the flow of affective information in fake news articles using a neural architecture. The proposed model, FakeFlow, learns this flow by combining topic and affective information extracted from text. We evaluate the model{'}s performance with several experiments on four real-world datasets. The results show that FakeFlow achieves superior results when compared against state-of-the-art methods, thus confirming the importance of capturing the flow of the affective information in news articles."
2021.clpsych-1.24,Understanding Patterns of Anorexia Manifestations in Social Media Data with Deep Learning,2021,-1,-1,3,0,3052,ana uban,Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access,0,"Eating disorders are a growing problem especially among young people, yet they have been under-studied in computational research compared to other mental health disorders such as depression. Computational methods have a great potential to aid with the automatic detection of mental health problems, but state-of-the-art machine learning methods based on neural networks are notoriously difficult to interpret, which is a crucial problem for applications in the mental health domain. We propose leveraging the power of deep learning models for automatically detecting signs of anorexia based on social media data, while at the same time focusing on interpreting their behavior. We train a hierarchical attention network to detect people with anorexia and use its internal encodings to discover different clusters of anorexia symptoms. We interpret the identified patterns from multiple perspectives, including emotion expression, psycho-linguistic features and personality traits, and we offer novel hypotheses to interpret our findings from a psycho-social perspective. Some interesting findings are patterns of word usage in some users with anorexia which show that they feel less as being part of a group compared to control cases, as well as that they have abandoned explanatory activity as a result of a greater feeling of helplessness and fear."
2020.semeval-1.115,{PRHLT}-{UPV} at {S}em{E}val-2020 Task 8: Study of Multimodal Techniques for Memes Analysis,2020,-1,-1,2,0,15151,gretel sarracen,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"This paper describes the system submitted by the PRHLT-UPV team for the task 8 of SemEval-2020: Memotion Analysis. We propose a multimodal model that combines pretrained models of the BERT and VGG architectures. The BERT model is used to process the textual information and VGG the images. The multimodal model is used to classify memes according to the presence of offensive, sarcastic, humorous and motivating content. Also, a sentiment analysis of memes is carried out with the proposed model. In the experiments, the model is compared with other approaches to analyze the relevance of the multimodal model. The results show encouraging performances on the final leaderboard of the competition, reaching good positions in the ranking of systems."
2020.semeval-1.172,{LIMSI}{\\_}{UPV} at {S}em{E}val-2020 Task 9: Recurrent Convolutional Neural Network for Code-mixed Sentiment Analysis,2020,-1,-1,5,0,8790,somnath banerjee,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"This paper describes the participation of LIMSI{\_}UPV team in SemEval-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text. The proposed approach competed in SentiMix HindiEnglish subtask, that addresses the problem of predicting the sentiment of a given Hindi-English code-mixed tweet. We propose Recurrent Convolutional Neural Network that combines both the recurrent neural network and the convolutional network to better capture the semantics of the text, for code-mixed sentiment analysis. The proposed system obtained 0.69 (best run) in terms of F1 score on the given test data and achieved the 9th place (Codalab username: somban) in the SentiMix Hindi-English subtask."
2020.semeval-1.209,{PRHLT}-{UPV} at {S}em{E}val-2020 Task 12: {BERT} for Multilingual Offensive Language Detection,2020,-1,-1,2,0,15151,gretel sarracen,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"The present paper describes the system submitted by the PRHLT-UPV team for the task 12 of SemEval-2020: OffensEval 2020. The official title of the task is Multilingual Offensive Language Identification in Social Media, and aims to identify offensive language in texts. The languages included in the task are English, Arabic, Danish, Greek and Turkish. We propose a model based on the BERT architecture for the analysis of texts in English. The approach leverages knowledge within a pre-trained model and performs fine-tuning for the particular task. In the analysis of the other languages the Multilingual BERT is used, which has been pre-trained for a large number of languages. In the experiments, the proposed method for English texts is compared with other approaches to analyze the relevance of the architecture used. Furthermore, simple models for the other languages are evaluated to compare them with the proposed one. The experimental results show that the model based on BERT outperforms other approaches. The main contribution of this work lies in this study, despite not obtaining the first positions in most cases of the competition ranking."
2020.restup-1.1,"Profiling Bots, Fake News Spreaders and Haters",2020,-1,-1,1,1,1722,paolo rosso,Proceedings of the Workshop on Resources and Techniques for User and Author Profiling in Abusive Language,0,"Author profiling studies how language is shared by people. Stylometry techniques help in identifying aspects such as gender, age, native language, or even personality. Author profiling is a problem of growing importance, not only in marketing and forensics, but also in cybersecurity. The aim is not only to identify users whose messages are potential threats from a terrorism viewpoint but also those whose messages are a threat from a social exclusion perspective because containing hate speech, cyberbullying etc. Bots often play a key role in spreading hate speech, as well as fake news, with the purpose of polarizing the public opinion with respect to controversial issues like Brexit or the Catalan referendum. For instance, the authors of a recent study about the 1 Oct 2017 Catalan referendum, showed that in a dataset with 3.6 million tweets, about 23.6{\%} of tweets were produced by bots. The target of these bots were pro-independence influencers that were sent negative, emotional and aggressive hateful tweets with hashtags such as {\#}sonunesbesties (i.e. {\#}theyareanimals). Since 2013 at the PAN Lab at CLEF (https://pan.webis.de/) we have addressed several aspects of author profiling in social media. In 2019 we investigated the feasibility of distinguishing whether the author of a Twitter feed is a bot, while this year we are addressing the problem of profiling those authors that are more likely to spread fake news in Twitter because they did in the past. We aim at identifying possible fake news spreaders as a first step towards preventing fake news from being propagated among online users (fake news aim to polarize the public opinion and may contain hate speech). In 2021 we specifically aim at addressing the challenging problem of profiling haters in social media in order to monitor abusive language and prevent cases of social exclusion in order to combat, for instance, racism, xenophobia and misogyny. Although we already started addressing the problem of detecting hate speech when targets are immigrants or women at the HatEval shared task in SemEval-2019, and when targets are women also in the Automatic Misogyny Identification tasks at IberEval-2018, Evalita-2018 and Evalita-2020, it was not done from an author profiling perspective. At the end of the keynote, I will present some insights in order to stress the importance of monitoring abusive language in social media, for instance, in foreseeing sexual crimes. In fact, previous studies confirmed that a correlation might lay between the yearly per capita rate of rape and the misogynistic language used in Twitter."
2020.lrec-1.627,Marking Irony Activators in a {U}niversal {D}ependencies Treebank: The Case of an {I}talian {T}witter Corpus,2020,-1,-1,4,1,17905,alessandra cignarella,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The recognition of irony is a challenging task in the domain of Sentiment Analysis, and the availability of annotated corpora may be crucial for its automatic processing. In this paper we describe a fine-grained annotation scheme centered on irony, in which we highlight the tokens that are responsible for its activation, (irony activators) and their morpho-syntactic features. As our case study we therefore introduce a recently released Universal Dependencies treebank for Italian which includes ironic tweets: TWITTIR{\`O}-UD. For the purposes of this study, we enriched the existing annotation in the treebank, with a further level that includes irony activators. A description and discussion of the annotation scheme is provided with a definition of irony activators and the guidelines for their annotation. This qualitative study on the different layers of annotation applied on the same dataset can shed some light on the process of human annotation, and irony annotation in particular, and on the usefulness of this representation for developing computational models of irony to be used for training purposes."
2020.coling-main.116,Multilingual Irony Detection with Dependency Syntax and Neural Models,2020,-1,-1,5,1,17905,alessandra cignarella,Proceedings of the 28th International Conference on Computational Linguistics,0,"This paper presents an in-depth investigation of the effectiveness of dependency-based syntactic features on the irony detection task in a multilingual perspective (English, Spanish, French and Italian). It focuses on the contribution from syntactic knowledge, exploiting linguistic resources where syntax is annotated according to the Universal Dependencies scheme. Three distinct experimental settings are provided. In the first, a variety of syntactic dependency-based features combined with classical machine learning classifiers are explored. In the second scenario, two well-known types of word embeddings are trained on parsed data and tested against gold standard datasets. In the third setting, dependency-based syntactic features are combined into the Multilingual BERT architecture. The results suggest that fine-grained dependency-based syntactic information is informative for the detection of irony."
W19-7723,Presenting {TWITTIR{\\`O}}-{UD}: An {I}talian {T}witter Treebank in {U}niversal {D}ependencies,2019,0,0,3,1,17905,alessandra cignarella,"Proceedings of the Fifth International Conference on Dependency Linguistics (Depling, SyntaxFest 2019)",0,None
S19-2007,{S}em{E}val-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in {T}witter,2019,0,26,7,0.499792,7,valerio basile,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"The paper describes the organization of the SemEval 2019 Task 5 about the detection of hate speech against immigrants and women in Spanish and English messages extracted from Twitter. The task is organized in two related classification subtasks: a main binary subtask for detecting the presence of hate speech, and a finer-grained one devoted to identifying further features in hateful contents such as the aggressive attitude and the target harassed, to distinguish if the incitement is against an individual rather than a group. HatEval has been one of the most popular tasks in SemEval-2019 with a total of 108 submitted runs for Subtask A and 70 runs for Subtask B, from a total of 74 different teams. Data provided for the task are described by showing how they have been collected and annotated. Moreover, the paper provides an analysis and discussion about the participant systems and the results they achieved in both subtasks."
S19-2104,{D}eep{A}nalyzer at {S}em{E}val-2019 Task 6: A deep learning-based ensemble method for identifying offensive tweets,2019,0,1,2,0,25019,gretel pena,Proceedings of the 13th International Workshop on Semantic Evaluation,0,This paper describes the system we developed for SemEval 2019 on Identifying and Categorizing Offensive Language in Social Media (OffensEval - Task 6). The task focuses on offensive language in tweets. It is organized into three sub-tasks for offensive language identification; automatic categorization of offense types and offense target identification. The approach for the first subtask is a deep learning-based ensemble method which uses a Bidirectional LSTM Recurrent Neural Network and a Convolutional Neural Network. Additionally we use the information from part-of-speech tagging of tweets for target identification and combine previous results for categorization of offense types.
S19-2135,{TUVD} team at {S}em{E}val-2019 Task 6: Offense Target Identification,2019,0,1,3,0,25100,elena shushkevich,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"This article presents our approach for detecting a target of offensive messages in Twitter, including Individual, Group and Others classes. The model we have created is an ensemble of simpler models, including Logistic Regression, Naive Bayes, Support Vector Machine and the interpolation between Logistic Regression and Naive Bayes with 0.25 coefficient of interpolation. The model allows us to achieve 0.547 macro F1-score."
S19-2197,{UPV}-28-{UNITO} at {S}em{E}val-2019 Task 7: Exploiting Post{'}s Nesting and Syntax Information for Rumor Stance Classification,2019,0,3,4,1,524,bilal ghanem,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"In the present paper we describe the UPV-28-UNITO system{'}s submission to the RumorEval 2019 shared task. The approach we applied for addressing both the subtasks of the contest exploits both classical machine learning algorithms and word embeddings, and it is based on diverse groups of features: stylistic, lexical, emotional, sentiment, meta-structural and Twitter-based. A novel set of features that take advantage of the syntactic information in texts is moreover introduced in the paper."
W18-5510,Stance Detection in Fake News A Combined Feature Representation,2018,0,10,2,1,524,bilal ghanem,Proceedings of the First Workshop on Fact Extraction and {VER}ification ({FEVER}),0,"With the uncontrolled increasing of fake news and rumors over the Web, different approaches have been proposed to address the problem. In this paper, we present an approach that combines lexical, word embeddings and n-gram features to detect the stance in fake news. Our approach has been tested on the Fake News Challenge (FNC-1) dataset. Given a news title-article pair, the FNC-1 task aims at determining the relevance of the article and the title. Our proposed approach has achieved an accurate result (59.6 {\%} Macro F1) that is close to the state-of-the-art result with 0.013 difference using a simple feature representation. Furthermore, we have investigated the importance of different lexicons in the detection of the classification labels."
W18-1605,Cross-corpus Native Language Identification via Statistical Embedding,2018,0,0,2,1,10591,francisco rangel,Proceedings of the Second Workshop on Stylistic Variation,0,"In this paper, we approach the task of native language identification in a realistic cross-corpus scenario where a model is trained with available data and has to predict the native language from data of a different corpus. The motivation behind this study is to investigate native language identification in the Australian academic scenario where a majority of students come from China, Indonesia, and Arabic-speaking nations. We have proposed a statistical embedding representation reporting a significant improvement over common single-layer approaches of the state of the art, identifying Chinese, Arabic, and Indonesian in a cross-corpus scenario. The proposed approach was shown to be competitive even when the data is scarce and imbalanced."
S18-1086,{LDR} at {S}em{E}val-2018 Task 3: A Low Dimensional Text Representation for Irony Detection,2018,0,2,3,1,524,bilal ghanem,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"In this paper we describe our participation in the SemEval-2018 task 3 Shared Task on Irony Detection. We have approached the task with our low dimensionality representation method (LDR), which exploits low dimensional features extracted from text on the basis of the occurrence probability of the words depending on each class. Our intuition is that words in ironic texts have different probability of occurrence than in non-ironic ones. Our approach obtained acceptable results in both subtasks A and B. We have performed an error analysis that shows the difference on correct and incorrect classified tweets."
S18-1097,{INAOE}-{UPV} at {S}em{E}val-2018 Task 3: An Ensemble Approach for Irony Detection in {T}witter,2018,0,2,4,0,28840,delia farias,Proceedings of The 12th International Workshop on Semantic Evaluation,0,This paper describes an ensemble approach to the SemEval-2018 Task 3. The proposed method is composed of two renowned methods in text classification together with a novel approach for capturing ironic content by exploiting a tailored lexicon for irony detection. We experimented with different ensemble settings. The obtained results show that our method has a good performance for detecting the presence of ironic content in Twitter.
S18-1105,{V}alen{TO} at {S}em{E}val-2018 Task 3: Exploring the Role of Affective Content for Detecting Irony in {E}nglish Tweets,2018,0,1,3,0,28840,delia farias,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"In this paper we describe the system used by the ValenTO team in the shared task on Irony Detection in English Tweets at SemEval 2018. The system takes as starting point emotIDM, an irony detection model that explores the use of affective features based on a wide range of lexical resources available for English, reflecting different facets of affect. We experimented with different settings, by exploiting different classifiers and features, and participated both to the binary irony detection task and to the task devoted to distinguish among different types of irony. We report on the results obtained by our system both in a constrained setting and unconstrained setting, where we explored the impact of using additional data in the training phase, such as corpora annotated for the presence of irony or sarcasm from the state of the art. Overall, the performance of our system seems to validate the important role that affective information has for identifying ironic content in Twitter."
L18-1615,{CATS}: A Tool for Customized Alignment of Text Simplification Corpora,2018,0,7,3,0.167449,24998,sanja vstajner,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-1205,Author Profiling at {PAN}: from Age and Gender Identification to Language Variety Identification (invited talk),2017,-1,-1,1,1,1722,paolo rosso,"Proceedings of the Fourth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial)",0,"Author profiling is the study of how language is shared by people, a problem of growing importance in applications dealing with security, in order to understand who could be behind an anonymous threat message, and marketing, where companies may be interested in knowing the demographics of people that in online reviews liked or disliked their products. In this talk we will give an overview of the PAN shared tasks that since 2013 have been organised at CLEF and FIRE evaluation forums, mainly on age and gender identification in social media, although also personality recognition in Twitter as well as in code sources was also addressed. In 2017 the PAN author profiling shared task addresses jointly gender and language variety identification in Twitter where tweets have been annotated with authors{'} gender and their specific variation of their native language: English (Australia, Canada, Great Britain, Ireland, New Zealand, United States), Spanish (Argentina, Chile, Colombia, Mexico, Peru, Spain, Venezuela), Portuguese (Brazil, Portugal), and Arabic (Egypt, Gulf, Levantine, Maghrebi)."
perez-estruch-etal-2017-learning,Learning Multimodal Gender Profile using Neural Networks,2017,7,0,3,0,32460,carlos estruch,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"Gender identification in social networks is one of the most popular aspects of user profile learning. Traditionally it has been linked to author profiling, a difficult problem to solve because of the little difference in the use of language between genders. This situation has led to the need of taking into account other information apart from textual data, favoring the emergence of multimodal data. The aim of this paper is to apply neural networks to perform data fusion, using an existing multimodal corpus, the NUS-MSS data set, that (not only) contains text data, but also image and location information. We improved previous results in terms of macro accuracy (87.8{\%}) obtaining the state-of-the-art performance of 91.3{\%}."
P17-2016,Sentence Alignment Methods for Improving Text Simplification Systems,2017,7,4,4,0.167449,24998,sanja vstajner,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We provide several methods for sentence-alignment of texts with different complexity levels. Using the best of them, we sentence-align the Newsela corpora, thus providing large training materials for automatic text simplification (ATS) systems. We show that using this dataset, even the standard phrase-based statistical machine translation models for ATS can outperform the state-of-the-art ATS systems."
E17-2089,Single and Cross-domain Polarity Classification using String Kernels,2017,0,4,3,0,33004,rosa gimenezperez,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"The polarity classification task aims at automatically identifying whether a subjective text is positive or negative. When the target domain is different from those where a model was trained, we refer to a cross-domain setting. That setting usually implies the use of a domain adaptation method. In this work, we study the single and cross-domain polarity classification tasks from the string kernels perspective. Contrary to classical domain adaptation methods, which employ texts from both domains to detect pivot features, we do not use the target domain for training. Our approach detects the lexical peculiarities that characterise the text polarity and maps them into a domain independent space by means of kernel discriminant analysis. Experimental results show state-of-the-art performance in single and cross-domain polarity classification."
E17-2106,Convolutional Neural Networks for Authorship Attribution of Short Texts,2017,0,30,5,0,11296,prasha shrestha,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,We present a model to perform authorship attribution of tweets using Convolutional Neural Networks (CNNs) over character n-grams. We also present a strategy that improves model interpretability by estimating the importance of input text fragments in the predicted classification. The experimental evaluation shows that text CNNs perform competitively and are able to outperform previous methods.
S16-1126,{UH}-{PRHLT} at {S}em{E}val-2016 Task 3: Combining Lexical and Semantic-based Features for Community Question Answering,2016,20,20,4,1,12613,marc francosalvador,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
L16-1683,Using a Small Lexicon with {CRF}s Confidence Measure to Improve {POS} Tagging Accuracy,2016,5,1,2,0,35394,mohamed outahajala,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Like most of the languages which have only recently started being investigated for the Natural Language Processing (NLP) tasks, Amazigh lacks annotated corpora and tools and still suffers from the scarcity of linguistic tools and resources. The main aim of this paper is to present a new part-of-speech (POS) tagger based on a new Amazigh tag set (AMTS) composed of 28 tags. In line with our goal we have trained Conditional Random Fields (CRFs) to build a POS tagger for the Amazigh language. We have used the 10-fold technique to evaluate and validate our approach. The CRFs 10 folds average level is 87.95{\%} and the best fold level result is 91.18{\%}. In order to improve this result, we have gathered a set of about 8k words with their POS tags. The collected lexicon was used with CRFs confidence measure in order to have a more accurate POS-tagger. Hence, we have obtained a better performance of 93.82{\%}."
2016.gwc-1.47,{A}rabic {W}ord{N}et: New Content and New Applications,2016,-1,-1,5,0,36145,yasser regragui,Proceedings of the 8th Global WordNet Conference (GWC),0,"The Arabic WordNet project has provided the Arabic Natural Language Processing (NLP) community with the first WordNet-compliant resource. It allowed new possibilities in terms of building sophisticated NLP applications related to this Semitic language. In this paper, we present the new content added to this resource, using semi-automatic techniques, and validated by Arabic native-speaker lexicographers. We also present how this content helps in the implementation of new Arabic NLP applications, especially for Question Answering (QA) systems. The obtained results show the contribution of the added content. The resource, fully transformed into the standard Lexical Markup Framework (LMF), is made available for the community."
W15-5403,Distributed Representations of Words and Documents for Discriminating Similar Languages,2015,-1,-1,2,1,12613,marc francosalvador,"Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialects",0,None
W15-5409,{NLEL} {UPV} Autoritas Participation at Discrimination between Similar Languages ({DSL}) 2015 Shared Task,2015,-1,-1,3,0,36474,raul fabraboluda,"Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialects",0,None
W15-2909,Classification of deceptive opinions using a low dimensionality representation,2015,24,7,2,0,36877,leticia cagnina,"Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Opinions in social media play such an important role for customers and companies that there is a growing tendency to post fake reviews in order to change purchase decisions and opinions. In this paper we propose the use of different features for a low dimension representation of opinions. We evaluate our proposal incorporating the features to a Support Vector Machines classifier and we use an available corpus with reviews of hotels in Chicago. We perform comparisons with previous works and we conclude that using our proposed features it is possible to obtain competitive results with a small amount of features for representing the data. Finally, we also investigate if the use of emotions can help to discriminate between truthful and deceptive opinions as previous works show to happen for deception detection in text in general."
S15-2080,{S}em{E}val-2015 Task 11: Sentiment Analysis of Figurative Language in {T}witter,2015,11,76,4,0,28830,aniruddha ghosh,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This report summarizes the objectives and evaluation of the SemEval 2015 task on the sentiment analysis of figurative language on Twitter (Task 11). This is the first sentiment analysis task wholly dedicated to analyzing figurative language on Twitter. Specifically, three broad classes of figurative language are considered: irony, sarcasm and metaphor. Gold standard sets of 8000 training tweets and 4000 test tweets were annotated using workers on the crowdsourcing platform CrowdFlower. Participating systems were required to provide a fine-grained sentiment score on an 11-point scale (-5 to 5, including 0 for neutral intent) for each tweet, and systems were evaluated against the gold standard using both a Cosinesimilarity and a Mean-Squared-Error measure."
W14-5901,{S}ocial{I}rony,2014,-1,-1,1,1,1722,paolo rosso,Proceedings of the Second Workshop on Natural Language Processing for Social Media ({S}ocial{NLP}),0,None
W14-3306,{E}nglish-to-{H}indi system description for {WMT} 2014: Deep Source-Context Features for {M}oses,2014,12,3,3,0,5326,marta costajussa,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper describes the IPN-UPV participation on the English-to-Hindi translation task from WMT 2014 International Evaluation Campaign. The system presented is based on Moses and enhanced with deep learning by means of a source-context feature function. This feature depends on the input sentence to translate, which makes it more challenging to adapt it into the Moses framework. This work reports the experimental details of the system putting special emphasis on: how the feature function is integrated in Moses and how the deep learning representations are trained and used."
dubey-etal-2014-enrichment,Enrichment of Bilingual Dictionary through News Stream Data,2014,14,1,4,0,39346,ajay dubey,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Bilingual dictionaries are the key component of the cross-lingual similarity estimation methods. Usually such dictionary generation is accomplished by manual or automatic means. Automatic generation approaches include to exploit parallel or comparable data to derive dictionary entries. Such approaches require large amount of bilingual data in order to produce good quality dictionary. Many time the language pair does not have large bilingual comparable corpora and in such cases the best automatic dictionary is upper bounded by the quality and coverage of such corpora. In this work we propose a method which exploits continuous quasi-comparable corpora to derive term level associations for enrichment of such limited dictionary. Though we propose our experiments for English and Hindi, our approach can be easily extendable to other languages. We evaluated dictionary by manually computing the precision. In experiments we show our approach is able to derive interesting term level associations across languages."
E14-1044,A Knowledge-based Representation for Cross-Language Document Retrieval and Categorization,2014,40,31,2,1,12613,marc francosalvador,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Current approaches to cross-language document retrieval and categorization are based on discriminative methods which represent documents in a low-dimensional vector space. In this paper we propose a shift from the supervised to the knowledge-based paradigm and provide a document similarity measure which draws on BabelNet, a large multilingual knowledge resource. Our experiments show state-of-the-art results in cross-lingual document retrieval and categorization."
D14-1153,Intrinsic Plagiarism Detection using N-gram Classes,2014,14,12,2,0,40143,imene bensalem,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"When it is not possible to compare the suspicious document to the source document(s) plagiarism has been committed from, the evidence of plagiarism has to be looked for intrinsically in the document itself. In this paper, we introduce a novel languageindependent intrinsic plagiarism detection method which is based on a new text representation that we called n-gram classes. The proposed method was evaluated on three publicly available standard corpora. The obtained results are comparable to the ones obtained by the best state-of-the-art methods."
C14-1116,Cross-Topic Authorship Attribution: Will Out-Of-Topic Data Help?,2014,25,12,5,0,34541,upendra sapkota,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Most previous research on authorship attribution (AA) assumes that the training and test data are drawn from same distribution. But in real scenarios, this assumption is too strong. The goal of this study is to improve the prediction results in cross-topic AA (CTAA), where the training data comes from one topic but the test data comes from another. Our proposed idea is to build a predictive model for one topic using documents from all other available topics. In addition to improving the performance of CTAA, we also make a thorough analysis of the sensitivity to changes in topic of four most commonly used feature types in AA. We empirically illustrate that our proposed framework is significantly better than the one trained on a single out-of-domain topic and is as effective, in some cases, as same-topic setting."
W13-1606,Using {PU}-Learning to Detect Deceptive Opinion Spam,2013,19,26,4,0,41062,donato fusilier,"Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Nowadays a large number of opinion reviews are posted on the Web. Such reviews are a very important source of information for customers and companies. The former rely more than ever on online reviews to make their purchase decisions and the latter to respond promptly to their clientsxe2x80x99 expectations. Due to the economic importance of these reviews there is a growing trend to incorporate spam on such sites, and, as a consequence, to develop methods for opinion spam detection. In this paper we focus on the detection of deceptive opinion spam, which consists of fictitious opinions that have been deliberately written to sound authentic, in order to deceive the consumers. In particular we propose a method based on the PU-learning approach which learns only from a few positive examples and a set of unlabeled data. Evaluation results in a corpus of hotel reviews demonstrate the appropriateness of the proposed method for real applications since it reached a f-measure of 0.84 in the detection of deceptive opinions using only 100 positive examples for training."
S13-1033,{INAOE}{\\_}{UPV}-{CORE}: Extracting Word Associations from Document Corpora to estimate Semantic Textual Similarity,2013,7,1,3,0,28841,fernando sanchezvega,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"This paper presents three methods to evaluate the Semantic Textual Similarity (STS). The first two methods do not require labeled training data; instead, they automatically extract semantic knowledge in the form of word associations from a given reference corpus. Two kinds of word associations are considered: cooccurrence statistics and the similarity of word contexts. The third method was done in collaboration with groups from the Universities of Paris 13, Matanzas and Alicante. It uses several word similarity measures as features in order to construct an accurate prediction model for the STS."
J13-4005,Plagiarism Meets Paraphrasing: Insights for the Next Generation in Automatic Plagiarism Detection,2013,49,75,4,1,15265,alberto barroncedeno,Computational Linguistics,0,"Although paraphrasing is the linguistic mechanism underlying many plagiarism cases, little attention has been paid to its analysis in the framework of automatic plagiarism detection. Therefore, state-of-the-art plagiarism detectors find it difficult to detect cases of paraphrase plagiarism. In this article, we analyze the relationship between paraphrasing and plagiarism, paying special attention to which paraphrase phenomena underlie acts of plagiarism and which of them are detected by plagiarism detection systems. With this aim in mind, we created the P4P corpus, a new resource that uses a paraphrase typology to annotate a subset of the PAN-PC-10 corpus for automatic plagiarism detection. The results of the Second International Competition on Plagiarism Detection were analyzed in the light of this annotation.n n The presented experiments show that i more complex paraphrase phenomena and a high density of paraphrase mechanisms make plagiarism detection more difficult, ii lexical substitutions are the paraphrase mechanisms used the most when plagiarizing, and iii paraphrase mechanisms tend to shorten the plagiarized text. For the first time, the paraphrase mechanisms behind plagiarism have been analyzed, providing critical insights for the improvement of automatic plagiarism detection systems."
W12-3717,On the Impact of Sentiment and Emotion Based Features in Detecting Online Sexual Predators,2012,22,18,2,0,33018,dasha bogdanova,Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis,0,"According to previous work on pedophile psychology and cyberpedophilia, sentiments and emotions in texts could be a good clue to detect online sexual predation. In this paper, we have suggested a list of high-level features, including sentiment and emotion based ones, for detection of online sexual predation. In particular, since pedophiles are known to be emotionally unstable, we were interested in investigating if emotion-based features could help in their detection. We have used a corpus of predators' chats with pseudo-victims downloaded from www.perverted-justice.com and two negative datasets of different nature: cybersex logs available online and the NPS chat corpus. Naive Bayes classification based on the proposed features achieves accuracies of up to 94% while baseline systems of word and character n-grams can only reach up to 72%."
W12-3208,Text Reuse with {ACL}: (Upward) Trends,2012,15,12,2,1,37277,parth gupta,Proceedings of the {ACL}-2012 Special Workshop on Rediscovering 50 Years of Discoveries,0,"With rapidly increasing community, a plethora of conferences related to Natural Language Processing and easy access to their proceedings make it essential to check the integrity and novelty of the new submissions. This study aims to investigate the trends of text reuse in the ACL submissions, if any. We carried a set of analyses on two spans of five years papers (the past and the present) of ACL using a publicly available text reuse detection application to notice the behaviour. In our study, we found some strong reuse cases which can be an indicator to establish a clear policy to handle text reuse for the upcoming editions of ACL. The results are anonymised."
W12-0413,Modelling Fixated Discourse in Chats with Cyberpedophiles,2012,-1,-1,2,0,33018,dasha bogdanova,Proceedings of the Workshop on Computational Approaches to Deception Detection,0,None
N12-3001,{D}e{S}o{C}o{R}e: Detecting Source Code Re-Use across Programming Languages,2012,8,12,3,0,42788,enrique flores,Proceedings of the Demonstration Session at the Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,Source code re-use has become an important problem in academia. The amount of code available makes necessary to develop systems supporting education that could address the problem of detection of source code re-use. We present the DeSoCoRe tool based on techniques of Natural Language Processing (NLP) applied to detect source code re-use. DeSoCoRe compares two source codes at the level of methods or functions even when written in different programming languages. The system provides an understandable output to the human reviewer in order to help a teacher to decide whether a source code is re-used.
roshchina-etal-2012-evaluating,Evaluating the Similarity Estimator component of the {TWIN} Personality-based Recommender System,2012,21,4,3,0,43364,alexandra roshchina,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"With the constant increase in the amount of information available in online communities, the task of building an appropriate Recommender System to support the user in her decision making process is becoming more and more challenging. In addition to the classical collaborative filtering and content based approaches, taking into account ratings, preferences and demographic characteristics of the users, a new type of Recommender System, based on personality parameters, has been emerging recently. In this paper we describe the TWIN (Tell Me What I Need) Personality Based Recommender System, and report on our experiments and experiences of utilizing techniques which allow the extraction of the personality type from text (following the Big Five model popular in the psychological research). We estimate the possibility of constructing the personality-based Recommender System that does not require users to fill in personality questionnaires. We are applying the proposed system in the online travelling domain to perform TripAdvisor hotels recommendation by analysing the text of user generated reviews, which are freely accessible from the community website."
C12-2043,Expected Divergence Based Feature Selection for Learning to Rank,2012,12,5,2,1,37277,parth gupta,Proceedings of {COLING} 2012: Posters,0,"Feature selection methods are essential for learning to rank (LTR) approaches as the number of features are directly proportional to computational cost and sometimes, might lead to the over-fitting of the ranking model. We propose an expected divergence based approach to select a subset of highly discriminating features over relevance categories. The proposed method is evaluated in terms of performance of standard LTR algorithms when trained with reduced features over a set of standard LTR datasets. The proposed method leads to not significantly worse, and in some cases, significantly better performance compared to the baselines with as few features as less than 10%. The proposed method is scalable and can easily be parallelised. TITLE AND ABSTRACT IN ANOTHER LANGUAGE, L2 (OPTIONAL, AND ON SAME PAGE)"
W11-3711,User Profile Construction in the {TWIN} Personality-based Recommender System,2011,14,10,3,0,43364,alexandra roshchina,Proceedings of the Workshop on Sentiment Analysis where {AI} meets Psychology ({SAAIP} 2011),0,"The information overload experienced by people who use online services and read usergenerated content (e.g. product reviews and ratings) to make their decisions has led to the development of the so-called recommender systems. We address the problem of the large increase in the user-generated reviews, which are added to each day and consequently make it difficult for the user to obtain a clear picture of the quality of the facility in which they are interested. In this paper, we describe the TWIN (xe2x80x9cTell me What I Needxe2x80x9d) personality-based recommender system, the aim of which is to select for the user reviews which have been written by like-minded individuals. We focus in particular on the task of User Profile construction. We apply the system in the travelling domain, to suggest hotels from the TripAdvisor 1 site by filtering out reviews produced by people with similar, or like-minded views, to those of the user. In order to establish the similarity between people we construct a user profile by modelling the userxe2x80x99s personality (according to the Big Five model) based on linguistic cues collected from the user-generated text."
W11-1715,Mining Subjective Knowledge from Customer Reviews: A Specific Case of Irony Detection,2011,25,27,2,0,37246,antonio reyes,Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis ({WASSA} 2.011),0,"The research described in this work focuses on identifying key components for the task of irony detection. By means of analyzing a set of customer reviews, which are considered as ironic both in social and mass media, we try to find hints about how to deal with this task from a computational point of view. Our objective is to gather a set of discriminating elements to represent irony. In particular, the kind of irony expressed in such reviews. To this end, we built a freely available data set with ironic reviews collected from Amazon. Such reviews were posted on the basis of an online viral effect; i.e. contents whose effect triggers a chain reaction on people. The findings were assessed employing three classifiers. The results show interesting hints regarding the patterns and, especially, regarding the implications for sentiment analysis."
W11-1719,On the Difficulty of Clustering Microblog Texts for Online Reputation Management,2011,14,16,4,0,44313,fernando pereztellez,Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis ({WASSA} 2.011),0,"In recent years microblogs have taken on an important role in the marketing sphere, in which they have been used for sharing opinions and/or experiences about a product or service. Companies and researchers have become interested in analysing the content generated over the most popular of these, the Twitter platform, to harvest information critical for their online reputation management (ORM). Critical to this task is the efficient and accurate identification of tweets which refer to a company distinguishing them from those which do not. The aim of this work is to present and compare two different approaches to achieve this. The obtained results are promising while at the same time highlighting the difficulty of this task."
P10-2052,{A}rabic Named Entity Recognition: Using Features Extracted from Noisy Data,2010,13,27,4,0.666667,7196,yassine benajiba,Proceedings of the {ACL} 2010 Conference Short Papers,0,"Building an accurate Named Entity Recognition (NER) system for languages with complex morphology is a challenging task. In this paper, we present research that explores the feature space using both gold and bootstrapped noisy features to build an improved highly accurate Arabic NER system. We bootstrap noisy features by projection from an Arabic-English parallel corpus that is automatically tagged with a baseline NER system. The feature space covers lexical, morphological, and syntactic features. The proposed approach yields an improvement of up to 1.64 F-measure (absolute)."
barron-cedeno-etal-2010-corpus,Corpus and Evaluation Measures for Automatic Plagiarism Detection,2010,20,21,3,1,15265,alberto barroncedeno,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The simple access to texts on digital libraries and the World Wide Web has led to an increased number of plagiarism cases in recent years, which renders manual plagiarism detection infeasible at large. Various methods for automatic plagiarism detection have been developed whose objective is to assist human experts in the analysis of documents for plagiarism. The methods can be divided into two main approaches: intrinsic and external. Unlike other tasks in natural language processing and information retrieval, it is not possible to publish a collection of real plagiarism cases for evaluation purposes since they cannot be properly anonymized. Therefore, current evaluations found in the literature are incomparable and, very often not even reproducible. Our contribution in this respect is a newly developed large-scale corpus of artificial plagiarism useful for the evaluation of intrinsic as well as external plagiarism detection. Additionally, new detection performance measures tailored to the evaluation of plagiarism detection algorithms are proposed."
sidorov-etal-2010-english,{E}nglish-{S}panish Large Statistical Dictionary of Inflectional Forms,2010,5,6,3,0,13871,grigori sidorov,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The paper presents an approach for constructing a weighted bilingual dictionary of inflectional forms using as input data a traditional bilingual dictionary, and not parallel corpora. An algorithm is developed that generates all possible morphological (inflectional) forms and weights them using information on distribution of corresponding grammar sets (grammar information) in large corpora for each language. The algorithm also takes into account the compatibility of grammar sets in a language pair; for example, verb in past tense in language L normally is expected to be translated by verb in past tense in Language L'. We consider that the developed method is universal, i.e. can be applied to any pair of languages. The obtained dictionary is freely available. It can be used in several NLP tasks, for example, statistical machine translation."
moreau-etal-2010-evaluation,Evaluation Protocol and Tools for Question-Answering on Speech Transcripts,2010,12,2,9,0,46066,nicolas moreau,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Question Answering (QA) technology aims at providing relevant answers to natural language questions. Most Question Answering research has focused on mining document collections containing written texts to answer written questions. In addition to written sources, a large (and growing) amount of potentially interesting information appears in spoken documents, such as broadcast news, speeches, seminars, meetings or telephone conversations. The QAST track (Question-Answering on Speech Transcripts) was introduced in CLEF to investigate the problem of question answering in such audio documents. This paper describes in detail the evaluation protocol and tools designed and developed for the CLEF-QAST evaluation campaigns that have taken place between 2007 and 2009. We first remind the data, question sets, and submission procedures that were produced or set up during these three campaigns. As for the evaluation procedure, the interface that was developed to ease the assessors work is described. In addition, this paper introduces a methodology for a semi-automatic evaluation of QAST systems based on time slot comparisons. Finally, the QAST Evaluation Package 2007-2009 resulting from these evaluation campaigns is also introduced."
panicheva-etal-2010-personal,Personal Sense and Idiolect: Combining Authorship Attribution and Opinion Analysis,2010,9,12,3,0,46133,polina panicheva,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Subjectivity analysis and authorship attribution are very popular areas of research. However, work in these two areas has been done separately. We believe that by combining information about subjectivity in texts and authorship, the performance of both tasks can be improved. In the paper a personalized approach to opinion mining is presented, in which the notions of personal sense and idiolect are introduced; the approach is applied to the polarity classification task. It is assumed that different authors express their private states in text individually, and opinion mining results could be improved by analyzing texts by different authors separately. The hypothesis is tested on a corpus of movie reviews by ten authors. The results of applying the personalized approach to opinion mining are presented, confirming that the approach increases the performance of the opinion mining task. Automatic authorship attribution is further applied to model the personalized approach, classifying documents by their assumed authorship. Although the automatic authorship classification imposes a number of limitations on the dataset for further experiments, after overcoming these issues the authorship attribution technique modeling the personalized approach confirms the increase over the baseline with no authorship information used."
reyes-etal-2010-evaluating,Evaluating Humour Features on Web Comments,2010,17,15,3,0,37246,antonio reyes,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Research on automatic humor recognition has developed several features which discriminate funny text from ordinary text. The features have been demonstrated to work well when classifying the funniness of single sentences up to entire blogs. In this paper we focus on evaluating a set of the best humor features reported in the literature over a corpus retrieved from the Slashdot Web site. The corpus is categorized in a community-driven process according to the following tags: funny, informative, insightful, offtopic, flamebait, interesting and troll. These kinds of comments can be found on almost every large Web site; therefore, they impose a new challenge to humor retrieval since they come along with unique characteristics compared to other text types. If funny comments were retrieved accurately, they would be of a great entertainment value for the visitors of a given Web page. Our objective, thus, is to distinguish between an implicit funny comment from a not funny one. Our experiments are preliminary but nonetheless large-scale: 600,000 Web comments. We evaluate the classification accuracy of naive Bayes classifiers, decision trees, and support vector machines. The results suggested interesting findings."
C10-2115,An Evaluation Framework for Plagiarism Detection,2010,20,223,4,0,6524,martin potthast,Coling 2010: Posters,0,"We present an evaluation framework for plagiarism detection. The framework provides performance measures that address the specifics of plagiarism detection, and the PAN-PC-10 corpus, which contains 64 558 artificial and 4 000 simulated plagiarism cases, the latter generated via Amazon's Mechanical Turk. We discuss the construction principles behind the measures and the corpus, and we compare the quality of our corpus to existing corpora. Our analysis gives empirical evidence that the construction of tailored training corpora for plagiarism detection can be automated, and hence be done on a large scale."
C10-1005,Plagiarism Detection across Distant Language Pairs,2010,27,45,2,1,15265,alberto barroncedeno,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Plagiarism, the unacknowledged reuse of text, does not end at language boundaries. Cross-language plagiarism occurs if a text is translated from a fragment written in a different language and no proper citation is provided. Regardless of the change of language, the contents and, in particular, the ideas remain the same. Whereas different methods for the detection of monolingual plagiarism have been developed, less attention has been paid to the cross-language case.n n In this paper we compare two recently proposed cross-language plagiarism detection methods (CL-CNG, based on character n-grams and CL-ASA, based on statistical translation), to a novel approach to this problem, based on machine translation and monolingual similarity analysis (TMA). We explore the effectiveness of the three approaches for less related languages. CL-CNG shows not be appropriate for this kind of language pairs, whereas TMA performs better than the previously proposed models."
W09-0808,Structure-Based Evaluation of an {A}rabic Semantic Query Expansion Using the {JIRS} Passage Retrieval System,2009,11,13,3,0,36146,lahsen abouenour,Proceedings of the {EACL} 2009 Workshop on Computational Approaches to {S}emitic Languages,0,"The adoption of semantic Query Expansion (QE) could be useful in the context of Question/Answering (Q/A) systems. For the Arabic language this is a challenging task since it has many particularities (short vowels, absence of capital letters, complex morphology, etc.). This paper presents an evaluation of a proposed semantic QE based on Arabic WordNet (AWN). Two types of experiments are conducted: the keyword-based evaluation which uses a classical search engine as passage retrieval system, and the structure-based evaluation that uses the Java Information Retrieval System (JIRS) which takes into account the structure of the question. Results show that the best performances in terms of accuracy and Mean Reciprocal Rank are reached when the proposed semantic QE together with JIRS are used."
buscaldi-rosso-2008-geo,Geo-{W}ord{N}et: Automatic Georeferencing of {W}ord{N}et,2008,12,21,2,0.333333,18793,davide buscaldi,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"WordNet has been used extensively as a resource for the Word Sense Disambiguation (WSD) task, both as a sense inventory and a repository of semantic relationships. Recently, we investigated the possibility to use it as a resource for the Geographical Information Retrieval task, more specifically for the toponym disambiguation task, which could be considered a specialization of WSD. We found that it would be very useful to assign to geographical entities inWordNet their coordinates, especially in order to implement geometric shapebased disambiguation methods. This paper presents Geo-WordNet, an automatic annotation of WordNet with geographical coordinates. The annotation has been carried out by extracting geographical synsets from WordNet, together with their holonyms and hypernyms, and comparing them to the entries in the Wikipedia-World geographical database. A weight was calculated for each of the candidate annotations, on the basis of matches found between the database entries and synset gloss, holonyms and hypernyms. The resulting resource may be used in Geographical Information Retrieval related tasks, especially for toponym disambiguation."
D08-1030,{A}rabic Named Entity Recognition using Optimized Feature Sets,2008,15,90,3,0.666667,7196,yassine benajiba,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"The Named Entity Recognition (NER) task has been garnering significant attention in NLP as it helps improve the performance of many natural language processing applications. In this paper, we investigate the impact of using different sets of features in two discriminative machine learning frameworks, namely, Support Vector Machines and Conditional Random Fields using Arabic data. We explore lexical, contextual and morphological features on eight standardized data-sets of different genres. We measure the impact of the different features in isolation, rank them according to their impact for each named entity class and incrementally combine them in order to infer the optimal machine learning approach and feature set. Our system yields a performance of Fxcexb2=1-measure=83.5 on ACE 2003 Broadcast News data."
S07-1096,{UPV}-{SI}: Word Sense Induction using Self Term Expansion,2007,23,15,2,0.882353,32083,david pinto,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"In this paper we are reporting the results obtained participating in the Evaluating Word Sense Induction and Discrimination Systems task of Semeval 2007. Our totally unsupervised system performed an automatic self-term expansion process by mean of co-ocurrence terms and, thereafter, it executed the unsupervised KStar clustering method. Two ranking tables with different evaluation measures were calculated by the task organizers, every table with two baselines and six runs submitted by different teams. We were ranked third place in both ranking tables obtaining a better performance than three different baselines, and outperforming the average score."
S07-1097,{UPV}-{WSD} : Combining different {WSD} Methods by means of Fuzzy Borda Voting,2007,9,18,2,0,18793,davide buscaldi,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"This paper describes the WSD system developed for our participation to the SemEval-1. It combines various methods by means of a fuzzy Borda voting. The fuzzy Borda vote-counting scheme is one of the best known methods in the field of collective decision making. In our system the different disambiguation methods are considered as experts that give a preference ranking for the senses a word can be assigned. Then the preferences are evaluated using the fuzzy Borda scheme in order to select the best sense. The methods we considered are the sense frequency probability calculated over SemCor, the Conceptual Density calculated over both hyperonyms and meronyms hyerarchies in WordNet, the extended Lesk by Banerjee and Pedersen, and finally a method based on WordNet domains."
buscaldi-rosso-2006-mining,Mining Knowledge from{W}ikipedia for the Question Answering task,2006,7,44,2,0,18793,davide buscaldi,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Although significant advances have been made recently in the Question Answering technology, more steps have to be undertaken in order to obtain better results. Moreover, the best systems at the CLEF and TREC evaluation exercises are very complex systems based on custom-built, expensive ontologies whose aim is to provide the systems with encyclopedic knowledge. In this paper we investigated the use of Wikipedia, the open domain encyclopedia, for the Question Answering task. Previous works considered Wikipedia as a resource where to look for the answers to the questions. We focused on some different aspects of the problem, such as the validation of the answers as returned by our Question Answering System and on the use of Wikipedia categories in order to determine a set of patterns that should fit with the expected answer. Validation consists in, given a possible answer, saying wether it is the right one or not. The possibility to exploit the categories ofWikipedia was not considered until now. We performed our experiments using the Spanish version of Wikipedia, with the set of questions of the last CLEF Spanish monolingual exercise. Results show that Wikipedia is a potentially useful resource for the Question Answering task."
W04-0820,The upv-unige-{CIAOSENSO} {WSD} system,2004,3,15,2,0,18793,davide buscaldi,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,None
