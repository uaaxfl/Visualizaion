2020.acl-main.212,D19-1418,0,0.0828238,"Missing"
2020.acl-main.212,K18-1007,0,0.0253979,"which is a contiguous subsequence of it) and the constituent heuristic (assume that a premise entails all of its constituents). While we focus on counteracting the lexical overlap heuristic, we will also test for generalization to the other heuristics, which can be seen as particularly challenging cases of lexical overlap. Examples of all constructions used to diagnose the three heuristics are given in Tables A.5, A.6 and A.7. Data augmentation is often employed to increase robustness in vision (Perez and Wang, 2017) and language (Belinkov and Bisk, 2018; Wei and Zou, 2019), including in NLI (Minervini and Riedel, 2018; Yanaka et al., 2019). In many cases, augmentation with one kind of example improves accuracy on that particular case, but does not generalize to other cases, suggesting that models overfit to the augmentation set (Jia and Liang, 2017; Ribeiro et al., 2018; Iyyer et al., 2018; Liu et al., 2019). In particular, McCoy et al. (2019b) found that augmentation with HANS examples generalized to a different word overlap challenge set (Dasgupta et al., 2018), but only for examples similar in length to HANS examples. We mitigate such overfitting to superficial properties by generating a diverse set of"
2020.acl-main.212,N18-1202,0,0.0183212,"ut affecting performance on the MNLI test set. This improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes BERT to recruit abstract syntactic representations. 1 Introduction In the supervised learning paradigm common in NLP, a large collection of labeled examples of a particular classification task is randomly split into a training set and a test set. The system is trained on this training set, and is then evaluated on the test set. Neural networks—in particular systems pretrained on a word prediction objective, such as ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019)—excel in this paradigm: with large enough pretraining corpora, these models match or even exceed the accuracy of untrained human annotators on many test sets (Raffel et al., 2019). At the same time, there is mounting evidence that high accuracy on a test set drawn from the (1) The lawyer saw the actor. (2) The actor saw the lawyer. McCoy et al. constructed the HANS challenge set, which includes examples of a range of such constructions, and used it to show that, when BERT is fine-tuned on the MNLI corpus (Williams et al., 2018), the fine-tuned model achieves high"
2020.acl-main.212,P18-1079,0,0.0477108,"be seen as particularly challenging cases of lexical overlap. Examples of all constructions used to diagnose the three heuristics are given in Tables A.5, A.6 and A.7. Data augmentation is often employed to increase robustness in vision (Perez and Wang, 2017) and language (Belinkov and Bisk, 2018; Wei and Zou, 2019), including in NLI (Minervini and Riedel, 2018; Yanaka et al., 2019). In many cases, augmentation with one kind of example improves accuracy on that particular case, but does not generalize to other cases, suggesting that models overfit to the augmentation set (Jia and Liang, 2017; Ribeiro et al., 2018; Iyyer et al., 2018; Liu et al., 2019). In particular, McCoy et al. (2019b) found that augmentation with HANS examples generalized to a different word overlap challenge set (Dasgupta et al., 2018), but only for examples similar in length to HANS examples. We mitigate such overfitting to superficial properties by generating a diverse set of corpus-based examples, which differ from the challenge set both lexically and syntactically. Finally, Kim et al. (2018) used a similar augmentation approach to ours but did not study generalization to types of examples not in the augmentation set. Backgroun"
2020.acl-main.212,P19-1485,0,0.048587,"Missing"
2020.acl-main.212,D19-1221,0,0.0187659,"ily Pitler2 Tal Linzen1 1 Department of Cognitive Science, Johns Hopkins University, Baltimore, MD 2 Google Research, New York, NY {jmin10, tom.mccoy, tal.linzen}@jhu.edu {dipanjand, epitler}@google.com Abstract same distribution as the training set does not indicate that the model has mastered the task. This discrepancy can manifest as a sharp drop in accuracy when the model is applied to a different dataset that illustrates the same task (Talmor and Berant, 2019; Yogatama et al., 2019), or as excessive sensitivity to linguistically irrelevant perturbations of the input (Jia and Liang, 2017; Wallace et al., 2019). One such discrepancy, where strong performance on a standard test set did not correspond to mastery of the task as a human would define it, was documented by McCoy et al. (2019b) for the Natural Language Inference (NLI) task. In this task, the system is given two sentences, and is expected to determine whether one (the premise) entails the other (the hypothesis). Most if not all humans would agree that NLI requires sensitivity to syntactic structure; for example, the following sentences do not entail each other, even though they contain the same words: Pretrained neural models such as BERT,"
2020.acl-main.212,D19-1670,0,0.0626803,"that a premise entails any hypothesis which is a contiguous subsequence of it) and the constituent heuristic (assume that a premise entails all of its constituents). While we focus on counteracting the lexical overlap heuristic, we will also test for generalization to the other heuristics, which can be seen as particularly challenging cases of lexical overlap. Examples of all constructions used to diagnose the three heuristics are given in Tables A.5, A.6 and A.7. Data augmentation is often employed to increase robustness in vision (Perez and Wang, 2017) and language (Belinkov and Bisk, 2018; Wei and Zou, 2019), including in NLI (Minervini and Riedel, 2018; Yanaka et al., 2019). In many cases, augmentation with one kind of example improves accuracy on that particular case, but does not generalize to other cases, suggesting that models overfit to the augmentation set (Jia and Liang, 2017; Ribeiro et al., 2018; Iyyer et al., 2018; Liu et al., 2019). In particular, McCoy et al. (2019b) found that augmentation with HANS examples generalized to a different word overlap challenge set (Dasgupta et al., 2018), but only for examples similar in length to HANS examples. We mitigate such overfitting to superfic"
2020.acl-main.212,N18-1101,0,0.0804875,"d prediction objective, such as ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019)—excel in this paradigm: with large enough pretraining corpora, these models match or even exceed the accuracy of untrained human annotators on many test sets (Raffel et al., 2019). At the same time, there is mounting evidence that high accuracy on a test set drawn from the (1) The lawyer saw the actor. (2) The actor saw the lawyer. McCoy et al. constructed the HANS challenge set, which includes examples of a range of such constructions, and used it to show that, when BERT is fine-tuned on the MNLI corpus (Williams et al., 2018), the fine-tuned model achieves high accuracy on the test set drawn from that corpus, yet displays little sensitivity to syntax; the model wrongly concluded, for example, that (1) entails (2). We consider two explanations as to why BERT fine-tuned on MNLI fails on HANS. Under the Representational Inadequacy Hypothesis, BERT fails on HANS because its pretrained representations are missing some necessary syntactic information. Under the Missed Connection Hypothesis, BERT extracts the relevant syntactic information from the input (cf. Goldberg 2019; 2339 Proceedings of the 58th Annual Meeting of"
2020.acl-main.212,S19-1027,0,\N,Missing
2020.acl-main.212,N19-1423,0,\N,Missing
2020.acl-main.704,P05-1074,0,0.0301198,"leverage this functionality by inserting masks at random positions in the Wikipedia sentences, and fill them with the language model. Thus, we introduce lexical alterations while maintaining the fluency of the sentence. We use two masking strategies—we either introduce the masks at random positions in the sentences, or we create contiguous sequences of masked tokens. More details are provided in the Appendix. Backtranslation: We generate paraphrases and perturbations with backtranslation, that is, round trips from English to another language and then back to English with a translation model (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013; Sennrich et al., 2016). Our primary aim is to create variants of the reference sentence that preserves semantics. Additionally, we use the mispredictions of the backtranslation models as a source of realistic alterations. Dropping words: We found it useful in our experiments to randomly drop words from the synthetic examples above to create other examples. This method prepares B LEURT for “pathological” behaviors or NLG systems, e.g., void predictions, or sentence truncation. 4.2 Pre-Training Signals The next step is to augment each sentence pair ˜ with a set of pr"
2020.acl-main.704,W17-4755,0,0.0734233,"/|τk |where |τk |is the dimension of τk and τˆk is computed by using a task-specific linear layer on top of the [CLS] embedding: τˆk = ℓpre-training M K 1 XX = γk ℓk (τkm , τˆkm ) M (1) m=1 k=1 where τkm is the target vector for example m, M is number of synthetic examples, and γk are hyperparameter weights obtained with grid search (more details in the Appendix). 5 Experiments In this section, we report our experimental results for two tasks, translation and data-to-text. First, we benchmark B LEURT against existing text generation metrics on the last 3 years of the WMT Metrics Shared Task (Bojar et al., 2017). We then evaluate its robustness to quality drifts with a series of synthetic datasets based on WMT17. We test B LEURT’s ability to adapt to different tasks with the WebNLG 2017 Challenge Dataset (Gardent et al., 2017). Finally, we measure the contribution of each pre-training task with ablation experiments. Our Models: Unless specified otherwise, all B LEURT models are trained in three steps: regular BERT pre-training (Devlin et al., 2019), pre-training on synthetic data (as explained in Section 4), and fine-tuning on task-specific ratings (translation and/or data-to-text). We experiment wit"
2020.acl-main.704,W16-2302,0,0.107175,"Missing"
2020.acl-main.704,D15-1075,0,0.0701549,"cope with a wide range of NLG domains and tasks. (2) The sentence pairs should contain a wide variety of lexical, syntactic, and semantic dissimilarities. The aim here is to anticipate all variations that an NLG system may produce, e.g., phrase substitution, paraphrases, noise, or omissions. (3) The pre-training objectives should effectively capture those phenomena, so that B LEURT can learn to identify them. The following sections present our approach. 4.1 Generating Sentence Pairs One way to expose B LEURT to a wide variety of sentence differences is to use existing sentence pairs datasets (Bowman et al., 2015; Williams et al., 2018; Wang et al., 2019). These sets are a rich source of related sentences, but they may fail to capture the errors and alterations that NLG systems produce (e.g., omissions, repetitions, nonsensical substitutions). We opted for an automatic approach instead, that can be scaled arbitrarily and at little cost: we generate synthetic sentence pairs ˜ by randomly perturbing 1.8 million seg(z, z) ments z from Wikipedia. We use three techniques: mask-filling with BERT, backtranslation, and randomly dropping out words. We obtain about 6.5 ˜ Let us describe those million perturbati"
2020.acl-main.704,P18-1060,0,0.0480328,"gical entailment. The first generation of metrics relied on handcrafted rules that measure the surface similarity between the sentences. To illustrate, BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), two popular metrics, rely on N-gram overlap. Because those metrics are only sensitive to lexical variation, they cannot appropriately reward semantic or syntactic variations of a given reference. Thus, they have been repeatedly shown to correlate poorly with human judgment, in particular when all the systems to compare have a similar level of accuracy (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018). Increasingly, NLG researchers have addressed those problems by injecting learned components in their metrics. To illustrate, consider the WMT Metrics Shared Task, an annual benchmark in which translation metrics are compared on their ability to imitate human assessments. The last two years of the competition were largely dominated by neural net-based approaches, RUSE, YiSi and ESIM (Ma et al., 2018, 2019). Current approaches largely fall into two categories. Fully learned metrics, such as BEER, RUSE, and ESIM are trained end-to-end, and they typically rely on handcrafted features and/or lear"
2020.acl-main.704,P17-1152,0,0.027203,"6 6 Related Work The WMT shared metrics competition (Bojar et al., 2016; Ma et al., 2018, 2019) has inspired 6 Do those results imply that BLEU and ROUGE should be removed from future versions of B LEURT? Doing so may indeed yield slight improvements on the WMT Metrics 2017 shared task. On the other hand the removal may hurt future tasks in which BLEU or ROUGE actually correlate with human assessments. We therefore leave the question open. the creation of many learned metrics, some of which use regression or deep learning (Stanojevic and Sima’an, 2014; Ma et al., 2017; Shimanaka et al., 2018; Chen et al., 2017; Mathur et al., 2019). Other metrics have been introduced, such as the recent MoverScore (Zhao et al., 2019) which combines contextual embeddings and Earth Mover’s Distance. We provide a head-to-head comparison with the best performing of those in our experiments. Other approaches do not attempt to estimate quality directly, but use information extraction or question answering as a proxy (Wiseman et al., 2017; Goodrich et al., 2019; Eyal et al., 2019). Those are complementary to our work. There has been recent work that uses BERT for evaluation. BERTScore (Zhang et al., 2020) proposes replaci"
2020.acl-main.704,N13-1092,0,0.0778161,"Missing"
2020.acl-main.704,W17-3518,0,0.47854,"s of Wikipedia sentences augmented with a diverse set of lexical and semantic-level supervision signals. To demonstrate our approach, we train B LEURT for English and evaluate it under different generalization regimes. We first verify that it provides state-of-the-art results on all recent years of the WMT Metrics Shared task (2017 to 2019, to-English language pairs). We then stress-test its ability to cope with quality drifts with a synthetic benchmark based on WMT 2017. Finally, we show that it can easily adapt to a different domain with three tasks from a data-to-text dataset, WebNLG 2017 (Gardent et al., 2017). Ablations show that our synthetic pretraining scheme increases performance in the IID setting, and is critical to ensure robustness when the training data is scarce, skewed, or out-of-domain. The code and pre-trained models are available online2 . 1 Bilingual Evaluation Understudy with Representations from Transformers. We refer the intrigued reader to Papineni et al. 2002 for a justification of the term understudy. 2 http://github.com/google-research/ bleurt 2 Preliminaries Define x = (x1 , .., xr ) to be the reference sentence of length r where each xi is a token and let ˜ = (˜ x x1 , ..,"
2020.acl-main.704,N18-1170,0,0.0296426,"D experimental setups, but also robust in the presence of scarce and out-of-distribution training data. To our knowledge no existing work has explored pretraining and extrapolation in the context of NLG. Previous studies have used noising for referenceless evaluation (Duˇsek et al., 2019). Noisy pre-training has also been proposed before for other tasks such as paraphrasing (Wieting et al., 2016; Tomar et al., 2017) but generally not with synthetic data. Generating synthetic data via paraphrases and perturbations has been commonly used for generating adversarial examples (Jia and Liang, 2017; Iyyer et al., 2018; Belinkov and Bisk, 2018; Ribeiro et al., 2018), an orthogonal line of research. 7 Conclusion We presented B LEURT, a reference-based text generation metric for English. Because the metric is trained end-to-end, B LEURT can model human assessment with superior accuracy. Furthermore, pre-training makes the metrics robust particularly robust to both domain and quality drifts. Future research directions include multilingual NLG evaluation, and hybrid methods involving both humans and classifiers. 7888 Acknowledgments Thanks to Eunsol Choi, Nicholas FitzGerald, Jacob Devlin, and to the members of"
2020.acl-main.704,D17-1215,0,0.0428011,"rt in conventional IID experimental setups, but also robust in the presence of scarce and out-of-distribution training data. To our knowledge no existing work has explored pretraining and extrapolation in the context of NLG. Previous studies have used noising for referenceless evaluation (Duˇsek et al., 2019). Noisy pre-training has also been proposed before for other tasks such as paraphrasing (Wieting et al., 2016; Tomar et al., 2017) but generally not with synthetic data. Generating synthetic data via paraphrases and perturbations has been commonly used for generating adversarial examples (Jia and Liang, 2017; Iyyer et al., 2018; Belinkov and Bisk, 2018; Ribeiro et al., 2018), an orthogonal line of research. 7 Conclusion We presented B LEURT, a reference-based text generation metric for English. Because the metric is trained end-to-end, B LEURT can model human assessment with superior accuracy. Furthermore, pre-training makes the metrics robust particularly robust to both domain and quality drifts. Future research directions include multilingual NLG evaluation, and hybrid methods involving both humans and classifiers. 7888 Acknowledgments Thanks to Eunsol Choi, Nicholas FitzGerald, Jacob Devlin, a"
2020.acl-main.704,P09-5002,0,0.0150042,"ions of synthetic examples to help the model generalize. B LEURT provides state-ofthe-art results on the last three years of the WMT Metrics shared task and the WebNLG Competition dataset. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution. 1 Introduction In the last few years, research in natural text generation (NLG) has made significant progress, driven largely by the neural encoder-decoder paradigm (Sutskever et al., 2014; Bahdanau et al., 2015) which can tackle a wide array of tasks including translation (Koehn, 2009), summarization (Mani, 1999; Chopra et al., 2016), structureddata-to-text generation (McKeown, 1992; Kukich, 1983; Wiseman et al., 2017) dialog (Smith and Hipp, 1994; Vinyals and Le, 2015) and image captioning (Fang et al., 2015). However, progress is increasingly impeded by the shortcomings of existing metrics (Wiseman et al., 2017; Ma et al., 2019; Tian et al., 2019). Human evaluation is often the best indicator of the quality of a system. However, designing crowd sourcing experiments is an expensive and high-latency process, which does not easily fit in a daily model development pipeline. T"
2020.acl-main.704,P83-1022,0,0.402708,"ee years of the WMT Metrics shared task and the WebNLG Competition dataset. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution. 1 Introduction In the last few years, research in natural text generation (NLG) has made significant progress, driven largely by the neural encoder-decoder paradigm (Sutskever et al., 2014; Bahdanau et al., 2015) which can tackle a wide array of tasks including translation (Koehn, 2009), summarization (Mani, 1999; Chopra et al., 2016), structureddata-to-text generation (McKeown, 1992; Kukich, 1983; Wiseman et al., 2017) dialog (Smith and Hipp, 1994; Vinyals and Le, 2015) and image captioning (Fang et al., 2015). However, progress is increasingly impeded by the shortcomings of existing metrics (Wiseman et al., 2017; Ma et al., 2019; Tian et al., 2019). Human evaluation is often the best indicator of the quality of a system. However, designing crowd sourcing experiments is an expensive and high-latency process, which does not easily fit in a daily model development pipeline. Therefore, NLG researchers commonly use automatic evaluation metrics, which provide an acceptable proxy for qualit"
2020.acl-main.704,W04-1013,0,0.558224,"in a daily model development pipeline. Therefore, NLG researchers commonly use automatic evaluation metrics, which provide an acceptable proxy for quality and are very cheap to compute. This paper investigates sentence-level, referencebased metrics, which describe the extent to which a candidate sentence is similar to a reference one. The exact definition of similarity may range from string overlap to logical entailment. The first generation of metrics relied on handcrafted rules that measure the surface similarity between the sentences. To illustrate, BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), two popular metrics, rely on N-gram overlap. Because those metrics are only sensitive to lexical variation, they cannot appropriately reward semantic or syntactic variations of a given reference. Thus, they have been repeatedly shown to correlate poorly with human judgment, in particular when all the systems to compare have a similar level of accuracy (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018). Increasingly, NLG researchers have addressed those problems by injecting learned components in their metrics. To illustrate, consider the WMT Metrics Shared Task, an annual bench"
2020.acl-main.704,D16-1230,0,0.060798,"arity may range from string overlap to logical entailment. The first generation of metrics relied on handcrafted rules that measure the surface similarity between the sentences. To illustrate, BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), two popular metrics, rely on N-gram overlap. Because those metrics are only sensitive to lexical variation, they cannot appropriately reward semantic or syntactic variations of a given reference. Thus, they have been repeatedly shown to correlate poorly with human judgment, in particular when all the systems to compare have a similar level of accuracy (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018). Increasingly, NLG researchers have addressed those problems by injecting learned components in their metrics. To illustrate, consider the WMT Metrics Shared Task, an annual benchmark in which translation metrics are compared on their ability to imitate human assessments. The last two years of the competition were largely dominated by neural net-based approaches, RUSE, YiSi and ESIM (Ma et al., 2018, 2019). Current approaches largely fall into two categories. Fully learned metrics, such as BEER, RUSE, and ESIM are trained end-to-end, and they typ"
2020.acl-main.704,N16-1012,0,0.0383792,"del generalize. B LEURT provides state-ofthe-art results on the last three years of the WMT Metrics shared task and the WebNLG Competition dataset. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution. 1 Introduction In the last few years, research in natural text generation (NLG) has made significant progress, driven largely by the neural encoder-decoder paradigm (Sutskever et al., 2014; Bahdanau et al., 2015) which can tackle a wide array of tasks including translation (Koehn, 2009), summarization (Mani, 1999; Chopra et al., 2016), structureddata-to-text generation (McKeown, 1992; Kukich, 1983; Wiseman et al., 2017) dialog (Smith and Hipp, 1994; Vinyals and Le, 2015) and image captioning (Fang et al., 2015). However, progress is increasingly impeded by the shortcomings of existing metrics (Wiseman et al., 2017; Ma et al., 2019; Tian et al., 2019). Human evaluation is often the best indicator of the quality of a system. However, designing crowd sourcing experiments is an expensive and high-latency process, which does not easily fit in a daily model development pipeline. Therefore, NLG researchers commonly use automatic"
2020.acl-main.704,2021.ccl-1.108,0,0.216141,"Missing"
2020.acl-main.704,N19-1423,0,0.641226,", and therefore a model trained on ratings data from 2015 may fail to distinguish top performing systems in 2019, especially for newer research tasks. An ideal learned metric would be able to both take full advantage of available ratings data for training, and be robust to distribution drifts, i.e., it should be able to extrapolate. Our insight is that it is possible to combine expressivity and robustness by pre-training a fully learned metric on large amounts of synthetic data, before fine-tuning it on human ratings. To this end, we introduce B LEURT,1 a text generation metric based on BERT (Devlin et al., 2019). A key ingredient of B LEURT is a novel pre-training scheme, which uses random perturbations of Wikipedia sentences augmented with a diverse set of lexical and semantic-level supervision signals. To demonstrate our approach, we train B LEURT for English and evaluate it under different generalization regimes. We first verify that it provides state-of-the-art results on all recent years of the WMT Metrics Shared task (2017 to 2019, to-English language pairs). We then stress-test its ability to cope with quality drifts with a synthetic benchmark based on WMT 2017. Finally, we show that it can ea"
2020.acl-main.704,W18-6450,0,0.340665,"have been repeatedly shown to correlate poorly with human judgment, in particular when all the systems to compare have a similar level of accuracy (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018). Increasingly, NLG researchers have addressed those problems by injecting learned components in their metrics. To illustrate, consider the WMT Metrics Shared Task, an annual benchmark in which translation metrics are compared on their ability to imitate human assessments. The last two years of the competition were largely dominated by neural net-based approaches, RUSE, YiSi and ESIM (Ma et al., 2018, 2019). Current approaches largely fall into two categories. Fully learned metrics, such as BEER, RUSE, and ESIM are trained end-to-end, and they typically rely on handcrafted features and/or learned embeddings. Conversely, hybrid metrics, such as YiSi and BERTscore combine trained elements, e.g., contextual embeddings, with handwritten logic, e.g., as token alignment rules. The first category typically offers great expressivity: if a training set of human ratings data is available, the metrics may take full advantage of it and fit the ratings distribution tightly. Fur7881 Proceedings of the"
2020.acl-main.704,W19-8644,0,0.0396072,"Missing"
2020.acl-main.704,W17-4768,0,0.0740946,"uman judgment may in fact harm the model.6 6 Related Work The WMT shared metrics competition (Bojar et al., 2016; Ma et al., 2018, 2019) has inspired 6 Do those results imply that BLEU and ROUGE should be removed from future versions of B LEURT? Doing so may indeed yield slight improvements on the WMT Metrics 2017 shared task. On the other hand the removal may hurt future tasks in which BLEU or ROUGE actually correlate with human assessments. We therefore leave the question open. the creation of many learned metrics, some of which use regression or deep learning (Stanojevic and Sima’an, 2014; Ma et al., 2017; Shimanaka et al., 2018; Chen et al., 2017; Mathur et al., 2019). Other metrics have been introduced, such as the recent MoverScore (Zhao et al., 2019) which combines contextual embeddings and Earth Mover’s Distance. We provide a head-to-head comparison with the best performing of those in our experiments. Other approaches do not attempt to estimate quality directly, but use information extraction or question answering as a proxy (Wiseman et al., 2017; Goodrich et al., 2019; Eyal et al., 2019). Those are complementary to our work. There has been recent work that uses BERT for evaluation. BERT"
2020.acl-main.704,N19-1395,0,0.0368019,"n of many learned metrics, some of which use regression or deep learning (Stanojevic and Sima’an, 2014; Ma et al., 2017; Shimanaka et al., 2018; Chen et al., 2017; Mathur et al., 2019). Other metrics have been introduced, such as the recent MoverScore (Zhao et al., 2019) which combines contextual embeddings and Earth Mover’s Distance. We provide a head-to-head comparison with the best performing of those in our experiments. Other approaches do not attempt to estimate quality directly, but use information extraction or question answering as a proxy (Wiseman et al., 2017; Goodrich et al., 2019; Eyal et al., 2019). Those are complementary to our work. There has been recent work that uses BERT for evaluation. BERTScore (Zhang et al., 2020) proposes replacing the hard n-gram overlap of BLEU with a soft-overlap using BERT embeddings. We use it in all our experiments. Bertr (Mathur et al., 2019) and YiSi (Mathur et al., 2019) also make use of BERT embeddings to capture similarity. SumQE (Xenouleas et al., 2019) fine-tunes BERT for quality estimation as we describe in Section 3. Our focus is different—we train metrics that are not only state-of-the-art in conventional IID experimental setups, but also robus"
2020.acl-main.704,W19-5302,0,0.313158,"ast few years, research in natural text generation (NLG) has made significant progress, driven largely by the neural encoder-decoder paradigm (Sutskever et al., 2014; Bahdanau et al., 2015) which can tackle a wide array of tasks including translation (Koehn, 2009), summarization (Mani, 1999; Chopra et al., 2016), structureddata-to-text generation (McKeown, 1992; Kukich, 1983; Wiseman et al., 2017) dialog (Smith and Hipp, 1994; Vinyals and Le, 2015) and image captioning (Fang et al., 2015). However, progress is increasingly impeded by the shortcomings of existing metrics (Wiseman et al., 2017; Ma et al., 2019; Tian et al., 2019). Human evaluation is often the best indicator of the quality of a system. However, designing crowd sourcing experiments is an expensive and high-latency process, which does not easily fit in a daily model development pipeline. Therefore, NLG researchers commonly use automatic evaluation metrics, which provide an acceptable proxy for quality and are very cheap to compute. This paper investigates sentence-level, referencebased metrics, which describe the extent to which a candidate sentence is similar to a reference one. The exact definition of similarity may range from stri"
2020.acl-main.704,P19-1269,0,0.277576,"and BLEURTbase -pre. The first two models are based on BERT-large and BERT-base. In the latter two versions, we skip the pre-training phase and fine-tune directly on the WMT ratings. For each year of the WMT shared task, we use the test set from the previous years for training and validation. We describe our setup in further detail in the Appendix. We compare B LEURT to participant data from the shared task and automatic metrics that we ran ourselves. In the former case, we use the the best-performing contestants for each year, that is, chrF++, BEER, Meteor++, RUSE, Yisi1, ESIM and Yisi1-SRL (Mathur et al., 2019). All the contestants use the same WMT training data, in addition to existing sentence or token embeddings. In the latter case, we use Moses sentenceBLEU, BERTscore (Zhang et al., 2020), and MoverScore (Zhao et al., 2019). For BERTscore, we use BERT-large uncased for fairness, and roBERTa (the recommended version) for completeness (Liu et al., 2019). We run MoverScore on WMT 2017 using the scripts published by the authors. 4 The official scripts are public but they suffer from documentation and dependency issues, as shown by a README file in the 2019 edition which explicitly discourages using"
2020.acl-main.704,D17-1238,0,0.20142,"Missing"
2020.acl-main.704,P02-1040,0,0.121478,"process, which does not easily fit in a daily model development pipeline. Therefore, NLG researchers commonly use automatic evaluation metrics, which provide an acceptable proxy for quality and are very cheap to compute. This paper investigates sentence-level, referencebased metrics, which describe the extent to which a candidate sentence is similar to a reference one. The exact definition of similarity may range from string overlap to logical entailment. The first generation of metrics relied on handcrafted rules that measure the surface similarity between the sentences. To illustrate, BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), two popular metrics, rely on N-gram overlap. Because those metrics are only sensitive to lexical variation, they cannot appropriately reward semantic or syntactic variations of a given reference. Thus, they have been repeatedly shown to correlate poorly with human judgment, in particular when all the systems to compare have a similar level of accuracy (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018). Increasingly, NLG researchers have addressed those problems by injecting learned components in their metrics. To illustrate, consider the WMT Metrics Shared"
2020.acl-main.704,P18-1079,0,0.0222115,"presence of scarce and out-of-distribution training data. To our knowledge no existing work has explored pretraining and extrapolation in the context of NLG. Previous studies have used noising for referenceless evaluation (Duˇsek et al., 2019). Noisy pre-training has also been proposed before for other tasks such as paraphrasing (Wieting et al., 2016; Tomar et al., 2017) but generally not with synthetic data. Generating synthetic data via paraphrases and perturbations has been commonly used for generating adversarial examples (Jia and Liang, 2017; Iyyer et al., 2018; Belinkov and Bisk, 2018; Ribeiro et al., 2018), an orthogonal line of research. 7 Conclusion We presented B LEURT, a reference-based text generation metric for English. Because the metric is trained end-to-end, B LEURT can model human assessment with superior accuracy. Furthermore, pre-training makes the metrics robust particularly robust to both domain and quality drifts. Future research directions include multilingual NLG evaluation, and hybrid methods involving both humans and classifiers. 7888 Acknowledgments Thanks to Eunsol Choi, Nicholas FitzGerald, Jacob Devlin, and to the members of the Google AI Language team for the proof-readi"
2020.acl-main.704,P16-1009,0,0.0565251,"itions in the Wikipedia sentences, and fill them with the language model. Thus, we introduce lexical alterations while maintaining the fluency of the sentence. We use two masking strategies—we either introduce the masks at random positions in the sentences, or we create contiguous sequences of masked tokens. More details are provided in the Appendix. Backtranslation: We generate paraphrases and perturbations with backtranslation, that is, round trips from English to another language and then back to English with a translation model (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013; Sennrich et al., 2016). Our primary aim is to create variants of the reference sentence that preserves semantics. Additionally, we use the mispredictions of the backtranslation models as a source of realistic alterations. Dropping words: We found it useful in our experiments to randomly drop words from the synthetic examples above to create other examples. This method prepares B LEURT for “pathological” behaviors or NLG systems, e.g., void predictions, or sentence truncation. 4.2 Pre-Training Signals The next step is to augment each sentence pair ˜ with a set of pre-training signals {τk }, (z, z) where τk is the ta"
2020.acl-main.704,W18-6456,0,0.123693,"in fact harm the model.6 6 Related Work The WMT shared metrics competition (Bojar et al., 2016; Ma et al., 2018, 2019) has inspired 6 Do those results imply that BLEU and ROUGE should be removed from future versions of B LEURT? Doing so may indeed yield slight improvements on the WMT Metrics 2017 shared task. On the other hand the removal may hurt future tasks in which BLEU or ROUGE actually correlate with human assessments. We therefore leave the question open. the creation of many learned metrics, some of which use regression or deep learning (Stanojevic and Sima’an, 2014; Ma et al., 2017; Shimanaka et al., 2018; Chen et al., 2017; Mathur et al., 2019). Other metrics have been introduced, such as the recent MoverScore (Zhao et al., 2019) which combines contextual embeddings and Earth Mover’s Distance. We provide a head-to-head comparison with the best performing of those in our experiments. Other approaches do not attempt to estimate quality directly, but use information extraction or question answering as a proxy (Wiseman et al., 2017; Goodrich et al., 2019; Eyal et al., 2019). Those are complementary to our work. There has been recent work that uses BERT for evaluation. BERTScore (Zhang et al., 202"
2020.acl-main.704,N18-1101,0,0.258484,"e of NLG domains and tasks. (2) The sentence pairs should contain a wide variety of lexical, syntactic, and semantic dissimilarities. The aim here is to anticipate all variations that an NLG system may produce, e.g., phrase substitution, paraphrases, noise, or omissions. (3) The pre-training objectives should effectively capture those phenomena, so that B LEURT can learn to identify them. The following sections present our approach. 4.1 Generating Sentence Pairs One way to expose B LEURT to a wide variety of sentence differences is to use existing sentence pairs datasets (Bowman et al., 2015; Williams et al., 2018; Wang et al., 2019). These sets are a rich source of related sentences, but they may fail to capture the errors and alterations that NLG systems produce (e.g., omissions, repetitions, nonsensical substitutions). We opted for an automatic approach instead, that can be scaled arbitrarily and at little cost: we generate synthetic sentence pairs ˜ by randomly perturbing 1.8 million seg(z, z) ments z from Wikipedia. We use three techniques: mask-filling with BERT, backtranslation, and randomly dropping out words. We obtain about 6.5 ˜ Let us describe those million perturbations z. techniques. Mask"
2020.acl-main.704,D17-1239,0,0.216115,"e WMT Metrics shared task and the WebNLG Competition dataset. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution. 1 Introduction In the last few years, research in natural text generation (NLG) has made significant progress, driven largely by the neural encoder-decoder paradigm (Sutskever et al., 2014; Bahdanau et al., 2015) which can tackle a wide array of tasks including translation (Koehn, 2009), summarization (Mani, 1999; Chopra et al., 2016), structureddata-to-text generation (McKeown, 1992; Kukich, 1983; Wiseman et al., 2017) dialog (Smith and Hipp, 1994; Vinyals and Le, 2015) and image captioning (Fang et al., 2015). However, progress is increasingly impeded by the shortcomings of existing metrics (Wiseman et al., 2017; Ma et al., 2019; Tian et al., 2019). Human evaluation is often the best indicator of the quality of a system. However, designing crowd sourcing experiments is an expensive and high-latency process, which does not easily fit in a daily model development pipeline. Therefore, NLG researchers commonly use automatic evaluation metrics, which provide an acceptable proxy for quality and are very cheap to"
2020.acl-main.704,D19-1053,0,0.128174,"use the test set from the previous years for training and validation. We describe our setup in further detail in the Appendix. We compare B LEURT to participant data from the shared task and automatic metrics that we ran ourselves. In the former case, we use the the best-performing contestants for each year, that is, chrF++, BEER, Meteor++, RUSE, Yisi1, ESIM and Yisi1-SRL (Mathur et al., 2019). All the contestants use the same WMT training data, in addition to existing sentence or token embeddings. In the latter case, we use Moses sentenceBLEU, BERTscore (Zhang et al., 2020), and MoverScore (Zhao et al., 2019). For BERTscore, we use BERT-large uncased for fairness, and roBERTa (the recommended version) for completeness (Liu et al., 2019). We run MoverScore on WMT 2017 using the scripts published by the authors. 4 The official scripts are public but they suffer from documentation and dependency issues, as shown by a README file in the 2019 edition which explicitly discourages using them. Results: Tables 2, 3, 4 show the results. For years 2017 and 2018, a B LEURT-based metric 7885 model sentBLEU BERTscore w/ BERT BERTscore w/ roBERTa ESIM YiSi1 SRL 19 BLEURTbase -pre BLEURTbase BLEURT -pre BLEURT de"
2020.acl-main.704,W17-4121,1,0.902952,"Missing"
2020.acl-main.704,W14-3354,0,\N,Missing
2020.emnlp-main.89,W13-2111,0,0.540524,") lack explicit signals for models on what to generate, which can lead to subjective content and evaluation challenges (Kry´sci´nski et al., 2019). On the other hand, data-to-text tasks that are limited to verbalizing a fully specified meaning representation (Gardent et al., 2017b) do not test a model’s ability to perform inference and thus remove a considerable amount of challenge from the task. Secondly, designing an annotation process to obtain natural but also clean targets is a significant challenge. One strategy employed by many datasets is to have annotators write targets from scratch (Banik et al., 2013; Wen et al., 2015; Gardent et al., 2017a) which can often lack variety in terms of structure and style (Gururangan et al., 2018; Poliak et al., 2018). An alternative is to pair naturally occurring text with tables (Lebret et al., 2016; Wiseman et al., 2017). While more diverse, naturally occurring targets are often noisy and contain information that cannot be inferred from the source. This can make it problematic to disentangle modeling weaknesses from data noise. In this work, we propose T OTT O, an opendomain table-to-text generation dataset that intro1173 Proceedings of the 2020 Conference"
2020.emnlp-main.89,2020.acl-main.708,0,0.215626,"nables T OTT O to maintain the varied language and structure found in natural sentences while producing cleaner targets. The technique of editing exemplar sentences has been used in semiparametric generation models (Guu et al., 2018; Pandey et al., 2018; Peng et al., 2019) and crowd-sourcing small, iterative changes to text has been shown to lead to higher-quality data and a more robust annotation process (Little et al., 2010). Perez-Beltrachini and Lapata (2018) also employed a revision strategy to construct a cleaner evaluation set for Wikibio (Lebret et al., 2016). Concurrent to this work, Chen et al. (2020) proposed LogicNLG which also uses Wikipedia tables, although omitting some of the more complex structured ones included in our dataset. Their target sentences are annotator-generated and their task is significantly more uncontrolled due to the lack of annotator highlighted cells. Annotation Process There are various existing strategies to create the reference target y. One strategy employed by many datasets is to have annotators write targets from scratch given a representation of the source (Banik et al., 2013; Wen et al., 2015; Gardent et al., 2017a). While this will result in a target that"
2020.emnlp-main.89,P19-1483,1,0.920838,"Existing data-to-text tasks have provided an important test-bed for neural generation models (Sutskever et al., 2014; Bahdanau et al., 2014). Neural models are known to be prone to hallucination, i.e., generating text that is fluent but not faithful to the source (Vinyals and Le, 2015; Koehn ∗ Work done during an internship at Google. T OTT O is available at https://github.com/ google-research-datasets/totto. 1 and Knowles, 2017; Lee et al., 2018; Tian et al., 2019) and it is often easier to assess faithfulness of the generated text when the source content is structured (Wiseman et al., 2017; Dhingra et al., 2019). Moreover, structured data can also test a model’s ability for reasoning and numerical inference (Wiseman et al., 2017) and for building representations of structured objects (Liu et al., 2018), providing an interesting complement to tasks that test these aspects in the NLU setting (Pasupat and Liang, 2015; Chen et al., 2019; Dua et al., 2019). However, constructing a data-to-text dataset can be challenging on two axes: task design and annotation process. First, tasks with open-ended output like summarization (Mani, 1999; Lebret et al., 2016; Wiseman et al., 2017) lack explicit signals for mo"
2020.emnlp-main.89,N19-1246,0,0.0250175,"ailable at https://github.com/ google-research-datasets/totto. 1 and Knowles, 2017; Lee et al., 2018; Tian et al., 2019) and it is often easier to assess faithfulness of the generated text when the source content is structured (Wiseman et al., 2017; Dhingra et al., 2019). Moreover, structured data can also test a model’s ability for reasoning and numerical inference (Wiseman et al., 2017) and for building representations of structured objects (Liu et al., 2018), providing an interesting complement to tasks that test these aspects in the NLU setting (Pasupat and Liang, 2015; Chen et al., 2019; Dua et al., 2019). However, constructing a data-to-text dataset can be challenging on two axes: task design and annotation process. First, tasks with open-ended output like summarization (Mani, 1999; Lebret et al., 2016; Wiseman et al., 2017) lack explicit signals for models on what to generate, which can lead to subjective content and evaluation challenges (Kry´sci´nski et al., 2019). On the other hand, data-to-text tasks that are limited to verbalizing a fully specified meaning representation (Gardent et al., 2017b) do not test a model’s ability to perform inference and thus remove a considerable amount of c"
2020.emnlp-main.89,P17-1017,0,0.0422605,"t to tasks that test these aspects in the NLU setting (Pasupat and Liang, 2015; Chen et al., 2019; Dua et al., 2019). However, constructing a data-to-text dataset can be challenging on two axes: task design and annotation process. First, tasks with open-ended output like summarization (Mani, 1999; Lebret et al., 2016; Wiseman et al., 2017) lack explicit signals for models on what to generate, which can lead to subjective content and evaluation challenges (Kry´sci´nski et al., 2019). On the other hand, data-to-text tasks that are limited to verbalizing a fully specified meaning representation (Gardent et al., 2017b) do not test a model’s ability to perform inference and thus remove a considerable amount of challenge from the task. Secondly, designing an annotation process to obtain natural but also clean targets is a significant challenge. One strategy employed by many datasets is to have annotators write targets from scratch (Banik et al., 2013; Wen et al., 2015; Gardent et al., 2017a) which can often lack variety in terms of structure and style (Gururangan et al., 2018; Poliak et al., 2018). An alternative is to pair naturally occurring text with tables (Lebret et al., 2016; Wiseman et al., 2017). Wh"
2020.emnlp-main.89,W17-3518,0,0.484701,"t to tasks that test these aspects in the NLU setting (Pasupat and Liang, 2015; Chen et al., 2019; Dua et al., 2019). However, constructing a data-to-text dataset can be challenging on two axes: task design and annotation process. First, tasks with open-ended output like summarization (Mani, 1999; Lebret et al., 2016; Wiseman et al., 2017) lack explicit signals for models on what to generate, which can lead to subjective content and evaluation challenges (Kry´sci´nski et al., 2019). On the other hand, data-to-text tasks that are limited to verbalizing a fully specified meaning representation (Gardent et al., 2017b) do not test a model’s ability to perform inference and thus remove a considerable amount of challenge from the task. Secondly, designing an annotation process to obtain natural but also clean targets is a significant challenge. One strategy employed by many datasets is to have annotators write targets from scratch (Banik et al., 2013; Wen et al., 2015; Gardent et al., 2017a) which can often lack variety in terms of structure and style (Gururangan et al., 2018; Poliak et al., 2018). An alternative is to pair naturally occurring text with tables (Lebret et al., 2016; Wiseman et al., 2017). Wh"
2020.emnlp-main.89,W18-6505,1,0.869327,"t h(D) be the set of 1178 header values for a given dataset D. We remove examples d from the training set where h(d) is both rare in the data as well as occurs in either the development or test sets. Specifically, Dtrain is defined as: pre-train a version of BERT on the Books corpus only, which we consider a more correct baseline. However, empirically we find that both models perform similarly in practice (Table 8). • Pointer-Generator (See et al., 2017): A Seq2Seq model with attention and copy mechanism. While originally designed for summarization it is commonly used in data-to-text as well (Gehrmann et al., 2018). • Puduppully et al. (2019): A Seq2Seq model with an explicit content selection and planning mechanism designed for data-to-text. Dtrain := {d : h(d) ∈ / (h(Ddev ) ∪ h(Dtest )) or  count h(d), Dorig-train &gt; α}. The count(h(d), Dorig-train ) function returns the number of examples in Dorig-train with header h(d). To choose the hyperparameter α we first split the test set as follows: Dtest-overlap := {d : h(d) ∈ h(Dtrain )} Dtest-nonoverlap := {d : h(d) ∈ / h(Dtrain )} The development set is analogously divided into Ddev-overlap and Ddev-nonoverlap . We then choose α = 5 so that Dtest-overlap"
2020.emnlp-main.89,N18-2017,0,0.136218,"´sci´nski et al., 2019). On the other hand, data-to-text tasks that are limited to verbalizing a fully specified meaning representation (Gardent et al., 2017b) do not test a model’s ability to perform inference and thus remove a considerable amount of challenge from the task. Secondly, designing an annotation process to obtain natural but also clean targets is a significant challenge. One strategy employed by many datasets is to have annotators write targets from scratch (Banik et al., 2013; Wen et al., 2015; Gardent et al., 2017a) which can often lack variety in terms of structure and style (Gururangan et al., 2018; Poliak et al., 2018). An alternative is to pair naturally occurring text with tables (Lebret et al., 2016; Wiseman et al., 2017). While more diverse, naturally occurring targets are often noisy and contain information that cannot be inferred from the source. This can make it problematic to disentangle modeling weaknesses from data noise. In this work, we propose T OTT O, an opendomain table-to-text generation dataset that intro1173 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1173–1186, c November 16–20, 2020. 2020 Association for Computationa"
2020.emnlp-main.89,Q18-1031,0,0.0326281,"s a summarization problem. However, summarization is much more subjective, which can make the task underconstrained and difficult to evaluate (Kry´sci´nski et al., 2019). We place T OTT O as a middle-ground where the highlighted cells provide some guidance on the topic of the target but still leave a considerable amount of content planning to be done by the model. the table. This enables T OTT O to maintain the varied language and structure found in natural sentences while producing cleaner targets. The technique of editing exemplar sentences has been used in semiparametric generation models (Guu et al., 2018; Pandey et al., 2018; Peng et al., 2019) and crowd-sourcing small, iterative changes to text has been shown to lead to higher-quality data and a more robust annotation process (Little et al., 2010). Perez-Beltrachini and Lapata (2018) also employed a revision strategy to construct a cleaner evaluation set for Wikibio (Lebret et al., 2016). Concurrent to this work, Chen et al. (2020) proposed LogicNLG which also uses Wikipedia tables, although omitting some of the more complex structured ones included in our dataset. Their target sentences are annotator-generated and their task is significantl"
2020.emnlp-main.89,W17-3204,0,0.0768419,"Missing"
2020.emnlp-main.89,D19-1051,0,0.0497763,"Missing"
2020.emnlp-main.89,P83-1022,0,0.499371,"obtain generated targets that are natural but also faithful to the source table, we introduce a dataset construction process where annotators directly revise existing candidate sentences from Wikipedia. We present systematic analyses of our dataset and annotation process as well as results achieved by several state-of-the-art baselines. While usually fluent, existing methods often hallucinate phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.1 1 Introduction Data-to-text generation (Kukich, 1983; McKeown, 1992) is the task of generating a target textual description y conditioned on source content x in the form of structured data such as a table. Examples include generating sentences given biographical data (Lebret et al., 2016), textual descriptions of restaurants given meaning representations (Novikova et al., 2017), basketball game summaries given boxscore statistics (Wiseman et al., 2017), and generating fun facts from superlative tables in Wikipedia (Korn et al., 2019). Existing data-to-text tasks have provided an important test-bed for neural generation models (Sutskever et al.,"
2020.emnlp-main.89,D16-1128,0,0.109807,"yses of our dataset and annotation process as well as results achieved by several state-of-the-art baselines. While usually fluent, existing methods often hallucinate phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.1 1 Introduction Data-to-text generation (Kukich, 1983; McKeown, 1992) is the task of generating a target textual description y conditioned on source content x in the form of structured data such as a table. Examples include generating sentences given biographical data (Lebret et al., 2016), textual descriptions of restaurants given meaning representations (Novikova et al., 2017), basketball game summaries given boxscore statistics (Wiseman et al., 2017), and generating fun facts from superlative tables in Wikipedia (Korn et al., 2019). Existing data-to-text tasks have provided an important test-bed for neural generation models (Sutskever et al., 2014; Bahdanau et al., 2014). Neural models are known to be prone to hallucination, i.e., generating text that is fluent but not faithful to the source (Vinyals and Le, 2015; Koehn ∗ Work done during an internship at Google. T OTT O is"
2020.emnlp-main.89,P09-1011,0,0.0375768,"hful to the source (see Table 4 and the Appendix for more complex examples). Our experiments demonstrate that state-of-the-art neural models struggle to generate faithful results, despite the high quality of the training data. These results suggest that our dataset could serve as a useful benchmark for controllable data-to-text generation. 2 Related Work T OTT O differs from existing datasets in both task design and annotation process as we describe below. A summary is given in Table 2. Task Design Most existing table-to-text datasets are restricted in topic and schema such as W EATH ER G OV (Liang et al., 2009), ROBO C UP (Chen and Mooney, 2008), Rotowire (Wiseman et al., 2017, basketball), E2E (Novikova et al., 2016, 2017, restaurants), KBGen (Banik et al., 2013, biology), and Wikibio (Lebret et al., 2016, biographies). In contrast, T OTT O contains tables with various schema spanning various topical categories all over Wikipedia. Moreover, T OTT O takes a different view of content selection compared to 1174 Dataset Train Size Wikibio (Lebret et al., 2016) 583K Rotowire (Wiseman et al., 2017) 4.9K WebNLG (Gardent et al., 2017b) 25.3K E2E (Novikova et al., 2017) 50.6K LogicNLG (Chen et al., 2020) 28"
2020.emnlp-main.89,W19-5302,0,0.0175146,"odel achieves the highest BLEU. Red indicates model errors and blue denotes interesting reference language not in the model output. in the non-overlap case, where we see a moderate effect favoring the book model. model is unable to make these inferences from the simplistic source representation that we used. 9 Evaluation metrics Many of the above issues are difficult to capture with metrics like BLEU since the reference and prediction may only differ by a word but largely differ in terms of semantic meaning. This urges for better metrics possibly built on learned models (Wiseman et al., 2017; Ma et al., 2019; Sellam et al., 2020). Thus, while we have a task leaderboard, it should not be interpreted as the definitive measure of model performance. Model Errors and Challenges Table 11 shows predictions from the BERT-toBERT Books model to illustrate challenges existing models face. Hallucination The model sometimes outputs phrases such as first, winning that seem reasonable but are not faithful to the table. This hallucination phenomenon has been widely observed in other existing data-to-text datasets (Lebret et al., 2016; Wiseman et al., 2017). However, the noisy references in these datasets make it"
2020.emnlp-main.89,W17-5525,0,0.0636405,"Missing"
2020.emnlp-main.89,W16-6644,0,0.148876,"hat state-of-the-art neural models struggle to generate faithful results, despite the high quality of the training data. These results suggest that our dataset could serve as a useful benchmark for controllable data-to-text generation. 2 Related Work T OTT O differs from existing datasets in both task design and annotation process as we describe below. A summary is given in Table 2. Task Design Most existing table-to-text datasets are restricted in topic and schema such as W EATH ER G OV (Liang et al., 2009), ROBO C UP (Chen and Mooney, 2008), Rotowire (Wiseman et al., 2017, basketball), E2E (Novikova et al., 2016, 2017, restaurants), KBGen (Banik et al., 2013, biology), and Wikibio (Lebret et al., 2016, biographies). In contrast, T OTT O contains tables with various schema spanning various topical categories all over Wikipedia. Moreover, T OTT O takes a different view of content selection compared to 1174 Dataset Train Size Wikibio (Lebret et al., 2016) 583K Rotowire (Wiseman et al., 2017) 4.9K WebNLG (Gardent et al., 2017b) 25.3K E2E (Novikova et al., 2017) 50.6K LogicNLG (Chen et al., 2020) 28.5K T OTT O 120K Domain Biographies Basketball 15 DBPedia categories Restaurants Wikipedia (open-domain) Wik"
2020.emnlp-main.89,P18-1123,0,0.0215634,"problem. However, summarization is much more subjective, which can make the task underconstrained and difficult to evaluate (Kry´sci´nski et al., 2019). We place T OTT O as a middle-ground where the highlighted cells provide some guidance on the topic of the target but still leave a considerable amount of content planning to be done by the model. the table. This enables T OTT O to maintain the varied language and structure found in natural sentences while producing cleaner targets. The technique of editing exemplar sentences has been used in semiparametric generation models (Guu et al., 2018; Pandey et al., 2018; Peng et al., 2019) and crowd-sourcing small, iterative changes to text has been shown to lead to higher-quality data and a more robust annotation process (Little et al., 2010). Perez-Beltrachini and Lapata (2018) also employed a revision strategy to construct a cleaner evaluation set for Wikibio (Lebret et al., 2016). Concurrent to this work, Chen et al. (2020) proposed LogicNLG which also uses Wikipedia tables, although omitting some of the more complex structured ones included in our dataset. Their target sentences are annotator-generated and their task is significantly more uncontrolled d"
2020.emnlp-main.89,P02-1040,0,0.108003,"aseline). • BERT-to-BERT (Rothe et al., 2020): A Transformer encoder-decoder model (Vaswani et al., 2017) where the encoder and decoder are both initialized with BERT (Devlin et al., 2018). The original BERT model is pre-trained with both Wikipedia and the Books corpus (Zhu et al., 2015), the former of which contains our (unrevised) test targets. Thus, we also In all cases, the cells are linearized with row and column separator tokens. We also experiment with prepending the table metadata to the source table.6 Evaluation metrics The model output is evaluated using two automatic metrics: BLEU (Papineni et al., 2002) and PARENT (Dhingra et al., 2019). PARENT is a metric recently proposed specifically for data-to-text evaluation that takes the table into account. We modify it to make it suitable for our dataset, described in the Appendix. Human evaluation is described in § 8.2. 8.1 Results Table 8 shows our results against multiple references with the subtable input format. Both the 6 The table section text is ignored, since it is usually missing or irrelevant. 1179 Overall Model Overlap Subset Nonoverlap Subset BLEU PARENT BLEU PARENT BLEU PARENT 44.0 43.9 41.6 19.2 52.6 52.6 51.6 29.2 52.7 52.7 50.6 24.5"
2020.emnlp-main.89,P15-1142,0,0.0470512,"uring an internship at Google. T OTT O is available at https://github.com/ google-research-datasets/totto. 1 and Knowles, 2017; Lee et al., 2018; Tian et al., 2019) and it is often easier to assess faithfulness of the generated text when the source content is structured (Wiseman et al., 2017; Dhingra et al., 2019). Moreover, structured data can also test a model’s ability for reasoning and numerical inference (Wiseman et al., 2017) and for building representations of structured objects (Liu et al., 2018), providing an interesting complement to tasks that test these aspects in the NLU setting (Pasupat and Liang, 2015; Chen et al., 2019; Dua et al., 2019). However, constructing a data-to-text dataset can be challenging on two axes: task design and annotation process. First, tasks with open-ended output like summarization (Mani, 1999; Lebret et al., 2016; Wiseman et al., 2017) lack explicit signals for models on what to generate, which can lead to subjective content and evaluation challenges (Kry´sci´nski et al., 2019). On the other hand, data-to-text tasks that are limited to verbalizing a fully specified meaning representation (Gardent et al., 2017b) do not test a model’s ability to perform inference and"
2020.emnlp-main.89,N19-1263,1,0.844058,"marization is much more subjective, which can make the task underconstrained and difficult to evaluate (Kry´sci´nski et al., 2019). We place T OTT O as a middle-ground where the highlighted cells provide some guidance on the topic of the target but still leave a considerable amount of content planning to be done by the model. the table. This enables T OTT O to maintain the varied language and structure found in natural sentences while producing cleaner targets. The technique of editing exemplar sentences has been used in semiparametric generation models (Guu et al., 2018; Pandey et al., 2018; Peng et al., 2019) and crowd-sourcing small, iterative changes to text has been shown to lead to higher-quality data and a more robust annotation process (Little et al., 2010). Perez-Beltrachini and Lapata (2018) also employed a revision strategy to construct a cleaner evaluation set for Wikibio (Lebret et al., 2016). Concurrent to this work, Chen et al. (2020) proposed LogicNLG which also uses Wikipedia tables, although omitting some of the more complex structured ones included in our dataset. Their target sentences are annotator-generated and their task is significantly more uncontrolled due to the lack of an"
2020.emnlp-main.89,D15-1199,0,0.398205,"als for models on what to generate, which can lead to subjective content and evaluation challenges (Kry´sci´nski et al., 2019). On the other hand, data-to-text tasks that are limited to verbalizing a fully specified meaning representation (Gardent et al., 2017b) do not test a model’s ability to perform inference and thus remove a considerable amount of challenge from the task. Secondly, designing an annotation process to obtain natural but also clean targets is a significant challenge. One strategy employed by many datasets is to have annotators write targets from scratch (Banik et al., 2013; Wen et al., 2015; Gardent et al., 2017a) which can often lack variety in terms of structure and style (Gururangan et al., 2018; Poliak et al., 2018). An alternative is to pair naturally occurring text with tables (Lebret et al., 2016; Wiseman et al., 2017). While more diverse, naturally occurring targets are often noisy and contain information that cannot be inferred from the source. This can make it problematic to disentangle modeling weaknesses from data noise. In this work, we propose T OTT O, an opendomain table-to-text generation dataset that intro1173 Proceedings of the 2020 Conference on Empirical Meth"
2020.emnlp-main.89,D17-1239,0,0.113448,"phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.1 1 Introduction Data-to-text generation (Kukich, 1983; McKeown, 1992) is the task of generating a target textual description y conditioned on source content x in the form of structured data such as a table. Examples include generating sentences given biographical data (Lebret et al., 2016), textual descriptions of restaurants given meaning representations (Novikova et al., 2017), basketball game summaries given boxscore statistics (Wiseman et al., 2017), and generating fun facts from superlative tables in Wikipedia (Korn et al., 2019). Existing data-to-text tasks have provided an important test-bed for neural generation models (Sutskever et al., 2014; Bahdanau et al., 2014). Neural models are known to be prone to hallucination, i.e., generating text that is fluent but not faithful to the source (Vinyals and Le, 2015; Koehn ∗ Work done during an internship at Google. T OTT O is available at https://github.com/ google-research-datasets/totto. 1 and Knowles, 2017; Lee et al., 2018; Tian et al., 2019) and it is often easier to assess faithfulnes"
2020.emnlp-main.89,N18-1137,0,0.0181728,"highlighted cells provide some guidance on the topic of the target but still leave a considerable amount of content planning to be done by the model. the table. This enables T OTT O to maintain the varied language and structure found in natural sentences while producing cleaner targets. The technique of editing exemplar sentences has been used in semiparametric generation models (Guu et al., 2018; Pandey et al., 2018; Peng et al., 2019) and crowd-sourcing small, iterative changes to text has been shown to lead to higher-quality data and a more robust annotation process (Little et al., 2010). Perez-Beltrachini and Lapata (2018) also employed a revision strategy to construct a cleaner evaluation set for Wikibio (Lebret et al., 2016). Concurrent to this work, Chen et al. (2020) proposed LogicNLG which also uses Wikipedia tables, although omitting some of the more complex structured ones included in our dataset. Their target sentences are annotator-generated and their task is significantly more uncontrolled due to the lack of annotator highlighted cells. Annotation Process There are various existing strategies to create the reference target y. One strategy employed by many datasets is to have annotators write targets f"
2020.emnlp-main.89,S18-2023,0,0.0622913,"Missing"
2020.emnlp-main.89,2020.tacl-1.18,0,0.0606276,"refore, the task is more challenging, as the model must generate a new sentence instead of revising an existing one. 8 Details about hyperparameter settings are provided in the Appendix. Moreover, we explore different strategies of representing the source content that resemble standard linearization approaches in the literature (Lebret et al., 2016; Wiseman et al., 2017) Experiments We present baseline results on T OTT O by examining three existing state-of-the-art approaches (Note that since our tables do not have a fixed schema it is difficult to design a template baseline). • BERT-to-BERT (Rothe et al., 2020): A Transformer encoder-decoder model (Vaswani et al., 2017) where the encoder and decoder are both initialized with BERT (Devlin et al., 2018). The original BERT model is pre-trained with both Wikipedia and the Books corpus (Zhu et al., 2015), the former of which contains our (unrevised) test targets. Thus, we also In all cases, the cells are linearized with row and column separator tokens. We also experiment with prepending the table metadata to the source table.6 Evaluation metrics The model output is evaluated using two automatic metrics: BLEU (Papineni et al., 2002) and PARENT (Dhingra et"
2020.emnlp-main.89,2020.acl-main.704,1,0.851685,"highest BLEU. Red indicates model errors and blue denotes interesting reference language not in the model output. in the non-overlap case, where we see a moderate effect favoring the book model. model is unable to make these inferences from the simplistic source representation that we used. 9 Evaluation metrics Many of the above issues are difficult to capture with metrics like BLEU since the reference and prediction may only differ by a word but largely differ in terms of semantic meaning. This urges for better metrics possibly built on learned models (Wiseman et al., 2017; Ma et al., 2019; Sellam et al., 2020). Thus, while we have a task leaderboard, it should not be interpreted as the definitive measure of model performance. Model Errors and Challenges Table 11 shows predictions from the BERT-toBERT Books model to illustrate challenges existing models face. Hallucination The model sometimes outputs phrases such as first, winning that seem reasonable but are not faithful to the table. This hallucination phenomenon has been widely observed in other existing data-to-text datasets (Lebret et al., 2016; Wiseman et al., 2017). However, the noisy references in these datasets make it difficult to disentan"
2020.emnlp-main.89,P17-1099,0,\N,Missing
2020.emnlp-main.89,N19-1423,0,\N,Missing
2020.emnlp-main.89,D19-1609,0,\N,Missing
2020.wmt-1.102,W19-5301,0,0.0563213,"Missing"
2020.wmt-1.102,E06-1040,0,0.266454,"LEURT’s predictions with those of Y I S I and use alternative reference translations to enhance the performance. Empirical results show that the models achieve competitive results on the WMT Metrics 2019 Shared Task, indicating their promise for the 2020 edition. 1 Introduction The recent progress in machine translation models has led researchers to question the use of ngram overlap metrics such as BLEU, which focus solely on surface-level aspects of the generated text, and thus may correlate poorly with human evaluation (Papineni et al., 2002; Lin, 2004; Ma et al., 2019; Mathur et al., 2020; Belz and Reiter, 2006; Callison-Burch et al., 2006). This has led to a surge of interest for more flexible metrics that use machine learning to capture semantic-level information (Celikyilmaz et al., 2020). Popular examples of such metrics include Y I S I -1 (Lo, 2019), ESIM (Mathur et al., 2019), BERT SCORE (Zhang et al., 2020), the Sentence 2 Background and Notations Task Reference-based NLG evaluation seeks to assign a score to a triplet of sentences (input, reference, candidate), where input is a sentence in the source language, reference is a reference translation kept secret at inference time, and candidate"
2020.wmt-1.102,W17-4755,0,0.0388605,"itialized from a publicly available BERT checkpoint. (2) The model is then “warmed up” by exposing it to ˜ obtained by millions of sentence pairs (x, x), randomly perturbing sentences from Wikipedia. During this phase, the model learns to predict a wide range of similarity scores that include existing metrics (BERT SCORE, BLEU, ROUGE), scores from an entailment model, and the likeli˜ was generated from x with a roundhood that x trip translation by a given translation model. We denote this stage as mid-training. (3) In the final stage, the model is fine-tuned on human ratings from WMT Metrics (Bojar et al., 2017; Ma et al., 2018, a regression loss P 2019), using 2 . We found that Enℓsupervised = N1 N ky −ˆ y k i n=1 glish B LEURT achieved competitive performance on four academic datasets, WebNLG (Gardent et al., 2017), and the WMT Metrics Shared Task years 2017 to 2019. 3 Details of M BERT-WMT pre-training We trained M BERT-WMT model with an MLM loss (Devlin et al., 2019), using a combination of public datasets: Wikipedia, the WMT 2019 News Crawl (Barrault et al.), the C4 variant of Common Crawl (Raffel et al., 2020), OPUS (Tiedemann, 2012), Nunavut Hansard (Joanis et al., 2020), WikiTitles2 , and Pa"
2020.wmt-1.102,E06-1032,0,0.183549,"h those of Y I S I and use alternative reference translations to enhance the performance. Empirical results show that the models achieve competitive results on the WMT Metrics 2019 Shared Task, indicating their promise for the 2020 edition. 1 Introduction The recent progress in machine translation models has led researchers to question the use of ngram overlap metrics such as BLEU, which focus solely on surface-level aspects of the generated text, and thus may correlate poorly with human evaluation (Papineni et al., 2002; Lin, 2004; Ma et al., 2019; Mathur et al., 2020; Belz and Reiter, 2006; Callison-Burch et al., 2006). This has led to a surge of interest for more flexible metrics that use machine learning to capture semantic-level information (Celikyilmaz et al., 2020). Popular examples of such metrics include Y I S I -1 (Lo, 2019), ESIM (Mathur et al., 2019), BERT SCORE (Zhang et al., 2020), the Sentence 2 Background and Notations Task Reference-based NLG evaluation seeks to assign a score to a triplet of sentences (input, reference, candidate), where input is a sentence in the source language, reference is a reference translation kept secret at inference time, and candidate is a translation produced by a"
2020.wmt-1.102,P19-1264,0,0.0359405,"Missing"
2020.wmt-1.102,N19-1423,0,0.0685014,"for Y, as observed in the past literature (Karthikeyan et al., 2019; Pires et al., 2019). We experiment with two pre-trained multilingual models: M BERT and M BERT-WMT, a custom multilingual variant of BERT. The M BERTWMT model is larger that M BERT (24 Transformer layers instead of 12), and it was pre-trained on 19 languages of the WMT Metrics shared task 2015 to 2020. B LEURT Most experiments presented in this paper are based on B LEURT, a metric that leverages transfer learning to achieve high accuracy and increase robustness (Sellam et al., 2020). B LEURT is a BERT-based regression model (Devlin et al., 2019). It embeds sentence pairs into a fixed-width ˜ with a pre-trained vector vBERT = BERT(x, x) Transformer, and feeds this vector to a linear layer: ˜ = W vBERT + b yˆ = f (x, x) where W and b are the weight matrix and bias vector respectively. In its original (English) version, B LEURT is trained in three stages. (1) It is initialized from a publicly available BERT checkpoint. (2) The model is then “warmed up” by exposing it to ˜ obtained by millions of sentence pairs (x, x), randomly perturbing sentences from Wikipedia. During this phase, the model learns to predict a wide range of similarity"
2020.wmt-1.102,W19-6721,0,0.0575301,"Missing"
2020.wmt-1.102,2020.emnlp-main.5,1,0.825175,"on’s correlation. The scores for Y I S I, Y I S I 1-SRL, and ESIM come from Ma et al. (2019). The scores for BERT SCORE and PRISM come from Thompson and Post (2020). free and calculates its score by comparing translations only to the source sentence. BLEURT is fine-tuned on previous human ratings, while Y I S I 1 is based on the cosine similarity between BERT embeddings of the reference and the candidate. In the remainder of this section, we report B LEURT results using the M BERT-WMT setup unless specified otherwise.3 with the standard WMT references as well as the paraphrased reference from Freitag et al. (2020). Improving YiSi’s Predictions Our baseline is similar to the Y I S I -1 submission from WMT 2019 (Lo, 2019): we run Y I S I -1 with the public multilingual M BERT checkpoint. We then experiment with the underlying checkpoint. We continued pre-training M BERT on the in-domain German NewsCrawl dataset. The resulting model +pre-train NewsCrawl layer 9 increases the correlation for both reference translations. We improve the correlation further on the paraphrased reference by using the 8th instead of the 9th layer. 5.1 Modifications to YiSi-1 Before combining BLEURT and Y I S I, we perform a seri"
2020.wmt-1.102,W17-3518,0,0.0236694,"this phase, the model learns to predict a wide range of similarity scores that include existing metrics (BERT SCORE, BLEU, ROUGE), scores from an entailment model, and the likeli˜ was generated from x with a roundhood that x trip translation by a given translation model. We denote this stage as mid-training. (3) In the final stage, the model is fine-tuned on human ratings from WMT Metrics (Bojar et al., 2017; Ma et al., 2018, a regression loss P 2019), using 2 . We found that Enℓsupervised = N1 N ky −ˆ y k i n=1 glish B LEURT achieved competitive performance on four academic datasets, WebNLG (Gardent et al., 2017), and the WMT Metrics Shared Task years 2017 to 2019. 3 Details of M BERT-WMT pre-training We trained M BERT-WMT model with an MLM loss (Devlin et al., 2019), using a combination of public datasets: Wikipedia, the WMT 2019 News Crawl (Barrault et al.), the C4 variant of Common Crawl (Raffel et al., 2020), OPUS (Tiedemann, 2012), Nunavut Hansard (Joanis et al., 2020), WikiTitles2 , and ParaCrawl (Espl`a-Gomis et al., 2019). We trained a new WordPiece vocabulary (Schuster and Nakajima, 2012; Wu et al., 2016), since the original vocabulary of mBERT does not support the alphabets of Pashto, Khmer"
2020.wmt-1.102,2020.lrec-1.312,0,0.0123545,"tings from WMT Metrics (Bojar et al., 2017; Ma et al., 2018, a regression loss P 2019), using 2 . We found that Enℓsupervised = N1 N ky −ˆ y k i n=1 glish B LEURT achieved competitive performance on four academic datasets, WebNLG (Gardent et al., 2017), and the WMT Metrics Shared Task years 2017 to 2019. 3 Details of M BERT-WMT pre-training We trained M BERT-WMT model with an MLM loss (Devlin et al., 2019), using a combination of public datasets: Wikipedia, the WMT 2019 News Crawl (Barrault et al.), the C4 variant of Common Crawl (Raffel et al., 2020), OPUS (Tiedemann, 2012), Nunavut Hansard (Joanis et al., 2020), WikiTitles2 , and ParaCrawl (Espl`a-Gomis et al., 2019). We trained a new WordPiece vocabulary (Schuster and Nakajima, 2012; Wu et al., 2016), since the original vocabulary of mBERT does not support the alphabets of Pashto, Khmer and Inuktitut. The model was trained for 1 million steps with the LAMB optimizer (You et al., 2020), using the learning rate 0.0018 and batch size 4096 on 64 TPU v3 chips. 3.2 Experimental Setup Datasets At the time of writing, no human ratings data is available for WMT Metrics 2020. Therefore, we use the human ratings from WMT Metrics years 2015 to 2019 for both tr"
2020.wmt-1.102,P19-1452,1,0.675575,"e Translation Beyond English B LEURT Submissions to the WMT Metrics 2020 Shared Task Thibault Sellam Amy Pu∗ Hyung Won Chung† Sebastian Gehrmann {tsellam, puamy, hwchung, gehrmann}@google.com Qijun Tan Markus Freitag Dipanjan Das Ankur P. Parikh {qijuntan, freitag, dipanjand, aparikh}@google.com Google Research Abstract Mover’s Similarity (Zhao et al., 2019; Clark et al., 2019), and B LEURT (Sellam et al., 2020). These metrics utilize contextual embeddings from large models such as BERT (Devlin et al., 2019) which have been shown to capture linguistic information beyond surface-level aspects (Tenney et al., 2019). The WMT Metrics 2020 Shared Task is the reference benchmark for evaluating these metrics in the context of machine translation. It tests the evaluation of systems that are to-English (X → En) and to other languages (X → Y), which requires a multilingual approach. An additional challenge for learned metrics is that human ratings are not available for all language pairs, and therefore, the models must use unlabeled data and perform zero-shot generalization. We describe several learned metrics based on B LEURT (Sellam et al., 2020), originally developed for English data. We first extend B LEURT"
2020.wmt-1.102,2020.emnlp-main.8,0,0.349541,"BERT-L2- LARGE are two regression models based on BERT and trained on to-English ratings. We use the same setup as English B LEURT, but we omit the mid-training phase. A similar approach was described in Shimanaka et al. (2019). BERT-C HINESE -L2 is similar to BERT-L2- BASE, but it uses BERTC HINESE and it is fine-tuned on to-Chinese ratings. 5 Other Systems We compare our setups to other state-of-the-art learned metrics: BERT SCORE (Zhang et al., 2020), and Yisi (Lo, 2019) all apply rules on top of BERT embeddings while ESIM (Mathur et al., 2019) is a neural sentence similarity model. PRISM (Thompson and Post, 2020) trains a multilingual translation model that is used as a zero-shot paraphrasing system. All the aforementioned systems take sentences pairs as input. Concurrent work has investigated incorporating the source with great Additional Improvements on English→German For English→German, the organizers of WMT20 provide three different reference translations: two standard references and one additional paraphrased reference. Given this novel setup, we investigate how to combine our predictions. Moreover, we use a similar framework to ensemble the predictions of different metrics. In particular, we ave"
2020.wmt-1.102,W04-1013,0,0.0568619,"glish to German and demonstrate how to combine B LEURT’s predictions with those of Y I S I and use alternative reference translations to enhance the performance. Empirical results show that the models achieve competitive results on the WMT Metrics 2019 Shared Task, indicating their promise for the 2020 edition. 1 Introduction The recent progress in machine translation models has led researchers to question the use of ngram overlap metrics such as BLEU, which focus solely on surface-level aspects of the generated text, and thus may correlate poorly with human evaluation (Papineni et al., 2002; Lin, 2004; Ma et al., 2019; Mathur et al., 2020; Belz and Reiter, 2006; Callison-Burch et al., 2006). This has led to a surge of interest for more flexible metrics that use machine learning to capture semantic-level information (Celikyilmaz et al., 2020). Popular examples of such metrics include Y I S I -1 (Lo, 2019), ESIM (Mathur et al., 2019), BERT SCORE (Zhang et al., 2020), the Sentence 2 Background and Notations Task Reference-based NLG evaluation seeks to assign a score to a triplet of sentences (input, reference, candidate), where input is a sentence in the source language, reference is a refere"
2020.wmt-1.102,tiedemann-2012-parallel,0,0.0440996,"the model is fine-tuned on human ratings from WMT Metrics (Bojar et al., 2017; Ma et al., 2018, a regression loss P 2019), using 2 . We found that Enℓsupervised = N1 N ky −ˆ y k i n=1 glish B LEURT achieved competitive performance on four academic datasets, WebNLG (Gardent et al., 2017), and the WMT Metrics Shared Task years 2017 to 2019. 3 Details of M BERT-WMT pre-training We trained M BERT-WMT model with an MLM loss (Devlin et al., 2019), using a combination of public datasets: Wikipedia, the WMT 2019 News Crawl (Barrault et al.), the C4 variant of Common Crawl (Raffel et al., 2020), OPUS (Tiedemann, 2012), Nunavut Hansard (Joanis et al., 2020), WikiTitles2 , and ParaCrawl (Espl`a-Gomis et al., 2019). We trained a new WordPiece vocabulary (Schuster and Nakajima, 2012; Wu et al., 2016), since the original vocabulary of mBERT does not support the alphabets of Pashto, Khmer and Inuktitut. The model was trained for 1 million steps with the LAMB optimizer (You et al., 2020), using the learning rate 0.0018 and batch size 4096 on 64 TPU v3 chips. 3.2 Experimental Setup Datasets At the time of writing, no human ratings data is available for WMT Metrics 2020. Therefore, we use the human ratings from WMT"
2020.wmt-1.102,W19-5358,0,0.200117,"s are not available for all language pairs, and therefore, the models must use unlabeled data and perform zero-shot generalization. We describe several learned metrics based on B LEURT (Sellam et al., 2020), originally developed for English data. We first extend B LEURT to the multilingual setup, and show that our approach achieves competitive results on the WMT Metrics 2019 Shared Task.1 We also present several simple BERT-based baselines, which we submit for analysis. Finally, we focus on English to German and enhance B LEURT’s performance by combining its predictions with those of Y I S I (Lo, 2019) as well as by using alternative references. The quality of machine translation systems has dramatically improved over the last decade, and as a result, evaluation has become an increasingly challenging problem. This paper describes our contribution to the WMT 2020 Metrics Shared Task, the main benchmark for automatic evaluation of translation. We make several submissions based on B LEURT, a previously published metric which uses transfer learning. We extend the metric beyond English and evaluate it on 14 language pairs for which fine-tuning data is available, as well as 4 “zero-shot” language"
2020.wmt-1.102,W18-6450,0,0.117728,"licly available BERT checkpoint. (2) The model is then “warmed up” by exposing it to ˜ obtained by millions of sentence pairs (x, x), randomly perturbing sentences from Wikipedia. During this phase, the model learns to predict a wide range of similarity scores that include existing metrics (BERT SCORE, BLEU, ROUGE), scores from an entailment model, and the likeli˜ was generated from x with a roundhood that x trip translation by a given translation model. We denote this stage as mid-training. (3) In the final stage, the model is fine-tuned on human ratings from WMT Metrics (Bojar et al., 2017; Ma et al., 2018, a regression loss P 2019), using 2 . We found that Enℓsupervised = N1 N ky −ˆ y k i n=1 glish B LEURT achieved competitive performance on four academic datasets, WebNLG (Gardent et al., 2017), and the WMT Metrics Shared Task years 2017 to 2019. 3 Details of M BERT-WMT pre-training We trained M BERT-WMT model with an MLM loss (Devlin et al., 2019), using a combination of public datasets: Wikipedia, the WMT 2019 News Crawl (Barrault et al.), the C4 variant of Common Crawl (Raffel et al., 2020), OPUS (Tiedemann, 2012), Nunavut Hansard (Joanis et al., 2020), WikiTitles2 , and ParaCrawl (Espl`a-G"
2020.wmt-1.102,W19-5302,0,0.495442,"rman and demonstrate how to combine B LEURT’s predictions with those of Y I S I and use alternative reference translations to enhance the performance. Empirical results show that the models achieve competitive results on the WMT Metrics 2019 Shared Task, indicating their promise for the 2020 edition. 1 Introduction The recent progress in machine translation models has led researchers to question the use of ngram overlap metrics such as BLEU, which focus solely on surface-level aspects of the generated text, and thus may correlate poorly with human evaluation (Papineni et al., 2002; Lin, 2004; Ma et al., 2019; Mathur et al., 2020; Belz and Reiter, 2006; Callison-Burch et al., 2006). This has led to a surge of interest for more flexible metrics that use machine learning to capture semantic-level information (Celikyilmaz et al., 2020). Popular examples of such metrics include Y I S I -1 (Lo, 2019), ESIM (Mathur et al., 2019), BERT SCORE (Zhang et al., 2020), the Sentence 2 Background and Notations Task Reference-based NLG evaluation seeks to assign a score to a triplet of sentences (input, reference, candidate), where input is a sentence in the source language, reference is a reference translation k"
2020.wmt-1.102,2020.acl-main.448,0,0.200326,"ate how to combine B LEURT’s predictions with those of Y I S I and use alternative reference translations to enhance the performance. Empirical results show that the models achieve competitive results on the WMT Metrics 2019 Shared Task, indicating their promise for the 2020 edition. 1 Introduction The recent progress in machine translation models has led researchers to question the use of ngram overlap metrics such as BLEU, which focus solely on surface-level aspects of the generated text, and thus may correlate poorly with human evaluation (Papineni et al., 2002; Lin, 2004; Ma et al., 2019; Mathur et al., 2020; Belz and Reiter, 2006; Callison-Burch et al., 2006). This has led to a surge of interest for more flexible metrics that use machine learning to capture semantic-level information (Celikyilmaz et al., 2020). Popular examples of such metrics include Y I S I -1 (Lo, 2019), ESIM (Mathur et al., 2019), BERT SCORE (Zhang et al., 2020), the Sentence 2 Background and Notations Task Reference-based NLG evaluation seeks to assign a score to a triplet of sentences (input, reference, candidate), where input is a sentence in the source language, reference is a reference translation kept secret at inferen"
2020.wmt-1.102,P19-1269,0,0.209865,"n The recent progress in machine translation models has led researchers to question the use of ngram overlap metrics such as BLEU, which focus solely on surface-level aspects of the generated text, and thus may correlate poorly with human evaluation (Papineni et al., 2002; Lin, 2004; Ma et al., 2019; Mathur et al., 2020; Belz and Reiter, 2006; Callison-Burch et al., 2006). This has led to a surge of interest for more flexible metrics that use machine learning to capture semantic-level information (Celikyilmaz et al., 2020). Popular examples of such metrics include Y I S I -1 (Lo, 2019), ESIM (Mathur et al., 2019), BERT SCORE (Zhang et al., 2020), the Sentence 2 Background and Notations Task Reference-based NLG evaluation seeks to assign a score to a triplet of sentences (input, reference, candidate), where input is a sentence in the source language, reference is a reference translation kept secret at inference time, and candidate is a translation produced by an MT system. 1 We use the following languages for fine-tuning and/or testing: Chinese, Czech, German, English, Estonian, Finnish, French, Gujarati, Kazakh, Lithuanian, Russian, and Turkish. In addition, we also pre-train on Inuktitut, Japanese, K"
2020.wmt-1.102,P02-1040,0,0.108653,"ionally, we focus on English to German and demonstrate how to combine B LEURT’s predictions with those of Y I S I and use alternative reference translations to enhance the performance. Empirical results show that the models achieve competitive results on the WMT Metrics 2019 Shared Task, indicating their promise for the 2020 edition. 1 Introduction The recent progress in machine translation models has led researchers to question the use of ngram overlap metrics such as BLEU, which focus solely on surface-level aspects of the generated text, and thus may correlate poorly with human evaluation (Papineni et al., 2002; Lin, 2004; Ma et al., 2019; Mathur et al., 2020; Belz and Reiter, 2006; Callison-Burch et al., 2006). This has led to a surge of interest for more flexible metrics that use machine learning to capture semantic-level information (Celikyilmaz et al., 2020). Popular examples of such metrics include Y I S I -1 (Lo, 2019), ESIM (Mathur et al., 2019), BERT SCORE (Zhang et al., 2020), the Sentence 2 Background and Notations Task Reference-based NLG evaluation seeks to assign a score to a triplet of sentences (input, reference, candidate), where input is a sentence in the source language, reference"
2020.wmt-1.102,D19-1053,0,0.0441387,"Missing"
2020.wmt-1.102,P19-1493,0,0.018569,"more accurate than monolingual models, possibly due to the larger amount of fine-tuning data. Thus, we opted for a simpler approach where we start with a multilingual BERT model and finetune it on all the human ratings data available for all languages (X → Y and X → En). In most cases, we found that such models could perform zero-shot evaluation: if a language Y does not have human ratings data, the metric can still perform evaluation in this target language as long as the base multilingual BERT model contains unlabeled data for Y, as observed in the past literature (Karthikeyan et al., 2019; Pires et al., 2019). We experiment with two pre-trained multilingual models: M BERT and M BERT-WMT, a custom multilingual variant of BERT. The M BERTWMT model is larger that M BERT (24 Transformer layers instead of 12), and it was pre-trained on 19 languages of the WMT Metrics shared task 2015 to 2020. B LEURT Most experiments presented in this paper are based on B LEURT, a metric that leverages transfer learning to achieve high accuracy and increase robustness (Sellam et al., 2020). B LEURT is a BERT-based regression model (Devlin et al., 2019). It embeds sentence pairs into a fixed-width ˜ with a pre-trained v"
2020.wmt-1.102,2020.emnlp-main.213,0,0.369399,"Missing"
2020.wmt-1.102,2020.acl-main.704,1,0.927496,"to capture linguistic information beyond surface-level aspects (Tenney et al., 2019). The WMT Metrics 2020 Shared Task is the reference benchmark for evaluating these metrics in the context of machine translation. It tests the evaluation of systems that are to-English (X → En) and to other languages (X → Y), which requires a multilingual approach. An additional challenge for learned metrics is that human ratings are not available for all language pairs, and therefore, the models must use unlabeled data and perform zero-shot generalization. We describe several learned metrics based on B LEURT (Sellam et al., 2020), originally developed for English data. We first extend B LEURT to the multilingual setup, and show that our approach achieves competitive results on the WMT Metrics 2019 Shared Task.1 We also present several simple BERT-based baselines, which we submit for analysis. Finally, we focus on English to German and enhance B LEURT’s performance by combining its predictions with those of Y I S I (Lo, 2019) as well as by using alternative references. The quality of machine translation systems has dramatically improved over the last decade, and as a result, evaluation has become an increasingly challe"
2021.acl-long.58,2021.ccl-1.108,0,0.106571,"Missing"
2021.acl-long.58,2020.acl-main.173,0,0.0663249,"ing approaches that combine extractive and abstractive generation methods to be more deliberately selective about which portions of evidence are being used and how they are integrated with information about the conversational discourse. 3 The appendix includes a full table of correlation coefficients Related Work Controlling hallucinations in text generation There is a body of work that has previously studied methods for integrating evidence in natural language generation tasks, with a focus on reducing hallucinations. Many of these works focus on other generation tasks such as summarization (Maynez et al., 2020; Zhao et al., 2020; Cao et al., 2018; Falke et al., 2019) or data-to-text generation (Puduppully et al., 2019). We investigate how the problem of reducing hallucinations can be applied to the task of knowledge grounded dialogue. Similar to our approach, Filippova (2020) also uses control codes to reduce hallucinations but focused instead on data-to-text generation tasks. Controllable Text Generation In order to control the faithfulness of responses, we draw on techniques from controllable text generation tasks. Most relevant is the development of control-codestyle input tokens such as in CTRL"
2021.acl-long.58,D18-1255,0,0.0275655,"the groundedness of a response. However, these metrics do not correlate to relevance or fluency. Based on these observations, it seems that these measures can be useful to gauge the general groundedness of the response but should still be viewed in tandem with other quality scores to get a more holistic understanding of performance. 5.6 Knowledge-Grounded Dialogue There has been significant prior work in tasks for designing dialogue agents that are grounded by document knowledge (Dinan et al., 2019; Qin et al., 2019; Ghazvininejad et al., 2018; Tian et al., 2020; Gopalakrishnan et al., 2019; Moghe et al., 2018). Some of these works investigate retrieving appropriate evidence (Lian et al., 2019; Meng et al., 2020; Kim et al., 2020), while we assume that a piece of evidence has already been retrieved and focus instead on how to craft generations that are more faithful to it. Our work is also novel in investigating controllable generation as one way of disentangling evidence-based utterances from more subjective utterances that may be present in the training data. Qualitative Examples In Table 5, we highlight some examples of model output (we also provide additional examples in the appendix). The respo"
2021.acl-long.58,Q18-1027,0,0.023156,"e et al., 2019) or data-to-text generation (Puduppully et al., 2019). We investigate how the problem of reducing hallucinations can be applied to the task of knowledge grounded dialogue. Similar to our approach, Filippova (2020) also uses control codes to reduce hallucinations but focused instead on data-to-text generation tasks. Controllable Text Generation In order to control the faithfulness of responses, we draw on techniques from controllable text generation tasks. Most relevant is the development of control-codestyle input tokens such as in CTRL (Keskar et al., 2019) or the LFT model of Niu and Bansal (2018). Others have used decoding-time re-ranking (Falke et al., 2019) to constrain the outputs in a way that is similar to our resampling method. Controllable generation has also been used previously with openended dialogue data (See et al., 2019) to improve qualities such as the engagingness; however, our work focuses on knowledge-grounded dialogues aiming to increase the faithfulness of the replies. Recently, Wu et al. (2020) used control phrases as controllable inputs to decrease hallucination as a form of content planning. We similarly use controllable features to reduce hallucinations in knowl"
2021.acl-long.58,P19-1539,0,0.101218,"k of knowledge-grounded dialogue, where a system produces a dialogue response using a piece of evidence from a grounding document and a previous conversation history as input (as in Figure 1). Whereas P ERSONAC HAT-style tasks (Zhang et al., 2018) may focus on dialogue systems that are meant to be engaging, this task focuses instead on systems that are meant to be informative, meaning that they only share verifiable information and exclude subjective or invented personal information. There are existing knowledge-grounded dialogue datasets (e.g. (Ghazvininejad et al., 2018; Dinan et al., 2019; Qin et al., 2019)) that could 704 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 704–718 August 1–6, 2021. ©2021 Association for Computational Linguistics be appropriate training resources for such an informative dialogue agent. However, we observe that these datasets often contain utterances with varying conversation styles and intents, including some utterances that are more informative and some that are chit-chat utterances or subjective commentary. For instance, in Figure 1, we show an"
2021.acl-long.58,N19-1170,0,0.0221431,"codes to reduce hallucinations but focused instead on data-to-text generation tasks. Controllable Text Generation In order to control the faithfulness of responses, we draw on techniques from controllable text generation tasks. Most relevant is the development of control-codestyle input tokens such as in CTRL (Keskar et al., 2019) or the LFT model of Niu and Bansal (2018). Others have used decoding-time re-ranking (Falke et al., 2019) to constrain the outputs in a way that is similar to our resampling method. Controllable generation has also been used previously with openended dialogue data (See et al., 2019) to improve qualities such as the engagingness; however, our work focuses on knowledge-grounded dialogues aiming to increase the faithfulness of the replies. Recently, Wu et al. (2020) used control phrases as controllable inputs to decrease hallucination as a form of content planning. We similarly use controllable features to reduce hallucinations in knowledge grounded dialogues, but our model uses stylis711 Document Evidence curry (, plural curries) is an umbrella term referring to a number of dishes originating in the cuisine of the indian subcontinent. Conversation History Speaker 1: i rece"
2021.acl-long.58,2020.acl-main.222,0,0.547333,"over the base model on all metrics. Results also show that using all control code features together generally improves the performance across the automatic metrics. 5.4 Automatic Metric Results on Test Set We show results on both portions of the Wizard of Wikipedia test set in Table 3. As baselines, we use finetuned GPT-2 and T5 without any controllable features or resampling. We also include results the end-to-end generative model (E2E) with gold knowledge that was introduced in the original Wizard of Wikipedia paper (Dinan et al., 2019) and the model in the follow-up work on dodecaDialogue (Shuster et al., 2020). These are transformer-based architectures that use the evidence and conversation history as inputs but do not explicitly control the model to be more faithful to the input. In general, we find that models with pre-trained or multi-task training set-ups (dodecaDialogue, GPT-2, and T5) have relatively consistent performance across both the seen and unseen topic partitions of the test set, indicating that these models can generalize fairly well to unseen topics. Results generally show improvements over the baselines when using control codes. By additionally using resampling at decoding time, we"
2021.acl-long.58,2020.acl-main.61,0,0.0917945,"imate 705 objective voice as a binary variable based on the presence of first person singular pronouns detected using a word list. Wizard of Wikipedia (Dinan et al., 2019) Lexical Precision We also want to ensure that the response is not adding extra information from what’s in the selected evidence. To estimate this, we measure the precision of the unigrams in the response with respect to the evidence. A high value indicates that most of the words in the response are contained somewhere in the evidence. We use this measure because it is relevant to grounding precision scores in previous work (Tian et al., 2020) and because it can reasonably gauge how extractive the response is, but one drawback of this measure is that it is based on lexical features which may not reflect semantic differences in the information being shared (e.g. dropping the word ‘not’ may yield high lexical precision but a very different semantic meaning from the original evidence). We leave investigation of more semantic-oriented measures of the precision of information to future work. Entailment Lastly, we want to encourage the model to produce a response that is semantically entailed by the source document. We use a state-ofthe-"
2021.acl-long.58,2020.acl-demos.30,0,0.0346752,"T-2 and T5 have been carefully selected, these large datasets may contain sources with unfair distributions and factual inaccuracies, and thus the models and the resulting generated synthetic data may have inherited these biases. Additionally, the output generated by these models may only succeed in being superficially similar to human-written text or dialogue turns. Generation Model As our underlying dialogue model, we use neural seq2seq architectures – T5 (Raffel et al., 2020) and GPT-2 (Radford et al., 2019), which are architectures used in state-of-the-art dialogue systems (e.g. DialoGPT (Zhang et al., 2020)). We fine-tune these models on our grounded dialogue dataset. The input to the model is a sequence of evidence tokens e1 ...ep and a dialogue history which we treat as a sequence of tokens x1 ...xm where the utterances are delimited by the speaker ID (either &lt;speaker1&gt; or &lt;speaker2&gt;). For the GPT-2 model, we also include special token-type embeddings that are added to the byte-pair embedding tokens and position embeddings. The tokentype embeddings denote the segments of the input that belong to the evidence and the two different speakers. We train the model to produce the next conversation ut"
2021.acl-long.58,2020.findings-emnlp.203,0,0.0146231,"tion coefficients Related Work Controlling hallucinations in text generation There is a body of work that has previously studied methods for integrating evidence in natural language generation tasks, with a focus on reducing hallucinations. Many of these works focus on other generation tasks such as summarization (Maynez et al., 2020; Zhao et al., 2020; Cao et al., 2018; Falke et al., 2019) or data-to-text generation (Puduppully et al., 2019). We investigate how the problem of reducing hallucinations can be applied to the task of knowledge grounded dialogue. Similar to our approach, Filippova (2020) also uses control codes to reduce hallucinations but focused instead on data-to-text generation tasks. Controllable Text Generation In order to control the faithfulness of responses, we draw on techniques from controllable text generation tasks. Most relevant is the development of control-codestyle input tokens such as in CTRL (Keskar et al., 2019) or the LFT model of Niu and Bansal (2018). Others have used decoding-time re-ranking (Falke et al., 2019) to constrain the outputs in a way that is similar to our resampling method. Controllable generation has also been used previously with openend"
2021.gem-1.10,2020.acl-main.424,0,0.0251404,"les WebNLG (Gardent et al., 2017) Produce a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmarks are critical in measuring modeling prog"
2021.gem-1.10,2020.acl-main.766,0,0.0426234,"sh datasets were ranked lower, with only WebNLG and MLSum among the top 15 datasets. We grouped all datasets by their high-level tasks and selected a group that would not violate the selection principles (e.g., only high-resource tasks). If two datasets fit, we picked the one with a higher interest rating. Among the 11 datasets, we have 18different languages, and the dataset sizes range from 5,000 examples to 1.5M, with most datasets between 50-150k examples. Two of them do not include English at all, which we hope reduces the dependence of the modeling approaches on anglocentric pretraining (Anastasopoulos and Neubig, 2020). The high-level tasks include Dialog, Summarization, Data-to-Text, and Simplification. About half of the datasets have multiple references and more than half had post-processing steps applied to them to ensure high data quality. 3.1 GEMifying the data We produce data cards (Bender and Friedman, 2018; Gebru et al., 2018) for all data sets in GEM, for which we developed an NLG-specific template.7 In addition to describing the data itself, the cards acknowledge potential limitations of a dataset regarding its creation process and describe its real-world use cases to ensure that the research is c"
2021.gem-1.10,W05-0909,0,0.165721,"otebook within 2-3 hours. 4.2 avoid overfitting to known metrics, we will use metrics on the test submissions that are not included in this initial writeup. Consequentially, the baseline results are an incomplete list which will be expanded upon the announcement of the test metrics. The set of metrics can be computed via the framework described at https://gem-benchmark. com/shared_task which comprises metrics in the following categories: Lexical Similarity. We include multiple “traditional” metrics as baseline metrics, notably BLEU (Papineni et al., 2002), ROUGE-1/2/L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005). These metrics can often be gamed, for example, ROUGE can be improved by increased the output length of the model (Sun et al., 2019). Moreover, the reliability of these metrics depends on the quality and number of the references (Mathur et al., 2020a; Freitag et al., 2020). However, on a system-level, they still correlate well with human judgments for some tasks (Reiter, 2018). Automated Evaluation As mentioned above, GEM provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 10"
2021.gem-1.10,W11-2832,0,0.0915033,"Missing"
2021.gem-1.10,2020.inlg-1.24,1,0.864971,"human evaluation standards. In recent work, Howcroft et al. (2020) investigated NLG papers from the last 2 For a more complete description of recent developments in NLG evaluation, we refer to the survey by Celikyilmaz et al. (2020). 99 twenty years and the evaluation methodologies differ drastically across papers. Moreover, in most cases, it is not even mentioned what the human evaluation aims to measure and that definitions of measures like “accuracy” or “fluency” are inconsistent. They thus suggest reporting standards for criteria and methods, following a classification system proposed by Belz et al. (2020). In addition, regularly scheduled shared tasks like WMT have lead to standardization of human evaluation setups and enabled controlled experimentation with them. GEM has the opportunity to develop reproducible standards for how human evaluation for NLG tasks beyond translation should be conducted while at the same time incorporating lessons from related work. Acting on the same need, the recently proposed GENIE (Khashabi et al., 2021) system aims to automate and standardize the human evaluation of different NLG systems, however with the contrasting goal of reducing the evaluating to a leaderb"
2021.gem-1.10,Q18-1041,0,0.236179,"han half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generation, Evaluation, and Metrics) that aims to enable research on a wide range of NLG challenges. To avoid the fallacy of encouraging hill climbing on a leaderboard ("
2021.gem-1.10,W17-4755,0,0.0383891,"Missing"
2021.gem-1.10,W16-2302,0,0.0489314,"Missing"
2021.gem-1.10,N18-2097,0,0.0498492,"Missing"
2021.gem-1.10,N19-1423,0,0.0210653,"provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 104 Semantic Equivalence. More recently, metrics that rely on pretrained language models have shown improved correlations with human judgments on the segment-level. We thus include BERTScore (Zhang et al., 2020b), a metric based on the similarity of sentence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation appr"
2021.gem-1.10,P19-1483,1,0.811795,"tions. 106 Figure 2: A screenshot of the interactive result exploration tool. [Top Left] The selection of tasks, task-groups, or individual submissions. [Top Right] The selection of metric-groups or metrics [Bottom] The parallel coordinates visualization of the selection. The selection here can be filtered by brushing over a section of an individual metric, as is shown here for BLEURT. Hovering over a line presents detailed information of the particular submission. grained and interpretable evaluation metrics, for example to measure consistency in data-to-text problems (Opitz and Frank, 2020; Dhingra et al., 2019). We are using one such metric called NUBIA (Kane et al., 2020), the NeUral Based Interchangeability Assessor, which combines multiple measures such as entailment and similarity into a decomposable and interpretable score. Diversity. As argued by Hashimoto et al. (2019) among many others, NLG models intrinsically trade off diversity and quality. A model can produce more diverse outputs through sampling but at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek"
2021.gem-1.10,K19-1037,0,0.027404,"Missing"
2021.gem-1.10,P17-1123,0,0.0287395,"Missing"
2021.gem-1.10,2020.acl-main.454,1,0.84177,"LEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen"
2021.gem-1.10,W19-8652,1,0.897973,"Missing"
2021.gem-1.10,C16-1191,0,0.0653297,"Missing"
2021.gem-1.10,P18-1082,0,0.0705277,"Missing"
2021.gem-1.10,W16-3622,1,0.897392,"Missing"
2021.gem-1.10,P16-2008,1,0.869379,"Missing"
2021.gem-1.10,W19-8670,1,0.884454,"Missing"
2021.gem-1.10,2020.emnlp-main.393,0,0.179031,"le to be able to address newly found limitations, and that the benchmark should focus on climbing a leaderboard. Instead, a living benchmark that can adjust its datasets and specific evaluation metrics can be much more powerful and long-lived. This can, for example, be seen in Dynabench,1 (Potts et al., 2020) which has a static evaluation, but interactively adds more test However, they also pose a risk that progress is reduced to the single number shown in a benchmark’s leaderboard and thus may encourage blindly optimizing it without regard to other considerations like model size or fairness (Ethayarajh and Jurafsky, 2020). This is especially challenging for benchmarks in NLG since, as discussed above, the performance cannot be described through a single metric and it is often not clear what metric to optimize for. This shortfall can be seen in benchmarks like DecaNLP (McCann et al., 2018) and GLGE (Liu et al., 2020a) which include NLG tasks but focus only on a single metric and, as a result, may mischaracterize a system’s performance. Moreover, an easy-to-use data infrastructure also disincentivizes researchers from interacting with 1 98 https://dynabench.org/ data through a human-in-the-loop approach. able mu"
2021.gem-1.10,N19-1395,0,0.0587235,"Missing"
2021.gem-1.10,2020.emnlp-main.751,0,0.503468,"nly on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover,"
2021.gem-1.10,P19-1346,1,0.897232,"Missing"
2021.gem-1.10,2020.webnlg-1.7,1,0.843551,"Missing"
2021.gem-1.10,2020.findings-emnlp.195,1,0.835947,"Missing"
2021.gem-1.10,2020.emnlp-main.5,0,0.126308,"e machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differently across tasks, setups, and languages, a multi-task NLG benchmark has the opportunity to act as a testbed to evaluate how the latest advances in automated metrics perform on these different tasks. The benchmark can facilitate this research through the release of system outputs and associated human annotations, which is what we are planning to do with GEM. Moreover, we allow the integrat"
2021.gem-1.10,2020.emnlp-main.489,0,0.0422168,"les 3 and 4. Our interactive system is centered around a parallel coordinates plot (Inselberg, 1985) which shows all results as lines through parallel axes. Every line intersects the axes at the corresponding mapped value. For instance, see the red line representing the results for task “ToTTo” of baseline “t5-small”. Filters can be applied along axes (see BLEURT axis in Figure 2) and the filtered selection is highlighted through bold lines. A selection can be a set of metrics, systems, or tasks. This style of presentation has not been used before for a benchmark. The closest prior work is by Fu et al. (2020) for namedentity recognition which allows similar filtering and sorting, but presents the results in a table. However, the parallel coordinates approach can scale to a much greater number of metrics than a table. Moreover, by using a parallel coordinates plot instead of a table, it is easy to spot patterns that span multiple metrics, systems, or tasks. For example, the highlighted line in Figure 2 uncovers that, for the T5 baseline on ToTTo, the diversity metrics score higher than other systems while scoring lower on reference-based metrics. Since we only have a single baseline for ToTTo, it i"
2021.gem-1.10,W17-3518,1,0.863605,"Missing"
2021.gem-1.10,N19-1169,1,0.923485,"eiter and Dale, 2000). These texts aim to fulfill an underlying communicative goal (e.g., to produce a summary of an article) while remaining faithful to the input information, fluent, grammatical, and natural-looking. An NLG system needs to be robust to shifts in the data distribution and be able to produce text in many different languages. Finally, it is often desired that repeated interactions with the model produce diverse outputs, for example, to explain concepts in multiple ways or to become a more interesting conversational agent. These optimization objectives can often be conflicting (Hashimoto et al., 2019) and, as a result, evaluations that focus only on a single aspect may fail to recognize the drawbacks of a particular method. To demonstrate this trade-off, consider an improvement on the CNN-DM summarization dataset (Hermann et al., 2015; Nallapati et al., 2016) measured by the ROUGE-L metWe introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent"
2021.gem-1.10,2020.ngt-1.1,0,0.0253511,"first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution"
2021.gem-1.10,2020.inlg-1.23,1,0.841029,"Missing"
2021.gem-1.10,D15-1229,0,0.0462438,"Missing"
2021.gem-1.10,2020.acl-main.709,1,0.887061,"Missing"
2021.gem-1.10,2020.acl-main.560,0,0.077305,"on does not consider differences between the languages that lead to higher modeling complexity, for example, a richer morphology or a flexible word-order. Still, the majority of work in NLP and almost all benchmarks exclusively focus on English (e.g., Wang et al., 2019b; Liu et al., 2020a; McCann et al., 2018). Even if multiple languages are considered, the availability of data in a language often does not represent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and Wiki"
2021.gem-1.10,2020.evalnlgeval-1.4,0,0.0163557,"ation tool. [Top Left] The selection of tasks, task-groups, or individual submissions. [Top Right] The selection of metric-groups or metrics [Bottom] The parallel coordinates visualization of the selection. The selection here can be filtered by brushing over a section of an individual metric, as is shown here for BLEURT. Hovering over a line presents detailed information of the particular submission. grained and interpretable evaluation metrics, for example to measure consistency in data-to-text problems (Opitz and Frank, 2020; Dhingra et al., 2019). We are using one such metric called NUBIA (Kane et al., 2020), the NeUral Based Interchangeability Assessor, which combines multiple measures such as entailment and similarity into a decomposable and interpretable score. Diversity. As argued by Hashimoto et al. (2019) among many others, NLG models intrinsically trade off diversity and quality. A model can produce more diverse outputs through sampling but at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek et al., 2020) and by van Miltenburg et al. (2018). These inclu"
2021.gem-1.10,D18-1208,0,0.0650198,"Missing"
2021.gem-1.10,Q18-1023,0,0.0642232,"Missing"
2021.gem-1.10,2020.findings-emnlp.360,1,0.934785,"e a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmarks are critical in measuring modeling progress. and conducting in-depth ana"
2021.gem-1.10,D16-1128,0,0.0680679,"Missing"
2021.gem-1.10,2020.acl-main.703,0,0.214186,"esent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared"
2021.gem-1.10,2020.acl-main.653,0,0.233553,"esent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared"
2021.gem-1.10,N16-1014,0,0.0410993,"t at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek et al., 2020) and by van Miltenburg et al. (2018). These include the Shannon Entropy (Shannon and Weaver, 1963) over unigrams and bigrams (H1 , H2 ), the mean segmented type token ratio over segment lengths of 100 (MSTTR, Johnson, 1944), the ratio of distinct n-grams over the total number of n-grams (Distinct1,2 ), and the count of n-grams that only appear once across the entire test output (Unique1,2 , Li et al., 2016). focus of this section will be on qualitative descriptions through model cards, we also gather quantitative information that is not necessarily associated with a judgment. As part of this, we collect the number of parameters of a system, as suggested by Ethayarajh and Jurafsky (2020). For each task, we additionally report the vocabulary size over the output (|V|) and the mean output length of a system (Sun et al., 2019). 5 Results One of the central aims of GEM is to measure the progress in NLG without misrepresenting the complex interactions between the sometimes contradicting measures. We t"
2021.gem-1.10,2020.findings-emnlp.165,0,0.0883829,"Missing"
2021.gem-1.10,W04-1013,0,0.355472,"n be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate. * Correspondence to gehrmann@google.com 96 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets"
2021.gem-1.10,2020.acl-main.465,0,0.123121,"; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generation, Evaluation, and Metrics) that aims to enable research on a wide range of NLG challenges. To avoid the fallacy of encouraging hill climbing on a leaderboard (Linzen, 2020), GEM focuses on an in-depth evaluation of model outputs across human and automatic evaluation that aims to uncover shortcomings and opportunities for progress. As datasets, metrics, and models improve, the benchmark environment will improve as well, replacing “solved” tasks with more challenging ones, incorporating newly developed metrics, and addressing discovered flaws in the experimental setup, as demonstrated in Figure 1. Making all model outputs available under an open-source license will support evaluation research and integrating new metrics will, in turn, help their adoption and incre"
2021.gem-1.10,2020.tacl-1.47,0,0.129414,"ich has a static evaluation, but interactively adds more test However, they also pose a risk that progress is reduced to the single number shown in a benchmark’s leaderboard and thus may encourage blindly optimizing it without regard to other considerations like model size or fairness (Ethayarajh and Jurafsky, 2020). This is especially challenging for benchmarks in NLG since, as discussed above, the performance cannot be described through a single metric and it is often not clear what metric to optimize for. This shortfall can be seen in benchmarks like DecaNLP (McCann et al., 2018) and GLGE (Liu et al., 2020a) which include NLG tasks but focus only on a single metric and, as a result, may mischaracterize a system’s performance. Moreover, an easy-to-use data infrastructure also disincentivizes researchers from interacting with 1 98 https://dynabench.org/ data through a human-in-the-loop approach. able much richer evaluation (as described in the next sections), and promote non-English datasets. In addition, it can ensure that the datasets created for those shared tasks continue being evaluated. Increasing multilingualism of NLG research. Another potentially harmful choice by benchmark creators is t"
2021.gem-1.10,2021.ccl-1.108,0,0.0502233,"Missing"
2021.gem-1.10,W15-4640,0,0.0777152,"Missing"
2021.gem-1.10,W18-6450,0,0.0170006,"ever, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automa"
2021.gem-1.10,W19-5302,0,0.0535855,"Missing"
2021.gem-1.10,2020.coling-main.420,0,0.022745,"s many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicting results since studies often omit crucial replication details and assume different definitions of the measured quantities (Howcroft et al., 2020). Improving Data Improving Metrics Evaluation with gameable metrics Improving Models Consistent Human Eval Non-repeatable human evaluation Figure 1: The opportunities of living benchmarks and pitfalls of evaluation. As models improve, we need consistent evaluations such that models can be compared to each other. This can only happen if we develop robust human evaluation standards and improve our automated metrics. Ot"
2021.gem-1.10,2020.acl-main.448,0,0.212993,"ere is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differe"
2021.gem-1.10,2020.wmt-1.77,0,0.124839,"ere is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differe"
2021.gem-1.10,2020.acl-main.173,1,0.838186,"to which we invite the entire NLG community to participate. * Correspondence to gehrmann@google.com 96 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicti"
2021.gem-1.10,W18-3601,1,0.834682,"where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way"
2021.gem-1.10,2020.msr-1.1,1,0.821875,"Missing"
2021.gem-1.10,C18-1147,1,0.871186,"Missing"
2021.gem-1.10,2020.emnlp-main.466,0,0.0504036,"Missing"
2021.gem-1.10,D15-1238,0,0.013053,"d this goal, GEM welcomes anyone interested in collaborating on this effort. 7.2 Personalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the results presented on our website to incorporate them. For the updates to the dataset selection, we want to consider the in"
2021.gem-1.10,K16-1028,0,0.065799,"Missing"
2021.gem-1.10,D18-1206,1,0.894493,"Missing"
2021.gem-1.10,W17-5525,1,0.880333,"Missing"
2021.gem-1.10,P02-1040,0,0.114424,"n NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achi"
2021.gem-1.10,2020.emnlp-main.89,1,0.895725,"Missing"
2021.gem-1.10,W17-3537,1,0.910798,"trics saturate, we need to evaluate them on more challenging datasets instead of continuing to move sideways on old ones. GEM aims to provide this environment for natural language generation. than half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmar"
2021.gem-1.10,2020.emnlp-main.185,0,0.0484252,"Missing"
2021.gem-1.10,2021.acl-long.186,0,0.0851423,"Missing"
2021.gem-1.10,P19-1195,0,0.0585453,"Missing"
2021.gem-1.10,N18-1012,0,0.0173282,"akers of low-resourced languages through a participatory research approach, as suggested by (∀ et al., 2020). Toward this goal, GEM welcomes anyone interested in collaborating on this effort. 7.2 Personalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the result"
2021.gem-1.10,Q19-1016,0,0.0583227,"Missing"
2021.gem-1.10,J18-3002,0,0.0872491,"for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated"
2021.gem-1.10,2020.acl-main.442,0,0.137138,"hem on more challenging datasets instead of continuing to move sideways on old ones. GEM aims to provide this environment for natural language generation. than half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generatio"
2021.gem-1.10,2020.emnlp-main.647,0,0.0370713,"Missing"
2021.gem-1.10,2021.emnlp-main.529,0,0.024648,"on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen BART T5 0.301 0.291 63.5 64.0 32.5 29.4 55.1 54.5 27.5 26.4 0.943 0.942 -0.400 -0.412 mT5-small mT5-base mT5-large mT5-XL TGen TGen+ TGen++ 0.229 0.23 0.233 0.229 0.152 0.151 0.167 47.3 48.1 51.3 52.1 13.6 13.8 9.7 28.6 28.8 30.0 31.3 0.0 0.0 0.0 43.0 44.2 46.4 47.3 13.6 13.8 9.7 17.9 17.1 17.5 17.0 0.03 0.03 0.03 0.895 0.898 0.902 0.905 0.6"
2021.gem-1.10,D19-1320,0,0.0210218,"ence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTSc"
2021.gem-1.10,2020.acl-main.704,1,0.824209,"er, on a system-level, they still correlate well with human judgments for some tasks (Reiter, 2018). Automated Evaluation As mentioned above, GEM provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 104 Semantic Equivalence. More recently, metrics that rely on pretrained language models have shown improved correlations with human judgments on the segment-level. We thus include BERTScore (Zhang et al., 2020b), a metric based on the similarity of sentence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang e"
2021.gem-1.10,P19-1212,0,0.0526263,"Missing"
2021.gem-1.10,P19-1646,0,0.0614703,"Missing"
2021.gem-1.10,Q16-1029,1,0.821779,"in a news article en *25k Articles WebNLG (Gardent et al., 2017) Produce a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmar"
2021.gem-1.10,W15-3031,0,0.06329,"Missing"
2021.gem-1.10,W19-2303,0,0.167787,"of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicting results since studies often omit crucial replication details and assume different definitions of the measured"
2021.gem-1.10,2020.nlp4convai-1.13,0,0.0861626,"Missing"
2021.gem-1.10,D16-1033,0,0.064867,"Missing"
2021.gem-1.10,2020.acl-main.450,0,0.015505,"2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen BART T5 0.301 0.291"
2021.gem-1.10,P18-1205,0,0.0245839,"nalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the results presented on our website to incorporate them. For the updates to the dataset selection, we want to consider the input of the wider NLG research community. To do so, we will set up a yearly selec"
D08-1017,P99-1070,0,0.0394765,"Missing"
D08-1017,W06-2920,0,0.089328,"l the arcs using a log-linear classifier with access to the full unlabeled parse (McDonald et al., 2005a; 162 McDonald et al., 2005b; McDonald and Pereira, 2006). In stacking experiments, the arc labels from the level 0 parser are also used as a feature.4 In the following subsections, we refer to our modification of the MSTParser as MST 1O (the arcfactored version) and MST 2O (the second-order arc-pair-factored version). All our experiments use the non-projective version of this parser. We refer to the MaltParser as Malt. We report experiments on twelve languages from the CoNLL-X shared task (Buchholz and Marsi, 2006).5 All experiments are evaluated using the labeled attachment score (LAS), using the default settings.6 Statistical significance is measured using Dan Bikel’s randomized parsing evaluation comparator with 10,000 iterations.7 The additional features used in the level 1 parser are enumerated in Table 1 and their various subsets are depicted in Table 2. The PredEdge features are exactly the six features used by Nivre and McDonald (2008) in their MSTMalt parser; therefore, feature set A is a replication of this parser except for modifications noted in footnote 4. In all our experiments, the number"
D08-1017,P04-1054,0,0.0321457,"Missing"
D08-1017,P05-1067,0,0.0603954,"Missing"
D08-1017,P99-1059,0,0.362595,"art dependency parsers. 1 • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of a"
D08-1017,C96-1058,0,0.903966,"of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Pr"
D08-1017,P06-2041,0,0.494122,"tures for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence A solution"
D08-1017,D07-1013,0,0.0628025,"of bias (Lafferty et al., 2001). Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall"
D08-1017,E06-1011,0,0.769242,"and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, c Honolulu, October 2008. 2008 Association for Computational Linguistics le"
D08-1017,W07-2216,0,0.226249,"nts to solving arg maxy∈Y(x) w> f (x, y), where w is a weight vector. With a projectivity constraint and arc factorization, the parsing problem can be solved in cubic time by dynamic programming (Eisner, 1996), and with a weaker “tree” constraint (permitting nonprojective parses) and arc factorization, a quadratic-time algorithm exists (Chu and Liu, 1965; Edmonds, 1967), as shown by McDonald et al. (2005b). In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the nonprojective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. McDonald and Pereira (2006) adopted an approximation based on O(n3 ) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. In §3 we adopt a framework that maintains O(n2 ) runtime (still exploiting the Chu-Liu-Edmonds algorithm) while approximating non arc-factored features. 2.2 Stacked Learning Stacked generalization was first proposed by Wolpert (1992) and Breiman (1996) for regression. The idea is to include two “levels” of predictors. The first level, “level 0,” includes one or more pred"
D08-1017,P05-1012,0,0.565159,"c P. Xing∗ ∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA † Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, Lisboa, Portugal {afm,dipanjan,nasmith,epxing}@cs.cmu.edu Abstract assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). We explore a stacked framework for learning to predict dependency structures for natural language sentences. A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b). Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich p"
D08-1017,H05-1066,0,0.389842,"c P. Xing∗ ∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA † Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, Lisboa, Portugal {afm,dipanjan,nasmith,epxing}@cs.cmu.edu Abstract assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). We explore a stacked framework for learning to predict dependency structures for natural language sentences. A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b). Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich p"
D08-1017,W06-2932,0,0.0406989,"odifications noted in footnote 4. In all our experiments, the number of ˜ is L = 2. partitions used to create D 5.2 Experiment: MST 2O + MST 2O Our first experiment stacks the highly accurate MST 2O parser with itself. At level 0, the parser uses only the standard features (§5.1), and at level 1, these are augmented by various subsets of features of x along with the output of the level 0 parser, g(x) (Table 2). The results are shown in Table 3. While we see improvements over the single-parser baseline 4 We made other modifications to MSTParser, implementing many of the successes described by (McDonald et al., 2006). Our version of the code is publicly available at http: //www.ark.cs.cmu.edu/MSTParserStacked. The modifications included an approximation to lemmas for datasets without lemmas (three-character prefixes), and replacing morphology/word and morphology/lemma features with morphology/POS features. 5 The CoNLL-X shared task actually involves thirteen languages; our experiments do not include Czech (the largest dataset), due to time constraints. Therefore, the average results plotted in the last rows of Tables 3, 4, and 5 are not directly comparable with previously published averages over thirteen"
D08-1017,P08-1108,0,0.189783,"ience, Carnegie Mellon University, Pittsburgh, PA 15213, USA † Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, Lisboa, Portugal {afm,dipanjan,nasmith,epxing}@cs.cmu.edu Abstract assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). We explore a stacked framework for learning to predict dependency structures for natural language sentences. A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b). Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich parsers must resort to search or greedi"
D08-1017,W04-2407,0,0.0834669,"Missing"
D08-1017,W06-2933,0,0.0492279,"Missing"
D08-1017,W06-1616,0,0.178317,"reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, c Honolulu, October 2008. 2008 Association for Computational Linguistics learning via the EM algorithm – none of which have 2.1 Dependency Parsing previous"
D08-1017,W05-1513,0,0.192799,"y parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable inde"
D08-1017,D08-1016,0,0.103348,"non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, c Honolulu, October 2008. 2008 Association for Computational Linguistics learning via the EM algorithm – none of which have 2.1 Dependency Parsing previously been to have exact learning via the known EM algorithm – nonenon-projective of which have implementations. previously been known to have exact non-projective Dependency syntax is a lightweight syntactic repWe then switch to models that account for where implementa"
D08-1017,W07-2218,0,0.0295174,"on scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, c Honolulu, October 2008. 2008 Association for Computational Linguistics learning via the EM algorithm – none of which have 2.1 Dependency Parsing previously been to have exact learning via the known EM algorith"
D08-1017,D07-1003,1,0.392434,"Missing"
D08-1017,W03-3023,0,0.499777,"Missing"
D11-1005,P10-1131,0,0.76042,"sed parameter estimation. Some previous multilingual research, such as Bayesian parameter tying across languages (Cohen and Smith, 2009) or models of parameter 1 Although the stated objective is often to build systems for resource-poor languages and domains, for evaluation purposes, annotated treebank test data figure prominently in this research (including in this paper). Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50–61, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics drift down phylogenetic trees (Berg-Kirkpatrick and Klein, 2010) is comparable, but the practical assumption of supervised helper languages is new to this work. Naseem et al. (2010) used universal syntactic categories and rules to improve grammar induction, but their model required expert handwritten rules as constraints. Herein, we specifically focus on two problems in linguistic structure prediction: unsupervised POS tagging and unsupervised dependency grammar induction. Our experiments demonstrate that the presented method outperforms strong state-of-the-art unsupervised baselines for both tasks. Our approach can be applied to other problems in which a"
D11-1005,N10-1083,0,0.0976671,"Missing"
D11-1005,W06-2920,0,0.0501669,"l during unsupervised learning, and are initialized using standard methods. 6 Experiments and Results In this section, we describe the experiments undertaken and the results achieved. We first note the characteristics of the datasets and the universal POS tags used in multilingual modeling. 6.1 Data For our experiments, we fixed a set of four helper languages with relatively large amounts of data, displaying nontrivial linguistic diversity: Czech (Slavic), English (West-Germanic), German (WestGermanic), and Italian (Romance). The datasets are the CoNLL-X shared task data for Czech and German (Buchholz and Marsi, 2006),5 the Penn Treebank for English (Marcus et al., 1993), and the CoNLL 2007 shared task data for Italian (Montemagni et al., 2003). This was the only set of helper languages we tested; improvements are likely possible. We leave an exploration of helper language choice (a subset selection problem) to future research, instead demonstrating that the concept has merit. We considered ten target languages: Bulgarian (Bg), Danish (Da), Dutch (Nl), Greek (El), Japanese (Jp), Portuguese (Pt), Slovene (Sl), Spanish (Es), Swedish (Sv), and Turkish (Tr). The data come from the CoNLL-X and CoNLL 2007 shared"
D11-1005,D08-1092,0,0.0368004,"such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to lear"
D11-1005,D07-1022,1,0.760276,"escribed in §6.4. All tag induction uses a dictionary as specified in §6.3. The last row in this table indicates the best results using multilingual guidance taken from our methods in Table 3. “Avg” denotes macro-average across the ten languages. 2. Apply the fine-grained tagger to the words in the training data for the dependency parser. We consider two variants: the most probable assignment of tags to words (denoted “Pipeline”), and the posterior distribution over tags for each word, represented as a weighted “sausage” lattice (denoted “Joint”). This idea was explored for joint inference by Cohen and Smith (2007). 3. We apply the Mixture+EM unsupervised parser learning method from §6.4 to the automatically tagged sentences, or the lattices. 4. Given the two models, we infer POS tags on the test data using DG or Mixture+DG to get a lattice (Joint) or a sequence (Pipeline) and then parse using the model from the previous step.10 The resulting dependency trees are evaluated against the gold standard. Results are reported in Table 4. In almost all cases, joint decoding of tags and trees performs better than the pipeline. Even though our part-of-speech tagger with multilingual guidance outperforms the comp"
D11-1005,N09-1009,1,0.852683,"begin with supervised maximum likelihood estimates for models of the helper languages. In the second stage, we learn a model for the target language using unannotated data, maximizing likelihood over interpolations of the helper language models’ distributions. The tying is performed at the parameter level, through coarse, nearly-universal syntactic categories (POS tags). The resulting model is then used to initialize learning of the target language’s model using standard unsupervised parameter estimation. Some previous multilingual research, such as Bayesian parameter tying across languages (Cohen and Smith, 2009) or models of parameter 1 Although the stated objective is often to build systems for resource-poor languages and domains, for evaluation purposes, annotated treebank test data figure prominently in this research (including in this paper). Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50–61, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics drift down phylogenetic trees (Berg-Kirkpatrick and Klein, 2010) is comparable, but the practical assumption of supervised helper languages is new to this work. Naseem"
D11-1005,P11-1061,1,0.909688,"and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another language that lacks annot"
D11-1005,A94-1009,0,0.0200607,"most frequent parts of speech and exist in one form or another in all of the languages that we studied. For each language in our data, a mapping from the fine-grained treebank POS tags to these universal POS tags was constructed manually by Petrov et al. (2011). 6.3 θy→y0 = Part-of-Speech Tagging 6.3.1 L X (`) β`,y θy→y0 (11) `=1 Our first experimental task is POS tagging, and here we describe the specific details of the model, training and inference and the results attained. Model The model is a hidden Markov model (HMM), which has been popular for unsupervised tagging tasks (Merialdo, 1994; Elworthy, 1994; Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010).6 We use a bigram model and a locally normalized loglinear parameterization, like Berg-Kirkpatrick et al. (2010). These locally normalized log-linear models can look at various aspects of the observation x given a tag y, or the pair of tags in a transition, incorporating overlapping features. In basic monolin6 gual experiments, we used the same set of features as Berg-Kirkpatrick et al. (2010). For the transition log-linear model, Berg-Kirkpatrick et al. (2010) used only a single indicator feature of a tag pair, essentially equating to a"
D11-1005,P10-2036,0,0.0191573,"Missing"
D11-1005,N06-1041,0,0.0979683,"described in §5. We call this model “Uniform+DG.” The second version estimates the mixture coefficients to maximize likelihood, then expands the POS tags (§5), using the result to initialize training of the final model. We call this model “Mixture+DG.” No Tag Dictionary For each of the above configurations, we ran purely unsupervised training without a tag dictionary, and evaluated using one-to-one mapping accuracy constraining at most one HMM state to map to a unique treebank tag in the test data, using maximum bipartite matching. This is a variant of the greedy one-to-one mapping scheme of Haghighi and Klein (2006).8 With a Tag Dictionary We also ran a second version of each experimental configuration, where we used a tag dictionary to restrict the possible path sequences of the HMM during both learning and inference. This tag dictionary was constructed only from the training section of a given language’s treebank. It is widely known that such knowledge improves the quality of the model, though it is an open debate whether such knowledge is realistic to assume. For this experiment we removed punctuation from the training and test data, enabling direct use within the dependency grammar induction experime"
D11-1005,N09-1012,0,0.0439172,"dency grammar induction. For Turkish, Japanese, Slovene and Dutch, our unsupervised learner from words outperforms unsupervised parsing using gold-standard part-of-speech tags. We note that some recent work gives a treatment to unsupervised parsing (but not of dependencies) 10 The decoding method on test data (Joint or Pipeline) was matched to the training method, though they are orthogonal in principle. 59 directly from words (Seginer, 2007). Earlier work that induced part-of-speech tags and then performed unsupervised parsing in a pipeline includes Klein and Manning (2004) and Smith (2006). Headden et al. (2009) described the use of a lexicalized variant of the DMV model, with the use of gold part-ofspeech tags. 7 Conclusion We presented an approach to exploiting annotated data in helper languages to infer part-of-speech tagging and dependency parsing models in a different, target language, without parallel data. Our approach performs well in many cases. We also described a way to do joint decoding of part-of-speech tags and dependencies which performs better than a pipeline. Future work might consider exploiting a larger number of treebanks, and more powerful techniques for combining models than sim"
D11-1005,N07-1018,0,0.0117058,"d data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing. 1 Introduction A major focus of recent NLP research has involved unsupervised learning of structure such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al.,"
D11-1005,P04-1061,0,0.550547,"available, using annotated data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing. 1 Introduction A major focus of recent NLP research has involved unsupervised learning of structure such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2"
D11-1005,J93-2004,0,0.0374901,"tandard methods. 6 Experiments and Results In this section, we describe the experiments undertaken and the results achieved. We first note the characteristics of the datasets and the universal POS tags used in multilingual modeling. 6.1 Data For our experiments, we fixed a set of four helper languages with relatively large amounts of data, displaying nontrivial linguistic diversity: Czech (Slavic), English (West-Germanic), German (WestGermanic), and Italian (Romance). The datasets are the CoNLL-X shared task data for Czech and German (Buchholz and Marsi, 2006),5 the Penn Treebank for English (Marcus et al., 1993), and the CoNLL 2007 shared task data for Italian (Montemagni et al., 2003). This was the only set of helper languages we tested; improvements are likely possible. We leave an exploration of helper language choice (a subset selection problem) to future research, instead demonstrating that the concept has merit. We considered ten target languages: Bulgarian (Bg), Danish (Da), Dutch (Nl), Greek (El), Japanese (Jp), Portuguese (Pt), Slovene (Sl), Spanish (Es), Swedish (Sv), and Turkish (Tr). The data come from the CoNLL-X and CoNLL 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007)."
D11-1005,D11-1006,0,0.610872,"nson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another language that lacks annotated data (the target la"
D11-1005,J94-2001,0,0.0620279,"bset) cover the most frequent parts of speech and exist in one form or another in all of the languages that we studied. For each language in our data, a mapping from the fine-grained treebank POS tags to these universal POS tags was constructed manually by Petrov et al. (2011). 6.3 θy→y0 = Part-of-Speech Tagging 6.3.1 L X (`) β`,y θy→y0 (11) `=1 Our first experimental task is POS tagging, and here we describe the specific details of the model, training and inference and the results attained. Model The model is a hidden Markov model (HMM), which has been popular for unsupervised tagging tasks (Merialdo, 1994; Elworthy, 1994; Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010).6 We use a bigram model and a locally normalized loglinear parameterization, like Berg-Kirkpatrick et al. (2010). These locally normalized log-linear models can look at various aspects of the observation x given a tag y, or the pair of tags in a transition, incorporating overlapping features. In basic monolin6 gual experiments, we used the same set of features as Berg-Kirkpatrick et al. (2010). For the transition log-linear model, Berg-Kirkpatrick et al. (2010) used only a single indicator feature of a tag pair, essential"
D11-1005,D10-1120,0,0.401513,"2009) or models of parameter 1 Although the stated objective is often to build systems for resource-poor languages and domains, for evaluation purposes, annotated treebank test data figure prominently in this research (including in this paper). Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50–61, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics drift down phylogenetic trees (Berg-Kirkpatrick and Klein, 2010) is comparable, but the practical assumption of supervised helper languages is new to this work. Naseem et al. (2010) used universal syntactic categories and rules to improve grammar induction, but their model required expert handwritten rules as constraints. Herein, we specifically focus on two problems in linguistic structure prediction: unsupervised POS tagging and unsupervised dependency grammar induction. Our experiments demonstrate that the presented method outperforms strong state-of-the-art unsupervised baselines for both tasks. Our approach can be applied to other problems in which a subset of the model parameters can be linked across languages. We also experiment with unsupervised learning of depen"
D11-1005,P07-1049,0,0.0401133,"tilingual guidance outperforms the completely unsupervised baseline, there is not always an advantage of using this multilingually guided partof-speech tagger for dependency grammar induction. For Turkish, Japanese, Slovene and Dutch, our unsupervised learner from words outperforms unsupervised parsing using gold-standard part-of-speech tags. We note that some recent work gives a treatment to unsupervised parsing (but not of dependencies) 10 The decoding method on test data (Joint or Pipeline) was matched to the training method, though they are orthogonal in principle. 59 directly from words (Seginer, 2007). Earlier work that induced part-of-speech tags and then performed unsupervised parsing in a pipeline includes Klein and Manning (2004) and Smith (2006). Headden et al. (2009) described the use of a lexicalized variant of the DMV model, with the use of gold part-ofspeech tags. 7 Conclusion We presented an approach to exploiting annotated data in helper languages to infer part-of-speech tagging and dependency parsing models in a different, target language, without parallel data. Our approach performs well in many cases. We also described a way to do joint decoding of part-of-speech tags and dep"
D11-1005,P05-1044,1,0.488925,"rts of speech and exist in one form or another in all of the languages that we studied. For each language in our data, a mapping from the fine-grained treebank POS tags to these universal POS tags was constructed manually by Petrov et al. (2011). 6.3 θy→y0 = Part-of-Speech Tagging 6.3.1 L X (`) β`,y θy→y0 (11) `=1 Our first experimental task is POS tagging, and here we describe the specific details of the model, training and inference and the results attained. Model The model is a hidden Markov model (HMM), which has been popular for unsupervised tagging tasks (Merialdo, 1994; Elworthy, 1994; Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010).6 We use a bigram model and a locally normalized loglinear parameterization, like Berg-Kirkpatrick et al. (2010). These locally normalized log-linear models can look at various aspects of the observation x given a tag y, or the pair of tags in a transition, incorporating overlapping features. In basic monolin6 gual experiments, we used the same set of features as Berg-Kirkpatrick et al. (2010). For the transition log-linear model, Berg-Kirkpatrick et al. (2010) used only a single indicator feature of a tag pair, essentially equating to a traditional multinomial"
D11-1005,D09-1086,0,0.10063,"and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another lan"
D11-1005,W04-3207,1,0.820928,"t al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another language that lacks annotated data (the target language). Unlike the previous work men"
D11-1005,P08-1084,0,0.0323582,"ts purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another language that lacks annotated data (the target language). Unlike the previous work mentioned above, our framework does not rely on parallel da"
D11-1005,P09-1009,0,0.126357,"Missing"
D11-1005,N10-1116,0,0.0267179,"fficients are learned automatically fares better than uniform weighting. With a tag dictionary, the multilingual variants outperform the baseline in seven out of ten cases, and the learned mixture outperforms or matches the uniform mixture in five of those seven (Table 2b). 6.4 Dependency Grammar Induction We next describe experiments for dependency grammar induction. As the basic grammatical model, we adopt the dependency model with valence (Klein and Manning, 2004), which forms the basis for stateof-the-art results for dependency grammar induction in various settings (Cohen and Smith, 2009; Spitkovsky et al., 2010; Gillenwater et al., 2010; Berg-Kirkpatrick and Klein, 2010). As shown in Table 3, DMV obtains much higher accuracy in the supervised setting than the unsupervised setting, suggesting that more can be achieved with this model family.9 For this reason, and because DMV is easily interpreted as a PCFG, it is our starting point and baseline. We consider four conditions. The independent variables are (1) whether we use uniform β (all set to 1 4 ) or estimate them using EM (as described in §4), and (2) whether we simply use the mixture model to decode the test data, or to initialize EM for the DMV."
D11-1005,H05-1107,0,0.0956265,"s involved unsupervised learning of structure such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data f"
D11-1005,N01-1026,0,0.0625168,"of recent NLP research has involved unsupervised learning of structure such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to usi"
D11-1005,D07-1096,0,\N,Missing
D13-1205,J92-4003,0,0.352783,"Missing"
D13-1205,H93-1039,0,0.199147,"Missing"
D13-1205,W06-2920,0,0.00623517,"English phrase. Word alignments are shown as links between English and Spanish words. word esponjas because this tag is in our dictionary for the latter word. For all our POS experiments, we evaluate on seventeen target languages. Fifteen of these languages were part of the experiments conducted by T¨ackstr¨om et al. (2013); we add Arabic and Hungarian to the set. The first column of Table 1 lists all seventeen languages using their two-letter abbreviation codes from the ISO 639-1 standard. The evaluation datasets correspond to the test sets from the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). For French we use the treebank of Abeill´e et al. (2003). English serves as our source language and we use the Penn Treebank (Marcus et al., 1993, with tags mapped to the universal tags) to train our supervised source-side model. 4.2 Named-Entity Segmentation Second, we investigate the task of named-entity segmentation. The goal of this task is to identify the boundaries of named-entities for a given language without classifying them by type. This is the unlabeled version of named-entity recognition, and is more amenable to cross-lingual supervision. To understand why th"
D13-1205,D10-1056,0,0.00607293,"his was popularized by Yarowsky and Ngai (2001) who applied this to POS tagging and shallow parsing. It was later applied to parsing (Hwa et al., 2005) and named entity recognition (Kim et al., 2012). The second idea, first proposed by Zeman and Resnik (2008) and applied more broadly by McDonald et al. (2011), is to train a model on a resource-rich language and apply it to a resourcepoor language directly. The disparity between the languages is mitigated by the choice of features. In addition to cross-lingual projection, purely unsupervised methods have been explored but with limited success (Christodoulopoulos et al., 2010). Here, we resort to cross-lingual projection and incorporate the first idea; we also follow Li et al. (2012) and use Wiktionary to further constrain the POS tagging task. Our learning setup is similar to that of Ganchev et al. (2009), who also use posterior regularization but focus on dependency parsing alone. Our work differs with respect to the tasks, the learning algorithm and also in that we use corpus-wide constraints, while Ganchev et al. use one constraint per sentence. For the part-of-speech tagging task, our approach is similar to that of T¨ackstr¨om et al. (2013), who use an almost"
D13-1205,P11-1061,1,0.628583,"been applied for several natural language processing problems, including partof-speech (POS) tagging (Yarowsky and Ngai, 2001), syntactic parsing (Hwa et al., 2005) and named-entity recognition (Kim et al., 2012). These methods are useful in several ways. First, they help in fast prototyping of natural language systems for new languages that do not boast human annotations. Second, the output of such systems could be used to bootstrap more extensive human annotation projects (Vlachos, 2006). Finally, they are significantly more accurate than purely unsupervised systems (McDonald et al., 2011; Das and Petrov, 2011). Recently, T¨ackstr¨om et al. (2013) presented a technique for coupling token constraints derived from projected cross-lingual information and type constraints derived from noisy tag dictionaries to learn POS taggers. Although this technique resulted in state-ofthe-art weakly supervised taggers, the authors used a heuristic to combine the aforementioned two sources of constraints: the dictionary constraints pruned the tagger’s search space, and the intersected token-level projections were treated as hard observations. On the other hand, Ganchev et al. (2009) presented a framework for learning"
D13-1205,P11-1043,0,0.0371084,"e Meulder, 2003). We project the spans using the maximal-span heuristic (Yarowsky and Ngai, 2001). We project into Dutch, German and Spanish and evaluate on the standard CoNLL 2002 and 2003 shared task data sets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). 4.3 Parallel Data For both tasks we use parallel data gathered automatically from the web using the method of Uszkoreit et al. (2010), as well as data from Europarl (Koehn, 2005) and the UN parallel corpus (UN, 2006), for languages covered by the latter two corpora. The parallel sentences are word aligned with the aligner of DeNero and Macherey (2011). The size of the parallel corpus is larger than we need for our tasks, so we follow T¨ackstr¨om et al. (2013) in sampling 500k tokens for POS tagging and 10k sentences for named-entity segmentation (see §5.1.2 and §5.2.2). 5 Experimental Details In this section, we provide details about taskspecific implementations of the supervised sourceside model and the word-alignment filtering techniques (steps 2 and 3 in Algorithm 1 respectively); we also briefly describe the setup of the cross-lingual experiments for each task. 5.1 Part-of-Speech Tagging We first focus on the experimental setup for the"
D13-1205,P09-1042,1,0.306666,"ed systems (McDonald et al., 2011; Das and Petrov, 2011). Recently, T¨ackstr¨om et al. (2013) presented a technique for coupling token constraints derived from projected cross-lingual information and type constraints derived from noisy tag dictionaries to learn POS taggers. Although this technique resulted in state-ofthe-art weakly supervised taggers, the authors used a heuristic to combine the aforementioned two sources of constraints: the dictionary constraints pruned the tagger’s search space, and the intersected token-level projections were treated as hard observations. On the other hand, Ganchev et al. (2009) presented a framework for learning weakly-supervised systems (in their case, dependency parsers) that incorporated alignment-based information too, but used the crosslingual information only as soft constraints, via posterior regularization. The advantage of this framework lay in the fact that the projections were only trusted to a certain degree, determined by a strength hyperparameter, which unfortunately the authors did not have an elegant way to tune. In this paper, we exploit the better aspects of these two lines of work: first, we extend the framework of T¨ackstr¨om et al. by treating t"
D13-1205,D07-1033,0,0.0631717,"Missing"
D13-1205,P12-1073,0,0.115637,"ge of interest, we assume that parallel data with a resource-rich source language exists. With the help of this bitext and a supervised system in the source language, we infer constraints over the label distribution in the target language, and train a discriminative model using posterior regularization (Ganchev et al., 2010). Cross-lingual learning of structured prediction models via parallel data has been applied for several natural language processing problems, including partof-speech (POS) tagging (Yarowsky and Ngai, 2001), syntactic parsing (Hwa et al., 2005) and named-entity recognition (Kim et al., 2012). These methods are useful in several ways. First, they help in fast prototyping of natural language systems for new languages that do not boast human annotations. Second, the output of such systems could be used to bootstrap more extensive human annotation projects (Vlachos, 2006). Finally, they are significantly more accurate than purely unsupervised systems (McDonald et al., 2011; Das and Petrov, 2011). Recently, T¨ackstr¨om et al. (2013) presented a technique for coupling token constraints derived from projected cross-lingual information and type constraints derived from noisy tag dictiona"
D13-1205,2005.mtsummit-papers.11,0,0.0769738,"ish as a source language and train a supervised English named-entity tagger with the labels in place, using the CoNLL 2003 shared task data (Tjong Kim Sang and De Meulder, 2003). We project the spans using the maximal-span heuristic (Yarowsky and Ngai, 2001). We project into Dutch, German and Spanish and evaluate on the standard CoNLL 2002 and 2003 shared task data sets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). 4.3 Parallel Data For both tasks we use parallel data gathered automatically from the web using the method of Uszkoreit et al. (2010), as well as data from Europarl (Koehn, 2005) and the UN parallel corpus (UN, 2006), for languages covered by the latter two corpora. The parallel sentences are word aligned with the aligner of DeNero and Macherey (2011). The size of the parallel corpus is larger than we need for our tasks, so we follow T¨ackstr¨om et al. (2013) in sampling 500k tokens for POS tagging and 10k sentences for named-entity segmentation (see §5.1.2 and §5.2.2). 5 Experimental Details In this section, we provide details about taskspecific implementations of the supervised sourceside model and the word-alignment filtering techniques (steps 2 and 3 in Algorithm"
D13-1205,D12-1127,0,0.126433,"Missing"
D13-1205,J93-2004,0,0.0459736,"OS experiments, we evaluate on seventeen target languages. Fifteen of these languages were part of the experiments conducted by T¨ackstr¨om et al. (2013); we add Arabic and Hungarian to the set. The first column of Table 1 lists all seventeen languages using their two-letter abbreviation codes from the ISO 639-1 standard. The evaluation datasets correspond to the test sets from the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). For French we use the treebank of Abeill´e et al. (2003). English serves as our source language and we use the Penn Treebank (Marcus et al., 1993, with tags mapped to the universal tags) to train our supervised source-side model. 4.2 Named-Entity Segmentation Second, we investigate the task of named-entity segmentation. The goal of this task is to identify the boundaries of named-entities for a given language without classifying them by type. This is the unlabeled version of named-entity recognition, and is more amenable to cross-lingual supervision. To understand why that is, consider again the example from Figure 1. The English supervised NE tagger correctly identifies Asian as a named entity of type MISC (miscellaneous). The word-al"
D13-1205,D11-1006,0,0.273269,"s via parallel data has been applied for several natural language processing problems, including partof-speech (POS) tagging (Yarowsky and Ngai, 2001), syntactic parsing (Hwa et al., 2005) and named-entity recognition (Kim et al., 2012). These methods are useful in several ways. First, they help in fast prototyping of natural language systems for new languages that do not boast human annotations. Second, the output of such systems could be used to bootstrap more extensive human annotation projects (Vlachos, 2006). Finally, they are significantly more accurate than purely unsupervised systems (McDonald et al., 2011; Das and Petrov, 2011). Recently, T¨ackstr¨om et al. (2013) presented a technique for coupling token constraints derived from projected cross-lingual information and type constraints derived from noisy tag dictionaries to learn POS taggers. Although this technique resulted in state-ofthe-art weakly supervised taggers, the authors used a heuristic to combine the aforementioned two sources of constraints: the dictionary constraints pruned the tagger’s search space, and the intersected token-level projections were treated as hard observations. On the other hand, Ganchev et al. (2009) presented a"
D13-1205,petrov-etal-2012-universal,1,0.133575,"d lead to different local optima even when the constraint is not relaxed (b = 0). 4 Tasks and Data In this section, we focus on the nature of the two tasks that we attempt to solve, describe the source language datasets we use to train our supervised models for transfer, the target language datasets on which we evaluate our models and the parallel data we use for cross-lingual transfer. 4.1 Part-of-Speech Tagging First, we focus on the task of part-of-speech tagging. Following previous work on cross-lingual POS tagging (Das and Petrov, 2011; T¨ackstr¨om et al., 2013), we adopt the POS tags of Petrov et al. (2012), version 1.03;2 we use the October 2012 version of Wiktionary3 as our tag dictionary. After pruning the search space with the dictionary, we place soft constraints derived by projecting POS tags across word alignments. The alignments are filtered for confidence (see §5.1.2), but we also filter any projected tags that are not licensed by the dictionary. The example in Figure 1 illustrates why this dictionary filtering step is important. Consider the English-Spanish phrase pair from Figure 1, which we observed in our training data. Our supervised tagger correctly tags Asian with the ADJ tag as"
D13-1205,N12-1052,0,0.142307,"Missing"
D13-1205,Q13-1001,1,0.385909,"Missing"
D13-1205,W12-1908,0,0.0261647,"Missing"
D13-1205,W03-0419,0,0.037235,"Missing"
D13-1205,W02-2024,0,0.0143299,"of this kind are common, it makes cross-lingual de2000 tection of NE boundaries as well as types hard.4 In this paper, we focus on named-entity segmentation alone, consider the full NER task out of scope. We use English as a source language and train a supervised English named-entity tagger with the labels in place, using the CoNLL 2003 shared task data (Tjong Kim Sang and De Meulder, 2003). We project the spans using the maximal-span heuristic (Yarowsky and Ngai, 2001). We project into Dutch, German and Spanish and evaluate on the standard CoNLL 2002 and 2003 shared task data sets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). 4.3 Parallel Data For both tasks we use parallel data gathered automatically from the web using the method of Uszkoreit et al. (2010), as well as data from Europarl (Koehn, 2005) and the UN parallel corpus (UN, 2006), for languages covered by the latter two corpora. The parallel sentences are word aligned with the aligner of DeNero and Macherey (2011). The size of the parallel corpus is larger than we need for our tasks, so we follow T¨ackstr¨om et al. (2013) in sampling 500k tokens for POS tagging and 10k sentences for named-entity segmentation (see §5."
D13-1205,P10-1040,0,0.0283901,"Missing"
D13-1205,P08-1086,0,0.0644619,"Missing"
D13-1205,C10-1124,0,0.0140359,"ne, consider the full NER task out of scope. We use English as a source language and train a supervised English named-entity tagger with the labels in place, using the CoNLL 2003 shared task data (Tjong Kim Sang and De Meulder, 2003). We project the spans using the maximal-span heuristic (Yarowsky and Ngai, 2001). We project into Dutch, German and Spanish and evaluate on the standard CoNLL 2002 and 2003 shared task data sets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). 4.3 Parallel Data For both tasks we use parallel data gathered automatically from the web using the method of Uszkoreit et al. (2010), as well as data from Europarl (Koehn, 2005) and the UN parallel corpus (UN, 2006), for languages covered by the latter two corpora. The parallel sentences are word aligned with the aligner of DeNero and Macherey (2011). The size of the parallel corpus is larger than we need for our tasks, so we follow T¨ackstr¨om et al. (2013) in sampling 500k tokens for POS tagging and 10k sentences for named-entity segmentation (see §5.1.2 and §5.2.2). 5 Experimental Details In this section, we provide details about taskspecific implementations of the supervised sourceside model and the word-alignment filt"
D13-1205,W06-2209,0,0.0517174,"ior regularization (Ganchev et al., 2010). Cross-lingual learning of structured prediction models via parallel data has been applied for several natural language processing problems, including partof-speech (POS) tagging (Yarowsky and Ngai, 2001), syntactic parsing (Hwa et al., 2005) and named-entity recognition (Kim et al., 2012). These methods are useful in several ways. First, they help in fast prototyping of natural language systems for new languages that do not boast human annotations. Second, the output of such systems could be used to bootstrap more extensive human annotation projects (Vlachos, 2006). Finally, they are significantly more accurate than purely unsupervised systems (McDonald et al., 2011; Das and Petrov, 2011). Recently, T¨ackstr¨om et al. (2013) presented a technique for coupling token constraints derived from projected cross-lingual information and type constraints derived from noisy tag dictionaries to learn POS taggers. Although this technique resulted in state-ofthe-art weakly supervised taggers, the authors used a heuristic to combine the aforementioned two sources of constraints: the dictionary constraints pruned the tagger’s search space, and the intersected token-le"
D13-1205,N01-1026,0,0.399612,"dels for the languages that lack annotated resources. For a given resource-poor target language of interest, we assume that parallel data with a resource-rich source language exists. With the help of this bitext and a supervised system in the source language, we infer constraints over the label distribution in the target language, and train a discriminative model using posterior regularization (Ganchev et al., 2010). Cross-lingual learning of structured prediction models via parallel data has been applied for several natural language processing problems, including partof-speech (POS) tagging (Yarowsky and Ngai, 2001), syntactic parsing (Hwa et al., 2005) and named-entity recognition (Kim et al., 2012). These methods are useful in several ways. First, they help in fast prototyping of natural language systems for new languages that do not boast human annotations. Second, the output of such systems could be used to bootstrap more extensive human annotation projects (Vlachos, 2006). Finally, they are significantly more accurate than purely unsupervised systems (McDonald et al., 2011; Das and Petrov, 2011). Recently, T¨ackstr¨om et al. (2013) presented a technique for coupling token constraints derived from pr"
D13-1205,I08-3008,0,0.0649592,"ure performance on standard benchmark datasets for both of these tasks, and report improvements over state-of-the-art baselines. 2 Prior Work Cross-lingual projection methods can be classified by their use of two very broad ideas. The first idea utilizes parallel data to create full or partial annotations in the low-resource language and trains from this data. This was popularized by Yarowsky and Ngai (2001) who applied this to POS tagging and shallow parsing. It was later applied to parsing (Hwa et al., 2005) and named entity recognition (Kim et al., 2012). The second idea, first proposed by Zeman and Resnik (2008) and applied more broadly by McDonald et al. (2011), is to train a model on a resource-rich language and apply it to a resourcepoor language directly. The disparity between the languages is mitigated by the choice of features. In addition to cross-lingual projection, purely unsupervised methods have been explored but with limited success (Christodoulopoulos et al., 2010). Here, we resort to cross-lingual projection and incorporate the first idea; we also follow Li et al. (2012) and use Wiktionary to further constrain the POS tagging task. Our learning setup is similar to that of Ganchev et al."
D13-1205,P10-2036,1,\N,Missing
D13-1205,D07-1096,0,\N,Missing
D14-1134,D11-1039,1,0.834478,"et al. (2011), Goldwasser and Roth (2011), Liang 6 Results still vary slightly due to multi-threading. et al. (2011), Chen and Mooney (2011), and Chen (2012). In all these approaches the grammar structure is fixed prior to parameter estimation. Zettlemoyer and Collins (2005) proposed the learning regime most related to ours. Their learner alternates between batch lexical induction and online parameter estimation. Our learning algorithm design combines aspects of previously studied approaches into a batch method, including gradient updates (Kwiatkowski et al., 2010) and using weak supervision (Artzi and Zettlemoyer, 2011). In contrast, Artzi and Zettlemoyer (2013b) use online perceptron-style updates to optimize a margin-based loss. Our work also focuses on CCG lexicon induction but differs in the use of corpuslevel statistics through voting and pruning for explicitly controlling the size of the lexicon. Our approach is also related to the grammar induction algorithm introduced by Carroll and Char1281 niak (1992). Similar to our method, they process the data using two batch steps: the first proposes grammar rules, analogous to our step that generates lexical entries, and the second estimates parsing parameters"
D14-1134,Q13-1005,1,0.449487,"owers 26 0 N λx.intersection(x) N : λx.hall(x) String Table 3: Example entries from a learned ORACLE corpus lexicon using batch learning. For each string we report the number of lexical entries without voting (C ONSENSUS VOTE) and pruning and with, and provide a few examples. Struck entries were successfully avoided when using voting and pruning. decreases from 16.77 for online training to 8.11. Finally, the total computational cost of our approach is roughly equivalent to online approaches. In both approaches, each pass over the data makes the same number of inference calls, and in practice, Artzi and Zettlemoyer (2013b) used 6-8 iterations for online learning while we used 10. A benefit of the batch method is its insensitivity to data ordering, as expressed by the lower standard deviation between randomized runs in Table 2.6 7 Related Work There has been significant work on learning for semantic parsing. The majority of approaches treat grammar induction and parameter estimation separately, e.g. Wong and Mooney (2006), Kate and Mooney (2006), Clarke et al. (2010), Goldwasser et al. (2011), Goldwasser and Roth (2011), Liang 6 Results still vary slightly due to multi-threading. et al. (2011), Chen and Mooney"
D14-1134,S13-1045,0,0.107865,"(Steedman, 1996, 2000, CCG, henceforth) is a commonly used formalism for semantic parsing – the task of mapping natural language sentences to formal meaning representations (Zelle and Mooney, 1996). Recently, CCG semantic parsers have been used for numerous language understanding tasks, including querying databases (Zettlemoyer and Collins, 2005), referring to physical objects (Matuszek et al., 2012), information extraction (Krishnamurthy and Mitchell, 2012), executing instructions (Artzi and Zettlemoyer, 2013b), generating regular expressions (Kushman and Barzilay, 2013), question-answering (Cai and Yates, 2013) and textual entailment (Lewis and Steedman, 2013). In CCG, a lexicon is used to map words to formal representations of their meaning, which are then combined using bottom-up operations. In this paper we present learning techniques ∗ This research was carried out at Google. to explicitly control the size of the CCG lexicon, and show that this results in improved task performance and more compact models. In most approaches for inducing CCGs for semantic parsing, lexicon learning and parameter estimation are performed jointly in an online algorithm, as introduced by Zettlemoyer and Collins (2007"
D14-1134,P12-1045,0,0.611188,"82.89 83.69 83.00 84.10 Sequence R 58.95 61.23 60.13 64.86 63.45 65.97 66.40 66.15 P 68.35 66.83 72.64 72.79 72.99 75.15 72.91 75.65 Lexicon size 5383 3104 6323 2588 2446 2791 2186 2101 F1 63.26 63.88 65.76 68.55 67.84 70.19 69.47 70.55 Table 1: Ablation study using cross-validation on the ORACLE corpus training data. We report mean precision (P), recall (R) and harmonic mean (F1) of execution accuracy on single sentences and sequences of instructions and mean lexicon sizes. Bold numbers represent the best performing method on a given metric. Single sentence R F1 Chen and Mooney (2011) 54.40 Chen (2012) 57.28 + additional data 57.62 SAIL Kim and Mooney (2012) 57.22 Kim and Mooney (2013) 62.81 Artzi and Zettlemoyer (2013b) 67.60 65.28 66.42 Our Approach 66.67 64.36 65.49 Artzi and Zettlemoyer (2013b) 81.17 (0.68) 78.63 (0.84) 79.88 (0.76) ORACLE Our Approach 79.86 (0.50) 77.87 (0.41) 78.85 (0.45) Final results P P 38.06 41.30 68.07 (2.72) 76.05 (1.79) Sequence R 16.18 19.18 20.64 20.17 26.57 31.93 35.44 58.05 (3.12) 68.53 (1.76) F1 Lexicon size 34.72 38.14 62.65 (2.91) 72.10 (1.77) 10051 2873 6213 (217) 2365 (57) Table 2: Our final results compared to previous work on the SAIL and ORACLE corp"
D14-1134,J07-4004,0,0.0157977,"you, ι(λx.chair(x)∧ intersect(x, ι(λy.intersection(y))))))∧ move(a) ∧ len(a, 2) hFORWARD, FORWARDi turn left λa.turn(a) ∧ direction(a, left) hLEFTi go to the end of the hall λx.move(a) ∧ to(a, ι(λx.end(x, ι(λy.hall(y))))) hFORWARD, FORWARDi Figure 3: Fragment of a map and instructions for the navigation domain. The fragment includes two intersecting hallways (red and blue), two chairs and an agent facing left (green pentagon), which follows instructions such as these listed below. Each instruction is paired with a logical form representing its meaning and its execution in the map. inspired by Clark and Curran (2007). Let G EN(x, s; Λ) ⊂ Y be the set of all possible CCG parses given the sentence x, the current state s and the lexicon Λ. In G EN(x, s; Λ), multiple parse trees may have the same logical form; let Y(z) ⊂ G EN(x, s; Λ) be the subset of such parses with the logical form z at the root. Also, let θ ∈ Rd be a d-dimensional parameter vector. We define the probability of the logical form z as: X p(z|x, s; θ, Λ) = p(y|x, s; θ, Λ) (1) y∈Y(z) Above, we marginalize out the probabilities of all parse trees with the same logical form z at the root. The probability of a parse tree y is defined as: eθ·φ(x,s"
D14-1134,W10-2903,0,0.0454206,"ly equivalent to online approaches. In both approaches, each pass over the data makes the same number of inference calls, and in practice, Artzi and Zettlemoyer (2013b) used 6-8 iterations for online learning while we used 10. A benefit of the batch method is its insensitivity to data ordering, as expressed by the lower standard deviation between randomized runs in Table 2.6 7 Related Work There has been significant work on learning for semantic parsing. The majority of approaches treat grammar induction and parameter estimation separately, e.g. Wong and Mooney (2006), Kate and Mooney (2006), Clarke et al. (2010), Goldwasser et al. (2011), Goldwasser and Roth (2011), Liang 6 Results still vary slightly due to multi-threading. et al. (2011), Chen and Mooney (2011), and Chen (2012). In all these approaches the grammar structure is fixed prior to parameter estimation. Zettlemoyer and Collins (2005) proposed the learning regime most related to ours. Their learner alternates between batch lexical induction and online parameter estimation. Our learning algorithm design combines aspects of previously studied approaches into a batch method, including gradient updates (Kwiatkowski et al., 2010) and using weak"
D14-1134,P11-1149,0,0.0730235,"e approaches. In both approaches, each pass over the data makes the same number of inference calls, and in practice, Artzi and Zettlemoyer (2013b) used 6-8 iterations for online learning while we used 10. A benefit of the batch method is its insensitivity to data ordering, as expressed by the lower standard deviation between randomized runs in Table 2.6 7 Related Work There has been significant work on learning for semantic parsing. The majority of approaches treat grammar induction and parameter estimation separately, e.g. Wong and Mooney (2006), Kate and Mooney (2006), Clarke et al. (2010), Goldwasser et al. (2011), Goldwasser and Roth (2011), Liang 6 Results still vary slightly due to multi-threading. et al. (2011), Chen and Mooney (2011), and Chen (2012). In all these approaches the grammar structure is fixed prior to parameter estimation. Zettlemoyer and Collins (2005) proposed the learning regime most related to ours. Their learner alternates between batch lexical induction and online parameter estimation. Our learning algorithm design combines aspects of previously studied approaches into a batch method, including gradient updates (Kwiatkowski et al., 2010) and using weak supervision (Artzi and Zet"
D14-1134,P11-1060,0,0.247859,"Missing"
D14-1134,P06-1115,0,0.0177015,"of our approach is roughly equivalent to online approaches. In both approaches, each pass over the data makes the same number of inference calls, and in practice, Artzi and Zettlemoyer (2013b) used 6-8 iterations for online learning while we used 10. A benefit of the batch method is its insensitivity to data ordering, as expressed by the lower standard deviation between randomized runs in Table 2.6 7 Related Work There has been significant work on learning for semantic parsing. The majority of approaches treat grammar induction and parameter estimation separately, e.g. Wong and Mooney (2006), Kate and Mooney (2006), Clarke et al. (2010), Goldwasser et al. (2011), Goldwasser and Roth (2011), Liang 6 Results still vary slightly due to multi-threading. et al. (2011), Chen and Mooney (2011), and Chen (2012). In all these approaches the grammar structure is fixed prior to parameter estimation. Zettlemoyer and Collins (2005) proposed the learning regime most related to ours. Their learner alternates between batch lexical induction and online parameter estimation. Our learning algorithm design combines aspects of previously studied approaches into a batch method, including gradient updates (Kwiatkowski et al.,"
D14-1134,D12-1040,0,0.301446,"Missing"
D14-1134,P13-1022,0,0.159261,"Missing"
D14-1134,D12-1069,0,0.080607,"ord chair as learned with no corpus-level statistics. Our approach is able to correctly learn only the top two bolded entries. Introduction Combinatory Categorial Grammar (Steedman, 1996, 2000, CCG, henceforth) is a commonly used formalism for semantic parsing – the task of mapping natural language sentences to formal meaning representations (Zelle and Mooney, 1996). Recently, CCG semantic parsers have been used for numerous language understanding tasks, including querying databases (Zettlemoyer and Collins, 2005), referring to physical objects (Matuszek et al., 2012), information extraction (Krishnamurthy and Mitchell, 2012), executing instructions (Artzi and Zettlemoyer, 2013b), generating regular expressions (Kushman and Barzilay, 2013), question-answering (Cai and Yates, 2013) and textual entailment (Lewis and Steedman, 2013). In CCG, a lexicon is used to map words to formal representations of their meaning, which are then combined using bottom-up operations. In this paper we present learning techniques ∗ This research was carried out at Google. to explicitly control the size of the CCG lexicon, and show that this results in improved task performance and more compact models. In most approaches for inducing CCG"
D14-1134,N13-1103,0,0.038827,"ies. Introduction Combinatory Categorial Grammar (Steedman, 1996, 2000, CCG, henceforth) is a commonly used formalism for semantic parsing – the task of mapping natural language sentences to formal meaning representations (Zelle and Mooney, 1996). Recently, CCG semantic parsers have been used for numerous language understanding tasks, including querying databases (Zettlemoyer and Collins, 2005), referring to physical objects (Matuszek et al., 2012), information extraction (Krishnamurthy and Mitchell, 2012), executing instructions (Artzi and Zettlemoyer, 2013b), generating regular expressions (Kushman and Barzilay, 2013), question-answering (Cai and Yates, 2013) and textual entailment (Lewis and Steedman, 2013). In CCG, a lexicon is used to map words to formal representations of their meaning, which are then combined using bottom-up operations. In this paper we present learning techniques ∗ This research was carried out at Google. to explicitly control the size of the CCG lexicon, and show that this results in improved task performance and more compact models. In most approaches for inducing CCGs for semantic parsing, lexicon learning and parameter estimation are performed jointly in an online algorithm, as i"
D14-1134,D10-1119,0,0.0362849,"e and Mooney (2006), Clarke et al. (2010), Goldwasser et al. (2011), Goldwasser and Roth (2011), Liang 6 Results still vary slightly due to multi-threading. et al. (2011), Chen and Mooney (2011), and Chen (2012). In all these approaches the grammar structure is fixed prior to parameter estimation. Zettlemoyer and Collins (2005) proposed the learning regime most related to ours. Their learner alternates between batch lexical induction and online parameter estimation. Our learning algorithm design combines aspects of previously studied approaches into a batch method, including gradient updates (Kwiatkowski et al., 2010) and using weak supervision (Artzi and Zettlemoyer, 2011). In contrast, Artzi and Zettlemoyer (2013b) use online perceptron-style updates to optimize a margin-based loss. Our work also focuses on CCG lexicon induction but differs in the use of corpuslevel statistics through voting and pruning for explicitly controlling the size of the lexicon. Our approach is also related to the grammar induction algorithm introduced by Carroll and Char1281 niak (1992). Similar to our method, they process the data using two batch steps: the first proposes grammar rules, analogous to our step that generates lex"
D14-1134,D11-1140,0,0.155997,"tching categories to strings in the sentence using the lexicon. For example, the lexical entry walk ` S/N P : λx.λa.move(a) ∧ direction(a, x) pairs the string walk with the example category above. Each intermediate parse node is constructed by applying 1274 one of a small set of binary CCG combinators or unary operators. For example, in Figure 2 the category of the span walk forward is combined with the category of twice using backward application (<). Parsing concludes with a logical form that captures the meaning of the complete sentence. We adopt a factored representation for CCG lexicons (Kwiatkowski et al., 2011), where entries are dynamically generated by combining lexemes and templates. A lexeme is a pair that consists of a natural language string and a set of logical constants, while the template contains the syntactic and semantic components of a CCG category, abstracting over logical constants. For example, consider the lexical entry walk ` S/N P : λx.λa.move(a) ∧ direction(a, x). Under the factored representation, this entry can be constructed by combining the lexeme hwalk, {move, direction}i and the template λv1 .λv2 .[S/N P : λx.λa.v1 (a) ∧ v2 (a, x)]. This representation allows for better gen"
D14-1134,Q13-1015,0,0.0213264,"commonly used formalism for semantic parsing – the task of mapping natural language sentences to formal meaning representations (Zelle and Mooney, 1996). Recently, CCG semantic parsers have been used for numerous language understanding tasks, including querying databases (Zettlemoyer and Collins, 2005), referring to physical objects (Matuszek et al., 2012), information extraction (Krishnamurthy and Mitchell, 2012), executing instructions (Artzi and Zettlemoyer, 2013b), generating regular expressions (Kushman and Barzilay, 2013), question-answering (Cai and Yates, 2013) and textual entailment (Lewis and Steedman, 2013). In CCG, a lexicon is used to map words to formal representations of their meaning, which are then combined using bottom-up operations. In this paper we present learning techniques ∗ This research was carried out at Google. to explicitly control the size of the CCG lexicon, and show that this results in improved task performance and more compact models. In most approaches for inducing CCGs for semantic parsing, lexicon learning and parameter estimation are performed jointly in an online algorithm, as introduced by Zettlemoyer and Collins (2007). To induce the lexicon, words extracted from the"
D14-1134,N06-1056,0,0.0579694,"otal computational cost of our approach is roughly equivalent to online approaches. In both approaches, each pass over the data makes the same number of inference calls, and in practice, Artzi and Zettlemoyer (2013b) used 6-8 iterations for online learning while we used 10. A benefit of the batch method is its insensitivity to data ordering, as expressed by the lower standard deviation between randomized runs in Table 2.6 7 Related Work There has been significant work on learning for semantic parsing. The majority of approaches treat grammar induction and parameter estimation separately, e.g. Wong and Mooney (2006), Kate and Mooney (2006), Clarke et al. (2010), Goldwasser et al. (2011), Goldwasser and Roth (2011), Liang 6 Results still vary slightly due to multi-threading. et al. (2011), Chen and Mooney (2011), and Chen (2012). In all these approaches the grammar structure is fixed prior to parameter estimation. Zettlemoyer and Collins (2005) proposed the learning regime most related to ours. Their learner alternates between batch lexical induction and online parameter estimation. Our learning algorithm design combines aspects of previously studied approaches into a batch method, including gradient upda"
D14-1134,D07-1071,0,0.0217413,"swering (Cai and Yates, 2013) and textual entailment (Lewis and Steedman, 2013). In CCG, a lexicon is used to map words to formal representations of their meaning, which are then combined using bottom-up operations. In this paper we present learning techniques ∗ This research was carried out at Google. to explicitly control the size of the CCG lexicon, and show that this results in improved task performance and more compact models. In most approaches for inducing CCGs for semantic parsing, lexicon learning and parameter estimation are performed jointly in an online algorithm, as introduced by Zettlemoyer and Collins (2007). To induce the lexicon, words extracted from the training data are paired with CCG categories one sample at a time (for an overview of CCG, see §2). Joint approaches have the potential advantage that only entries participating in successful parses are added to the lexicon. However, new entries are added greedily and these decisions are never revisited at later stages. In practice, this often results in a large and noisy lexicon. Figure 1 lists a sample of CCG lexical entries learned for the word chair with a greedy joint algorithm (Artzi and Zettlemoyer, 2013b). In the studied navigation doma"
D15-1112,P98-1013,0,0.873679,"everal coarse syntactic categories. 2.2 Related Work SRL using PropBank conventions (Palmer et al., 2005) has been widely studied. There have been two shared tasks at CoNLL 2004-2005 to identify the phrasal arguments of verbal predicates (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). The CoNLL 2008-2009 shared tasks introduced a variant where semantic dependencies are annotated rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Similar approaches (Das et al., 2014; Hermann et al., 2014) have been applied to frame-semantic parsing using FrameNet conventions (Baker et al., 1998). We treat PropBank and FrameNet annotations in a common framework, similar to Hermann et al. (2014). Most prior work on SRL rely on syntactic parses provided as input and use locally estimated classifiers for each span-role pair that are only combined at prediction time.2 This is done by picking the highest scoring role for each span, subject to a set of structural constraints, such as avoiding overlapping arguments and repeated core roles. Typically, these constraints have been enforced by integer linear programming (ILP), as in Punyakanok et al. (2008). Täckström et al. (2015) interpreted t"
D15-1112,C10-3009,0,0.0814256,"ed on low-dimensional representations that reduce the need for intensive feature engineering and lead to better generalization in the face of data sparsity. Lei et al. (2015) employ low-rank tensor factorization to induce a compact representation of the full cross-product of atomic features; akin to this work, they represent semantic roles as real-valued vectors, but use a different scoring formulation for modeling potential arguments. Moreover, they restrict their experiments to CoNLL 2009 semantic dependencies. Roth and Woodsend (2014) improve on the state-of-the-art feature-based system of Björkelund et al. (2010) by adding distributional word representations trained on large unlabeled corpora as features. Collobert and Weston (2007) use a neural network and do not rely on syntactic parses as input. While they use non-standard evaluation, they report accuracy similar to the ASSERT system (Pradhan et al., 2005), to which we compare in Table 4. Very recently, Zhou and Xu (2015) proposed a deep bidirectional LSTM model for SRL that does not rely on syntax trees as input; their approach achieves the best results on CoNLL 2005 and 2012 corpora to date, but unlike this work, they do not report results on Fra"
D15-1112,J92-4003,0,0.148953,"Missing"
D15-1112,W04-2412,0,0.0150355,"r meaning. In contrast, a FrameNet frame often associates with multiple lexical units and the frame lexicon 1 We borrow the term “lexical unit” from the frame semantics literature. The CoNLL 2005 dataset is restricted to verbal lexical units, while the CoNLL 2009 and 2012 datasets contains both verbal and nominal lexical units. FrameNet has lexical units of several coarse syntactic categories. 2.2 Related Work SRL using PropBank conventions (Palmer et al., 2005) has been widely studied. There have been two shared tasks at CoNLL 2004-2005 to identify the phrasal arguments of verbal predicates (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). The CoNLL 2008-2009 shared tasks introduced a variant where semantic dependencies are annotated rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Similar approaches (Das et al., 2014; Hermann et al., 2014) have been applied to frame-semantic parsing using FrameNet conventions (Baker et al., 1998). We treat PropBank and FrameNet annotations in a common framework, similar to Hermann et al. (2014). Most prior work on SRL rely on syntactic parses provided as input and use locally estimated classifiers for each span-role pair that are only co"
D15-1112,W05-0620,0,0.651568,"ameNet frame often associates with multiple lexical units and the frame lexicon 1 We borrow the term “lexical unit” from the frame semantics literature. The CoNLL 2005 dataset is restricted to verbal lexical units, while the CoNLL 2009 and 2012 datasets contains both verbal and nominal lexical units. FrameNet has lexical units of several coarse syntactic categories. 2.2 Related Work SRL using PropBank conventions (Palmer et al., 2005) has been widely studied. There have been two shared tasks at CoNLL 2004-2005 to identify the phrasal arguments of verbal predicates (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). The CoNLL 2008-2009 shared tasks introduced a variant where semantic dependencies are annotated rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Similar approaches (Das et al., 2014; Hermann et al., 2014) have been applied to frame-semantic parsing using FrameNet conventions (Baker et al., 1998). We treat PropBank and FrameNet annotations in a common framework, similar to Hermann et al. (2014). Most prior work on SRL rely on syntactic parses provided as input and use locally estimated classifiers for each span-role pair that are only combined at prediction time.2 T"
D15-1112,D14-1082,0,0.00975207,"arallel.4 Moreover, note that span v s and frame-role v (f,r) representations are decoupled in this model. This decoupling is important as it allows us to train a single model in a multitask setting. We demonstrate this by successfully combining PropBank and FrameNet training data, as described in §5. 3.3 Parameter Estimation We consider two methods for parameter estimation. 963 3 Various other network structures are worth investigating, such as concatenating the span, frame and role representations and passing them through fully connected layers. This treatment, for example, has been used by Chen and Manning (2014) for syntactic parsing. We leave these explorations to future work. 4 We found that adding feature conjunctions to the network’s input layer did not improve performance in practice. Local Estimation In local estimation, we treat each span-role assignment pair (s, r) ∈ S×R as an individual binary decision problem and maximize the corresponding log-likelihood of the training set.5 Denote by zs,r ∈ {0, 1} the decision variable, such that zs,r = 1 iff span s is assigned role r. By passing the potential gNN (s, r; θ) through the logistic function, we obtain the log-likelihood l(zs,r ; θ) , log p(zs"
D15-1112,P07-1071,0,0.0614034,"ization in the face of data sparsity. Lei et al. (2015) employ low-rank tensor factorization to induce a compact representation of the full cross-product of atomic features; akin to this work, they represent semantic roles as real-valued vectors, but use a different scoring formulation for modeling potential arguments. Moreover, they restrict their experiments to CoNLL 2009 semantic dependencies. Roth and Woodsend (2014) improve on the state-of-the-art feature-based system of Björkelund et al. (2010) by adding distributional word representations trained on large unlabeled corpora as features. Collobert and Weston (2007) use a neural network and do not rely on syntactic parses as input. While they use non-standard evaluation, they report accuracy similar to the ASSERT system (Pradhan et al., 2005), to which we compare in Table 4. Very recently, Zhou and Xu (2015) proposed a deep bidirectional LSTM model for SRL that does not rely on syntax trees as input; their approach achieves the best results on CoNLL 2005 and 2012 corpora to date, but unlike this work, they do not report results on FrameNet and CoNLL 2009 dependencies and do not investigate joint learning approaches involving multiple annotation conventio"
D15-1112,J14-1002,1,0.878598,"the CoNLL 2009 and 2012 datasets contains both verbal and nominal lexical units. FrameNet has lexical units of several coarse syntactic categories. 2.2 Related Work SRL using PropBank conventions (Palmer et al., 2005) has been widely studied. There have been two shared tasks at CoNLL 2004-2005 to identify the phrasal arguments of verbal predicates (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). The CoNLL 2008-2009 shared tasks introduced a variant where semantic dependencies are annotated rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Similar approaches (Das et al., 2014; Hermann et al., 2014) have been applied to frame-semantic parsing using FrameNet conventions (Baker et al., 1998). We treat PropBank and FrameNet annotations in a common framework, similar to Hermann et al. (2014). Most prior work on SRL rely on syntactic parses provided as input and use locally estimated classifiers for each span-role pair that are only combined at prediction time.2 This is done by picking the highest scoring role for each span, subject to a set of structural constraints, such as avoiding overlapping arguments and repeated core roles. Typically, these constraints have been"
D15-1112,P15-1030,0,0.0215591,"forces most of the constraints proposed by Punyakanok et al. and showed how the DP can be used to take these constraints into account during learning. Here, we use an identical graphical model, but extend the model of Täckström et al. by replacing its linear potential func961 2 Some recent work have successfully proposed joint models for syntactic parsing and SRL instead of a pipeline approach (Lewis et al., 2015). tions with a multi-layer neural network. A similar use of non-linear potential functions in a structured model was proposed by Do and Artières (2010) for speech recognition, and by Durrett and Klein (2015) for syntactic phrase-structure parsing. Feature-based approaches to SRL employ handengineered linguistically-motivated feature templates to represent the semantic structure. Some recent work has focused on low-dimensional representations that reduce the need for intensive feature engineering and lead to better generalization in the face of data sparsity. Lei et al. (2015) employ low-rank tensor factorization to induce a compact representation of the full cross-product of atomic features; akin to this work, they represent semantic roles as real-valued vectors, but use a different scoring formu"
D15-1112,P14-1136,1,0.55904,"y challenge in this task is sparsity of labeled data: a given predicate-role instance may only occur a handful of times in the training set. Most existing SRL systems model each semantic role as an atomic unit of meaning, ignoring finer-grained semantic similarity between roles that can be leveraged to share context between similar labels, both within and across annotation conventions. Low-dimensional embedding representations have been shown to be successful in overcoming sparsity and representing label similarity across a wide range of tasks (Weston et al., 2011; Srikumar and Manning, 2014; Hermann et al., 2014; Lei et al., 2015). In this paper, we present a new model for SRL that embeds candidate arguments and semantic roles (in context of a predicate frame) in a shared vector space. A feed-forward neural ∗ Work carried out during an internship at Google. network is learned to capture correlations of the respective embedding dimensions to create argument and role representations. The similarity of these two representations, as measured by their dot product, is used to score possible roles for candidate arguments within a graphical model. This graphical model jointly models the assignment of semanti"
D15-1112,P15-2036,0,0.281469,"not rely on syntactic parses as input. While they use non-standard evaluation, they report accuracy similar to the ASSERT system (Pradhan et al., 2005), to which we compare in Table 4. Very recently, Zhou and Xu (2015) proposed a deep bidirectional LSTM model for SRL that does not rely on syntax trees as input; their approach achieves the best results on CoNLL 2005 and 2012 corpora to date, but unlike this work, they do not report results on FrameNet and CoNLL 2009 dependencies and do not investigate joint learning approaches involving multiple annotation conventions. For FrameNet-style SRL, Kshirsagar et al. (2015) recently proposed the use of PropBankbased features, but their system performance falls short of the state of the art. Roth and Lapata (2015) proposed another approach exploring linguistically motivated features tuned towards the FrameNet lexicon, but their performance metrics are significantly worse than our best results. The inspiration behind our approach stems from recent work on bilinear models (Mnih and Hinton, 2007). There have been several recent studies representing input observations and output labels with distributed representations, for example, in the W SABIE model for image anno"
D15-1112,N15-1121,0,0.311845,"sk is sparsity of labeled data: a given predicate-role instance may only occur a handful of times in the training set. Most existing SRL systems model each semantic role as an atomic unit of meaning, ignoring finer-grained semantic similarity between roles that can be leveraged to share context between similar labels, both within and across annotation conventions. Low-dimensional embedding representations have been shown to be successful in overcoming sparsity and representing label similarity across a wide range of tasks (Weston et al., 2011; Srikumar and Manning, 2014; Hermann et al., 2014; Lei et al., 2015). In this paper, we present a new model for SRL that embeds candidate arguments and semantic roles (in context of a predicate frame) in a shared vector space. A feed-forward neural ∗ Work carried out during an internship at Google. network is learned to capture correlations of the respective embedding dimensions to create argument and role representations. The similarity of these two representations, as measured by their dot product, is used to score possible roles for candidate arguments within a graphical model. This graphical model jointly models the assignment of semantic roles to all argu"
D15-1112,D15-1169,0,0.0473471,"röm et al. (2015) interpreted this as a graphical model with local factors for each span-role pair, and global factors that encode the structural constraints. They derived a dynamic program (DP) that enforces most of the constraints proposed by Punyakanok et al. and showed how the DP can be used to take these constraints into account during learning. Here, we use an identical graphical model, but extend the model of Täckström et al. by replacing its linear potential func961 2 Some recent work have successfully proposed joint models for syntactic parsing and SRL instead of a pipeline approach (Lewis et al., 2015). tions with a multi-layer neural network. A similar use of non-linear potential functions in a structured model was proposed by Do and Artières (2010) for speech recognition, and by Durrett and Klein (2015) for syntactic phrase-structure parsing. Feature-based approaches to SRL employ handengineered linguistically-motivated feature templates to represent the semantic structure. Some recent work has focused on low-dimensional representations that reduce the need for intensive feature engineering and lead to better generalization in the face of data sparsity. Lei et al. (2015) employ low-rank t"
D15-1112,J05-1004,0,0.845218,"further arguments of a frame, such as temporal (AM-TMP) and locative (AM-LOC) adjuncts. The non-core roles are shared between all frames and assume similar meaning. In contrast, a FrameNet frame often associates with multiple lexical units and the frame lexicon 1 We borrow the term “lexical unit” from the frame semantics literature. The CoNLL 2005 dataset is restricted to verbal lexical units, while the CoNLL 2009 and 2012 datasets contains both verbal and nominal lexical units. FrameNet has lexical units of several coarse syntactic categories. 2.2 Related Work SRL using PropBank conventions (Palmer et al., 2005) has been widely studied. There have been two shared tasks at CoNLL 2004-2005 to identify the phrasal arguments of verbal predicates (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). The CoNLL 2008-2009 shared tasks introduced a variant where semantic dependencies are annotated rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Similar approaches (Das et al., 2014; Hermann et al., 2014) have been applied to frame-semantic parsing using FrameNet conventions (Baker et al., 1998). We treat PropBank and FrameNet annotations in a common framework, similar to Hermann"
D15-1112,D14-1162,0,0.118328,"model of Lei et al. (2015). We also trained the linear model of Täckström et al. on this dataset (their work omitted this experiment), as a baseline. Finally, for the FrameNet experiments, we compare to the state-of-the-art system of Hermann et al. (2014), which combines a frame-identification model based on W SABIE (Weston et al., 2011) with a log-linear role labeling model. 4.4 Hyperparameters and Initialization There are several hyperparameters in our model (§3.2). First, the span embedding dimension of es was fixed to 300 to match the dimension of the pretrained GloVe word embeddings from Pennington et al. (2014) that we use to initialize the embeddings of the word-based features in φ(s, x, t, `). Preliminary experiments showed random initialization of the word-based embeddings to be inferior to pre-trained embeddings. The remaining model parameters were randomly initialized. The frame embedding dimension was chosen from {100, 200, 300, 500}, while the hidden layer dimension was chosen from {300, 500}. For PropBank, we fixed the role embedding dimension to 27, which is the number of semantic roles in PropBank datasets (ignoring the AA role, that appears with negligible frequency). For FrameNet, the ro"
D15-1112,J08-2005,0,0.968102,"emantic parsing using FrameNet conventions (Baker et al., 1998). We treat PropBank and FrameNet annotations in a common framework, similar to Hermann et al. (2014). Most prior work on SRL rely on syntactic parses provided as input and use locally estimated classifiers for each span-role pair that are only combined at prediction time.2 This is done by picking the highest scoring role for each span, subject to a set of structural constraints, such as avoiding overlapping arguments and repeated core roles. Typically, these constraints have been enforced by integer linear programming (ILP), as in Punyakanok et al. (2008). Täckström et al. (2015) interpreted this as a graphical model with local factors for each span-role pair, and global factors that encode the structural constraints. They derived a dynamic program (DP) that enforces most of the constraints proposed by Punyakanok et al. and showed how the DP can be used to take these constraints into account during learning. Here, we use an identical graphical model, but extend the model of Täckström et al. by replacing its linear potential func961 2 Some recent work have successfully proposed joint models for syntactic parsing and SRL instead of a pipeline ap"
D15-1112,Q15-1003,1,0.37455,"., 2005), to which we compare in Table 4. Very recently, Zhou and Xu (2015) proposed a deep bidirectional LSTM model for SRL that does not rely on syntax trees as input; their approach achieves the best results on CoNLL 2005 and 2012 corpora to date, but unlike this work, they do not report results on FrameNet and CoNLL 2009 dependencies and do not investigate joint learning approaches involving multiple annotation conventions. For FrameNet-style SRL, Kshirsagar et al. (2015) recently proposed the use of PropBankbased features, but their system performance falls short of the state of the art. Roth and Lapata (2015) proposed another approach exploring linguistically motivated features tuned towards the FrameNet lexicon, but their performance metrics are significantly worse than our best results. The inspiration behind our approach stems from recent work on bilinear models (Mnih and Hinton, 2007). There have been several recent studies representing input observations and output labels with distributed representations, for example, in the W SABIE model for image annotation (Weston et al., 2011), in models for embedding labels in structured graphical models (Srikumar and Manning, 2014), and in techniques to"
D15-1112,D14-1045,0,0.126819,"feature templates to represent the semantic structure. Some recent work has focused on low-dimensional representations that reduce the need for intensive feature engineering and lead to better generalization in the face of data sparsity. Lei et al. (2015) employ low-rank tensor factorization to induce a compact representation of the full cross-product of atomic features; akin to this work, they represent semantic roles as real-valued vectors, but use a different scoring formulation for modeling potential arguments. Moreover, they restrict their experiments to CoNLL 2009 semantic dependencies. Roth and Woodsend (2014) improve on the state-of-the-art feature-based system of Björkelund et al. (2010) by adding distributional word representations trained on large unlabeled corpora as features. Collobert and Weston (2007) use a neural network and do not rely on syntactic parses as input. While they use non-standard evaluation, they report accuracy similar to the ASSERT system (Pradhan et al., 2005), to which we compare in Table 4. Very recently, Zhou and Xu (2015) proposed a deep bidirectional LSTM model for SRL that does not rely on syntax trees as input; their approach achieves the best results on CoNLL 2005"
D15-1112,W08-2121,0,0.0242864,"CoNLL 2005 dataset is restricted to verbal lexical units, while the CoNLL 2009 and 2012 datasets contains both verbal and nominal lexical units. FrameNet has lexical units of several coarse syntactic categories. 2.2 Related Work SRL using PropBank conventions (Palmer et al., 2005) has been widely studied. There have been two shared tasks at CoNLL 2004-2005 to identify the phrasal arguments of verbal predicates (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). The CoNLL 2008-2009 shared tasks introduced a variant where semantic dependencies are annotated rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Similar approaches (Das et al., 2014; Hermann et al., 2014) have been applied to frame-semantic parsing using FrameNet conventions (Baker et al., 1998). We treat PropBank and FrameNet annotations in a common framework, similar to Hermann et al. (2014). Most prior work on SRL rely on syntactic parses provided as input and use locally estimated classifiers for each span-role pair that are only combined at prediction time.2 This is done by picking the highest scoring role for each span, subject to a set of structural constraints, such as avoiding overlapping arguments and"
D15-1112,J08-2002,0,0.117558,"embedding model. To ensure a fair comparison with the closest linear model baseline, we ensured that the preprocessing steps, the argument candidate generation algorithm for the span-based datasets and the frame identification methods are identical to Täckström et al. (2015, §3.2, §6.2-§6.3). 4.3 Baseline Systems In addition to comparing to Täckström et al. (2015), whose setup is closest to ours, we also compare to prior state-of-the-art systems from the literature. For CoNLL 2005, we compare to the best nonensemble and ensemble systems of Surdeanu et al. (2007), Punyakanok et al. (2008) and Toutanova et al. (2008). The ensemble variants of these systems use multiple parses and multiple SRL systems to leverage diversity. In contrast to these ensemble systems, our product-of-experts model uses only a single architecture and one syntactic parse; the constituent models differ only in random initialization. We also compare with the recent deep bidirectional LSTM model of Zhou and Xu (2015). For CoNLL 2012, we compare to Pradhan et al. (2013), who report results with the (non-ensemble) A SSERT system (Pradhan et al., 2005), and to the model of Zhou and Xu (2015). For CoNLL 2009, we compare to the top system"
D15-1112,P10-1040,0,0.0499848,"Missing"
D15-1112,W04-3212,0,0.0271849,"Missing"
D15-1112,P14-2107,0,0.0178692,"Missing"
D15-1112,W09-1208,0,0.013256,"s of these systems use multiple parses and multiple SRL systems to leverage diversity. In contrast to these ensemble systems, our product-of-experts model uses only a single architecture and one syntactic parse; the constituent models differ only in random initialization. We also compare with the recent deep bidirectional LSTM model of Zhou and Xu (2015). For CoNLL 2012, we compare to Pradhan et al. (2013), who report results with the (non-ensemble) A SSERT system (Pradhan et al., 2005), and to the model of Zhou and Xu (2015). For CoNLL 2009, we compare to the top system from the shared task (Zhao et al., 2009), two state-of-the-art systems that employ a reranker (Björkelund et al., 2010; Roth and Woodsend, 2014), and the recent tensor-based model of Lei et al. (2015). We also trained the linear model of Täckström et al. on this dataset (their work omitted this experiment), as a baseline. Finally, for the FrameNet experiments, we compare to the state-of-the-art system of Hermann et al. (2014), which combines a frame-identification model based on W SABIE (Weston et al., 2011) with a log-linear role labeling model. 4.4 Hyperparameters and Initialization There are several hyperparameters in our model ("
D15-1112,P15-1109,0,0.489288,"different scoring formulation for modeling potential arguments. Moreover, they restrict their experiments to CoNLL 2009 semantic dependencies. Roth and Woodsend (2014) improve on the state-of-the-art feature-based system of Björkelund et al. (2010) by adding distributional word representations trained on large unlabeled corpora as features. Collobert and Weston (2007) use a neural network and do not rely on syntactic parses as input. While they use non-standard evaluation, they report accuracy similar to the ASSERT system (Pradhan et al., 2005), to which we compare in Table 4. Very recently, Zhou and Xu (2015) proposed a deep bidirectional LSTM model for SRL that does not rely on syntax trees as input; their approach achieves the best results on CoNLL 2005 and 2012 corpora to date, but unlike this work, they do not report results on FrameNet and CoNLL 2009 dependencies and do not investigate joint learning approaches involving multiple annotation conventions. For FrameNet-style SRL, Kshirsagar et al. (2015) recently proposed the use of PropBankbased features, but their system performance falls short of the state of the art. Roth and Lapata (2015) proposed another approach exploring linguistically m"
D15-1112,W13-3516,0,\N,Missing
D15-1112,C98-1013,0,\N,Missing
D15-1112,W09-1201,0,\N,Missing
D16-1244,P13-2009,0,0.00513434,"Missing"
D16-1244,H05-1079,0,0.0127832,"Missing"
D16-1244,D15-1075,0,0.893374,"Missing"
D16-1244,P16-1139,0,0.401607,"Missing"
D16-1244,N10-1066,0,0.0271427,"Missing"
D16-1244,D16-1053,0,0.61743,"Missing"
D16-1244,P09-1053,1,0.136849,"Missing"
D16-1244,P13-1158,0,0.00714955,"Missing"
D16-1244,H05-1049,0,0.328661,"Missing"
D16-1244,W07-1428,0,0.0744877,"Missing"
D16-1244,P09-5002,0,0.0434403,"Missing"
D16-1244,W09-3714,0,0.056555,"Missing"
D16-1244,N06-1006,0,0.0992132,"Missing"
D16-1244,D08-1084,0,0.264758,"Missing"
D16-1244,W05-1201,0,0.103264,"Missing"
D16-1244,D14-1162,0,0.127079,"Missing"
D16-1244,N16-1170,0,0.754882,"Missing"
D16-1244,Q16-1019,0,0.574921,"Missing"
D18-1028,P16-1231,0,0.0141616,"ordpiece model (Schuster and Nakajima, 2012) with a vocabulary of 16,000. Experimental Design. We train one version of this model on the same set of 23M English examples as the discriminative insertion model from §6.1; we refer to the model trained on this data as Edits. For comparison, we train an identical model on a set of simulated insertions which we create by sampling sentences from Wikipedia and removing contiguous spans of tokens, which we then treat as the insertion phrases. To ensure that this data is reasonably comparable to the WikiAtomicEdits data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Do"
D18-1028,W12-4006,0,0.388033,"a is reasonably comparable to the WikiAtomicEdits data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than using fully-random spans. 312 She is cited as the first female superstar of Hindi Cinema and India ’s Meryl Str"
D18-1028,N13-1055,0,0.0610098,"parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than using fully-random spans. 312 She is cited as the first female superstar of Hindi Cinema and India ’s Meryl Streep Edits General and is the best actress of the film in j"
D18-1028,max-wisniewski-2010-mining,0,0.0968145,"ns, which we then treat as the insertion phrases. To ensure that this data is reasonably comparable to the WikiAtomicEdits data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than using fully-random spans. 312 She is cited"
D18-1028,L18-1008,0,0.0227062,"13). We train this language model for each language on an average of ∼ 500 million tokens from Wikipedia. Second, we evaluate a discriminative model specifically trained on the insertion data (Discriminative Model). This model represents the base sentence using a sentence encoder that produces a contextdependent representation of every word index in the sentence, and then at test time, compares the learned representation of each index with the representation of the phrase p to be inserted. We use a 256-dimensional 2-layer biLSTM encoder, initialized with FastText 300-dimensional word vectors (Mikolov et al., 2018; Grave et al., 2018).7 We hold out 50K and 10K insertion edits for each language as development and test sets, and use the remaining edits (insertions and deletions) as training data. This provides us with at least 1 million examples for training in each language (cf. Table 2). See Supplementary Material for additional details. General LM 68.1 58.7 67.0 69.9 69.0 73.0 72.9 65.5 68.0 Discr. Model 72.9 68.4 70.1 73.4 72.9 74.2 74.3 68.9 71.8 Table 8: Insertion accuracy on the test set. make. For each model, we take a random sample of 50 examples on which the model made a correct prediction and"
D18-1028,D17-1070,0,0.0589149,"Missing"
D18-1028,C12-1044,0,0.175753,"” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than using fully-random spans. 312 She is cited as the first female superstar of Hindi Cinema and India ’s Meryl Streep Edits General and is the best actress of the film in japan and is the best actress of the Indian cinema in june in 2011 and is the best actress of the film industry He is married to Aida Leanca and has two children Edits General and has a daughter in january , and has"
D18-1028,D17-1064,0,0.0514149,"Missing"
D18-1028,D13-1055,0,0.14171,"Missing"
D18-1028,N09-2061,0,0.0135701,"1.1 42.9 Table 2: The number of instances (in millions) of atomic insertions/deletions for each language. 3 3.1 WikiAtomicEdits: Corpus Creation Extracting Edits Wikipedia edits can be accessed through Wikipedia dumps. The edits are stored as diffs on the entire Wikipedia page, meaning some processing is required to reconstruct the changes that were made at the sentence level. We use historical snapshots of each Wikipedia document and compare against subsequent snapshots to extract sentence-level edits. We strip the HTML tags and Wikipedia markup of the page and then run a sentence splitter (Gillick, 2009) to obtain a list of sentences for each snapshot. Rather than run a full, quadratic-time (Myers, 1986) sequence alignment to compare the two lists of sentences, which is infeasible for long articles, we propose an efficient precision-oriented approximation. Given n sentences in one snapshot (“base”) and m sentences in a subsequent one (“edited”), we assume that most edits are local and restrict our attention to a fixed-size window. For each sentence si in the base snapshot, we compute pairwise BLEU scores (Papineni et al., 2002) between si i+k and the sentences {tj }j=i−k (k = 5) in the edited"
D18-1028,L18-1550,0,0.0242656,"guage model for each language on an average of ∼ 500 million tokens from Wikipedia. Second, we evaluate a discriminative model specifically trained on the insertion data (Discriminative Model). This model represents the base sentence using a sentence encoder that produces a contextdependent representation of every word index in the sentence, and then at test time, compares the learned representation of each index with the representation of the phrase p to be inserted. We use a 256-dimensional 2-layer biLSTM encoder, initialized with FastText 300-dimensional word vectors (Mikolov et al., 2018; Grave et al., 2018).7 We hold out 50K and 10K insertion edits for each language as development and test sets, and use the remaining edits (insertions and deletions) as training data. This provides us with at least 1 million examples for training in each language (cf. Table 2). See Supplementary Material for additional details. General LM 68.1 58.7 67.0 69.9 69.0 73.0 72.9 65.5 68.0 Discr. Model 72.9 68.4 70.1 73.4 72.9 74.2 74.3 68.9 71.8 Table 8: Insertion accuracy on the test set. make. For each model, we take a random sample of 50 examples on which the model made a correct prediction and 50 examples on which"
D18-1028,P02-1040,0,0.105117,"L tags and Wikipedia markup of the page and then run a sentence splitter (Gillick, 2009) to obtain a list of sentences for each snapshot. Rather than run a full, quadratic-time (Myers, 1986) sequence alignment to compare the two lists of sentences, which is infeasible for long articles, we propose an efficient precision-oriented approximation. Given n sentences in one snapshot (“base”) and m sentences in a subsequent one (“edited”), we assume that most edits are local and restrict our attention to a fixed-size window. For each sentence si in the base snapshot, we compute pairwise BLEU scores (Papineni et al., 2002) between si i+k and the sentences {tj }j=i−k (k = 5) in the edited snapshot. We consider the sentence with the highest BLEU score in this window as a candidate. If the sentences are not identical and the difference consists of an insertion or deletion of a single contiguous phrase2 , we add this example to the corpus. For each article, we run this algorithm over the most recent 100,000 snapshots as of February 2018. We extract edits for 8 languages. Statistics are shown in Table 2. 1. The original sentence s does not effectively communicate some piece of information. 2. A reasonable reader of"
D18-1028,D14-1162,0,0.0809643,"n to reporting standard LM perplexity, we compute two measures of performance, which are intended to provide an intuitive picture of how well each model captures the nature of the information that is introduced by the human editors. Specifically, we compute Exact Match as the proportion of sentences for which the model produced the gold phrase (i.e. the phrase inserted by the human editor) somewhere among the top 10 phrases. We also compute Similarity@1 as the mean cosine similarity of each top-ranked phrase and respective gold phrase over the test set. We use the sum of the Glove embeddings (Pennington et al., 2014) of each word in the phrase as a simple approximation of the phrase vector. Table 11 shows the results. We see that, compared to the model trained on General Wikipedia, the model trained on WikiAtomicEdits generates edits which are more similar to the human insertions, according to all of our metrics. Table 10 provides a few qualitative examples of how the phrases generated by the Edits model differ from those generated by the General model. Specifically, we see that the Edits model proposes phrases which better capture the discourse function of the human edit: e.g. providing context for/elabo"
D18-1028,N18-1202,0,0.0802425,"Missing"
D18-1028,Q18-1031,0,0.0624248,"Missing"
D18-1028,P14-2066,0,0.0255462,"e(s)) have been POS-tagged and dependency parsed (Andor et al., 2016) as well as scored using a SOTA LM (Jozefowicz et al., 2016). We also release the 5K 5-way human insertion annotations for English, and 1K 3-way annotations each for Spanish and German, as described in §4. Table 11: Comparison of how closely each model’s generated phrases match the phrase inserted by the human editor. “Edits” was trained on WikiAtomicEdits and “General” was trained on comparable data not derived from human edits. We consider the top 10 phrases generated by each model. and to better understand argumentation (Tan and Lee, 2014). Particular attention has been given to spam edits (Adler et al., 2011) and editor quality (Leskovec et al., 2010). Our work differs in that WikiAtomicEdits is much larger than currently available corpora, both by number of languages and by size of individual languages. In addition, our focus on atomic edits should facilitate more controlled studies of semantics and discourse. 8 Conclusion We have introduced the WikiAtomicEdits corpus, derived from Wikipedia’s edit history, which contains 43M examples of atomic insertions and deletions in 8 languages. We have shown that the language in this c"
D18-1028,D15-1059,0,0.0126598,"index i, generate candidate phrases that would be appropriate to insert into s at i (§6.2). 6.1 Predicting Insertion Locations Task. This task–given a phrase p and a sentence s, choose the best index i in s at which to insert p–is identical to the task we asked humans to perform in §4. We consider two simple models for performing this task: a basic language model and a discriminative model trained on the insertion data. We report performance as overall accuracy. We analyze whether a model which is trained to 6 We note that the addition of “former” is likely tied to changes in the real world (Wijaya et al., 2015). 310 model insertions directly captures something different than a general language model in terms of the types of errors each model makes. German English Spanish French Italian Japanese Russian Chinese Average Models. We evaluate two models. First, we evaluate a standard language modeling baseline (General LM), in which we simply insert the phrase p at every possible point in s and chose the index which yields the lowest perplexity. We use the LSTM language model from Jozefowicz et al. (2016), which obtained SOTA results on language modeling on the one billion words benchmark for English (Ch"
D18-1028,P08-2035,0,0.0313163,"g sentences from Wikipedia and removing contiguous spans of tokens, which we then treat as the insertion phrases. To ensure that this data is reasonably comparable to the WikiAtomicEdits data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more reali"
D18-1028,D17-1213,0,0.555849,"n at both the phrase and the sentence level. We mine Wikipedia edit history to create a corpus of 43 million atomic insertion and deletion edits covering 8 languages. We argue that the corpus contains distinct semantic signals not present in raw text. We thus focus our experiments on answering the following questions: Introduction Written language often undergoes several rounds of revision as human authors determine exactly what information they want their words to convey. On Wikipedia, this process is carried out collectively by a large community at a rate of nearly two revisions per second (Yang et al., 2017). While Wikipedia’s revision history contains arbitrarily complex edits, our corpus and analysis focuses on atomic insertion edits: instances in which an editor has inserted a single, contiguous span of text into an existing complete sentence (Table 1). This restriction allows us to make several assumptions which we believe make the data an especially powerful source of signal. Namely, we can assume that 1) some information was not communicated by the original sentence, 2) that information should have been communicated (according to a human editor), and 3) that information is communicated by t"
D18-1028,N10-1056,0,0.06066,"and removing contiguous spans of tokens, which we then treat as the insertion phrases. To ensure that this data is reasonably comparable to the WikiAtomicEdits data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than us"
D18-1028,E12-1054,0,0.080273,"its data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than using fully-random spans. 312 She is cited as the first female superstar of Hindi Cinema and India ’s Meryl Streep Edits General and is the best act"
D18-1028,W10-3504,0,\N,Missing
D18-1080,D11-1038,0,0.525718,"its. Figure 1 gives an example of how a Wikipedia editor rewrote a single sentence into two simpler ones. We create WikiSplit, a set of one million such examples mined from English Wikipedia, and show that models trained with this resource produce dramatically better output for split and rephrase. A complex sentence can typically be rewritten into multiple simpler ones that together retain the same meaning. Performing this split-and-rephrase task is one of the main operations in text simplification, alongside paraphrasing and dropping less salient content (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSpli"
D18-1080,P18-2114,0,0.345794,"ddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in the data splits, and demonstrate that neural ∗ Our primary contributions are: • A scalable, language agnostic method for extracting split-and-rephrase rewrites from Wikipedia edits. • Public release of the English WikiSplit dataset, containing one million rewrites: http://goo.gl/language/wiki-split • By incorporating WikiSplit into training, we more than double (30.5 to 62.4) the BLEU score obtained on WebSplit by Aharoni and Goldberg (2018). Both authors contributed equally. 732 Proceedings of the 2018 Conference on Empirical Methods in Natural Languag"
D18-1080,Q15-1021,0,0.177909,"kipedia, and show that models trained with this resource produce dramatically better output for split and rephrase. A complex sentence can typically be rewritten into multiple simpler ones that together retain the same meaning. Performing this split-and-rephrase task is one of the main operations in text simplification, alongside paraphrasing and dropping less salient content (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in the data splits, and"
D18-1080,P17-1017,0,0.0261232,"oldberg, 2018). For training, we delimit the simple sentences with a special symbol. We depart from the prior work by only using a subset of the WebSplit training set: we take a fixed sub-sample such that each distinct C is paired with a single S, randomly selected from the multiple possibilities in the dataset. This scheme produced superior performance in preliminary experiments. As a quality measure, we report multi-reference corpus-level BLEU4 (Papineni et al., 2002), but Comparison to WebSplit Narayan et al. (2017) derived the WebSplit corpus by matching up sentences in the WebNLG corpus (Gardent et al., 2017) according to partitions of their underlying meaning representations (RDF triples). The WebNLG corpus itself was created by having crowd workers write sentential realizations of one or more RDF triples. The resulting language is often unnatural, for example, “Akeem Dent once played for the Houston Texans team which is based in Houston in Texas.”2 Repetition arises because the same sentence fragment may appear in many different examples. 3 We use WebSplit v1.0 throughout, which is the scaledup re-release by Narayan et al. (2017) at http://github. com/shashiongithub/Split-and-Rephrase, commit a9"
D18-1080,P08-2035,0,0.0421371,"lter out misaligned pairs, we use BLEU scores (Papineni et al., 2002) to ensure similarity between the original and the split versions. Specifically, we discard pairs where BLEU(C, S1 ) or BLEU(C, S2 ) is less than δ (an empirically chosen threshold). If multiple candidates remain for a given sentence C, we retain arg maxS (BLEU(C, S1 ) + BLEU(C, S2 )).1 Mining Wikipedia Edits Wikipedia maintains snapshots of entire documents at different timestamps, which makes it possible to reconstruct edit histories for documents. This has been exploited for many NLP tasks, including sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011; Tonelli et al., 2016) and modeling semantic edit intentions (Yang et al., 2017). To construct the WikiSplit corpus, we identify edits that involve sentences being split. A list of sentences for each snapshot is obtained by stripping HTML tags and Wikipedia markup and running a sentence break detector (Gillick, 2009). Temporally adjacent snapshots of a Wikipedia page are then compared to check for sentences that have undergone a split like that shown in Figure 1. We search for splits in both temporal directions. Given all c"
D18-1080,N09-2061,0,0.0244161,"napshots of entire documents at different timestamps, which makes it possible to reconstruct edit histories for documents. This has been exploited for many NLP tasks, including sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011; Tonelli et al., 2016) and modeling semantic edit intentions (Yang et al., 2017). To construct the WikiSplit corpus, we identify edits that involve sentences being split. A list of sentences for each snapshot is obtained by stripping HTML tags and Wikipedia markup and running a sentence break detector (Gillick, 2009). Temporally adjacent snapshots of a Wikipedia page are then compared to check for sentences that have undergone a split like that shown in Figure 1. We search for splits in both temporal directions. Given all candidate examples extracted this way, we use a high-precision heuristic to retain only high quality splits. To extract a full sentence C and its candidate split into S = (S1 , S2 ), we 2.2 Corpus Statistics and Quality Our extraction heuristic is imperfect, so we manually assess corpus quality using the same categorization schema proposed by Aharoni and Goldberg (2018); see Table 1 for"
D18-1080,D17-1213,0,0.0136428,"d pairs where BLEU(C, S1 ) or BLEU(C, S2 ) is less than δ (an empirically chosen threshold). If multiple candidates remain for a given sentence C, we retain arg maxS (BLEU(C, S1 ) + BLEU(C, S2 )).1 Mining Wikipedia Edits Wikipedia maintains snapshots of entire documents at different timestamps, which makes it possible to reconstruct edit histories for documents. This has been exploited for many NLP tasks, including sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011; Tonelli et al., 2016) and modeling semantic edit intentions (Yang et al., 2017). To construct the WikiSplit corpus, we identify edits that involve sentences being split. A list of sentences for each snapshot is obtained by stripping HTML tags and Wikipedia markup and running a sentence break detector (Gillick, 2009). Temporally adjacent snapshots of a Wikipedia page are then compared to check for sentences that have undergone a split like that shown in Figure 1. We search for splits in both temporal directions. Given all candidate examples extracted this way, we use a high-precision heuristic to retain only high quality splits. To extract a full sentence C and its candid"
D18-1080,P16-1154,0,0.06498,"ata produces a model with qualitatively better predictions that score 32 BLEU points above the prior best result on the WebSplit benchmark. 1 A classic leaf symptom is the appearance of angular, water-soaked lesions between the veins. The angular appearance results where the lesion edge and vein meet. Figure 1: A split-and-rephrase example extracted from a Wikipedia edit, where the top sentence had been edited into two new sentences by removing some words (yellow) and adding others (blue). encoder-decoder models (Bahdanau et al., 2014) perform poorly, even when enhanced with a copy mechanism (Gu et al., 2016; See et al., 2017). Introduction One limitation of the WebSplit examples themselves is that they contain fairly unnatural linguistic expression using a small vocabulary. We introduce new training data mined from Wikipedia edit histories that have some noise, but which have a rich and varied vocabulary over naturally expressed sentences and their extracted splits. Figure 1 gives an example of how a Wikipedia editor rewrote a single sentence into two simpler ones. We create WikiSplit, a set of one million such examples mined from English Wikipedia, and show that models trained with this resourc"
D18-1080,N10-1056,0,0.0293049,"pineni et al., 2002) to ensure similarity between the original and the split versions. Specifically, we discard pairs where BLEU(C, S1 ) or BLEU(C, S2 ) is less than δ (an empirically chosen threshold). If multiple candidates remain for a given sentence C, we retain arg maxS (BLEU(C, S1 ) + BLEU(C, S2 )).1 Mining Wikipedia Edits Wikipedia maintains snapshots of entire documents at different timestamps, which makes it possible to reconstruct edit histories for documents. This has been exploited for many NLP tasks, including sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011; Tonelli et al., 2016) and modeling semantic edit intentions (Yang et al., 2017). To construct the WikiSplit corpus, we identify edits that involve sentences being split. A list of sentences for each snapshot is obtained by stripping HTML tags and Wikipedia markup and running a sentence break detector (Gillick, 2009). Temporally adjacent snapshots of a Wikipedia page are then compared to check for sentences that have undergone a split like that shown in Figure 1. We search for splits in both temporal directions. Given all candidate examples extracted this way, we us"
D18-1080,D15-1076,0,0.0320725,"iddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in the data splits, and demonstrate that neural ∗ Our primary contributions are: • A scalable, language agnostic method for extracting split-and-rephrase rewrites from Wikipedia edits. • Public release of the English WikiSplit dataset, containing one million rewrites: http://goo.gl/language/wiki-split • By incorporating WikiSplit into training, we more than double (30.5 to 62.4) the BLEU score obtaine"
D18-1080,D17-1004,0,0.0332047,"simpler ones that together retain the same meaning. Performing this split-and-rephrase task is one of the main operations in text simplification, alongside paraphrasing and dropping less salient content (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in the data splits, and demonstrate that neural ∗ Our primary contributions are: • A scalable, language agnostic method for extracting split-and-rephrase rewrites from Wikipedia edits. • Public release"
D18-1080,P17-4012,0,0.0291392,"(sBLEU) for direct comparison to past work.5 We also report lengthbased statistics to quantify splitting. We use the same sequence-to-sequence architecture that produced the top result for Aharoni and Goldberg (2018), “Copy512”, which is a onelayer, bi-directional LSTM (cell size 512) with attention (Bahdanau et al., 2014) and a copying mechanism (See et al., 2017) that dynamically interpolates the standard word distribution with a distribution over the words in the input sentence. Training details are as described in the Appendix of Aharoni and Goldberg (2018) using the OpenNMT-py framework (Klein et al., 2017).6 3.1 58.7 55.7 30.5 34.2 60.4 62.4 sBLEU – 56.1 53.0 25.5 30.5 58.0 60.1 In contrast, the W IKI S PLIT model achieves 59.4 BLEU on the WebSplit validation set, without observing any in-domain data. It also outperforms the two deterministic baselines on both validation sets by a non-trivial BLEU margin. This indicates that the WikiSplit training data enable better generalization than when using WebSplit by itself. Reintroducing the downsampled, in-domain training data (B OTH) further improves performance on the WebSplit evaluation. Results We compare to the S OURCE baseline, which is the prev"
D18-1080,C10-1152,0,0.275307,"heir extracted splits. Figure 1 gives an example of how a Wikipedia editor rewrote a single sentence into two simpler ones. We create WikiSplit, a set of one million such examples mined from English Wikipedia, and show that models trained with this resource produce dramatically better output for split and rephrase. A complex sentence can typically be rewritten into multiple simpler ones that together retain the same meaning. Performing this split-and-rephrase task is one of the main operations in text simplification, alongside paraphrasing and dropping less salient content (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. ("
D18-1080,W17-3204,0,0.0248613,"same meaning. Performing this split-and-rephrase task is one of the main operations in text simplification, alongside paraphrasing and dropping less salient content (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in the data splits, and demonstrate that neural ∗ Our primary contributions are: • A scalable, language agnostic method for extracting split-and-rephrase rewrites from Wikipedia edits. • Public release of the English WikiSplit dataset, containi"
D18-1080,D17-1064,0,0.516619,"alex,jasonbaldridge,dipanjand}@google.com Google AI Language A classic leaf symptom is water-soaked lesions between the veins which appear as angular leaf-spots where the lesion edge and vein meet. Abstract Split and rephrase is the task of breaking down a sentence into shorter ones that together convey the same meaning. We extract a rich new dataset for this task by mining Wikipedia’s edit history: WikiSplit contains one million naturally occurring sentence rewrites, providing sixty times more distinct split examples and a ninety times larger vocabulary than the WebSplit corpus introduced by Narayan et al. (2017) as a benchmark for this task. Incorporating WikiSplit as training data produces a model with qualitatively better predictions that score 32 BLEU points above the prior best result on the WebSplit benchmark. 1 A classic leaf symptom is the appearance of angular, water-soaked lesions between the veins. The angular appearance results where the lesion edge and vein meet. Figure 1: A split-and-rephrase example extracted from a Wikipedia edit, where the top sentence had been edited into two new sentences by removing some words (yellow) and adding others (blue). encoder-decoder models (Bahdanau et a"
D18-1080,P02-1040,0,0.102593,"t for training them. To that end, we introduce the WikiSplit corpus and detail its construction next. 2.1 Correct 161 168 169 Unsupp. 35 35 31 Miss. 6 4 4 Size 1.4m 1.0m 0.5m Table 2: Quality vs corpus size trade-off when setting the similarity threshold. The counts are for a random sample of 100 split-and-rephrase examples extracted using our method (i.e., 200 simple sentences). Keys: Unsupported; Missing require that C and S1 have the same trigram prefix, C and S2 have the same trigram suffix, and S1 and S2 have different trigram suffixes. To filter out misaligned pairs, we use BLEU scores (Papineni et al., 2002) to ensure similarity between the original and the split versions. Specifically, we discard pairs where BLEU(C, S1 ) or BLEU(C, S2 ) is less than δ (an empirically chosen threshold). If multiple candidates remain for a given sentence C, we retain arg maxS (BLEU(C, S1 ) + BLEU(C, S2 )).1 Mining Wikipedia Edits Wikipedia maintains snapshots of entire documents at different timestamps, which makes it possible to reconstruct edit histories for documents. This has been exploited for many NLP tasks, including sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010"
D18-1080,P17-1099,0,0.206771,"del with qualitatively better predictions that score 32 BLEU points above the prior best result on the WebSplit benchmark. 1 A classic leaf symptom is the appearance of angular, water-soaked lesions between the veins. The angular appearance results where the lesion edge and vein meet. Figure 1: A split-and-rephrase example extracted from a Wikipedia edit, where the top sentence had been edited into two new sentences by removing some words (yellow) and adding others (blue). encoder-decoder models (Bahdanau et al., 2014) perform poorly, even when enhanced with a copy mechanism (Gu et al., 2016; See et al., 2017). Introduction One limitation of the WebSplit examples themselves is that they contain fairly unnatural linguistic expression using a small vocabulary. We introduce new training data mined from Wikipedia edit histories that have some noise, but which have a rich and varied vocabulary over naturally expressed sentences and their extracted splits. Figure 1 gives an example of how a Wikipedia editor rewrote a single sentence into two simpler ones. We create WikiSplit, a set of one million such examples mined from English Wikipedia, and show that models trained with this resource produce dramatica"
D18-1080,D16-1033,0,0.104655,"Missing"
D18-1091,D15-1159,0,0.0213141,"fication. Model We use a feed-forward neural network with 2 hidden layers with ReLU activations (Glorot et al., 2011) on each layer and a softmax at the output layer predicting 0 or 1. We extract a variety of features from the query which can be helpful in the classification. We extract character-3, 4-grams and word-1, 2-grams as they can be helpful in capturing spelling errors. In addition to lexical features, we also extract syntactic features that can inform the model on any anomaly in the structure of the query. Specifically, we annotate the query with POS-tags using SyntaxNet POS tagger (Alberti et al., 2015) and extract POS-1, 2, 3-grams.3 Every feature in the network is represented as a real-valued embedding. All the n-grams embeddings of every feature type are summed together and concatenated to form the input layer as shown in Figure 2. The model is trained using crossentropy loss against the gold labels for each query. The hyperparameters are tuned to maximize accuracy on the dev set and results are reported on the test set. Model majority class baseline word bi-LSTM baseline question word baseline word-1 word-1, 2 word-1, 2 char-3, 4 word-1, 2 POS-1, 2, 3 word-1, 2 char-3, 4 POS-1, 2, 3 Appr"
D18-1091,P17-1123,0,0.0854325,"del using wellformedness probability. Sentence: montana is home to the rocky mountain elk foundation and has a historic big game hunting tradition. Gold question: what is the name of the big game hunting foundation in montana? seq2seq: what is a historic big game hunting tradition? (pwf = 0.7) Reranked: what is the name of the historic big game tradition? (pwf = 0.8) Improving Question Generation Automatic question generation is the task of generating questions that ask about the information or facts present in either a given sentence or paragraph (Vanderwende, 2008; Heilman and Smith, 2010). Du et al. (2017) present a state-of-theart neural sequence-to-sequence model to generate questions from a given sentence/paragraph. The model used is an attention-based encoder-decoder network (Bahdanau et al., 2015), where the encoder reads in a given text and the decoder is an LSTM RNN that produces the question by predicting one word at a time. Du et al. (2017) use the SQuAD questionanswering dataset (Rajpurkar et al., 2016) to develop a question generation dataset by pairing sentences from the text with the corresponding questions. The question generation dataset contains approx 70k, 10k, and 12k training"
D18-1091,P13-1158,0,0.0683087,"t not a question Ungrammatical and not a question Grammatical and an explicit question An explicit question but ungrammatical Table 1: Examples of well-formed and non-wellformed queries according to the annotation guideline. 2 Query (q) population of owls just in north america? who disscoverd rihanna? what countries have genocide happened in? what is released when an ion is formed? Well-formed Natural Language Question Classifier In this section we describe the data annotation, and the models used for question well-formedness classification. 2.1 Dataset Construction We use the Paralex corpus (Fader et al., 2013) that contains pairs of noisy paraphrase questions. These questions were issued by users in WikiAnswers (a Question-Answer forum) and consist of both web-search query like constructs (“5 parts of chloroplast?”) and well-formed questions (“What is the punishment for grand theft?”), and thus is a good resource for constructing the question well-formedness dataset. We select 25,100 queries from the unique list of queries extracted from the corpus such that no two queries in the selected set are paraphrases. The queries are then annotated into well-formed or non-wellformed questions. We define a q"
D18-1091,D08-1107,0,0.100356,"Missing"
D18-1091,N10-1086,0,0.031318,"seq question generation model using wellformedness probability. Sentence: montana is home to the rocky mountain elk foundation and has a historic big game hunting tradition. Gold question: what is the name of the big game hunting foundation in montana? seq2seq: what is a historic big game hunting tradition? (pwf = 0.7) Reranked: what is the name of the historic big game tradition? (pwf = 0.8) Improving Question Generation Automatic question generation is the task of generating questions that ask about the information or facts present in either a given sentence or paragraph (Vanderwende, 2008; Heilman and Smith, 2010). Du et al. (2017) present a state-of-theart neural sequence-to-sequence model to generate questions from a given sentence/paragraph. The model used is an attention-based encoder-decoder network (Bahdanau et al., 2015), where the encoder reads in a given text and the decoder is an LSTM RNN that produces the question by predicting one word at a time. Du et al. (2017) use the SQuAD questionanswering dataset (Rajpurkar et al., 2016) to develop a question generation dataset by pairing sentences from the text with the corresponding questions. The question generation dataset contains approx 70k, 10k"
D18-1091,D07-1086,0,0.104793,"Missing"
D18-1091,N16-1062,0,0.0686451,"Missing"
D18-1091,D15-1075,0,0.064273,"Missing"
D18-1091,P09-1097,0,0.0804238,"Missing"
D18-1091,E06-1032,0,0.0608393,"nd BLEU-4 scores.6 Table 4 shows that the reranked question selected using our query well-formedness clas6 BLEU-1 41.3 41.6 Figure 3: Example showing question selection from the n-best list using our reranking model. sifier improves the BLEU-4 score of a seq-toseq question generation model from 12.0 to 12.2. The oracle improvement, by selecting the sentence from the list that maximizes the BLEU-4 score is 15.2. However, its worth noting that an increase in well-formedness doesn’t guarantee an improved BLEU score, as the oracle sentence maximizing the BLEU score might be fairly non-wellformed (Callison-Burch et al., 2006). For example, “who was elected the president of notre dame in?” has a higher BLEU score to the reference “who was the president of notre dame in 1934?” than our wellformed question “who was elected the president of notre dame?”. Figure 3 shows a question generation example with the output of Du et al. (2017) as the baseline result and the reranked question using the wellformed probability. 4 Related Work We have referenced much of the related work throughout the paper. We now review another orthogonally related field of work. Grammatical error correction (GEC) is the task of correcting the gr"
D18-1091,P16-1170,0,0.0476613,"Missing"
D18-1091,W14-1701,0,0.0165175,"of notre dame in?” has a higher BLEU score to the reference “who was the president of notre dame in 1934?” than our wellformed question “who was elected the president of notre dame?”. Figure 3 shows a question generation example with the output of Du et al. (2017) as the baseline result and the reranked question using the wellformed probability. 4 Related Work We have referenced much of the related work throughout the paper. We now review another orthogonally related field of work. Grammatical error correction (GEC) is the task of correcting the grammatical errors (if any) in a piece of text (Ng et al., 2014). As GEC includes not just identification of ungrammatical text but also correcting the text to produce grammatical text, its a more complex task. However, grammatical error prediction (Schmaltz et al., 2016; Daudaravicius et al., 2016) is the task of classifying whether or not a sentence is grammatical, which is more closely related to BLEU-x uses precision computed over [1, x]-grams. 801 our task as classifying a question as well-formed requires making judgement on both the style and grammar of the text. 5 Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language par"
D18-1091,D17-1061,0,0.10441,"Missing"
D18-1091,P02-1040,0,0.10072,"ing dataset (Rajpurkar et al., 2016) to develop a question generation dataset by pairing sentences from the text with the corresponding questions. The question generation dataset contains approx 70k, 10k, and 12k training, development and test examples. Their current best model selects the top ranked question from the n-best list produced by the decoder as the output. We augment their system by training a discriminative reranker (Collins and Koo, 2005) with the model score of the question generation model and the wellformedness probability of our classifier as features to optimize BLEU score (Papineni et al., 2002) between the selected question from the 10-best list and the reference question on the development set. We then use this reranker to select the best question from the 10-best list of the test set. We use the evaluation package released by Chen et al. (2015) to compute BLEU-1 and BLEU-4 scores.6 Table 4 shows that the reranked question selected using our query well-formedness clas6 BLEU-1 41.3 41.6 Figure 3: Example showing question selection from the n-best list using our reranking model. sifier improves the BLEU-4 score of a seq-toseq question generation model from 12.0 to 12.2. The oracle im"
D18-1091,D16-1264,0,0.0591965,"c question generation is the task of generating questions that ask about the information or facts present in either a given sentence or paragraph (Vanderwende, 2008; Heilman and Smith, 2010). Du et al. (2017) present a state-of-theart neural sequence-to-sequence model to generate questions from a given sentence/paragraph. The model used is an attention-based encoder-decoder network (Bahdanau et al., 2015), where the encoder reads in a given text and the decoder is an LSTM RNN that produces the question by predicting one word at a time. Du et al. (2017) use the SQuAD questionanswering dataset (Rajpurkar et al., 2016) to develop a question generation dataset by pairing sentences from the text with the corresponding questions. The question generation dataset contains approx 70k, 10k, and 12k training, development and test examples. Their current best model selects the top ranked question from the n-best list produced by the decoder as the output. We augment their system by training a discriminative reranker (Collins and Koo, 2005) with the model score of the question generation model and the wellformedness probability of our classifier as features to optimize BLEU score (Papineni et al., 2002) between the s"
D18-1091,W16-0528,0,0.0211642,"uestion generation example with the output of Du et al. (2017) as the baseline result and the reranked question using the wellformed probability. 4 Related Work We have referenced much of the related work throughout the paper. We now review another orthogonally related field of work. Grammatical error correction (GEC) is the task of correcting the grammatical errors (if any) in a piece of text (Ng et al., 2014). As GEC includes not just identification of ungrammatical text but also correcting the text to produce grammatical text, its a more complex task. However, grammatical error prediction (Schmaltz et al., 2016; Daudaravicius et al., 2016) is the task of classifying whether or not a sentence is grammatical, which is more closely related to BLEU-x uses precision computed over [1, x]-grams. 801 our task as classifying a question as well-formed requires making judgement on both the style and grammar of the text. 5 Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Comput. Linguist., 31(1):25–70. Ann A Copestake and Dan Flickinger. 2000. An open source grammar development environment and broad-coverage english grammar using HPSG. In Proc. of LREC. Conclusion We p"
D18-1091,W17-4121,1,0.887661,"Missing"
I08-1035,P01-1017,0,0.0107517,"cribing them. The dataset showed regularity in sentence structure and belonged to a closed domain, making the variations in surface forms more constrained than completely free text. After sentence segmentation we arrived at a set of 3262 sentences. From this set, we selected 3000 for template extraction and kept aside 262 sentences for testing. 3.2 Preprocessing For semantic analysis, we used the ASSERT toolkit (Pradhan et al., 2004) that produces shallow semantic parses using the PropBank conventions. As a by product, it also produces syntactic parses of sentences, using the Charniak parser (Charniak, 2001). For each sentence, we maintained a part-of-speech tagged (leaves of the parse tree), parsed, baseNP2 tagged and semantic role tagged version. The baseNPs were retrieved by pruning the parse trees and not by using a separate NP chunker. The reason for having a baseNP tagged corpus will become clear as we go into the detail of our template extraction techniques. Figure 1 shows a typical output from the Charniak parser and Figure 2 shows the same tree with nodes under the baseNPs pruned. We identified the need to have a domain entity tagger for matching constituents in the sentences. 266 1 2 ht"
I08-1035,P06-2027,0,0.0191762,"erns from un-annotated text starting from seed patterns. Once again, the text analysis is not deep and the patterns extracted are not sentence surface forms. (Collier, 1998) proposed automatic domain template extraction for IE purposes where MUC type templates for particular types of events were constructed. The method relies on the idea from (Luhn, 1958) where statistically significant words of a corpus were extracted. Based on these words, sentences containing them were chosen and aligned using subject-object-verb patterns. However, this method did not look at arbitrary syntactic patterns. (Filatova et al., 2006) improved the paradigm by looking at the most frequent verbs occurring in a corpus and aligning subtrees containing the verb, by using the syntactic parses as a similarity metric. However, long distance dependencies of verbs with constituents were not looked at and deep semantic analysis was not performed on the sentences to find out similar verb subcategorization frames. In contrast, in our predicate argument based approach we look into deeper semantic structures, and align sentences not only based on similar syntactic parses, but also based on the constituents’ roles with respect to the main"
I08-1035,W04-1013,0,0.0401119,"ION ] will move [ DIRECTION ] across [ LOCATION ] [ TIME ] In the current work we address the first problem of automatically extracting domain templates from human written reports. We take a two-step approach to the problem; first, we cluster report sentences based on similarity and second, we extract template(s) corresponding to each cluster by aligning the instances in the cluster. We experimented with two independent, arguably complementary techniques for clustering and aligning – a predicate argument based approach that extracts more general templates containing one predicate and a ROUGE (Lin, 2004) based 265 approach that can extract templates containing multiple verbs. As we will see below, both approaches show promise. 2 Related Work There has been instances of template based summarization in popular Information Extraction (IE) evaluations like MUC (Marsh & Perzanowski, 1998; Onyshkevych, 1994) and ACE (ACE, 2007) where hand engineered slots were to be filled for events in text; but the focus lay on template filling rather than their creation. (Riloff, 1996) describes an interesting work on the generation of extraction patterns from untagged text, but the analysis is syntactic and the"
I08-1035,H94-1031,0,0.077152,"xtract template(s) corresponding to each cluster by aligning the instances in the cluster. We experimented with two independent, arguably complementary techniques for clustering and aligning – a predicate argument based approach that extracts more general templates containing one predicate and a ROUGE (Lin, 2004) based 265 approach that can extract templates containing multiple verbs. As we will see below, both approaches show promise. 2 Related Work There has been instances of template based summarization in popular Information Extraction (IE) evaluations like MUC (Marsh & Perzanowski, 1998; Onyshkevych, 1994) and ACE (ACE, 2007) where hand engineered slots were to be filled for events in text; but the focus lay on template filling rather than their creation. (Riloff, 1996) describes an interesting work on the generation of extraction patterns from untagged text, but the analysis is syntactic and the patterns do not resemble the templates that we aim to extract. (Yangarber et al., 2000) describe another system called ExDisco, that extracts event patterns from un-annotated text starting from seed patterns. Once again, the text analysis is not deep and the patterns extracted are not sentence surface"
I08-1035,2001.mtsummit-papers.68,0,0.0172638,"ucture are tagged alongside. we manually looked at the output clusters and made a judgement call whether the candidate clusters are reasonably coherent and potentially correspond to templates. The reason for the poor performance of the approach was the classical parameter estimation problem of determining a priori the number of clusters. We could not find an elegant solution for the problem without losing the motivation of an automated approach. 4.2 A ROUGE Based Approach ROUGE (Lin, 2004) is the standard automatic evaluation metric in the Summarization community. It is derived from the BLEU (Papineni et al., 2001) score which is the evaluation metric used in the Machine Translation community. The underlying idea in the metric is comparing the candidate and the reference sentences (or summaries) based on their token co-occurrence statistics. For example, a unigram based measure would compare the vocabulary overlap between the candidate and reference sentences. Thus, intuitively, we may use the ROUGE score as a measure for clustering the sentences. Amongst the various ROUGE statistics, the most appealing is Weighted Longest Common Subsequence(WLCS). WLCS favors contiguous LCS which corresponds to the int"
I08-1035,N04-1030,0,0.0231612,"ext, we used a corpus from the domain of weather forecasts (Reiter et al., 2005). This is a freely available parallel corpus1 consisting of weather data and human written forecasts describing them. The dataset showed regularity in sentence structure and belonged to a closed domain, making the variations in surface forms more constrained than completely free text. After sentence segmentation we arrived at a set of 3262 sentences. From this set, we selected 3000 for template extraction and kept aside 262 sentences for testing. 3.2 Preprocessing For semantic analysis, we used the ASSERT toolkit (Pradhan et al., 2004) that produces shallow semantic parses using the PropBank conventions. As a by product, it also produces syntactic parses of sentences, using the Charniak parser (Charniak, 2001). For each sentence, we maintained a part-of-speech tagged (leaves of the parse tree), parsed, baseNP2 tagged and semantic role tagged version. The baseNPs were retrieved by pruning the parse trees and not by using a separate NP chunker. The reason for having a baseNP tagged corpus will become clear as we go into the detail of our template extraction techniques. Figure 1 shows a typical output from the Charniak parser"
I08-1035,P03-1002,0,0.0317515,"h we look into deeper semantic structures, and align sentences not only based on similar syntactic parses, but also based on the constituents’ roles with respect to the main predicate. Also, they relied on typical Named Entities (NEs) like location, organization, person etc. and included another entity that they termed as NUMBER. However, for specific domains like weather forecasts, medical reports or student reports, more varied domain entities form slots in templates, as we observe in our data; hence, existence of a module handling domain specific entities become essential for such a task. (Surdeanu et al., 2003) identify arguments for predicates in a sentence and emphasize how semantic role information may assist in IE related tasks, but their primary focus remained on the extraction of PropBank (Kingsbury et al., 2002) type semantic roles. To our knowledge, the ROUGE metric has not been used for automatic extraction of templates. 3 The Data 3.1 Data Description Since our focus is on creating summary items from events or structured data rather than from text, we used a corpus from the domain of weather forecasts (Reiter et al., 2005). This is a freely available parallel corpus1 consisting of weather"
I08-1035,C00-2136,0,0.0217882,"e verbs. As we will see below, both approaches show promise. 2 Related Work There has been instances of template based summarization in popular Information Extraction (IE) evaluations like MUC (Marsh & Perzanowski, 1998; Onyshkevych, 1994) and ACE (ACE, 2007) where hand engineered slots were to be filled for events in text; but the focus lay on template filling rather than their creation. (Riloff, 1996) describes an interesting work on the generation of extraction patterns from untagged text, but the analysis is syntactic and the patterns do not resemble the templates that we aim to extract. (Yangarber et al., 2000) describe another system called ExDisco, that extracts event patterns from un-annotated text starting from seed patterns. Once again, the text analysis is not deep and the patterns extracted are not sentence surface forms. (Collier, 1998) proposed automatic domain template extraction for IE purposes where MUC type templates for particular types of events were constructed. The method relies on the idea from (Luhn, 1958) where statistically significant words of a corpus were extracted. Based on these words, sentences containing them were chosen and aligned using subject-object-verb patterns. How"
I08-1035,P02-1040,0,\N,Missing
I08-1035,M98-1002,0,\N,Missing
J14-1002,S07-1018,0,0.666381,"Missing"
J14-1002,boas-2002-bilingual,0,0.0274075,"experiments on recently released FrameNet 1.5 data. In other work based on FrameNet, Matsubayashi, Okazaki, and Tsujii (2009) investigated various uses of FrameNet’s taxonomic relations for learning generalizations over roles; they trained a log-linear model on the SemEval 2007 data to evaluate features for the subtask of argument identification. Another line of work has sought to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style r"
J14-1002,W04-2412,0,0.0162126,"Missing"
J14-1002,W05-0620,0,0.105832,"Missing"
J14-1002,D11-1003,0,0.0540671,"kled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing (Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and Macherey 2011; Rush and Collins 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” 4 In prior work, there are exceptions where identification and classification of arguments have been treated in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on semantic role labeling (Carreras and M`arquez 2004). 16 Das et al. Frame-Semantic Parsing decomposition, that is, one with few overlapping components. This leaves out m"
J14-1002,S10-1059,1,0.89659,"y filled; in the SemEval 2007 development data, the average number of roles an evoked frame defines is 6.7, but the average number of overt arguments is only 1.7.29 In 29 In the annotated data, each core role is filled with one of three types of null instantiations indicating how the role is conveyed implicitly. For instance, the imperative construction implicitly designates a role as filled by the addressee, and the corresponding filler is thus CNI (constructional null instantiation). In this work we do not distinguish different types of null instantiation. The interested reader may refer to Chen et al. (2010), who handle the different types of null instantions during argument identification. 31 Computational Linguistics Volume 40, Number 1 training, if a labeled argument is not a subtree of the dependency parse, we add its span to S.30 Let Ai denote the mapping of roles in Rfi to spans in S. Our model makes a prediction for each Ai (rk ) (for all roles rk ∈ Rfi ) using: Ai (rk ) ← argmax pψ (s |rk , fi , ti , x) s∈S (7) We use a conditional log-linear model over spans for each role of each evoked frame: exp ψ h(s, rk , fi , ti , x) pψ (Ai (rk ) = s |fi , ti , x) =  exp ψ h(s , rk , fi , ti , x"
J14-1002,S12-1029,1,0.386125,"Missing"
J14-1002,P11-1061,1,0.647271,"Missing"
J14-1002,N10-1138,1,0.945675,"sition algorithm (Section 7) that collectively predicts all the arguments of a frame together, thereby incorporating linguistic constraints in a principled fashion. Our open-source parser, named SEMAFOR (Semantic Analyzer of Frame Representations)1 achieves the best published results to date on the SemEval 2007 frame-semantic structure extraction task (Baker, Ellsworth, and Erk 2007). Herein, we also present results on newly released data with FrameNet 1.5, the latest edition of the lexicon. Some of the material presented in this article has appeared in previously published conference papers: Das et al. (2010) presented the basic model, Das and Smith (2011) described semi-supervised lexicon expansion, Das and Smith (2012) demonstrated a sparse variant of lexicon expansion, and Das, Martins, and Smith (2012) presented the dual decomposition algorithm for constrained joint argument identification. We present here a synthesis of those results and several additional details: 1. The set of features used in the two statistical models for frame identification and argument identification. 2. Details of a greedy beam search algorithm for argument identification that avoids illegal argument overlap. 3. Error"
J14-1002,P11-1144,1,0.926493,"y predicts all the arguments of a frame together, thereby incorporating linguistic constraints in a principled fashion. Our open-source parser, named SEMAFOR (Semantic Analyzer of Frame Representations)1 achieves the best published results to date on the SemEval 2007 frame-semantic structure extraction task (Baker, Ellsworth, and Erk 2007). Herein, we also present results on newly released data with FrameNet 1.5, the latest edition of the lexicon. Some of the material presented in this article has appeared in previously published conference papers: Das et al. (2010) presented the basic model, Das and Smith (2011) described semi-supervised lexicon expansion, Das and Smith (2012) demonstrated a sparse variant of lexicon expansion, and Das, Martins, and Smith (2012) presented the dual decomposition algorithm for constrained joint argument identification. We present here a synthesis of those results and several additional details: 1. The set of features used in the two statistical models for frame identification and argument identification. 2. Details of a greedy beam search algorithm for argument identification that avoids illegal argument overlap. 3. Error analysis pertaining to the dual decomposition a"
J14-1002,N12-1086,1,0.870084,"ating linguistic constraints in a principled fashion. Our open-source parser, named SEMAFOR (Semantic Analyzer of Frame Representations)1 achieves the best published results to date on the SemEval 2007 frame-semantic structure extraction task (Baker, Ellsworth, and Erk 2007). Herein, we also present results on newly released data with FrameNet 1.5, the latest edition of the lexicon. Some of the material presented in this article has appeared in previously published conference papers: Das et al. (2010) presented the basic model, Das and Smith (2011) described semi-supervised lexicon expansion, Das and Smith (2012) demonstrated a sparse variant of lexicon expansion, and Das, Martins, and Smith (2012) presented the dual decomposition algorithm for constrained joint argument identification. We present here a synthesis of those results and several additional details: 1. The set of features used in the two statistical models for frame identification and argument identification. 2. Details of a greedy beam search algorithm for argument identification that avoids illegal argument overlap. 3. Error analysis pertaining to the dual decomposition argument identification algorithm, in contrast with the beam search"
J14-1002,P11-1043,0,0.0102251,"solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing (Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and Macherey 2011; Rush and Collins 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” 4 In prior work, there are exceptions where identification and classification of arguments have been treated in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on semantic role labeling (Carreras and M`arquez 2004). 16 Das et al. Frame-Semantic Parsing decomposition, that is, one with few overlapping components. This leaves out many declarative constraine"
J14-1002,D09-1003,0,0.0126412,"een verbs using a graph alignment method; this method represents sentences and their syntactic analysis as graphs and graph alignment is used to project annotations from seed examples to unlabeled sentences. This alignment problem is again modeled as a linear program. ¨ Furstenau and Lapata (2012) present an detailed expansion of the aforementioned papers. Although this line of work presents a novel direction in the area of SRL, the published approach does not yet deal with non-verbal predicates and does not evaluate the presented methods on the full text annotations of the FrameNet releases. Deschacht and Moens (2009) present a technique of incorporating additional information from unlabeled data by using a latent words language model. Latent variables are used to model the underlying representation of words, and parameters of this model 15 Computational Linguistics Volume 40, Number 1 are estimated using standard unsupervised methods. Next, the latent information is used as features for an SRL model. Improvements over supervised SRL techniques are observed with the augmentation of these extra features. The authors also compare ¨ their method with the aforementioned two methods of Furstenau and Lapata (200"
J14-1002,W03-1007,0,0.112387,"Missing"
J14-1002,C04-1134,0,0.0437403,"on recently released FrameNet 1.5 data. In other work based on FrameNet, Matsubayashi, Okazaki, and Tsujii (2009) investigated various uses of FrameNet’s taxonomic relations for learning generalizations over roles; they trained a log-linear model on the SemEval 2007 data to evaluate features for the subtask of argument identification. Another line of work has sought to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style representations, a fe"
J14-1002,P10-1160,0,0.00634321,"ables z are binary. Here, apart from the ILP formulation, we will consider the following relaxation of Equation (11), which replaces the binary constraint z ∈ {0, 1}d by a unit interval constraint z ∈ [0, 1]d , yielding a linear program: maximize  c(r, s) × zr,s r∈Rf s∈S with respect to such that z ∈ [0, 1]d Az ≤ b. (17) 42 We noticed that, in the annotated data, in some cases, the “requires” constraint is violated by the FrameNet annotators. This happens mostly when one of the required roles is absent in the sentence containing the target, but is rather instantiated in an earlier sentence (Gerber and Chai 2010). We apply the hard constraint in Equation (16), though extending our algorithm to seek arguments outside the sentence is straightforward. For preliminary work extending SEMAFOR this way, see Chen et al. (2010). 41 Computational Linguistics Volume 40, Number 1 There are several LP and ILP solvers available, and a great deal of effort has been spent by the optimization community to devise efficient generic solvers. An example is CPLEX, a state-of-the-art solver for mixed integer programming that we use as a baseline to solve the ILP in Equation (11) as well as its LP relaxation in Equation (17)"
J14-1002,J02-3001,0,0.989881,"012; accepted for publication: 22 December 2012. doi:10.1162/COLI a 00163 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 1 1. Introduction FrameNet (Fillmore, Johnson, and Petruck 2003) is a linguistic resource storing considerable information about lexical and predicate-argument semantics in English. Grounded in the theory of frame semantics (Fillmore 1982), it suggests—but does not formally define—a semantic representation that blends representations familiar from word-sense disambiguation (Ide and V´eronis 1998) and semantic role labeling (SRL; Gildea and Jurafsky 2002). Given the limited size of available resources, accurately producing richly structured frame-semantic structures with high coverage will require data-driven techniques beyond simple supervised classification, such as latent variable modeling, semi-supervised learning, and joint inference. In this article, we present a computational and statistical model for frame-semantic parsing, the problem of extracting from text semantic predicate-argument structures such as those shown in Figure 1. We aim to predict a frame-semantic representation with two statistical models rather than a collection of l"
J14-1002,S07-1003,0,0.00995843,"Missing"
J14-1002,W09-1201,0,0.0598567,"Missing"
J14-1002,J98-1001,0,0.0264943,"Missing"
J14-1002,S07-1048,0,0.102556,"s handling many more labels, and resulting in richer frame-semantic parses. Recent work in frame-semantic parsing—in which sentences may contain multiple frames which need to be recognized along with their arguments—was undertaken as the SemEval 2007 task 19 of frame-semantic structure extraction (Baker, Ellsworth, and Erk 2007). This task leveraged FrameNet 1.3, and also released a small corpus 3 Available at http://framenet.icsi.berkeley.edu as of 19 January 2013. 14 Das et al. Frame-Semantic Parsing containing a little more than 2,000 sentences with full text annotations. The LTH system of Johansson and Nugues (2007), which we use as our baseline (Section 3.4), had the best performance in the SemEval 2007 task in terms of full frame-semantic parsing. Johansson and Nugues broke down the task as identifying targets that could evoke frames in a sentence, identifying the correct semantic frame for a target, and finally determining the arguments that fill the semantic roles of a frame. They used a series of SVMs to classify the frames for a given target, associating unseen lexical items to frames and identifying and classifying token spans as various semantic roles. Both the full text annotation corpus as well"
J14-1002,D08-1008,0,0.0243643,"Missing"
J14-1002,kingsbury-palmer-2002-treebank,0,0.713501,"ly discuss work done on PropBank-style semantic role labeling, following which we will concentrate on the more relevant problem of frame-semantic structure extraction. Next, we review previous work that has used semi-supervised learning for shallow semantic parsing. Finally, we discuss prior work on joint structure prediction relevant to frame-semantic parsing. 2.1 Semantic Role Labeling Since Gildea and Jurafsky (2002) pioneered statistical semantic role labeling, there has been a great deal of computational work using predicate-argument structures for semantics. The development of PropBank (Kingsbury and Palmer 2002), followed by CoNLL shared tasks on semantic role labeling (Carreras and M`arquez 2004, 2005) boosted research in this area. Figure 2(a) shows an annotation from PropBank. PropBank annotations are closely tied to syntax, because the data set consists of the 1 See http://www.ark.cs.cmu.edu/SEMAFOR. 11 Computational Linguistics Volume 40, Number 1 (a) (b) Figure 2 (a) A phrase-structure tree taken from the Penn Treebank and annotated with PropBank predicate-argument structures. The verbs created and pushed serve as predicates in this sentence. Dotted arrows connect each predicate to its semantic"
J14-1002,D10-1125,0,0.0072494,", as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing (Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and Macherey 2011; Rush and Collins 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” 4 In prior work, there are exceptions where identification and classification of arguments have been treated in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on semantic role labeling (Carreras and M`"
J14-1002,N10-1137,0,0.00679874,"d Lapata (2009a, 2009b) and show relative improvements. Experiments are performed on the CoNLL 2008 shared task data set (Surdeanu et al. 2008), which follows the PropBank conventions and only labels verbal and nominal predicates—in contrast to our work, which includes most lexicosyntactic categories. A similar approach is presented by Weston, Ratle, and Collobert (2008), who use neural embeddings of words, which are eventually used for SRL; improvements over state-of-the-art PropBank-style SRL systems are observed. Recently, there has been related work in unsupervised semantic role labeling (Lang and Lapata 2010, 2011; Titov and Klementiev 2012) that attempts to induce semantic roles automatically from unannotated data. This line of work may be useful in discovering new semantic frames and roles, but here we stick to the concrete representation provided in FrameNet, without seeking to expand its inventory of semantic types. We present a new semi-supervised technique to expand the set of lexical items with the potential semantic frames that they could evoke; we use a graph-based semi-supervised learning framework to achieve this goal (Section 5.5). 2.4 Joint Inference and Shallow Semantic Parsing Most"
J14-1002,D11-1122,0,0.0108005,"Missing"
J14-1002,P93-1016,0,0.0449607,"e constraints on frame identification. 5.5.1 Graph Construction. We construct a graph with lexical units as vertices. Thus, each vertex corresponds to a lemmatized word or phrase appended with a coarse POS tag. We use two resources for graph construction. First, we take all the words and phrases present in a dependency-based thesaurus constructed using syntactic cooccurrence statistics (Lin 1998), and aggregate words and phrases that share the same lemma and coarse POS tag. To construct this resource, Lin used a corpus containing 64 million words that was parsed with a fast dependency parser (Lin 1993, 1994), and syntactic contexts were used to find similar lexical items for a given word or phrase. Lin separately treated nouns, verbs, and adjectives/adverbs, so these form the three parts of the thesaurus. This resource gave us a list of possible LUs, much larger in size than the LUs present in FrameNet data. The second component of graph construction comes from FrameNet itself. We scanned the exemplar sentences in FrameNet 1.5 and the training section of the full text annotations and gathered a distribution over frames for each LU appearing in FrameNet data. For a pair of LUs, we measured"
J14-1002,C94-1079,0,0.14144,"Missing"
J14-1002,P98-2127,0,0.0298059,"ed learning of POS taggers by using bilingual graph-based projections (Das and Petrov 2011). We describe our approach to graph construction, propagation for lexicon expansion, and the use of the result to impose constraints on frame identification. 5.5.1 Graph Construction. We construct a graph with lexical units as vertices. Thus, each vertex corresponds to a lemmatized word or phrase appended with a coarse POS tag. We use two resources for graph construction. First, we take all the words and phrases present in a dependency-based thesaurus constructed using syntactic cooccurrence statistics (Lin 1998), and aggregate words and phrases that share the same lemma and coarse POS tag. To construct this resource, Lin used a corpus containing 64 million words that was parsed with a fast dependency parser (Lin 1993, 1994), and syntactic contexts were used to find similar lexical items for a given word or phrase. Lin separately treated nouns, verbs, and adjectives/adverbs, so these form the three parts of the thesaurus. This resource gave us a list of possible LUs, much larger in size than the LUs present in FrameNet data. The second component of graph construction comes from FrameNet itself. We sca"
J14-1002,S07-1005,0,0.0401753,"Missing"
J14-1002,J93-2004,0,0.0478041,"Missing"
J14-1002,J08-2001,0,0.0552021,"Missing"
J14-1002,D11-1022,1,0.788077,"Missing"
J14-1002,P09-1039,1,0.427643,"Missing"
J14-1002,D10-1004,1,0.864365,"r knowledge, the separate line of work investigating frame-semantic parsing has not previously dealt with joint inference. A common trait in prior work, both in PropBank and FrameNet conventions, has been the use of a two-stage model that identifies arguments first, then labels them, often using dynamic programming or integer linear programs (ILPs); we treat both problems together here.4 Recent work in natural language processing (NLP) problems has focused on ILP formulations for complex structure prediction tasks like dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009; Martins et al. 2010), sequence tagging (Roth and Yih 2004), as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more ac"
J14-1002,P09-1003,0,0.0644821,"Missing"
J14-1002,P05-1012,0,0.0484643,"Missing"
J14-1002,W04-2705,0,0.72468,".v, ... Inheritance relation Causative_of relation Excludes relation Figure 3 Partial illustration of frames, roles, and lexical units related to the C AUSE TO MAKE NOISE frame, from the FrameNet lexicon. Core roles are filled bars. Non-core roles (such as Place and Time) are unfilled bars. No particular significance is ascribed to the ordering of a frame’s roles in its lexicon entry (the selection and ordering of roles above is for illustrative convenience). C AUSE TO MAKE NOISE defines a total of 14 roles, many of them not shown here. Whereas PropBank contains verbal predicates and NomBank (Meyers et al. 2004) contains nominal predicates, FrameNet counts these as well as allowing adjectives, adverbs, and prepositions among its lexical units. Finally, FrameNet frames organize predicates according to semantic principles, both by allowing related terms to evoke a common frame (e.g., push.V, raise.V, and growth.N for C AUSE CHANGE POSITION ON A SCALE) and by defining frames and their roles within a hierarchy (see Figure 3). PropBank does not explicitly encode relationships among predicates. Most early work on frame-semantic parsing has made use of the exemplar sentences in the FrameNet corpus (see Sect"
J14-1002,C04-1100,0,0.0140332,"n. Another line of work has sought to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style representations, a few improve¨ ments over vanilla supervised methods using unlabeled data are notable. Furstenau and Lapata (2009b) present a method of projecting predicate-argument structures from some seed examples to unlabeled sentences, and use a linear program formulation to find the best alignment explaining the projection. Next, the projected information a"
J14-1002,H05-1108,0,0.0829981,"Missing"
J14-1002,D08-1048,0,0.100472,"Missing"
J14-1002,N04-1030,0,0.00857398,"-DIR, and ARGM-TMP are shown in the figure. PropBank defines core roles ARG0 through ARG5, which receive different interpretations for different predicates. Additional modifier roles ARGM-* include ARGM-TMP (temporal) and ARGM-DIR (directional), as shown in Figure 2(a). The PropBank representation therefore has a small number of roles, and the training data set comprises some 40,000 sentences, thus making the semantic role labeling task an attractive one from the perspective of machine learning. There are many instances of influential work on semantic role labeling using PropBank conventions. Pradhan et al. (2004) present a system that uses support vector machines (SVMs) to identify the arguments in a syntax tree that can serve as semantic roles, followed by classification of the identified arguments to role names via a collection of binary SVMs. Punyakanok et al. (2004) describe a semantic role labeler that uses integer linear programming for inference and uses several global constraints to find the best 12 Das et al. Frame-Semantic Parsing suited predicate-argument structures. Joint modeling for semantic role labeling with discriminative log-linear models is presented by Toutanova, Haghighi, and Mann"
J14-1002,J08-2005,0,0.157863,"Missing"
J14-1002,C04-1197,0,0.401501,"igure 2(a). The PropBank representation therefore has a small number of roles, and the training data set comprises some 40,000 sentences, thus making the semantic role labeling task an attractive one from the perspective of machine learning. There are many instances of influential work on semantic role labeling using PropBank conventions. Pradhan et al. (2004) present a system that uses support vector machines (SVMs) to identify the arguments in a syntax tree that can serve as semantic roles, followed by classification of the identified arguments to role names via a collection of binary SVMs. Punyakanok et al. (2004) describe a semantic role labeler that uses integer linear programming for inference and uses several global constraints to find the best 12 Das et al. Frame-Semantic Parsing suited predicate-argument structures. Joint modeling for semantic role labeling with discriminative log-linear models is presented by Toutanova, Haghighi, and Manning (2005), where global features looking at all arguments of a particular verb together are incorporated into a dynamic programming and reranking framework. The Computational Linguistics special issue on semantic role labeling (M`arquez et al. 2008) includes ot"
J14-1002,W96-0213,0,0.464731,"Missing"
J14-1002,W06-1616,0,0.0176524,"or semantic role labeling (M`arquez et al. 2008). To our knowledge, the separate line of work investigating frame-semantic parsing has not previously dealt with joint inference. A common trait in prior work, both in PropBank and FrameNet conventions, has been the use of a two-stage model that identifies arguments first, then labels them, often using dynamic programming or integer linear programs (ILPs); we treat both problems together here.4 Recent work in natural language processing (NLP) problems has focused on ILP formulations for complex structure prediction tasks like dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009; Martins et al. 2010), sequence tagging (Roth and Yih 2004), as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since th"
J14-1002,W04-2401,0,0.0218668,"nvestigating frame-semantic parsing has not previously dealt with joint inference. A common trait in prior work, both in PropBank and FrameNet conventions, has been the use of a two-stage model that identifies arguments first, then labels them, often using dynamic programming or integer linear programs (ILPs); we treat both problems together here.4 Recent work in natural language processing (NLP) problems has focused on ILP formulations for complex structure prediction tasks like dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009; Martins et al. 2010), sequence tagging (Roth and Yih 2004), as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing ("
J14-1002,P11-1008,0,0.0341732,") proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing (Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and Macherey 2011; Rush and Collins 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” 4 In prior work, there are exceptions where identification and classification of arguments have been treated in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on semantic role labeling (Carreras and M`arquez 2004). 16 Das et al. Frame-Semantic Parsing decomposition, that is, one with few overlapping components. This leaves out many declarative constrained problems, for which su"
J14-1002,D10-1001,0,0.0101386,"Missing"
J14-1002,N03-1028,0,0.0145026,"labels (Subramanya and Bilmes 2008, 2009; Talukdar and Crammer 2009). For us, graph vertices correspond to natural language types (not tokens) and undirected edges between them are weighted using a similarity metric. Recently, this set-up has been used to learn soft labels on natural language types (say, word n-grams or in our case, syntactically disambiguated 27 The use of unsupported features (i.e., those that can fire for an analysis in the partition function but not observed to fire in the training data) has been observed to give performance improvements in NLP problems; see, for example, Sha and Pereira (2003) and Martins et al. (2010). 27 Computational Linguistics Volume 40, Number 1 predicates) from seed data, resulting in large but noisy lexicons, which are used to constrain structured prediction models. Applications have ranged from domain adaptation of sequence models (Subramanya, Petrov, and Pereira 2010) to unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov 2011). We describe our approach to graph construction, propagation for lexicon expansion, and the use of the result to impose constraints on frame identification. 5.5.1 Graph Construction. We c"
J14-1002,D07-1002,0,0.273124,"ught to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style representations, a few improve¨ ments over vanilla supervised methods using unlabeled data are notable. Furstenau and Lapata (2009b) present a method of projecting predicate-argument structures from some seed examples to unlabeled sentences, and use a linear program formulation to find the best alignment explaining the projection. Next, the projected information as well as the seeds are"
J14-1002,W04-2008,0,0.0362235,"r roles within a hierarchy (see Figure 3). PropBank does not explicitly encode relationships among predicates. Most early work on frame-semantic parsing has made use of the exemplar sentences in the FrameNet corpus (see Section 3.1), each of which is annotated for a single frame and its arguments. Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson, Levy, and Manning (2003) used a generative model for both the frame and its arguments. Fleischman, Kwon, and Hovy (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pado´ (2006) introduced the Shalmaneser tool, which uses naive Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet, containing around 300 frames and fewer than 500 unique semantic roles. Unlike this body of work, we experimented with the larger SemEval 2007 shared task data set, and also the newer FrameNet 1.5,3 which lists 877 frames and 1,068 role types—thus handling ma"
J14-1002,D08-1016,0,0.00944644,"Missing"
J14-1002,D08-1114,0,0.00506907,"en shown to perform better than several other semi-supervised algorithms ¨ on benchmark data sets (Chapelle, Scholkopf, and Zien 2006, chapter 21). The method constructs a graph where a small portion of vertices correspond to labeled instances, and the rest are unlabeled. Pairs of vertices are connected by weighted edges denoting the similarity between the pair. Traditionally, Markov random walks (Szummer and Jaakkola 2001; Baluja et al. 2008) or optimization of a loss function based on smoothness properties of the graph (e.g., Corduneanu and Jaakkola 2003; Zhu, Ghahramani, and Lafferty 2003; Subramanya and Bilmes 2008) are performed to propagate labels from the labeled vertices to the unlabeled ones. In our work, we are interested in multi-class generalizations of graph-propagation algorithms suitable for NLP applications, where each graph vertex can assume one or more out of many possible labels (Subramanya and Bilmes 2008, 2009; Talukdar and Crammer 2009). For us, graph vertices correspond to natural language types (not tokens) and undirected edges between them are weighted using a similarity metric. Recently, this set-up has been used to learn soft labels on natural language types (say, word n-grams or i"
J14-1002,D10-1017,0,0.0238482,"Missing"
J14-1002,P03-1002,0,0.0695193,"rained a log-linear model on the SemEval 2007 data to evaluate features for the subtask of argument identification. Another line of work has sought to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style representations, a few improve¨ ments over vanilla supervised methods using unlabeled data are notable. Furstenau and Lapata (2009b) present a method of projecting predicate-argument structures from some seed examples to unlabeled sentences, and"
J14-1002,E12-1003,0,0.0172185,"show relative improvements. Experiments are performed on the CoNLL 2008 shared task data set (Surdeanu et al. 2008), which follows the PropBank conventions and only labels verbal and nominal predicates—in contrast to our work, which includes most lexicosyntactic categories. A similar approach is presented by Weston, Ratle, and Collobert (2008), who use neural embeddings of words, which are eventually used for SRL; improvements over state-of-the-art PropBank-style SRL systems are observed. Recently, there has been related work in unsupervised semantic role labeling (Lang and Lapata 2010, 2011; Titov and Klementiev 2012) that attempts to induce semantic roles automatically from unannotated data. This line of work may be useful in discovering new semantic frames and roles, but here we stick to the concrete representation provided in FrameNet, without seeking to expand its inventory of semantic types. We present a new semi-supervised technique to expand the set of lexical items with the potential semantic frames that they could evoke; we use a graph-based semi-supervised learning framework to achieve this goal (Section 5.5). 2.4 Joint Inference and Shallow Semantic Parsing Most high-performance SRL systems that"
J14-1002,P05-1073,0,0.0166888,"Missing"
J14-1002,P10-1040,0,0.0142959,"Missing"
J14-1002,W04-3212,0,0.126024,"Missing"
J14-1002,N07-1069,0,0.0626288,"Missing"
J14-1002,erk-pado-2006-shalmaneser,0,\N,Missing
J14-1002,W08-2121,0,\N,Missing
J14-1002,E09-1026,0,\N,Missing
J14-1002,D09-1002,0,\N,Missing
J14-1002,P11-1048,0,\N,Missing
J14-1002,J12-1005,0,\N,Missing
J14-1002,C98-2122,0,\N,Missing
J14-1002,P10-2069,0,\N,Missing
N10-1038,P07-1053,0,0.00905126,"ge all of this work has used polarity of the review or the number of stars given to it by a critic, rather than the review text directly (Terry et al., 2005). Our task is related to sentiment analysis (Pang et al., 2002) on movie reviews. The key difference is that our goal is to predict a future real-valued quantity, restricting us from using any post-release text data such as user reviews. Further, the most important clues about revenue may have little to do with whether the reviewer liked the movie, but rather what the reviewer found worth mentioning. This paper is more in the tradition of Ghose et al. (2007) and Kogan et al. (2009), who used text regression to directly quantify review “value” and make predictions about future financial variables, respectively. Our aim in using the full text is to identify particular words and phrases that predict the movie-going tendencies of the public. We can also perform syntactic and semantic analysis on the text to identify richer constructions that are good predictors. Furthermore, since we consider multiple reviews for each movie, we can compare these features across reviews to observe how they differ both in frequency and predictive performance across dif"
N10-1038,N09-1031,1,0.594354,"used polarity of the review or the number of stars given to it by a critic, rather than the review text directly (Terry et al., 2005). Our task is related to sentiment analysis (Pang et al., 2002) on movie reviews. The key difference is that our goal is to predict a future real-valued quantity, restricting us from using any post-release text data such as user reviews. Further, the most important clues about revenue may have little to do with whether the reviewer liked the movie, but rather what the reviewer found worth mentioning. This paper is more in the tradition of Ghose et al. (2007) and Kogan et al. (2009), who used text regression to directly quantify review “value” and make predictions about future financial variables, respectively. Our aim in using the full text is to identify particular words and phrases that predict the movie-going tendencies of the public. We can also perform syntactic and semantic analysis on the text to identify richer constructions that are good predictors. Furthermore, since we consider multiple reviews for each movie, we can compare these features across reviews to observe how they differ both in frequency and predictive performance across different media outlets and"
N10-1038,W02-1011,0,0.0198514,"did not frame the task as a revenue prediction problem.) Zhang and Skiena (2009) used a news aggregation system to identify entities and obtain domain-specific sentiment for each entity in several domains. They used the aggregate sentiment scores and mention counts of each movie in news articles as predictors. While there has been substantial prior work on using critics’ reviews, to our knowledge all of this work has used polarity of the review or the number of stars given to it by a critic, rather than the review text directly (Terry et al., 2005). Our task is related to sentiment analysis (Pang et al., 2002) on movie reviews. The key difference is that our goal is to predict a future real-valued quantity, restricting us from using any post-release text data such as user reviews. Further, the most important clues about revenue may have little to do with whether the reviewer liked the movie, but rather what the reviewer found worth mentioning. This paper is more in the tradition of Ghose et al. (2007) and Kogan et al. (2009), who used text regression to directly quantify review “value” and make predictions about future financial variables, respectively. Our aim in using the full text is to identify"
N10-1038,W00-1308,0,0.035303,"o our pool of features in the following order: whether the 4.2 Text Features We extract three types of text features (described below). We only included feature instances that occurred in at least five different movies’ reviews. We stem and downcase individual word components in all our features. I. n-grams. We considered unigrams, bigrams, and trigrams. A 25-word stoplist was used; bigrams and trigrams were only filtered if all words were stopwords. II. Part-of-speech n-grams. As with words, we added unigrams, bigrams, and trigrams. Tags were obtained from the Stanford part-of-speech tagger (Toutanova and Manning, 2000). III. Dependency relations. We used the Stanford parser (Klein and Manning, 2003) to parse the critic reviews and extract syntactic dependencies. The dependency relation features consist of just the relation part of a dependency triple hrelation, head word, modifier wordi. We consider three ways to combine the collection of reviews for a given movie. The first (“−”) simply concatenates all of a movie’s reviews into a single document before extracting features. The second (“+”) conjoins each feature with the source site (e.g., New York Times) from whose review it was extracted. A third version"
N10-1138,S07-1018,0,0.696004,"though our models often involve strong independence assumptions, the probabilistic framework we adopt is highly amenable to future extension through new features, relaxed independence assumptions, and semisupervised learning. Some novel aspects of our current approach include a latent-variable model that permits disambiguation of words not in the FrameNet lexicon, a unified model for finding and labeling arguments, and a precision-boosting constraint that forbids arguments of the same predicate to overlap. Our parser achieves the best published results to date on the SemEval’07 FrameNet task (Baker et al., 2007). 2 Resources and Task We consider frame-semantic parsing resources. 2.1 FrameNet Lexicon The FrameNet lexicon is a taxonomy of manually identified general-purpose frames for English.1 Listed in the lexicon with each frame are several lemmas (with part of speech) that can denote the frame or some aspect of it—these are called lexical units (LUs). In a sentence, word or phrase tokens that evoke a frame are known as targets. The set of LUs listed for a frame in FrameNet may not be exhaustive; we may see a target in new data that does not correspond to an LU for the frame it evokes. Each frame de"
N10-1138,boas-2002-bilingual,0,0.0220307,"7). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over t"
N10-1138,erk-pado-2006-shalmaneser,0,0.128834,"Missing"
N10-1138,W03-1007,0,0.0683486,"a set of shallow predicate-argument annotations for Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Na¨ıve Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet. Recent work on frame-semantic parsing—in which sentences may contain multiple frames to be recognized along with their arguments—has used the SemEval’"
N10-1138,C04-1134,0,0.0348662,"system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over the state of the art a"
N10-1138,E09-1026,0,0.0300148,"Missing"
N10-1138,J02-3001,0,0.820262,"riefly, we highlight some relevant work, particularly research that has made use of FrameNet. (Note that much related research has focused on PropBank (Kingsbury and Palmer, 2002), a set of shallow predicate-argument annotations for Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Na¨ıve Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, s"
N10-1138,S07-1048,0,0.547847,"easure for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one. We present precision, recall, and F1 -measure microaveraged across the test documents, report labels-only matching scores (spans must match exactly), and do not use named entity labels. More details can be found in Baker et al. (2007). For our experiments, statistical significance is measured using a reimplementation of Dan Bikel’s randomized parsing evaluation comparator.3 2.4 Baseline A strong baseline for frame-semantic parsing is the system presented by Johansson and Nugues (2007, hereafter J&N’07), the best system in the SemEval’07 shared task. For frame identification, they used an SVM classifier to disambiguate frames for known frame-evoking words. They used WordNet synsets to extend the vocabulary of frameevoking words to cover unknown words, and then 3 http://www.cis.upenn.edu/˜dbikel/ software.html#comparator 950 TARGET I DENTIFICATION Our technique (§3) Baseline: J&N’07 P 89.92 87.87 R 70.79 67.11 F1 79.21 76.10 Table 3. Target identification results for our system and the baseline. Scores in bold denote significant improvements over the baseline (p &lt; 0.05). us"
N10-1138,D08-1008,0,0.010832,"dels trained on the given data split, a problem we have not sought to address here. 2 http://framenet.icsi.berkeley.edu/ semeval/FSSE.html Preprocessing. We preprocess sentences in our dataset with a standard set of annotations: POS tags from MXPOST (Ratnaparkhi, 1996) and dependency parses from the MST parser (McDonald et al., 2005) since manual syntactic parses are not available for most of the FrameNet-annotated documents. We used WordNet (Fellbaum, 1998) for lemmatization. We also labeled each verb in the data as having AC TIVE or PASSIVE voice, using code from the SRL system described by Johansson and Nugues (2008). 2.3 Task and Evaluation Automatic annotations of frame-semantic structure can be broken into three parts: (1) targets, the words or phrases that evoke frames; (2) the frame type, defined in the lexicon, evoked by each target; and (3) the arguments, or spans of words that serve to fill roles defined by each evoked frame. These correspond to the three subtasks in our parser, each described and evaluated in turn: target identification (§3), frame identification (§4, not unlike wordsense disambiguation), and argument identification (§5, not unlike semantic role labeling). The standard evaluation"
N10-1138,kingsbury-palmer-2002-treebank,0,0.137074,".57 68.46 49.68 42.82 46.00 58.08 38.76 46.49 51.59 35.44 42.01 partial frame matching P R F1 57.85 49.86 53.56 62.76 41.89 50.24 56.01 38.48 45.62 Table 5. Argument identification results. ∗ indicates that gold-standard labels were used for a given pipeline stage. For full parsing, bolded scores indicate significant improvements relative to the baseline (p &lt; 0.05). putational work has investigated predicate-argument structures for semantics. Briefly, we highlight some relevant work, particularly research that has made use of FrameNet. (Note that much related research has focused on PropBank (Kingsbury and Palmer, 2002), a set of shallow predicate-argument annotations for Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and"
N10-1138,J93-2004,0,0.0372018,"1 38.48 45.62 Table 5. Argument identification results. ∗ indicates that gold-standard labels were used for a given pipeline stage. For full parsing, bolded scores indicate significant improvements relative to the baseline (p &lt; 0.05). putational work has investigated predicate-argument structures for semantics. Briefly, we highlight some relevant work, particularly research that has made use of FrameNet. (Note that much related research has focused on PropBank (Kingsbury and Palmer, 2002), a set of shallow predicate-argument annotations for Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004"
N10-1138,J08-2001,0,0.0694048,"Missing"
N10-1138,P09-1003,0,0.164686,"ict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Na¨ıve Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet. Recent work on frame-semantic parsing—in which sentences may contain multiple frames to be recognized along with their arguments—has used the SemEval’07 data (Baker et al., 2007). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to quest"
N10-1138,P05-1012,0,0.00937302,"s. Notice that the test set contains more annotations per word, both in terms of frames and arguments. Moreover, there are many more out-of-lexicon frame, role, and LU types in the test set than in the training set. This inconsistency in the data results in poor recall scores for all models trained on the given data split, a problem we have not sought to address here. 2 http://framenet.icsi.berkeley.edu/ semeval/FSSE.html Preprocessing. We preprocess sentences in our dataset with a standard set of annotations: POS tags from MXPOST (Ratnaparkhi, 1996) and dependency parses from the MST parser (McDonald et al., 2005) since manual syntactic parses are not available for most of the FrameNet-annotated documents. We used WordNet (Fellbaum, 1998) for lemmatization. We also labeled each verb in the data as having AC TIVE or PASSIVE voice, using code from the SRL system described by Johansson and Nugues (2008). 2.3 Task and Evaluation Automatic annotations of frame-semantic structure can be broken into three parts: (1) targets, the words or phrases that evoke frames; (2) the frame type, defined in the lexicon, evoked by each target; and (3) the arguments, or spans of words that serve to fill roles defined by eac"
N10-1138,C04-1100,0,0.0279741,"ication features exploiting various 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over the state of the art at each stage of processing and collectively, and is amenable to future extension. Our parser is available for download at http://www.ark.cs.cmu.edu/SEMAFOR. Acknowledgments We thank Collin Baker, Katrin Erk, Richard J"
N10-1138,H05-1108,0,0.0518989,"Missing"
N10-1138,D08-1048,0,0.0129921,"Missing"
N10-1138,W96-0213,0,0.500149,"ta to create a development set for tuning model hyperparameters. Notice that the test set contains more annotations per word, both in terms of frames and arguments. Moreover, there are many more out-of-lexicon frame, role, and LU types in the test set than in the training set. This inconsistency in the data results in poor recall scores for all models trained on the given data split, a problem we have not sought to address here. 2 http://framenet.icsi.berkeley.edu/ semeval/FSSE.html Preprocessing. We preprocess sentences in our dataset with a standard set of annotations: POS tags from MXPOST (Ratnaparkhi, 1996) and dependency parses from the MST parser (McDonald et al., 2005) since manual syntactic parses are not available for most of the FrameNet-annotated documents. We used WordNet (Fellbaum, 1998) for lemmatization. We also labeled each verb in the data as having AC TIVE or PASSIVE voice, using code from the SRL system described by Johansson and Nugues (2008). 2.3 Task and Evaluation Automatic annotations of frame-semantic structure can be broken into three parts: (1) targets, the words or phrases that evoke frames; (2) the frame type, defined in the lexicon, evoked by each target; and (3) the ar"
N10-1138,D07-1002,0,0.0551122,"ious 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over the state of the art at each stage of processing and collectively, and is amenable to future extension. Our parser is available for download at http://www.ark.cs.cmu.edu/SEMAFOR. Acknowledgments We thank Collin Baker, Katrin Erk, Richard Johansson, and Nils Reit"
N10-1138,W04-2008,0,0.148854,"(Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Na¨ıve Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet. Recent work on frame-semantic parsing—in which sentences may contain multiple frames to be recognized along with their arguments—has used the SemEval’07 data (Baker et al., 2007). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), perfo"
N10-1138,P03-1002,0,0.025474,"ught to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over the state of the art at each stage of processing and collectively, and is amenable to future extension. Our parser is available for download at http://www.ark.cs.cmu.edu/SEMAFOR. Acknowledgments We thank Collin Baker, Katrin Erk, Richard Johansson, and Nils Reiter for software, data, evaluation scripts, and methodological details. We thank the re"
N10-1138,D09-1029,0,0.0207559,"rames to be recognized along with their arguments—has used the SemEval’07 data (Baker et al., 2007). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on Se"
N12-1086,P11-1061,1,0.342589,"2009). For us, graph vertices correspond to natural language types (not tokens) and undirected edges between them are weighted using a similarity metric. Recently, this setup has been used to learn soft labels on natural language types (say, word n-grams or syntactically disambiguated predicates) from seed data, resulting in large but noisy lexicons, which are used to constrain structured prediction models. Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al., 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith, 2011). However, none of the above captured the empirical fact that only a few categories typically associate with a given type (vertex). Take the case of POS tagging: Subramanya et al. (2010) construct a graph over trigram types as vertices, with 45 possible tags for the middle word of a trigram as the 677 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 677–687, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistic"
N12-1086,P11-1144,1,0.41045,"ens) and undirected edges between them are weighted using a similarity metric. Recently, this setup has been used to learn soft labels on natural language types (say, word n-grams or syntactically disambiguated predicates) from seed data, resulting in large but noisy lexicons, which are used to constrain structured prediction models. Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al., 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith, 2011). However, none of the above captured the empirical fact that only a few categories typically associate with a given type (vertex). Take the case of POS tagging: Subramanya et al. (2010) construct a graph over trigram types as vertices, with 45 possible tags for the middle word of a trigram as the 677 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 677–687, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics label set for each vertex. It is empirically observed that contextualized"
N12-1086,P09-1056,0,0.00726954,"roblems such as the induction of labels on natural language types, which typically associate with only a few labels. Compared to standard graph-based learning methods, for two lexicon expansion problems, our approach produces significantly smaller lexicons and obtains better predictive performance. 1 Introduction Semi-supervised learning (SSL) is attractive for the learning of complex phenomena, for example, linguistic structure, where data annotation is expensive. Natural language processing applications have benefited from various SSL techniques, such as distributional word representations (Huang and Yates, 2009; Turian et al., 2010; Dhillon et al., 2011), self-training (McClosky et al., 2006), and entropy regularization (Jiao et al., 2006; Smith and Eisner, 2007). In this paper, we focus on semi-supervised learning that uses a graph constructed from labeled and unlabeled data. This framework, graph-based SSL—see Bengio et al. (2006) and Zhu (2008) for introductory material on this topic—has been widely used and has been shown to perform better than several other semi-supervised algorithms on benchmark datasets (Chapelle et al., 2006, ch. 21). The method constructs a graph where a small portion of ve"
N12-1086,P06-1027,0,0.02999,"ard graph-based learning methods, for two lexicon expansion problems, our approach produces significantly smaller lexicons and obtains better predictive performance. 1 Introduction Semi-supervised learning (SSL) is attractive for the learning of complex phenomena, for example, linguistic structure, where data annotation is expensive. Natural language processing applications have benefited from various SSL techniques, such as distributional word representations (Huang and Yates, 2009; Turian et al., 2010; Dhillon et al., 2011), self-training (McClosky et al., 2006), and entropy regularization (Jiao et al., 2006; Smith and Eisner, 2007). In this paper, we focus on semi-supervised learning that uses a graph constructed from labeled and unlabeled data. This framework, graph-based SSL—see Bengio et al. (2006) and Zhu (2008) for introductory material on this topic—has been widely used and has been shown to perform better than several other semi-supervised algorithms on benchmark datasets (Chapelle et al., 2006, ch. 21). The method constructs a graph where a small portion of vertices correspond to labeled instances, and the rest are unlabeled. Pairs of vertices are connected by weighted edges denoting the"
N12-1086,J93-2004,0,0.0485563,"zed measures at each graph vertex. We achieve this by penalizing the graph propagation objective with the `1 norm or the mixed `1,2 norm (Kowalski and Torr´esani, 2009) of the measures at each vertex, aiming for global and vertex-level sparsity, respectively. Importantly, the proposed graph objective functions are convex, so we avoid degenerate solutions and local minima. We present experiments on two natural language lexicon expansion problems in a semi-supervised setting: (i) inducing distributions of POS tags over n-gram types in the Wall Street Journal section of the Penn Treebank corpus (Marcus et al., 1993) and (ii) inducing distributions of semantic frames (Fillmore, 1982) over predicates unseen in anno1 Moreover, we also assume the edge weights in a given graph are unconstrained, consistent with prior work on graphbased SSL (Das and Petrov, 2011; Das and Smith, 2011; Subramanya and Bilmes, 2008; Subramanya and Bilmes, 2009; Subramanya et al., 2010; Zhu and Ghahramani, 2002). 678 tated data. Our methods produce sparse measures at graph vertices resulting in compact lexicons, and also result in better performance with respect to label propagation using Gaussian penalties (Zhu and Ghahramani, 200"
N12-1086,N06-1020,0,0.00484305,"associate with only a few labels. Compared to standard graph-based learning methods, for two lexicon expansion problems, our approach produces significantly smaller lexicons and obtains better predictive performance. 1 Introduction Semi-supervised learning (SSL) is attractive for the learning of complex phenomena, for example, linguistic structure, where data annotation is expensive. Natural language processing applications have benefited from various SSL techniques, such as distributional word representations (Huang and Yates, 2009; Turian et al., 2010; Dhillon et al., 2011), self-training (McClosky et al., 2006), and entropy regularization (Jiao et al., 2006; Smith and Eisner, 2007). In this paper, we focus on semi-supervised learning that uses a graph constructed from labeled and unlabeled data. This framework, graph-based SSL—see Bengio et al. (2006) and Zhu (2008) for introductory material on this topic—has been widely used and has been shown to perform better than several other semi-supervised algorithms on benchmark datasets (Chapelle et al., 2006, ch. 21). The method constructs a graph where a small portion of vertices correspond to labeled instances, and the rest are unlabeled. Pairs of vertic"
N12-1086,D07-1070,0,0.0248675,"rning methods, for two lexicon expansion problems, our approach produces significantly smaller lexicons and obtains better predictive performance. 1 Introduction Semi-supervised learning (SSL) is attractive for the learning of complex phenomena, for example, linguistic structure, where data annotation is expensive. Natural language processing applications have benefited from various SSL techniques, such as distributional word representations (Huang and Yates, 2009; Turian et al., 2010; Dhillon et al., 2011), self-training (McClosky et al., 2006), and entropy regularization (Jiao et al., 2006; Smith and Eisner, 2007). In this paper, we focus on semi-supervised learning that uses a graph constructed from labeled and unlabeled data. This framework, graph-based SSL—see Bengio et al. (2006) and Zhu (2008) for introductory material on this topic—has been widely used and has been shown to perform better than several other semi-supervised algorithms on benchmark datasets (Chapelle et al., 2006, ch. 21). The method constructs a graph where a small portion of vertices correspond to labeled instances, and the rest are unlabeled. Pairs of vertices are connected by weighted edges denoting the similarity between the p"
N12-1086,D08-1114,0,0.102509,"his topic—has been widely used and has been shown to perform better than several other semi-supervised algorithms on benchmark datasets (Chapelle et al., 2006, ch. 21). The method constructs a graph where a small portion of vertices correspond to labeled instances, and the rest are unlabeled. Pairs of vertices are connected by weighted edges denoting the similarity between the pair. Traditionally, Markov random walks (Szummer and Jaakkola, 2001; Baluja et al., 2008) or optimization of a loss function based on smoothness properties of the graph (Corduneanu and Jaakkola, 2003; Zhu et al., 2003; Subramanya and Bilmes, 2008, inter alia) are performed to propagate labels from the labeled vertices to the unlabeled ones. In this work, we are interested in multi-class generalizations of graph-propagation algorithms suitable for NLP applications, where each graph vertex can assume one or more out of many possible labels (Talukdar and Crammer, 2009; Subramanya and Bilmes, 2008, 2009). For us, graph vertices correspond to natural language types (not tokens) and undirected edges between them are weighted using a similarity metric. Recently, this setup has been used to learn soft labels on natural language types (say, wo"
N12-1086,D10-1017,0,0.401698,"an assume one or more out of many possible labels (Talukdar and Crammer, 2009; Subramanya and Bilmes, 2008, 2009). For us, graph vertices correspond to natural language types (not tokens) and undirected edges between them are weighted using a similarity metric. Recently, this setup has been used to learn soft labels on natural language types (say, word n-grams or syntactically disambiguated predicates) from seed data, resulting in large but noisy lexicons, which are used to constrain structured prediction models. Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al., 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith, 2011). However, none of the above captured the empirical fact that only a few categories typically associate with a given type (vertex). Take the case of POS tagging: Subramanya et al. (2010) construct a graph over trigram types as vertices, with 45 possible tags for the middle word of a trigram as the 677 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technol"
N12-1086,P10-1040,0,0.0187725,"uction of labels on natural language types, which typically associate with only a few labels. Compared to standard graph-based learning methods, for two lexicon expansion problems, our approach produces significantly smaller lexicons and obtains better predictive performance. 1 Introduction Semi-supervised learning (SSL) is attractive for the learning of complex phenomena, for example, linguistic structure, where data annotation is expensive. Natural language processing applications have benefited from various SSL techniques, such as distributional word representations (Huang and Yates, 2009; Turian et al., 2010; Dhillon et al., 2011), self-training (McClosky et al., 2006), and entropy regularization (Jiao et al., 2006; Smith and Eisner, 2007). In this paper, we focus on semi-supervised learning that uses a graph constructed from labeled and unlabeled data. This framework, graph-based SSL—see Bengio et al. (2006) and Zhu (2008) for introductory material on this topic—has been widely used and has been shown to perform better than several other semi-supervised algorithms on benchmark datasets (Chapelle et al., 2006, ch. 21). The method constructs a graph where a small portion of vertices correspond to"
N12-1086,S07-1018,0,\N,Missing
N19-1263,D14-1179,0,0.0730948,"Missing"
N19-1263,P16-1188,0,0.227806,"Graff et al., 2003; Napoles et al., 2012). Gigaword contains news articles sourced from various news services over the last two decades. To produce the dataset, we follow the split and preprocessing by Rush et al. (2015), and pair the first sentences and the headlines in the news articles. It results in a 3.8M/190K/1,951 train/dev./test split. The average lengths of the source and target texts are 31.4 and 8.2, respectively. • New York Times Annotated Corpus (NYT; Sandaus, 2008). It contains news articles published between 1996 and 2007 by New York Times. We use the split and preprocessing by Durrett et al. (2016).10 Following their effort, we evaluate on a smaller portion of the test set, where the gold summaries are longer than 50 tokens. We further randomly sample 9,000 instances from the training data for validation, resulting in a 91,834/9,000/3,452 train/dev./test split. Compared to Gigaword, the inputs and targets in NYT are much longer (averaging 939.0 and 48.6, respectively). Table 1 summarizes some statistics of the datasets. We note that some recent works use a different split of the NYT corpus (Paulus et al., 2018; Gehrmann et al., 2018), and thus are not comparable to the models in Table 3"
N19-1263,W18-2706,0,0.0338599,"train/dev./test split. Compared to Gigaword, the inputs and targets in NYT are much longer (averaging 939.0 and 48.6, respectively). Table 1 summarizes some statistics of the datasets. We note that some recent works use a different split of the NYT corpus (Paulus et al., 2018; Gehrmann et al., 2018), and thus are not comparable to the models in Table 3. We decide to use the one by Durrett et al. (2016) because their preprocessing script is publicly available. For both datasets, we apply byte-paired encoding (BPE; Sennrich et al., 2016), which proves to improve the generation of proper nouns (Fan et al., 2018). Empirical results. Table 2 compares the models on Gigaword test set in ROUGE F1 (Lin, 2004).11 By using adaptive decoders, our model (A DA D EC) improves over S EQ 2 SEQ by more than 1.1 ROUGE scores. Cao et al. (2018b) and the F ULL model by Cao et al. (2018a) hold the best published results. The former uses extensive handcrafted features and relies on external information extraction and syntactic parsing systems; 10 https://github.com/gregdurrett/ berkeley-doc-summarizer. 11 Version 1.5.5 of the official script. Model R G-1 R G-2 R G-L Open-NMT † Cao et al., 2018a (BASIC ) † Cao et al., 20"
N19-1263,W04-0601,0,0.616883,"oder generates the text autoregressively, token-by-token, conditioning on the feature representations encoded from the source, typically with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016; See et al., 2017). This paradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degenerate outputs and favoring generic utterances (Vinyals and Le, 2015; Li et al., 2016). The encoder-decoder approach differs considerably from earlier template-based meth⇤ Work done during internship at Google. ods (Becker, 2002; Foster and White, 2004; Reiter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability problems with handcrafted templates, it has been proposed to use similar training samples as exemplars, to guide the decoding process (Gu et al., 2018; Guu et al., 20"
N19-1263,W09-0613,0,0.0705198,"en-by-token, conditioning on the feature representations encoded from the source, typically with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016; See et al., 2017). This paradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degenerate outputs and favoring generic utterances (Vinyals and Le, 2015; Li et al., 2016). The encoder-decoder approach differs considerably from earlier template-based meth⇤ Work done during internship at Google. ods (Becker, 2002; Foster and White, 2004; Reiter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability problems with handcrafted templates, it has been proposed to use similar training samples as exemplars, to guide the decoding process (Gu et al., 2018; Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018"
N19-1263,D18-1443,0,0.0236684,"New York Times. We use the split and preprocessing by Durrett et al. (2016).10 Following their effort, we evaluate on a smaller portion of the test set, where the gold summaries are longer than 50 tokens. We further randomly sample 9,000 instances from the training data for validation, resulting in a 91,834/9,000/3,452 train/dev./test split. Compared to Gigaword, the inputs and targets in NYT are much longer (averaging 939.0 and 48.6, respectively). Table 1 summarizes some statistics of the datasets. We note that some recent works use a different split of the NYT corpus (Paulus et al., 2018; Gehrmann et al., 2018), and thus are not comparable to the models in Table 3. We decide to use the one by Durrett et al. (2016) because their preprocessing script is publicly available. For both datasets, we apply byte-paired encoding (BPE; Sennrich et al., 2016), which proves to improve the generation of proper nouns (Fan et al., 2018). Empirical results. Table 2 compares the models on Gigaword test set in ROUGE F1 (Lin, 2004).11 By using adaptive decoders, our model (A DA D EC) improves over S EQ 2 SEQ by more than 1.1 ROUGE scores. Cao et al. (2018b) and the F ULL model by Cao et al. (2018a) hold the best publis"
N19-1263,W02-2211,0,0.766154,"used. The decoder generates the text autoregressively, token-by-token, conditioning on the feature representations encoded from the source, typically with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016; See et al., 2017). This paradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degenerate outputs and favoring generic utterances (Vinyals and Le, 2015; Li et al., 2016). The encoder-decoder approach differs considerably from earlier template-based meth⇤ Work done during internship at Google. ods (Becker, 2002; Foster and White, 2004; Reiter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability problems with handcrafted templates, it has been proposed to use similar training samples as exemplars, to guide the decoding process (Gu et a"
N19-1263,P16-1154,0,0.334646,"roduction Conditioned text generation is the essence of many natural language processing (NLP) tasks, e.g., text summarization (Mani, 1999), machine translation (Koehn, 2009), and data-to-text generation (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997). In its common neural sequence-tosequence formulation (Sutskever et al., 2014; Cho et al., 2014), an encoder-decoder architecture is used. The decoder generates the text autoregressively, token-by-token, conditioning on the feature representations encoded from the source, typically with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016; See et al., 2017). This paradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degenerate outputs and favoring generic utterances (Vinyals and Le, 2015; Li et al., 2016). The encoder-decoder approach differs considerably from earlier template-based meth⇤ Work done during internship at Google. ods (Becker, 2002; Foster and White, 2004; Reiter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision comp"
N19-1263,P18-1015,0,0.489164,"inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability problems with handcrafted templates, it has been proposed to use similar training samples as exemplars, to guide the decoding process (Gu et al., 2018; Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018a, inter alia).1 In general, existing methods accomplish this by (a) using traditional information retrieval (IR) techniques for exemplar extraction (e.g., TF-IDF), and then (b) concatenating the exemplar to the source as additional inputs, allowing the decoder to attend over and copy from both. We propose a different strategy for using exemplars. For motivation, Figure 1 shows a sourcetarget pair together with its exemplar from the Gigaword dataset (Graff et al., 2003). The target is a summary of the source sentence, and the exemplar is retrieved from the training set (§3.2).2 There is word o"
N19-1263,Q18-1031,0,0.471171,"nd White, 2004; Reiter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability problems with handcrafted templates, it has been proposed to use similar training samples as exemplars, to guide the decoding process (Gu et al., 2018; Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018a, inter alia).1 In general, existing methods accomplish this by (a) using traditional information retrieval (IR) techniques for exemplar extraction (e.g., TF-IDF), and then (b) concatenating the exemplar to the source as additional inputs, allowing the decoder to attend over and copy from both. We propose a different strategy for using exemplars. For motivation, Figure 1 shows a sourcetarget pair together with its exemplar from the Gigaword dataset (Graff et al., 2003). The target is a summary of the source sentence, and the exemplar"
N19-1263,D15-1166,0,0.0738266,"Missing"
N19-1263,W14-1602,0,0.0227032,"ng at the bottom example, the model in general follows the exemplar in using noun adjuncts or prepositional phrases (e.g., new home sales vs. sales of new homes), except the first one. Perhaps confused by the distraction in Exemplar 3, the model makes a judgment on the specific amount of growth, but gets it wrong. 6 Exemplar 3: Related Work Exemplar-based generation. Partly inspired by traditional template-based generation (Kukich, 1983; Reiter and Dale, 1997, inter alia), many recent efforts have been devoted to augmenting text generation models with retrieved exemplars (Hodosh et al., 2013; Mason and Charniak, 2014; Song et al., 2016; Lin et al., 2017, inter alia). Source: Sales of new homes in the U.S. increased by 11.8 percent in May, the biggest gain in 26 years... Exemplar 1: U.S. sales of new homes up strongly in March. Output 1: US new home sales rise 11.8 percent in May. Exemplar 2: The sales of new homes in the U.S. grow strongly. Output 2: Sales of new homes in US rise in May. Exemplar 3: U.S. economic statistics: new home sales grow by 2.5 percent. Output 3: US new home sales grow 26 percent in May. Figure 3: Two randomly sampled Gigaword development instances used for qualitative evaluation ("
N19-1263,W12-3018,0,0.0171138,"Missing"
N19-1263,P09-5002,0,0.0129907,"digm, it first encodes the content representation from the given input text; to produce the output, it retrieves exemplar text from the training data as “soft templates,” which are then used to construct an exemplar-specific decoder. We evaluate the proposed model on abstractive text summarization and data-to-text generation. Empirical results show that this model achieves strong performance and outperforms comparable baselines. 1 Introduction Conditioned text generation is the essence of many natural language processing (NLP) tasks, e.g., text summarization (Mani, 1999), machine translation (Koehn, 2009), and data-to-text generation (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997). In its common neural sequence-tosequence formulation (Sutskever et al., 2014; Cho et al., 2014), an encoder-decoder architecture is used. The decoder generates the text autoregressively, token-by-token, conditioning on the feature representations encoded from the source, typically with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016; See et al., 2017). This paradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degen"
N19-1263,P83-1022,0,0.500588,"tation from the given input text; to produce the output, it retrieves exemplar text from the training data as “soft templates,” which are then used to construct an exemplar-specific decoder. We evaluate the proposed model on abstractive text summarization and data-to-text generation. Empirical results show that this model achieves strong performance and outperforms comparable baselines. 1 Introduction Conditioned text generation is the essence of many natural language processing (NLP) tasks, e.g., text summarization (Mani, 1999), machine translation (Koehn, 2009), and data-to-text generation (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997). In its common neural sequence-tosequence formulation (Sutskever et al., 2014; Cho et al., 2014), an encoder-decoder architecture is used. The decoder generates the text autoregressively, token-by-token, conditioning on the feature representations encoded from the source, typically with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016; See et al., 2017). This paradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degenerate outputs and favoring generic utteranc"
N19-1263,D16-1128,0,0.165538,"et al., 2014; Cho et al., 2014). In the interest of the noConditioned text generation and the encoderdecoder architecture. Our discussion centers around conditioned text generation, i.e., the model aims to output the target y = y1 y2 . . . yT given the source input x = x1 x2 . . . xS , both of which are sequences of tokens. Each token xi , yi takes one value from a vocabulary V. x and y could vary depending on the tasks, e.g., they will respectively be articles and summaries for text summarization; and for data-to-text generation, x would be structured data, which can sometimes be linearized (Lebret et al., 2016; Wiseman et al., 2018, inter alia), and y is the output text. We aim to learn a (parameterized) conditional distribution of the target text y given the source x, p y|x = T Y t=1 p yt |y&lt;t , x , (1) where y&lt;t = y1 . . . yt 1 is the prefix of y up to the (t 1)th token (inclusive). The probability of each target token is usually estimated with a softmax function: exp h&gt; t 1 w yt p (yt |y&lt;t , x) = P . &gt; y exp ht 1 wy (2) wy denotes a learned vector for token y 2 V. ht 1 depends on y&lt;t and x, and is computed by a function which we will describe soon. A typical implementation choice for computing h"
N19-1263,N16-1014,0,0.0688925,"d Dale, 1997). In its common neural sequence-tosequence formulation (Sutskever et al., 2014; Cho et al., 2014), an encoder-decoder architecture is used. The decoder generates the text autoregressively, token-by-token, conditioning on the feature representations encoded from the source, typically with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016; See et al., 2017). This paradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degenerate outputs and favoring generic utterances (Vinyals and Le, 2015; Li et al., 2016). The encoder-decoder approach differs considerably from earlier template-based meth⇤ Work done during internship at Google. ods (Becker, 2002; Foster and White, 2004; Reiter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability"
N19-1263,P09-1011,0,0.0392243,"nstead of F1 on fulllength predictions. 2560 Model ROUGE-1 ROUGE-2 Durrett et al. (2016) Paulus et al. (2018) 42.2 42.9 24.9 26.0 This work (S EQ 2 SEQ) † This work (ATT E XP ) 41.9 42.5 25.1 25.7 † This 43.2 26.4 work (A DA D EC) Table 3: NYT text summarization test performance in ROUGE recall values. This is a smaller portion of the original test data, after filtering out instances with summaries shorter than 50 tokens (§4.2; Durrett et al., 2016). † denotes the models using retrieved exemplars, and bold font indicates best performance. seen as a table consisting of a collection of records (Liang et al., 2009). For a given entity, each record is an (attribute, value) tuple. Figure 2 shows an example for entity Jacques-Louis David. The table specifies the entity’s properties with tuples (born, 30 August 1748), (nationality, French), and so forth. The table is paired with a description, which the model is supposed to generate using the table as input. We refer the readers to Lebret et al. (2016) for more details about the task. Dataset and implementation details. We use the Wikibio dataset (Lebret et al., 2016). It is automatically constructed by pairing the tables and the opening sentences of biogra"
N19-1263,P18-1123,0,0.179153,"tt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability problems with handcrafted templates, it has been proposed to use similar training samples as exemplars, to guide the decoding process (Gu et al., 2018; Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018a, inter alia).1 In general, existing methods accomplish this by (a) using traditional information retrieval (IR) techniques for exemplar extraction (e.g., TF-IDF), and then (b) concatenating the exemplar to the source as additional inputs, allowing the decoder to attend over and copy from both. We propose a different strategy for using exemplars. For motivation, Figure 1 shows a sourcetarget pair together with its exemplar from the Gigaword dataset (Graff et al., 2003). The target is a summary of the source sentence, and the exemplar is retrieved from the training set (§3.2)"
N19-1263,P02-1040,0,0.104453,"Missing"
N19-1263,D18-1152,1,0.829983,"oder construction. 1: 2: 3: 4: 5: 6: procedure (x) Retrieve the exemplar zx . §3.2 Compute zx ’s representation a . §3.2 Compute coefficients . Eq.11 Construct the decoder f . Eqs.8, 10 end procedure Closing this section, Algorithm 1 summarizes the procedure to construct an adaptive decoder. 3.3 Discussion. Although we’ve based our discussion on Elman networks so far, it is straightforward to apply this method to its gated variants (Hochreiter and Schmidhuber, 1997; Cho et al., 2014, inter alia), and other quasi-/non-recurrent neural architectures (Bradbury et al., 2017; Vaswani et al., 2017; Peng et al., 2018a, inter alia). Throughout the experiments, we will be using an adaptive LSTM decoder (§4). As a drop-in replacement in the encoder-decoder architecture, it introduces a reasonable amount of additional parameters and computational overhead, especially when one uses a small encoder for the exemplar (i.e., the sizes of the ci vectors in Equation 11 are small). It can benefit from the highly-optimized GPU implementations, e.g., CuDNN, since it uses the same recurrent computation as a standard nonadaptive RNN. In addition to the neural networks, the adaptive decoder requires access to the full tra"
N19-1263,P17-1186,1,0.815842,"r h i (p) Vp = v1 , . . . , vr(p) . (7b) Equations 5 and 6 can be compactly written as P = Up ⇤ Vp&gt; . (8) where ⇤ is the diagonal matrix built from the rdimensional coefficient vector = [ 1 , . . . , r ]&gt; : 2 3 6 ⇤ = diag( ) = 4 1 .. . r 7 5. (9) The construction of Q is similar, but with a different set of parameters matrices Uq and Vq :5 Q = Uq ⇤ Vq&gt; . (10) Note that, despite their similarities to SVD at a first glance, Equations 8 and 10 are not performing matrix factorization. Rather, we are learning {Up , Vp , Uq , Vq } directly; P, Q, {Pi }, and {Qi } are never explicitly instantiated (Peng et al., 2017, 2018c). To summarize, we reparameterize P and Q as interpolations of rank-1 matrices. By the fact that rank(A + B)  rank(A) + rank(B), the ranks of P and Q are upper-bounded by r. As pointed out by Krueger and Memisevic (2017), the parameter matrices of a trained RNN tend to have full rank. Therefore, in the experiments, we set r equal to the hidden size d, aiming to allow the adaptive decoder to use full-rank matrices in the recurrent computation. Yet, if one holds a priori beliefs that the matrices should have lower ranks, using r &lt; d could be desirable. When r = d, an adaptive RNN constr"
N19-1263,P18-1173,1,0.807432,"oder construction. 1: 2: 3: 4: 5: 6: procedure (x) Retrieve the exemplar zx . §3.2 Compute zx ’s representation a . §3.2 Compute coefficients . Eq.11 Construct the decoder f . Eqs.8, 10 end procedure Closing this section, Algorithm 1 summarizes the procedure to construct an adaptive decoder. 3.3 Discussion. Although we’ve based our discussion on Elman networks so far, it is straightforward to apply this method to its gated variants (Hochreiter and Schmidhuber, 1997; Cho et al., 2014, inter alia), and other quasi-/non-recurrent neural architectures (Bradbury et al., 2017; Vaswani et al., 2017; Peng et al., 2018a, inter alia). Throughout the experiments, we will be using an adaptive LSTM decoder (§4). As a drop-in replacement in the encoder-decoder architecture, it introduces a reasonable amount of additional parameters and computational overhead, especially when one uses a small encoder for the exemplar (i.e., the sizes of the ci vectors in Equation 11 are small). It can benefit from the highly-optimized GPU implementations, e.g., CuDNN, since it uses the same recurrent computation as a standard nonadaptive RNN. In addition to the neural networks, the adaptive decoder requires access to the full tra"
N19-1263,N18-1135,1,0.833271,"oder construction. 1: 2: 3: 4: 5: 6: procedure (x) Retrieve the exemplar zx . §3.2 Compute zx ’s representation a . §3.2 Compute coefficients . Eq.11 Construct the decoder f . Eqs.8, 10 end procedure Closing this section, Algorithm 1 summarizes the procedure to construct an adaptive decoder. 3.3 Discussion. Although we’ve based our discussion on Elman networks so far, it is straightforward to apply this method to its gated variants (Hochreiter and Schmidhuber, 1997; Cho et al., 2014, inter alia), and other quasi-/non-recurrent neural architectures (Bradbury et al., 2017; Vaswani et al., 2017; Peng et al., 2018a, inter alia). Throughout the experiments, we will be using an adaptive LSTM decoder (§4). As a drop-in replacement in the encoder-decoder architecture, it introduces a reasonable amount of additional parameters and computational overhead, especially when one uses a small encoder for the exemplar (i.e., the sizes of the ci vectors in Equation 11 are small). It can benefit from the highly-optimized GPU implementations, e.g., CuDNN, since it uses the same recurrent computation as a standard nonadaptive RNN. In addition to the neural networks, the adaptive decoder requires access to the full tra"
N19-1263,W04-1013,0,0.0798224,"ng 939.0 and 48.6, respectively). Table 1 summarizes some statistics of the datasets. We note that some recent works use a different split of the NYT corpus (Paulus et al., 2018; Gehrmann et al., 2018), and thus are not comparable to the models in Table 3. We decide to use the one by Durrett et al. (2016) because their preprocessing script is publicly available. For both datasets, we apply byte-paired encoding (BPE; Sennrich et al., 2016), which proves to improve the generation of proper nouns (Fan et al., 2018). Empirical results. Table 2 compares the models on Gigaword test set in ROUGE F1 (Lin, 2004).11 By using adaptive decoders, our model (A DA D EC) improves over S EQ 2 SEQ by more than 1.1 ROUGE scores. Cao et al. (2018b) and the F ULL model by Cao et al. (2018a) hold the best published results. The former uses extensive handcrafted features and relies on external information extraction and syntactic parsing systems; 10 https://github.com/gregdurrett/ berkeley-doc-summarizer. 11 Version 1.5.5 of the official script. Model R G-1 R G-2 R G-L Open-NMT † Cao et al., 2018a (BASIC ) † Cao et al., 2018a (F ULL ) ? Cao et al. (2018b) 35.0 36.0 37.0 37.3 16.6 17.1 19.0 17.6 32.4 33.2 34.5 34.2"
N19-1263,D14-1162,0,0.0865355,"., 2016). It is automatically constructed by pairing the tables and the opening sentences of biography articles from English Wikipedia. We follow the split and preprocessing provided along with the dataset, with around 583K/73K/73K train/dev./test instances. Following Lebret et al. (2016), we linearize the tables, such that we can conveniently train the sequence-to-sequence style models described in §4.1. Table 1 summarizes some statistics of the dataset. In contrast to the text summarization experiment (§4.2), we do not apply BPE here. Further, the word embeddings are initialized with GloVe (Pennington et al., 2014; fixed during training), and not tied with the softmax weights. In addition to the models introduced in §4.1, we additionally compare to A DA D EC +ATT E XP, aiming to study whether the adaptive decoder can further benefit from attention and copy mechanisms over the exemplars. Empirical results. Following Liu et al. (2018) we report ROUGE-4 and B LEU scores (Papineni Jacques-Louis David (30 August 1748 – 29 December 1825) was a French painter in the Neoclassical style. Figure 2: A training instance from the Wikibio dataset. It consists of a collections of records for Jacques-Louis David (top)"
N19-1263,D18-1039,0,0.21384,"es of the ci vectors in Equation 11 are small). It can benefit from the highly-optimized GPU implementations, e.g., CuDNN, since it uses the same recurrent computation as a standard nonadaptive RNN. In addition to the neural networks, the adaptive decoder requires access to the full training set due to the retrieval step. In this sense it is semi-parametric.9 The idea to dynamically construct the parameters is inspired by Hypernetworks (Ha et al., 2017) and earlier works therein. It proves successful in tasks such as classification (Jia et al., 2016; Liu et al., 2017) and machine translation (Platanios et al., 2018). Many recent template-based generation models include the exemplars as content in addition to the source, and allow the decoder to attend over and copy from both (Gu et al., 2018; Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018a, inter alia). We compare to this approach in the experiments, and show that our model offers fa9 Nothing prohibits adaptively constructing other components of the model, e.g., the encoder g✓ . Yet, our motivation is to use exemplars to inform how to say it, which is primarily determined by the decoder (in contrast, the encoder relates more"
N19-1263,E17-2025,0,0.0220484,"describe the architectures of the compared models in §4.1. 4.1 Compared Models In addition to previous works, we compare to the following baselines, aiming to control for confounding factors due to detailed implementation choices. • S EQ 2 SEQ. The encoder-decoder architecture enhanced with attention and copy mechanisms. The encoder is implemented with a bi-directional LSTM (BiLSTM; Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997; Graves, 2012), and the decoder a uni-directional one. We tie the input embeddings of both the encoder and the decoder, as well as the softmax weights (Press and Wolf, 2017). We use beam search during evaluation, with length penalty (Wu et al., 2016). • ATT E XP. It is based on S EQ 2 SEQ. It encodes, attends over, and copies from the exemplars, in addition to the source inputs. Our model using the adaptive decoder (A DA D EC) closely builds upon S EQ 2 SEQ. It uses a dynamically constructed LSTM decoder, and does not use attention or copy mechanisms over the encoded exemplars. The extracted exemplars are the same as those used by ATT E XP. To ensure fair comparisons, we use comparable training procedures and regularization techniques for the above models. The re"
N19-1263,D15-1044,0,0.071714,"the same as those used by ATT E XP. To ensure fair comparisons, we use comparable training procedures and regularization techniques for the above models. The readers are referred to the appendix for further details such as hyperparameters. 2559 4.2 Text Summarization Datasets. We empirically evaluate our model on two benchmark text summarization datasets: • Annotated Gigaword corpus (Gigaword; Graff et al., 2003; Napoles et al., 2012). Gigaword contains news articles sourced from various news services over the last two decades. To produce the dataset, we follow the split and preprocessing by Rush et al. (2015), and pair the first sentences and the headlines in the news articles. It results in a 3.8M/190K/1,951 train/dev./test split. The average lengths of the source and target texts are 31.4 and 8.2, respectively. • New York Times Annotated Corpus (NYT; Sandaus, 2008). It contains news articles published between 1996 and 2007 by New York Times. We use the split and preprocessing by Durrett et al. (2016).10 Following their effort, we evaluate on a smaller portion of the test set, where the gold summaries are longer than 50 tokens. We further randomly sample 9,000 instances from the training data for"
N19-1263,P17-1099,0,0.371675,"oned text generation is the essence of many natural language processing (NLP) tasks, e.g., text summarization (Mani, 1999), machine translation (Koehn, 2009), and data-to-text generation (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997). In its common neural sequence-tosequence formulation (Sutskever et al., 2014; Cho et al., 2014), an encoder-decoder architecture is used. The decoder generates the text autoregressively, token-by-token, conditioning on the feature representations encoded from the source, typically with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016; See et al., 2017). This paradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degenerate outputs and favoring generic utterances (Vinyals and Le, 2015; Li et al., 2016). The encoder-decoder approach differs considerably from earlier template-based meth⇤ Work done during internship at Google. ods (Becker, 2002; Foster and White, 2004; Reiter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural appr"
N19-1263,P16-1162,0,0.0246657,"stances from the training data for validation, resulting in a 91,834/9,000/3,452 train/dev./test split. Compared to Gigaword, the inputs and targets in NYT are much longer (averaging 939.0 and 48.6, respectively). Table 1 summarizes some statistics of the datasets. We note that some recent works use a different split of the NYT corpus (Paulus et al., 2018; Gehrmann et al., 2018), and thus are not comparable to the models in Table 3. We decide to use the one by Durrett et al. (2016) because their preprocessing script is publicly available. For both datasets, we apply byte-paired encoding (BPE; Sennrich et al., 2016), which proves to improve the generation of proper nouns (Fan et al., 2018). Empirical results. Table 2 compares the models on Gigaword test set in ROUGE F1 (Lin, 2004).11 By using adaptive decoders, our model (A DA D EC) improves over S EQ 2 SEQ by more than 1.1 ROUGE scores. Cao et al. (2018b) and the F ULL model by Cao et al. (2018a) hold the best published results. The former uses extensive handcrafted features and relies on external information extraction and syntactic parsing systems; 10 https://github.com/gregdurrett/ berkeley-doc-summarizer. 11 Version 1.5.5 of the official script. Mod"
N19-1263,W18-5713,0,0.312935,"iter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability problems with handcrafted templates, it has been proposed to use similar training samples as exemplars, to guide the decoding process (Gu et al., 2018; Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018a, inter alia).1 In general, existing methods accomplish this by (a) using traditional information retrieval (IR) techniques for exemplar extraction (e.g., TF-IDF), and then (b) concatenating the exemplar to the source as additional inputs, allowing the decoder to attend over and copy from both. We propose a different strategy for using exemplars. For motivation, Figure 1 shows a sourcetarget pair together with its exemplar from the Gigaword dataset (Graff et al., 2003). The target is a summary of the source sentence, and the exemplar is retrieved from th"
N19-1263,D17-1239,0,0.0873391,"aradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degenerate outputs and favoring generic utterances (Vinyals and Le, 2015; Li et al., 2016). The encoder-decoder approach differs considerably from earlier template-based meth⇤ Work done during internship at Google. ods (Becker, 2002; Foster and White, 2004; Reiter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability problems with handcrafted templates, it has been proposed to use similar training samples as exemplars, to guide the decoding process (Gu et al., 2018; Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018a, inter alia).1 In general, existing methods accomplish this by (a) using traditional information retrieval (IR) techniques for exemplar extraction (e.g., TF-IDF), and then (b"
N19-1263,D18-1356,0,0.437454,"al., 2014). In the interest of the noConditioned text generation and the encoderdecoder architecture. Our discussion centers around conditioned text generation, i.e., the model aims to output the target y = y1 y2 . . . yT given the source input x = x1 x2 . . . xS , both of which are sequences of tokens. Each token xi , yi takes one value from a vocabulary V. x and y could vary depending on the tasks, e.g., they will respectively be articles and summaries for text summarization; and for data-to-text generation, x would be structured data, which can sometimes be linearized (Lebret et al., 2016; Wiseman et al., 2018, inter alia), and y is the output text. We aim to learn a (parameterized) conditional distribution of the target text y given the source x, p y|x = T Y t=1 p yt |y&lt;t , x , (1) where y&lt;t = y1 . . . yt 1 is the prefix of y up to the (t 1)th token (inclusive). The probability of each target token is usually estimated with a softmax function: exp h&gt; t 1 w yt p (yt |y&lt;t , x) = P . &gt; y exp ht 1 wy (2) wy denotes a learned vector for token y 2 V. ht 1 depends on y&lt;t and x, and is computed by a function which we will describe soon. A typical implementation choice for computing ht is the encoder-decod"
P09-1053,W04-3219,0,0.0968852,"We estimate the distributions over dependency labels, POS tags, and named entity classes using the transformed treebank (footnote 4). The distribution over words is taken from the Gigaword corpus (as in §3.4). It is important to note that G0 is designed to give a smoothed estimate of the probability of a particular parsed, named entity-tagged sentence. It is never used for parsing or for generation; it is only used as a component in the generative probability model presented in §2 (Eq. 2). 3.6 4 In all our experiments, we have used the Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Quirk et al., 2004). The corpus contains 5,801 pairs of sentences that have been marked as “equivalent” or “not equivalent.” It was constructed from thousands of news sources on the web. Dolan and Brockett (2005) remark that this corpus was created semi-automatically by first training an SVM classifier on a disjoint annotated 10,000 sentence pair dataset and then applying the SVM on an unseen 49,375 sentence pair corpus, with its output probabilities skewed towards over-identification, i.e., towards generating some false paraphrases. 5,801 out of these 49,375 pairs were randomly selected and presented to human j"
P09-1053,W96-0213,0,0.665591,"ank (see footnote 4). For unobserved cases, the conditional probability is estimated by backing off to the parent POS tag and child direction. We discuss next how to parameterize the probability pkid that appears in Equations 4, 5, and 6. This conditional distribution forms the core of our QGs, and we deviate from earlier research using QGs in defining pkid in a fully generative way. In addition to assuming that dependency parse trees for s and t are observable, we also assume each word wi comes with POS and named entity tags. In our experiments these were obtained automatically using MXPOST (Ratnaparkhi, 1996) and BBN’s Identifinder (Bikel et al., 1999). s t, x P (τ t,j |tj , x(j), τ s ) j∈λt (i)∪ρt (i) x(j)=0 pQ (t |Gp (s)) = p(τ |Gp (τ )) = m X Y |Gp (τ s )) (3) Because the QG is essentially a context-free dependency grammar, we can factor it into recursive steps as follows (let i be an arbitrary index in {1, ..., n}): P (τ t,i |ti , x(i), τ s ) = pval (|λt (i)|, |ρt (i) ||ti ) 4 In our experiments, we use the parser described by McDonald et al. (2005), trained on sections 2–21 of the WSJ Penn Treebank, transformed to dependency trees following Yamada and Matsumoto (2003). (The same treebank data"
P09-1053,W06-3104,0,0.0876301,"niversity Pittsburgh, PA 15213, USA {dipanjan,nasmith}@cs.cmu.edu Abstract task (Zhang and Patrick, 2005; Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005, inter alia), but do not explicitly model correspondence structure (or “alignment”) between the parts of two sentences. In this paper, we adopt a model that posits correspondence between the words in the two sentences, defining it in loose syntactic terms: if two sentences are paraphrases, we expect their dependency trees to align closely, though some divergences are also expected, with some more likely than others. Following Smith and Eisner (2006), we adopt the view that the syntactic structure of sentences paraphrasing some sentence s should be “inspired” by the structure of s. Because dependency syntax is still only a crude approximation to semantic structure, we augment the model with a lexical semantics component, based on WordNet (Miller, 1995), that models how words are probabilistically altered in generating a paraphrase. This combination of loose syntax and lexical semantics is similar to the “Jeopardy” model of Wang et al. (2007). This syntactic framework represents a major departure from useful and popular surface similarity"
P09-1053,2003.mtsummit-papers.51,0,0.036401,"Missing"
P09-1053,U06-1019,0,0.715078,"generating a paraphrase. This combination of loose syntax and lexical semantics is similar to the “Jeopardy” model of Wang et al. (2007). This syntactic framework represents a major departure from useful and popular surface similarity features, and the latter are difficult to incorporate into our probabilistic model. We use a product of experts (Hinton, 2002) to bring together a logistic regression classifier built from n-gram overlap features and our syntactic model. This combined model leverages complementary strengths of the two approaches, outperforming a strong state-ofthe-art baseline (Wan et al., 2006). This paper is organized as follows. We introduce our probabilistic model in §2. The model makes use of three quasi-synchronous grammar models (Smith and Eisner, 2006, QG, hereafter) as components (one modeling paraphrase, one modeling not-paraphrase, and one a base grammar); these are detailed, along with latent-variable inference and discriminative training algorithms, in §3. We discuss the Microsoft Research Paraphrase Corpus, upon which we conduct experiments, in §4. In §5, we present experiments on paraphrase We present a novel approach to deciding whether two sentences hold a paraphrase"
P09-1053,D07-1003,1,0.749287,"sely, though some divergences are also expected, with some more likely than others. Following Smith and Eisner (2006), we adopt the view that the syntactic structure of sentences paraphrasing some sentence s should be “inspired” by the structure of s. Because dependency syntax is still only a crude approximation to semantic structure, we augment the model with a lexical semantics component, based on WordNet (Miller, 1995), that models how words are probabilistically altered in generating a paraphrase. This combination of loose syntax and lexical semantics is similar to the “Jeopardy” model of Wang et al. (2007). This syntactic framework represents a major departure from useful and popular surface similarity features, and the latter are difficult to incorporate into our probabilistic model. We use a product of experts (Hinton, 2002) to bring together a logistic regression classifier built from n-gram overlap features and our syntactic model. This combined model leverages complementary strengths of the two approaches, outperforming a strong state-ofthe-art baseline (Wan et al., 2006). This paper is organized as follows. We introduce our probabilistic model in §2. The model makes use of three quasi-syn"
P09-1053,J97-3002,0,0.114124,"Missing"
P09-1053,W05-1205,0,0.240934,"Missing"
P09-1053,W03-3023,0,0.0319775,"ained automatically using MXPOST (Ratnaparkhi, 1996) and BBN’s Identifinder (Bikel et al., 1999). s t, x P (τ t,j |tj , x(j), τ s ) j∈λt (i)∪ρt (i) x(j)=0 pQ (t |Gp (s)) = p(τ |Gp (τ )) = m X Y |Gp (τ s )) (3) Because the QG is essentially a context-free dependency grammar, we can factor it into recursive steps as follows (let i be an arbitrary index in {1, ..., n}): P (τ t,i |ti , x(i), τ s ) = pval (|λt (i)|, |ρt (i) ||ti ) 4 In our experiments, we use the parser described by McDonald et al. (2005), trained on sections 2–21 of the WSJ Penn Treebank, transformed to dependency trees following Yamada and Matsumoto (2003). (The same treebank data were also to estimate many of the parameters of our model, as discussed in the text.) Though it leads to a partial “pipeline” approximation of the posterior probability p(c |s, t), we believe that the relatively high quality of English dependency parsing makes this approximation reasonable. 470 For clarity, let j = τpt (i) and let l = x(j). Configuration Description parent-child τps (x(i)) = x(j), appended with τ`s (x(i)) child-parent x(i) = τps (x(j)), appended with τ`s (x(j)) grandparent- τps (τps (x(i))) = x(j), appended with grandchild τ`s (x(i)) siblings τps (x(i"
P09-1053,U05-1023,0,0.132167,"Missing"
P09-1053,N03-1003,0,0.135255,"atures. We evaluate our models on the task of distinguishing true paraphrase pairs from false ones on a standard corpus, giving competitive state-of-the-art performance. 1 Introduction The problem of modeling paraphrase relationships between natural language utterances (McKeown, 1979) has recently attracted interest. For computational linguists, solving this problem may shed light on how best to model the semantics of sentences. For natural language engineers, the problem bears on information management systems like abstractive summarizers that must measure semantic overlap between sentences (Barzilay and Lee, 2003), question answering modules (Marsi and Krahmer, 2005) and machine translation (Callison-Burch et al., 2006). The paraphrase identification problem asks whether two sentences have essentially the same meaning. Although paraphrase identification is defined in semantic terms, it is usually solved using statistical classifiers based on shallow lexical, n-gram, and syntactic “overlap” features. Such overlap features give the best-published classification accuracy for the paraphrase identification 468 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 468–476,"
P09-1053,N06-1003,0,0.050362,"standard corpus, giving competitive state-of-the-art performance. 1 Introduction The problem of modeling paraphrase relationships between natural language utterances (McKeown, 1979) has recently attracted interest. For computational linguists, solving this problem may shed light on how best to model the semantics of sentences. For natural language engineers, the problem bears on information management systems like abstractive summarizers that must measure semantic overlap between sentences (Barzilay and Lee, 2003), question answering modules (Marsi and Krahmer, 2005) and machine translation (Callison-Burch et al., 2006). The paraphrase identification problem asks whether two sentences have essentially the same meaning. Although paraphrase identification is defined in semantic terms, it is usually solved using statistical classifiers based on shallow lexical, n-gram, and syntactic “overlap” features. Such overlap features give the best-published classification accuracy for the paraphrase identification 468 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 468–476, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP identification with our model and make comparisons"
P09-1053,W05-1203,0,0.0490343,"Missing"
P09-1053,I05-5002,0,0.0364744,"d corpus (as in §3.4). It is important to note that G0 is designed to give a smoothed estimate of the probability of a particular parsed, named entity-tagged sentence. It is never used for parsing or for generation; it is only used as a component in the generative probability model presented in §2 (Eq. 2). 3.6 4 In all our experiments, we have used the Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Quirk et al., 2004). The corpus contains 5,801 pairs of sentences that have been marked as “equivalent” or “not equivalent.” It was constructed from thousands of news sources on the web. Dolan and Brockett (2005) remark that this corpus was created semi-automatically by first training an SVM classifier on a disjoint annotated 10,000 sentence pair dataset and then applying the SVM on an unseen 49,375 sentence pair corpus, with its output probabilities skewed towards over-identification, i.e., towards generating some false paraphrases. 5,801 out of these 49,375 pairs were randomly selected and presented to human judges for refinement into true and false paraphrases. 3,900 of the pairs were marked as having Discriminative Training D (i) EN (i) Given training data hs1 , s2 , c(i) i , we train i=1 the mode"
P09-1053,C04-1051,0,0.650031,"(s |G0 ) = C 0 (0). We estimate the distributions over dependency labels, POS tags, and named entity classes using the transformed treebank (footnote 4). The distribution over words is taken from the Gigaword corpus (as in §3.4). It is important to note that G0 is designed to give a smoothed estimate of the probability of a particular parsed, named entity-tagged sentence. It is never used for parsing or for generation; it is only used as a component in the generative probability model presented in §2 (Eq. 2). 3.6 4 In all our experiments, we have used the Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Quirk et al., 2004). The corpus contains 5,801 pairs of sentences that have been marked as “equivalent” or “not equivalent.” It was constructed from thousands of news sources on the web. Dolan and Brockett (2005) remark that this corpus was created semi-automatically by first training an SVM classifier on a disjoint annotated 10,000 sentence pair dataset and then applying the SVM on an unseen 49,375 sentence pair corpus, with its output probabilities skewed towards over-identification, i.e., towards generating some false paraphrases. 5,801 out of these 49,375 pairs were randomly selected and"
P09-1053,W07-1401,0,0.0226089,"L Wan et al. SVM and pQ pQ and pL Accuracy 66.49 75.63 75.42 68.64 73.33 73.86 75.36 76.06 80.17 83.42 83.19 Results Recall 100.00 90.00 90.14 96.51 91.10 91.28 87.44 86.05 92.07 96.60 95.29 Table 2: Accuracy, p-class precision, and p-class recall on the test set (N = 1,725). See text for differences in implementation between Wan et al. and our replication; their reported score does not include the full test set. the more intricate QG to the straightforward SVM. First, the QG discovers hidden alignments between words. Alignments have been leveraged in related tasks such as textual entailment (Giampiccolo et al., 2007); they make the model more interpretable in analyzing system output (e.g., Fig. 2). Second, the paraphrases of a sentence can be considered to be monolingual translations. We model the paraphrase problem using a direct machine translation model, thus providing a translation interpretation of the problem. This framework could be extended to permit paraphrase generation, or to exploit other linguistic annotations, such as representations of semantics (see, e.g., Qiu et al., 2006). Nonetheless, the usefulness of surface overlap features is difficult to ignore. We next provide an efficient way to"
P09-1053,W05-1612,0,0.0146939,"uishing true paraphrase pairs from false ones on a standard corpus, giving competitive state-of-the-art performance. 1 Introduction The problem of modeling paraphrase relationships between natural language utterances (McKeown, 1979) has recently attracted interest. For computational linguists, solving this problem may shed light on how best to model the semantics of sentences. For natural language engineers, the problem bears on information management systems like abstractive summarizers that must measure semantic overlap between sentences (Barzilay and Lee, 2003), question answering modules (Marsi and Krahmer, 2005) and machine translation (Callison-Burch et al., 2006). The paraphrase identification problem asks whether two sentences have essentially the same meaning. Although paraphrase identification is defined in semantic terms, it is usually solved using statistical classifiers based on shallow lexical, n-gram, and syntactic “overlap” features. Such overlap features give the best-published classification accuracy for the paraphrase identification 468 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 468–476, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFN"
P09-1053,P05-1012,0,0.0848019,"t are observable, we also assume each word wi comes with POS and named entity tags. In our experiments these were obtained automatically using MXPOST (Ratnaparkhi, 1996) and BBN’s Identifinder (Bikel et al., 1999). s t, x P (τ t,j |tj , x(j), τ s ) j∈λt (i)∪ρt (i) x(j)=0 pQ (t |Gp (s)) = p(τ |Gp (τ )) = m X Y |Gp (τ s )) (3) Because the QG is essentially a context-free dependency grammar, we can factor it into recursive steps as follows (let i be an arbitrary index in {1, ..., n}): P (τ t,i |ti , x(i), τ s ) = pval (|λt (i)|, |ρt (i) ||ti ) 4 In our experiments, we use the parser described by McDonald et al. (2005), trained on sections 2–21 of the WSJ Penn Treebank, transformed to dependency trees following Yamada and Matsumoto (2003). (The same treebank data were also to estimate many of the parameters of our model, as discussed in the text.) Though it leads to a partial “pipeline” approximation of the posterior probability p(c |s, t), we believe that the relatively high quality of English dependency parsing makes this approximation reasonable. 470 For clarity, let j = τpt (i) and let l = x(j). Configuration Description parent-child τps (x(i)) = x(j), appended with τ`s (x(i)) child-parent x(i) = τps (x"
P09-1053,P79-1016,0,0.676755,"paraphrase relationship. The model cleanly incorporates both syntax and lexical semantics using quasi-synchronous dependency grammars (Smith and Eisner, 2006). Furthermore, using a product of experts (Hinton, 2002), we combine the model with a complementary logistic regression model based on state-of-the-art lexical overlap features. We evaluate our models on the task of distinguishing true paraphrase pairs from false ones on a standard corpus, giving competitive state-of-the-art performance. 1 Introduction The problem of modeling paraphrase relationships between natural language utterances (McKeown, 1979) has recently attracted interest. For computational linguists, solving this problem may shed light on how best to model the semantics of sentences. For natural language engineers, the problem bears on information management systems like abstractive summarizers that must measure semantic overlap between sentences (Barzilay and Lee, 2003), question answering modules (Marsi and Krahmer, 2005) and machine translation (Callison-Burch et al., 2006). The paraphrase identification problem asks whether two sentences have essentially the same meaning. Although paraphrase identification is defined in sem"
P09-1053,P04-1083,0,0.0205113,"which came first (i.e., which was s and which was s0 ). Both orderings are assumed to be equally probable. For class c, ht, li → ht, liht0 , ki or ht, li → ht0 , kiht, li where t and t0 range over the vocabulary of the target language, and l and k ∈ {0, ..., m} are indices in the source sentence, with 0 denoting null.3 Hard or soft constraints can be applied between l and k in a rule. These constraints imply permissible “configurations.” For example, requiring l 6= 0 and, if k 6= 0 then sk must be a child of sl in the source tree, we can implement a synchronous dependency grammar similar to (Melamed, 2004). Smith and Eisner (2006) used a quasisynchronous grammar to discover the correspondence between words implied by the correspondence between the trees. We follow Wang et al. (2007) in treating the correspondences as latent variables, and in using a WordNet-based lexical semantics model to generate the target words. pQ (s1 , s2 |c) = 0.5 × pQ (s1 |G0 ) × pQ (s2 |Gc (s1 )) + 0.5 × pQ (s2 |G0 ) × pQ (s1 |Gc (s2 ))(2) where c can be p or n; Gp (s) is the QG that generates paraphrases for sentence s, while Gn (s) is the QG that generates sentences that are not paraphrases of sentence s. This latter"
P09-1053,2001.mtsummit-papers.68,0,0.0416791,"Missing"
P09-1053,W06-1603,0,0.617126,"hidden alignments between words. Alignments have been leveraged in related tasks such as textual entailment (Giampiccolo et al., 2007); they make the model more interpretable in analyzing system output (e.g., Fig. 2). Second, the paraphrases of a sentence can be considered to be monolingual translations. We model the paraphrase problem using a direct machine translation model, thus providing a translation interpretation of the problem. This framework could be extended to permit paraphrase generation, or to exploit other linguistic annotations, such as representations of semantics (see, e.g., Qiu et al., 2006). Nonetheless, the usefulness of surface overlap features is difficult to ignore. We next provide an efficient way to combine a surface model with pQ . Tab. 2 shows performance achieved by the baseline SVM and variations on pQ on the test set. We performed a few feature ablation studies, evaluating on the development data. We removed the lexical semantics component of the QG,10 and disallowed the syntactic configurations one by one, to investigate which components of pQ contributes to system performance. The lexical semantics component is critical, as seen by the drop in accuracy from the tabl"
P09-1053,P02-1040,0,\N,Missing
P09-1053,I05-5003,0,\N,Missing
P11-1061,N10-1083,0,0.250923,"Missing"
P11-1061,A00-1031,0,0.143858,"n into account yet. • With LP: Our full model uses both stages of label propagation (Eq. 2) before extracting the constraint features. As a result, we are able to extract the constraint feature for all foreign word types and furthermore expect the projected tag distributions to be smoother and more stable. Our oracles took advantage of the labeled treebanks: • TB Dictionary: We extracted tagging dictionaries from the treebanks and and used them as constraint features in the feature-based HMM. Evaluation was done using the prespecified mappings. • Supervised: We trained the supervised model of Brants (2000) on the original treebanks and mapped the language-specific tags to the universal tags for evaluation. 6.4 Experimental Setup While we tried to minimize the number of free parameters in our model, there are a few hyperparameters that need to be set. Fortunately, performance was stable across various values, and we were able to use the same hyperparameters for all languages. We used C = 1.0 as the L2 regularization constant in (Eq. 10) and trained both EM and L-BFGS for 1000 iterations. When extracting the vector Model EM-HMM baselines Feature-HMM Projection No LP our approach With LP TB Dictio"
P11-1061,J93-2003,0,0.0267443,"Missing"
P11-1061,W06-2920,0,0.320179,"e presenting our results, we describe the datasets that we used, as well as two baselines. 6.1 z −CkΘk22 Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side. However, we do not explore this possibility in the current work. Datasets We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side. The availability of these resources guided our selection of foreign languages. For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). The parallel data came from the Europarl corpus (Koehn, 2005) and the ODS United Nations dataset (UN, 2006). Taking the intersection of languages in these resources, and selecting languages with large amounts of parallel data, yields the following set of eight Indo-European languages: Danish, Dutch, German, Greek, Italian, Portuguese, Spanish and Swedish. Of course, we are primarily interested in applying our techniques to languages for which no labeled resources are available. However, we needed to restrict ourselves to these languages in order to be able to evaluate th"
P11-1061,D10-1056,0,0.0918227,"anguages. Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus. As discussed in more detail in §3, we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types, while the vertices on the English side are individual word types. Graph construction does not require any labeled data, but makes use of two similarity functions. The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function, designed to indicate how syntactically 2 See Christodoulopoulos et al. (2010) for a discussion of metrics for evaluating unsupervised POS induction systems. 601 Algorithm 1 Bilingual POS Induction Require: Parallel English and foreign language data De and Df , unlabeled foreign training data Γf ; English tagger. Ensure: Θf , a set of parameters learned using a constrained unsupervised model (§5). 1: D e↔f ← word-align-bitext(D e , D f ) ce ← pos-tag-supervised(De ) 2: D ce ) 3: A ← extract-alignments(D e↔f , D f f 4: G ← construct-graph(Γ , D , A) ˜ ← graph-propagate(G) 5: G ˜ 6: ∆ ← extract-word-constraints(G) f 7: Θ ← pos-induce-constrained(Γf , ∆) 8: Return Θf simil"
P11-1061,P09-1042,0,0.572213,"Missing"
P11-1061,D07-1031,0,0.0799468,"or our experiments; however, one could as well have fixed the set of HMM states to be a constant across languages, and created one mapping to the universal POS tagset. 6.3 Various Models To provide a thorough analysis, we evaluated three baselines and two oracles in addition to two variants of our graph-based approach. We were intentionally lenient with our baselines: • EM-HMM: A traditional HMM baseline, with multinomial emission and transition distributions estimated by the Expectation Maximization algorithm. We evaluated POS tagging accuracy using the lenient many-to-1 evaluation approach (Johnson, 2007). • Feature-HMM: The vanilla feature-HMM of Berg-Kirkpatrick et al. (2010) (i.e. no additional constraint feature) served as a second baseline. Model parameters were estimated with L-BFGS and evaluation again used a greedy many-to-1 mapping. • Projection: Our third baseline incorporates bilingual information by projecting POS tags directly across alignments in the parallel data. For unaligned words, we set the tag to the most frequent tag in the corresponding treebank. For 606 each language, we took the same number of sentences from the bitext as there are in its treebank, and trained a superv"
P11-1061,2005.mtsummit-papers.11,0,0.0472091,"−CkΘk22 Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side. However, we do not explore this possibility in the current work. Datasets We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side. The availability of these resources guided our selection of foreign languages. For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). The parallel data came from the Europarl corpus (Koehn, 2005) and the ODS United Nations dataset (UN, 2006). Taking the intersection of languages in these resources, and selecting languages with large amounts of parallel data, yields the following set of eight Indo-European languages: Danish, Dutch, German, Greek, Italian, Portuguese, Spanish and Swedish. Of course, we are primarily interested in applying our techniques to languages for which no labeled resources are available. However, we needed to restrict ourselves to these languages in order to be able to evaluate the performance of our approach. We paid particular attention to minimize the number o"
P11-1061,J93-2004,0,0.0522945,"Missing"
P11-1061,D10-1120,0,0.173472,"Missing"
P11-1061,petrov-etal-2012-universal,1,0.831691,"Missing"
P11-1061,P09-1057,0,0.179296,"Missing"
P11-1061,P07-1096,0,0.045915,"Missing"
P11-1061,P09-1009,0,0.228365,"est. To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language. This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al., 2000; Xi and Hwa, 2005; Ganchev et al., 2009). Naseem et al. (2009) and Snyder et al. (2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available. Our work is closest to that of Yarowsky and Ngai (2001), but differs in two important ways. First, we use a novel graph-based framework for projecting syntactic information across language boundaries. To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4). Second, we treat the project"
P11-1061,D10-1017,1,0.393286,"l (Zhu et al., 2003). Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand, using individual words as the vertices throws away the context 3 The word alignment methods do not use POS information. necessary for disambiguation; on the other hand, it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences. Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning. More recently, Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model. They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types, and edge weights based on distributional similarity, to improve a supervised conditional random field tagger. 3.1 Graph Vertices We extend Subramanya et al.’s intuitions to our bilingual setup. Because the information flow in our graph is asymmetric (from English to the foreign language), we use different types of vertices for each language. The foreign language vertices (denoted by Vf ) correspond to fore"
P11-1061,C96-2141,0,0.211995,"Missing"
P11-1061,H05-1107,0,0.173919,"Missing"
P11-1061,N01-1026,0,0.91404,"ource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language. This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al., 2000; Xi and Hwa, 2005; Ganchev et al., 2009). Naseem et al. (2009) and Snyder et al. (2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available. Our work is closest to that of Yarowsky and Ngai (2001), but differs in two important ways. First, we use a novel graph-based framework for projecting syntactic information across language boundaries. To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4). Second, we treat the projected labels as features in an unsuper1 For simplicity of exposition we refer to the resource-poor language as the “foreign language.” Similarly, we use English as the resource-rich language, but an"
P11-1061,D07-1096,0,\N,Missing
P11-1144,boas-2002-bilingual,0,0.0377024,"stic models to improve coverage on unseen predicates.2 Expert resources have limited coverage, and FrameNet is no exception. Automatic induction of semantic resources has been a major effort in recent years (Snow et al., 2006; Ponzetto and Strube, 2007, inter alia). In the domain of frame semantics, previous work has sought to extend the coverage of FrameNet by exploiting resources like VerbNet, WordNet, or Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005). Although these approaches have increased coverage to various degrees, they rely on other lexicons and resources created by experts. F¨urstenau and Lapata (2009) proposed the use of unlabeled data to improve coverage, but their work was limited to verbs. Bejan (2009) used self-training to improve frame identification and reported improvements, but did not explicitly model unknown targets. In contrast, we use statistics gathered from large volumes of unlabeled data to improve the coverage of a frame-semantic parser on several syntactic categories,"
P11-1144,P11-1061,1,0.101357,"k an approach to word-sense disambiguation due to Niu et al. (2005). Their formulation was transductive, so that the test data was part of the constructed graph, and they did not consider predicate-argument analysis. In contrast, we make use of the smoothed graph during inference in a probabilistic setting, in turn using it for the full frame-semantic parsing task. Recently, Subramanya et al. (2010) proposed the use of a graph over substructures of an underlying sequence model, and used a smoothed graph for domain adaptation of part-of-speech taggers. Subramanya et al.’s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers. Our semi-supervised learning setting is similar to these two lines of work and, like them, we use the graph to arrive at better final structures, in an inductive setting (i.e., where a parametric model is learned and then separately applied to test data, following most NLP research). 3 Approach Overview Our overall approach to handling unobserved targets consists of four distinct stages. Before going into the details of each stage individually, we provide their overview here: Graph Construction: A graph consisting of"
P11-1144,N10-1138,1,0.0647197,"taining instantiations of semantic frames, and full-text annotations provide supervision for learning frame-semantic parsers. Yet these annotations lack coverage, including only 9,300 annotated target types. Recent papers have tried to address the coverage problem. Johansson and Nugues (2007) used WordNet (Fellbaum, 1998) to expand the list of targets that can evoke frames and trained classifiers to identify the best-suited frame for the newly created targets. In past work, we described an approach where latent variables were used in a probabilistic model to predict frames for unseen targets (Das et al., 2010a).1 Relatedly, for the argument identification subtask, Matsubayashi et al. (2009) proposed a technique for generalization of semantic roles to overcome data sparseness. Unseen targets continue to present a major obstacle to domain-general semantic analysis. In this paper, we address the problem of idenfifying the semantic frames for targets unseen either in FrameNet (including the exemplar sentences) or the collection of full-text annotations released along with the lexicon. Using a standard model for the argument identification stage (Das et al., 2010a), our proposed method improves overall"
P11-1144,erk-pado-2006-shalmaneser,0,0.414117,"Missing"
P11-1144,W03-1007,0,0.136534,"coverage for hitherto unobserved predicates (§6). 2 Background Before going into the details of our model, we provide some background on two topics relevant to this paper: frame-semantic parsing and graph-based learning applied to natural language tasks. 2.1 Frame-semantic Parsing Gildea and Jurafsky (2002) pioneered SRL, and since then there has been much applied research on predicate-argument semantics. Early work on frame-semantic role labeling made use of the exemplar sentences in the FrameNet corpus, each of which is annotated for a single frame and its arguments (Thompson et al., 2003; Fleischman et al., 2003; Shi and Mihalcea, 2004; Erk and Pad´o, 2006, inter alia). Most of this work was done on an older, smaller version of FrameNet. Recently, since the release of full-text annotations in SemEval’07 (Baker et al., 2007), there has been work on identifying multiple frames and their corresponding sets of ar1436 guments in a sentence. The LTH system of Johansson and Nugues (2007) performed the best in the SemEval’07 shared task on frame-semantic parsing. Our probabilistic frame-semantic parser outperforms LTH on that task and dataset (Das et al., 2010a). The current paper builds on those probabilist"
P11-1144,C04-1134,0,0.0378492,"to improve coverage on unseen predicates.2 Expert resources have limited coverage, and FrameNet is no exception. Automatic induction of semantic resources has been a major effort in recent years (Snow et al., 2006; Ponzetto and Strube, 2007, inter alia). In the domain of frame semantics, previous work has sought to extend the coverage of FrameNet by exploiting resources like VerbNet, WordNet, or Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005). Although these approaches have increased coverage to various degrees, they rely on other lexicons and resources created by experts. F¨urstenau and Lapata (2009) proposed the use of unlabeled data to improve coverage, but their work was limited to verbs. Bejan (2009) used self-training to improve frame identification and reported improvements, but did not explicitly model unknown targets. In contrast, we use statistics gathered from large volumes of unlabeled data to improve the coverage of a frame-semantic parser on several syntactic categories, in a novel framework"
P11-1144,E09-1026,0,0.022886,"Missing"
P11-1144,J02-3001,0,0.624656,"Missing"
P11-1144,S07-1048,0,0.525432,"he use of two statistical classifiers corresponding to the aforementioned subtasks: the first one to identify the most suitable semantic frame for a marked lexical predicate (target, henceforth) in a sentence, and the second for performing semantic role labeling (SRL) given the frame. The FrameNet lexicon, its exemplar sentences containing instantiations of semantic frames, and full-text annotations provide supervision for learning frame-semantic parsers. Yet these annotations lack coverage, including only 9,300 annotated target types. Recent papers have tried to address the coverage problem. Johansson and Nugues (2007) used WordNet (Fellbaum, 1998) to expand the list of targets that can evoke frames and trained classifiers to identify the best-suited frame for the newly created targets. In past work, we described an approach where latent variables were used in a probabilistic model to predict frames for unseen targets (Das et al., 2010a).1 Relatedly, for the argument identification subtask, Matsubayashi et al. (2009) proposed a technique for generalization of semantic roles to overcome data sparseness. Unseen targets continue to present a major obstacle to domain-general semantic analysis. In this paper, we"
P11-1144,P93-1016,0,0.0368514,"appended with a coarse POS tag, and it resembles the lexical units in the FrameNet lexicon. For example, two targets corresponding to the same lemma would look like boast.N and boast.V. Here, the first target is a noun, while the second is a verb. An example multiword target is chemical weapon.N. We use two resources for graph construction. First, we take all the words and phrases present in the dependency-based thesaurus constructed using syntactic cooccurrence statistics (Lin, 1998).3 To construct this resource, a corpus containing 64 million words was parsed with a fast dependency parser (Lin, 1993; Lin, 1994), and syntactic contexts were used to find similar lexical items for a given word 3 This resource is available at http://webdocs.cs. ualberta.ca/˜lindek/Downloads/sim.tgz POVERTY powerlessness.N rich.A deprivation.N POVERTY wealthy.A SIMILARITY variant.N SIMILARITY similarity.N Figure 2: Excerpt from a graph over targets. Green targets are SIMILARITY POVERTY observed in the FrameNet data. poverty.N resemblance.N inequality.N Above/below them are shown the destitution.N resemble.V most frequently observed frame homelessness.N SIMILARITY unemployment that these targets evoke. The bla"
P11-1144,C94-1079,0,0.12035,"ith a coarse POS tag, and it resembles the lexical units in the FrameNet lexicon. For example, two targets corresponding to the same lemma would look like boast.N and boast.V. Here, the first target is a noun, while the second is a verb. An example multiword target is chemical weapon.N. We use two resources for graph construction. First, we take all the words and phrases present in the dependency-based thesaurus constructed using syntactic cooccurrence statistics (Lin, 1998).3 To construct this resource, a corpus containing 64 million words was parsed with a fast dependency parser (Lin, 1993; Lin, 1994), and syntactic contexts were used to find similar lexical items for a given word 3 This resource is available at http://webdocs.cs. ualberta.ca/˜lindek/Downloads/sim.tgz POVERTY powerlessness.N rich.A deprivation.N POVERTY wealthy.A SIMILARITY variant.N SIMILARITY similarity.N Figure 2: Excerpt from a graph over targets. Green targets are SIMILARITY POVERTY observed in the FrameNet data. poverty.N resemblance.N inequality.N Above/below them are shown the destitution.N resemble.V most frequently observed frame homelessness.N SIMILARITY unemployment that these targets evoke. The black diﬀerence"
P11-1144,P98-2127,0,0.164038,"nstruction We construct a graph with targets as vertices. For us, each target corresponds to a lemmatized word or phrase appended with a coarse POS tag, and it resembles the lexical units in the FrameNet lexicon. For example, two targets corresponding to the same lemma would look like boast.N and boast.V. Here, the first target is a noun, while the second is a verb. An example multiword target is chemical weapon.N. We use two resources for graph construction. First, we take all the words and phrases present in the dependency-based thesaurus constructed using syntactic cooccurrence statistics (Lin, 1998).3 To construct this resource, a corpus containing 64 million words was parsed with a fast dependency parser (Lin, 1993; Lin, 1994), and syntactic contexts were used to find similar lexical items for a given word 3 This resource is available at http://webdocs.cs. ualberta.ca/˜lindek/Downloads/sim.tgz POVERTY powerlessness.N rich.A deprivation.N POVERTY wealthy.A SIMILARITY variant.N SIMILARITY similarity.N Figure 2: Excerpt from a graph over targets. Green targets are SIMILARITY POVERTY observed in the FrameNet data. poverty.N resemblance.N inequality.N Above/below them are shown the destituti"
P11-1144,P09-1003,0,0.0472891,"ide supervision for learning frame-semantic parsers. Yet these annotations lack coverage, including only 9,300 annotated target types. Recent papers have tried to address the coverage problem. Johansson and Nugues (2007) used WordNet (Fellbaum, 1998) to expand the list of targets that can evoke frames and trained classifiers to identify the best-suited frame for the newly created targets. In past work, we described an approach where latent variables were used in a probabilistic model to predict frames for unseen targets (Das et al., 2010a).1 Relatedly, for the argument identification subtask, Matsubayashi et al. (2009) proposed a technique for generalization of semantic roles to overcome data sparseness. Unseen targets continue to present a major obstacle to domain-general semantic analysis. In this paper, we address the problem of idenfifying the semantic frames for targets unseen either in FrameNet (including the exemplar sentences) or the collection of full-text annotations released along with the lexicon. Using a standard model for the argument identification stage (Das et al., 2010a), our proposed method improves overall frame-semantic parsing, especially for unseen targets. To better handle these unse"
P11-1144,P05-1012,0,0.0112118,"exemplar sentences and the training data, we arrived at a set of 877 frames, 1,068 roles,10 and 9,263 targets. Our training split of the full-text annotations contained 3,256 sentences with 19,582 frame annotatations with corresponding roles, while the test set contained 2,420 sentences with 4,458 annotations (the test set contained fewer annotated targets per sentence). We also divide the 55 training documents into 5 parts for crossvalidation (see §6.3). The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005) following Das et al. (2010a). In this work we assume the frame-evoking targets have been correctly identified in training and test data. 10 Note that the number of listed roles in the lexicon is nearly 9,000, but their number in actual annotations is a lot fewer. 1441 Baselines We compare our model with three baselines. The first baseline is the purely supervised model of Das et al. (2010a) trained on the training split of 55 documents. Note that this is the strongest baseline available for this task;11 we refer to this model as “SEMAFOR.” The second baseline is a semi-supervised selftrained"
P11-1144,P05-1049,0,0.00894607,"xtended in this work. 2.2 Graph-based Semi-Supervised Learning In graph-based semi-supervised learning, one constructs a graph whose vertices are labeled and unlabeled examples. Weighted edges in the graph, connecting pairs of examples/vertices, encode the degree to which they are expected to have the same label (Zhu et al., 2003). Variants of label propagation are used to transfer labels from the labeled to the unlabeled examples. There are several instances of the use of graph-based methods for natural language tasks. Most relevant to our work an approach to word-sense disambiguation due to Niu et al. (2005). Their formulation was transductive, so that the test data was part of the constructed graph, and they did not consider predicate-argument analysis. In contrast, we make use of the smoothed graph during inference in a probabilistic setting, in turn using it for the full frame-semantic parsing task. Recently, Subramanya et al. (2010) proposed the use of a graph over substructures of an underlying sequence model, and used a smoothed graph for domain adaptation of part-of-speech taggers. Subramanya et al.’s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for uns"
P11-1144,H05-1108,0,0.0372655,"Missing"
P11-1144,D08-1048,0,0.125485,"Missing"
P11-1144,W96-0213,0,0.0519558,"remaining ones as our test set. After scanning the exemplar sentences and the training data, we arrived at a set of 877 frames, 1,068 roles,10 and 9,263 targets. Our training split of the full-text annotations contained 3,256 sentences with 19,582 frame annotatations with corresponding roles, while the test set contained 2,420 sentences with 4,458 annotations (the test set contained fewer annotated targets per sentence). We also divide the 55 training documents into 5 parts for crossvalidation (see §6.3). The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005) following Das et al. (2010a). In this work we assume the frame-evoking targets have been correctly identified in training and test data. 10 Note that the number of listed roles in the lexicon is nearly 9,000, but their number in actual annotations is a lot fewer. 1441 Baselines We compare our model with three baselines. The first baseline is the purely supervised model of Das et al. (2010a) trained on the training split of 55 documents. Note that this is the strongest baseline available for this task;11 we refer to this model as “SEMAFOR.”"
P11-1144,W04-2008,0,0.0905875,"observed predicates (§6). 2 Background Before going into the details of our model, we provide some background on two topics relevant to this paper: frame-semantic parsing and graph-based learning applied to natural language tasks. 2.1 Frame-semantic Parsing Gildea and Jurafsky (2002) pioneered SRL, and since then there has been much applied research on predicate-argument semantics. Early work on frame-semantic role labeling made use of the exemplar sentences in the FrameNet corpus, each of which is annotated for a single frame and its arguments (Thompson et al., 2003; Fleischman et al., 2003; Shi and Mihalcea, 2004; Erk and Pad´o, 2006, inter alia). Most of this work was done on an older, smaller version of FrameNet. Recently, since the release of full-text annotations in SemEval’07 (Baker et al., 2007), there has been work on identifying multiple frames and their corresponding sets of ar1436 guments in a sentence. The LTH system of Johansson and Nugues (2007) performed the best in the SemEval’07 shared task on frame-semantic parsing. Our probabilistic frame-semantic parser outperforms LTH on that task and dataset (Das et al., 2010a). The current paper builds on those probabilistic models to improve cov"
P11-1144,P06-1101,0,0.0289538,"al., 2007), there has been work on identifying multiple frames and their corresponding sets of ar1436 guments in a sentence. The LTH system of Johansson and Nugues (2007) performed the best in the SemEval’07 shared task on frame-semantic parsing. Our probabilistic frame-semantic parser outperforms LTH on that task and dataset (Das et al., 2010a). The current paper builds on those probabilistic models to improve coverage on unseen predicates.2 Expert resources have limited coverage, and FrameNet is no exception. Automatic induction of semantic resources has been a major effort in recent years (Snow et al., 2006; Ponzetto and Strube, 2007, inter alia). In the domain of frame semantics, previous work has sought to extend the coverage of FrameNet by exploiting resources like VerbNet, WordNet, or Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005). Although these approaches have increased coverage to various degrees, they rely on other lexicons and resources created by experts. F¨urstenau and Lapata (2009) propos"
P11-1144,D10-1017,0,0.352335,"003). Variants of label propagation are used to transfer labels from the labeled to the unlabeled examples. There are several instances of the use of graph-based methods for natural language tasks. Most relevant to our work an approach to word-sense disambiguation due to Niu et al. (2005). Their formulation was transductive, so that the test data was part of the constructed graph, and they did not consider predicate-argument analysis. In contrast, we make use of the smoothed graph during inference in a probabilistic setting, in turn using it for the full frame-semantic parsing task. Recently, Subramanya et al. (2010) proposed the use of a graph over substructures of an underlying sequence model, and used a smoothed graph for domain adaptation of part-of-speech taggers. Subramanya et al.’s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers. Our semi-supervised learning setting is similar to these two lines of work and, like them, we use the graph to arrive at better final structures, in an inductive setting (i.e., where a parametric model is learned and then separately applied to test data, following most NLP research). 3 Approach Overview"
P11-1144,D09-1029,0,0.0212473,"rms LTH on that task and dataset (Das et al., 2010a). The current paper builds on those probabilistic models to improve coverage on unseen predicates.2 Expert resources have limited coverage, and FrameNet is no exception. Automatic induction of semantic resources has been a major effort in recent years (Snow et al., 2006; Ponzetto and Strube, 2007, inter alia). In the domain of frame semantics, previous work has sought to extend the coverage of FrameNet by exploiting resources like VerbNet, WordNet, or Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005). Although these approaches have increased coverage to various degrees, they rely on other lexicons and resources created by experts. F¨urstenau and Lapata (2009) proposed the use of unlabeled data to improve coverage, but their work was limited to verbs. Bejan (2009) used self-training to improve frame identification and reported improvements, but did not explicitly model unknown targets. In contrast, we use statistics gathered from large volumes of unlabeled data to i"
P11-1144,S07-1018,0,\N,Missing
P11-1144,C98-2122,0,\N,Missing
P11-2008,C10-2005,0,0.591162,"toP jailN ., ThankV uO God∧ ., #amen# Figure 1: Example tweets with gold annotations. Underlined tokens show tagger improvements due to features detailed in Section 3 (respectively: TAG D ICT, M ETAPH, and D IST S IM). Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn T"
P11-2008,W10-0713,0,0.446157,"enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB; Marcus et al., 1993). Tagging performance degrades on out-of-domain data, and Twitter poses additional challenges due to the conversational nature of the text, the lack of conventional orthography, and 140character limit of each message (“tweet”). Figure 1 shows three tweets which illustrate t"
P11-2008,J93-2004,0,0.0664899,"erman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB; Marcus et al., 1993). Tagging performance degrades on out-of-domain data, and Twitter poses additional challenges due to the conversational nature of the text, the lack of conventional orthography, and 140character limit of each message (“tweet”). Figure 1 shows three tweets which illustrate these challenges. In this paper, we produce an English POS tagger that is designed especially for Twitter data. Our contributions are as follows: • we developed a POS tagset for Twitter, • we manually tagged 1,827 tweets, • we developed features for Twitter POS tagging and conducted experiments to evaluate them, and • we prov"
P11-2008,petrov-etal-2012-universal,1,0.297019,"Missing"
P11-2008,N10-1020,0,0.140487,"yD licenseN and& #2$ notR takinV druN booN toP jailN ., ThankV uO God∧ ., #amen# Figure 1: Example tweets with gold annotations. Underlined tokens show tagger improvements due to features detailed in Section 3 (respectively: TAG D ICT, M ETAPH, and D IST S IM). Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such a"
P11-2008,N10-1100,0,0.180483,"notR takinV druN booN toP jailN ., ThankV uO God∧ ., #amen# Figure 1: Example tweets with gold annotations. Underlined tokens show tagger improvements due to features detailed in Section 3 (respectively: TAG D ICT, M ETAPH, and D IST S IM). Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Jour"
P11-2008,N03-1033,0,0.210842,"Missing"
P11-2008,P10-1040,0,0.263127,"butional similarity. When training data is limited, distributional features from unlabeled text can improve performance (Sch¨utze and Pedersen, 1993). We used 1.9 million tokens from 134,000 unlabeled tweets to construct distributional features from the successor and predecessor probabilities for the 10,000 most common terms. The successor and predecessor transition matrices are horizontally concatenated into a sparse matrix M, which we approximate using a truncated singular value decomposition: M ≈ USVT , where U is limited to 50 columns. Each term’s feature vector is its row in U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1. M ETAPH : Phonetic normalization. Since Twitter includes many alternate spellings of words, we used the Metaphone algorithm (Philips, 1990)9 to create a coarse phonetic normalization of words to simpler keys. Metaphone consists of 19 rules that rewrite consonants and delete vowels. For example, in our 7 1 α = 100 , C = 10; this score is equivalent to the posterior probability of capitalization with a Beta(0.1, 9.9) prior. 8 Both WSJ and Brown corpora, no case normalization. We also tried adding the WordNet (Fellbaum, 1998) and Moby (War"
P13-2017,W06-2920,0,0.808379,"ebank is made freely available in order to facilitate research on multilingual dependency parsing.1 1 Introduction In recent years, syntactic representations based on head-modifier dependency relations between words have attracted a lot of interest (K¨ubler et al., 2009). Research in dependency parsing – computational methods to predict such representations – has increased dramatically, due in large part to the availability of dependency treebanks in a number of languages. In particular, the CoNLL shared tasks on dependency parsing have provided over twenty data sets in a standardized format (Buchholz and Marsi, 2006; Nivre et al., 2007). While these data sets are standardized in terms of their formal representation, they are still heterogeneous treebanks. That is to say, despite them all being dependency treebanks, which annotate each sentence with a dependency tree, they subscribe to different annotation schemes. This can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions. Common divergences are found in the 1 Downloadable at https://code.google.com/p/uni-dep-tb/. 92 Proceedings of the 51st Annual Mee"
P13-2017,W02-1503,0,0.0535563,"Missing"
P13-2017,W09-2307,0,0.0712427,"Missing"
P13-2017,P11-1061,1,0.243183,"aking fine-grained label distinctions was discouraged. Once these guidelines were fixed, annotators selected roughly an equal amount of sentences to be annotated from each domain in the unlabeled data. As the sentences were already randomly selected from a larger corpus, annotators were told to view the sentences in order and to discard a sentence only if it was 1) fragmented because of a sentence splitting error; 2) not from the language of interest; 3) incomprehensible to a native speaker; or 4) shorter than three words. The selected sentences were pre-processed using cross-lingual taggers (Das and Petrov, 2011) and parsers (McDonald et al., 2011). The annotators modified the pre-parsed trees using the TrEd2 tool. At the beginning of the annotation process, double-blind annotation, followed by manual arbitration and consensus, was used iteratively for small batches of data until the guidelines were finalized. Most of the data was annotated using single-annotation and full review: one annotator annotating the data and another reviewing it, making changes in close collaboration with the original annotator. As a final step, all annotated data was semi-automatically checked for annotation consistency. 2."
P13-2017,W08-1301,0,0.173029,"Missing"
P13-2017,D11-1006,1,0.855635,"icient if one’s goal is to build monolingual parsers and evaluate their quality without reference to other languages, as in the original CoNLL shared tasks, but there are many cases where heterogenous treebanks are less than adequate. First, a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components. Second, consistent syntactic representations are desirable in the evaluation of unsupervised (Klein and Manning, 2004) or cross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to cr"
P13-2017,P07-1122,1,0.763455,"as head in copula constructions. For Swedish, we developed a set of deterministic rules for converting the Talbanken part of the Swedish Treebank (Nivre and Megyesi, 2007) to a representation as close as possible to the Stanford dependencies for English. This mainly consisted in relabeling dependency relations and, due to the fine-grained label set used in the Swedish Treebank (Teleman, 1974), this could be done with high precision. In addition, a small number of constructions required structural conversion, notably coordination, which in the Swedish Treebank is given a Prague style analysis (Nilsson et al., 2007). For both English and Swedish, we mapped the language-specific partof-speech tags to universal tags using the mappings of Petrov et al. (2012). Towards A Universal Treebank The Stanford typed dependencies for English (De Marneffe et al., 2006; de Marneffe and Manning, 2008) serve as the point of departure for our ‘universal’ dependency representation, together with the tag set of Petrov et al. (2012) as the underlying part-of-speech representation. The Stanford scheme, partly inspired by the LFG framework, has emerged as a de facto standard for dependency annotation in English and has recentl"
P13-2017,de-marneffe-etal-2006-generating,0,0.333356,"Missing"
P13-2017,P09-1042,1,0.250489,"arsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to create homogenous syntactic dependency annotation in multiple languages. In terms of automatic construction, Zeman et al. (2012) attempt to harmonize a large number of dependency treebanks by mapping their annotation to a version of the Prague Dependency Treebank scheme (Hajiˇc et al., 2001; B¨ohmov´a et al., 2003). Additionally, there have been efforts to manually or semimanually construct resources with common synWe present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages: German, English,"
P13-2017,W12-1909,0,0.0247816,"Missing"
P13-2017,petrov-etal-2012-universal,1,0.702758,"nal Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do the same for syntactic dependencies and present cross-lingual parsing experiments to highlight some of the benefits of cross-lingually consistent annotation. First, results largely conform to our expectations of which target languages should be useful for which source languages, unlike in the study of McDonald et al. (2011). Second, the evaluation scores in general are significantly higher than previous cross-lingual studies, su"
P13-2017,D09-1086,0,0.0317978,"ross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to create homogenous syntactic dependency annotation in multiple languages. In terms of automatic construction, Zeman et al. (2012) attempt to harmonize a large number of dependency treebanks by mapping their annotation to a version of the Prague Dependency Treebank scheme (Hajiˇc et al., 2001; B¨ohmov´a et al., 2003). Additionally, there have been efforts to manually or semimanually construct resources with common synWe present a new collection of treebanks with homogeneous syntactic dependency annotation for six lang"
P13-2017,Q13-1001,1,0.0665003,"Missing"
P13-2017,W04-2709,0,0.0429789,"annotation schemes. This can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions. Common divergences are found in the 1 Downloadable at https://code.google.com/p/uni-dep-tb/. 92 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 201"
P13-2017,P13-2103,0,0.129897,"Missing"
P13-2017,N06-2015,0,0.0347787,"s can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions. Common divergences are found in the 1 Downloadable at https://code.google.com/p/uni-dep-tb/. 92 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do th"
P13-2017,zeman-etal-2012-hamledt,0,0.060315,"Missing"
P13-2017,P03-1054,0,0.0144203,"nch data set is shown in Figure 1. We take two approaches to generating data. The first is traditional manual annotation, as previously used by Helmreich et al. (2004) for multilingual syntactic treebank construction. The second, used only for English and Swedish, is to automatically convert existing treebanks, as in Zeman et al. (2012). 2.1 Automatic Conversion Since the Stanford dependencies for English are taken as the starting point for our universal annotation scheme, we begin by describing the data sets produced by automatic conversion. For English, we used the Stanford parser (v1.6.8) (Klein and Manning, 2003) to convert the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) to basic dependency trees, including punctuation and with the copula verb as head in copula constructions. For Swedish, we developed a set of deterministic rules for converting the Talbanken part of the Swedish Treebank (Nivre and Megyesi, 2007) to a representation as close as possible to the Stanford dependencies for English. This mainly consisted in relabeling dependency relations and, due to the fine-grained label set used in the Swedish Treebank (Teleman, 1974), this could be done with high precision. In"
P13-2017,P11-2033,1,0.192695,"Missing"
P13-2017,P04-1061,0,0.0673447,"word expressions (Nilsson et al., 2007; K¨ubler et al., 2009; Zeman et al., 2012). These data sets can be sufficient if one’s goal is to build monolingual parsers and evaluate their quality without reference to other languages, as in the original CoNLL shared tasks, but there are many cases where heterogenous treebanks are less than adequate. First, a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components. Second, consistent syntactic representations are desirable in the evaluation of unsupervised (Klein and Manning, 2004) or cross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2"
P13-2017,D12-1125,0,0.0145332,"for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do the same for syntactic dependencies and present cross-lingual parsing experiments to highlight some of the benefits of cross-lingually consistent annotation. First, results largely conform to our expectations of which target languages should be useful for which source languages, unlike in the study of McDonald et al. (2011). Second, the evaluation scores in general are significantly higher than previous cross-lingual studies, suggesting that most of these studies underestimate true accuracy. Finally, unlike all previous cross-ling"
P13-2017,J93-2004,0,\N,Missing
P13-2017,W08-1300,0,\N,Missing
P13-2017,D07-1096,1,\N,Missing
P14-1136,P98-1013,0,0.728343,"frame identification, namely the disambiguation of a given predicate to a frame, and argument identification (or semantic role labeling), the analysis of words and phrases in the sentential context that satisfy the frame’s semantic roles (Das et al., 2010; Das et al., 2014).1 Here, we focus on the first subtask of frame identification for given predicates; we use our novel method (§3) in conjunction with a standard argument identification model (§4) to perform full frame-semantic parsing. We present experiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date. Second, we present results on PropBank-style semantic role labeling (Palmer et al., 2005; Meyers et al., 2004; M`arquez et al., 2008), that approach strong baselines, and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky"
P14-1136,S07-1018,0,0.244314,"and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1 There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies candidate arguments, and argument identification on the filtered candidates (Baker et al., 2007; Johansson and Nugues, 2007). COMMERCE_BUY buy.01 buy.V buy.V John bought a car . Buyer Goods John bought a car . A0 A1 COMMERCE_BUY sell.01 sell.V sell.V Mary sold Seller a car . Goods Mary sold A0 (a) a car . A1 (b) Figure 1: Example sentences with frame-semantic analyses. FrameNet annotation conventions are used in (a) while (b) denotes PropBank conventions. 2004; Carreras and M`arquez, 2005) on PropBank semantic role labeling (SRL), it has been treated as an important NLP problem. However, research has mostly focused on argument analysis, skipping the frame disambiguation step, and its in"
P14-1136,W04-2412,0,0.084247,"Missing"
P14-1136,W05-0620,0,0.04886,"Missing"
P14-1136,J08-2001,0,0.0329993,"Missing"
P14-1136,W04-2705,0,0.205383,"3) in conjunction with a standard argument identification model (§4) to perform full frame-semantic parsing. We present experiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date. Second, we present results on PropBank-style semantic role labeling (Palmer et al., 2005; Meyers et al., 2004; M`arquez et al., 2008), that approach strong baselines, and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1 There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies candidate arguments"
P14-1136,P08-1028,0,0.0174848,"tion method inspired by prior work, we achieve state-ofthe-art results on FrameNet-style framesemantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work. 1 Introduction Distributed representations of words have proved useful for a number of tasks. By providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis (Socher et al., 2011), topic classification (Klementiev et al., 2012) or word-word similarity (Mitchell and Lapata, 2008). We present a new technique for semantic frame identification that leverages distributed word representations. According to the theory of frame semantics (Fillmore, 1982), a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the ∗ The majority of this research was carried out during an internship at Google. event. Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a frame, and argument identification (or semantic role"
P14-1136,J05-1004,0,0.453818,"e our novel method (§3) in conjunction with a standard argument identification model (§4) to perform full frame-semantic parsing. We present experiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date. Second, we present results on PropBank-style semantic role labeling (Palmer et al., 2005; Meyers et al., 2004; M`arquez et al., 2008), that approach strong baselines, and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1 There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifie"
P14-1136,N10-1138,1,0.583937,"According to the theory of frame semantics (Fillmore, 1982), a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the ∗ The majority of this research was carried out during an internship at Google. event. Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a frame, and argument identification (or semantic role labeling), the analysis of words and phrases in the sentential context that satisfy the frame’s semantic roles (Das et al., 2010; Das et al., 2014).1 Here, we focus on the first subtask of frame identification for given predicates; we use our novel method (§3) in conjunction with a standard argument identification model (§4) to perform full frame-semantic parsing. We present experiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report"
P14-1136,J08-2005,0,0.196193,"xperiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date. Second, we present results on PropBank-style semantic role labeling (Palmer et al., 2005; Meyers et al., 2004; M`arquez et al., 2008), that approach strong baselines, and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1 There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies candidate arguments, and argument identification on the filtered candidates (Baker et al., 2007; Johansson and Nugues, 2007). COMMERCE_BUY buy.01"
P14-1136,J14-1002,1,0.233868,"heory of frame semantics (Fillmore, 1982), a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the ∗ The majority of this research was carried out during an internship at Google. event. Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a frame, and argument identification (or semantic role labeling), the analysis of words and phrases in the sentential context that satisfy the frame’s semantic roles (Das et al., 2010; Das et al., 2014).1 Here, we focus on the first subtask of frame identification for given predicates; we use our novel method (§3) in conjunction with a standard argument identification model (§4) to perform full frame-semantic parsing. We present experiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results o"
P14-1136,D11-1014,0,0.012179,"ion. The latter is used for semantic frame identification; with a standard argument identification method inspired by prior work, we achieve state-ofthe-art results on FrameNet-style framesemantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work. 1 Introduction Distributed representations of words have proved useful for a number of tasks. By providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis (Socher et al., 2011), topic classification (Klementiev et al., 2012) or word-word similarity (Mitchell and Lapata, 2008). We present a new technique for semantic frame identification that leverages distributed word representations. According to the theory of frame semantics (Fillmore, 1982), a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the ∗ The majority of this research was carried out during an internship at Google. event. Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namel"
P14-1136,P10-1040,0,0.0243059,"in sentential context, we want to disambiguate its frame. (Although PropBank never formally uses the term lexical unit, we adopt its usage from the frame semantics literature.) 2.2 Distributed Frame Identification We present a model that takes word embeddings as input and learns to identify semantic frames. A word embedding is a distributed representation of meaning where each word is represented as a vector in Rn . Such representations allow a model to share meaning between similar words, and have been used to capture semantic, syntactic and morphological content (Collobert and Weston, 2008; Turian et al., 2010, inter alia). We use word embeddings to represent the syntactic context of a particular predicate instance as a vector. For example, consider the sentence “He runs the company.” The predicate runs has two syntactic dependents – a subject and direct object (but no prepositional phrases or clausal complements). We could represent the syntactic context of runs as a vector with blocks for all the possible dependents warranted by a syntactic parser; for example, we could assume that positions 0 . . . n in the vector correspond to the subject dependent, n+1 . . . 2n correspond to the clausal comple"
P14-1136,P08-1086,0,0.0271771,"fe and Manning, 2013) and uses an arc-eager transition system with beam size of 8; the parser and its features are described by Zhang and Nivre (2011). Before parsing the data, it is tagged with a POS tagger trained with a conditional random field (Lafferty et al., 2001) with the following emission features: word, the word cluster, word suffixes of length 1, 2 and 3, capitalization, whether it has a hyphen, digit and punctuation. Beyond the bias transition feature, we have two cluster features for the left and right words in the transition. We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. We use the same word clusters for the argument identification features in Table 1. We learn the initial embedding representations for our frame identification model (§3) using a deep neural language model similar to the one proposed by Bengio et al. (2003). We use 3 hidden layers each with 1024 neurons and learn a 128dimensional embedding from a large corpus containing over 100 billion tokens. In order to speed up learning, we use an unnormalized output layer and a hinge-loss objective. The objective tries to ensure that the correct wor"
P14-1136,J02-3001,0,0.444782,"(Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date. Second, we present results on PropBank-style semantic role labeling (Palmer et al., 2005; Meyers et al., 2004; M`arquez et al., 2008), that approach strong baselines, and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1 There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies candidate arguments, and argument identification on the filtered candidates (Baker et al., 2007; Johansson and Nugues, 2007). COMMERCE_BUY buy.01 buy.V buy.V John bought a car . Buyer Goods John bought a car . A0 A1 COMMERCE_BUY sell.01 se"
P14-1136,S07-1048,0,0.0165202,"prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1 There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies candidate arguments, and argument identification on the filtered candidates (Baker et al., 2007; Johansson and Nugues, 2007). COMMERCE_BUY buy.01 buy.V buy.V John bought a car . Buyer Goods John bought a car . A0 A1 COMMERCE_BUY sell.01 sell.V sell.V Mary sold Seller a car . Goods Mary sold A0 (a) a car . A1 (b) Figure 1: Example sentences with frame-semantic analyses. FrameNet annotation conventions are used in (a) while (b) denotes PropBank conventions. 2004; Carreras and M`arquez, 2005) on PropBank semantic role labeling (SRL), it has been treated as an important NLP problem. However, research has mostly focused on argument analysis, skipping the frame disambiguation step, and its interaction with argument ident"
P14-1136,W04-3212,0,0.0777944,"search for the stochastic gradient learning rate in {0.0001, 0.001, 0.01}, the margin γ ∈ {0.001, 0.01, 0.1, 1} and the dimensionality of the final vector space m ∈ {256, 512}, to maximize the frame identification accuracy of ambiguous lexical units; by ambiguous, we imply lexical units that appear in the training data or the lexicon with more than one semantic frame. The underlined values are the chosen hyperparameters used to analyze the test data. Argument Candidates The candidate argument extraction method used for the FrameNet data, (as mentioned in §4) was adapted from the algorithm of Xue and Palmer (2004) applied to dependency trees. Since the original algorithm was designed for verbs, we added a few extra rules to handle non-verbal predicates: we added 1) the predicate itself as a candidate argument, 2) the span ranging from the sentence position to the right of the predicate to the rightmost index of the subtree headed by the predicate’s head; this helped capture cases like “a few months” (where few is the predicate and months is the argument), and 3) the span ranging from the leftmost index of the subtree headed by the predicate’s head to the position immediately before the predicate, for c"
P14-1136,C12-1089,0,0.00922816,"dentification; with a standard argument identification method inspired by prior work, we achieve state-ofthe-art results on FrameNet-style framesemantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work. 1 Introduction Distributed representations of words have proved useful for a number of tasks. By providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis (Socher et al., 2011), topic classification (Klementiev et al., 2012) or word-word similarity (Mitchell and Lapata, 2008). We present a new technique for semantic frame identification that leverages distributed word representations. According to the theory of frame semantics (Fillmore, 1982), a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the ∗ The majority of this research was carried out during an internship at Google. event. Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a f"
P14-1136,P11-2033,0,0.00588532,"decouple the W SABIE training from the embedding input, and trains a log linear model using the embeddings. So the second baseline has the same input representation as W SABIE E MBEDDING but uses a log-linear model instead of W SABIE. We call this model L OG -L INEAR E MBED DING . 5.3 Common Experimental Setup We process our PropBank and FrameNet training, development and test corpora with a shift-reduce dependency parser that uses the Stanford conventions (de Marneffe and Manning, 2013) and uses an arc-eager transition system with beam size of 8; the parser and its features are described by Zhang and Nivre (2011). Before parsing the data, it is tagged with a POS tagger trained with a conditional random field (Lafferty et al., 2001) with the following emission features: word, the word cluster, word suffixes of length 1, 2 and 3, capitalization, whether it has a hyphen, digit and punctuation. Beyond the bias transition feature, we have two cluster features for the left and right words in the transition. We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. We use the same word clusters for the argument identification fea"
P14-1136,C98-1013,0,\N,Missing
P14-5020,N13-1138,0,0.0154841,"across languages, but also for the same language), it can be useful to examine and potentially aggregate the frequencies of all inflected forms. This can be accomplished by manually deriving all inflected forms and then using arithmetic operations to aggregate their counts. Our new inflected form search accomplishes this automatically. By appending the keyword INF to a word, a set of ngrams with all inflected forms of the word will be retrieved. To generate the inflected forms we make use of Wiktionary3 and supplement it with automatically generated inflection tables based on the approach of Durrett and DeNero (2013). Because there are at most a few dozen inflected forms for any given word, we can afford to substitute and retrieve all inflections of the marked word, even the ones that are not grammatical in a given ngram context. This has the advantage that we only need to store inflected forms for individual words rather than entire ngrams. If a generated ngram has no support in the corpus, we simply omit it from the final set of results. We do not perform any additional filtering; as a result, an inflection search can produce many results, especially for morphologically rich languages like Russian. We h"
P14-5020,P12-3029,1,0.911178,"and retrieve a collection of ngrams, to facilitate the discovery of patterns in the underlying data. First, users can replace one query term with a placeholder symbol ‘*’ (wildcard, henceforth), which will return the ten most frequent expansions of the wildcard in the corpus for the specified year range. Second, by adding a specific marker to any word in a query (‘ INF’), ngrams with all Introduction The Google Books Ngram project facilitates the analysis of cultural, social and linguistic trends through five centuries of written text in eight languages. The Ngram Corpus (Michel et al., 2011; Lin et al., 2012) consists of words and phrases (i.e., ngrams) and their usage frequency over time.1 The interactive Ngram Viewer2 allows users to retrieve and plot the frequency of multiple ngrams on a simple webpage. The Viewer is widely popular and can be used to efficiently explore and visualize patterns in the underlying ngram data. For example, the ngram data has been used to detect emotion trends in 20th century books (Acerbi et al., 2013), to analyze text focusing on market capitalism throughout the past century (Schulz and Robinson, 2013), detect social and cultural impact of historical personalities"
P14-5020,petrov-etal-2012-universal,1,0.750823,"d is described in Lin et al. 116 tences to enable the distinction of sentence medial ngrams from those near sentence boundaries. They also ensure that sentences that span across page boundaries are included. Due to these differences, as well as the availability of additional book data, improvements to the optical character recognition algorithms and metadata extraction for dating the books, the ngrams counts from the two editions are not the same. The edition from Lin et al. (2012) additionally includes syntactic ngrams. The corpus is tagged using the universal part-of-speech (POS) tag set of Petrov et al. (2012): NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners and articles), ADP (prepositions and postpositions), CONJ (conjunctions). Words can be disambiguated by their POS tag by simply appending the tag to the word with an underscore (e.g. book NOUN) and can also be replaced by POS tags in the ngrams, see Lin et al. (2012) for details. The corpus is parsed with a dependency parser and head-modifier syntactic relations between words in the same sentence are extracted. Dependency relations are represented as ‘=>’ in the corpus. Our new enhanced search feat"
P19-1452,P19-1441,0,0.0293115,": part-of-speech (POS), constituents (Consts.), dependencies (Deps.), entities, semantic role labeling (SRL), coreference (Coref.), semantic proto-roles (SPR; Reisinger et al., 2015), and relation classification (SemEval). These tasks are derived from standard benchmark datasets, and evaluated with a common metric–micro-averaged F1–to facilitate comparison across tasks. 2 BERT. The BERT model (Devlin et al., 2019) has shown state-of-the-art performance on many tasks, and its deep Transformer architecture (Vaswani et al., 2017) is typical of many recent models (e.g. Radford et al., 2018, 2019; Liu et al., 2019). We focus on the stock BERT models (base and large, uncased), which are trained with a multi-task objective (masked language modeling and next-sentence prediction) over a 3.3B word English corpus. Since we want to understand how the network represents language as a result of pretraining, we follow Tenney et al. (2019) (departing from standard BERT usage) and freeze the encoder weights. This prevents the encoder from rearranging its internal representations to better suit the probing task. Given input tokens T = [t0 , t1 , . . . , tn ], a deep encoder produces a set of layer activations H (0)"
P19-1452,P14-5010,0,0.00350132,"sponsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lowerlevel decisions on the basis of disambiguating information from higher-level representations. 1 Introduction Pre-trained sentence encoders such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) have rapidly improved the state of the art on many NLP tasks, and seem poised to displace both static word embeddings (Mikolov et al., 2013) and discrete pipelines (Manning et al., 2014) as the basis for natural language processing systems. While this has been a boon for performance, it has come at the cost of interpretability, and it remains unclear whether such models are in fact learning the kind of abstractions that we intuitively believe are important for representing natural language, or are simply modeling complex co-occurrence statistics. A wave of recent work has begun to “probe” state-of-the-art models to understand whether they are representing language in a satisfying way. Much of this work is behavior-based, designing controlled test sets and analyzing errors in"
P19-1452,D18-1151,0,0.0688029,"t of interpretability, and it remains unclear whether such models are in fact learning the kind of abstractions that we intuitively believe are important for representing natural language, or are simply modeling complex co-occurrence statistics. A wave of recent work has begun to “probe” state-of-the-art models to understand whether they are representing language in a satisfying way. Much of this work is behavior-based, designing controlled test sets and analyzing errors in order to reverse-engineer the types of abstractions the model may or may not be representing (e.g. Conneau et al., 2018; Marvin and Linzen, 2018; Poliak et al., 2018). Parallel efforts inspect the structure of the network directly, to assess whether there exist localizable regions associated with distinct types of linguistic decisions. Such work has produced evidence that deep language models can encode a range of syntactic and semantic information (e.g. Shi et al., 2016; Belinkov, 2018; Tenney et al., 2019), and that more complex structures are represented hierarchically in the higher layers of the model (Peters et al., 2018b; Blevins et al., 2018). We build on this latter line of work, focusing on the BERT model (Devlin et al., 2019"
P19-1452,N18-1202,0,0.640756,"aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lowerlevel decisions on the basis of disambiguating information from higher-level representations. 1 Introduction Pre-trained sentence encoders such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) have rapidly improved the state of the art on many NLP tasks, and seem poised to displace both static word embeddings (Mikolov et al., 2013) and discrete pipelines (Manning et al., 2014) as the basis for natural language processing systems. While this has been a boon for performance, it has come at the cost of interpretability, and it remains unclear whether such models are in fact learning the kind of abstractions that we intuitively believe are important for representing natural language, or are simply modeling complex co-occurrence statistics. A wave of rec"
P19-1452,D18-1179,0,0.306082,"aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lowerlevel decisions on the basis of disambiguating information from higher-level representations. 1 Introduction Pre-trained sentence encoders such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) have rapidly improved the state of the art on many NLP tasks, and seem poised to displace both static word embeddings (Mikolov et al., 2013) and discrete pipelines (Manning et al., 2014) as the basis for natural language processing systems. While this has been a boon for performance, it has come at the cost of interpretability, and it remains unclear whether such models are in fact learning the kind of abstractions that we intuitively believe are important for representing natural language, or are simply modeling complex co-occurrence statistics. A wave of rec"
P19-1452,W18-5441,1,0.742275,"Missing"
P19-1452,Q15-1034,0,0.0798062,"Missing"
P19-1452,D16-1159,0,0.0182544,"y are representing language in a satisfying way. Much of this work is behavior-based, designing controlled test sets and analyzing errors in order to reverse-engineer the types of abstractions the model may or may not be representing (e.g. Conneau et al., 2018; Marvin and Linzen, 2018; Poliak et al., 2018). Parallel efforts inspect the structure of the network directly, to assess whether there exist localizable regions associated with distinct types of linguistic decisions. Such work has produced evidence that deep language models can encode a range of syntactic and semantic information (e.g. Shi et al., 2016; Belinkov, 2018; Tenney et al., 2019), and that more complex structures are represented hierarchically in the higher layers of the model (Peters et al., 2018b; Blevins et al., 2018). We build on this latter line of work, focusing on the BERT model (Devlin et al., 2019), and use a suite of probing tasks (Tenney et al., 2019) derived from the traditional NLP pipeline to quantify where specific types of linguistic information are encoded. Building on observations (Peters et al., 2018b) that lower layers of a language model encode more local syntax while higher layers capture more complex semanti"
P19-1452,silveira-etal-2014-gold,0,0.0635284,"Missing"
P19-1452,D17-3004,0,0.0648136,"Missing"
P19-1483,W00-1401,0,0.21157,"e an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). The underlying assumption behind these metrics is that the reference text is gold-standard, i.e., it is the ideal target text that a system should generate. In practice, however, when datasets are collected automatically and heuristically, the reference texts are often not ideal. Figure 1 shows an example from the WikiBio dataset (Lebret et al., 2016). Here the reference contains extra information which no system can be expected to produce given only the associated tab"
P19-1483,H05-1042,0,0.0465407,"Missing"
P19-1483,E06-1040,0,0.0647752,"ema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006; Reiter, 2018; Liu et al., 2016; Kilickaya et al., 2017; Gatt and Krahmer, 2018). The only consensus from these studies seems to be that no single metric is suitable across all tasks. A recurring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show any correlation at the sentence level. Ours i"
P19-1483,W07-0718,0,0.171507,"Missing"
P19-1483,E06-1032,0,0.111485,"al., 2016) and Q-BLEU for question generation (Nema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006; Reiter, 2018; Liu et al., 2016; Kilickaya et al., 2017; Gatt and Krahmer, 2018). The only consensus from these studies seems to be that no single metric is suitable across all tasks. A recurring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show an"
P19-1483,W17-3209,0,0.0568619,"Missing"
P19-1483,D16-1128,0,0.611103,"the information extraction based evaluation proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has lar"
P19-1483,P18-1060,0,0.0664973,"Missing"
P19-1483,W14-3346,0,0.0160021,"f the longest common subsequence between x and y. The LCS function, borrowed from ROUGE, ensures that entity names in r¯k appear in the same order in the text as the table. Higher values of Er (T i ) denote that more records are likely to be mentioned in Gi . The entailed precision and recall are combined into an F-score to give the PARENT metric for one instance. The system-level PARENT score for a Smoothing & Multiple References. The danger with geometric averages is that if any of the components being averaged become 0, the average will also be 0. Hence, we adopt a smoothing technique from Chen and Cherry (2014) that assigns a small positive value  to any of Epn , Ern (Ri ) and Er (T i ) which are 0. When multiple references are available for a table, we compute PARENT against each reference and take the maximum as its overall score, similar to METEOR (Denkowski and Lavie, 2014). Choosing λ and . To set the value of λ we can tune it to maximize the correlation of the metric with human judgments, when such data is available. When such data is not available, we can use the recall of the reference against the table, using Eq. 6, as the value of 1 − λ. The intuition here is that if the recall of the re"
P19-1483,P09-1011,0,0.345045,"Missing"
P19-1483,W04-1013,0,0.218157,"et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). The underlying assumption behind these metrics is that the reference text is gold-standard, i.e., it is the ideal target text that a system should generate. In practice, however, when datasets are collected automatically and heuristically, the reference texts are often not ideal. Figure 1 shows an example from the WikiBio dataset (Lebret et al., 2016). Here the reference contains extra information which no system can be expected to produce given only the associated table. We call such reference texts divergent from the table. We show that existing automatic metrics, including BLEU, correlate"
P19-1483,W14-3348,0,0.355805,"led precision and recall are combined into an F-score to give the PARENT metric for one instance. The system-level PARENT score for a Smoothing & Multiple References. The danger with geometric averages is that if any of the components being averaged become 0, the average will also be 0. Hence, we adopt a smoothing technique from Chen and Cherry (2014) that assigns a small positive value  to any of Epn , Ern (Ri ) and Er (T i ) which are 0. When multiple references are available for a table, we compute PARENT against each reference and take the maximum as its overall score, similar to METEOR (Denkowski and Lavie, 2014). Choosing λ and . To set the value of λ we can tune it to maximize the correlation of the metric with human judgments, when such data is available. When such data is not available, we can use the recall of the reference against the table, using Eq. 6, as the value of 1 − λ. The intuition here is that if the recall of the reference against the table is high, it already covers most of the information, and we can assign it a high weight in Eq. 4. This leads to a separate value of λ automatically set for each instance.7  is set to 10−5 for all experiments. 4 Evaluation via Information Extractio"
P19-1483,P17-1017,0,0.371266,"al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). Th"
P19-1483,W05-1208,0,0.0215623,"recision Epn for n-grams of order n is given by: Epn = P g∈Gin P   / Rni )w(g) #Gin (g) P r(g ∈ Rni ) + P r(g ∈ P , g∈Gin #Gin (g) g∈Gin = #Gin (g)w(g) + #Gin ,Rni (g)[1 − w(g)] P . g∈Gin #Gin (g) (2) In words, an n-gram receives a reward of 1 if it appears in the reference, with probability P r(g ∈ Rni ), and otherwise it receives a reward of w(g). Both numerator and denominator are weighted by the count of the n-gram in Gin . P r(g ∈ Rni ) rewards an n-gram for appearing as many times as it appears in the reference, not more. We combine precisions for n-gram orders 1-4 using a geometric 5 Glickman and Dagan (2005) used a product instead of geometric mean. Here we use a geometric mean to ensure that n-grams of different lengths have comparable probabilities of being entailed. 6 It is unlikely that an automated system produces the same extra n-gram as present in the reference, thus a match with the reference n-gram is considered positive. For example, in Figure 1, it is highly unlikely that a system would produce “Silkworm” when it is not present in the table. 4886 model M is the average of instance level PARENT scores across the evaluation set: average, similar to BLEU: Ep = exp 4 X 1 n=1 4 ! log Epn N"
P19-1483,D14-1020,0,0.0226453,"an idea of how the correlation of each metric varies with the underlying data. In each iteration, we sample with replacement, tables along with their references and all the generated texts for that table. Then we compute aggregated human evaluation and metric scores for each of the models and compute the correlation between the two. We report the average correlation across all bootstrap samples for each metric in Table 2. The distribution of correlations for the best performing metrics are shown in Figure 3. Table 2 also indicates whether PARENT is significantly better than a baseline metric. Graham and Baldwin (2014) suggest using the William’s test for this purpose, but since we are computing correlations between only 4/13 systems at a time, this test has very weak power in our case. Hence, we use the bootstrap samples to obtain a 1 − α confidence interval of the difference in correlation 4889 1.0 WikiBio-Systems 0.8 1.0 WikiBio-Hyperparams 0.8 0.8 0.6 0.6 0.4 0.6 0.2 0.0 U -T -F C BLE BLEU RG PRT- PRT-WPRT*-W 0.0 U -T -F C BLE BLEU RG PRT- PRT-WPRT*-W Figure 3: Distribution of metric correlations across 500 bootstrap samples. PRT = PARENT. between PARENT and any other metric and check whether this is ab"
P19-1483,D17-1274,0,0.041252,"Missing"
P19-1483,P83-1022,0,0.360705,"gh a large scale human evaluation study of table-to-text models for WikiBio, we show that PARENT correlates with human judgments better than existing text generation metrics. We also adapt and evaluate the information extraction based evaluation proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system."
P19-1483,D10-1090,0,0.0335412,"Bio, the recall of the references against the table is generally low, and hence the recall of the generated text relies more on the table. For WebNLG, where the references are elicited from humans, this recall is much higher (often 1.0), and hence 12 We omit extractive evaluation metrics since no extraction systems are publicly available for this dataset, and developing one is beyond the scope of this work. 4891 While iBLEU penalizes texts which match the source, here we reward such texts since our task values accuracy of generated text more than the need for paraphrasing the tabular content (Liu et al., 2010). Similar to SARI for text simplification (Xu et al., 2016) and Q-BLEU for question generation (Nema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic met"
P19-1483,D16-1230,0,0.0372707,"nder the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006; Reiter, 2018; Liu et al., 2016; Kilickaya et al., 2017; Gatt and Krahmer, 2018). The only consensus from these studies seems to be that no single metric is suitable across all tasks. A recurring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show any correlation at the sentence level. Ours is the first study which checks t"
P19-1483,D18-1429,0,0.0249933,"relies more on the table. For WebNLG, where the references are elicited from humans, this recall is much higher (often 1.0), and hence 12 We omit extractive evaluation metrics since no extraction systems are publicly available for this dataset, and developing one is beyond the scope of this work. 4891 While iBLEU penalizes texts which match the source, here we reward such texts since our task values accuracy of generated text more than the need for paraphrasing the tabular content (Liu et al., 2010). Similar to SARI for text simplification (Xu et al., 2016) and Q-BLEU for question generation (Nema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006"
P19-1483,D17-1238,0,0.267569,"proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) a"
P19-1483,W17-5525,0,0.311156,"proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) a"
P19-1483,P02-1040,0,0.106059,"7; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). The underlying assumption behind these metrics is that the reference text is gold-standard, i.e., it is the ideal target text that a system should generate. In practice, however, when datasets are collected automatically and heuristically, the reference texts are often not ideal. Figure 1 shows an example from the WikiBio dataset (Lebret et al., 2016). Here the reference contains extra information which no system can be expected to produce given only the associated table. We call such reference texts divergent from the table. We show that existing automatic metrics, inc"
P19-1483,D14-1162,0,0.0825959,"Missing"
P19-1483,J18-3002,0,0.0906513,"PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006; Reiter, 2018; Liu et al., 2016; Kilickaya et al., 2017; Gatt and Krahmer, 2018). The only consensus from these studies seems to be that no single metric is suitable across all tasks. A recurring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show any correlation at the sentence level. Ours is the first st"
P19-1483,D18-1437,0,0.0608232,"curring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show any correlation at the sentence level. Ours is the first study which checks the quality of metrics when tableto-text references are divergent. We show that in this case even system level correlations can be unreliable. Hallucination (Rohrbach et al., 2018; Lee et al., 2018) refers to when an NLG system generates text which mentions extra information than what is present in the source from which it is generated. Divergence can be viewed as hallucination in the reference text itself. PARENT deals with hallucination by discounting n-grams which do not overlap with either the reference or the table. PARENT draws inspiration from iBLEU (Sun and Zhou, 2012), a metric for evaluating paraphrase generation, which compares the generated text to both the source text and the reference. Conclusions We study the automatic evaluation of table-to-text systems"
P19-1483,P12-2008,0,0.369925,"erences were also included in the evaluation, and they received a lower score than PG-Net, highlighting the divergence in WikiBio. 5.3 Compared Metrics Text only: We compare BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014), CIDEr and CIDErD (Vedantam et al., 2015) using their publicly available implementations. Information Extraction based: We compare the CS, RG and RG-F metrics discussed in §4. Text & Table: We compare a variant of BLEU, denoted as BLEU-T, where the values from the table are used as additional references. BLEUT draws inspiration from iBLEU (Sun and Zhou, 2012) but instead rewards n-grams which match the table rather than penalizing them. For PARENT, we compare both the word-overlap model (PARENT-W) and the co-occurrence model (PARENT-C) for determining entailment. We also compare versions where a single λ is tuned on the entire dataset to maximize correlation with human judgments, denoted as PARENT*-W/C. WikiBio Systems WikiBio Hyperparams Avg ROUGE CIDEr CIDEr-D METEOR BLEU 0.518±0.07C,W 0.674±0.06C,W 0.646±0.06C,W 0.697±0.06C,W 0.548±0.07C,W -0.585±0.15C,W -0.516±0.15C,W -0.372±0.16C,W -0.079±0.24C,W 0.407±0.15C,W -0.034 0.079 0.137 0.309 0.478 C"
P19-1483,N18-1136,0,0.0306942,"Missing"
P19-1483,D17-1239,0,0.530722,"red data. We show that metrics which rely solely on the reference texts, such as BLEU and ROUGE, show poor correlation with human judgments when those references diverge. We propose a new metric, PARENT, which aligns n-grams from the reference and generated texts to the semi-structured data before computing their precision and recall. Through a large scale human evaluation study of table-to-text models for WikiBio, we show that PARENT correlates with human judgments better than existing text generation metrics. We also adapt and evaluate the information extraction based evaluation proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et"
P19-1483,Q16-1029,0,0.0311333,"ally low, and hence the recall of the generated text relies more on the table. For WebNLG, where the references are elicited from humans, this recall is much higher (often 1.0), and hence 12 We omit extractive evaluation metrics since no extraction systems are publicly available for this dataset, and developing one is beyond the scope of this work. 4891 While iBLEU penalizes texts which match the source, here we reward such texts since our task values accuracy of generated text more than the need for paraphrasing the tabular content (Liu et al., 2010). Similar to SARI for text simplification (Xu et al., 2016) and Q-BLEU for question generation (Nema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Bu"
P19-1483,E17-1019,0,\N,Missing
P19-1483,P17-1099,0,\N,Missing
petrov-etal-2012-universal,D07-1013,1,\N,Missing
petrov-etal-2012-universal,zeman-2008-reusable,0,\N,Missing
petrov-etal-2012-universal,dickinson-jochim-2008-simple,0,\N,Missing
petrov-etal-2012-universal,nivre-etal-2006-talbanken05,0,\N,Missing
petrov-etal-2012-universal,J93-2004,0,\N,Missing
petrov-etal-2012-universal,D11-1018,0,\N,Missing
petrov-etal-2012-universal,boguslavsky-etal-2002-development,0,\N,Missing
petrov-etal-2012-universal,D10-1120,0,\N,Missing
petrov-etal-2012-universal,A97-1014,0,\N,Missing
petrov-etal-2012-universal,N03-1033,0,\N,Missing
petrov-etal-2012-universal,H05-1107,0,\N,Missing
petrov-etal-2012-universal,W06-2920,0,\N,Missing
petrov-etal-2012-universal,N01-1026,0,\N,Missing
petrov-etal-2012-universal,D10-1056,0,\N,Missing
petrov-etal-2012-universal,P10-1131,0,\N,Missing
petrov-etal-2012-universal,N09-1010,0,\N,Missing
petrov-etal-2012-universal,A00-1031,0,\N,Missing
petrov-etal-2012-universal,P11-1061,1,\N,Missing
petrov-etal-2012-universal,I08-3008,0,\N,Missing
petrov-etal-2012-universal,P04-1061,0,\N,Missing
petrov-etal-2012-universal,P11-2008,1,\N,Missing
petrov-etal-2012-universal,P95-1039,0,\N,Missing
petrov-etal-2012-universal,D11-1005,1,\N,Missing
petrov-etal-2012-universal,W00-1906,0,\N,Missing
petrov-etal-2012-universal,dzeroski-etal-2006-towards,0,\N,Missing
petrov-etal-2012-universal,simov-etal-2002-building,0,\N,Missing
petrov-etal-2012-universal,D07-1096,1,\N,Missing
petrov-etal-2012-universal,rambow-etal-2006-parallel,0,\N,Missing
petrov-etal-2012-universal,erjavec-2004-multext,0,\N,Missing
petrov-etal-2012-universal,afonso-etal-2002-floresta,0,\N,Missing
petrov-etal-2012-universal,P05-1044,0,\N,Missing
petrov-etal-2012-universal,P07-1096,0,\N,Missing
Q13-1001,I05-1075,0,\N,Missing
Q13-1001,D12-1127,0,\N,Missing
Q13-1001,J93-2004,0,\N,Missing
Q13-1001,N12-1052,1,\N,Missing
Q13-1001,N10-1083,0,\N,Missing
Q13-1001,H05-1107,0,\N,Missing
Q13-1001,W06-2920,0,\N,Missing
Q13-1001,N01-1026,0,\N,Missing
Q13-1001,C10-1124,0,\N,Missing
Q13-1001,D10-1056,0,\N,Missing
Q13-1001,P08-1085,0,\N,Missing
Q13-1001,D12-1075,0,\N,Missing
Q13-1001,P08-1086,0,\N,Missing
Q13-1001,P09-1057,0,\N,Missing
Q13-1001,P02-1035,0,\N,Missing
Q13-1001,petrov-etal-2012-universal,1,\N,Missing
Q13-1001,P10-1040,0,\N,Missing
Q13-1001,2005.mtsummit-papers.11,0,\N,Missing
Q13-1001,D07-1096,1,\N,Missing
Q13-1001,P05-1044,0,\N,Missing
Q15-1003,P98-1013,0,0.0420238,"rpora annotated with semantic roles for both verbal and nominal predicates (Weischedel et al., 2011) and strongly outperform the prior state of the art (Pradhan et al., 2013). Finally, we present results on FrameNet 1.5 data, again achieving state-of-the-art results. 2 (thing expected) A1 Task Overview We seek to predict the semantic argument structure of predicates in text. For brevity and practical reasons, the exposition and empirical study is primarily focused on PropBank-style annotations (Palmer et al., 2005). However, our approach applies directly to FrameNet-style annotations as well (Baker et al., 1998) and as shown empirically in §6, a similar trend 30 C-A1 to rain . me . who knew R-A0 know.01 A1 (thing known or thought) Figure 2: Examples showing continuation and reference roles according to PropBank. The role prefix C- indicates continuation of an argument, while the prefix R- indicates reference to another overt argument of the same predicate. holds across both types of annotation. In both cases, we are provided with a frame lexicon that contains type-level information for lexical units (a lemma conjoined with a coarse-grained part-ofspeech tag).1 For each lexical unit, a list of senses,"
Q15-1003,S07-1018,0,0.153159,"Missing"
Q15-1003,J92-4003,0,0.541434,"Missing"
Q15-1003,W04-2412,0,0.163315,"structured model results in significant improvements over its local counterpart, achieving state-of-the-art results on both PropBank- and FrameNet-annotated corpora. 1 Introduction Semantic role labeling (henceforth, SRL) is the task of identifying the semantic arguments of predicates in natural language text. Pioneered by Gildea and Jurafsky (2002), this task has been widely investigated by the NLP community. There have been two shared tasks at CoNLL 2004 and 2005 focusing on this problem, using PropBank conventions to identify the phrasal arguments of verbal predicates (Palmer et al., 2005; Carreras and Màrquez, 2004, 2005). Since then, there has been work on SRL for nominal predicates (Meyers et al., 2004; Gerber and Chai, 2010) and variants that investigated the prediction of semantic dependencies rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Here, we present an inference method for SRL, addressing the problem of phrasal argument structure Dipanjan Das Google New York dipanjand@google.com prediction (as opposed to semantic dependencies). In contrast to most prior semantic role labeling work focusing on PropBank conventions, barring notable exceptions such as Meza-Ruiz and R"
Q15-1003,W05-0620,0,0.707585,"nova et al., 2008; Johansson and Nugues, 2008). However, none of these methods provide any means to perform efficient marginal inference and this work is the first to use a globally normalized probabilistic model with structural constraints for this task. 6 Empirical Study We next present our experimental setup, datasets used, preprocessing details and empirical results. 6.1 Datasets and Evaluation We measure experimental results on three datasets. First, we use the CoNLL 2005 shared task data annotated according to PropBank conventions with the standard training, development and test splits (Carreras and Màrquez, 2005). These were originally constructed from sections 02-21, section 24 and section 23 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993). The PropBank I resource was used to construct the verb frame lexicon for the CoNLL 2005 experiments. Second, we perform experiments on a substantially larger data set annotated according to PropBank conventions, using the recent OntoNotes 5.0 corpus (Weischedel et al., 2011), with the CoNLL 2012 training, development and test splits from Pradhan et al. (2013). The frame lexicon for these experiments is 8 While the dynamic progra"
Q15-1003,S12-1029,1,0.517682,"that can be tractably enforced in the dynamic program. In effect, p(z |x, t, `, f ) = 0 for any z that violates the constraints. We estimate the parameters of this globally normalized model by maximizing the regularized conditional likelihood of the training set, using the standard forward-backward algorithm on the dynamic program lattice to compute the required normalizer and feature expectations. There have been several studies of the use of constrained MAP inference for semantic role labeling on top of the predictions of local classifiers (Tromble and Eisner, 2006; Punyakanok et al., 2008; Das et al., 2012), as well as on ensembles for combining the predictions of separate systems using integer linear programming (Surdeanu et al., 2007; Punyakanok et al., 2008).8 Meza-Ruiz and Riedel (2009) further used a Markov Logic Network formulation to incorporate a subset of these constraints during learning. Another popular approach has been to apply a reranking model, which can incorporate soft structural constraints in the form of features, on top of the k-best output of local classifiers (Toutanova et al., 2008; Johansson and Nugues, 2008). However, none of these methods provide any means to perform ef"
Q15-1003,J14-1002,1,0.514886,"contrast to most prior semantic role labeling work focusing on PropBank conventions, barring notable exceptions such as Meza-Ruiz and Riedel (2009), our framework first performs frame identification, the subtask of disambiguating the predicate frame; this makes our analysis more interpretable. The focus of this paper, however, is the subtask of semantic role labeling, wherein we take a set of (potentially overlapping) candidate sentential phrases and identify and label them with the semantic roles associated with the predicted frame. This treatment is commonly used in frame semantic parsing (Das et al., 2014; Hermann et al., 2014) and our two-stage framework is able to model both PropBank and FrameNet conventions. Previous work focusing on semantic role labeling imposed several structural constraints warranted by the annotation conventions of the task and other linguistic considerations, such as avoiding overlapping arguments and repeated core roles in the final prediction. Such global inference often leads to improved results and more meaningful predictions compared to local unconstrained methods (Màrquez et al., 2008). A popular framework for imposing these constraints has been integer linear p"
Q15-1003,P10-1160,0,0.0189773,"both PropBank- and FrameNet-annotated corpora. 1 Introduction Semantic role labeling (henceforth, SRL) is the task of identifying the semantic arguments of predicates in natural language text. Pioneered by Gildea and Jurafsky (2002), this task has been widely investigated by the NLP community. There have been two shared tasks at CoNLL 2004 and 2005 focusing on this problem, using PropBank conventions to identify the phrasal arguments of verbal predicates (Palmer et al., 2005; Carreras and Màrquez, 2004, 2005). Since then, there has been work on SRL for nominal predicates (Meyers et al., 2004; Gerber and Chai, 2010) and variants that investigated the prediction of semantic dependencies rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Here, we present an inference method for SRL, addressing the problem of phrasal argument structure Dipanjan Das Google New York dipanjand@google.com prediction (as opposed to semantic dependencies). In contrast to most prior semantic role labeling work focusing on PropBank conventions, barring notable exceptions such as Meza-Ruiz and Riedel (2009), our framework first performs frame identification, the subtask of disambiguating the predicate frame;"
Q15-1003,J02-3001,0,0.031816,"ining a globally-normalized log-linear model with respect to constrained conditional likelihood. We show that the dynamic program is several times faster than an off-the-shelf integer linear programming solver, while reaching the same solution. Furthermore, we show that our structured model results in significant improvements over its local counterpart, achieving state-of-the-art results on both PropBank- and FrameNet-annotated corpora. 1 Introduction Semantic role labeling (henceforth, SRL) is the task of identifying the semantic arguments of predicates in natural language text. Pioneered by Gildea and Jurafsky (2002), this task has been widely investigated by the NLP community. There have been two shared tasks at CoNLL 2004 and 2005 focusing on this problem, using PropBank conventions to identify the phrasal arguments of verbal predicates (Palmer et al., 2005; Carreras and Màrquez, 2004, 2005). Since then, there has been work on SRL for nominal predicates (Meyers et al., 2004; Gerber and Chai, 2010) and variants that investigated the prediction of semantic dependencies rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Here, we present an inference method for SRL, addressing the p"
Q15-1003,P14-1136,1,0.640787,"prior semantic role labeling work focusing on PropBank conventions, barring notable exceptions such as Meza-Ruiz and Riedel (2009), our framework first performs frame identification, the subtask of disambiguating the predicate frame; this makes our analysis more interpretable. The focus of this paper, however, is the subtask of semantic role labeling, wherein we take a set of (potentially overlapping) candidate sentential phrases and identify and label them with the semantic roles associated with the predicted frame. This treatment is commonly used in frame semantic parsing (Das et al., 2014; Hermann et al., 2014) and our two-stage framework is able to model both PropBank and FrameNet conventions. Previous work focusing on semantic role labeling imposed several structural constraints warranted by the annotation conventions of the task and other linguistic considerations, such as avoiding overlapping arguments and repeated core roles in the final prediction. Such global inference often leads to improved results and more meaningful predictions compared to local unconstrained methods (Màrquez et al., 2008). A popular framework for imposing these constraints has been integer linear programming (ILP), where"
Q15-1003,W05-1506,0,0.0516311,"n features • the predicate t • the lemma ` • tag of t’s children • parent word of t • dep. label of t • word to the left of t • tag to the left of t • word cluster of t Features additionally conjoined with the frame • tag of t • children words of t • tag of t’s parent • subcat. frame of t • dep. labels of t’s children • word to the right of t • tag to the right of t • word clusters of t’s children Table 1: Frame identification features. By subcategorization frame, we refer to the sequence of dependency labels of t’s children in the dependency tree. the previous section, using the algorithm of Huang and Chiang (2005) and picking the best solution that satisfies all the constraints. 5 Local and Structured Learning To train our models, we assume a training set where each predicate t (with lemma `) in sentence x has been identified and labeled with its semantic frame f , as well as with each candidate span and role pair (s, r) ∈ S × R. We first consider a local log-linear model. Let the local score of span s and role r be given by g(s, r) = θ · f (r, s, x, t, `, f ), where θ denotes the vector of model parameters and f (·) the feature function (see Table 2 for the specific features employed). We treat the lo"
Q15-1003,D08-1008,0,0.066986,"of local classifiers (Tromble and Eisner, 2006; Punyakanok et al., 2008; Das et al., 2012), as well as on ensembles for combining the predictions of separate systems using integer linear programming (Surdeanu et al., 2007; Punyakanok et al., 2008).8 Meza-Ruiz and Riedel (2009) further used a Markov Logic Network formulation to incorporate a subset of these constraints during learning. Another popular approach has been to apply a reranking model, which can incorporate soft structural constraints in the form of features, on top of the k-best output of local classifiers (Toutanova et al., 2008; Johansson and Nugues, 2008). However, none of these methods provide any means to perform efficient marginal inference and this work is the first to use a globally normalized probabilistic model with structural constraints for this task. 6 Empirical Study We next present our experimental setup, datasets used, preprocessing details and empirical results. 6.1 Datasets and Evaluation We measure experimental results on three datasets. First, we use the CoNLL 2005 shared task data annotated according to PropBank conventions with the standard training, development and test splits (Carreras and Màrquez, 2005). These were origin"
Q15-1003,J93-2004,0,0.0642824,"globally normalized probabilistic model with structural constraints for this task. 6 Empirical Study We next present our experimental setup, datasets used, preprocessing details and empirical results. 6.1 Datasets and Evaluation We measure experimental results on three datasets. First, we use the CoNLL 2005 shared task data annotated according to PropBank conventions with the standard training, development and test splits (Carreras and Màrquez, 2005). These were originally constructed from sections 02-21, section 24 and section 23 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993). The PropBank I resource was used to construct the verb frame lexicon for the CoNLL 2005 experiments. Second, we perform experiments on a substantially larger data set annotated according to PropBank conventions, using the recent OntoNotes 5.0 corpus (Weischedel et al., 2011), with the CoNLL 2012 training, development and test splits from Pradhan et al. (2013). The frame lexicon for these experiments is 8 While the dynamic program in §4 could be used to efficiently implement such ensembles, since it solves the equivalent ILP, our focus in this work is on learning a single accurate model. 36 d"
Q15-1003,J08-2001,0,0.123073,"th the predicted frame. This treatment is commonly used in frame semantic parsing (Das et al., 2014; Hermann et al., 2014) and our two-stage framework is able to model both PropBank and FrameNet conventions. Previous work focusing on semantic role labeling imposed several structural constraints warranted by the annotation conventions of the task and other linguistic considerations, such as avoiding overlapping arguments and repeated core roles in the final prediction. Such global inference often leads to improved results and more meaningful predictions compared to local unconstrained methods (Màrquez et al., 2008). A popular framework for imposing these constraints has been integer linear programming (ILP), wherein the inference problem is specified declaratively (Punyakanok et al., 2008). However, ILP-based inference methods often rely on generic off-the-shelf solvers that fail to exploit problem-specific structure (Martins et al., 2011). Instead, we present a dynamic program (DP) that exactly enforces most of the constraints examined by Punyakanok et al. (2008); remaining constraints are enforced by reverting to k-best inference if needed. We show that this technique solves the inference problem more"
Q15-1003,D11-1022,0,0.0238453,"ions of the task and other linguistic considerations, such as avoiding overlapping arguments and repeated core roles in the final prediction. Such global inference often leads to improved results and more meaningful predictions compared to local unconstrained methods (Màrquez et al., 2008). A popular framework for imposing these constraints has been integer linear programming (ILP), wherein the inference problem is specified declaratively (Punyakanok et al., 2008). However, ILP-based inference methods often rely on generic off-the-shelf solvers that fail to exploit problem-specific structure (Martins et al., 2011). Instead, we present a dynamic program (DP) that exactly enforces most of the constraints examined by Punyakanok et al. (2008); remaining constraints are enforced by reverting to k-best inference if needed. We show that this technique solves the inference problem more than four times faster than a state-of-the-art off-the-shelf ILP solver, while 29 Transactions of the Association for Computational Linguistics, vol. 3, pp. 29–41, 2015. Action Editor: Kristina Toutanova. c Submission batch: 9/2014; Revision batch 1/2015; Published 1/2015. 2015 Association for Computational Linguistics. (wanter)"
Q15-1003,W04-2705,0,0.109046,"f-the-art results on both PropBank- and FrameNet-annotated corpora. 1 Introduction Semantic role labeling (henceforth, SRL) is the task of identifying the semantic arguments of predicates in natural language text. Pioneered by Gildea and Jurafsky (2002), this task has been widely investigated by the NLP community. There have been two shared tasks at CoNLL 2004 and 2005 focusing on this problem, using PropBank conventions to identify the phrasal arguments of verbal predicates (Palmer et al., 2005; Carreras and Màrquez, 2004, 2005). Since then, there has been work on SRL for nominal predicates (Meyers et al., 2004; Gerber and Chai, 2010) and variants that investigated the prediction of semantic dependencies rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Here, we present an inference method for SRL, addressing the problem of phrasal argument structure Dipanjan Das Google New York dipanjand@google.com prediction (as opposed to semantic dependencies). In contrast to most prior semantic role labeling work focusing on PropBank conventions, barring notable exceptions such as Meza-Ruiz and Riedel (2009), our framework first performs frame identification, the subtask of disambiguat"
Q15-1003,N09-1018,0,0.0737464,"d Màrquez, 2004, 2005). Since then, there has been work on SRL for nominal predicates (Meyers et al., 2004; Gerber and Chai, 2010) and variants that investigated the prediction of semantic dependencies rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Here, we present an inference method for SRL, addressing the problem of phrasal argument structure Dipanjan Das Google New York dipanjand@google.com prediction (as opposed to semantic dependencies). In contrast to most prior semantic role labeling work focusing on PropBank conventions, barring notable exceptions such as Meza-Ruiz and Riedel (2009), our framework first performs frame identification, the subtask of disambiguating the predicate frame; this makes our analysis more interpretable. The focus of this paper, however, is the subtask of semantic role labeling, wherein we take a set of (potentially overlapping) candidate sentential phrases and identify and label them with the semantic roles associated with the predicted frame. This treatment is commonly used in frame semantic parsing (Das et al., 2014; Hermann et al., 2014) and our two-stage framework is able to model both PropBank and FrameNet conventions. Previous work focusing"
Q15-1003,J05-1004,0,0.267486,"re, we show that our structured model results in significant improvements over its local counterpart, achieving state-of-the-art results on both PropBank- and FrameNet-annotated corpora. 1 Introduction Semantic role labeling (henceforth, SRL) is the task of identifying the semantic arguments of predicates in natural language text. Pioneered by Gildea and Jurafsky (2002), this task has been widely investigated by the NLP community. There have been two shared tasks at CoNLL 2004 and 2005 focusing on this problem, using PropBank conventions to identify the phrasal arguments of verbal predicates (Palmer et al., 2005; Carreras and Màrquez, 2004, 2005). Since then, there has been work on SRL for nominal predicates (Meyers et al., 2004; Gerber and Chai, 2010) and variants that investigated the prediction of semantic dependencies rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Here, we present an inference method for SRL, addressing the problem of phrasal argument structure Dipanjan Das Google New York dipanjand@google.com prediction (as opposed to semantic dependencies). In contrast to most prior semantic role labeling work focusing on PropBank conventions, barring notable except"
Q15-1003,J08-2005,0,0.923573,"opBank and FrameNet conventions. Previous work focusing on semantic role labeling imposed several structural constraints warranted by the annotation conventions of the task and other linguistic considerations, such as avoiding overlapping arguments and repeated core roles in the final prediction. Such global inference often leads to improved results and more meaningful predictions compared to local unconstrained methods (Màrquez et al., 2008). A popular framework for imposing these constraints has been integer linear programming (ILP), wherein the inference problem is specified declaratively (Punyakanok et al., 2008). However, ILP-based inference methods often rely on generic off-the-shelf solvers that fail to exploit problem-specific structure (Martins et al., 2011). Instead, we present a dynamic program (DP) that exactly enforces most of the constraints examined by Punyakanok et al. (2008); remaining constraints are enforced by reverting to k-best inference if needed. We show that this technique solves the inference problem more than four times faster than a state-of-the-art off-the-shelf ILP solver, while 29 Transactions of the Association for Computational Linguistics, vol. 3, pp. 29–41, 2015. Action"
Q15-1003,N10-1117,0,0.0219029,"xponential in |RC |, in practice this is a modest constant as each frame only has a small number of possible core roles (two or three for many frames).6 Furthermore, since many of the potential edges are pruned by the constraints, as described below, the added computational complexity is further reduced. Lattice Weights The edges are weighted in the same way as in §4.1. It is easy to verify that the structure enforces unique core roles, but is otherwise equivalent to that in §4.1. Since the weights are identical, the proof of Proposition 1 carries over directly. 5 We note that the approach of Riedel and Smith (2010) could potentially be used to compute the marginals in an incremental fashion similar to Tromble and Eisner (2006). 6 In the OntoNotes 5.0 development set, there are on average 10.4 core-role combinations per predicate frame. 34 ∅ 1,1 A0 1,0 ∅ 1,0 ∅ 1,0 ∅ ∅ 1,0 1,0 ∅ 1,0 C-A1 A1 0,1 ∅ 0,1 A0 A0 0,0 ∅ 0,0 It Lattice Structure The set of vertices are now V = {v0 , vn+1 , vjk : j ∈ [1, n] , k ∈ {0, 1}|RC |}, where v0 and vn+1 are the start and end vertices. The remaining vertices vjk are analogous to the ones in §4.1 but are annotated with a bit vector encoding the subset of core roles that have"
Q15-1003,J08-2002,0,0.861067,"Missing"
Q15-1003,N06-1054,0,0.360983,"for the direct correspondence with the ILP score, its weight is constant across variable assignments. Thus, this edge only adds a constant offset of c∅ ∗ to the ILP solution. For the same reason, its presence has no influence on the arg max or marginal computations and we therefore drop it in our implementation. 4.2 Unique Core Roles To incorporate the unique core roles constraint, we add state signatures to the vertices in the lattice and restrict the edges accordingly. This increases the size of the lattice by O(2|RC |), where RC is the set of core roles. Our approach is similar to that of Tromble and Eisner (2006), but whereas they suggest incorporating the uniqueness constraints incrementally, we apply them all at once. This is necessary since we seek to train a structured probabilistic model, which requires the marginals with respect to the full set of constraints.5 While the number of signatures is exponential in |RC |, in practice this is a modest constant as each frame only has a small number of possible core roles (two or three for many frames).6 Furthermore, since many of the potential edges are pruned by the constraints, as described below, the added computational complexity is further reduced."
Q15-1003,P10-1040,0,0.00527171,"Missing"
Q15-1003,W04-3212,0,0.228435,"s described by Turian et al. (2010), which are induced with the Brown algorithm (Brown et al., 1992). 6.3 Candidate Argument Extraction We use a rule-based heuristic to extract candidate arguments for role labeling. Most prior work on PropBank-style semantic role labeling have relied on constituency syntax for candidate argument extraction. Instead, we rely on dependency syntax, which allows faster preprocessing and potential extension to the many languages for which only dependency annotations are available. To this end, we adapt the constituency-based candidate argument extraction method of Xue and Palmer (2004) to dependencies. In gold PropBank annotations, syntactic constituents serve as arguments in all constructions. However, extracting constituents from a dependency tree is not straightforward. The full dependency subtree under a particular head word often merges syntactic constituents. For example, in the tree fragment root det The 6.4 Baseline Systems We compare our local and structured models to the top performing constituency-based systems from the 11 rcmod who All but the following labels are treated as offensive: advmod, amod, appos, aux, auxpass, cc, conj, dep, det, mwe, neg, nn, npadvmod"
Q15-1003,P14-2107,0,0.0586704,"Missing"
Q15-1003,W08-2121,0,\N,Missing
Q15-1003,W13-3516,0,\N,Missing
Q15-1003,C98-1013,0,\N,Missing
Q15-1003,W09-1201,0,\N,Missing
Q16-1010,D15-1138,0,0.00679455,"atkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vecto"
Q16-1010,P02-1041,0,0.0927099,"(Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on te"
Q16-1010,P14-1091,0,0.0366925,"Missing"
Q16-1010,P14-1133,0,0.232833,"Missing"
Q16-1010,Q15-1039,0,0.656552,"Missing"
Q16-1010,D13-1160,0,0.418417,"za ) ∧ arg1 (xe , ya ) ∧ arg2 (xe , za ) (c) The composed lambda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-arg"
Q16-1010,D14-1067,0,0.304718,"in and obtains the best result to date. Interestingly, D EP T REE outperforms S IMPLE G RAPH in this case. We attribute this to the small training set and larger lexical variation of Free917. The structural features of the graph-based representations seem highly beneficial in this case. 6.3 Error Analysis We categorized 100 errors made by D EP L AMBDA (+C +E) on the WebQuestions development set. In 43 cases the correct answer is present in the beam, 136 Method Cai and Yates (2013) Berant et al. (2013) Kwiatkowski et al. (2013) Yao and Van Durme (2014) Berant and Liang (2014) Bao et al. (2014) Bordes et al. (2014) Yao (2015) Yih et al. (2015) (FB API) Bast and Haussmann (2015) Berant and Liang (2015) Yih et al. (2015) (Y&C) Free917 Accuracy WebQuestions Average F1 59.0 62.0 68.0 – 68.5 – – – – 76.4 – – – 35.7 – 33.0 39.9 37.5 39.2 44.3 48.4 49.4 49.7 52.5 This Work D EP T REE S IMPLE G RAPH CCGG RAPH (+ C + E) D EP L AMBDA (+ C + E) 53.2 43.7 73.3 78.0 40.4 48.5 48.6 50.3 Table 3: Question-answering results on the WebQuestions and Free917 test sets. but ranked below an incorrect answer (e.g., for where does volga river start, the annotated gold answer is Valdai Hills, which is ranked second, with Russi"
Q16-1010,C04-1180,1,0.158689,"There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on textual entailment."
Q16-1010,P13-1042,0,0.356088,"Missing"
Q16-1010,P15-1127,1,0.568655,"was sworn into office when john f kennedy was assassinated ), we do not have a special treatment for them in the semantic representation. Differences in syntactic parsing performance and the somewhat limited expressivity of the semantic representation are likely the reasons for CCGG RAPH’s lower performance. 7 Related Work There are two relevant strands of prior work: general purpose ungrounded semantics and grounded semantic parsing. The former have been studied on their own and as a component in tasks such as semantic parsing to knowledge bases (Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015; Krishnamurthy and Mitchell, 2015), sentence simplification (Narayan and Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from"
Q16-1010,W09-3726,0,0.0740413,"it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on textual entailment. Our work, in contrast, assumes access only to dependency trees and offers an alternative method based on the lambda calculus, mimicking the stru"
Q16-1010,W02-1001,1,0.101306,"∈ &lt;n denotes the features for the pair of ungrounded and grounded graphs. Note that for a given query there may be multiple ungrounded graphs, primarily due to the optional use of the CON TRACT operation.3 The feature function has access to the ungrounded and grounded graphs, to the question, as well as to the content of the knowledge base and the denotation |g|K (the denotation of a grounded graph is defined as the set of entities or attributes reachable at its TARGET node). See Section 5.3 for the features employed. The model parameters are estimated with the averaged structured perceptron (Collins, 2002; Fre3 Another source of ambiguity may be a lexical item having multiple lambda-calculus entries; in our rules this only arises when analyzing count expressions such as how many. und and Schapire, 1999). Given a training questionanswer pair (q, A), the update is: θt+1 ← θt + Φ(u+ , g + , q, K) − Φ(ˆ u, gˆ, q, K) , where (u+ , g + ) denotes the pair of gold ungrounded and grounded graphs for q. Since we do not have direct access to these gold graphs, we instead rely on the set of oracle graphs, OK,A (q), as a proxy: (u+ , g + ) = arg max θt · Φ(u, g, q, K) , (u,g)∈OK,A (q) where OK,A (q) is def"
Q16-1010,P01-1019,0,0.126066,"nd Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph"
Q16-1010,C04-1026,0,0.10619,"tics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on textual entailment. Our work, in contrast, assumes access only to dependency trees and offers an alternative method based on the lambda calculus,"
Q16-1010,P15-1026,0,0.238247,"et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally deriving logical forms. The resulting representation is subsequently grounded to Freebase by learning from question-answer pairs. E"
Q16-1010,P14-1134,0,0.0159776,"gh an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally deriving logical forms. The resulting represent"
Q16-1010,E03-1030,0,0.118015,"araphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to"
Q16-1010,P09-1069,0,0.0392655,"entation is an inherent problem with approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to ra"
Q16-1010,P15-1143,0,0.0199683,"ntic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally deriving logical forms. The resulting representation is subsequently grou"
Q16-1010,N10-1145,0,0.0444525,"ource semantic representation and the target application’s representation is an inherent problem with approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion"
Q16-1010,D12-1069,0,0.104733,"cquired Pixar nnp vbd nnp (nsubj (dobj acquired Pixar) Disney) (b) The s-expression for the dependency tree. λx. ∃yz. acquired(xe ) ∧ Disney(ya ) ∧ Pixar(za ) ∧ arg1 (xe , ya ) ∧ arg2 (xe , za ) (c) The composed lambda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances ove"
Q16-1010,Q15-1019,0,0.0603458,"s-expression is beta-reduced to give the logical form in Figure 1(c). Since dependency syntax does not have an associated type theory, we introduce a type system that assigns a single type to all constituents, thus avoiding the need for type checking (Section 2). D EP L AMBDA uses this system to generate robust logical forms, even when the dependency structure does not mirror predicate-argument relationships in constructions such as conjunctions, prepositional phrases, relative clauses, and wh-questions (Section 3). These ungrounded logical forms (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015) are used for question answering against Freebase, by passing them as input to G RAPH PARSER (Reddy et al., 2014), a system that learns to map logical predicates to Freebase, resulting in grounded Freebase queries (Section 4). We show that our approach achieves state-of-the-art performance on the Free917 dataset and competitive performance on the WebQuestions dataset, whereas building the Freebase queries directly from dependency trees gives significantly lower performance. Finally, we show that our approach outperforms a directly comparable method that generates ungrounded logical forms using"
Q16-1010,D10-1119,1,0.364959,"tructures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of"
Q16-1010,D13-1161,1,0.949182,"lambda-calculus expression and the relabeled s-expression is beta-reduced to give the logical form in Figure 1(c). Since dependency syntax does not have an associated type theory, we introduce a type system that assigns a single type to all constituents, thus avoiding the need for type checking (Section 2). D EP L AMBDA uses this system to generate robust logical forms, even when the dependency structure does not mirror predicate-argument relationships in constructions such as conjunctions, prepositional phrases, relative clauses, and wh-questions (Section 3). These ungrounded logical forms (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015) are used for question answering against Freebase, by passing them as input to G RAPH PARSER (Reddy et al., 2014), a system that learns to map logical predicates to Freebase, resulting in grounded Freebase queries (Section 4). We show that our approach achieves state-of-the-art performance on the Free917 dataset and competitive performance on the WebQuestions dataset, whereas building the Freebase queries directly from dependency trees gives significantly lower performance. Finally, we show that our approach outperforms a directly comparab"
Q16-1010,D14-1107,1,0.663912,"XPAND. development set. +(-)C: with(out) CONTRACT. +(-)E: with(out) EXPAND. Syntactic Parsing. We recase the resolved entity mentions and run a case-sensitive second-order conditional random field part-of-speech tagger (Lafferty et al., 2001). The hypergraph parser of Zhang and McDonald (2014) is used for dependency parsing. The tagger and parser are both trained on the OntoNotes 5.0 corpus (Weischedel et al., 2011), with constituency trees converted to Stanford-style dependencies (De Marneffe and Manning, 2008). To derive the CCG-based representation, we use the output of the EasyCCG parser (Lewis and Steedman, 2014). edge, we can ground the edge to a Freebase relation, contract the edge in either direction, or skip the edge. For an entity type node, we can ground the node to a Freebase type, or skip the node. The order of traversal is based on the number of named entities connected to an edge. After an edge is grounded, the entity type nodes connected to it are grounded in turn, before the next edge is processed. To restrict the search, if two beam items correspond to the same grounded graph, the one with the lower score is discarded. A beam size of 100 was used in all experiments. Features. We use the f"
Q16-1010,P11-1060,0,0.141153,"plication’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally der"
Q16-1010,N15-1114,0,0.0108635,"representation. Differences in syntactic parsing performance and the somewhat limited expressivity of the semantic representation are likely the reasons for CCGG RAPH’s lower performance. 7 Related Work There are two relevant strands of prior work: general purpose ungrounded semantics and grounded semantic parsing. The former have been studied on their own and as a component in tasks such as semantic parsing to knowledge bases (Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015; Krishnamurthy and Mitchell, 2015), sentence simplification (Narayan and Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TA"
Q16-1010,P14-5010,0,0.00310002,"presentation of Reddy et al. (2014), adding the CONTRACT and EXPAND operations to increase its expressivity. 5.3 Implementation Details Below are more details of our entity resolution model, the syntactic parser used, features in the grounding model and the beam search procedure. Entity Resolution. For Free917, we follow prior work and resolve entities by string match against the entity lexicon provided with the dataset. For WebQuestions, we use eight handcrafted part-of-speech patterns to identify entity span candidates. We use the Stanford CoreNLP caseless tagger for part-of-speech tagging (Manning et al., 2014). For each candidate mention span, we retrieve the top 10 entities according to the Freebase API.5 We then create a lattice in which the nodes correspond to mention-entity pairs, scored by their Freebase API scores, and the edges encode the fact that no joint assignment of entities to mentions can contain overlapping spans. Finally, we generate ungrounded graphs for the top 10 paths through the lattice and treat the final entity disambiguation as part of the semantic parsing problem. 4 5 http://github.com/sivareddyg/graph-parser http://developers.google.com/freebase/ Representation -C -E -C +E"
Q16-1010,P13-2109,0,0.018844,"robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-argument structures.1 We empirically validate the utility of these logical forms for question answering from databases. Since our approach uses dependency trees as input, we hypothesize that it will generalize better to domains that are well covered by dependency parsers than methods that induce semantic grammars from scratch. The system that maps a dependency tree to its logical form (hencefo"
Q16-1010,H05-1066,0,0.0191937,"Missing"
Q16-1010,P14-1041,0,0.019399,"special treatment for them in the semantic representation. Differences in syntactic parsing performance and the somewhat limited expressivity of the semantic representation are likely the reasons for CCGG RAPH’s lower performance. 7 Related Work There are two relevant strands of prior work: general purpose ungrounded semantics and grounded semantic parsing. The former have been studied on their own and as a component in tasks such as semantic parsing to knowledge bases (Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015; Krishnamurthy and Mitchell, 2015), sentence simplification (Narayan and Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al.,"
Q16-1010,N15-1077,0,0.0234129,"ith approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given q"
Q16-1010,P15-1146,0,0.0242314,"Missing"
Q16-1010,P13-1092,0,0.00889818,"nt problem with approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential"
Q16-1010,Q14-1030,1,0.452199,"mbda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-argument structures.1 We empirically validate the utility of t"
Q16-1010,N15-1118,0,0.0159445,"Missing"
Q16-1010,N15-1040,0,0.0128228,"epresentations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and a"
Q16-1010,N06-1056,0,0.0904852,"trongest result to date on Free917 and competitive results on WebQuestions. 1 Disney acquired Pixar nnp vbd nnp (nsubj (dobj acquired Pixar) Disney) (b) The s-expression for the dependency tree. λx. ∃yz. acquired(xe ) ∧ Disney(ya ) ∧ Pixar(za ) ∧ arg1 (xe , ya ) ∧ arg2 (xe , za ) (c) The composed lambda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et"
Q16-1010,P07-1121,0,0.0158316,"converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provid"
Q16-1010,P15-1049,0,0.0484602,"Missing"
Q16-1010,P14-1090,0,0.408064,"Missing"
Q16-1010,N13-1106,0,0.0699006,"Missing"
Q16-1010,N15-3014,0,0.0687989,"iginal dependency tree. An event is created for each parent and its dependents in the tree. Each dependent is linked to this event with an edge labeled with its dependency relation, while the parent is linked to the event with an edge labeled arg0 . If a word is a question word, an additional TARGET predicate is attached to its entity node. S IMPLE G RAPH. This representation has a single event to which all entities in the question are connected by the predicate arg1 . An additional TARGET node is connected to the event by the predicate arg0 . This is similar to the template representation of Yao (2015) and Bast and Haussmann (2015). Note that this cannot represent any compositional structure. CCGG RAPH. Finally, we compare to the CCGbased semantic representation of Reddy et al. (2014), adding the CONTRACT and EXPAND operations to increase its expressivity. 5.3 Implementation Details Below are more details of our entity resolution model, the syntactic parser used, features in the grounding model and the beam search procedure. Entity Resolution. For Free917, we follow prior work and resolve entities by string match against the entity lexicon provided with the dataset. For WebQuestions, we use"
Q16-1010,P15-1128,0,0.133944,"into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-argument structures.1 We empirically validate the utility of these logical forms for question answering from databases. Since ou"
Q16-1010,P14-2107,0,0.0173573,".0 73.4 (c) Accuracy 42.6 48.2 46.5 48.8 42.6 48.2 48.9 50.4 D EP T REE S IMPLE G RAPH CCGG RAPH D EP L AMBDA Table 1: Oracle statistics and accuracies on the Web21.3 40.9 68.3 69.3 21.3 40.9 69.4 71.3 Table 2: Oracle statistics and accuracies on the Free917 Questions development set. +(-)C: with(out) CONTRACT. +(-)E: with(out) EXPAND. development set. +(-)C: with(out) CONTRACT. +(-)E: with(out) EXPAND. Syntactic Parsing. We recase the resolved entity mentions and run a case-sensitive second-order conditional random field part-of-speech tagger (Lafferty et al., 2001). The hypergraph parser of Zhang and McDonald (2014) is used for dependency parsing. The tagger and parser are both trained on the OntoNotes 5.0 corpus (Weischedel et al., 2011), with constituency trees converted to Stanford-style dependencies (De Marneffe and Manning, 2008). To derive the CCG-based representation, we use the output of the EasyCCG parser (Lewis and Steedman, 2014). edge, we can ground the edge to a Freebase relation, contract the edge in either direction, or skip the edge. For an entity type node, we can ground the node to a Freebase type, or skip the node. The order of traversal is based on the number of named entities connect"
Q16-1010,J90-1001,0,\N,Missing
Q16-1010,D15-1198,0,\N,Missing
S10-1059,N10-1138,1,0.246183,"rnegie Mellon University, Pittsburgh, PA 15213, USA {desaic@andrew,dipanjan@cs,nschneid@cs,nasmith@cs}.cmu.edu Abstract we detect null instantiations via a simple two-stage pipeline: the first stage predicts whether a given role is null-instantiated, and the second stage (§4) predicts how it is null-instantiated, if it is not overt. We report performance on the SemEval 2010 test set under the full-SRL and NI-only conditions. This paper describes the SEMAFOR system’s performance in the SemEval 2010 task on linking events and their participants in discourse. Our entry is based upon SEMAFOR 1.0 (Das et al., 2010a), a frame-semantic probabilistic parser built from log-linear models. The extended system models null instantiations, including non-local argument reference. Performance is evaluated on the task data with and without gold-standard overt arguments. In both settings, it fares the best of the submitted systems with respect to recall and F1 . 1 2 Data The SemEval 2007 task on frame-semantic parsing (Baker et al., 2007) provided a small (about 50,000 words and 2,000 sentences) dataset of news text, travel guides, and bureaucratic accounts of weapons stockpiles. Sentences in this dataset were full"
S10-1059,S07-1018,0,\N,Missing
S10-1059,S10-1008,0,\N,Missing
S12-1029,P11-1048,0,0.0143799,"emantic frame abstractions, and employ explicit role labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by work adopting this method for syntax and translation (Koo et al., 2010; Auli and Lopez, 2011; DeNero and Macherey, 2011; Rush and Collins, 2011; Chang and Collins, 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” decomposition, i.e., one with few overlapping components. This leaves out many declarative constrained problems, for which such a good decomposition is not readily available. For those, Martins et al. (2011b) proposed the AD3 al216 gorithm, which retains the modularity of previous methods, but can handle thousands of small overlapping components. Exact d"
S12-1029,S07-1018,0,0.0523873,"xact solution. Two observations are noteworthy. First, the optimal value of the relaxed problem (Eq. 11) provides an upper bound to the original problem (Eq. 2). This is because Eq. 2 has the additional integer constraint on the variables. In particular, any feasible dual point provides an upper bound to the original 214 4.1 Experiments and Results Dataset, Preprocessing, and Learning In our experiments, we use FrameNet 1.5, which contains a lexicon of 877 frames and 1,068 role labels, and 78 documents with multiple predicateargument annotations (a superset of the SemEval shared task dataset; Baker et al., 2007). We used the same split as Das and Smith (2011), with 55 documents for training (containing 19,582 frame annotations) and 23 for testing (with 4,458 annotations). We randomly selected 4,462 predicates in the training set as development data. The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005). The state-of-the-art system for this task is SEMAFOR, an open source tool (Das et al., 2010a)5 that provides a baseline benchmark for our new algorithm. We use the components of SEMAFOR as-is to"
S12-1029,D11-1003,0,0.0195552,"it in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by work adopting this method for syntax and translation (Koo et al., 2010; Auli and Lopez, 2011; DeNero and Macherey, 2011; Rush and Collins, 2011; Chang and Collins, 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” decomposition, i.e., one with few overlapping components. This leaves out many declarative constrained problems, for which such a good decomposition is not readily available. For those, Martins et al. (2011b) proposed the AD3 al216 gorithm, which retains the modularity of previous methods, but can handle thousands of small overlapping components. Exact decoding: This paper contributes an exact branch-and-bound technique wrapped a"
S12-1029,S10-1059,1,0.488928,"Missing"
S12-1029,P11-1144,1,0.396255,"First, the optimal value of the relaxed problem (Eq. 11) provides an upper bound to the original problem (Eq. 2). This is because Eq. 2 has the additional integer constraint on the variables. In particular, any feasible dual point provides an upper bound to the original 214 4.1 Experiments and Results Dataset, Preprocessing, and Learning In our experiments, we use FrameNet 1.5, which contains a lexicon of 877 frames and 1,068 role labels, and 78 documents with multiple predicateargument annotations (a superset of the SemEval shared task dataset; Baker et al., 2007). We used the same split as Das and Smith (2011), with 55 documents for training (containing 19,582 frame annotations) and 23 for testing (with 4,458 annotations). We randomly selected 4,462 predicates in the training set as development data. The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005). The state-of-the-art system for this task is SEMAFOR, an open source tool (Das et al., 2010a)5 that provides a baseline benchmark for our new algorithm. We use the components of SEMAFOR as-is to define the features h and train the weights ψ u"
S12-1029,N10-1138,1,0.116028,"cuments with multiple predicateargument annotations (a superset of the SemEval shared task dataset; Baker et al., 2007). We used the same split as Das and Smith (2011), with 55 documents for training (containing 19,582 frame annotations) and 23 for testing (with 4,458 annotations). We randomly selected 4,462 predicates in the training set as development data. The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005). The state-of-the-art system for this task is SEMAFOR, an open source tool (Das et al., 2010a)5 that provides a baseline benchmark for our new algorithm. We use the components of SEMAFOR as-is to define the features h and train the weights ψ used in the scoring function c. We also use its 5 http://www.ark.cs.cmu.edu/SEMAFOR heuristic mechanism to find potential spans St for a given predicate t. SEMAFOR learns weights using `2 -penalized log-likelihood; we augmented its dev set-tuning procedure to tune both the regularization strength and the AD3 penalty strength ρ. We initialize ρ = 0.1 and follow Martins et al. (2011b) in dynamically adjusting it. Note that we do not use SEMAFOR’s a"
S12-1029,P11-1043,0,0.0256002,"ions, and employ explicit role labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by work adopting this method for syntax and translation (Koo et al., 2010; Auli and Lopez, 2011; DeNero and Macherey, 2011; Rush and Collins, 2011; Chang and Collins, 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” decomposition, i.e., one with few overlapping components. This leaves out many declarative constrained problems, for which such a good decomposition is not readily available. For those, Martins et al. (2011b) proposed the AD3 al216 gorithm, which retains the modularity of previous methods, but can handle thousands of small overlapping components. Exact decoding: This paper contrib"
S12-1029,P10-1160,0,0.0220157,"of effort has been spent by the optimization community to devise efficient generic solvers. An example is CPLEX, a state-of-the-art solver for mixed integer programming that we employ as a baseline to solve the ILP in Eq. 2 as well as its LP relaxation in Eq. 11. Like many of the best implementations, CPLEX is proprietary. 4 We noticed in the annotated data, in some cases, the “requires” constraint is violated by the FrameNet annotators. This happens mostly when one of the required roles is absent in the sentence containing the predicate, but is rather instantiated in an earlier sentence; see Gerber and Chai (2010). We apply the hard constraint in Eq. 10, though extending our algorithm to seek arguments outside the sentence is straightforward (Chen et al., 2010). 2.2 Linguistic Constraints from FrameNet a pair of roles that share the “requires” relationship. Although enforcing the four different sets of constraints above is intuitive from a general linguistic perspective, we ground their use in definitive linguistic information present in the FrameNet lexicon (Fillmore et al., 2003). FrameNet, along with lists of semantic frames, associated semantic roles, and predicates that could evoke the frames, giv"
S12-1029,J02-3001,0,0.124019,"“requires” and “excludes” constraints of §2. Finally decoding time (without feature computation steps) on the whole test set is shown in the last column averaged over 5 runs. 5 Related Work Semantic role labeling: Most SRL systems use conventions from PropBank (Kingsbury and Palmer, 2002) and NomBank (Meyers et al., 2004), which store information about verbal and nominal predicates and corresponding symbolic and meaningspecific semantic roles. A separate line of work, including this paper, investigates SRL systems that use FrameNet conventions; while less popular, these systems, pioneered by Gildea and Jurafsky (2002), consider predicates of a wider variety of syntactic categories, use semantic frame abstractions, and employ explicit role labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by w"
S12-1029,S07-1048,0,0.0706484,"Missing"
S12-1029,kingsbury-palmer-2002-treebank,0,0.514983,"using FrameNet, these interactions have been largely ignored, though they 1 have the potential to improve the quality and consistency of semantic analysis. In this paper, we present an algorithm that finds the full collection of arguments of a predicate given its semantic frame. Although we work within the conventions of FrameNet, our approach is generalizable to other semantic role labeling (SRL) frameworks. We model this argument identification task as constrained optimization, where the constraints come from expert knowledge encoded in a lexicon. Following prior work on PropBank-style SRL (Kingsbury and Palmer, 2002) that dealt with similar constrained problems (Punyakanok et al., 2004; Punyakanok et al., 2008, inter alia), we incorporate this declarative knowledge in an integer linear program (ILP). Because general-purpose ILP solvers are proprietary and do not fully exploit the structure of the problem, we turn to a class of optimization techniques called dual decomposition (Komodakis et al., 2007; Rush et al., 2010; Martins et al., 2011a). We derive a modular, extensible, parallelizable approach in which semantic constraints map not just to declarative components of the algorithm, but also to procedura"
S12-1029,D10-1125,0,0.0857449,"categories, use semantic frame abstractions, and employ explicit role labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by work adopting this method for syntax and translation (Koo et al., 2010; Auli and Lopez, 2011; DeNero and Macherey, 2011; Rush and Collins, 2011; Chang and Collins, 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” decomposition, i.e., one with few overlapping components. This leaves out many declarative constrained problems, for which such a good decomposition is not readily available. For those, Martins et al. (2011b) proposed the AD3 al216 gorithm, which retains the modularity of previous methods, but can handle thousands of small overlappi"
S12-1029,J08-2001,0,0.0417426,"Missing"
S12-1029,D10-1004,1,0.572151,"rtner 1 and Partner 2 that share the “requires” relationship. In fact, out of 877 frames in FrameNet 1.5, the lexicon’s latest edition, 204 frames have at least a pair of roles that share the “excludes” relationship, and 54 list at least 2.3 Constraints as Factors in a Graphical Model 212 The LP in Eq. 11 can be represented as a maximum a posteriori (MAP) inference problem in an undirected graphical model. In the factor graph, each component of z corresponds to a binary variable, and each instantiation of a constraint in Eqs. 3, 5, 8 and 10 corresponds to a factor. Smith and Eisner (2008) and Martins et al. (2010) used such a representation to impose constraints in a dependency parsing problem; the latter discussed the equivalence of linear programs and factor graphs for representing discrete optimization problems. Each of our constraints take standard factor forms we can describe using the terminology of Smith and Eisner (2008) and Martins et al. (2010). The uniqueness constraint in Eq. 3 corresponds to an X OR factor, while the overlap constraint in Eq. 5 corresponds to an AT M OST O NE factor. The constraints in Eq. 8 enforcing the “excludes” relationship can be represented with an O R factor. Final"
S12-1029,D11-1022,1,0.770273,"entification task as constrained optimization, where the constraints come from expert knowledge encoded in a lexicon. Following prior work on PropBank-style SRL (Kingsbury and Palmer, 2002) that dealt with similar constrained problems (Punyakanok et al., 2004; Punyakanok et al., 2008, inter alia), we incorporate this declarative knowledge in an integer linear program (ILP). Because general-purpose ILP solvers are proprietary and do not fully exploit the structure of the problem, we turn to a class of optimization techniques called dual decomposition (Komodakis et al., 2007; Rush et al., 2010; Martins et al., 2011a). We derive a modular, extensible, parallelizable approach in which semantic constraints map not just to declarative components of the algorithm, but also to procedural ones, in the form of “workers.” While dual decomposition algorithms only solve a relaxation of the original problem, we make a novel contribution by wrapping the algorithm in a branch-andbound search procedure, resulting in exact solutions. We experimentally find that our algorithm achieves accuracy comparable to a state-of-the-art system, while respecting all imposed linguistic constraints. In comparison to inexact beam sear"
S12-1029,P05-1012,0,0.0760088,"ments, we use FrameNet 1.5, which contains a lexicon of 877 frames and 1,068 role labels, and 78 documents with multiple predicateargument annotations (a superset of the SemEval shared task dataset; Baker et al., 2007). We used the same split as Das and Smith (2011), with 55 documents for training (containing 19,582 frame annotations) and 23 for testing (with 4,458 annotations). We randomly selected 4,462 predicates in the training set as development data. The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005). The state-of-the-art system for this task is SEMAFOR, an open source tool (Das et al., 2010a)5 that provides a baseline benchmark for our new algorithm. We use the components of SEMAFOR as-is to define the features h and train the weights ψ used in the scoring function c. We also use its 5 http://www.ark.cs.cmu.edu/SEMAFOR heuristic mechanism to find potential spans St for a given predicate t. SEMAFOR learns weights using `2 -penalized log-likelihood; we augmented its dev set-tuning procedure to tune both the regularization strength and the AD3 penalty strength ρ. We initialize ρ = 0.1 and f"
S12-1029,W04-2705,0,0.0292647,".17 ± 0.01 4.78 ± 0.04 Table 1: Comparison of decoding strategies in §4.2. We evaluate in terms of precision, recall and F1 score on a test set containing 4,458 predicates. We also compute the number of structural violations each model makes: number of overlapping arguments and violations of the “requires” and “excludes” constraints of §2. Finally decoding time (without feature computation steps) on the whole test set is shown in the last column averaged over 5 runs. 5 Related Work Semantic role labeling: Most SRL systems use conventions from PropBank (Kingsbury and Palmer, 2002) and NomBank (Meyers et al., 2004), which store information about verbal and nominal predicates and corresponding symbolic and meaningspecific semantic roles. A separate line of work, including this paper, investigates SRL systems that use FrameNet conventions; while less popular, these systems, pioneered by Gildea and Jurafsky (2002), consider predicates of a wider variety of syntactic categories, use semantic frame abstractions, and employ explicit role labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has t"
S12-1029,C04-1197,0,0.149314,"1 have the potential to improve the quality and consistency of semantic analysis. In this paper, we present an algorithm that finds the full collection of arguments of a predicate given its semantic frame. Although we work within the conventions of FrameNet, our approach is generalizable to other semantic role labeling (SRL) frameworks. We model this argument identification task as constrained optimization, where the constraints come from expert knowledge encoded in a lexicon. Following prior work on PropBank-style SRL (Kingsbury and Palmer, 2002) that dealt with similar constrained problems (Punyakanok et al., 2004; Punyakanok et al., 2008, inter alia), we incorporate this declarative knowledge in an integer linear program (ILP). Because general-purpose ILP solvers are proprietary and do not fully exploit the structure of the problem, we turn to a class of optimization techniques called dual decomposition (Komodakis et al., 2007; Rush et al., 2010; Martins et al., 2011a). We derive a modular, extensible, parallelizable approach in which semantic constraints map not just to declarative components of the algorithm, but also to procedural ones, in the form of “workers.” While dual decomposition algorithms"
S12-1029,J08-2005,0,0.890274,"mprove the quality and consistency of semantic analysis. In this paper, we present an algorithm that finds the full collection of arguments of a predicate given its semantic frame. Although we work within the conventions of FrameNet, our approach is generalizable to other semantic role labeling (SRL) frameworks. We model this argument identification task as constrained optimization, where the constraints come from expert knowledge encoded in a lexicon. Following prior work on PropBank-style SRL (Kingsbury and Palmer, 2002) that dealt with similar constrained problems (Punyakanok et al., 2004; Punyakanok et al., 2008, inter alia), we incorporate this declarative knowledge in an integer linear program (ILP). Because general-purpose ILP solvers are proprietary and do not fully exploit the structure of the problem, we turn to a class of optimization techniques called dual decomposition (Komodakis et al., 2007; Rush et al., 2010; Martins et al., 2011a). We derive a modular, extensible, parallelizable approach in which semantic constraints map not just to declarative components of the algorithm, but also to procedural ones, in the form of “workers.” While dual decomposition algorithms only solve a relaxation o"
S12-1029,W96-0213,0,0.0498838,"Dataset, Preprocessing, and Learning In our experiments, we use FrameNet 1.5, which contains a lexicon of 877 frames and 1,068 role labels, and 78 documents with multiple predicateargument annotations (a superset of the SemEval shared task dataset; Baker et al., 2007). We used the same split as Das and Smith (2011), with 55 documents for training (containing 19,582 frame annotations) and 23 for testing (with 4,458 annotations). We randomly selected 4,462 predicates in the training set as development data. The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005). The state-of-the-art system for this task is SEMAFOR, an open source tool (Das et al., 2010a)5 that provides a baseline benchmark for our new algorithm. We use the components of SEMAFOR as-is to define the features h and train the weights ψ used in the scoring function c. We also use its 5 http://www.ark.cs.cmu.edu/SEMAFOR heuristic mechanism to find potential spans St for a given predicate t. SEMAFOR learns weights using `2 -penalized log-likelihood; we augmented its dev set-tuning procedure to tune both the regularization strength and t"
S12-1029,P11-1008,0,0.0134412,"ole labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by work adopting this method for syntax and translation (Koo et al., 2010; Auli and Lopez, 2011; DeNero and Macherey, 2011; Rush and Collins, 2011; Chang and Collins, 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” decomposition, i.e., one with few overlapping components. This leaves out many declarative constrained problems, for which such a good decomposition is not readily available. For those, Martins et al. (2011b) proposed the AD3 al216 gorithm, which retains the modularity of previous methods, but can handle thousands of small overlapping components. Exact decoding: This paper contributes an exact branch-and"
S12-1029,D10-1001,0,0.260942,"el this argument identification task as constrained optimization, where the constraints come from expert knowledge encoded in a lexicon. Following prior work on PropBank-style SRL (Kingsbury and Palmer, 2002) that dealt with similar constrained problems (Punyakanok et al., 2004; Punyakanok et al., 2008, inter alia), we incorporate this declarative knowledge in an integer linear program (ILP). Because general-purpose ILP solvers are proprietary and do not fully exploit the structure of the problem, we turn to a class of optimization techniques called dual decomposition (Komodakis et al., 2007; Rush et al., 2010; Martins et al., 2011a). We derive a modular, extensible, parallelizable approach in which semantic constraints map not just to declarative components of the algorithm, but also to procedural ones, in the form of “workers.” While dual decomposition algorithms only solve a relaxation of the original problem, we make a novel contribution by wrapping the algorithm in a branch-andbound search procedure, resulting in exact solutions. We experimentally find that our algorithm achieves accuracy comparable to a state-of-the-art system, while respecting all imposed linguistic constraints. In compariso"
S12-1029,D08-1016,0,0.0185648,"tners, also has two roles Partner 1 and Partner 2 that share the “requires” relationship. In fact, out of 877 frames in FrameNet 1.5, the lexicon’s latest edition, 204 frames have at least a pair of roles that share the “excludes” relationship, and 54 list at least 2.3 Constraints as Factors in a Graphical Model 212 The LP in Eq. 11 can be represented as a maximum a posteriori (MAP) inference problem in an undirected graphical model. In the factor graph, each component of z corresponds to a binary variable, and each instantiation of a constraint in Eqs. 3, 5, 8 and 10 corresponds to a factor. Smith and Eisner (2008) and Martins et al. (2010) used such a representation to impose constraints in a dependency parsing problem; the latter discussed the equivalence of linear programs and factor graphs for representing discrete optimization problems. Each of our constraints take standard factor forms we can describe using the terminology of Smith and Eisner (2008) and Martins et al. (2010). The uniqueness constraint in Eq. 3 corresponds to an X OR factor, while the overlap constraint in Eq. 5 corresponds to an AT M OST O NE factor. The constraints in Eq. 8 enforcing the “excludes” relationship can be represented"
S12-1029,P05-1073,0,0.0878624,"t line imposes constraints on the mapping between roles and spans; these are motivated on linguistic grounds and are described next.3 Uniqueness: Each role r is filled by at most one span in St . This constraint can be expressed by: P ∀r ∈ Rf , s∈St zr,s = 1. (3) There are O(|Rf |) such constraints. Note that since St contains the null span ∅, non-overt roles are also captured using the above constraints. Such a constraint is used extensively in prior literature (Punyakanok et al., 2008, §3.4.1). Overlap: SRL systems commonly constrain roles to be filled by non-overlapping spans. For example, Toutanova et al. (2005) used dynamic programming over a phrase structure tree to prevent overlaps between arguments, and Punyakanok et al. (2008) used 3 Note that equality constraints a · z = b can be transformed into double-side inequalities a · z ≤ b and −a · z ≤ −b. constraints in an ILP to respect this requirement. Inspired by the latter, we require that each input sentence position of x be covered by at most one argument. For each role r ∈ Rf , we define: Gr (i) = {s |s ∈ St , s covers position i in x}. (4) We can define our overlap constraints in terms of Gr as follows, for every sentence position i: P P ∀i ∈"
W09-2813,I08-1035,1,0.241935,"cular history of a session will affect what is considered to be briefing-worthy. Figure 2: The category tree showing the information types that we expect in a briefing. events are the task creation and task completion actions logged by various cognitive assistants in the system (so-called specialists). As part of the design phase for the template-based generation component, we identified a set of templates, based on the actual briefings written by users in a separate experiment. Ideally, we would like to adopt a corpus-based approach to automatically extract the templates in the domain, like (Kumar et al., 2008), but since the sample briefings available to us were very few, the application of such corpusbased techniques was not necessary. Based on this set of templates we identified the patterns that needed to be extracted from the event logs in order to populate the templates. A ranking model was also designed for ordering instantiations of this set of templates and to recommend the top 4 most relevant ones for a given session. The overall data flow for BA during a session (runtime) is shown in Figure 1. The various specialist modules generate task related events that are logged in a database. The a"
W09-2813,W09-2813,1,0.0523105,"in that experiment chose to include in their reports corresponding to nine categories shown in Figure 2. We found that information can be conveyed at different levels of granularity (for example, qualitatively or quantitatively). The appropriate choice of granularity for 3 System Overview Figure 1: Briefing Assistant Data Flow. The Briefing Assistant Model: We treat the task of briefing generation in the current domain1 as non-textual event-based summarization. The 1 More details about the domain and the interaction of BA with the larger system are mentioned in a longer version of the paper (Kumar et al., 2009) 68 is external to our system; it took into account the template categories we earlier identified. Figure 4 shows a sample briefing email stimulus. The mapping from the sample email in the figure to the categories is as follows: “expected attendance” - Property-Session; “how many sessions have been rescheduled”, “how many still need to be rescheduled”, “any problems you see as you try to reschedule” - Session-Reschedule; “status of food service (I am worried about the keynote lunch)” - Catering Vendors. Training: Eleven expert users5 were asked to provide training by using the system then gene"
W09-2813,P06-1047,0,0.32611,"ing their tasks faster and more accurately. The difference between this work from most previous efforts, primarily based on textextraction approaches is the emphasis on learning to summarize event patterns. This work also differs in its emphasis on learning from user behavior in the context of a task. 2 Related Work Event based summarization has been studied in the summarization community. (Daniel et al., 2003) described identification of sub-events in multiple documents. (Filatova and Hatzivassiloglou, 2004) mentioned the use of event-based features in extractive summarization and (Wu, 2006; Li et al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sports (Oh and Shrob"
W09-2813,N04-1015,0,0.058674,"eration and Summarisation, ACL-IJCNLP 2009, pages 67–71, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sports (Oh and Shrobe, 2008) etc. However, in our case we want to summarize a variety of tasks that the user is performing and present a summary to an intended audience (as defined by a report request). Recent advances in NLG research use statistical approaches at various stages of processing in the generation pipeline like content selection (Duboue and McKeown, 2003; Barzilay and Lee, 2004), probabilistic generation rules (Belz, 2007). Our proposed approach differs from these in that we apply machine learning after generation of all the templates, as a post-processing step, to rank them for inclusion in the final briefing. We could have used a general purpose template-based generation framework like TG/2 (Busemann, 2005), but since the number of templates and their corresponding aggregators is limited, we chose an approach based on string manipulation. We found in our work that an approach based on modeling individual users and then combining the outputs of such models using a v"
W09-2813,N07-1021,0,0.149906,"n and (Wu, 2006; Li et al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sports (Oh and Shrobe, 2008) etc. However, in our case we want to summarize a variety of tasks that the user is performing and present a summary to an intended audience (as defined by a report request). Recent advances in NLG research use statistical approaches at various stages of processing in the generation pipeline like content selection (Duboue and McKeown, 2003; Barzilay and Lee, 2004), probabilistic generation rules (Belz, 2007). Our proposed approach differs from these in that we apply machine learning after generation of all the templates, as a post-processing step, to rank them for"
W09-2813,W05-1603,0,0.207137,"orming and present a summary to an intended audience (as defined by a report request). Recent advances in NLG research use statistical approaches at various stages of processing in the generation pipeline like content selection (Duboue and McKeown, 2003; Barzilay and Lee, 2004), probabilistic generation rules (Belz, 2007). Our proposed approach differs from these in that we apply machine learning after generation of all the templates, as a post-processing step, to rank them for inclusion in the final briefing. We could have used a general purpose template-based generation framework like TG/2 (Busemann, 2005), but since the number of templates and their corresponding aggregators is limited, we chose an approach based on string manipulation. We found in our work that an approach based on modeling individual users and then combining the outputs of such models using a voting scheme gives the best results, although our approach is distinguishable from collaborative filtering techniques used for driving recommendation systems (Hofmann, 2004). We believe this is due to the fact that the individual sessions from which ranking models are learned, although they range over the same collection of component t"
W09-2813,W08-1124,0,0.156171,"t al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sports (Oh and Shrobe, 2008) etc. However, in our case we want to summarize a variety of tasks that the user is performing and present a summary to an intended audience (as defined by a report request). Recent advances in NLG research use statistical approaches at various stages of processing in the generation pipeline like content selection (Duboue and McKeown, 2003; Barzilay and Lee, 2004), probabilistic generation rules (Belz, 2007). Our proposed approach differs from these in that we apply machine learning after generation of all the templates, as a post-processing step, to rank them for inclusion in the final briefi"
W09-2813,W03-0502,0,0.377088,"es draft reports for another set of users. This system, the Briefing Assistant(BA), is part of a set of learning-based cognitive assistants each of which observes users and learns to assist users in performing their tasks faster and more accurately. The difference between this work from most previous efforts, primarily based on textextraction approaches is the emphasis on learning to summarize event patterns. This work also differs in its emphasis on learning from user behavior in the context of a task. 2 Related Work Event based summarization has been studied in the summarization community. (Daniel et al., 2003) described identification of sub-events in multiple documents. (Filatova and Hatzivassiloglou, 2004) mentioned the use of event-based features in extractive summarization and (Wu, 2006; Li et al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pag"
W09-2813,J98-3005,0,0.334522,"Agarwal and Alexander I. Rudnicky Language Technologies Institute Carnegie Mellon University, Pittsburgh, USA mohitkum,dipanjan,sachina,air@cs.cmu.edu Abstract Report generation from non-textual sources has been previously explored in the Natural Language Generation (NLG) community in a variety of domains, based on, for example, a database of events. However, a purely generative approach is not suitable in our circumstances, as we want to summarize a variety of tasks that the user is performing and present a summary tailored to a target audience, a desirable characteristic of good briefings (Radev and McKeown, 1998). Thus we approach the problem by applying learning techniques combined with a template-based generation system to instantiate the briefing-worthy report items. The task of instantiating the briefing-worthy items is similar to the task of Content Selection (Duboue, 2004) in the Generation pipeline however our approach minimizes linguistic involvement. Our choice of a template-based generative system was motivated by recent discussions in the NLG community (van Deemter et al., 2005) about the practicality and effectiveness of this approach. The plan of the paper is as follows. We describe relev"
W09-2813,W03-1016,0,0.169791,"9 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sports (Oh and Shrobe, 2008) etc. However, in our case we want to summarize a variety of tasks that the user is performing and present a summary to an intended audience (as defined by a report request). Recent advances in NLG research use statistical approaches at various stages of processing in the generation pipeline like content selection (Duboue and McKeown, 2003; Barzilay and Lee, 2004), probabilistic generation rules (Belz, 2007). Our proposed approach differs from these in that we apply machine learning after generation of all the templates, as a post-processing step, to rank them for inclusion in the final briefing. We could have used a general purpose template-based generation framework like TG/2 (Busemann, 2005), but since the number of templates and their corresponding aggregators is limited, we chose an approach based on string manipulation. We found in our work that an approach based on modeling individual users and then combining the outputs"
W09-2813,J05-1002,0,0.411147,"Missing"
W09-2813,W04-1017,0,0.113097,"is part of a set of learning-based cognitive assistants each of which observes users and learns to assist users in performing their tasks faster and more accurately. The difference between this work from most previous efforts, primarily based on textextraction approaches is the emphasis on learning to summarize event patterns. This work also differs in its emphasis on learning from user behavior in the context of a task. 2 Related Work Event based summarization has been studied in the summarization community. (Daniel et al., 2003) described identification of sub-events in multiple documents. (Filatova and Hatzivassiloglou, 2004) mentioned the use of event-based features in extractive summarization and (Wu, 2006; Li et al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP reports based on database of events"
W09-2813,P06-3007,0,0.230527,"in performing their tasks faster and more accurately. The difference between this work from most previous efforts, primarily based on textextraction approaches is the emphasis on learning to summarize event patterns. This work also differs in its emphasis on learning from user behavior in the context of a task. 2 Related Work Event based summarization has been studied in the summarization community. (Daniel et al., 2003) described identification of sub-events in multiple documents. (Filatova and Hatzivassiloglou, 2004) mentioned the use of event-based features in extractive summarization and (Wu, 2006; Li et al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sp"
W09-2813,W00-0410,0,\N,Missing
W10-2925,J93-2004,0,0.0383684,"Missing"
W10-2925,J94-2001,0,0.150122,"Missing"
W10-2925,W03-0301,0,0.0211998,"Missing"
W10-2925,W03-0419,0,0.0128423,"Missing"
W10-2925,J93-2003,0,0.0351098,"rgence criterion is left unspecified here. Langford et al. (2009) present convergence rates via regret bounds, which are linear in D. The convergence rate √ of asynchronous stochastic gradient descent is O( T D), where T is the total number of updates made. In addition to the situation in which function components are chosen uniformly at random, Langford et al. provide results for several other scenarios, including the case in which an adversary supplies the training examples in whatever ordering he chooses. of training examples. For example, for simple word alignment models like IBM Model 1 (Brown et al., 1993), only parameters corresponding to words appearing in the particular subsample of sentence pairs are needed. The error introduced when making asynchronous updates should intuitively be less severe in these cases, where different mini-batches use small and mostly nonoverlapping subsets of θ. 5.1 Below we experiment with optimization of both convex and non-convex functions, using fixed step sizes and decreasing step size formulas, and consider several values of D. Even when exploring regions of the experimental space that are not yet supported by theoretical results, asynchronous algorithms perf"
W10-2925,D08-1024,0,0.0252101,"essors in systems without shared memory). Nonetheless, this simple approach is widely used in practice; approaches in which the gradient computation is distributed via MapReduce have recently been described in machine learning and NLP (Chu et al., 2006; Dyer et al., 2008; Wolfe et al., 2008). Mann et al. (2009) compare this framework to one in which each processor maintains a separate parameter vector which is updated independently of the others. At the end of learning, the parameter vectors are averaged or a vote is taken during prediction. A similar parameter-averaging approach was taken by Chiang et al. (2008) when parallelizing MIRA (Crammer et al., 2006). In this paper, we restrict our attention to distributed frameworks which maintain and update a single copy of the parameters θ. The use of multiple parameter vectors is essentially orthogonal to the framework we discuss here and we leave the integration of the two ideas for future exploration. 4 (4) 5 Distributed Asynchronous Mini-Batch Optimization An asynchronous framework may use multiple processors more efficiently and minimize idle time (Nedic et al., 2001; Langford et al., 2009). In this setting, the master sends θ and a mini-batch Mk to e"
W10-2925,W08-0333,0,0.0135295,"-batch is shorter than the time for a full batch, mini-batch algorithms make far more updates and some processor cycles will be wasted in computing each one. Also, more mini-batches imply that more time will be lost due to per-mini-batch overhead (e.g., waiting for synchronization locks in sharedmemory systems, or sending data and θ to the processors in systems without shared memory). Nonetheless, this simple approach is widely used in practice; approaches in which the gradient computation is distributed via MapReduce have recently been described in machine learning and NLP (Chu et al., 2006; Dyer et al., 2008; Wolfe et al., 2008). Mann et al. (2009) compare this framework to one in which each processor maintains a separate parameter vector which is updated independently of the others. At the end of learning, the parameter vectors are averaged or a vote is taken during prediction. A similar parameter-averaging approach was taken by Chiang et al. (2008) when parallelizing MIRA (Crammer et al., 2006). In this paper, we restrict our attention to distributed frameworks which maintain and update a single copy of the parameters θ. The use of multiple parameter vectors is essentially orthogonal to the fra"
W10-2925,P08-1109,0,0.312135,"nguage Technologies Institute Carnegie Mellon Univeristy Pittsburgh, PA 15213, USA {kgimpel,dipanjan,nasmith}@cs.cmu.edu Abstract Online algorithms offer fast convergence rates and scalability to large datasets, but distributed computing is a more natural fit for algorithms that require a lot of computation—e.g., processing a large batch of training examples—to be done between updates. Typically, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al., 2008). Each mini-batch is processed only after the previous one has completed. Synchronous frameworks are appealing in that they simulate the same algorithms that work on a single processor, but they have the drawback that the benefits of parallelism are only obtainable within one mini-batch iteration. Moreover, empirical evaluations suggest that online methods only converge faster than batch algorithms when using very small mini-batches (Liang and Klein, 2009). In this case, synchronous parallelization will not offer much benefit. In this paper, we focus our attention on asynchronous algorithms th"
W10-2925,D07-1031,0,0.0117089,"s,9 resulting in a total of 10 runs for each algorithm. We ran each for six hours, saving models every five minutes. After training completed, using each model we decoded the entire training data using posterior decoding and computed the log-likelihood. The results for 5 initial models and two example orderings are shown in Figure 6. We evaluated tagging performance using many-to-1 accuracy, which is obtained by mapping the HMM states to gold standard POS tags so as to maximize accuracy, where multiple states can be mapped to the same tag. This is the metric used by Liang and Klein (2009) and Johnson (2007), who report figures comparable to ours. The asynchronous algorithm converges much faster than the single-node algorithm, allowing a tagger to be trained from the Penn Treebank in less than two hours using a single machine. Furthermore, the 4-processor synchronous algorithm improves only marginally EM converges faster as m decreases. With large mini-batches, load-balancing becomes less important as there will be less variation in per-minibatch observed runtime. These results suggest that asynchronous mini-batch algorithms will be most useful for learning problems in which small minibatches wor"
W10-2925,D07-1033,0,0.0101298,"ingle−processor 86 0 2 4 6 Wall clock time (hours) 8 10 Figure 2: NER: (Top) Synchronous optimization improves very little when moving from 2 to 4 processors due to the need for load-balancing, leaving some processors idle for stretches of time. (Bottom) Asynchronous optimization does not require load balancing and therefore improves when moving from 2 to 4 processors because each processor is in nearconstant use. All curves use a mini-batch size of 4 and the “Single-processor” curve is identical in the two plots. Named Entity Recognition Our NER CRF used a standard set of features, following Kazama and Torisawa (2007), along with token shape features like those in Collins (2002) and simple gazetteer features; a feature was included if and only it occurred at least once in training data (total 1.3M). We used a diagonal Gaussian prior with a variance of 1.0 for each weight. We compared SGD on a single processor to distributed synchronous SGD and distributed asynchronous SGD. For all experiments, we used a fixed step size of 0.01 and chose each training example for each mini-batch uniformly at random from the full data set.3 We report performance by 3 88 plotting test-set accuracy against wall-time over 12 ho"
W10-2925,N03-1017,0,0.00503619,"k and NER is the size of the mini-batch used, so we experimented with several values for the mini-batch size m. Figure 5 shows the results. As m decreases, a larger fraction of time is spent updating parameters; this slows observed convergence time even when using the sparse update rule. It can be seen that, though synchronous and asynchronous stepwise EM converge at the same rate with a large minibatch size (m = 10000), asynchronous stepwise We trained IBM Model 1 in both directions. To align test data, we symmetrized both directional Viterbi alignments using the “grow-diag-final” heuristic (Koehn et al., 2003). We evaluated our models using alignment error rate (AER). Experiments on a Single Machine We followed Liang and Klein (2009) in using synchronous (mini-batch) stepwise EM on a single processor for this task. We used the same learning rate formula (η (t) = (t + 2)−q , with 0.5 &lt; q ≤ 1). We also used asynchronous stepwise EM by using the same update rule, but gathered sufficient statistics on 4 processors of a single machine in parallel, analogous to our asynchronous method from §5. Whenever a processor was done gathering the expected counts for its mini-batch, it updated the sufficient statis"
W10-2925,N09-1069,0,0.450305,"setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al., 2008). Each mini-batch is processed only after the previous one has completed. Synchronous frameworks are appealing in that they simulate the same algorithms that work on a single processor, but they have the drawback that the benefits of parallelism are only obtainable within one mini-batch iteration. Moreover, empirical evaluations suggest that online methods only converge faster than batch algorithms when using very small mini-batches (Liang and Klein, 2009). In this case, synchronous parallelization will not offer much benefit. In this paper, we focus our attention on asynchronous algorithms that generalize those presented by Nedic et al. (2001) and Langford et al. (2009). In these algorithms, multiple mini-batches are processed simultaneously, each using potentially different and typically stale parameters. The key advantage of an asynchronous framework is that it allows processors to remain in near-constant use, preventing them from wasting cycles waiting for other processors to complete their portion of the current mini-batch. In this way, as"
W10-2925,W03-1013,0,\N,Missing
W10-2925,P02-1034,0,\N,Missing
W10-2925,P02-1062,0,\N,Missing
W14-3007,W04-2705,0,0.130199,"Missing"
W14-3007,J14-1002,1,0.888217,"taining one target per sentence) as opposed to a “corpus” of sentences, it resulted in a flurry of work in the area of automatic semantic role labeling (M`arquez et al., 2008). However, the focus of semantic role labeling (SRL) research has mostly been on PropBank (Palmer et al., 2005) conventions, where verbal targets could evoke a 26 Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore (1929–2014), pages 26–29, c Baltimore, Maryland USA, June 27, 2014. 2014 Association for Computational Linguistics Figure 1: A partial depiction of frame-semantic structures taken from Das et al. (2014). The words in bold correspond to targets, which evoke semantic frames that are denoted in capital letters. Above each target is shown the corresponding lexical unit, which is a lemma appended by a coarse part-of-speech tag. Every frame is shown in a distinct color; each frame’s arguments are annotated with the same color, and are marked below the sentence, at different levels. For the C ARDINAL NUMBERS frame, “M” denotes the role Multiplier and “E” denotes the role Entity. SemEval’07 Data (automatic targets) FrameNet 1.5 Release (gold targets) P R F1 Johansson and Nugues (2007) Das et al. (20"
W14-3007,N09-1018,0,0.068344,"Missing"
W14-3007,J12-4003,0,0.0499513,"Missing"
W14-3007,W12-1901,0,0.0853213,"eature sets in both the models were sufficiently different from prior work, resulting in improvements. Table 1 shows results on the SemEval 2007 data for these two systems. The FrameNet project released more annotations and a larger frame lexicon in 2010; Das et al. (2014) used this dataset, and presented a variety of experiments improving upon their prior work, setting the new state of the art. A few salient aspects 1 2 See http://www.ark.cs.cmu.edu/SEMAFOR. 27 Related work on the analysis of implicit arguments for in developing unsupervised techniques for inducing frame-semantic structures (Modi et al., 2012), to induce FrameNet-like lexicons from weak supervision, such as syntactic parses. 3 lines, such that these annotations can be produced outside the FrameNet project. Other than increasing the amount of labeled data, there is a necessity of automatically aligning predicate-level semantic knowledge present in resources like FrameNet, PropBank, NomBank and VerbNet (Schuler, 2005). These lexicons share a lot of knowledge about predicates and current resources like Ontonotes do align some of the information, but a lot remains missing. For example, alignment between these lexicons could be done wit"
W14-3007,J02-3001,0,0.232233,"isambiguating their semantic frame representing an event and scenario in discourse, and annotating arguments consisting of words or phrases in text with various frame elements (or roles). The FrameNet lexicon (Baker et al., 1998), an ontology inspired by the theory of frame semantics (Fillmore, 1982), serves as a repository of semantic frames and their roles. Figure 1 depicts a sentence with three evoked frames for the targets “million”, “created” and “pushed” with FrameNet frames and roles. Automatic analysis of text using framesemantic structures can be traced back to the pioneering work of Gildea and Jurafsky (2002). Although their experimental setup relied on a primitive version of FrameNet and only made use of “exemplars” or example usages of semantic frames (containing one target per sentence) as opposed to a “corpus” of sentences, it resulted in a flurry of work in the area of automatic semantic role labeling (M`arquez et al., 2008). However, the focus of semantic role labeling (SRL) research has mostly been on PropBank (Palmer et al., 2005) conventions, where verbal targets could evoke a 26 Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore (1929–2014), pages 26–29, c Balti"
W14-3007,J05-1004,0,0.179902,"reated” and “pushed” with FrameNet frames and roles. Automatic analysis of text using framesemantic structures can be traced back to the pioneering work of Gildea and Jurafsky (2002). Although their experimental setup relied on a primitive version of FrameNet and only made use of “exemplars” or example usages of semantic frames (containing one target per sentence) as opposed to a “corpus” of sentences, it resulted in a flurry of work in the area of automatic semantic role labeling (M`arquez et al., 2008). However, the focus of semantic role labeling (SRL) research has mostly been on PropBank (Palmer et al., 2005) conventions, where verbal targets could evoke a 26 Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore (1929–2014), pages 26–29, c Baltimore, Maryland USA, June 27, 2014. 2014 Association for Computational Linguistics Figure 1: A partial depiction of frame-semantic structures taken from Das et al. (2014). The words in bold correspond to targets, which evoke semantic frames that are denoted in capital letters. Above each target is shown the corresponding lexical unit, which is a lemma appended by a coarse part-of-speech tag. Every frame is shown in a distinct color; ea"
W14-3007,P13-1026,0,0.0204767,"ften capture more semantic information than surface-form features could be exploited in various aspects of the frame-semantic parsing task. Applications Shallow semantic analysis based on FrameNet data has been recently utilized across various natural language processing applications with success. These include the generation of meeting summaries (Kleinbauer, 2012), the prediction of stock price movement using (Xie et al., 2013), inducing slots for domain-specific dialog systems (Chen et al., 2013), stance classification in debates (Hasan and Ng, 2013), modeling the clarity of student essays (Persing and Ng, 2013) to name a few. There is strong potential in using framesemantic structures in other applications such as question answering and machine translation, as demonstrated by prior work using PropBank-style SRL annotations (Shen and Lapata, 2007; Liu and Gildea, 2010). 4 Future Directions Acknowledgments Given the wide body of work in frame-semantic analysis of text, and recent interest in using framesemantic parsers in NLP applications, the future directions of research look exciting. First and foremost, to improve the quality of automatic frame-semantic parsers, the coverage of the FrameNet lexico"
W14-3007,W09-1201,0,0.0433126,"Missing"
W14-3007,S10-1008,0,0.0228202,"to measure the performance of statistical methods because the distribution of annotated targets in the data varied significantly across documents and domains, making it difficult to build a learnable system for target identification. The aforementioned papers focused on the task of sentence-internal frame-semantic analysis. There have been some investigation of finding implicit arguments of frames that may be present in other parts of a document, outside the sentential context. Although there has not been extensive research on this topic, a shared task at SemEval 2010 focused on this problem (Ruppenhofer et al., 2010).2 Moreover, there has been significant effort Johansson and Nugues (2007) presented the system that resulted in the best F1 score on the SemEval 2007 task of collectively identifying frameevoking targets, a disambiguated frame for each target, and the set of role-labeled arguments for each frame. The system contained a set of rulebased heuristics to identify targets followed by a cascade of three learned models as mentioned in §1. Das et al. (2010) presented a tool called SEMAFOR,1 which improved upon this system with a similar framework for target identification, but only used two probabilis"
W14-3007,W13-3514,0,0.0146208,"e of data sparsity; distributed word representations, which often capture more semantic information than surface-form features could be exploited in various aspects of the frame-semantic parsing task. Applications Shallow semantic analysis based on FrameNet data has been recently utilized across various natural language processing applications with success. These include the generation of meeting summaries (Kleinbauer, 2012), the prediction of stock price movement using (Xie et al., 2013), inducing slots for domain-specific dialog systems (Chen et al., 2013), stance classification in debates (Hasan and Ng, 2013), modeling the clarity of student essays (Persing and Ng, 2013) to name a few. There is strong potential in using framesemantic structures in other applications such as question answering and machine translation, as demonstrated by prior work using PropBank-style SRL annotations (Shen and Lapata, 2007; Liu and Gildea, 2010). 4 Future Directions Acknowledgments Given the wide body of work in frame-semantic analysis of text, and recent interest in using framesemantic parsers in NLP applications, the future directions of research look exciting. First and foremost, to improve the quality of automa"
W14-3007,P14-1136,1,0.892125,"antic frames that are denoted in capital letters. Above each target is shown the corresponding lexical unit, which is a lemma appended by a coarse part-of-speech tag. Every frame is shown in a distinct color; each frame’s arguments are annotated with the same color, and are marked below the sentence, at different levels. For the C ARDINAL NUMBERS frame, “M” denotes the role Multiplier and “E” denotes the role Entity. SemEval’07 Data (automatic targets) FrameNet 1.5 Release (gold targets) P R F1 Johansson and Nugues (2007) Das et al. (2010) 51.59 58.08 35.44 38.76 42.01 46.49 Das et al. (2014) Hermann et al. (2014) 68.33 72.79 61.14 64.95 64.54 68.64 Table 1: We show the current state of the art on the frame-semantic parsing task. The first section shows results on the SemEval 2007 shared task. The best system in the task, presented by Johansson and Nugues (2007) was later outperformed by SEMAFOR, a system described by Das et al. (2010). Both systems use a rule-based module to identify targets. On the FrameNet 1.5 data, Das et al. (2014) presented additional semi-supervised experiments using gold targets, which was recently outperformed by an approach presented by Hermann et al. (2014) that made use of"
W14-3007,N06-2015,0,0.0252251,"en the wide body of work in frame-semantic analysis of text, and recent interest in using framesemantic parsers in NLP applications, the future directions of research look exciting. First and foremost, to improve the quality of automatic frame-semantic parsers, the coverage of the FrameNet lexicon on free English text, and the number of annotated targets needs to increase. For example, the training dataset used for the state-ofthe-art system of Hermann et al. (2014) contains only 4,458 labeled targets, which is approximately 40 times less than the number of annotated targets in Ontonotes 4.0 (Hovy et al., 2006), a standard NLP dataset, containing PropBank-style verb annotations. This comparison is important because FrameNet covers many more syntactic categories than the PropBank-style annotations, and features more than 1,000 semantic role labels compared to 51 in Ontonotes, but severely lacks annotations. A machine learned system would find it very hard to generalize to new data given such data sparsity. Increasing the quantity of such annotations requires exhaustive inter-annotator agreement studies (which has been rare in FrameNet corpora generation) and the development of annotation guideThe aut"
W14-3007,D07-1002,0,0.082798,"ious natural language processing applications with success. These include the generation of meeting summaries (Kleinbauer, 2012), the prediction of stock price movement using (Xie et al., 2013), inducing slots for domain-specific dialog systems (Chen et al., 2013), stance classification in debates (Hasan and Ng, 2013), modeling the clarity of student essays (Persing and Ng, 2013) to name a few. There is strong potential in using framesemantic structures in other applications such as question answering and machine translation, as demonstrated by prior work using PropBank-style SRL annotations (Shen and Lapata, 2007; Liu and Gildea, 2010). 4 Future Directions Acknowledgments Given the wide body of work in frame-semantic analysis of text, and recent interest in using framesemantic parsers in NLP applications, the future directions of research look exciting. First and foremost, to improve the quality of automatic frame-semantic parsers, the coverage of the FrameNet lexicon on free English text, and the number of annotated targets needs to increase. For example, the training dataset used for the state-ofthe-art system of Hermann et al. (2014) contains only 4,458 labeled targets, which is approximately 40 ti"
W14-3007,S07-1048,0,0.72855,"iner, conjunction and preposition syntactic categories to serve as targets and evoke frames, unlike any other single dataset; it also allowed targets from different syntactic categories share frames, and therefore roles. The repository of semantic role types was also much richer than PropBankstyle lexicons, numbering in several hundreds. Most systems participating in the task resorted to a cascade of classifiers and rule-based modules: identifying targets (a non-trivial subtask), disambiguating frames, identifying potential arguments, and then labeling them with roles. The system described by Johansson and Nugues (2007) performed the best in this shared task. Next, we focus on its performance, and subsequent improvements made by the research community on this task. We present a brief history and overview of statistical methods in frame-semantic parsing – the automatic analysis of text using the theory of frame semantics. We discuss how the FrameNet lexicon and frameannotated datasets have been used by statistical NLP researchers to build usable, state-of-the-art systems. We also focus on future directions in frame-semantic parsing research, and discuss NLP applications that could benefit from this line of wo"
W14-3007,P10-2018,0,0.0161014,"ls for Frame-Semantic Parsing Dipanjan Das Google Inc. 76 9th Avenue, New York, NY 10011 dipanjand@google.com Abstract “sense” frame, which is not shared across targets, making the frame disambiguation setup different from the representation in FrameNet. Furthermore, it is fair to say that early research on PropBank focused primarily on argument structure prediction, and the interaction between frame and argument structure analysis has mostly been unaddressed (M`arquez et al., 2008). There are exceptions, where the verb frame has been taken into account during SRL (Meza-Ruiz and Riedel, 2009; Watanabe et al., 2010). Moreoever, the CoNLL 2008 and 2009 shared tasks also include the verb and noun frame identification task in their evaluations, although the overall goal was to predict semantic dependencies based on PropBank, and not full argument spans (Surdeanu et al., 2008; Hajiˇc et al., 2009). The SemEval 2007 shared task (Baker et al., 2007) attempted to revisit the frame-semantic analysis task based on FrameNet. It introduced a larger FrameNet lexicon (version 1.3), and also a larger corpus with full-text annotations compared to prior work, with multiple targets annotated per sentence. The corpus allo"
W14-3007,C10-1081,0,0.0705024,"rocessing applications with success. These include the generation of meeting summaries (Kleinbauer, 2012), the prediction of stock price movement using (Xie et al., 2013), inducing slots for domain-specific dialog systems (Chen et al., 2013), stance classification in debates (Hasan and Ng, 2013), modeling the clarity of student essays (Persing and Ng, 2013) to name a few. There is strong potential in using framesemantic structures in other applications such as question answering and machine translation, as demonstrated by prior work using PropBank-style SRL annotations (Shen and Lapata, 2007; Liu and Gildea, 2010). 4 Future Directions Acknowledgments Given the wide body of work in frame-semantic analysis of text, and recent interest in using framesemantic parsers in NLP applications, the future directions of research look exciting. First and foremost, to improve the quality of automatic frame-semantic parsers, the coverage of the FrameNet lexicon on free English text, and the number of annotated targets needs to increase. For example, the training dataset used for the state-ofthe-art system of Hermann et al. (2014) contains only 4,458 labeled targets, which is approximately 40 times less than the numbe"
W14-3007,P13-1086,0,0.0211643,"iscovered automatically. Finally, the FrameNet data is an attractive test bed for semi-supervised learning techniques because of data sparsity; distributed word representations, which often capture more semantic information than surface-form features could be exploited in various aspects of the frame-semantic parsing task. Applications Shallow semantic analysis based on FrameNet data has been recently utilized across various natural language processing applications with success. These include the generation of meeting summaries (Kleinbauer, 2012), the prediction of stock price movement using (Xie et al., 2013), inducing slots for domain-specific dialog systems (Chen et al., 2013), stance classification in debates (Hasan and Ng, 2013), modeling the clarity of student essays (Persing and Ng, 2013) to name a few. There is strong potential in using framesemantic structures in other applications such as question answering and machine translation, as demonstrated by prior work using PropBank-style SRL annotations (Shen and Lapata, 2007; Liu and Gildea, 2010). 4 Future Directions Acknowledgments Given the wide body of work in frame-semantic analysis of text, and recent interest in using framesemantic pars"
W14-3007,J08-2001,0,0.0670722,"Missing"
W14-3007,N10-1138,1,\N,Missing
W14-3007,W08-2121,0,\N,Missing
W14-3007,S07-1018,0,\N,Missing
W14-3007,P98-1013,0,\N,Missing
W14-3007,C98-1013,0,\N,Missing
W17-4121,D15-1075,0,0.0217277,"ting et al., 2016, inter alia) who aim for general purpose character n-gram embeddings. In contrast, we pretrain all model parameters on automatic but in-domain paraphrase data. We employ the same neural architecture as our end task, similar to prior work on multi-task learning (Søgaard and Goldberg, 2016, inter alia), but use a simpler learning setup. 3 Approach Our starting point is the decomposable attention model (Parikh et al., 2016, D EC ATT henceforth), which despite its simplicity and efficiency has been shown to work remarkably well for the related task of natural language inference (Bowman et al., 2015). We extend this model with character n-gram embeddings and noisy pretraining for the task of question paraphrase identification. 3.1 3.2 The D EC ATT model divides the prediction into three steps: Attend, Compare and Aggregate. Due to lack of space, we only provide a brief outline below and refer to Parikh et al. (2016) for further details on each of these steps. ¯ are aligned ¯ and b Attend. First, the elements of a using a variant of neural attention (Bahdanau et al., 2015) to decompose the problem into the comparison of aligned phrases. eij := F (¯ ai )> F (¯bj ) . (1) The function F is a"
W17-4121,N10-1066,0,0.0125583,"l architectures, all using pretrained word embeddings. Second, to significantly improve our model performance, we pretrain all our model parameters on the noisy, automatically collected question-paraphrase corpus Paralex (Fader et al., 2013), followed by fine-tuning the parameters on the Quora dataset. This two-stage training procedure achieves the best result on the Quora dataset to date, and is also significantly better than learning only the character n-gram embeddings during the pretraining stage. 2 Related Work Paraphrase identification is a well-studied task in NLP (Das and Smith, 2009; Chang et al., 2010; He et al., 2015; Wang et al., 2016, inter alia). Here, we focus on an instance, that of finding questions with identical meaning. Lei et al. (2016) consider a related task leveraging the AskUbuntu corpus (dos Santos et al., 2015), but it contains two orders of magnitude less annotations, thus limiting the quality of any model. Most relevant to this work is that of Wang et al. (2017), who present the best results on the Quora dataset prior to this work. The bilateral multi-perspective matching model (B I MPM) of Wang et al. uses a character-based LSTM (Hochreiter and Schmidhuber, 1997) at its"
W17-4121,D16-1053,0,0.00539161,"Missing"
W17-4121,P16-1160,0,0.0203045,"works to (self-)attend, compare and predict, leading to a more efficient architecture. B I MPM falls short of our best performing model pretrained on noisy paraphrase data and uses more parameters than our best model. Character-level modeling of text is a popular approach. While conceptually simple, character n-gram embeddings are a highly competitive representation (Huang et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016). More complex representations built directly from individual characters have also been proposed (Sennrich et al., 2016; Luong and Manning, 2016; Kim et al., 2016; Chung et al., 2016; Ling et al., 2015). These representations are robust to out-of-vocabulary items, often producing improved results. Our pretraining procedure is reminiscent of several recent papers (Wieting et al., 2016, inter alia) who aim for general purpose character n-gram embeddings. In contrast, we pretrain all model parameters on automatic but in-domain paraphrase data. We employ the same neural architecture as our end task, similar to prior work on multi-task learning (Søgaard and Goldberg, 2016, inter alia), but use a simpler learning setup. 3 Approach Our starting point is the decomposable attentio"
W17-4121,P09-1053,1,0.27673,"several complex neural architectures, all using pretrained word embeddings. Second, to significantly improve our model performance, we pretrain all our model parameters on the noisy, automatically collected question-paraphrase corpus Paralex (Fader et al., 2013), followed by fine-tuning the parameters on the Quora dataset. This two-stage training procedure achieves the best result on the Quora dataset to date, and is also significantly better than learning only the character n-gram embeddings during the pretraining stage. 2 Related Work Paraphrase identification is a well-studied task in NLP (Das and Smith, 2009; Chang et al., 2010; He et al., 2015; Wang et al., 2016, inter alia). Here, we focus on an instance, that of finding questions with identical meaning. Lei et al. (2016) consider a related task leveraging the AskUbuntu corpus (dos Santos et al., 2015), but it contains two orders of magnitude less annotations, thus limiting the quality of any model. Most relevant to this work is that of Wang et al. (2017), who present the best results on the Quora dataset prior to this work. The bilateral multi-perspective matching model (B I MPM) of Wang et al. uses a character-based LSTM (Hochreiter and Schmi"
W17-4121,P15-2114,0,0.0126784,"t al., 2013), followed by fine-tuning the parameters on the Quora dataset. This two-stage training procedure achieves the best result on the Quora dataset to date, and is also significantly better than learning only the character n-gram embeddings during the pretraining stage. 2 Related Work Paraphrase identification is a well-studied task in NLP (Das and Smith, 2009; Chang et al., 2010; He et al., 2015; Wang et al., 2016, inter alia). Here, we focus on an instance, that of finding questions with identical meaning. Lei et al. (2016) consider a related task leveraging the AskUbuntu corpus (dos Santos et al., 2015), but it contains two orders of magnitude less annotations, thus limiting the quality of any model. Most relevant to this work is that of Wang et al. (2017), who present the best results on the Quora dataset prior to this work. The bilateral multi-perspective matching model (B I MPM) of Wang et al. uses a character-based LSTM (Hochreiter and Schmidhuber, 1997) at its input representation layer, a layer of bi-LSTMs for computing context information, four different types of multi-perspective matching layers, an additional bi-LSTM aggregation layer, followed by a 142 Proceedings of the First Work"
W17-4121,P13-1158,0,0.0133858,"asks (Chen et al., 2016; Kim et al., 2017). We present two contributions. First, to mitigate data sparsity, we modify the input representation of the decomposable attention model to use sums of character n-gram embeddings instead of word embeddings. We show that this model trained on the Quora dataset produces comparable or better results with respect to several complex neural architectures, all using pretrained word embeddings. Second, to significantly improve our model performance, we pretrain all our model parameters on the noisy, automatically collected question-paraphrase corpus Paralex (Fader et al., 2013), followed by fine-tuning the parameters on the Quora dataset. This two-stage training procedure achieves the best result on the Quora dataset to date, and is also significantly better than learning only the character n-gram embeddings during the pretraining stage. 2 Related Work Paraphrase identification is a well-studied task in NLP (Das and Smith, 2009; Chang et al., 2010; He et al., 2015; Wang et al., 2016, inter alia). Here, we focus on an instance, that of finding questions with identical meaning. Lei et al. (2016) consider a related task leveraging the AskUbuntu corpus (dos Santos et al"
W17-4121,D15-1181,0,0.0421116,"using pretrained word embeddings. Second, to significantly improve our model performance, we pretrain all our model parameters on the noisy, automatically collected question-paraphrase corpus Paralex (Fader et al., 2013), followed by fine-tuning the parameters on the Quora dataset. This two-stage training procedure achieves the best result on the Quora dataset to date, and is also significantly better than learning only the character n-gram embeddings during the pretraining stage. 2 Related Work Paraphrase identification is a well-studied task in NLP (Das and Smith, 2009; Chang et al., 2010; He et al., 2015; Wang et al., 2016, inter alia). Here, we focus on an instance, that of finding questions with identical meaning. Lei et al. (2016) consider a related task leveraging the AskUbuntu corpus (dos Santos et al., 2015), but it contains two orders of magnitude less annotations, thus limiting the quality of any model. Most relevant to this work is that of Wang et al. (2017), who present the best results on the Quora dataset prior to this work. The bilateral multi-perspective matching model (B I MPM) of Wang et al. uses a character-based LSTM (Hochreiter and Schmidhuber, 1997) at its input representa"
W17-4121,P82-1020,0,0.571628,"Missing"
W17-4121,N16-1153,0,0.0215357,"Missing"
W17-4121,D15-1176,0,0.0144794,"nd, compare and predict, leading to a more efficient architecture. B I MPM falls short of our best performing model pretrained on noisy paraphrase data and uses more parameters than our best model. Character-level modeling of text is a popular approach. While conceptually simple, character n-gram embeddings are a highly competitive representation (Huang et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016). More complex representations built directly from individual characters have also been proposed (Sennrich et al., 2016; Luong and Manning, 2016; Kim et al., 2016; Chung et al., 2016; Ling et al., 2015). These representations are robust to out-of-vocabulary items, often producing improved results. Our pretraining procedure is reminiscent of several recent papers (Wieting et al., 2016, inter alia) who aim for general purpose character n-gram embeddings. In contrast, we pretrain all model parameters on automatic but in-domain paraphrase data. We employ the same neural architecture as our end task, similar to prior work on multi-task learning (Søgaard and Goldberg, 2016, inter alia), but use a simpler learning setup. 3 Approach Our starting point is the decomposable attention model (Parikh et a"
W17-4121,P16-1100,0,0.0240942,"tion model uses four simple feedforward networks to (self-)attend, compare and predict, leading to a more efficient architecture. B I MPM falls short of our best performing model pretrained on noisy paraphrase data and uses more parameters than our best model. Character-level modeling of text is a popular approach. While conceptually simple, character n-gram embeddings are a highly competitive representation (Huang et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016). More complex representations built directly from individual characters have also been proposed (Sennrich et al., 2016; Luong and Manning, 2016; Kim et al., 2016; Chung et al., 2016; Ling et al., 2015). These representations are robust to out-of-vocabulary items, often producing improved results. Our pretraining procedure is reminiscent of several recent papers (Wieting et al., 2016, inter alia) who aim for general purpose character n-gram embeddings. In contrast, we pretrain all model parameters on automatic but in-domain paraphrase data. We employ the same neural architecture as our end task, similar to prior work on multi-task learning (Søgaard and Goldberg, 2016, inter alia), but use a simpler learning setup. 3 Approach Our start"
W17-4121,D16-1244,1,0.334898,"Missing"
W17-4121,D14-1162,0,0.0962242,"Missing"
W17-4121,P16-1162,0,0.0152455,"the decomposable attention model uses four simple feedforward networks to (self-)attend, compare and predict, leading to a more efficient architecture. B I MPM falls short of our best performing model pretrained on noisy paraphrase data and uses more parameters than our best model. Character-level modeling of text is a popular approach. While conceptually simple, character n-gram embeddings are a highly competitive representation (Huang et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016). More complex representations built directly from individual characters have also been proposed (Sennrich et al., 2016; Luong and Manning, 2016; Kim et al., 2016; Chung et al., 2016; Ling et al., 2015). These representations are robust to out-of-vocabulary items, often producing improved results. Our pretraining procedure is reminiscent of several recent papers (Wieting et al., 2016, inter alia) who aim for general purpose character n-gram embeddings. In contrast, we pretrain all model parameters on automatic but in-domain paraphrase data. We employ the same neural architecture as our end task, similar to prior work on multi-task learning (Søgaard and Goldberg, 2016, inter alia), but use a simpler learning se"
W17-4121,P16-2038,0,0.021594,"m individual characters have also been proposed (Sennrich et al., 2016; Luong and Manning, 2016; Kim et al., 2016; Chung et al., 2016; Ling et al., 2015). These representations are robust to out-of-vocabulary items, often producing improved results. Our pretraining procedure is reminiscent of several recent papers (Wieting et al., 2016, inter alia) who aim for general purpose character n-gram embeddings. In contrast, we pretrain all model parameters on automatic but in-domain paraphrase data. We employ the same neural architecture as our end task, similar to prior work on multi-task learning (Søgaard and Goldberg, 2016, inter alia), but use a simpler learning setup. 3 Approach Our starting point is the decomposable attention model (Parikh et al., 2016, D EC ATT henceforth), which despite its simplicity and efficiency has been shown to work remarkably well for the related task of natural language inference (Bowman et al., 2015). We extend this model with character n-gram embeddings and noisy pretraining for the task of question paraphrase identification. 3.1 3.2 The D EC ATT model divides the prediction into three steps: Attend, Compare and Aggregate. Due to lack of space, we only provide a brief outline bel"
W17-4121,C16-1127,0,0.0165089,"word embeddings. Second, to significantly improve our model performance, we pretrain all our model parameters on the noisy, automatically collected question-paraphrase corpus Paralex (Fader et al., 2013), followed by fine-tuning the parameters on the Quora dataset. This two-stage training procedure achieves the best result on the Quora dataset to date, and is also significantly better than learning only the character n-gram embeddings during the pretraining stage. 2 Related Work Paraphrase identification is a well-studied task in NLP (Das and Smith, 2009; Chang et al., 2010; He et al., 2015; Wang et al., 2016, inter alia). Here, we focus on an instance, that of finding questions with identical meaning. Lei et al. (2016) consider a related task leveraging the AskUbuntu corpus (dos Santos et al., 2015), but it contains two orders of magnitude less annotations, thus limiting the quality of any model. Most relevant to this work is that of Wang et al. (2017), who present the best results on the Quora dataset prior to this work. The bilateral multi-perspective matching model (B I MPM) of Wang et al. uses a character-based LSTM (Hochreiter and Schmidhuber, 1997) at its input representation layer, a layer"
W17-4121,D16-1157,0,0.00735478,"penhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics. two-layer feedforward network for prediction. In contrast, the decomposable attention model uses four simple feedforward networks to (self-)attend, compare and predict, leading to a more efficient architecture. B I MPM falls short of our best performing model pretrained on noisy paraphrase data and uses more parameters than our best model. Character-level modeling of text is a popular approach. While conceptually simple, character n-gram embeddings are a highly competitive representation (Huang et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016). More complex representations built directly from individual characters have also been proposed (Sennrich et al., 2016; Luong and Manning, 2016; Kim et al., 2016; Chung et al., 2016; Ling et al., 2015). These representations are robust to out-of-vocabulary items, often producing improved results. Our pretraining procedure is reminiscent of several recent papers (Wieting et al., 2016, inter alia) who aim for general purpose character n-gram embeddings. In contrast, we pretrain all model parameters on automatic but in-domain paraphrase data. We employ the same neural a"
