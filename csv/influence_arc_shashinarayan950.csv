2020.acl-main.173,P19-1483,0,0.41514,"ata-to-text generation (Lebret et al., 2016; Wiseman et al., 2017) which are not open-ended, require models to be factual and/or faithful to the source text. Despite recent improvements in conditional text generation, most summarization systems are trained to maximize the log-likelihood of the reference summary at the word-level, which does not necessarily reward models for being faithful. Moreover, models are usually agnostic to the noises or artifacts of the training data, such as reference divergence, making them vulnerable to hallucinations (Kryscinski et al., 2019a; Wiseman et al., 2017; Dhingra et al., 2019). Thus, models can generate texts that are not consistent with the input, yet would likely have reasonable model log-likelihood. 2.1 Intrinsic and Extrinsic Hallucinations Given a document D and its abstractive summary S, we try to identify all hallucinations in S with respect to the content of D, regardless of the quality of the summary. In this work, we define a summary as being hallucinated if it has a span(s) wi . . . wi+j , j ≥ i, that is not supported by the input document. To distinguish hallucinations further in the context of a document and a summary, we categorize hallucinations by t"
2020.acl-main.173,P16-1154,0,0.0367126,"ot only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.1 1 Introduction Current state of the art conditional text generation models accomplish a high level of fluency and coherence, mostly thanks to advances in sequenceto-sequence architectures with attention and copy (Sutskever et al., 2014; Bahdanau et al., 2015; Gu et al., 2016), fully attention-based Transformer architectures (Vaswani et al., 2017; Dai et al., 2019) and more recently pretrained language modeling for natural language understanding (Devlin et al., 2019; Radford et al., 2018; Yang et al., 2019; Liu et al., 2019). There has been a growing interest in ∗ The first two authors contributed equally. Our human annotated summaries for faithfulness and factuality will be released at https://github.com/google-researchdatasets/xsum hallucination annotations. 1 understanding how maximum likelihood training and approximate beam-search decoding in these models lead"
2020.acl-main.173,2021.ccl-1.108,0,0.226531,"Missing"
2020.acl-main.173,W01-0100,0,0.502282,"tions. 1 understanding how maximum likelihood training and approximate beam-search decoding in these models lead to less human-like text in open-ended text generation such as language modeling and story generation (Holtzman et al., 2020; Welleck et al., 2020; See et al., 2019). In this paper we investigate how these models are prone to generate hallucinated text in conditional text generation, specifically, extreme abstractive document summarization (Narayan et al., 2018a). Document summarization — the task of producing a shorter version of a document while preserving its information content (Mani, 2001; Nenkova and McKeown, 2011) — requires models to generate text that is not only human-like but also faithful and/or factual given the document. The example in Figure 1 illustrates that the faithfulness and factuality are yet to be conquered by conditional text generators. The article describes an event of “Conservative MP Zac Smith winning the primary for 2016 London mayoral election”, but summaries often forge entities (e.g., “Nigel Goldsmith” or “Zac Goldwin”) or information (e.g., “UKIP leader Nigel Goldsmith”, “Nigel Goldsmith winning the mayoral election”, “Sadiq Khan being the former Lo"
2020.acl-main.173,D18-1206,1,0.740796,"uman annotated summaries for faithfulness and factuality will be released at https://github.com/google-researchdatasets/xsum hallucination annotations. 1 understanding how maximum likelihood training and approximate beam-search decoding in these models lead to less human-like text in open-ended text generation such as language modeling and story generation (Holtzman et al., 2020; Welleck et al., 2020; See et al., 2019). In this paper we investigate how these models are prone to generate hallucinated text in conditional text generation, specifically, extreme abstractive document summarization (Narayan et al., 2018a). Document summarization — the task of producing a shorter version of a document while preserving its information content (Mani, 2001; Nenkova and McKeown, 2011) — requires models to generate text that is not only human-like but also faithful and/or factual given the document. The example in Figure 1 illustrates that the faithfulness and factuality are yet to be conquered by conditional text generators. The article describes an event of “Conservative MP Zac Smith winning the primary for 2016 London mayoral election”, but summaries often forge entities (e.g., “Nigel Goldsmith” or “Zac Goldwin"
2020.acl-main.173,N18-1158,1,0.86231,"uman annotated summaries for faithfulness and factuality will be released at https://github.com/google-researchdatasets/xsum hallucination annotations. 1 understanding how maximum likelihood training and approximate beam-search decoding in these models lead to less human-like text in open-ended text generation such as language modeling and story generation (Holtzman et al., 2020; Welleck et al., 2020; See et al., 2019). In this paper we investigate how these models are prone to generate hallucinated text in conditional text generation, specifically, extreme abstractive document summarization (Narayan et al., 2018a). Document summarization — the task of producing a shorter version of a document while preserving its information content (Mani, 2001; Nenkova and McKeown, 2011) — requires models to generate text that is not only human-like but also faithful and/or factual given the document. The example in Figure 1 illustrates that the faithfulness and factuality are yet to be conquered by conditional text generators. The article describes an event of “Conservative MP Zac Smith winning the primary for 2016 London mayoral election”, but summaries often forge entities (e.g., “Nigel Goldsmith” or “Zac Goldwin"
2020.acl-main.173,N04-1019,0,0.253075,"quality of summaries (Nenkova, 2005). Most popular among them is the automatic metric ROUGE (Lin and Hovy, 2003) that measures the unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a proxy for assessing informativeness and the longest common subsequence (ROUGE-L), for fluency. ROUGE, however, can be misleading when used as the only means to assess the informativeness of summaries (Schluter, 2017). Hence, the ROUGE score is often complemented with subjective human assessment of summaries. More objective measures have been proposed to improve agreement among human annotators. Pyramid method (Nenkova and Passonneau, 2004) requires summaries to be annotated by experts for salient information. Narayan et al. (2018a,b) used a questionanswering based approach where a summary is used as context to answer questions which were written based on its reference summary. Hardy et al. (2019) proposed a reference-less approach where a summary is assessed against the source document, highlighted with its pertinent content. There has not been much work on evaluating faithfulness and truthfulness of abstractive summaries. The automatic evaluation such as ROUGE and the human evaluation of saliency and linguistic quality of summ"
2020.acl-main.173,P19-1459,0,0.0611913,"Missing"
2020.acl-main.173,N18-2102,0,0.222406,"ly sampled 500 articles from the test set to facilitate our study. Using the full test set was unfeasible given its size and the cost of human judgments. We have trained annotators (fluent in English) specifically for our assessment. Our annotators went through two pilot studies to have a better understanding of intrinsic and extrinsic hallucinations, and factuality of summaries. Documents used in the pilot studies were not used in the final annotation. We also report on ROUGE (Lin and Hovy, 2003) scores, BERTScore (Zhang et al., 2020) and semantic inference metric such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019; Falke et al., 2019; Kryscinski et al., 2019b) and question answering (Arumae and Liu, 2019; Wang et al., 2020). 5.1 Automatic Evaluation of Summaries ROUGE (Lin and Hovy, 2003) provides a means to quickly assess a model’s ability to generate summaries closer to human authored summaries. We report on ROUGE-1 and ROUGE-2 for informativeness and ROUGE-L, for fluency. Like ROUGE, BERTScore (Zhang et al., 2020) computes a similarity score for each token in the candidate summary with each token in the reference summary. However, instead of exact matches, it computes token sim"
2020.acl-main.173,2020.tacl-1.18,1,0.943458,"tion (e.g., “UKIP leader Nigel Goldsmith”, “Nigel Goldsmith winning the mayoral election”, “Sadiq Khan being the former London mayor” or “Zac Goldwin being the Labour’s candidate”) that are not supported by the document or are factually wrong. Interestingly, all summaries are topical and fluent, and perform well in terms of ROUGE scores (Lin and Hovy, 2003). We conducted a large-scale human evaluation of hallucinated content in systems that use Recurrent Neural Network (RNN) (See et al., 2017), Convolutional Neural Network (CNN) (Narayan et al., 2018a), and Transformers (Radford et al., 2019; Rothe et al., 2020), as well as human written summaries for the recently introduced eXtreme S UMmarization task (XS UM, Narayan et al., 2018a). We seek to answer the following questions: (i) How frequently do abstractive summarizers hallucinate content?; (ii) Do models hal1906 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919 c July 5 - 10, 2020. 2020 Association for Computational Linguistics G OLD Zac Goldsmith will contest the 2016 London mayoral election for the Conservatives, it has been announced. D OCUMENT: The Richmond Park and North Kingston MP said"
2020.acl-main.173,E17-2007,0,0.0221606,"e a good balance between ROUGE and better faithfulness. 6 Related Work Following the Document Understanding Conference (DUC; Dang, 2005), a majority of work has focused on evaluating the content and the linguistic quality of summaries (Nenkova, 2005). Most popular among them is the automatic metric ROUGE (Lin and Hovy, 2003) that measures the unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a proxy for assessing informativeness and the longest common subsequence (ROUGE-L), for fluency. ROUGE, however, can be misleading when used as the only means to assess the informativeness of summaries (Schluter, 2017). Hence, the ROUGE score is often complemented with subjective human assessment of summaries. More objective measures have been proposed to improve agreement among human annotators. Pyramid method (Nenkova and Passonneau, 2004) requires summaries to be annotated by experts for salient information. Narayan et al. (2018a,b) used a questionanswering based approach where a summary is used as context to answer questions which were written based on its reference summary. Hardy et al. (2019) proposed a reference-less approach where a summary is assessed against the source document, highlighted with i"
2020.acl-main.173,P17-1099,0,0.649396,"2016 London mayoral election”, but summaries often forge entities (e.g., “Nigel Goldsmith” or “Zac Goldwin”) or information (e.g., “UKIP leader Nigel Goldsmith”, “Nigel Goldsmith winning the mayoral election”, “Sadiq Khan being the former London mayor” or “Zac Goldwin being the Labour’s candidate”) that are not supported by the document or are factually wrong. Interestingly, all summaries are topical and fluent, and perform well in terms of ROUGE scores (Lin and Hovy, 2003). We conducted a large-scale human evaluation of hallucinated content in systems that use Recurrent Neural Network (RNN) (See et al., 2017), Convolutional Neural Network (CNN) (Narayan et al., 2018a), and Transformers (Radford et al., 2019; Rothe et al., 2020), as well as human written summaries for the recently introduced eXtreme S UMmarization task (XS UM, Narayan et al., 2018a). We seek to answer the following questions: (i) How frequently do abstractive summarizers hallucinate content?; (ii) Do models hal1906 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919 c July 5 - 10, 2020. 2020 Association for Computational Linguistics G OLD Zac Goldsmith will contest the 2016 Londo"
2020.acl-main.173,K19-1079,0,0.10635,"al language understanding (Devlin et al., 2019; Radford et al., 2018; Yang et al., 2019; Liu et al., 2019). There has been a growing interest in ∗ The first two authors contributed equally. Our human annotated summaries for faithfulness and factuality will be released at https://github.com/google-researchdatasets/xsum hallucination annotations. 1 understanding how maximum likelihood training and approximate beam-search decoding in these models lead to less human-like text in open-ended text generation such as language modeling and story generation (Holtzman et al., 2020; Welleck et al., 2020; See et al., 2019). In this paper we investigate how these models are prone to generate hallucinated text in conditional text generation, specifically, extreme abstractive document summarization (Narayan et al., 2018a). Document summarization — the task of producing a shorter version of a document while preserving its information content (Mani, 2001; Nenkova and McKeown, 2011) — requires models to generate text that is not only human-like but also faithful and/or factual given the document. The example in Figure 1 illustrates that the faithfulness and factuality are yet to be conquered by conditional text gener"
2020.acl-main.173,P19-1363,0,0.0636683,"m the test set to facilitate our study. Using the full test set was unfeasible given its size and the cost of human judgments. We have trained annotators (fluent in English) specifically for our assessment. Our annotators went through two pilot studies to have a better understanding of intrinsic and extrinsic hallucinations, and factuality of summaries. Documents used in the pilot studies were not used in the final annotation. We also report on ROUGE (Lin and Hovy, 2003) scores, BERTScore (Zhang et al., 2020) and semantic inference metric such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019; Falke et al., 2019; Kryscinski et al., 2019b) and question answering (Arumae and Liu, 2019; Wang et al., 2020). 5.1 Automatic Evaluation of Summaries ROUGE (Lin and Hovy, 2003) provides a means to quickly assess a model’s ability to generate summaries closer to human authored summaries. We report on ROUGE-1 and ROUGE-2 for informativeness and ROUGE-L, for fluency. Like ROUGE, BERTScore (Zhang et al., 2020) computes a similarity score for each token in the candidate summary with each token in the reference summary. However, instead of exact matches, it computes token similarity using contextu"
2020.acl-main.173,N18-1101,0,0.0295043,"D. For factuality, the differences between P T G EN, TC ONV S2S, and T RAN S2S were insignificant. 5.4 Automatic Measures for Hallucinations Summaries are a proxy for their source documents under the assumption that they highlight the most important content. With this assumption, we further studied the extent to which the hallucinated content can be measured by semantic inference related measures, such as textual entailment and question answering. Textual Entailment. We trained an entailment classifier by finetuning a BERT-Large pretrained model (Devlin et al., 2019) on the Multi-NLI dataset (Williams et al., 2018). We calculated the entailment probability score between the document and its abstractive summaries. Note that this entailment classifier is not optimal for the BBC article-summary pairs; the Multi-NLI dataset contains sentence-sentence pairs. Ideally a summary should entail the document or perhaps be neutral to the document, but never contradict the document. As can be seen in Table 3, the B ERT S2S abstracts showed the least number of 5 See Appendix for full results. Models P T G EN TC ONV S2S T RAN S2S B ERT S2S G OLD Textual Entailment entail. neut. cont. 38.4 34.4 27.2 29.6 37.4 33.0 34.6"
2020.acl-main.173,2020.acl-main.450,0,0.364319,"n judgments. We have trained annotators (fluent in English) specifically for our assessment. Our annotators went through two pilot studies to have a better understanding of intrinsic and extrinsic hallucinations, and factuality of summaries. Documents used in the pilot studies were not used in the final annotation. We also report on ROUGE (Lin and Hovy, 2003) scores, BERTScore (Zhang et al., 2020) and semantic inference metric such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019; Falke et al., 2019; Kryscinski et al., 2019b) and question answering (Arumae and Liu, 2019; Wang et al., 2020). 5.1 Automatic Evaluation of Summaries ROUGE (Lin and Hovy, 2003) provides a means to quickly assess a model’s ability to generate summaries closer to human authored summaries. We report on ROUGE-1 and ROUGE-2 for informativeness and ROUGE-L, for fluency. Like ROUGE, BERTScore (Zhang et al., 2020) computes a similarity score for each token in the candidate summary with each token in the reference summary. However, instead of exact matches, it computes token similarity using contextual embeddings. Results are presented in Table 1. For both cases, the pretrained encoder-decoder architecture B E"
2020.acl-main.173,N03-1020,0,\N,Missing
2020.acl-main.173,D17-1239,0,\N,Missing
2020.acl-main.173,D18-2012,0,\N,Missing
2020.acl-main.173,D18-1443,0,\N,Missing
2020.acl-main.173,Q19-1026,0,\N,Missing
2020.acl-main.173,N19-1264,0,\N,Missing
2020.acl-main.173,P19-1330,1,\N,Missing
2020.acl-main.173,P19-1620,0,\N,Missing
2020.acl-main.173,P19-1213,0,\N,Missing
2020.acl-main.173,N19-1423,0,\N,Missing
2020.acl-main.173,P19-1285,0,\N,Missing
2020.emnlp-main.339,2020.emnlp-main.19,0,0.168895,"is paper, we propose encoder-centric stepwise models for extractive summarization using 4143 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4143–4159, c November 16–20, 2020. 2020 Association for Computational Linguistics structured transformers. Structured transformers are transformer-based architectures that have the flexibility to model some form of structure of the input, e.g., hierarchical document structure. In this paper, we specifically study two such architectures – HiBERT (Zhang et al., 2019) and Extended Transformers Construction (ETC; Ainslie et al., 2020). Details of these are given in Sections 4 and 5. We enable stepwise summarization by injecting the previously planned summary content into the structured transformer as an auxiliary sub-structure. The model then can holistically learn any documentlevel coherence properties, such as saliency, redundancy, and ordering, embodied in the gold summaries. This differs from other methods which are either task specific (e.g., redundancy aware modeling in Bi et al., 2020) or not holistic (e.g., manually curated features in Liu et al., 2019a). An added advantage of structured encoders is that they break"
2020.emnlp-main.339,P16-1046,0,0.370803,"r content selection, planning and ordering, highlighting the strength of stepwise modeling. Amongst the two structured transformers we test, stepwise Extended Transformers provides the best performance across both datasets and sets a new standard for these challenges.1 1 Introduction Extractive document summarization is the task of creating a summary by identifying (and subsequently concatenating) the most important sentences in a document (Erkan and Radev, 2004; Nenkova and McKeown, 2011). In recent years this task has matured significantly, mostly thanks to advances in deep neural networks. Cheng and Lapata (2016) conceptualize extractive summarization as a sequence labeling task in which first a hierarchical long short-term memory network (LSTM; ∗ Equal contribution. The code and data are available at https://github. com/google-research/google-research/ tree/master/etcsum. 1 Hochreiter and Schmidhuber, 1997) is used to encode a document and then another LSTM is used to predict for each sentence whether it should be included in the summary. This architecture was later adopted by Nallapati et al. (2016a), Nallapati et al. (2017), Narayan et al. (2018b), Zhang et al. (2018) and Dong et al. (2018). Follow"
2020.emnlp-main.339,P19-1500,0,0.105669,"ber, 1997) is used to encode a document and then another LSTM is used to predict for each sentence whether it should be included in the summary. This architecture was later adopted by Nallapati et al. (2016a), Nallapati et al. (2017), Narayan et al. (2018b), Zhang et al. (2018) and Dong et al. (2018). Following the success of pre-trained transformerbased architectures for many tasks (Vaswani et al., 2017; Devlin et al., 2019), the current state-of-theart approach to extractive summarization uses transformers to learn sentence representations and to rank sentences by their saliency (Liu, 2019; Liu and Lapata, 2019b; Zhang et al., 2019; Zhong et al., 2019a; Bi et al., 2020). The top scoring sentences are then assembled to produce an extract of the document. Summaries built in this fashion (Cheng and Lapata, 2016; Narayan et al., 2018a; Zhang et al., 2018; Dong et al., 2018) are prone to contain redundant information. Several recent approaches have explored mechanisms to better handle redundancy, such as heuristic-based Trigram Blocking (TriBlk; Liu and Lapata, 2019b; Wang et al., 2020), handcrafted feature-driven models (Ren et al., 2017) and redundancy aware neural sequence models (Zhou et al., 2018; B"
2020.emnlp-main.339,D19-1387,0,0.12856,"ber, 1997) is used to encode a document and then another LSTM is used to predict for each sentence whether it should be included in the summary. This architecture was later adopted by Nallapati et al. (2016a), Nallapati et al. (2017), Narayan et al. (2018b), Zhang et al. (2018) and Dong et al. (2018). Following the success of pre-trained transformerbased architectures for many tasks (Vaswani et al., 2017; Devlin et al., 2019), the current state-of-theart approach to extractive summarization uses transformers to learn sentence representations and to rank sentences by their saliency (Liu, 2019; Liu and Lapata, 2019b; Zhang et al., 2019; Zhong et al., 2019a; Bi et al., 2020). The top scoring sentences are then assembled to produce an extract of the document. Summaries built in this fashion (Cheng and Lapata, 2016; Narayan et al., 2018a; Zhang et al., 2018; Dong et al., 2018) are prone to contain redundant information. Several recent approaches have explored mechanisms to better handle redundancy, such as heuristic-based Trigram Blocking (TriBlk; Liu and Lapata, 2019b; Wang et al., 2020), handcrafted feature-driven models (Ren et al., 2017) and redundancy aware neural sequence models (Zhou et al., 2018; B"
2020.emnlp-main.339,2021.ccl-1.108,0,0.0813545,"Missing"
2020.emnlp-main.339,N19-1397,1,0.942134,"cking (TriBlk; Liu and Lapata, 2019b; Wang et al., 2020), handcrafted feature-driven models (Ren et al., 2017) and redundancy aware neural sequence models (Zhou et al., 2018; Bi et al., 2020). One common problem with these models is that their focus is limited to content overlap and to respecting length budgets. However, these are but a small subset of the dimensions necessary to produce informative and coherent summaries. Ideally, models would utilize enriched document and summary representations in order to implicitly learn better extractive plans for producing summaries (Liu et al., 2019a; Mendes et al., 2019). One such method is stepwise summarization (Liu et al., 2019a), where a summary is constructed incrementally by choosing new content conditioned on previously planned content. In this paper, we propose encoder-centric stepwise models for extractive summarization using 4143 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4143–4159, c November 16–20, 2020. 2020 Association for Computational Linguistics structured transformers. Structured transformers are transformer-based architectures that have the flexibility to model some form of structure of the"
2020.emnlp-main.339,D19-5633,0,0.028386,"2017). Liu and Lapata (2019a) used cross-document attention mechanism to share information as opposed to simply concatenating text spans using hierarchical transformers. Similar to this motivation, we also explore better encoding of long inputs with structured transformers. Table-to-Text Content Planning. Wiseman et al. (2017) introduced the Rotowire dataset, which requires multi-sentence summaries of large tables. Several works found that the key to generate fluent and informative summaries for this task is to have dedicated content planning and realization steps (Puduppully et al., 2019a,c; Miculicich et al., 2019). Miculicich et al. (2019) and Gong et al. (2019b) used a transformer encoder, and, Gong et al. (2019a) used multi-dimensional hierarchical LSTM encoders to compute better table entry representations. Following these lines of work, we evaluate our models to generate long content plans for this task using structured transformers. 3 Problem: Stepwise Content Extraction We define a general paradigm for stepwise content extraction that can be easily tailored to both extractive summarization and table-to-text generation. Given an input D = {s1 , s2 , . . . , sn } with n content units, the goal is t"
2020.emnlp-main.339,D19-5630,0,0.35119,"learn and leverage content plans directly from the data. Moreover, structured transformers form the basis of our model, which are flexible in terms of content type (e.g., text or tables) that can be modeled. We demonstrate this by learning intricate extractive content plan for the Rotowire table-to-text generation task (Wiseman et al., 2017). This task requires the generation of long summaries from large score tables detailing the the specifics of a sports match, which often necessitates dedicated content selection and planning models to generate a high-quality summary (Wiseman et al., 2017; Puduppully et al., 2019a). We show that our stepwise framework achieves higher content selection, planning and ordering scores relative to prior work with task-specific planning mechanisms. The contributions of the paper are as follows: 1) this is first study to use ETC (Ainslie et al., 2020) for summarization for its ability and flexibility to better model long and structured inputs; 2) we propose augmentions of two structured transformers, HiBERT and ETC, in order to enable stepwise models for extractive planning; 3) we demonstrate empirically that our models are general purpose and can be adapted as an extractive"
2020.emnlp-main.339,2020.tacl-1.18,1,0.685354,"ly: content planning, which consists of selecting which records in the table should be mentioned in the summary, in what order, and how they should be organized into sentences; and realization, which uses the content plan to create a human-readable summary. We refer the reader to the supplementary material for an example. Our main focus in this paper is to demonstrate our models’ ability to model long and structured Rotowire input tables, and generate long meaningful content plans. For realization, we simply use a RoBERTa (Liu et al., 2019b) initialized sequence-to-sequence transformer model (Rothe et al., 2020), trained to emit the realization sentence by sentence. We train our stepwise models to take a score table and the partially generated content plan, and predict the next element in the content plan. This can be either one of the entries in the score table, a sentence break or a token marking the end of the plan. Unlike extractive summarization, here an optimal extractive content plan can have repeated entries from the input table (e.g. team names) to better preserve and generate discourse relations among sentences in the target summary (Puduppully et al., 2019b), making it a challenging task f"
2020.emnlp-main.339,2020.acl-main.553,0,0.0917732,"summarization uses transformers to learn sentence representations and to rank sentences by their saliency (Liu, 2019; Liu and Lapata, 2019b; Zhang et al., 2019; Zhong et al., 2019a; Bi et al., 2020). The top scoring sentences are then assembled to produce an extract of the document. Summaries built in this fashion (Cheng and Lapata, 2016; Narayan et al., 2018a; Zhang et al., 2018; Dong et al., 2018) are prone to contain redundant information. Several recent approaches have explored mechanisms to better handle redundancy, such as heuristic-based Trigram Blocking (TriBlk; Liu and Lapata, 2019b; Wang et al., 2020), handcrafted feature-driven models (Ren et al., 2017) and redundancy aware neural sequence models (Zhou et al., 2018; Bi et al., 2020). One common problem with these models is that their focus is limited to content overlap and to respecting length budgets. However, these are but a small subset of the dimensions necessary to produce informative and coherent summaries. Ideally, models would utilize enriched document and summary representations in order to implicitly learn better extractive plans for producing summaries (Liu et al., 2019a; Mendes et al., 2019). One such method is stepwise summar"
2020.emnlp-main.339,K17-1045,0,0.0197473,", our models are first to use summary representations with structured transformers for summarization. Our models learn to make summary-informed next-sentence predictions without any hand-curated features. 4144 Long-form Summarization. It is well known that a better content selection benefits abstractive summarizers to generate summaries that are not only fluent but also informative (Gehrmann et al., 2018; Hsu et al., 2018; Xiao et al., 2020). It can be particularly important when generating long abstractive summaries (Liu et al., 2018; Liu and Lapata, 2019a) or summarizing multiple documents (Yasunaga et al., 2017). Earlier multi-document summarization methods have addressed the issue of long form input by graph-based representations of sentences or passages (Erkan and Radev, 2004; Christensen et al., 2013). Recently, Yasunaga et al. (2017) proposed a neural version of this framework using graph convolutional networks (Kipf and Welling, 2017). Liu and Lapata (2019a) used cross-document attention mechanism to share information as opposed to simply concatenating text spans using hierarchical transformers. Similar to this motivation, we also explore better encoding of long inputs with structured transforme"
2020.emnlp-main.339,D18-1088,0,0.125588,"s in deep neural networks. Cheng and Lapata (2016) conceptualize extractive summarization as a sequence labeling task in which first a hierarchical long short-term memory network (LSTM; ∗ Equal contribution. The code and data are available at https://github. com/google-research/google-research/ tree/master/etcsum. 1 Hochreiter and Schmidhuber, 1997) is used to encode a document and then another LSTM is used to predict for each sentence whether it should be included in the summary. This architecture was later adopted by Nallapati et al. (2016a), Nallapati et al. (2017), Narayan et al. (2018b), Zhang et al. (2018) and Dong et al. (2018). Following the success of pre-trained transformerbased architectures for many tasks (Vaswani et al., 2017; Devlin et al., 2019), the current state-of-theart approach to extractive summarization uses transformers to learn sentence representations and to rank sentences by their saliency (Liu, 2019; Liu and Lapata, 2019b; Zhang et al., 2019; Zhong et al., 2019a; Bi et al., 2020). The top scoring sentences are then assembled to produce an extract of the document. Summaries built in this fashion (Cheng and Lapata, 2016; Narayan et al., 2018a; Zhang et al., 2018; Dong et al.,"
2020.tacl-1.18,P19-1285,0,0.0311841,"o evaluated on natural language inference tasks. In the extended version of GPT-2, the model was evaluated on more general natural language processing tasks, like machine translation, reading comprehension, summarization, and language modeling. GPT-2 achieved new stateof-the-art results on several language modeling datasets. On the other tasks, GPT-2 outperformed some unsupervised baselines but is still far behind supervised or task-specific approaches. After we performed the majority of our experiments, XLNet (Yang et al., 2019), an autoregressive pre-training method based on Transformer XL (Dai et al., 2019), was released. XLNet achieved new state-of-the-art results on several NLP tasks. We leave the experiments with their public checkpoint for future work. 8 Conclusion We performed an extensive study on leveraging pre-trained checkpoints for sequence generation. Our findings show that a pre-trained encoder is an essential part. Most tasks also profit from sharing the weights between the encoder and the decoder, which additionally decreases the memory footprint. While combing BERT and GPT-2 into a single model often underperformed a randomly initialized baseline, combining RoBERTa and GPT-2 achie"
2020.tacl-1.18,N19-1423,0,0.667274,"ce Generation. We developed a Transformer-based sequence-to-sequence model that is compatible with publicly available pre-trained BERT, GPT-2, and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model, both encoder and decoder, with these checkpoints. Our models result in new state-of-the-art results on Machine Translation, Text Summarization, Sentence Splitting, and Sentence Fusion. 1 Introduction Unsupervised and self-supervised pre-training methods, such as ELMo (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), and more recently BERT (Devlin et al., 2019), GPT and GPT-2 (Radford et al., 2018, 2019), XLNet (Yang et al., 2019), and RoBERTa (Liu et al., 2019) have established a qualitatively new level of baseline performance for many widely used Natural Language Understanding (NLU) benchmarks including some of the most popular, like GLUE (Williams et al., 2018) and SQuAD (Rajpurkar et al., 2018). The most appealing part about this massive shift towards using large architectures pre-trained on large collections of texts is that the pretrained checkpoints along with the inference code are made freely available. This saves hundreds of TPU/GPU hours,"
2020.tacl-1.18,P18-2114,0,0.0418067,"Missing"
2020.tacl-1.18,D18-1080,0,0.0180237,"ence generation models? For example, one could imagine using a BERT checkpoint to initialize the encoder for better input understanding and choosing GPT-2 model as the decoder for better text generation. One of the main contributions of this paper is that we rigorously experiment with a large number of different settings to combine BERT, GPT, and RoBERTa pre-trained checkpoints to initialize our Transformer-based model. We report results on three canonical conditional text generation tasks of increasing complexity: sentence-level fusion (DiscoFuse, Geva et al., 2019) and splitting (WikiSplit, Botha et al., 2018), Unsupervised pre-training of large neural models has recently revolutionized Natural Language Processing. By warm-starting from the publicly released checkpoints, NLP practitioners have pushed the state-of-the-art on multiple benchmarks while saving significant amounts of compute time. So far the focus has been mainly on the Natural Language Understanding tasks. In this paper, we demonstrate the efficacy of pre-trained checkpoints for Sequence Generation. We developed a Transformer-based sequence-to-sequence model that is compatible with publicly available pre-trained BERT, GPT-2, and RoBERT"
2020.tacl-1.18,D18-1045,0,0.0202717,"e top section of Table 4) and pre-train a BERT model on the English and German subset of the Wikipedia dump in the same way as the multilingual BERT checkpoint was obtained. We initialize our best-performing setups, BERT2RND and BERTSHARE, with this checkpoint (the third block of Table 4). This provides a further +0.5 (En ↔ De) and +0.8 (De ↔ En) BLEU improvements on newstest2014, and, +1.1 and +0.7 on newstest2016, yielding an overall very strong performance on the challenging WMT14 task. Experiments with the larger models (the last block) show further improvements of up to +1.1 BLEU points. Edunov et al. (2018) report better results when they augment the training set with a massive amount of back-translated sentence pairs. To the best of our knowledge, among the approaches that only leverage parallel data from WMT14, our results are state-of-the-art on both newstest2014 and newstest2016. 269 4.4 Abstractive Summarization Document summarization is the task of producing a short version of a document while preserving its salient information content. We evaluate our setups on three different summarization datasets of varying characteristics: Gigaword (Napoles et al., 2012), CNN and DailyMail (Hermann et"
2020.tacl-1.18,D18-1443,0,0.132027,"37.39 34.47 29.03 22.99 33.79 30.90 38.42 32.44 37.53 38.52 39.87 22.21 28.48 27.79 19.91 10.23 15.83 11.52 15.24 16.12 17.50 4.89 8.77 8.37 5.20 24.24 30.80 25.65 30.05 31.13 32.37 16.69 22.30 21.91 15.88 37.01 37.62 38.93 41.45 16.35 18.79 31.52 33.90 Table 5: Summarization results of different models and their initialization setups. We compare our setups (the bottom block) against both non-pre-trained (the top block) and pre-trained (the middle block) models on various datasets: the Lead baseline, PtGen (See et al., 2017), ConvS2S (Gehring et al., 2017), MMN (Kim et al., 2019), Bottom-Up (Gehrmann et al., 2018), MASS (Song et al., 2019), TransLM (Khandelwal et al., 2019), and UniLM (Dong et al., 2019). The Lead results for the CNN/DailyMail dataset is taken from Narayan et al. (2018b), whereas Lead, PtGen, and ConvS2S results on the BBC dataset are taken from Narayan et al. (2018a). Our best results are boldfaced and the best results on the datasets are italicized. BBC dataset and BERT2GPT to RND2GPT on the CNN/DailyMail dataset. However, this is not the case with the Gigaword dataset, which has 3.8M training instances; BERT2GPT and ROBERTA2GPT perform better than RND2GPT. ROBERTASHARE performs the"
2020.tacl-1.18,N19-1348,0,0.0294008,"pretrained checkpoints for warm-starting sequence generation models? For example, one could imagine using a BERT checkpoint to initialize the encoder for better input understanding and choosing GPT-2 model as the decoder for better text generation. One of the main contributions of this paper is that we rigorously experiment with a large number of different settings to combine BERT, GPT, and RoBERTa pre-trained checkpoints to initialize our Transformer-based model. We report results on three canonical conditional text generation tasks of increasing complexity: sentence-level fusion (DiscoFuse, Geva et al., 2019) and splitting (WikiSplit, Botha et al., 2018), Unsupervised pre-training of large neural models has recently revolutionized Natural Language Processing. By warm-starting from the publicly released checkpoints, NLP practitioners have pushed the state-of-the-art on multiple benchmarks while saving significant amounts of compute time. So far the focus has been mainly on the Natural Language Understanding tasks. In this paper, we demonstrate the efficacy of pre-trained checkpoints for Sequence Generation. We developed a Transformer-based sequence-to-sequence model that is compatible with publicly"
2020.tacl-1.18,P82-1020,0,0.805576,"Missing"
2020.tacl-1.18,P18-1031,0,0.14927,"he efficacy of pre-trained checkpoints for Sequence Generation. We developed a Transformer-based sequence-to-sequence model that is compatible with publicly available pre-trained BERT, GPT-2, and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model, both encoder and decoder, with these checkpoints. Our models result in new state-of-the-art results on Machine Translation, Text Summarization, Sentence Splitting, and Sentence Fusion. 1 Introduction Unsupervised and self-supervised pre-training methods, such as ELMo (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), and more recently BERT (Devlin et al., 2019), GPT and GPT-2 (Radford et al., 2018, 2019), XLNet (Yang et al., 2019), and RoBERTa (Liu et al., 2019) have established a qualitatively new level of baseline performance for many widely used Natural Language Understanding (NLU) benchmarks including some of the most popular, like GLUE (Williams et al., 2018) and SQuAD (Rajpurkar et al., 2018). The most appealing part about this massive shift towards using large architectures pre-trained on large collections of texts is that the pretrained checkpoints along with the inference code are made freely av"
2020.tacl-1.18,P12-1092,0,0.0696102,"uality); there is still room for improvements in these models (Dong et al., 2019; Song et al., 2019; Lewis et al., 2019). 274 7 Related Work Representation Learning. Starting around 2013, word embeddings like word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014) became popular as they were easy to train in an unsupervised fashion on raw text and they improved several downstream tasks when used as features. These word embeddings are invariant to the context in which we the word. There has been previously work to contextualize these embeddings, mainly to account for synonyms (e.g., Huang et al., 2012; Rothe and Sch¨utze, 2015) but only in 2018 did training of the contextualized embeddings using large deep neural networks and an unsupervised training scheme become popular. Whereas ELMo (Peters et al., 2018) and ULMFiT (Howard and Ruder, 2018) are based on LSTMs (Hochreiter and Schmidhuber, 1997), BERT and GPT are based on the transformer architecture (Vaswani et al., 2017). This architecture outperforms LSTMs on several NLP tasks and we therefore concentrated on these two pre-trained models. The contextualized embedding for each input token is given by the corresponding output of the last"
2020.tacl-1.18,P16-1154,0,0.107777,"Missing"
2020.tacl-1.18,P19-1356,0,0.0183561,"8) and grounded commonsense inference (SWAG, Zellers et al., 2018). All of these tasks are a form of classification or regression. Liu (2019) fine-tuned BERT for extractive summarization. An analysis of different layers of the BERT model was performed by Tenney et al. (2019). They found that the classical NLP pipeline appears in the expected sequence. In the context of our experiments in the Initializing a Subset of Layers section, this would mean that the DiscoFuse task profits the most from pre-trained information about POS, constituents, dependencies, and semantic roles. A similar study by Jawahar et al. (2019) found that BERT captures phrase-level information in the lower layers and linguistic information in intermediate layers, with surface features at the bottom, syntactic features in the middle, and semantic features at the top. GPT was also evaluated on natural language inference tasks. In the extended version of GPT-2, the model was evaluated on more general natural language processing tasks, like machine translation, reading comprehension, summarization, and language modeling. GPT-2 achieved new stateof-the-art results on several language modeling datasets. On the other tasks, GPT-2 outperfor"
2020.tacl-1.18,N19-1260,0,0.0170833,"32.96 35.95 33.97 36.29 36.33 37.39 34.47 29.03 22.99 33.79 30.90 38.42 32.44 37.53 38.52 39.87 22.21 28.48 27.79 19.91 10.23 15.83 11.52 15.24 16.12 17.50 4.89 8.77 8.37 5.20 24.24 30.80 25.65 30.05 31.13 32.37 16.69 22.30 21.91 15.88 37.01 37.62 38.93 41.45 16.35 18.79 31.52 33.90 Table 5: Summarization results of different models and their initialization setups. We compare our setups (the bottom block) against both non-pre-trained (the top block) and pre-trained (the middle block) models on various datasets: the Lead baseline, PtGen (See et al., 2017), ConvS2S (Gehring et al., 2017), MMN (Kim et al., 2019), Bottom-Up (Gehrmann et al., 2018), MASS (Song et al., 2019), TransLM (Khandelwal et al., 2019), and UniLM (Dong et al., 2019). The Lead results for the CNN/DailyMail dataset is taken from Narayan et al. (2018b), whereas Lead, PtGen, and ConvS2S results on the BBC dataset are taken from Narayan et al. (2018a). Our best results are boldfaced and the best results on the datasets are italicized. BBC dataset and BERT2GPT to RND2GPT on the CNN/DailyMail dataset. However, this is not the case with the Gigaword dataset, which has 3.8M training instances; BERT2GPT and ROBERTA2GPT perform better than"
2020.tacl-1.18,W12-3018,0,0.0511549,"Missing"
2020.tacl-1.18,D18-2012,0,0.0274029,"e WordPiece (Wu et al., 2016) to match the BERT pre-trained vocabulary. Depending on the experiment, we use one of the following publicly available checkpoints: BERT-Base Cased, BERTBase Uncased, BERT-Base Multilingual Cased (Devlin et al., 2019).1 The first two checkpoints have a vocabulary size of around ∼30k wordpieces, whereas the multilingual checkpoint has a much larger vocabulary size of ∼110k. BERT also trains positional embeddings for up to 512 positions, which is the maximum input and output length in all experiments. GPT-2 Checkpoints. We tokenize our text using the SentencePieces (Kudo and Richardson, 2018) to match the GPT-2 pre-trained vocabulary.2 Note that, although the available checkpoint is frequently called 117M, which suggests the same number of parameters, we count 125M parameters in the checkpoint. This is the smallest architecture they trained, and the number of layers, hidden size, and filter size are comparable to BERT-Base. The model was trained mainly on English data but does contain some foreign language. The vocabulary size is ∼50k. While GPT-2 has positional embeddings for up to 1,024 positions, we only use the first 512 to make the results comparable with BERT. RoBERTa Checkp"
2020.tacl-1.18,D18-1206,1,0.91584,"TPU/GPU hours, as warm-starting a model 264 Transactions of the Association for Computational Linguistics, vol. 8, pp. 264–280, 2020. https://doi.org/10.1162/tacl a 00313 Action Editor: Wenjie (Maggie) Li. Submission batch: 9/2019; Revision batch: 12/2019; Published 6/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. WMT14 En↔De machine translation using most common eval sets: newstest2014 and newstest2016, and abstractive summarization using three datasets: Gigaword (Napoles et al., 2012), CNN and DailyMail (Hermann et al., 2015), and BBC extreme (Narayan et al., 2018a). Our models report significant improvements over randomly initialized models, demonstrating the benefit of leveraging unsupervised pre-trained models. More importantly, this simple strategy results in new state-of-the-art results on machine translation, text summarization, sentence splitting, and sentence fusion. Our results also demonstrate that a pre-trained encoder is an essential component for sequence generation tasks and often these tasks benefit from sharing the weights between the encoder and the decoder. Overall, we have run over 300 experiments spending thousands of TPU v3 hours t"
2020.tacl-1.18,N03-1020,0,0.590839,"tokens for CNN/DailyMail, 64 for BBC, and 32 for Gigaword. We used a global batch size of 128 document–summary pairs for CNN/ DailyMail and BBC, and 256 document– summary pairs for Gigaword. We adapted to different number of training steps depending on the training data sizes. Models were trained for 500k, 300k, and 200k steps for the Gigaword, CNN/DailyMail, and BBC summarization datasets respectively. In all cases, we used the standard publicly available test sets; these consists of 1951 instances for Gigaword, 11,490 for CNN/DailyMail, and 11,334 for BBC. We report on the ROUGE F1 scores (Lin and Hovy, 2003); in particular, we report on ROUGE-1 and ROUGE-2 for informativeness and ROUGE-L for fluency in Table 5. 270 Document Understanding. All BERT encoder-based setups (i.e., BERT2RND, BERTSHARE, ROBERTASHARE, and BERT2BERT) outperform the baseline RND2RND by a large margin. The improvements of the RND2BERT setup, where only the decoder is initialized, are narrow. These results overall validate the significance of document representation in the encoder-decoder framework for summarization. On the BBC extreme summarization in particular, these four models achieve on average +6.85 point improvement i"
2020.tacl-1.18,N18-1158,1,0.948925,"TPU/GPU hours, as warm-starting a model 264 Transactions of the Association for Computational Linguistics, vol. 8, pp. 264–280, 2020. https://doi.org/10.1162/tacl a 00313 Action Editor: Wenjie (Maggie) Li. Submission batch: 9/2019; Revision batch: 12/2019; Published 6/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. WMT14 En↔De machine translation using most common eval sets: newstest2014 and newstest2016, and abstractive summarization using three datasets: Gigaword (Napoles et al., 2012), CNN and DailyMail (Hermann et al., 2015), and BBC extreme (Narayan et al., 2018a). Our models report significant improvements over randomly initialized models, demonstrating the benefit of leveraging unsupervised pre-trained models. More importantly, this simple strategy results in new state-of-the-art results on machine translation, text summarization, sentence splitting, and sentence fusion. Our results also demonstrate that a pre-trained encoder is an essential component for sequence generation tasks and often these tasks benefit from sharing the weights between the encoder and the decoder. Overall, we have run over 300 experiments spending thousands of TPU v3 hours t"
2020.tacl-1.18,D17-1064,1,0.890522,"Missing"
2020.tacl-1.18,D14-1162,0,0.0866075,"e over the RND2RND summaries, but have more repetitions than the RND2GPT summaries. See examples in Figure 1 for redundant repeated spans marked in orange. Overall, BERTSHARE and ROBERTASHARE summaries are unequivocally better than RND2GPT summaries in terms of both automatic evaluations (assessing ROUGE) and human evaluations (assessing summary quality); there is still room for improvements in these models (Dong et al., 2019; Song et al., 2019; Lewis et al., 2019). 274 7 Related Work Representation Learning. Starting around 2013, word embeddings like word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014) became popular as they were easy to train in an unsupervised fashion on raw text and they improved several downstream tasks when used as features. These word embeddings are invariant to the context in which we the word. There has been previously work to contextualize these embeddings, mainly to account for synonyms (e.g., Huang et al., 2012; Rothe and Sch¨utze, 2015) but only in 2018 did training of the contextualized embeddings using large deep neural networks and an unsupervised training scheme become popular. Whereas ELMo (Peters et al., 2018) and ULMFiT (Howard and Ruder, 2018) are based"
2020.tacl-1.18,N18-1202,0,0.108811,"n this paper, we demonstrate the efficacy of pre-trained checkpoints for Sequence Generation. We developed a Transformer-based sequence-to-sequence model that is compatible with publicly available pre-trained BERT, GPT-2, and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model, both encoder and decoder, with these checkpoints. Our models result in new state-of-the-art results on Machine Translation, Text Summarization, Sentence Splitting, and Sentence Fusion. 1 Introduction Unsupervised and self-supervised pre-training methods, such as ELMo (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), and more recently BERT (Devlin et al., 2019), GPT and GPT-2 (Radford et al., 2018, 2019), XLNet (Yang et al., 2019), and RoBERTa (Liu et al., 2019) have established a qualitatively new level of baseline performance for many widely used Natural Language Understanding (NLU) benchmarks including some of the most popular, like GLUE (Williams et al., 2018) and SQuAD (Rajpurkar et al., 2018). The most appealing part about this massive shift towards using large architectures pre-trained on large collections of texts is that the pretrained checkpoints along with the"
2020.tacl-1.18,W19-4302,0,0.0441671,"Missing"
2020.tacl-1.18,N18-2074,0,0.0761454,"Missing"
2020.tacl-1.18,P18-2124,0,0.181275,"rt results on Machine Translation, Text Summarization, Sentence Splitting, and Sentence Fusion. 1 Introduction Unsupervised and self-supervised pre-training methods, such as ELMo (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), and more recently BERT (Devlin et al., 2019), GPT and GPT-2 (Radford et al., 2018, 2019), XLNet (Yang et al., 2019), and RoBERTa (Liu et al., 2019) have established a qualitatively new level of baseline performance for many widely used Natural Language Understanding (NLU) benchmarks including some of the most popular, like GLUE (Williams et al., 2018) and SQuAD (Rajpurkar et al., 2018). The most appealing part about this massive shift towards using large architectures pre-trained on large collections of texts is that the pretrained checkpoints along with the inference code are made freely available. This saves hundreds of TPU/GPU hours, as warm-starting a model 264 Transactions of the Association for Computational Linguistics, vol. 8, pp. 264–280, 2020. https://doi.org/10.1162/tacl a 00313 Action Editor: Wenjie (Maggie) Li. Submission batch: 9/2019; Revision batch: 12/2019; Published 6/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 lic"
2020.tacl-1.18,P19-1452,0,0.0157563,"BERT) and a third new objective called translation language modeling to improve cross-lingual pre-training. Leveraging Public Checkpoints. BERT has been used for various NLP tasks, such as question answering on the SQuAD dataset (Rajpurkar et al., 2018). It also achieved new state-of-the-art results on the GLUE benchmark (Williams et al., 2018) and grounded commonsense inference (SWAG, Zellers et al., 2018). All of these tasks are a form of classification or regression. Liu (2019) fine-tuned BERT for extractive summarization. An analysis of different layers of the BERT model was performed by Tenney et al. (2019). They found that the classical NLP pipeline appears in the expected sequence. In the context of our experiments in the Initializing a Subset of Layers section, this would mean that the DiscoFuse task profits the most from pre-trained information about POS, constituents, dependencies, and semantic roles. A similar study by Jawahar et al. (2019) found that BERT captures phrase-level information in the lower layers and linguistic information in intermediate layers, with surface features at the bottom, syntactic features in the middle, and semantic features at the top. GPT was also evaluated on n"
2020.tacl-1.18,D17-1039,0,0.0195003,"n a specific task or to use the learned representations as features (i.e., freezing the pre-trained model). Their results suggest that the relative performance of fine-tuning vs. feature extraction depends on the similarity between the pre-training and the target tasks. Wang et al. (2019b) propose a combination of both, where first the model is trained with the BERT parameters being frozen and then the entire model is fine-tuned. This is the training scheme we used in the Initializing Only Layers section. Pre-training for Sequence Generation. Pretraining for seq2seq learning was first done by Ramachandran et al. (2017). They used a language model to pre-train the encoder and decoder of an RNN seq2seq model. Their method improved BLEU scores on newstest2014 by 3 points and ROUGE-L on CNN/DailyMail also by 3 points. However, their BLEU score of 24.7 on newstest2014 275 En→De, compared to 30.6 in this work, and 29.4 ROUGE-L on CNN/DailyMail, compared with 36.33, also show the superiority of the transformer model as well as the masked language model objective of BERT. MASS (Song et al., 2019) is a BERT-inspired method of pre-training seq2seq models. One advantage of this method is that, in contrast to our setup"
2020.tacl-1.18,P15-1173,1,0.897542,"Missing"
2020.tacl-1.18,P17-1099,0,0.362893,"Missing"
2020.tacl-1.18,P19-1439,0,0.034252,"Missing"
2020.tacl-1.18,P16-1162,0,0.235251,"Missing"
2020.tacl-1.18,Q16-1029,0,0.0589021,"Missing"
2020.tacl-1.18,D18-1009,0,0.0181722,"odel that encodes the source and generates the target. This setup matches our GPT setup. Conneau and Lample (2019) pre-train their model using casual language modeling (like GPT), masked language modeling (like BERT) and a third new objective called translation language modeling to improve cross-lingual pre-training. Leveraging Public Checkpoints. BERT has been used for various NLP tasks, such as question answering on the SQuAD dataset (Rajpurkar et al., 2018). It also achieved new state-of-the-art results on the GLUE benchmark (Williams et al., 2018) and grounded commonsense inference (SWAG, Zellers et al., 2018). All of these tasks are a form of classification or regression. Liu (2019) fine-tuned BERT for extractive summarization. An analysis of different layers of the BERT model was performed by Tenney et al. (2019). They found that the classical NLP pipeline appears in the expected sequence. In the context of our experiments in the Initializing a Subset of Layers section, this would mean that the DiscoFuse task profits the most from pre-trained information about POS, constituents, dependencies, and semantic roles. A similar study by Jawahar et al. (2019) found that BERT captures phrase-level inform"
2021.acl-long.474,N19-1423,0,0.0119301,"ion tasks (Durmus et al., 2020; Kry´sci´nski et al., 2019; Sellam et al., 2020; Rei et al., 2020). We evaluate FAME models on semantic inference metrics such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019b; Falke et al., 2019; Kryscinski et al., 2019) and question answering (Arumae and Liu, 2019; Wang et al., 2020a). In particular, we report the probability of a summary entailing (ent.) its input document (Maynez et al., 2020) and QA-based Feqa scores (Durmus et al., 2020). For ent. scores, we train an entailment classifier by fine-tuning a BERT-Large pretrained model (Devlin et al., 2019) on the MultiNLI dataset (Williams et al., 2018). For Feqa, we use a fine-tuned BART (Lewis et al., 2019) language model for question generation to generate questions from the summaries, and a BERTbase model fine-tuned on SQuAD (Rajpurkar et al., 2018) to answer the generated questions with input document as context.7 In addition to ent. and Feqa, we train a scorer leveraging manually annotated document-summary pairs for faithfulness, as a surrogate for human evaluation and call this metric BERTFaithful.8 In particular, we finetune a BERT-Base classi7 We used the Feqa code available here: http"
2021.acl-long.474,D17-1091,0,0.0242281,"Others avoid text degeneration by truncating the unreliable tail of the probability distribution at each decoding step, either by sampling from the top-k tokens (Top-k Sampling; Fan et al., 2018) or by sampling from a dynamic nucleus of tokens with the bulk of the probability mass (Nucleus Sampling; Holtzman et al., 2020). Others modify the training objective to make the distribution sparse (Martins et al., 2020) or assign lower probability to unlikely generations (Welleck et al., 2019a). For conditional text generation, most work focuses on generating diverse questions (Narayan et al., 2016; Dong et al., 2017; Sultan et al., 2020; Wang et al., 2020b) or paraphrases (Li et al., 2016b; Dai et al., 2017; Xu et al., 2018; Cao and Wan, 2020). Following Gehrmann et al. (2018), Cho et al. (2019) use a mixture of experts to sample different binary masks on the source sequence for diverse content selection for summarization. Our focus sampling is similar to top-k and nucleus sampling methods; in that it truncates the tail of the probability distribution. However, instead of truncating it at each decoding step, it biases the decoder proactively to generate output from a set of tokens which are topically-rel"
2021.acl-long.474,P19-1331,0,0.126886,"es (Vaswani et al., 2017), and large pretrained language models (Devlin et al., ∗ Work done when authors were interning/working at Google. Figure 1: Block A shows the best predictions from P EGASUS and our P EG FAME (P EGASUS with FAME) model, along with the G OLD summary for an XS UM article. Block B presents diverse summaries generated from P EGASUS using top-k and nucleus sampling. Block C shows diverse summaries generated using our P EG FAME model with Focus sampling. The text in orange is not supported by the input article. 2019; Radford et al., 2018; Yang et al., 2019; Liu et al., 2019; Dong et al., 2019a; Song et al., 2019; Lewis et al., 2019; Rothe et al., 2020; Raffel et al., 2019; Zhang et al., 2019). However, in terms of summary quality, many challenges remain. For example, generating summaries that are faithful to the input is an unsolved problem (Kryscinski et al., 2020; Maynez et al., 2020; Gabriel et al., 2020). Furthermore, there can be multiple equally good summaries per source docu6078 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6078–6095 August 1–6, 2021."
2021.acl-long.474,2020.acl-main.454,0,0.0183992,"ary for a document. Focus sampling is not used here. See Section 4.3 for details on the evaluation metrics reported. Best number for each metric is boldfaced. Faithfulness ROUGE and BERTScore do not correlate well with faithfulness of the generated summaries (Maynez et al., 2020). Human evaluation is traditionally considered as the gold standard for measuring faithfulness. But recent research has shown that even human evaluation has shortcomings (Schoch et al., 2020). Moreover, it is prohibitively expensive. This has led to the proposal of meta-evaluation metrics for various generation tasks (Durmus et al., 2020; Kry´sci´nski et al., 2019; Sellam et al., 2020; Rei et al., 2020). We evaluate FAME models on semantic inference metrics such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019b; Falke et al., 2019; Kryscinski et al., 2019) and question answering (Arumae and Liu, 2019; Wang et al., 2020a). In particular, we report the probability of a summary entailing (ent.) its input document (Maynez et al., 2020) and QA-based Feqa scores (Durmus et al., 2020). For ent. scores, we train an entailment classifier by fine-tuning a BERT-Large pretrained model (Devlin et al., 2019) on the Mu"
2021.acl-long.474,W19-4103,0,0.025381,"ndix C). Topic-Aware Generation Models The idea of capturing document-level semantic information has been widely explored in the summarization community. Barzilay and Elhadad (1997) use WordNet (Fellbaum, 1998) to model a text’s content relative to a topic based on lexical chains. Lin and Hovy (2000) propose to learn topic signatures for summarizing documents. Recently, document-level topic information has been used for improving neural language models (Mikolov and Zweig, 2012; Ghosh et al., 2016; Dieng et al., 2017; Karmaker Santu et al., 2019), neural response generators (Xing et al., 2017; Dziri et al., 2019), and not surprisingly, neural summarizers (Narayan et al., 2018; Ailem et al., 2019; Wang et al., 2020c). Both, Narayan et al. (2018) and Ailem et al. (2019), use a pretrained Latent Dirichlet Allocation (LDA; Blei et al., 2003) model, whereas, Wang et al. (2020c) use Poisson factor analysis (Zhou et al., 2012), to synthesize topic vectors for the input. Instead, we dynamically learn a target-induced topic distribution for the input under the assumption that the human-written 6079 summary is a good proxy for the input document. FAME Faithful Generation Models Cao et al. (2017) force faithful"
2021.acl-long.474,P19-1213,0,0.0422724,"Missing"
2021.acl-long.474,P18-1082,0,0.408582,", 2020; Maynez et al., 2020; Gabriel et al., 2020). Furthermore, there can be multiple equally good summaries per source docu6078 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6078–6095 August 1–6, 2021. ©2021 Association for Computational Linguistics ment. Neural generation models fail to account for this and tend to generate outputs with low diversity due to standard likelihood training, approximate decoding objectives, and lack of high quality multireference datasets (Fan et al., 2018; Kulikov et al., 2019; Freitag et al., 2020; Choi et al., 2020). Not much attention has been given to generation of diverse, yet faithful summaries – two goals are often challenging to achieve simultaneously (Hashimoto et al., 2019); a model can produce diverse outputs through sampling (Fan et al., 2018; Holtzman et al., 2020), but at the cost of quality. In this paper we introduce a Focus Attention MEchanism (or FAME) to transformer-based seq2seq architectures. FAME is inspired by how humans write summaries. Specifically, FAME aims to perform source-side planning to focus the summary on supp"
2021.acl-long.474,2020.emnlp-main.5,0,0.0248696,"al., 2020). Furthermore, there can be multiple equally good summaries per source docu6078 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6078–6095 August 1–6, 2021. ©2021 Association for Computational Linguistics ment. Neural generation models fail to account for this and tend to generate outputs with low diversity due to standard likelihood training, approximate decoding objectives, and lack of high quality multireference datasets (Fan et al., 2018; Kulikov et al., 2019; Freitag et al., 2020; Choi et al., 2020). Not much attention has been given to generation of diverse, yet faithful summaries – two goals are often challenging to achieve simultaneously (Hashimoto et al., 2019); a model can produce diverse outputs through sampling (Fan et al., 2018; Holtzman et al., 2020), but at the cost of quality. In this paper we introduce a Focus Attention MEchanism (or FAME) to transformer-based seq2seq architectures. FAME is inspired by how humans write summaries. Specifically, FAME aims to perform source-side planning to focus the summary on supported and topical content. FAME achieves thi"
2021.acl-long.474,D18-1443,0,0.0827254,"arization. 2 Related Work Task-Specific Architectural Priors Several works enhance seq2seq architectures with taskspecific priors. Pointer-generator style models (See et al., 2017; Xu et al., 2020) can accurately generate mostly extractive summaries by copying words from the source text via pointing. Text editing models (Malmi et al., 2019; Dong et al., 2019b; Mallinson et al., 2020) cast text generation as a sequence tagging problem with carefully selected edit operations required for the task. Others focus on improving content selection to better constrain the model to likely input phrases (Gehrmann et al., 2018) or by improving the representation of relevant input tokens (Zhou et al., 2017). Instead of directly modeling such priors, FAME learns the theme of the document through dynamic vocabulary biasing. Thus, FAME can be seen as a generalization of Pointer-generator or text-editing models via soft vocabulary learning. In fact, our FAME models achieve state-of-the-art on text-editing tasks (Appendix C). Topic-Aware Generation Models The idea of capturing document-level semantic information has been widely explored in the summarization community. Barzilay and Elhadad (1997) use WordNet (Fellbaum, 199"
2021.acl-long.474,N19-1348,0,0.0283256,"te a high level of abstractiveness, and generating them automatically requires document-level inference, abstraction, and paraphrasing. Due to their extreme nature, XS UM summaries are ideal to evaluate FAME models’ ability to capture the theme of the document.4 We use on the original cased version consisting of 204,045/11,332/11,334 training/validation/test document-summary pairs. During training, the input documents are truncated to 512 tokens. The 4 We further experiment with long-form story highlight generation (C NN /D M; Hermann et al., 2015) and two text editing tasks: Sentence Fusion (Geva et al., 2019) and Sentence Splitting (Botha et al., 2018). Their results can be found in Appendix B and C. Our FAME models achieve SOTA on both text-editing tasks. length of the summaries are limited to 64. 4.2 Pretrained Models with FAME We introduce FAME to two popular seq2seq architectures: RoBERTa initialized seq2seq (ROBERTA S2S, Rothe et al., 2020) and P EGASUS (Zhang et al., 2019). We refer ROBERTA S2S models with FAME as ROB FAME and P EGASUS with FAME with P EG FAME. We experiment with ROBERTA S2S-Large with shared encoder and decoder; it has 24 layers, a hidden size of 1024, filter size of 4096,"
2021.acl-long.474,P16-1154,0,0.038209,"diplomatic staff after it said the country was responsible for the use of Australian visas used in the killing of a Palestinian in the Middle East. Introduction Document summarization — producing the shorter version of a document while preserving salient information (Mani, 2001; Nenkova and McKeown, 2011) — is challenging even for humans. Today, systems can generate summaries with a high level of fluency and coherence. This is due to recent advances such as sequence-to-sequence architectures (seq2seq) with attention and copy mechanism (Hochreiter and Schmidhuber, 1997; Bahdanau et al., 2015; Gu et al., 2016), fully attention-based Transformer architectures (Vaswani et al., 2017), and large pretrained language models (Devlin et al., ∗ Work done when authors were interning/working at Google. Figure 1: Block A shows the best predictions from P EGASUS and our P EG FAME (P EGASUS with FAME) model, along with the G OLD summary for an XS UM article. Block B presents diverse summaries generated from P EGASUS using top-k and nucleus sampling. Block C shows diverse summaries generated using our P EG FAME model with Focus sampling. The text in orange is not supported by the input article. 2019; Radford et a"
2021.acl-long.474,N19-1169,0,0.0166244,"11th International Joint Conference on Natural Language Processing, pages 6078–6095 August 1–6, 2021. ©2021 Association for Computational Linguistics ment. Neural generation models fail to account for this and tend to generate outputs with low diversity due to standard likelihood training, approximate decoding objectives, and lack of high quality multireference datasets (Fan et al., 2018; Kulikov et al., 2019; Freitag et al., 2020; Choi et al., 2020). Not much attention has been given to generation of diverse, yet faithful summaries – two goals are often challenging to achieve simultaneously (Hashimoto et al., 2019); a model can produce diverse outputs through sampling (Fan et al., 2018; Holtzman et al., 2020), but at the cost of quality. In this paper we introduce a Focus Attention MEchanism (or FAME) to transformer-based seq2seq architectures. FAME is inspired by how humans write summaries. Specifically, FAME aims to perform source-side planning to focus the summary on supported and topical content. FAME achieves this through a novel technique which augments standard contextual representations with a dynamic source-conditioned vocabulary biasing layer. We present the following experimental findings: FA"
2021.acl-long.474,K19-1073,0,0.0265635,"our FAME models achieve state-of-the-art on text-editing tasks (Appendix C). Topic-Aware Generation Models The idea of capturing document-level semantic information has been widely explored in the summarization community. Barzilay and Elhadad (1997) use WordNet (Fellbaum, 1998) to model a text’s content relative to a topic based on lexical chains. Lin and Hovy (2000) propose to learn topic signatures for summarizing documents. Recently, document-level topic information has been used for improving neural language models (Mikolov and Zweig, 2012; Ghosh et al., 2016; Dieng et al., 2017; Karmaker Santu et al., 2019), neural response generators (Xing et al., 2017; Dziri et al., 2019), and not surprisingly, neural summarizers (Narayan et al., 2018; Ailem et al., 2019; Wang et al., 2020c). Both, Narayan et al. (2018) and Ailem et al. (2019), use a pretrained Latent Dirichlet Allocation (LDA; Blei et al., 2003) model, whereas, Wang et al. (2020c) use Poisson factor analysis (Zhou et al., 2012), to synthesize topic vectors for the input. Instead, we dynamically learn a target-induced topic distribution for the input under the assumption that the human-written 6079 summary is a good proxy for the input documen"
2021.acl-long.474,W19-8609,0,0.148748,"al., 2020; Gabriel et al., 2020). Furthermore, there can be multiple equally good summaries per source docu6078 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6078–6095 August 1–6, 2021. ©2021 Association for Computational Linguistics ment. Neural generation models fail to account for this and tend to generate outputs with low diversity due to standard likelihood training, approximate decoding objectives, and lack of high quality multireference datasets (Fan et al., 2018; Kulikov et al., 2019; Freitag et al., 2020; Choi et al., 2020). Not much attention has been given to generation of diverse, yet faithful summaries – two goals are often challenging to achieve simultaneously (Hashimoto et al., 2019); a model can produce diverse outputs through sampling (Fan et al., 2018; Holtzman et al., 2020), but at the cost of quality. In this paper we introduce a Focus Attention MEchanism (or FAME) to transformer-based seq2seq architectures. FAME is inspired by how humans write summaries. Specifically, FAME aims to perform source-side planning to focus the summary on supported and topical cont"
2021.acl-long.474,N16-1014,0,0.0329108,"ability distribution at each decoding step, either by sampling from the top-k tokens (Top-k Sampling; Fan et al., 2018) or by sampling from a dynamic nucleus of tokens with the bulk of the probability mass (Nucleus Sampling; Holtzman et al., 2020). Others modify the training objective to make the distribution sparse (Martins et al., 2020) or assign lower probability to unlikely generations (Welleck et al., 2019a). For conditional text generation, most work focuses on generating diverse questions (Narayan et al., 2016; Dong et al., 2017; Sultan et al., 2020; Wang et al., 2020b) or paraphrases (Li et al., 2016b; Dai et al., 2017; Xu et al., 2018; Cao and Wan, 2020). Following Gehrmann et al. (2018), Cho et al. (2019) use a mixture of experts to sample different binary masks on the source sequence for diverse content selection for summarization. Our focus sampling is similar to top-k and nucleus sampling methods; in that it truncates the tail of the probability distribution. However, instead of truncating it at each decoding step, it biases the decoder proactively to generate output from a set of tokens which are topically-relevant to the input. 3 tx1 Dense tx2 tx3 GELU ... tX Dense yt x1 x2 x3 ..."
2021.acl-long.474,C00-1072,0,0.427932,"2017). Instead of directly modeling such priors, FAME learns the theme of the document through dynamic vocabulary biasing. Thus, FAME can be seen as a generalization of Pointer-generator or text-editing models via soft vocabulary learning. In fact, our FAME models achieve state-of-the-art on text-editing tasks (Appendix C). Topic-Aware Generation Models The idea of capturing document-level semantic information has been widely explored in the summarization community. Barzilay and Elhadad (1997) use WordNet (Fellbaum, 1998) to model a text’s content relative to a topic based on lexical chains. Lin and Hovy (2000) propose to learn topic signatures for summarizing documents. Recently, document-level topic information has been used for improving neural language models (Mikolov and Zweig, 2012; Ghosh et al., 2016; Dieng et al., 2017; Karmaker Santu et al., 2019), neural response generators (Xing et al., 2017; Dziri et al., 2019), and not surprisingly, neural summarizers (Narayan et al., 2018; Ailem et al., 2019; Wang et al., 2020c). Both, Narayan et al. (2018) and Ailem et al. (2019), use a pretrained Latent Dirichlet Allocation (LDA; Blei et al., 2003) model, whereas, Wang et al. (2020c) use Poisson fact"
2021.acl-long.474,N03-1020,0,0.657432,"Missing"
2021.acl-long.474,2021.ccl-1.108,0,0.0643904,"Missing"
2021.acl-long.474,2020.emnlp-main.348,0,0.0440949,"Missing"
2021.acl-long.474,2020.acl-main.173,1,0.855929,"ock B presents diverse summaries generated from P EGASUS using top-k and nucleus sampling. Block C shows diverse summaries generated using our P EG FAME model with Focus sampling. The text in orange is not supported by the input article. 2019; Radford et al., 2018; Yang et al., 2019; Liu et al., 2019; Dong et al., 2019a; Song et al., 2019; Lewis et al., 2019; Rothe et al., 2020; Raffel et al., 2019; Zhang et al., 2019). However, in terms of summary quality, many challenges remain. For example, generating summaries that are faithful to the input is an unsolved problem (Kryscinski et al., 2020; Maynez et al., 2020; Gabriel et al., 2020). Furthermore, there can be multiple equally good summaries per source docu6078 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6078–6095 August 1–6, 2021. ©2021 Association for Computational Linguistics ment. Neural generation models fail to account for this and tend to generate outputs with low diversity due to standard likelihood training, approximate decoding objectives, and lack of high quality multireference datasets (Fan et al., 2018; Kulikov e"
2021.acl-long.474,2020.findings-emnlp.111,0,0.0166544,"existing diversity and faithfulness measures. Empirically, we find that optimizing for high diversity often comes at the cost of faithfulness. Thus FAME provides a mechanism for trading-off high faithfulness with better diversity in summarization. 2 Related Work Task-Specific Architectural Priors Several works enhance seq2seq architectures with taskspecific priors. Pointer-generator style models (See et al., 2017; Xu et al., 2020) can accurately generate mostly extractive summaries by copying words from the source text via pointing. Text editing models (Malmi et al., 2019; Dong et al., 2019b; Mallinson et al., 2020) cast text generation as a sequence tagging problem with carefully selected edit operations required for the task. Others focus on improving content selection to better constrain the model to likely input phrases (Gehrmann et al., 2018) or by improving the representation of relevant input tokens (Zhou et al., 2017). Instead of directly modeling such priors, FAME learns the theme of the document through dynamic vocabulary biasing. Thus, FAME can be seen as a generalization of Pointer-generator or text-editing models via soft vocabulary learning. In fact, our FAME models achieve state-of-the-art"
2021.acl-long.474,2020.findings-emnlp.217,0,0.0283636,"Missing"
2021.acl-long.474,D19-1510,1,0.853043,". Sampling technique using a variety of existing diversity and faithfulness measures. Empirically, we find that optimizing for high diversity often comes at the cost of faithfulness. Thus FAME provides a mechanism for trading-off high faithfulness with better diversity in summarization. 2 Related Work Task-Specific Architectural Priors Several works enhance seq2seq architectures with taskspecific priors. Pointer-generator style models (See et al., 2017; Xu et al., 2020) can accurately generate mostly extractive summaries by copying words from the source text via pointing. Text editing models (Malmi et al., 2019; Dong et al., 2019b; Mallinson et al., 2020) cast text generation as a sequence tagging problem with carefully selected edit operations required for the task. Others focus on improving content selection to better constrain the model to likely input phrases (Gehrmann et al., 2018) or by improving the representation of relevant input tokens (Zhou et al., 2017). Instead of directly modeling such priors, FAME learns the theme of the document through dynamic vocabulary biasing. Thus, FAME can be seen as a generalization of Pointer-generator or text-editing models via soft vocabulary learning. In f"
2021.acl-long.474,P17-1099,0,0.552242,"ctiveness of our new Focus 1 In the paper we focus on assessing FAME on XS UM. But other summarization and text editing results can be found in Appendix B and C. Sampling technique using a variety of existing diversity and faithfulness measures. Empirically, we find that optimizing for high diversity often comes at the cost of faithfulness. Thus FAME provides a mechanism for trading-off high faithfulness with better diversity in summarization. 2 Related Work Task-Specific Architectural Priors Several works enhance seq2seq architectures with taskspecific priors. Pointer-generator style models (See et al., 2017; Xu et al., 2020) can accurately generate mostly extractive summaries by copying words from the source text via pointing. Text editing models (Malmi et al., 2019; Dong et al., 2019b; Mallinson et al., 2020) cast text generation as a sequence tagging problem with carefully selected edit operations required for the task. Others focus on improving content selection to better constrain the model to likely input phrases (Gehrmann et al., 2018) or by improving the representation of relevant input tokens (Zhou et al., 2017). Instead of directly modeling such priors, FAME learns the theme of the docu"
2021.acl-long.474,2020.acl-main.704,0,0.0221615,"ere. See Section 4.3 for details on the evaluation metrics reported. Best number for each metric is boldfaced. Faithfulness ROUGE and BERTScore do not correlate well with faithfulness of the generated summaries (Maynez et al., 2020). Human evaluation is traditionally considered as the gold standard for measuring faithfulness. But recent research has shown that even human evaluation has shortcomings (Schoch et al., 2020). Moreover, it is prohibitively expensive. This has led to the proposal of meta-evaluation metrics for various generation tasks (Durmus et al., 2020; Kry´sci´nski et al., 2019; Sellam et al., 2020; Rei et al., 2020). We evaluate FAME models on semantic inference metrics such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019b; Falke et al., 2019; Kryscinski et al., 2019) and question answering (Arumae and Liu, 2019; Wang et al., 2020a). In particular, we report the probability of a summary entailing (ent.) its input document (Maynez et al., 2020) and QA-based Feqa scores (Durmus et al., 2020). For ent. scores, we train an entailment classifier by fine-tuning a BERT-Large pretrained model (Devlin et al., 2019) on the MultiNLI dataset (Williams et al., 2018). For Feqa"
2021.acl-long.474,2020.acl-main.500,0,0.0381542,"egeneration by truncating the unreliable tail of the probability distribution at each decoding step, either by sampling from the top-k tokens (Top-k Sampling; Fan et al., 2018) or by sampling from a dynamic nucleus of tokens with the bulk of the probability mass (Nucleus Sampling; Holtzman et al., 2020). Others modify the training objective to make the distribution sparse (Martins et al., 2020) or assign lower probability to unlikely generations (Welleck et al., 2019a). For conditional text generation, most work focuses on generating diverse questions (Narayan et al., 2016; Dong et al., 2017; Sultan et al., 2020; Wang et al., 2020b) or paraphrases (Li et al., 2016b; Dai et al., 2017; Xu et al., 2018; Cao and Wan, 2020). Following Gehrmann et al. (2018), Cho et al. (2019) use a mixture of experts to sample different binary masks on the source sequence for diverse content selection for summarization. Our focus sampling is similar to top-k and nucleus sampling methods; in that it truncates the tail of the probability distribution. However, instead of truncating it at each decoding step, it biases the decoder proactively to generate output from a set of tokens which are topically-relevant to the input. 3"
2021.acl-long.474,2020.acl-main.450,0,0.410602,"widely explored in the summarization community. Barzilay and Elhadad (1997) use WordNet (Fellbaum, 1998) to model a text’s content relative to a topic based on lexical chains. Lin and Hovy (2000) propose to learn topic signatures for summarizing documents. Recently, document-level topic information has been used for improving neural language models (Mikolov and Zweig, 2012; Ghosh et al., 2016; Dieng et al., 2017; Karmaker Santu et al., 2019), neural response generators (Xing et al., 2017; Dziri et al., 2019), and not surprisingly, neural summarizers (Narayan et al., 2018; Ailem et al., 2019; Wang et al., 2020c). Both, Narayan et al. (2018) and Ailem et al. (2019), use a pretrained Latent Dirichlet Allocation (LDA; Blei et al., 2003) model, whereas, Wang et al. (2020c) use Poisson factor analysis (Zhou et al., 2012), to synthesize topic vectors for the input. Instead, we dynamically learn a target-induced topic distribution for the input under the assumption that the human-written 6079 summary is a good proxy for the input document. FAME Faithful Generation Models Cao et al. (2017) force faithful generation by conditioning on both source text and extracted fact descriptions from the source text. So"
2021.acl-long.474,2020.findings-emnlp.194,0,0.406061,"widely explored in the summarization community. Barzilay and Elhadad (1997) use WordNet (Fellbaum, 1998) to model a text’s content relative to a topic based on lexical chains. Lin and Hovy (2000) propose to learn topic signatures for summarizing documents. Recently, document-level topic information has been used for improving neural language models (Mikolov and Zweig, 2012; Ghosh et al., 2016; Dieng et al., 2017; Karmaker Santu et al., 2019), neural response generators (Xing et al., 2017; Dziri et al., 2019), and not surprisingly, neural summarizers (Narayan et al., 2018; Ailem et al., 2019; Wang et al., 2020c). Both, Narayan et al. (2018) and Ailem et al. (2019), use a pretrained Latent Dirichlet Allocation (LDA; Blei et al., 2003) model, whereas, Wang et al. (2020c) use Poisson factor analysis (Zhou et al., 2012), to synthesize topic vectors for the input. Instead, we dynamically learn a target-induced topic distribution for the input under the assumption that the human-written 6079 summary is a good proxy for the input document. FAME Faithful Generation Models Cao et al. (2017) force faithful generation by conditioning on both source text and extracted fact descriptions from the source text. So"
2021.emnlp-main.12,W04-1013,0,0.0488049,"e that task-specific priors into BERT language model pretraining improves on low-resource finetuning tasks. This is also done by Zhang et al. (2020) with P EGASUS, where important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, to teach summarization models to do better content selection and Narayan et al. (2021) proposed a content ∗ XSum planning pretraining objective with P EGASUS, by pre-pending the output sequence with the entity plans observed in it. P EGASUS achieved state of the art ROUGE-1/2/-L scores (Lin, 2004) on BBC XSum (Narayan et al., 2018) with 47.21 / 24.56 / 39.25 and CNN/DailyMail with 44.17 / 21.47 / 41.11. These numbers could not be matched by Raffel et al. (2019) even when using a much larger model with up to 11 billion parameters. This seems to support the intuition that task specific pretraining is important for the best performance. However, Raffel et al. (2019) used a beam search length penalty (beam alpha) of 0.6. We set the beam alpha parameter to the optimal value and report new state of the art results on XSum and SAMSum (Table 1). Given these new results we want to answer the qu"
2021.emnlp-main.12,2021.emnlp-main.468,0,0.0761832,"Missing"
2021.emnlp-main.12,D19-5545,0,0.0366672,"Missing"
2021.emnlp-main.12,W19-4406,0,0.023911,"n and grammatical error correction. Our findings are that, while pretraining for summarization is very important, we found no evidence that task specific pretraining improved on common benchmarks for abstractive datasets, even in a low resource setting. On extractive datasets, task specific pretraining showed benefits but the results are below a sentence selection baseline, questioning the practical usefulness. Given the trend to larger neural network Datasets and Eval Metrics For Grammatical Error Correction we fine-tune our pre-trained models on the FCE (Yannakoudakis et al., 2011) and W&I (Bryant et al., 2019) corpora. We evaluate on the standard benchmark of CoNLL-14, using CoNLL13 as the development set. Reported numbers in 143 3 https://github.com/nusnlp/m2scorer models with significant costs to train them, we recommend to use a task agnostic pretraining regime. Corrupted span prediction is currently our most successful candidate, with state-of-the-art results on two investigated summarization benchmarks. But we are curious if even more flexible pretraining technique will emerge. For grammar error correction, task specific pretraining was showing superior performance, especially in a low resourc"
2021.emnlp-main.12,D18-1206,1,0.941331,"ors into BERT language model pretraining improves on low-resource finetuning tasks. This is also done by Zhang et al. (2020) with P EGASUS, where important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, to teach summarization models to do better content selection and Narayan et al. (2021) proposed a content ∗ XSum planning pretraining objective with P EGASUS, by pre-pending the output sequence with the entity plans observed in it. P EGASUS achieved state of the art ROUGE-1/2/-L scores (Lin, 2004) on BBC XSum (Narayan et al., 2018) with 47.21 / 24.56 / 39.25 and CNN/DailyMail with 44.17 / 21.47 / 41.11. These numbers could not be matched by Raffel et al. (2019) even when using a much larger model with up to 11 billion parameters. This seems to support the intuition that task specific pretraining is important for the best performance. However, Raffel et al. (2019) used a beam search length penalty (beam alpha) of 0.6. We set the beam alpha parameter to the optimal value and report new state of the art results on XSum and SAMSum (Table 1). Given these new results we want to answer the questions if task-specific pretrainin"
2021.emnlp-main.12,2020.emnlp-main.697,0,0.0307586,"very different average lengths in predicted summaries for different models, with P EGASUS being closest to the target lengths. However, by 1000 training examples, all models start generating summaries of comparable lengths. 4 10 19.54 21.71 03.78 Grammatical Error Correction We further investigate if our findings translate to other generation tasks. Here, we focus on the task of grammatical error correction, but also other important aspects of text generation show benefit from task specific pretraining and are still underexplored; e.g., improving evaluations (Sellam et al., 2020), factuality (Chen et al., 2020) or planning for grounded generation (Narayan et al., 2021). Table 3 are F0.5 scores (Dahlmeier and Ng, 2012) computed by the M 2 scorer.3 Results As shown in Table 3 T EXT C OR outperforms T5 on all dataset sizes. The results also show that an unrelated task-specific pretraining objective hurts performance even when training on the full dataset. This is notable as for example the MR ND S pretraining is not that far of from a normal language model pretraining and should learn a reasonable amount about language and well formed sentences. Zero Shot In contrast to summarization, no easy baseline"
2021.emnlp-main.12,N12-1067,0,0.0374697,"to the target lengths. However, by 1000 training examples, all models start generating summaries of comparable lengths. 4 10 19.54 21.71 03.78 Grammatical Error Correction We further investigate if our findings translate to other generation tasks. Here, we focus on the task of grammatical error correction, but also other important aspects of text generation show benefit from task specific pretraining and are still underexplored; e.g., improving evaluations (Sellam et al., 2020), factuality (Chen et al., 2020) or planning for grounded generation (Narayan et al., 2021). Table 3 are F0.5 scores (Dahlmeier and Ng, 2012) computed by the M 2 scorer.3 Results As shown in Table 3 T EXT C OR outperforms T5 on all dataset sizes. The results also show that an unrelated task-specific pretraining objective hurts performance even when training on the full dataset. This is notable as for example the MR ND S pretraining is not that far of from a normal language model pretraining and should learn a reasonable amount about language and well formed sentences. Zero Shot In contrast to summarization, no easy baseline exists for grammatical error correction. A simple copy baseline would give us a high word overlap like B LEU"
2021.emnlp-main.12,2020.tacl-1.18,1,0.714648,"0 33.76 / 11.30 / 29.43 31.03 / 10.25 / 27.94 33.75 / 10.97 / 29.83 29.76 / 09.83 / 26.61 41.28 / 16.51 / 37.04 42.12 / 18.06 / 38.22 39.02 / 15.10 / 34.89 38.94 / 15.62 / 34.77 46.18 / 21.44 / 41.59 47.25 / 21.39 / 42.20 45.18 / 20.11 / 40.10 46.45 / 21.15 / 41.29 Table 2: ROUGE-1 / -2 / -L scores in summarization datasets. Results are shown on their full test sets using only 10, 100, 1000 and 10000 training examples, and the whole training set (all). We also report on zero-shot results. We report Lead-1 baseline for BBC from (Narayan et al., 2018) and Lead-3 baseline for CNN/DailyMail from (Rothe et al., 2020). For SAMSum, we achieve the best lead scores when we select top 5 sentences for each input. Result in gray are worse than the lead sentence baseline. Best results in each block are bolded. Results marked with ∗ are not comparable, see text. For results marked with o , the untrained checkpoint at step 0 was performing best on the development set. -2 and -L as metric. The datasets differ in the degree of abstraction and summarization length. The summaries of CNN/DailyMail are more of extractive nature and have an average length of 3 sentences. The summaries of BBC XSum are singlesentences and m"
2021.emnlp-main.12,2020.acl-main.704,0,0.0246361,"for the zero-shot case we observe very different average lengths in predicted summaries for different models, with P EGASUS being closest to the target lengths. However, by 1000 training examples, all models start generating summaries of comparable lengths. 4 10 19.54 21.71 03.78 Grammatical Error Correction We further investigate if our findings translate to other generation tasks. Here, we focus on the task of grammatical error correction, but also other important aspects of text generation show benefit from task specific pretraining and are still underexplored; e.g., improving evaluations (Sellam et al., 2020), factuality (Chen et al., 2020) or planning for grounded generation (Narayan et al., 2021). Table 3 are F0.5 scores (Dahlmeier and Ng, 2012) computed by the M 2 scorer.3 Results As shown in Table 3 T EXT C OR outperforms T5 on all dataset sizes. The results also show that an unrelated task-specific pretraining objective hurts performance even when training on the full dataset. This is notable as for example the MR ND S pretraining is not that far of from a normal language model pretraining and should learn a reasonable amount about language and well formed sentences. Zero Shot In contrast to"
2021.emnlp-main.12,2020.findings-emnlp.285,0,0.0430023,"Table 1: Current state-of-the-art ROUGE-1 / -2 / -L scores for summarization datasets and our results in the T5 framework with optimal beam alpha. Raffel et al. (2019) only report numbers for CNN/DailyMail. Introduction Previous work mostly used task-agnostic pretraining methods like corrupted span prediction (T5; Raffel et al., 2019), masked language model (BERT; Devlin et al., 2018), denoising objective (BART; Lewis et al., 2019 or a vanilla language model (GPT; Radford et al., 2019). Intuitively it makes sense to refine the pretraining to a setup that closer resembles the downstream task. Wang et al. (2020) demonstrate that task-specific priors into BERT language model pretraining improves on low-resource finetuning tasks. This is also done by Zhang et al. (2020) with P EGASUS, where important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, to teach summarization models to do better content selection and Narayan et al. (2021) proposed a content ∗ XSum planning pretraining objective with P EGASUS, by pre-pending the output sequence with the entity plans observed in it. P EGASUS achieved state of the art ROUGE-1/2/"
2021.emnlp-main.12,P11-1019,0,0.0514751,"t text generation tasks, summarization and grammatical error correction. Our findings are that, while pretraining for summarization is very important, we found no evidence that task specific pretraining improved on common benchmarks for abstractive datasets, even in a low resource setting. On extractive datasets, task specific pretraining showed benefits but the results are below a sentence selection baseline, questioning the practical usefulness. Given the trend to larger neural network Datasets and Eval Metrics For Grammatical Error Correction we fine-tune our pre-trained models on the FCE (Yannakoudakis et al., 2011) and W&I (Bryant et al., 2019) corpora. We evaluate on the standard benchmark of CoNLL-14, using CoNLL13 as the development set. Reported numbers in 143 3 https://github.com/nusnlp/m2scorer models with significant costs to train them, we recommend to use a task agnostic pretraining regime. Corrupted span prediction is currently our most successful candidate, with state-of-the-art results on two investigated summarization benchmarks. But we are curious if even more flexible pretraining technique will emerge. For grammar error correction, task specific pretraining was showing superior performanc"
2021.findings-emnlp.133,N19-1423,0,0.00680822,"s”. We follow Xu et al. (2020) and represent facts in a sentence by adapting Semantic Role Labelling (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. The facts in  the document and summary are represented as F1D , F2D , · · · FID and  S S F1 , F2 , · · · FJS , respectively. We apply automatic content weighting as defined in (Xu et al., 2020) and weight each fact Fj in the summary using its maximum semantic similarity to the facts in the document wjf = maxi∈I dfij , where dfij is the semantic similarity based on BERT embeddings (Devlin et al., 2019). The Summary Fact-weights score is then defined as the average weights over all facts in the summary: SFweights = avgj=1···J wjf ∈ [−1, 1] (1) A high SFweights score indicates that the facts in the summaries are well supported by the facts mentioned in the documents. The top section in Table 3 shows SFweights scores reported on M I RA NEWS(S-D), M I RA NEWS(S-A) and M I RA NEWS(S-D&A), which weight facts in the summaries using facts in the main document, assisting documents, and both, respectively. As expected, SFweights on M I RA NEWS(S-D) is higher than on M I RA NEWS(SA), indicating that t"
2021.findings-emnlp.133,P19-1483,0,0.0592215,"Missing"
2021.findings-emnlp.133,P19-1102,0,0.0209277,"4. 7 Related Work Single Document Summarization aims to compress a single textual document while keeping salient information. SDS includes two directions: extractive summarization (Nallapati et al., 2017) which aims at extracting salient sentences from the input document, and abstractive summarization (See et al., 2017; Narayan et al., 2018a; Yang et al., 2019; Liu and Lapata, 2019b; Liu et al., 2020; Rothe et al., 2020; Raffel et al., 2020) which generates a novel short representation of the input. Multi-Document Summarization aims to compress multiple textual documents to a shorter summary (Fabbri et al., 2019). Approaches mainly focus on increasing the capacity of the encoder to process longer inputs (Liu and Lapata, 2019a; Beltagy et al., 2020; Zaheer et al., 2020; Zhang et al., 2020a; Huang et al., 2021), leveraging knowledge graphs (Fan et al., 2019; Li et al., 2020; Jin et al., 2020), and including content selection steps (Nayeem et al., 2018; Wang et al., 2020; Xu and Lapata, 2020; Grenander et al., 2019; Liu et al., 2018). tures, training and decoding, e.g. Cao et al. (2018); Zhang et al. (2020c); Falke et al. (2019); Zhao et al. (2020b). However, we are the first research aiming to reduce th"
2021.findings-emnlp.133,P19-1213,0,0.0501629,"Missing"
2021.findings-emnlp.133,D19-1428,0,0.0547221,"Missing"
2021.findings-emnlp.133,D19-1620,0,0.0414923,"Missing"
2021.findings-emnlp.133,N18-1065,0,0.0602853,"Missing"
2021.findings-emnlp.133,2021.naacl-main.112,0,0.0488549,"Missing"
2021.findings-emnlp.133,2020.acl-main.556,0,0.0528558,"Missing"
2021.findings-emnlp.133,2020.acl-main.703,0,0.0121967,"on), but is covered in the related assisting document (bottom section). We highlight the information in the summary that is aligned to its corresponding main and assisting documents with yellow and pink colors, respectively. conditional generation models, including sequenceto-sequence architectures with attention and copy The vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. datasets are curated by pairing a single document (2020) classified hallucinations into intrinsic that with human authored highlights/description as the mistakenly manipulate information from the source summary. This tas"
2021.findings-emnlp.133,2020.acl-main.555,0,0.0271442,"Missing"
2021.findings-emnlp.133,N03-1020,0,0.691106,"Missing"
2021.findings-emnlp.133,P19-1500,0,0.276332,"oderick were among the early arrivals... Figure 1: An example where the summary (top section) contains information that is not explicitly included in its main document (middle section), but is covered in the related assisting document (bottom section). We highlight the information in the summary that is aligned to its corresponding main and assisting documents with yellow and pink colors, respectively. conditional generation models, including sequenceto-sequence architectures with attention and copy The vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. datasets are curated by pairing a single"
2021.findings-emnlp.133,J05-1004,0,0.199508,"support the summary better. • EO M I RA NEWS(S-D&A) in Table 2 contains the best three sentences from the main and assisting documents against the summary. The higher ROUGE scores on M I RA NEWS(S-D&A), as compared to M I RA NEWS(S-D), indicate that assisting documents A contribute additional information to the summaries, which is absent from the main document D. • Summary Fact-weights evaluate the semantic correspondence between a document and its summary using a representation based on “facts”. We follow Xu et al. (2020) and represent facts in a sentence by adapting Semantic Role Labelling (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. The facts in  the document and summary are represented as F1D , F2D , · · · FID and  S S F1 , F2 , · · · FJS , respectively. We apply automatic content weighting as defined in (Xu et al., 2020) and weight each fact Fj in the summary using its maximum semantic similarity to the facts in the document wjf = maxi∈I dfij , where dfij is the semantic similarity based on BERT embeddings (Devlin et al., 2019). The Summary Fact-weights score is then defined as the average weights over all facts in the summary:"
2021.findings-emnlp.133,D19-1387,0,0.25318,"oderick were among the early arrivals... Figure 1: An example where the summary (top section) contains information that is not explicitly included in its main document (middle section), but is covered in the related assisting document (bottom section). We highlight the information in the summary that is aligned to its corresponding main and assisting documents with yellow and pink colors, respectively. conditional generation models, including sequenceto-sequence architectures with attention and copy The vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. datasets are curated by pairing a single"
2021.findings-emnlp.133,2020.acl-main.173,1,0.88083,"Missing"
2021.findings-emnlp.133,D18-1206,1,0.844209,"nguage model and will be prone to extrinsic hallucinations. In this work, we tackle the problem of extrinsic hallucinations by introducing a new task, MultiResource-Assisted News Summarization and a novel dataset (M I RA NEWS). Following Maynez et al. (2020), we regard the incorporation of background knowledge within a generated summary as the desired property. However, instead of sourcing this knowledge via pretraining on large datasets,2 2 Although they report B ERT S2S (Rothe et al., 2020) to output more factual hallucinations in the summary than their non-pre-trained counterparts on XSum (Narayan et al., 2018a), we base our work on the assumption that articles from alternative news resources covering the same news event can complement the background knowledge in an easier to learn, more direct, and explainable way. Consider the example in Figure 1, where the assisting document (bottom section) from another news resource recounts some facts in the summary (highlighted in pink) in a more explicit way. Note that, as shown in Figure 2 (left), our task is different from both Single-document Summarization (SDS, middle) and Multi-document Summarization (MDS, right): SDS aims at generating a summary for a"
2021.findings-emnlp.133,N18-1158,1,0.720606,"nguage model and will be prone to extrinsic hallucinations. In this work, we tackle the problem of extrinsic hallucinations by introducing a new task, MultiResource-Assisted News Summarization and a novel dataset (M I RA NEWS). Following Maynez et al. (2020), we regard the incorporation of background knowledge within a generated summary as the desired property. However, instead of sourcing this knowledge via pretraining on large datasets,2 2 Although they report B ERT S2S (Rothe et al., 2020) to output more factual hallucinations in the summary than their non-pre-trained counterparts on XSum (Narayan et al., 2018a), we base our work on the assumption that articles from alternative news resources covering the same news event can complement the background knowledge in an easier to learn, more direct, and explainable way. Consider the example in Figure 1, where the assisting document (bottom section) from another news resource recounts some facts in the summary (highlighted in pink) in a more explicit way. Note that, as shown in Figure 2 (left), our task is different from both Single-document Summarization (SDS, middle) and Multi-document Summarization (MDS, right): SDS aims at generating a summary for a"
2021.findings-emnlp.133,C18-1102,0,0.0553052,"Missing"
2021.findings-emnlp.133,2020.emnlp-main.748,0,0.207296,"s at the end of the main document. Since each document contains around 700 words on average (see Table 1), we truncate the main document to half the size of the model capacity, i.e. 500 words for BART-large and 1000 words for HT, respectively. To include information from all assisting documents, we truncate each of them to fill the 7 Implementation used: https://huggingface.co/ transformers/model_doc/bart.html. 8 We use the implementation from https://github. com/nlpyang/hiersumm. remaining half of the model capacity evenly. • Pipeline (-P): Previous approaches T-DMCA (Liu et al., 2018), TLM (Pilault et al., 2020) and SEAL (Zhao et al., 2020a) show that long input settings for abstractive summarization benefit from a content extraction preprocessing step. We thus introduce a simple weakly supervised content extraction method for the assisting documents, and concatenate the selected content to the end of the main document on the input. Note that the content selection in M I RA NEWS is conditioned on the main document, which is different from content selection in both SDS and MDS that select sentences without additional conditioning. In particular, we first compute a contextual embedding for each sentenc"
2021.findings-emnlp.133,2020.tacl-1.18,1,0.89944,"h data divergence issues between the source and target texts (Dhingra et al., 2019) will function more as an open-ended language model and will be prone to extrinsic hallucinations. In this work, we tackle the problem of extrinsic hallucinations by introducing a new task, MultiResource-Assisted News Summarization and a novel dataset (M I RA NEWS). Following Maynez et al. (2020), we regard the incorporation of background knowledge within a generated summary as the desired property. However, instead of sourcing this knowledge via pretraining on large datasets,2 2 Although they report B ERT S2S (Rothe et al., 2020) to output more factual hallucinations in the summary than their non-pre-trained counterparts on XSum (Narayan et al., 2018a), we base our work on the assumption that articles from alternative news resources covering the same news event can complement the background knowledge in an easier to learn, more direct, and explainable way. Consider the example in Figure 1, where the assisting document (bottom section) from another news resource recounts some facts in the summary (highlighted in pink) in a more explicit way. Note that, as shown in Figure 2 (left), our task is different from both Single"
2021.findings-emnlp.133,2020.emnlp-main.647,0,0.0184006,"vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. datasets are curated by pairing a single document (2020) classified hallucinations into intrinsic that with human authored highlights/description as the mistakenly manipulate information from the source summary. This task is typically approached using document resulting in counterfactual output, and 1 extrinsic that introduce information not grounded Our code and data are available at: https://github.com/XinnuoXu/MiRANews in the document (see Figure 1). Extrinsic halluci1541 1 Introduction Findings of the Association for Computational Linguis"
2021.findings-emnlp.133,P17-1099,0,0.319088,"edian, who and husband matthew broderick were among the early arrivals... Figure 1: An example where the summary (top section) contains information that is not explicitly included in its main document (middle section), but is covered in the related assisting document (bottom section). We highlight the information in the summary that is aligned to its corresponding main and assisting documents with yellow and pink colors, respectively. conditional generation models, including sequenceto-sequence architectures with attention and copy The vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. data"
2021.findings-emnlp.133,2020.emnlp-main.32,0,0.0429564,"Missing"
2021.findings-emnlp.133,2020.acl-main.455,1,0.740794,"han M I RA NEWS(SD). Introducing the assisting documents contributes new information to support the summary better. • EO M I RA NEWS(S-D&A) in Table 2 contains the best three sentences from the main and assisting documents against the summary. The higher ROUGE scores on M I RA NEWS(S-D&A), as compared to M I RA NEWS(S-D), indicate that assisting documents A contribute additional information to the summaries, which is absent from the main document D. • Summary Fact-weights evaluate the semantic correspondence between a document and its summary using a representation based on “facts”. We follow Xu et al. (2020) and represent facts in a sentence by adapting Semantic Role Labelling (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. The facts in  the document and summary are represented as F1D , F2D , · · · FID and  S S F1 , F2 , · · · FJS , respectively. We apply automatic content weighting as defined in (Xu et al., 2020) and weight each fact Fj in the summary using its maximum semantic similarity to the facts in the document wjf = maxi∈I dfij , where dfij is the semantic similarity based on BERT embeddings (Devlin et al., 2019). The Summ"
2021.findings-emnlp.133,2020.emnlp-main.296,0,0.0567443,"Missing"
2021.findings-emnlp.133,2020.acl-main.458,0,0.027433,"42.29 36.18 43.22 36.06 43.11 36.08 43.13 46.72 55.39 43.15 51.02 BertScore P R F1 .701 .674 .684 .701 .666 .679 .701 .677 .685 .685 .682 .680 .690 .682 .682 .684 .686 .681 .769 .745 .755 .716 .731 .721 Table 4: Evaluation on ROUGE and BertScore. 4.2 Evaluation Metrics We evaluate the approaches described in Section 4.1 from four perspectives: • Similarity to Reference focuses on evaluating the generated summary with respect to its similarity to a human-authored ground-truth reference summary. We adopt the exact-matching metric ROUGE (Lin and Hovy, 2003) and the softmatching metric BertScore (Zhang et al., 2020b). • Extractiveness level aims at the bias of each system towards generating extractive summaries. We introduce the n-grams coverage, which equals to 1 − n-gram novelty (see Section 3), to measure the percentage of n-grams in the generated summary that appear in the main and assisting documents. Higher n-gram coverage scores indicate that the system is more extractive. • Support from Assisting Documents measures the proportion of information appearing in the generated summary that originates from assisting documents only. We propose the n-grams coverage over n-grams in the generated summary w"
2021.findings-emnlp.133,2020.findings-emnlp.203,0,0.0149793,"t. Since each document contains around 700 words on average (see Table 1), we truncate the main document to half the size of the model capacity, i.e. 500 words for BART-large and 1000 words for HT, respectively. To include information from all assisting documents, we truncate each of them to fill the 7 Implementation used: https://huggingface.co/ transformers/model_doc/bart.html. 8 We use the implementation from https://github. com/nlpyang/hiersumm. remaining half of the model capacity evenly. • Pipeline (-P): Previous approaches T-DMCA (Liu et al., 2018), TLM (Pilault et al., 2020) and SEAL (Zhao et al., 2020a) show that long input settings for abstractive summarization benefit from a content extraction preprocessing step. We thus introduce a simple weakly supervised content extraction method for the assisting documents, and concatenate the selected content to the end of the main document on the input. Note that the content selection in M I RA NEWS is conditioned on the main document, which is different from content selection in both SDS and MDS that select sentences without additional conditioning. In particular, we first compute a contextual embedding for each sentence in both main and assisting"
2021.gem-1.10,2020.acl-main.424,0,0.0251404,"les WebNLG (Gardent et al., 2017) Produce a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmarks are critical in measuring modeling prog"
2021.gem-1.10,2020.acl-main.766,0,0.0426234,"sh datasets were ranked lower, with only WebNLG and MLSum among the top 15 datasets. We grouped all datasets by their high-level tasks and selected a group that would not violate the selection principles (e.g., only high-resource tasks). If two datasets fit, we picked the one with a higher interest rating. Among the 11 datasets, we have 18different languages, and the dataset sizes range from 5,000 examples to 1.5M, with most datasets between 50-150k examples. Two of them do not include English at all, which we hope reduces the dependence of the modeling approaches on anglocentric pretraining (Anastasopoulos and Neubig, 2020). The high-level tasks include Dialog, Summarization, Data-to-Text, and Simplification. About half of the datasets have multiple references and more than half had post-processing steps applied to them to ensure high data quality. 3.1 GEMifying the data We produce data cards (Bender and Friedman, 2018; Gebru et al., 2018) for all data sets in GEM, for which we developed an NLG-specific template.7 In addition to describing the data itself, the cards acknowledge potential limitations of a dataset regarding its creation process and describe its real-world use cases to ensure that the research is c"
2021.gem-1.10,W05-0909,0,0.165721,"otebook within 2-3 hours. 4.2 avoid overfitting to known metrics, we will use metrics on the test submissions that are not included in this initial writeup. Consequentially, the baseline results are an incomplete list which will be expanded upon the announcement of the test metrics. The set of metrics can be computed via the framework described at https://gem-benchmark. com/shared_task which comprises metrics in the following categories: Lexical Similarity. We include multiple “traditional” metrics as baseline metrics, notably BLEU (Papineni et al., 2002), ROUGE-1/2/L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005). These metrics can often be gamed, for example, ROUGE can be improved by increased the output length of the model (Sun et al., 2019). Moreover, the reliability of these metrics depends on the quality and number of the references (Mathur et al., 2020a; Freitag et al., 2020). However, on a system-level, they still correlate well with human judgments for some tasks (Reiter, 2018). Automated Evaluation As mentioned above, GEM provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 10"
2021.gem-1.10,W11-2832,0,0.0915033,"Missing"
2021.gem-1.10,2020.inlg-1.24,1,0.864971,"human evaluation standards. In recent work, Howcroft et al. (2020) investigated NLG papers from the last 2 For a more complete description of recent developments in NLG evaluation, we refer to the survey by Celikyilmaz et al. (2020). 99 twenty years and the evaluation methodologies differ drastically across papers. Moreover, in most cases, it is not even mentioned what the human evaluation aims to measure and that definitions of measures like “accuracy” or “fluency” are inconsistent. They thus suggest reporting standards for criteria and methods, following a classification system proposed by Belz et al. (2020). In addition, regularly scheduled shared tasks like WMT have lead to standardization of human evaluation setups and enabled controlled experimentation with them. GEM has the opportunity to develop reproducible standards for how human evaluation for NLG tasks beyond translation should be conducted while at the same time incorporating lessons from related work. Acting on the same need, the recently proposed GENIE (Khashabi et al., 2021) system aims to automate and standardize the human evaluation of different NLG systems, however with the contrasting goal of reducing the evaluating to a leaderb"
2021.gem-1.10,Q18-1041,0,0.236179,"han half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generation, Evaluation, and Metrics) that aims to enable research on a wide range of NLG challenges. To avoid the fallacy of encouraging hill climbing on a leaderboard ("
2021.gem-1.10,W17-4755,0,0.0383891,"Missing"
2021.gem-1.10,W16-2302,0,0.0489314,"Missing"
2021.gem-1.10,N18-2097,0,0.0498492,"Missing"
2021.gem-1.10,N19-1423,0,0.0210653,"provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 104 Semantic Equivalence. More recently, metrics that rely on pretrained language models have shown improved correlations with human judgments on the segment-level. We thus include BERTScore (Zhang et al., 2020b), a metric based on the similarity of sentence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation appr"
2021.gem-1.10,P19-1483,1,0.811795,"tions. 106 Figure 2: A screenshot of the interactive result exploration tool. [Top Left] The selection of tasks, task-groups, or individual submissions. [Top Right] The selection of metric-groups or metrics [Bottom] The parallel coordinates visualization of the selection. The selection here can be filtered by brushing over a section of an individual metric, as is shown here for BLEURT. Hovering over a line presents detailed information of the particular submission. grained and interpretable evaluation metrics, for example to measure consistency in data-to-text problems (Opitz and Frank, 2020; Dhingra et al., 2019). We are using one such metric called NUBIA (Kane et al., 2020), the NeUral Based Interchangeability Assessor, which combines multiple measures such as entailment and similarity into a decomposable and interpretable score. Diversity. As argued by Hashimoto et al. (2019) among many others, NLG models intrinsically trade off diversity and quality. A model can produce more diverse outputs through sampling but at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek"
2021.gem-1.10,K19-1037,0,0.027404,"Missing"
2021.gem-1.10,P17-1123,0,0.0287395,"Missing"
2021.gem-1.10,2020.acl-main.454,1,0.84177,"LEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen"
2021.gem-1.10,W19-8652,1,0.897973,"Missing"
2021.gem-1.10,C16-1191,0,0.0653297,"Missing"
2021.gem-1.10,P18-1082,0,0.0705277,"Missing"
2021.gem-1.10,W16-3622,1,0.897392,"Missing"
2021.gem-1.10,P16-2008,1,0.869379,"Missing"
2021.gem-1.10,W19-8670,1,0.884454,"Missing"
2021.gem-1.10,2020.emnlp-main.393,0,0.179031,"le to be able to address newly found limitations, and that the benchmark should focus on climbing a leaderboard. Instead, a living benchmark that can adjust its datasets and specific evaluation metrics can be much more powerful and long-lived. This can, for example, be seen in Dynabench,1 (Potts et al., 2020) which has a static evaluation, but interactively adds more test However, they also pose a risk that progress is reduced to the single number shown in a benchmark’s leaderboard and thus may encourage blindly optimizing it without regard to other considerations like model size or fairness (Ethayarajh and Jurafsky, 2020). This is especially challenging for benchmarks in NLG since, as discussed above, the performance cannot be described through a single metric and it is often not clear what metric to optimize for. This shortfall can be seen in benchmarks like DecaNLP (McCann et al., 2018) and GLGE (Liu et al., 2020a) which include NLG tasks but focus only on a single metric and, as a result, may mischaracterize a system’s performance. Moreover, an easy-to-use data infrastructure also disincentivizes researchers from interacting with 1 98 https://dynabench.org/ data through a human-in-the-loop approach. able mu"
2021.gem-1.10,N19-1395,0,0.0587235,"Missing"
2021.gem-1.10,2020.emnlp-main.751,0,0.503468,"nly on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover,"
2021.gem-1.10,P19-1346,1,0.897232,"Missing"
2021.gem-1.10,2020.webnlg-1.7,1,0.843551,"Missing"
2021.gem-1.10,2020.findings-emnlp.195,1,0.835947,"Missing"
2021.gem-1.10,2020.emnlp-main.5,0,0.126308,"e machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differently across tasks, setups, and languages, a multi-task NLG benchmark has the opportunity to act as a testbed to evaluate how the latest advances in automated metrics perform on these different tasks. The benchmark can facilitate this research through the release of system outputs and associated human annotations, which is what we are planning to do with GEM. Moreover, we allow the integrat"
2021.gem-1.10,2020.emnlp-main.489,0,0.0422168,"les 3 and 4. Our interactive system is centered around a parallel coordinates plot (Inselberg, 1985) which shows all results as lines through parallel axes. Every line intersects the axes at the corresponding mapped value. For instance, see the red line representing the results for task “ToTTo” of baseline “t5-small”. Filters can be applied along axes (see BLEURT axis in Figure 2) and the filtered selection is highlighted through bold lines. A selection can be a set of metrics, systems, or tasks. This style of presentation has not been used before for a benchmark. The closest prior work is by Fu et al. (2020) for namedentity recognition which allows similar filtering and sorting, but presents the results in a table. However, the parallel coordinates approach can scale to a much greater number of metrics than a table. Moreover, by using a parallel coordinates plot instead of a table, it is easy to spot patterns that span multiple metrics, systems, or tasks. For example, the highlighted line in Figure 2 uncovers that, for the T5 baseline on ToTTo, the diversity metrics score higher than other systems while scoring lower on reference-based metrics. Since we only have a single baseline for ToTTo, it i"
2021.gem-1.10,W17-3518,1,0.863605,"Missing"
2021.gem-1.10,N19-1169,1,0.923485,"eiter and Dale, 2000). These texts aim to fulfill an underlying communicative goal (e.g., to produce a summary of an article) while remaining faithful to the input information, fluent, grammatical, and natural-looking. An NLG system needs to be robust to shifts in the data distribution and be able to produce text in many different languages. Finally, it is often desired that repeated interactions with the model produce diverse outputs, for example, to explain concepts in multiple ways or to become a more interesting conversational agent. These optimization objectives can often be conflicting (Hashimoto et al., 2019) and, as a result, evaluations that focus only on a single aspect may fail to recognize the drawbacks of a particular method. To demonstrate this trade-off, consider an improvement on the CNN-DM summarization dataset (Hermann et al., 2015; Nallapati et al., 2016) measured by the ROUGE-L metWe introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent"
2021.gem-1.10,2020.ngt-1.1,0,0.0253511,"first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution"
2021.gem-1.10,2020.inlg-1.23,1,0.841029,"Missing"
2021.gem-1.10,D15-1229,0,0.0462438,"Missing"
2021.gem-1.10,2020.acl-main.709,1,0.887061,"Missing"
2021.gem-1.10,2020.acl-main.560,0,0.077305,"on does not consider differences between the languages that lead to higher modeling complexity, for example, a richer morphology or a flexible word-order. Still, the majority of work in NLP and almost all benchmarks exclusively focus on English (e.g., Wang et al., 2019b; Liu et al., 2020a; McCann et al., 2018). Even if multiple languages are considered, the availability of data in a language often does not represent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and Wiki"
2021.gem-1.10,2020.evalnlgeval-1.4,0,0.0163557,"ation tool. [Top Left] The selection of tasks, task-groups, or individual submissions. [Top Right] The selection of metric-groups or metrics [Bottom] The parallel coordinates visualization of the selection. The selection here can be filtered by brushing over a section of an individual metric, as is shown here for BLEURT. Hovering over a line presents detailed information of the particular submission. grained and interpretable evaluation metrics, for example to measure consistency in data-to-text problems (Opitz and Frank, 2020; Dhingra et al., 2019). We are using one such metric called NUBIA (Kane et al., 2020), the NeUral Based Interchangeability Assessor, which combines multiple measures such as entailment and similarity into a decomposable and interpretable score. Diversity. As argued by Hashimoto et al. (2019) among many others, NLG models intrinsically trade off diversity and quality. A model can produce more diverse outputs through sampling but at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek et al., 2020) and by van Miltenburg et al. (2018). These inclu"
2021.gem-1.10,D18-1208,0,0.0650198,"Missing"
2021.gem-1.10,Q18-1023,0,0.0642232,"Missing"
2021.gem-1.10,2020.findings-emnlp.360,1,0.934785,"e a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmarks are critical in measuring modeling progress. and conducting in-depth ana"
2021.gem-1.10,D16-1128,0,0.0680679,"Missing"
2021.gem-1.10,2020.acl-main.703,0,0.214186,"esent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared"
2021.gem-1.10,2020.acl-main.653,0,0.233553,"esent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared"
2021.gem-1.10,N16-1014,0,0.0410993,"t at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek et al., 2020) and by van Miltenburg et al. (2018). These include the Shannon Entropy (Shannon and Weaver, 1963) over unigrams and bigrams (H1 , H2 ), the mean segmented type token ratio over segment lengths of 100 (MSTTR, Johnson, 1944), the ratio of distinct n-grams over the total number of n-grams (Distinct1,2 ), and the count of n-grams that only appear once across the entire test output (Unique1,2 , Li et al., 2016). focus of this section will be on qualitative descriptions through model cards, we also gather quantitative information that is not necessarily associated with a judgment. As part of this, we collect the number of parameters of a system, as suggested by Ethayarajh and Jurafsky (2020). For each task, we additionally report the vocabulary size over the output (|V|) and the mean output length of a system (Sun et al., 2019). 5 Results One of the central aims of GEM is to measure the progress in NLG without misrepresenting the complex interactions between the sometimes contradicting measures. We t"
2021.gem-1.10,2020.findings-emnlp.165,0,0.0883829,"Missing"
2021.gem-1.10,W04-1013,0,0.355472,"n be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate. * Correspondence to gehrmann@google.com 96 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets"
2021.gem-1.10,2020.acl-main.465,0,0.123121,"; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generation, Evaluation, and Metrics) that aims to enable research on a wide range of NLG challenges. To avoid the fallacy of encouraging hill climbing on a leaderboard (Linzen, 2020), GEM focuses on an in-depth evaluation of model outputs across human and automatic evaluation that aims to uncover shortcomings and opportunities for progress. As datasets, metrics, and models improve, the benchmark environment will improve as well, replacing “solved” tasks with more challenging ones, incorporating newly developed metrics, and addressing discovered flaws in the experimental setup, as demonstrated in Figure 1. Making all model outputs available under an open-source license will support evaluation research and integrating new metrics will, in turn, help their adoption and incre"
2021.gem-1.10,2020.tacl-1.47,0,0.129414,"ich has a static evaluation, but interactively adds more test However, they also pose a risk that progress is reduced to the single number shown in a benchmark’s leaderboard and thus may encourage blindly optimizing it without regard to other considerations like model size or fairness (Ethayarajh and Jurafsky, 2020). This is especially challenging for benchmarks in NLG since, as discussed above, the performance cannot be described through a single metric and it is often not clear what metric to optimize for. This shortfall can be seen in benchmarks like DecaNLP (McCann et al., 2018) and GLGE (Liu et al., 2020a) which include NLG tasks but focus only on a single metric and, as a result, may mischaracterize a system’s performance. Moreover, an easy-to-use data infrastructure also disincentivizes researchers from interacting with 1 98 https://dynabench.org/ data through a human-in-the-loop approach. able much richer evaluation (as described in the next sections), and promote non-English datasets. In addition, it can ensure that the datasets created for those shared tasks continue being evaluated. Increasing multilingualism of NLG research. Another potentially harmful choice by benchmark creators is t"
2021.gem-1.10,2021.ccl-1.108,0,0.0502233,"Missing"
2021.gem-1.10,W15-4640,0,0.0777152,"Missing"
2021.gem-1.10,W18-6450,0,0.0170006,"ever, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automa"
2021.gem-1.10,W19-5302,0,0.0535855,"Missing"
2021.gem-1.10,2020.coling-main.420,0,0.022745,"s many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicting results since studies often omit crucial replication details and assume different definitions of the measured quantities (Howcroft et al., 2020). Improving Data Improving Metrics Evaluation with gameable metrics Improving Models Consistent Human Eval Non-repeatable human evaluation Figure 1: The opportunities of living benchmarks and pitfalls of evaluation. As models improve, we need consistent evaluations such that models can be compared to each other. This can only happen if we develop robust human evaluation standards and improve our automated metrics. Ot"
2021.gem-1.10,2020.acl-main.448,0,0.212993,"ere is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differe"
2021.gem-1.10,2020.wmt-1.77,0,0.124839,"ere is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differe"
2021.gem-1.10,2020.acl-main.173,1,0.838186,"to which we invite the entire NLG community to participate. * Correspondence to gehrmann@google.com 96 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicti"
2021.gem-1.10,W18-3601,1,0.834682,"where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way"
2021.gem-1.10,2020.msr-1.1,1,0.821875,"Missing"
2021.gem-1.10,C18-1147,1,0.871186,"Missing"
2021.gem-1.10,2020.emnlp-main.466,0,0.0504036,"Missing"
2021.gem-1.10,D15-1238,0,0.013053,"d this goal, GEM welcomes anyone interested in collaborating on this effort. 7.2 Personalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the results presented on our website to incorporate them. For the updates to the dataset selection, we want to consider the in"
2021.gem-1.10,K16-1028,0,0.065799,"Missing"
2021.gem-1.10,D18-1206,1,0.894493,"Missing"
2021.gem-1.10,W17-5525,1,0.880333,"Missing"
2021.gem-1.10,P02-1040,0,0.114424,"n NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achi"
2021.gem-1.10,2020.emnlp-main.89,1,0.895725,"Missing"
2021.gem-1.10,W17-3537,1,0.910798,"trics saturate, we need to evaluate them on more challenging datasets instead of continuing to move sideways on old ones. GEM aims to provide this environment for natural language generation. than half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmar"
2021.gem-1.10,2020.emnlp-main.185,0,0.0484252,"Missing"
2021.gem-1.10,2021.acl-long.186,0,0.0851423,"Missing"
2021.gem-1.10,P19-1195,0,0.0585453,"Missing"
2021.gem-1.10,N18-1012,0,0.0173282,"akers of low-resourced languages through a participatory research approach, as suggested by (∀ et al., 2020). Toward this goal, GEM welcomes anyone interested in collaborating on this effort. 7.2 Personalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the result"
2021.gem-1.10,Q19-1016,0,0.0583227,"Missing"
2021.gem-1.10,J18-3002,0,0.0872491,"for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated"
2021.gem-1.10,2020.acl-main.442,0,0.137138,"hem on more challenging datasets instead of continuing to move sideways on old ones. GEM aims to provide this environment for natural language generation. than half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generatio"
2021.gem-1.10,2020.emnlp-main.647,0,0.0370713,"Missing"
2021.gem-1.10,2021.emnlp-main.529,0,0.024648,"on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen BART T5 0.301 0.291 63.5 64.0 32.5 29.4 55.1 54.5 27.5 26.4 0.943 0.942 -0.400 -0.412 mT5-small mT5-base mT5-large mT5-XL TGen TGen+ TGen++ 0.229 0.23 0.233 0.229 0.152 0.151 0.167 47.3 48.1 51.3 52.1 13.6 13.8 9.7 28.6 28.8 30.0 31.3 0.0 0.0 0.0 43.0 44.2 46.4 47.3 13.6 13.8 9.7 17.9 17.1 17.5 17.0 0.03 0.03 0.03 0.895 0.898 0.902 0.905 0.6"
2021.gem-1.10,D19-1320,0,0.0210218,"ence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTSc"
2021.gem-1.10,2020.acl-main.704,1,0.824209,"er, on a system-level, they still correlate well with human judgments for some tasks (Reiter, 2018). Automated Evaluation As mentioned above, GEM provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 104 Semantic Equivalence. More recently, metrics that rely on pretrained language models have shown improved correlations with human judgments on the segment-level. We thus include BERTScore (Zhang et al., 2020b), a metric based on the similarity of sentence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang e"
2021.gem-1.10,P19-1212,0,0.0526263,"Missing"
2021.gem-1.10,P19-1646,0,0.0614703,"Missing"
2021.gem-1.10,Q16-1029,1,0.821779,"in a news article en *25k Articles WebNLG (Gardent et al., 2017) Produce a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmar"
2021.gem-1.10,W15-3031,0,0.06329,"Missing"
2021.gem-1.10,W19-2303,0,0.167787,"of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicting results since studies often omit crucial replication details and assume different definitions of the measured"
2021.gem-1.10,2020.nlp4convai-1.13,0,0.0861626,"Missing"
2021.gem-1.10,D16-1033,0,0.064867,"Missing"
2021.gem-1.10,2020.acl-main.450,0,0.015505,"2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen BART T5 0.301 0.291"
2021.gem-1.10,P18-1205,0,0.0245839,"nalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the results presented on our website to incorporate them. For the updates to the dataset selection, we want to consider the input of the wider NLG research community. To do so, we will set up a yearly selec"
C12-1123,W11-2832,0,0.0690707,"orms and whose leaf nodes represent the clusters of error mined data grouped according to the suspicious forms characterizing their elements. Like in a decision tree, each cluster in the suspicion tree is characterized by the set of attributes (suspicious forms) labelling its ancestors; and the tree itself represents a disjunction of mutually exclusive error cases. We illustrate the impact of our error mining algorithm on error analysis by applying it to detect and analyse the most likely sources of failure in a surface realiser developed to participate in the Surface Realisation Shared Task (Belz et al., 2011); and we show how this error mining algorithm permits improving the surface realiser. The paper is structured as follows. We start (Section 2) by introducing our error mining algorithm. In essence, this algorithm adapts (Quinlan, 1986)’s ID3 algorithm to build a suspicion tree such that the clusters obtained group together sets of input data that share similar sources of failure (called suspicious forms); and the attributes labelling these clusters are the suspicious forms indicating which are these most likely causes of failure. In Section 3, we show how this error mining algorithm helps impr"
C12-1123,W09-2609,0,0.0398769,"Missing"
C12-1123,W11-2929,0,0.0668978,"Missing"
C12-1123,P12-1062,1,0.575067,"i . . . w n is the ratio P(w i . . . w n ) = C(w i ...w n |OK) where C(w i . . . w n ) is the number of sentences in which the n-gram w i . . . w n occurs and C(w i ...w n ) C(w i . . . w n |OK), the number of sentences containing w i . . . w n which could be parsed. In other words, the parsability rate of an n-gram is the proportion of sentences in which this n-gram occurs and for which parsing succeeds. An n-gram then, is a suspicious form if it has a low parsability rate. (van Noord, 2004)’s approach was extended and refined in (Sagot and de la Clergerie, 2006), (de Kok et al., 2009) and (Gardent and Narayan, 2012) as follows. (Sagot and de la Clergerie, 2006) defines a suspicion rate for n-grams which takes into account the number of occurrences of a given word form and iteratively defines the suspicion rate of each word form in a sentence based on the suspicion rate of this word form in the corpus. Further, (de Kok et al., 2009) extends this iterative error mining to n-grams of words and POS tags of arbitrary length. And (Gardent and Narayan, 2012) extends (van Noord, 2004)’s approach to mine for suspicious subtrees rather than n-grams. An important limitation shared by all these error mining approach"
C12-1123,C12-1124,1,0.769309,"s on lexical and grammatical issues. Attributes The attributes used to partition the SR data are suspicious trees i.e., subtrees of the SR dependency trees whose frequency is above a given threshold. Following (Gardent and Narayan, 2012), we allow for various views on errors by mining for forms labelled with lemmas only (word); with parts of speech (POS); with dependency relations (dep); with lemmas and parts of speech (word/POS); and with dependency relations and parts of speech (dep-POS) (cf. Figure 3). Generation System The system to be tested is the symbolic Surface Realiser described in (Narayan and Gardent, 2012). We ran this surface realiser on the SR input data and separately stored the input dependency trees for which generation succeeded (PASS) and the input dependency trees for which generation failed (FAIL). We then removed from the failed data, those cases where generation failed either because a word was missing in the lexicon or because a grammar rule was missing but required by the lexicon and the input data. These cases can easily be detected using the generation system and thus do not need to be handled by error mining. Error Mining We iterate several times between error mining and perform"
C12-1123,W11-2836,0,0.0306671,"frontier indicates that the main distinct suspicious forms are, in that order: 1. Possessive NPs (POSS is the part of speech tag assigned to possessive ’s3 ) The suspicious form (POSS) points to a mismatch between the representation of genitive NPs (e.g., Oakland’s thief) in the SR Task data and in the grammar. While our generator expects the representation of ‘Oakland’s thief’ to be (thief/NN, (’s/POSS, (oakland/NNP))), the structure provided by the SR Task is (thief/NN, (oakland/NNP, (’s/POSS))). Hence whenever a possessive appears in the input data, generation fails. This is in line with (Rajkumar et al., 2011)’s finding that the logical 2 Iteration stops either when the results are perfect (perfect coverage and perfect BLEU score) or when the trees fail to be discriminative enough (low number of FAIL instances associated with the suspicion tree leaves). So far, the latter situation did not occur and we are still using the suspicion tree to identify the main sources of errors for the remaining error cases. 3 In fact, the part of speech tag assigned to possessive ’s in the SR data is POS not POSS. We renamed it to avoid confusion with POS as an abbreviation for part-of-speech. 2017 (POSS) yes no (NN)"
C12-1123,P06-1042,0,0.0513126,"Missing"
C12-1123,C96-2103,0,0.322685,"Missing"
C12-1123,P04-1057,0,0.0422848,"Missing"
C12-1124,J99-2004,0,0.0617305,"cores. To address the fact that there are n! ways to combine any n modifiers with a single constituent, (White, 2004) proposes to use a language model to prune the chart of identical edges representing different modifier permutations, e.g., to choose between fierce black cat and black fierce cat. Similarly, (Bangalore and Rambow, 2000) assumes a single derivation tree that encodes a word lattice (a {fierce black, black fierce} cat), and uses statistical knowledge to select the best linearisation. Recently, (Espinosa et al., 2008) adapted the supertagging techniques first proposed for parsing (Bangalore and Joshi, 1999) to surface realisation. Given a treebank in the appropriate format, this technique permits filtering the initial search space by using a model trained on that treebank. Supertagging was shown to improve the performance of symbolic parsers and generators significantly. However, it requires the existence of a treebank in a format appropriate to generate the supertagging model. In sum, various symbolic and statistical techniques have been developed to improve the efficiency of grammar-based surface realisation. However, statistical systems using supertagging require the existence of a treebank i"
C12-1124,W11-2832,0,0.285323,"tional optimisation techniques. From the lexicalist approach, it adapts two techniques designed to prune the search space, namely a so-called polarity filter on local input trees (Bonfante et al., 2004); and the use of a language model to prune competing intermediate substructures. In addition, the algorithm is parallelised to explore the possible completions of the top-down predictions simultaneously rather than sequentially. The algorithm was implemented using a Feature-Based Lexicalised Tree Adjoining Grammar for English and tested on the Generation Challenge Surface Realisation task data (Belz et al., 2011). We compare our algorithm with a baseline lexicalist approach which processes the input tree top 2028 down. The results show that the algorithm we propose drastically improves on the baseline, reducing generation time for sentences longer than 6 words w.r.t. this baseline. This paper is structured as follows. Section 2 situates our approach with respect to related work. Section 3 introduces the input data provided by the Generation Challenge Surface Realisation task and used for the evaluation. Section 4 introduces the tree adjoining grammar used by the algorithm. Section 5 presents the surfa"
C12-1124,W11-2835,0,0.0280494,"pertagging model. In sum, various symbolic and statistical techniques have been developed to improve the efficiency of grammar-based surface realisation. However, statistical systems using supertagging require the existence of a treebank in an appropriate format while the purely symbolic systems described in (Carroll and Oepen, 2005; Gardent and Kow, 2005; Koller and Striegnitz, 2002; Gardent and Perez-Beltrachini, 2010) have not been evaluated on large corpora of arbitrarily long sentences such as provided by the surface realisation (SR) task (Belz et al., 2011). Recently, (Guo et al., 2011; Bohnet et al., 2011; Stent, 2011) have developed statistical dependency realisers that do not make use of an explicit grammar but use cascaded classifiers and n-gram models to map in SR input data to sentences. They obtain the best results in the SR task partly because, for grammar based systems, converting the provided input into the format expected by the grammar proved to be extremely difficult. The algorithm we propose departs from these approaches in that it is a grammar-based approach; it is optimised by combining parallel processing, top-down prediction and local bottom-up polarity filtering; and it was e"
C12-1124,C04-1044,0,0.0169404,"oach. On the one hand, rule selection is guided, as in the lexicalist approach, by the elementary units present in the input rather than by its structure – In this way, the logical form equivalence issue is avoided. On the other hand, the structure of the input is used to provide top-down guidance for the search and thereby restrict the combinatorics. To further improve efficiency, the algorithm integrates three additional optimisation techniques. From the lexicalist approach, it adapts two techniques designed to prune the search space, namely a so-called polarity filter on local input trees (Bonfante et al., 2004); and the use of a language model to prune competing intermediate substructures. In addition, the algorithm is parallelised to explore the possible completions of the top-down predictions simultaneously rather than sequentially. The algorithm was implemented using a Feature-Based Lexicalised Tree Adjoining Grammar for English and tested on the Generation Challenge Surface Realisation task data (Belz et al., 2011). We compare our algorithm with a baseline lexicalist approach which processes the input tree top 2028 down. The results show that the algorithm we propose drastically improves on the"
C12-1124,I05-1015,0,0.670205,", a grammar will associate with natural language expressions only one of these logically equivalent formula. Hence a generator will be able to produce the natural language expression E only when given the formula φ associated by the grammar with E . For all other formulae logically equivalent to φ , it will fail. Since, the problem of computing logical equivalence for first order logic is undecidable, the problem is quite deep. For flat semantic representations such as MRSs (Minimal Recursion Semantics, (Copestake et al., 2001)) on the other hand, lexicalist approaches (Espinosa et al., 2010; Carroll and Oepen, 2005; Gardent and Kow, 2005) have extensively been used because (i) they impose few constraints on the grammar thereby making it easier to maintain bi-directional grammars that can be used both for parsing and for generation; and (ii) the approach eschews the logical form equivalence problem – Since the semantic representations are unstructured, there is no requirement on the generator to mirror a semantic structure. One known drawback of lexicalist approaches however is that they generally lack efficiency. Indeed, previous work has shown that the high combinatorics of lexicalist approaches stem f"
C12-1124,P01-1019,0,0.0285512,"distinct formulae. For instance p ∧ q is logically equivalent to q ∧ p. In general though, a grammar will associate with natural language expressions only one of these logically equivalent formula. Hence a generator will be able to produce the natural language expression E only when given the formula φ associated by the grammar with E . For all other formulae logically equivalent to φ , it will fail. Since, the problem of computing logical equivalence for first order logic is undecidable, the problem is quite deep. For flat semantic representations such as MRSs (Minimal Recursion Semantics, (Copestake et al., 2001)) on the other hand, lexicalist approaches (Espinosa et al., 2010; Carroll and Oepen, 2005; Gardent and Kow, 2005) have extensively been used because (i) they impose few constraints on the grammar thereby making it easier to maintain bi-directional grammars that can be used both for parsing and for generation; and (ii) the approach eschews the logical form equivalence problem – Since the semantic representations are unstructured, there is no requirement on the generator to mirror a semantic structure. One known drawback of lexicalist approaches however is that they generally lack efficiency. I"
C12-1124,D10-1055,0,0.161244,"trFкA dsr  sADArZ “ Upr s  nFc  ” pr Ert шNd - sE&gt;яt trFкo к  mкAbl  u(pAdn smy кo a(yEDк GVA  dtA h {। Keywords: Generation, Tree Adjoining Grammar, Surface Realization. Keywords in Hindi: u(pAdn , v&quot; - sV - &yAкrZ , sth spAdn. Proceedings of COLING 2012: Technical Papers, pages 2027–2042, COLING 2012, Mumbai, December 2012. 2027 1 Introduction Depending on the type of semantic representation encoded by the grammar, two main types of algorithms have been proposed for generating sentences with bi-directional, unification-based grammars such as CCG (Combinatory Categorial Grammar, (Espinosa et al., 2010)), HPSG (HeadDriven Phrase Structure Grammar, (Carroll et al., 1999)) and TAG (Tree Adjoining Grammar, (Gardent and Kow, 2005)). For recursive semantic representations such as first-order logic formulae, head-driven algorithms (Shieber et al., 1990) have been argued to be best because they restrict the combinatorics inherent to bottom-up search; they avoid non termination by using lexical items to guide the search ; and they allow for semantically nonmonotonic grammars (i.e., grammars where the semantics of a rule at the left hand side need not be subsumed by the semantics of the rule at the r"
C12-1124,P08-1022,0,0.0573499,"hite’s system (White, 2004), the best paraphrase is determined on the basis of n-gram scores. To address the fact that there are n! ways to combine any n modifiers with a single constituent, (White, 2004) proposes to use a language model to prune the chart of identical edges representing different modifier permutations, e.g., to choose between fierce black cat and black fierce cat. Similarly, (Bangalore and Rambow, 2000) assumes a single derivation tree that encodes a word lattice (a {fierce black, black fierce} cat), and uses statistical knowledge to select the best linearisation. Recently, (Espinosa et al., 2008) adapted the supertagging techniques first proposed for parsing (Bangalore and Joshi, 1999) to surface realisation. Given a treebank in the appropriate format, this technique permits filtering the initial search space by using a model trained on that treebank. Supertagging was shown to improve the performance of symbolic parsers and generators significantly. However, it requires the existence of a treebank in a format appropriate to generate the supertagging model. In sum, various symbolic and statistical techniques have been developed to improve the efficiency of grammar-based surface realisa"
C12-1124,W05-1605,1,0.440197,"neration, Tree Adjoining Grammar, Surface Realization. Keywords in Hindi: u(pAdn , v&quot; - sV - &yAкrZ , sth spAdn. Proceedings of COLING 2012: Technical Papers, pages 2027–2042, COLING 2012, Mumbai, December 2012. 2027 1 Introduction Depending on the type of semantic representation encoded by the grammar, two main types of algorithms have been proposed for generating sentences with bi-directional, unification-based grammars such as CCG (Combinatory Categorial Grammar, (Espinosa et al., 2010)), HPSG (HeadDriven Phrase Structure Grammar, (Carroll et al., 1999)) and TAG (Tree Adjoining Grammar, (Gardent and Kow, 2005)). For recursive semantic representations such as first-order logic formulae, head-driven algorithms (Shieber et al., 1990) have been argued to be best because they restrict the combinatorics inherent to bottom-up search; they avoid non termination by using lexical items to guide the search ; and they allow for semantically nonmonotonic grammars (i.e., grammars where the semantics of a rule at the left hand side need not be subsumed by the semantics of the rule at the right hand side). One main issue with this approach however is the so-called logical form equivalence problem (Shieber, 1993)."
C12-1124,P07-1042,1,0.960212,"realisation. For HPSG, (Carroll and Oepen, 2005) present a bottom-up, lexicalist, surface realiser which uses a chart based strategy, subsumption-based local ambiguity factoring and a procedure to selectively unpack the generation forest according to a probability distribution given by a conditional, discriminative model. The algorithm is evaluated on the hike treebank, a collection of 330 sentences of instructional text taken from Norwegian tourism brochures with an average length of 12.8 words. Practical generation times average below or around one second for outputs of 15 words. For TAG, (Gardent and Kow, 2007) propose a three step surface realisation algorithm for FB-LTAG (Feature-Based Lexicalised Tree-Adjoining Grammar) where first, a so-called polarity filter is used to prune the initial search space second, substitution is applied to combine trees together and third, adjunction is applied. In essence, polarity filtering filters out combinations of FB-LTAG elementary trees which cover the input semantics but cannot yield a valid parse tree either because a syntactic requirement cannot be satisfied or because a syntactic resource cannot be used. In this way, the exponential impact of lexical ambi"
C12-1124,C10-1042,1,0.696013,"se tree either because a syntactic requirement cannot be satisfied or because a syntactic resource cannot be used. In this way, the exponential impact of lexical ambiguity can be reduced. Furthermore applying substitution before adjunction means that first a skeleton sentence is built before modifiers are adjoined. This permits reducing the combinatorics introduced by intersective modifiers as the multiple intermediate structures they may license do not propagate to the rest of the sentence tree. In practice however, evaluation is restricted to short input and the algorithm fails to scale up (Gardent and Perez-Beltrachini, 2010). (Koller and Striegnitz, 2002) present a surface realisation algorithm where (i) the XTAG FB-LTAG grammar (The XTAG Research Group, 2001) is converted to a dependency grammar capturing the derivation trees of XTAG and (ii) a constraint-based dependency parser is used to construct derivation trees from semantic representations. The parser used was specifically developed for the efficient parsing of free word order languages and is shown to efficiently handle both the lexical ambiguity and the lack of order information in the input that are characteristic of surface realisation from a flat sema"
C12-1124,W11-2833,0,0.0609029,"Missing"
C12-1124,P02-1003,0,0.709703,"irement cannot be satisfied or because a syntactic resource cannot be used. In this way, the exponential impact of lexical ambiguity can be reduced. Furthermore applying substitution before adjunction means that first a skeleton sentence is built before modifiers are adjoined. This permits reducing the combinatorics introduced by intersective modifiers as the multiple intermediate structures they may license do not propagate to the rest of the sentence tree. In practice however, evaluation is restricted to short input and the algorithm fails to scale up (Gardent and Perez-Beltrachini, 2010). (Koller and Striegnitz, 2002) present a surface realisation algorithm where (i) the XTAG FB-LTAG grammar (The XTAG Research Group, 2001) is converted to a dependency grammar capturing the derivation trees of XTAG and (ii) a constraint-based dependency parser is used to construct derivation trees from semantic representations. The parser used was specifically developed for the efficient parsing of free word order languages and is shown to efficiently handle both the lexical ambiguity and the lack of order information in the input that are characteristic of surface realisation from a flat semantics. The evaluation however i"
C12-1124,C96-2103,0,0.530266,"Missing"
C12-1124,W08-2319,0,0.110621,"developed for the efficient parsing of free word order languages and is shown to efficiently handle both the lexical ambiguity and the lack of order information in the input that are characteristic of surface realisation from a flat semantics. The evaluation however is restricted to a few hand constructed example inputs; and the grammar conversion ignores feature structure information. To address these shortcomings, (Gardent and Perez-Beltrachini, 2010) present an approach which makes use of the procedure for converting an FB-LTAG to a Feature-Based Regular Tree Grammar (FB-RTG) described in (Schmitz and Roux, 2008). Like in (Koller and Striegnitz, 2002), the initial FB-LTAG is converted to a grammar of its derivation trees. However in this case, the grammar conversion and the resulting feature-based RTGs accurately translates the full range of unification 2029 mechanisms employed in the initial FB-LTAG. An Earley, bottom-up algorithm is developed and the approach is tested on a large benchmark of artificially constructed examples illustrating different levels of linguistic complexity (different input lengths, different numbers of clauses and of modifiers). The approach is shown to outperform the algorit"
C12-1124,J93-1008,0,0.276547,"t and Kow, 2005)). For recursive semantic representations such as first-order logic formulae, head-driven algorithms (Shieber et al., 1990) have been argued to be best because they restrict the combinatorics inherent to bottom-up search; they avoid non termination by using lexical items to guide the search ; and they allow for semantically nonmonotonic grammars (i.e., grammars where the semantics of a rule at the left hand side need not be subsumed by the semantics of the rule at the right hand side). One main issue with this approach however is the so-called logical form equivalence problem (Shieber, 1993). A logic formula may have several logically equivalent but syntactically distinct formulae. For instance p ∧ q is logically equivalent to q ∧ p. In general though, a grammar will associate with natural language expressions only one of these logically equivalent formula. Hence a generator will be able to produce the natural language expression E only when given the formula φ associated by the grammar with E . For all other formulae logically equivalent to φ , it will fail. Since, the problem of computing logical equivalence for first order logic is undecidable, the problem is quite deep. For f"
C12-1124,J90-1004,0,0.55773,"Missing"
C12-1124,W11-2834,0,0.0199333,"sum, various symbolic and statistical techniques have been developed to improve the efficiency of grammar-based surface realisation. However, statistical systems using supertagging require the existence of a treebank in an appropriate format while the purely symbolic systems described in (Carroll and Oepen, 2005; Gardent and Kow, 2005; Koller and Striegnitz, 2002; Gardent and Perez-Beltrachini, 2010) have not been evaluated on large corpora of arbitrarily long sentences such as provided by the surface realisation (SR) task (Belz et al., 2011). Recently, (Guo et al., 2011; Bohnet et al., 2011; Stent, 2011) have developed statistical dependency realisers that do not make use of an explicit grammar but use cascaded classifiers and n-gram models to map in SR input data to sentences. They obtain the best results in the SR task partly because, for grammar based systems, converting the provided input into the format expected by the grammar proved to be extremely difficult. The algorithm we propose departs from these approaches in that it is a grammar-based approach; it is optimised by combining parallel processing, top-down prediction and local bottom-up polarity filtering; and it was evaluated on a"
C12-1124,C88-2147,0,0.738259,"serts an auxiliary tree into a tree. Derivation in an FB-LTAG yields two trees: a derived tree which is, like for context free grammars, the tree produced by combining the grammar rules (here, the elementary trees) licensed by the input; and a derivation tree which indicates how 2031 the derived tree was built i.e., which elementary trees were used and how they were combined. Figure 3 show the derived and derivation trees associated by the grammar shown in Figure 2 with the sentence “Which fruit has John eaten?”. For a detailed presentation of the FB-LTAG formalism, the reader is referred to (Vijay-Shanker and Joshi, 1988). 4.2 FB-RTG As shown in (Koller and Striegnitz, 2002; Gardent and Perez-Beltrachini, 2010), processing the derivation trees of a given FB-LTAG rather than its derived trees is more efficient. Following (Gardent and Perez-Beltrachini, 2010), we therefore use not the initial FB-LTAG described in the previous section, but the FB-RTG grammar of derivation trees that can be derived from it. That is, the surface realisation algorithm first builds a derivation tree. The generated sentence is then extracted from the derived tree1 which can be reconstructed from this derivation tree using the original"
C12-1124,W00-2004,0,\N,Missing
C18-1115,P17-1183,0,0.113652,"models), while for general string transduction problems, one needs models such as encoder-decoders or grammatical models. Such expressive models, on the other hand, are not a good fit for string transduction problems with a strong notion of locality, as they are too complex and lead to weaker generalization power for such problems (Rastogi et al., 2016). We show this weakness in our experiments with seq2seq models supporting similar conclusions in previous work such as by Schnober et al. (2016). Our experiments show that even when seq2seq models are incorporated with hard monotonic attention (Aharoni and Goldberg, 2017), our reduction to sequence labeling outperforms such models on certain problems with a strong notion of locality. Our approach to reduce string transduction to sequence labeling relies on a simple observation: in most cases in transduction, it is easier to delete a symbol than to insert a symbol. This is true because insertion requires identifying both the point of insertion and the character that needs to be inserted, while deletion is a “binary” decision – either the decoding algorithm chooses to delete a specific symbol or not to. A similar observation was made by Schnober et al. (2016), R"
C18-1115,P05-1022,0,0.0769671,"n placeholders that are deleted during decoding. In contrast to the other datasets, where perhaps the assumption of monotonicity is too strong, seq2seq models with an attention mechanism designed to handle monotonic alignments (Aharoni and Goldberg, 2017) perform quite well on this task, even better than the vanilla seq2seq models. Finally, we also consider the case in which we use an ensemble method, combining several spectral models together (the top 50 performing models on the development set from the hyperparameter sweep). We combine the models using a MaxEnt reranker such as described by Charniak and Johnson (2005) and Narayan and Cohen (2015). We find that the ensemble approach does improve the results significantly for the 13SIA dataset, and also for the 2PKE-z dataset. 5 Conclusion We presented a technique to frame general string transduction problems as sequence labeling. Our technique works by adding to the string to be transduced additional insertion markers, which are later potentially deleted during the sequence labeling process. Our approach is general and works with any sequence labeling algorithm. We developed our technique with conditional random fields, refinement hidden Markov models and n"
C18-1115,P12-1024,1,0.823958,"ng and then remove the D symbols from the string. 3.2 Sequence Labeling Models We experiment with three models for sequence labeling: refinement hidden Markov models (R-HMMs; Stratos et al., 2013), conditional random fields (CRFs; Lafferty et al., 2001) and bidirectional Long Short Term Memory neural networks (biLSTMs; Graves and Schmidhuber, 2005). A graphical depiction of the models that we experiment with is given in Figure 1. For R-HMMs, we experiment with two learning algorithms, the expectation-maximization algorithm (EM; Dempster et al., 1977) and an L-PCFG spectral learning algorithm (Cohen et al., 2012; Cohen et al., 2013).3 4 Experiments We describe in this section a set of experiments on datasets from three problems: social media spelling correction, optical character recognition (OCR) correction and morphological inflection. We believe that this array of datasets represents a broad set of problems in which string transduction as sequence 3 We use the code from https://github.com/shashiongithub/Rainbow-Parser. 1363 labeling can be tested. We apply the insertion technique to a set of sequence labelers: conditional random fields, refinement HMMs with expectation-maximization, spectral algor"
C18-1115,N13-1015,1,0.800426,"he D symbols from the string. 3.2 Sequence Labeling Models We experiment with three models for sequence labeling: refinement hidden Markov models (R-HMMs; Stratos et al., 2013), conditional random fields (CRFs; Lafferty et al., 2001) and bidirectional Long Short Term Memory neural networks (biLSTMs; Graves and Schmidhuber, 2005). A graphical depiction of the models that we experiment with is given in Figure 1. For R-HMMs, we experiment with two learning algorithms, the expectation-maximization algorithm (EM; Dempster et al., 1977) and an L-PCFG spectral learning algorithm (Cohen et al., 2012; Cohen et al., 2013).3 4 Experiments We describe in this section a set of experiments on datasets from three problems: social media spelling correction, optical character recognition (OCR) correction and morphological inflection. We believe that this array of datasets represents a broad set of problems in which string transduction as sequence 3 We use the code from https://github.com/shashiongithub/Rainbow-Parser. 1363 labeling can be tested. We apply the insertion technique to a set of sequence labelers: conditional random fields, refinement HMMs with expectation-maximization, spectral algorithms, and neural net"
C18-1115,P14-2102,0,0.0217992,"orks best for problems in which string transduction imposes a strong notion of locality (no long range dependencies). We experiment with spelling correction for social media, OCR correction, and morphological inflection, and we see that it behaves better than seq2seq models and yields state-of-the-art results in several cases. 1 Introduction String transduction (mapping one string to another) is an essential ingredient in the natural language processing (NLP) toolkit that helps solve problems ranging from morphological inflection (Dreyer et al., 2008) and lemmatization to spelling correction (Cotterell et al., 2014), text normalization (Porta and Sancho, 2013) and machine translation (Kumar et al., 2006). Finite-state technology is often used with string transduction (Allauzen et al., 2007), especially when there is a strong notion of locality in the transduction problem, i.e., when there are no long range dependencies between the predicted characters. More recently, neural network methods have become more common for such problems using the seq2seq models that were introduced for machine translation (Bahdanau et al., 2015). Similarly, sequence labeling algorithms have been the mainstay for an array of pr"
C18-1115,D08-1113,0,0.0414885,"roach can be used with any sequence labeling algorithm and it works best for problems in which string transduction imposes a strong notion of locality (no long range dependencies). We experiment with spelling correction for social media, OCR correction, and morphological inflection, and we see that it behaves better than seq2seq models and yields state-of-the-art results in several cases. 1 Introduction String transduction (mapping one string to another) is an essential ingredient in the natural language processing (NLP) toolkit that helps solve problems ranging from morphological inflection (Dreyer et al., 2008) and lemmatization to spelling correction (Cotterell et al., 2014), text normalization (Porta and Sancho, 2013) and machine translation (Kumar et al., 2006). Finite-state technology is often used with string transduction (Allauzen et al., 2007), especially when there is a strong notion of locality in the transduction problem, i.e., when there are no long range dependencies between the predicted characters. More recently, neural network methods have become more common for such problems using the seq2seq models that were introduced for machine translation (Bahdanau et al., 2015). Similarly, sequ"
C18-1115,P02-1001,0,0.112997,"e between the two, as string transduction can re-write a string into a completely different string, while sequence labeling has a stronger notion of locality. As such, sequence labeling is considered an easier problem than general string transduction. Yet, we show in this paper how to exploit sequence labeling algorithms, with their flexibility and efficiency, to do general string transduction. 1361 3 Transduction as Insertion and Labeling Most approaches to string transduction involve inducing an alignment between symbols in the input and output strings (Knight and Graehl, 1998; Clark, 2001; Eisner, 2002; Azawi et al., 2013; Bailly et al., 2013). In an alignment, unaligned input symbols are called deletions, while unaligned output symbols are called insertions. It is challenging to jointly induce alignments and learn a transduction model. A second challenge is that, at prediction time, it is difficult to predict the insertions, as there can be an arbitrary number of them between any two input symbols. The prediction problem would be much simpler if the insertion positions were in place, because the model would only need to decide which symbol goes in each position. Our approach is based on th"
C18-1115,P16-2090,0,0.0418981,"Missing"
C18-1115,D15-1214,1,0.833557,"during decoding. In contrast to the other datasets, where perhaps the assumption of monotonicity is too strong, seq2seq models with an attention mechanism designed to handle monotonic alignments (Aharoni and Goldberg, 2017) perform quite well on this task, even better than the vanilla seq2seq models. Finally, we also consider the case in which we use an ensemble method, combining several spectral models together (the top 50 performing models on the development set from the hyperparameter sweep). We combine the models using a MaxEnt reranker such as described by Charniak and Johnson (2005) and Narayan and Cohen (2015). We find that the ensemble approach does improve the results significantly for the 13SIA dataset, and also for the 2PKE-z dataset. 5 Conclusion We presented a technique to frame general string transduction problems as sequence labeling. Our technique works by adding to the string to be transduced additional insertion markers, which are later potentially deleted during the sequence labeling process. Our approach is general and works with any sequence labeling algorithm. We developed our technique with conditional random fields, refinement hidden Markov models and neural networks. We tested our"
C18-1115,N16-1076,0,0.0888006,"he sentence in the target language can be shorter, longer, and contain a significant amount of re-ordering. Indeed, re-ordering is a challenge with string transduction. A sequence labeling model usually maintains higher order of monotonicity (such as with Markovian models), while for general string transduction problems, one needs models such as encoder-decoders or grammatical models. Such expressive models, on the other hand, are not a good fit for string transduction problems with a strong notion of locality, as they are too complex and lead to weaker generalization power for such problems (Rastogi et al., 2016). We show this weakness in our experiments with seq2seq models supporting similar conclusions in previous work such as by Schnober et al. (2016). Our experiments show that even when seq2seq models are incorporated with hard monotonic attention (Aharoni and Goldberg, 2017), our reduction to sequence labeling outperforms such models on certain problems with a strong notion of locality. Our approach to reduce string transduction to sequence labeling relies on a simple observation: in most cases in transduction, it is easier to delete a symbol than to insert a symbol. This is true because insertio"
C18-1115,C16-1160,0,0.34387,"with string transduction. A sequence labeling model usually maintains higher order of monotonicity (such as with Markovian models), while for general string transduction problems, one needs models such as encoder-decoders or grammatical models. Such expressive models, on the other hand, are not a good fit for string transduction problems with a strong notion of locality, as they are too complex and lead to weaker generalization power for such problems (Rastogi et al., 2016). We show this weakness in our experiments with seq2seq models supporting similar conclusions in previous work such as by Schnober et al. (2016). Our experiments show that even when seq2seq models are incorporated with hard monotonic attention (Aharoni and Goldberg, 2017), our reduction to sequence labeling outperforms such models on certain problems with a strong notion of locality. Our approach to reduce string transduction to sequence labeling relies on a simple observation: in most cases in transduction, it is easier to delete a symbol than to insert a symbol. This is true because insertion requires identifying both the point of insertion and the character that needs to be inserted, while deletion is a “binary” decision – either t"
C18-1115,W16-2406,0,0.274929,"Missing"
C18-1115,W13-3507,1,0.894402,"nd the transformation of the final y to i to accommodate the new suffix). For example, the past tense of may is also may, and in this case, if the string may was also in the training set, may should retain its form and be transduced to may. With the context function mentioned above we would have the pair mayε (input sequence), mayD (output sequence) to learn from. During decoding, we apply the σ function on the input string and then remove the D symbols from the string. 3.2 Sequence Labeling Models We experiment with three models for sequence labeling: refinement hidden Markov models (R-HMMs; Stratos et al., 2013), conditional random fields (CRFs; Lafferty et al., 2001) and bidirectional Long Short Term Memory neural networks (biLSTMs; Graves and Schmidhuber, 2005). A graphical depiction of the models that we experiment with is given in Figure 1. For R-HMMs, we experiment with two learning algorithms, the expectation-maximization algorithm (EM; Dempster et al., 1977) and an L-PCFG spectral learning algorithm (Cohen et al., 2012; Cohen et al., 2013).3 4 Experiments We describe in this section a set of experiments on datasets from three problems: social media spelling correction, optical character recogn"
D15-1214,H91-1060,0,0.233311,"Missing"
D15-1214,W08-2102,0,0.0721588,"Missing"
D15-1214,P05-1022,0,0.702038,"PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all"
D15-1214,D15-1160,0,0.0117897,"German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniques to exploit diver"
D15-1214,P14-1099,1,0.867448,"e foot of the outside tree to the root of the tree which is different from the head node of the foot node. • The width of the spans to the left and to the right of the foot node, paired with the label of the foot node. Other Spectral Algorithms The SVD step on the Ωa matrix is pivotal to many algorithms, and has been used in the past for other L-PCFG estimation algorithms. Cohen et al. (2012) used it for developing a spectral algorithm that identifies the parameters of the L-PCFG up to a linear transformation. Their algorithm generalizes the work of Hsu et al. (2009) and Bailly et al. (2010). Cohen and Collins (2014) also developed an algorithm that makes use of an SVD step on the inside-outside. It relies on the idea of “pivot features” – features that uniquely identify latent states. Louis and Cohen (2015) used a clustering algorithm that resembles ours but does not separate inside trees from outside trees or follows up with a singular value decomposition step. Their algorithm was applied to both L-PCFGs and linear context-free rewriting systems. Their application was the analysis of hierarchical structure of conversations in online forums. In our preliminary experiments, we found out that the clusterin"
D15-1214,P12-1024,1,0.948709,"arsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate multiple models, where the underlying features are perturbed with several perturbation schemes. Each such model, during test time, yields a single parse, and all parses are then used together in several ways to select a single best parse. The main contributions of this paper are twofold. First, we present an algorithm for estimating L-PCFGs, akin to the spectral algorithm of Cohen et al. (2012), but simpler to understand and implement. This algorithm has value for readers who are interested in learning more about spectral algorithms – it demonstrates some of the core ideas in spectral learning in a rather intuitive way. In addition, this algorithm leads to sparse grammar estimates and compact models. Second, we describe how a diverse set of predictors can be used with spectral learning techniques. 1868 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1868–1878, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational L"
D15-1214,N13-1015,1,0.916617,"in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petr"
D15-1214,J03-4003,0,0.671112,"Missing"
D15-1214,P05-1039,0,0.0765088,"Missing"
D15-1214,N09-2064,0,0.141107,"experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More"
D15-1214,P96-1024,0,0.477791,",” and rely on the marginals that each model gives. Decoding with Multiple Models Let G1 , . . . , Gp be a set of L-PCFG grammars. In §6, we create such models using the noising techniques described above. The question that remains is how to combine these models together to get a single best output parse tree given an input sentence. With L-PCFGs, decoding a single sentence requires marginalizing out the latent states to find the best skeletal tree2 for a given string. Let s be a sentence. We define t(Gi , s) to be the output tree according to minimum Bayes risk decoding. This means we follow Goodman (1996), who uses dynamic programming to compute the tree that maximizes the sum of all marginals of all nonterminals in the output tree. Each marginal, for each span ha, i, ji (where a is a nonterminal and i and j are endpoints in the sentence), is computed by using the inside-outside algorithm. p X X t∗ = arg max MaxEnt reranking: We train a MaxEnt reranker on a training set that includes outputs from multiple models, and then, during testing time, decode with each of the models, and use the trained reranker to select one of the parses. We use the reranker of Charniak and Johnson (2005).3 As we see"
D15-1214,W99-0623,0,0.464086,"ree ways to decode with multiple models. In addition, we describe a simple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information fro"
D15-1214,D10-1004,0,0.10846,"Missing"
D15-1214,P05-1010,0,0.573618,"techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate multiple models, where the underlying features are perturbed with several perturbation schemes. Each such model, during test time, yields a single parse, and all parses are then used toget"
D15-1214,N15-1076,0,0.0159453,"ormation from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single m"
D15-1214,N07-1051,0,0.0308175,"2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate multiple models, where the underlying features are perturbed with several perturbation schemes. Each such model, during test time, yields a single parse, and all parses are then used together in several ways to select a single best parse. The main contributions of this paper are twofold. First, we present an algorithm for estimating L-PCFGs, akin to the spectral algorithm of Cohen et al. (2012), but simpler to understand and implement. This algorithm has value for readers who are interested in learning more a"
D15-1214,P06-1055,0,0.293158,"nts have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate multiple models, where the underlying features are perturbed with several perturbation schemes. Each such model, during test time, yields a single parse, and all parses are then used together in several ways to select a single best parse. The main contributions of this paper are twofold. First, we present an algorithm for"
D15-1214,N10-1003,0,0.763531,"r English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniq"
D15-1214,N15-1058,0,0.0639382,"Missing"
D15-1214,N06-2033,0,0.0654012,"to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made fo"
D15-1214,P12-1046,0,0.0877184,"Missing"
D15-1214,D15-1178,1,0.77639,"bel of the foot node. Other Spectral Algorithms The SVD step on the Ωa matrix is pivotal to many algorithms, and has been used in the past for other L-PCFG estimation algorithms. Cohen et al. (2012) used it for developing a spectral algorithm that identifies the parameters of the L-PCFG up to a linear transformation. Their algorithm generalizes the work of Hsu et al. (2009) and Bailly et al. (2010). Cohen and Collins (2014) also developed an algorithm that makes use of an SVD step on the inside-outside. It relies on the idea of “pivot features” – features that uniquely identify latent states. Louis and Cohen (2015) used a clustering algorithm that resembles ours but does not separate inside trees from outside trees or follows up with a singular value decomposition step. Their algorithm was applied to both L-PCFGs and linear context-free rewriting systems. Their application was the analysis of hierarchical structure of conversations in online forums. In our preliminary experiments, we found out that the clustering algorithm by itself performs worse than the spectral algorithm of Cohen et al. (2013). We believe that the reason is two-fold: (a) k-means finds a local maximum during clustering; (b) we do har"
D15-1214,A97-1014,0,0.175074,"Missing"
D15-1214,N15-1028,0,0.0242867,"utions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate"
D15-1214,E12-1042,0,0.0554628,"anking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berke"
D15-1214,D07-1105,0,0.0158895,"ple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use"
D15-1214,J93-2004,0,0.050059,"Missing"
D15-1214,J01-2002,0,0.0406511,"Missing"
D15-1214,D13-1117,0,0.105259,"spectral learning in a rather intuitive way. In addition, this algorithm leads to sparse grammar estimates and compact models. Second, we describe how a diverse set of predictors can be used with spectral learning techniques. 1868 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1868–1878, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Our approach relies on adding noise to the feature functions that help the spectral algorithm compute the latent states. Our noise schemes are similar to those described by Wang et al. (2013). We add noise to the whole training data, then train a model using our algorithm (or other spectral algorithms; Cohen et al., 2013), and repeat this process multiple times. We then use the set of parses we get from all models in a recombination step. The rest of the paper is organized as follows. In §2 we describe notation and background about L-PCFG parsing. In §3 we describe our new spectral algorithm for estimating L-PCFGs. It is based on similar intuitions as older spectral algorithms for L-PCFGs. In §4 we describe the various noise schemes we use with our spectral algorithm and the spect"
D15-1214,D09-1161,0,0.0772842,"language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we de"
D17-1064,C10-1037,0,0.0810355,"Missing"
D17-1064,D15-1042,0,0.0886682,"Missing"
D17-1064,P05-1074,0,0.0497637,"B, UK ‡ CNRS, LORIA, UMR 7503, Vandoeuvre-l`es-Nancy, F-54500, France shashi.narayan@ed.ac.uk claire.gardent@loria.fr scohen@inf.ed.ac.uk anastasia.shimorina@loria.fr Abstract Strube, 2008; Pitler, 2010; Filippova et al., 2015; Toutanova et al., 2016). Sentence fusion consists of combining two or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sent"
D17-1064,W08-1105,0,0.143307,"Missing"
D17-1064,P01-1008,0,0.114122,"ton Street, Edinburgh EH8 9AB, UK ‡ CNRS, LORIA, UMR 7503, Vandoeuvre-l`es-Nancy, F-54500, France shashi.narayan@ed.ac.uk claire.gardent@loria.fr scohen@inf.ed.ac.uk anastasia.shimorina@loria.fr Abstract Strube, 2008; Pitler, 2010; Filippova et al., 2015; Toutanova et al., 2016). Sentence fusion consists of combining two or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operatio"
D17-1064,D11-1108,0,0.0392016,"Missing"
D17-1064,P16-2055,0,0.0228674,"Missing"
D17-1064,E99-1042,0,0.232209,"(Rephr.) and meaning preserving (MPre.) operations (Y: yes, N: No, ?Y: should do but most existing approaches do not). ing step which facilitates and improves the performance of parsers (Tomita, 1985; Chandrasekar and Srinivas, 1997; McDonald and Nivre, 2011; Jel´ınek, 2014), semantic role labelers (Vickrey and Koller, 2008) and statistical machine translation (SMT) systems (Chandrasekar et al., 1996). In addition, because it allows the conversion of longer sentences into shorter ones, it should also be of use for people with reading disabilities (Inui et al., 2003) such as aphasia patients (Carroll et al., 1999), low-literacy readers (Watanabe et al., 2009), language learners (Siddharthan, 2002) and children (De Belder and Moens, 2010). 2 Related Work We briefly review previous work on sentence splitting and rephrasing. Sentence Splitting. Of the four sentence rewriting tasks (paraphrasing, fusion, compression and simplification) mentioned above, only sentence simplification involves sentence splitting. Most simplification methods learn a statistical model (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014) from the parallel dataset"
D17-1064,P17-1017,1,0.850809,"Missing"
D17-1064,C96-2183,0,0.729533,"kes the input complex sentence into account when generating while the other does not. Table 1: Similarities and differences between sentence rewriting tasks with respect to splitting (Split), deletion (Delete), rephrasing (Rephr.) and meaning preserving (MPre.) operations (Y: yes, N: No, ?Y: should do but most existing approaches do not). ing step which facilitates and improves the performance of parsers (Tomita, 1985; Chandrasekar and Srinivas, 1997; McDonald and Nivre, 2011; Jel´ınek, 2014), semantic role labelers (Vickrey and Koller, 2008) and statistical machine translation (SMT) systems (Chandrasekar et al., 1996). In addition, because it allows the conversion of longer sentences into shorter ones, it should also be of use for people with reading disabilities (Inui et al., 2003) such as aphasia patients (Carroll et al., 1999), low-literacy readers (Watanabe et al., 2009), language learners (Siddharthan, 2002) and children (De Belder and Moens, 2010). 2 Related Work We briefly review previous work on sentence splitting and rephrasing. Sentence Splitting. Of the four sentence rewriting tasks (paraphrasing, fusion, compression and simplification) mentioned above, only sentence simplification involves sent"
D17-1064,W03-1602,0,0.291741,"itting (Split), deletion (Delete), rephrasing (Rephr.) and meaning preserving (MPre.) operations (Y: yes, N: No, ?Y: should do but most existing approaches do not). ing step which facilitates and improves the performance of parsers (Tomita, 1985; Chandrasekar and Srinivas, 1997; McDonald and Nivre, 2011; Jel´ınek, 2014), semantic role labelers (Vickrey and Koller, 2008) and statistical machine translation (SMT) systems (Chandrasekar et al., 1996). In addition, because it allows the conversion of longer sentences into shorter ones, it should also be of use for people with reading disabilities (Inui et al., 2003) such as aphasia patients (Carroll et al., 1999), low-literacy readers (Watanabe et al., 2009), language learners (Siddharthan, 2002) and children (De Belder and Moens, 2010). 2 Related Work We briefly review previous work on sentence splitting and rephrasing. Sentence Splitting. Of the four sentence rewriting tasks (paraphrasing, fusion, compression and simplification) mentioned above, only sentence simplification involves sentence splitting. Most simplification methods learn a statistical model (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Naray"
D17-1064,C08-1018,0,0.0473474,"e and make available a benchmark consisting of 1,066,115 tuples mapping a single complex sentence to a sequence of sentences expressing the same meaning.1 Second, we propose five models (vanilla sequence-to-sequence to semantically-motivated models) to understand the difficulty of the proposed task. 1 Introduction Several sentence rewriting operations have been extensively discussed in the literature: sentence compression, multi-sentence fusion, sentence paraphrasing and sentence simplification. Sentence compression rewrites an input sentence into a shorter paraphrase (Knight and Marcu, 2000; Cohn and Lapata, 2008; Filippova and 1 The Split-and-Rephrase dataset is available here: https://github.com/shashiongithub/ Split-and-Rephrase. 606 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 606–616 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Compression Fusion Paraphrasing Simplification Split-and-Rephrase Split N N N Y Y Delete Y Y N Y N Rephr. ?Y Y Y Y Y MPre. N ?Y Y N Y ing. This allows for the learning of semanticallyinformed models (cf. Section 5). Our second contribution is to provide five models to understand"
D17-1064,jelinek-2014-improvements,0,0.112961,"Missing"
D17-1064,W11-1601,0,0.0946978,"Missing"
D17-1064,W04-1016,0,0.0257937,"Missing"
D17-1064,D15-1166,0,0.0302707,"Missing"
D17-1064,E17-1083,0,0.0153724,"-54500, France shashi.narayan@ed.ac.uk claire.gardent@loria.fr scohen@inf.ed.ac.uk anastasia.shimorina@loria.fr Abstract Strube, 2008; Pitler, 2010; Filippova et al., 2015; Toutanova et al., 2016). Sentence fusion consists of combining two or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sentence splitting and deletion. We propose a new"
D17-1064,N06-2009,0,0.107081,"Missing"
D17-1064,P14-5010,0,0.00544315,"Missing"
D17-1064,J11-1007,0,0.0187072,"les) of the complex sentence into smaller units and then generate a text for each RDF subset in that partition. One model is multi-source and takes the input complex sentence into account when generating while the other does not. Table 1: Similarities and differences between sentence rewriting tasks with respect to splitting (Split), deletion (Delete), rephrasing (Rephr.) and meaning preserving (MPre.) operations (Y: yes, N: No, ?Y: should do but most existing approaches do not). ing step which facilitates and improves the performance of parsers (Tomita, 1985; Chandrasekar and Srinivas, 1997; McDonald and Nivre, 2011; Jel´ınek, 2014), semantic role labelers (Vickrey and Koller, 2008) and statistical machine translation (SMT) systems (Chandrasekar et al., 1996). In addition, because it allows the conversion of longer sentences into shorter ones, it should also be of use for people with reading disabilities (Inui et al., 2003) such as aphasia patients (Carroll et al., 1999), low-literacy readers (Watanabe et al., 2009), language learners (Siddharthan, 2002) and children (De Belder and Moens, 2010). 2 Related Work We briefly review previous work on sentence splitting and rephrasing. Sentence Splitting. Of th"
D17-1064,I13-1198,0,0.0149176,"i Narayan† Claire Gardent‡ Shay B. Cohen† Anastasia Shimorina‡ School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB, UK ‡ CNRS, LORIA, UMR 7503, Vandoeuvre-l`es-Nancy, F-54500, France shashi.narayan@ed.ac.uk claire.gardent@loria.fr scohen@inf.ed.ac.uk anastasia.shimorina@loria.fr Abstract Strube, 2008; Pitler, 2010; Filippova et al., 2015; Toutanova et al., 2016). Sentence fusion consists of combining two or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all f"
D17-1064,N10-1044,0,0.0303032,"Missing"
D17-1064,P14-1041,1,0.893088,"consists of combining two or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sentence splitting and deletion. We propose a new sentence simplification task, which we dub Split-and-Rephrase, where the goal is to split a complex input sentence into shorter sentences while preserving meaning. In that task, the emphasis is on sentence splitting"
D17-1064,W16-6620,1,0.862186,"with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sentence splitting and deletion. We propose a new sentence simplification task, which we dub Split-and-Rephrase, where the goal is to split a complex input sentence into shorter sentences while preserving meaning. In that task, the emphasis is on sentence splitting and rephrasing. There is no deletion and no"
D17-1064,D16-1033,0,0.0899926,"Missing"
D17-1064,W16-6625,1,0.901333,"Missing"
D17-1064,P08-1040,0,0.140872,"text for each RDF subset in that partition. One model is multi-source and takes the input complex sentence into account when generating while the other does not. Table 1: Similarities and differences between sentence rewriting tasks with respect to splitting (Split), deletion (Delete), rephrasing (Rephr.) and meaning preserving (MPre.) operations (Y: yes, N: No, ?Y: should do but most existing approaches do not). ing step which facilitates and improves the performance of parsers (Tomita, 1985; Chandrasekar and Srinivas, 1997; McDonald and Nivre, 2011; Jel´ınek, 2014), semantic role labelers (Vickrey and Koller, 2008) and statistical machine translation (SMT) systems (Chandrasekar et al., 1996). In addition, because it allows the conversion of longer sentences into shorter ones, it should also be of use for people with reading disabilities (Inui et al., 2003) such as aphasia patients (Carroll et al., 1999), low-literacy readers (Watanabe et al., 2009), language learners (Siddharthan, 2002) and children (De Belder and Moens, 2010). 2 Related Work We briefly review previous work on sentence splitting and rephrasing. Sentence Splitting. Of the four sentence rewriting tasks (paraphrasing, fusion, compression a"
D17-1064,P02-1040,0,0.0989634,"Missing"
D17-1064,W04-3219,0,0.196979,"Missing"
D17-1064,D11-1038,0,0.13372,", 2015; Toutanova et al., 2016). Sentence fusion consists of combining two or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sentence splitting and deletion. We propose a new sentence simplification task, which we dub Split-and-Rephrase, where the goal is to split a complex input sentence into shorter sentences while preserving meaning. In"
D17-1064,P02-1006,0,0.0227466,"Missing"
D17-1064,P12-1107,0,0.137343,"Missing"
D17-1064,D15-1044,0,0.0384958,"Missing"
D17-1064,W10-4223,0,0.0690832,"Missing"
D17-1064,P15-1152,0,0.0512972,"Missing"
D17-1064,Q15-1021,0,0.06186,"or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sentence splitting and deletion. We propose a new sentence simplification task, which we dub Split-and-Rephrase, where the goal is to split a complex input sentence into shorter sentences while preserving meaning. In that task, the emphasis is on sentence splitting and rephrasing."
D17-1064,Q16-1029,0,0.184709,"Missing"
D17-1064,W10-4213,0,0.073206,"Missing"
D17-1064,W11-2802,0,0.0930787,"Missing"
D17-1064,D17-1062,0,0.0548473,"on content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sentence splitting and deletion. We propose a new sentence simplification task, which we dub Split-and-Rephrase, where the goal is to split a complex input sentence into shorter sentences while preserving meaning. In that task, the emphasis is on sentence splitting and rephrasing. There is no deletion and no lexical or phrasal simpl"
D17-1064,E14-1076,0,0.254892,"Missing"
D17-1064,P08-1089,0,0.0467264,"Missing"
D17-1064,C04-1129,0,0.059261,"Strube, 2008; Pitler, 2010; Filippova et al., 2015; Toutanova et al., 2016). Sentence fusion consists of combining two or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sentence splitting and deletion. We propose a new sentence simplification task, which we dub Split-and-Rephrase, where the goal is to split a complex input sentence into sh"
D17-1064,C10-1152,0,0.335065,"; Filippova et al., 2015; Toutanova et al., 2016). Sentence fusion consists of combining two or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sentence splitting and deletion. We propose a new sentence simplification task, which we dub Split-and-Rephrase, where the goal is to split a complex input sentence into shorter sentences wh"
D17-1064,N16-1004,0,0.034648,"Missing"
D17-1064,P07-2009,0,\N,Missing
D18-1001,P15-1073,0,0.036556,"Missing"
D18-1001,P16-2096,0,0.0202405,"privacy (Dwork, 2006) provides privacy guarantees for the problem of releasing information without compromising confidential data, and usually involves adding noise in the released information. It has been applied to the training of deep learning models (Abadi et al., 2016; Papernot et al., 2016; Papernot et al., 2018), and Bayesian topic models (Schein et al., 2018). The notion of privacy is particularly crucial to NLP, since it deals with textual data, oftentimes user-generated data, that contain a lot of private information. For example, textual data contain a lot of signal about authors (Hovy and Spruit, 2016). and can be leveraged to predict demographic variables (Rosenthal and McKeown, 2011; Preot¸iucPietro et al., 2015). Oftentimes, this information is not explicit in the text but latent and related to the usage of various linguistic traits. Our work is based on a stronger hypothesis: this latent information is still present in vectorial representations of texts, even if the representations have not been supervised by these latent variables. Li et al. (2017) study the privacy of unsupervised representations of images, and measures their privacy with the peak signal to noise ratio between an orig"
D18-1001,P11-1077,0,0.180648,"type of attack on neural representations: an attacker eavesdrops on the hidden representations of novel input examples (that are not in the training set) and tries to recover information about the content of the input text (Figure 1). A typical scenario where such attacks would occur is when the computation of a deep neural net Private information can take the form of key phrases explicitly contained in the text. However, it can also be implicit. For example, demographic information about the author of a text can be predicted with above chance accuracy from linguistic cues in the text itself (Rosenthal and McKeown, 2011; Preot¸iuc-Pietro et al., 2015). Independently of its explicitness, some of this private information correlates with the output labels, and therefore will be learned by the network. In such a case, there is a tradeoff between the utility of the representation (measured by the accuracy of the network) and its privacy. It might be 1 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1–10 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics latent representation of x used by the main classifier. We illustrat"
D18-1001,P18-2005,0,0.0566168,", since it is also 8 References If an attacker has access to e.g. a trained language model, they are likely to be able to generate sentences from the training set, since the language model is trained to assign high probabilities to those sentences. Such memorization is problematic when the training data contains private information and personal data. The experimental setting we explore is different from these works: we assume that the attacker has access to a hidden layer of the network and tries to recover information about an input example that is not in the training set. In a recent study, Li et al. (2018) proposed a method based on GAN designed to improve the robustness and privacy of neural representations, applied to part-of-speech tagging and sentiment analysis. They use a training scheme with two agents similar to our multidetasking strategy (Section 3.1.1), and found that it made neural representations more robust and accurate. However, they only use a single adversary to alter the training of the main model and to evaluate the privacy of the representations, with the risk of overestimating privacy. In contrast, once the parameters of our main model are fixed, we train a new classifier fr"
D18-1001,D16-1058,0,0.0389437,"ase 2. Generation of a dataset of pairs (r(x), z) for the attacker, r is the representation function of the main classifier (r is defined in Section 2.1); Phase 3. Training of the attacker’s network and evaluation of its performance for measuring privacy. In the remainder of this section, we describe the main classifier (Section 2.1), and the attacker’s model (Section 2.2). 2.1 As our base model, we chose a standard LSTM architecture (Hochreiter and Schmidhuber, 1997) for sequence classification. LSTM-based architectures have been applied to many NLP tasks, including sentiment classification (Wang et al., 2016) and text classification (Zhou et al., 2016). First, an LSTM encoder computes a fixed-size representation r(x) from a sequence of tokens x = (x1 , x2 , . . . , xn ) projected to an embedding space. We use θ r to denote the parameters used to construct r. They include the parameters of the LSTM, as well as the word embeddings. Then, the encoder output r(x) is fed as input to a feedforward network with parameters θ p that predicts the label y of the text, with a softmax output activation. In the standard setting, the model is trained to minimize the negative log-likelihood of y labels: • We prop"
D18-1001,C16-1329,0,0.021223,"), z) for the attacker, r is the representation function of the main classifier (r is defined in Section 2.1); Phase 3. Training of the attacker’s network and evaluation of its performance for measuring privacy. In the remainder of this section, we describe the main classifier (Section 2.1), and the attacker’s model (Section 2.2). 2.1 As our base model, we chose a standard LSTM architecture (Hochreiter and Schmidhuber, 1997) for sequence classification. LSTM-based architectures have been applied to many NLP tasks, including sentiment classification (Wang et al., 2016) and text classification (Zhou et al., 2016). First, an LSTM encoder computes a fixed-size representation r(x) from a sequence of tokens x = (x1 , x2 , . . . , xn ) projected to an embedding space. We use θ r to denote the parameters used to construct r. They include the parameters of the LSTM, as well as the word embeddings. Then, the encoder output r(x) is fed as input to a feedforward network with parameters θ p that predicts the label y of the text, with a softmax output activation. In the standard setting, the model is trained to minimize the negative log-likelihood of y labels: • We propose a metric to measure the privacy of the n"
D18-1001,I17-1102,0,0.0259719,"Missing"
D18-1001,P15-1169,0,0.120266,"Missing"
D18-1206,N18-1150,0,0.418261,"osing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summarization datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). However, these datasets often favor e"
D18-1206,P16-1046,1,0.86461,"tem and state-of-the-art abstractive approaches when evaluated automatically and by humans.1 1 Introduction Automatic summarization is one of the central problems in Natural Language Processing (NLP) posing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing to"
D18-1206,P17-1012,0,0.256314,"h Broadcasting Corporation (BBC) that often include a firstsentence summary. We further propose a novel deep learning model which we argue is well-suited to the extreme summarization task. Unlike most existing abstractive approaches (Rush et al., 2015; Chen et al., 2016; Nallapati et al., 2016; See et al., 2017; Tan and Wan, 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018) which rely on an encoder-decoder architecture modeled by recurrent neural networks (RNNs), we present a topic-conditioned neural model which is based entirely on convolutional neural networks (Gehring et al., 2017b). Convolution layers capture longrange dependencies between words in the document more effectively compared to RNNs, allowing to perform document-level inference, abstraction, and paraphrasing. Our convolutional encoder associates each word with a topic vector capturing whether it is representative of the document’s content, while our convolutional decoder conditions each word prediction on a document topic vector. Experimental results show that when evaluated automatically (in terms of ROUGE) our topicaware convolutional model outperforms an oracle extractive system and state-of-the-art RNN"
D18-1206,N18-1065,0,0.359024,", 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summarization datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). However, these datasets often favor extractive models which create a summary by identifying (and subsequently concatenating) the most important sentences in a document (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018b). Abstractive approaches, despite being more faithful to the actual summarization task, either lag behind extractive ones or are mostly extractive, exhibiting a small degree of abstraction (See et al., 2017; Tan and Wan, 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018). In this paper we introduce extreme summariza1797 Proceedi"
D18-1206,J10-3005,1,0.862457,"Missing"
D18-1206,P16-1188,0,0.0555743,"abstractive approaches when evaluated automatically and by humans.1 1 Introduction Automatic summarization is one of the central problems in Natural Language Processing (NLP) posing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic anno"
D18-1206,W18-2706,0,0.0695067,"Missing"
D18-1206,P18-1082,0,0.0248868,"extremely 1799 [P A D . w on [P A D ] ] En gl an d conditioning each word prediction on the document topic vector. t0i ⊗ tD xi + p i e Convolutions f GLU f ⊗ f ⊗ ⊗ zu Attention ⊕ w w  P P P c` hL h` ⊗ f GLU ⊗ f ⊕ ⊗ f Convolutions . po rt re t ch po r re ] at M D [P A D ] [P A D ] [P A M at ch x0i + p0i tD g Figure 2: Topic-conditioned convolutional model for extreme summarization. high, and pertinent content can be easily missed. Recently, a convolutional alternative to sequence modeling has been proposed showing promise for machine translation (Gehring et al., 2017a,b) and story generation (Fan et al., 2018). We believe that convolutional architectures are attractive for our summarization task for at least two reasons. Firstly, contrary to recurrent networks which view the input as a chain structure, convolutional networks can be stacked to represent large context sizes. Secondly, hierarchical features can be extracted over larger and larger contents, allowing to represent long-range dependencies efficiently through shorter paths. Our model builds on the work of Gehring et al. (2017b) who develop an encoder-decoder architecture for machine translation with an attention mechanism (Sukhbaatar et al"
D18-1206,N03-1020,0,0.627338,"Missing"
D18-1206,P18-1188,1,0.923319,"elating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summarization datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). However, these datasets often favor extractive models which"
D18-1206,N18-1158,1,0.9104,"elating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summarization datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). However, these datasets often favor extractive models which"
D18-1206,D15-1044,0,0.353547,"ery different from a headline whose aim is to encourage readers to read the story; it draws on information interspersed in various parts of the document (not only the beginning) and displays multiple levels of abstraction including paraphrasing, fusion, synthesis, and inference. We build a dataset for the proposed task by harvesting online articles from the British Broadcasting Corporation (BBC) that often include a firstsentence summary. We further propose a novel deep learning model which we argue is well-suited to the extreme summarization task. Unlike most existing abstractive approaches (Rush et al., 2015; Chen et al., 2016; Nallapati et al., 2016; See et al., 2017; Tan and Wan, 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018) which rely on an encoder-decoder architecture modeled by recurrent neural networks (RNNs), we present a topic-conditioned neural model which is based entirely on convolutional neural networks (Gehring et al., 2017b). Convolution layers capture longrange dependencies between words in the document more effectively compared to RNNs, allowing to perform document-level inference, abstraction, and paraphrasing. Our convolutional encoder associate"
D18-1206,E17-2007,0,0.157128,"Missing"
D18-1206,P17-1099,0,0.185195,"and by humans.1 1 Introduction Automatic summarization is one of the central problems in Natural Language Processing (NLP) posing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-sca"
D18-1206,D16-1248,0,0.0229063,"Let D denote a document consisting of a sequence of words (w1 , . . . , wm ); we embed D into a distributional space x = (x1 , . . . , xm ) where xi ∈ Rf is a column in embedding matrix M ∈ RV ×f (where V is the vocabulary size). We also embed the absolute word positions in the document p = (p1 , . . . , pm ) where pi ∈ Rf is a column in position matrix 1800 P ∈ RN ×f , and N is the maximum number of positions. Position embeddings have proved useful for convolutional sequence modeling (Gehring et al., 2017b), because, in contrast to RNNs, they do not observe the temporal positions of words 0 (Shi et al., 2016). Let tD ∈ Rf be the topic distribution of document D and t0 = (t01 , . . . , t0m ) the topic distributions of words in the document 0 (where t0i ∈ Rf ). During encoding, we represent document D via e = (e1 , . . . , em ), where ei is: 0 ei = [(xi + pi ); (t0i ⊗ tD )] ∈ Rf +f , and ⊗ denotes point-wise multiplication. The topic distribution t0i of word wi essentially captures how topical the word is in itself (local context), whereas the topic distribution tD represents the overall theme of the document (global context). The encoder essentially enriches the context of the word with its topical"
D18-1206,P17-1108,0,0.49684,"Introduction Automatic summarization is one of the central problems in Natural Language Processing (NLP) posing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summari"
D18-1206,N18-2102,0,0.377679,"Language Processing (NLP) posing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summarization datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). However, th"
E17-3029,P09-1039,0,0.0167833,"Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in n"
E17-3029,N13-1008,1,0.77865,"Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in news https://github.com/andre-martins/ TurboParser 118 Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu Hoang. 2016. Is neural machine translation ready for deployment? A case study on 30 translation directions. CoRR, abs/1610.01108. documents are connected. 2.9 Storyline Construction and Summarization Storylines are co"
E17-3029,E17-3017,1,0.751559,"m a multilingual corpus of nearly 600k documents in 8 of the 9 SUMMA languages (all except Latvian), which were manually annotated by journalists at Deutsche Welle. The document model is a hierarchical attention network with attention at each level of the hierarchy, inspired by Yang et al. (2016), followed by a sigmoid classification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is"
E17-3029,P13-1020,0,0.025126,"Missing"
E17-3029,E17-1051,1,0.815701,"lassification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 K"
J15-1003,W12-4613,0,0.0126653,"es were combined using which operations to yield that derivation. In this tree, each vertex is labeled with a tree name and each edge with a description of the operation (node address and operation type) used to combine the trees labeling its end vertices. As we shall see in Section 3.2, in TAG, each derivation tree specifies a unique parse tree, also called derived tree. In previous work, it has been argued that TAG derivation trees provide a good approximation of semantic dependencies between the words of a sentence (Kroch 1989; Rambow, Vijay-Shanker, and Weir 1995; Candito and Kahane 1998; Kallmeyer and Kuhlmann 2012). As shown by Schabes and Shieber (1994), however, there are several possible ways of defining TAG derivation trees, depending on how multiple adjunction of several auxiliary trees at the same tree node is handled. The standard notion of derivation proposed by Vijay-Shanker (1987) forbids multiple adjunction, thus enforcing dependent derivations. In contrast, the extended notion of derivation proposed by Schabes ∗ UMR 7503, Campus Scientifique, BP 239, F-54506 Vandoeuvre-l`es-Nancy Cedex, France. E-mail:{claire.gardent,shashi.narayan}@loria.fr. Submission received: 2 August 2013; revised versi"
J15-1003,P95-1021,0,0.870671,"Missing"
J15-1003,C92-2065,0,0.426241,"join to the tree selected by reminded. Thus, constraints placed by the verb on its modifiers must be passed through by modifier trees (here, the tree for yesterday) to also rule out sentences such as Example (2a). Propagating selective adjunction constraints in TAG would lead to a formalism for which derivation trees are no longer context-free (Schabes and Shieber 1994). The second motivation for independent adjunction stems from probabilistic approaches. Stochastic lexicalized TAG specifies the probability of an adjunction of a given auxiliary tree at a given node in another elementary tree (Resnik 1992; Schabes 1992). Thus, under the standard notion of derivation, the overall probability of the string roasted red pepper would be determined by the probability of red adjoining to pepper and the probability of roasted adjoining to red. In contrast, independent adjunction would result in a derivation such that the overall probability of the string roasted red pepper would be determined by the probability of both red and roasted adjoining to pepper. Schabes and Shieber (1994, page 97) argue that it is plausible that “the most important relationships to characterize statistically are those betwee"
J15-1003,1991.iwpt-1.4,0,0.339851,"Missing"
J15-1003,C92-2066,0,0.499547,"ree selected by reminded. Thus, constraints placed by the verb on its modifiers must be passed through by modifier trees (here, the tree for yesterday) to also rule out sentences such as Example (2a). Propagating selective adjunction constraints in TAG would lead to a formalism for which derivation trees are no longer context-free (Schabes and Shieber 1994). The second motivation for independent adjunction stems from probabilistic approaches. Stochastic lexicalized TAG specifies the probability of an adjunction of a given auxiliary tree at a given node in another elementary tree (Resnik 1992; Schabes 1992). Thus, under the standard notion of derivation, the overall probability of the string roasted red pepper would be determined by the probability of red adjoining to pepper and the probability of roasted adjoining to red. In contrast, independent adjunction would result in a derivation such that the overall probability of the string roasted red pepper would be determined by the probability of both red and roasted adjoining to pepper. Schabes and Shieber (1994, page 97) argue that it is plausible that “the most important relationships to characterize statistically are those between modifier and"
J15-1003,P88-1032,0,0.469864,"Missing"
J15-1003,P92-1022,0,0.367781,"Missing"
J15-1003,J94-1004,0,0.121632,"yield that derivation. In this tree, each vertex is labeled with a tree name and each edge with a description of the operation (node address and operation type) used to combine the trees labeling its end vertices. As we shall see in Section 3.2, in TAG, each derivation tree specifies a unique parse tree, also called derived tree. In previous work, it has been argued that TAG derivation trees provide a good approximation of semantic dependencies between the words of a sentence (Kroch 1989; Rambow, Vijay-Shanker, and Weir 1995; Candito and Kahane 1998; Kallmeyer and Kuhlmann 2012). As shown by Schabes and Shieber (1994), however, there are several possible ways of defining TAG derivation trees, depending on how multiple adjunction of several auxiliary trees at the same tree node is handled. The standard notion of derivation proposed by Vijay-Shanker (1987) forbids multiple adjunction, thus enforcing dependent derivations. In contrast, the extended notion of derivation proposed by Schabes ∗ UMR 7503, Campus Scientifique, BP 239, F-54506 Vandoeuvre-l`es-Nancy Cedex, France. E-mail:{claire.gardent,shashi.narayan}@loria.fr. Submission received: 2 August 2013; revised version received: 16 June 2014; accepted for"
J15-1003,W98-0106,0,0.0430327,"which elementary TAG trees were combined using which operations to yield that derivation. In this tree, each vertex is labeled with a tree name and each edge with a description of the operation (node address and operation type) used to combine the trees labeling its end vertices. As we shall see in Section 3.2, in TAG, each derivation tree specifies a unique parse tree, also called derived tree. In previous work, it has been argued that TAG derivation trees provide a good approximation of semantic dependencies between the words of a sentence (Kroch 1989; Rambow, Vijay-Shanker, and Weir 1995; Candito and Kahane 1998; Kallmeyer and Kuhlmann 2012). As shown by Schabes and Shieber (1994), however, there are several possible ways of defining TAG derivation trees, depending on how multiple adjunction of several auxiliary trees at the same tree node is handled. The standard notion of derivation proposed by Vijay-Shanker (1987) forbids multiple adjunction, thus enforcing dependent derivations. In contrast, the extended notion of derivation proposed by Schabes ∗ UMR 7503, Campus Scientifique, BP 239, F-54506 Vandoeuvre-l`es-Nancy Cedex, France. E-mail:{claire.gardent,shashi.narayan}@loria.fr. Submission received"
J15-1003,C08-1032,1,0.828088,"modifier auxiliary trees (Schabes and Shieber (1994), Section 3.1), they define a parsing algorithm that assigns dependent derivations to predicative auxiliary trees but independent derivations to multiple modifier auxiliary trees adjoining to the same node. In case both predicative and modifier auxiliary trees adjoin to the same node, their parsing algorithm ensures that predicative trees appear above the modifier trees in the derived tree. This parsing algorithm is defined for featureless variants of TAG. In contrast, in implemented TAGs (e.g., XTAG [The XTAG Research Group 2001], SemXTAG [Gardent 2008], or XXTAG1 [Alahverdzhieva 2008]) feature structures and feature unification are central. They are used to minimize the size of the grammar; to model linguistic phenomena such as verb/subject agreement; and to encode a unification-based syntax/semantics interface (e.g., Gardent and Kallmeyer 2003). In this article, we extend Schabes and Shieber’s proposal to Feature-Based TAG (FB-TAG); and we show that the resulting parsing algorithm naturally accounts for the interplay of dependent vs. independent derivation structures with syntactic constraints, linear ordering, and scopal vs. nonscopal se"
J15-1003,E03-1030,1,0.799083,"and modifier auxiliary trees adjoin to the same node, their parsing algorithm ensures that predicative trees appear above the modifier trees in the derived tree. This parsing algorithm is defined for featureless variants of TAG. In contrast, in implemented TAGs (e.g., XTAG [The XTAG Research Group 2001], SemXTAG [Gardent 2008], or XXTAG1 [Alahverdzhieva 2008]) feature structures and feature unification are central. They are used to minimize the size of the grammar; to model linguistic phenomena such as verb/subject agreement; and to encode a unification-based syntax/semantics interface (e.g., Gardent and Kallmeyer 2003). In this article, we extend Schabes and Shieber’s proposal to Feature-Based TAG (FB-TAG); and we show that the resulting parsing algorithm naturally accounts for the interplay of dependent vs. independent derivation structures with syntactic constraints, linear ordering, and scopal vs. nonscopal semantic dependencies. The article is organized as follows. In Section 2, we recap the motivations for independent derivations put forward by Schabes and Shieber (1994) and we briefly discuss the interactions that may arise between dependent and independent derivations. Section 3 summarizes their appr"
J15-1003,W00-2015,0,0.0297186,"needs to be adjoined above a predicative adjunction. Figure 15 shows the complete recognition algorithm modified to rule out spurious parses in the case of multiple scopal auxiliary trees and intersective modifier auxiliary trees. 5.3.4 Weak Generative Equivalence. The weak-generative equivalence refers to the set of strings characterized by the formal system. In contrast, the strong-generative equivalence relates to the set of structural descriptions (such as derivation trees, dags, proof trees, etc.) assigned by a formal system to the strings that it specifies (Vijay-Shankar and Joshi 1985; Joshi 2000). Using an argument similar to that put forward by Schabes and Shieber (1994), we can prove the weak-generative equivalence of TAGs under the dependent and our independent derivations. We call the set of languages generated by the standard derivation in TAG, TALstd ; the set of languages generated by Schabes and Shieber’s extended derivation in TAG, TALextss ; the set of languages generated with our modifications for FB-TAG, TALext ; and the set of languages generated by the LIG, LIL. Our derivation allows both dependent and independent derivations; therefore, our derivation will recognize all"
J15-1003,P85-1011,0,0.730448,"node prediction (the tree dominated by the foot of the auxiliary tree has been recognized), the Type 5 completor rule unifies the bottom feature structure of the foot of the auxiliary tree with the bottom feature structure of the adjunction site. Finally, the Type 6 completor unifies the top feature structure of a substitution node with the top feature structure of the root of the tree being substituted. r Scanner: hb[..η] → Γ • w∆, i, j, k, li , hb[..η] → Γw • ∆, i, j, k, l + 1i w = wl+1 , ∅ 5 The indices hi, j, k, li have been used in previous parsing algorithms for tree-adjoining grammars (Vijay-Shankar and Joshi 1985; Schabes and Joshi 1988; Schabes 1991). They deliver the same functionality here. 58 Gardent and Narayan Multiple Adjunction in Feature-Based Tree Adjoining Grammar If w (a terminal symbol) occurs at position l+1, the scanner rule creates a new item whose span extends to l+1. r Predictor: hN[..η] → Γ • N′ [µ]∆, i, j, k, li , hN′ [µ] → •Θ, l, −, −, li ∅ Predictor rules are produced for all types of production rules. N and N′ are LIG variables taking the value t or b. Γ, ∆, and Θ are the sequences of LIG nonterminals associated with stacks of node indices. µ is a sequence of node indices. r Typ"
J15-1003,C88-2147,0,0.90548,"djunction of the modifier auxiliary tree with the root node ηr , the following LIG production rule is generated. b[..η] → t[..ηηr ] Figure 5 LIG variant of TAG for Schabes and Shieber’s Extended derivation and associated production rules. The top and bottom components of the nodes are presented by •. Type 4(a) transitions support dependent derivations, and Type 4(b) transitions support independent derivations. 4.1 Feature-Based Tree Adjoining Grammar We start by a brief description of FB-TAG and of the unifications performed during derivation. FB-TAG was introduced by Vijay-Shanker (1987) and Vijay-Shanker and Joshi (1988, 1991) to support the use of feature structures in TAG. Figure 6 shows a toy FB-TAG for illustration. An FB-TAG differs from a TAG in that tree nodes are decorated with feature structures. Nonterminal and foot nodes are decorated with two feature structures called top (T) and bottom (B), and substitution nodes are decorated with a single top feature structure. During derivation, feature structure unification constrains tree combination, as illustrated in Figure 7. Substitution unifies the top feature structure of a substitution node with the top feature structure of the root node of the tree"
J15-1003,J93-4002,0,0.364846,"Missing"
J15-1003,P88-1034,0,0.418365,"e associated with a stack of symbols, called indices. A LIG rule can thus be represented as follows: N[..µ] → N1 [µ1 ] . . . Ni−1 [µi−1 ]Ni [..µi ]Ni+1 [µi+1 ] . . . Nn [µn ] (8) N and Ni are nonterminals whereas µ and µi are strings of stack symbols. The symbol .. stands for the remainder of the stack symbols. Note that the remainder of the stack symbols associated with the left-hand side is associated with only one of the nonterminal (namely, Ni ) on the right-hand side. 46 Gardent and Narayan Multiple Adjunction in Feature-Based Tree Adjoining Grammar LIGs have been used in the literature (Weir and Joshi 1988; Vijay-Shanker and Weir 1991) to provide a common framework for the extensions of context-free grammars. In particular, Vijay-Shanker and Weir (1991, 1993) showed a weak equivalence between LIGs, TAGs, and combinatory categorial grammars (Steedman 2000) and proposed a LIG-based polynomial-time CYK recognition algorithm for TAGs and combinatory categorical grammars. In what follows, we show how Schabes and Shieber (1994) use a LIG variant of TAGs to license both dependent and independent derivations. 3.4 TAG to LIG Compilation The TAG-to-LIG compilation proposed by Vijay-Shanker and Weir (1991"
J15-1003,J13-3005,1,\N,Missing
J15-1003,H86-1020,0,\N,Missing
N18-1158,P13-1020,0,0.0258573,"of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any linguistic preprocessing (e.g., named entities). Like traditional methods, these approaches also suffer from the mismatch between the learning objective and the evaluation criterion (e.g., ROUGE) used at the test time. In comparison, our neural model globally optimizes the ROUGE evaluation metric through a reinforcement learning objective: sentences are highly ranked if they occur in highly scoring summaries. Re"
N18-1158,W97-0703,0,0.66836,"e that L EAD is indeed more informative than See et al. (2017) but humans prefer shorter summaries. The average length of L EAD summaries is 105.7 words compared to 61.6 for See et al. (2017). 7 Related Work Traditional summarization methods manually define features to rank sentences for their salience in order to identify the most important sentences in a document or set of documents (Kupiec et al., 1995; Mani, 2001; Radev et al., 2004; Filatova and Hatzivassiloglou, 2004; Nenkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨"
N18-1158,P11-1049,0,0.0913272,"nkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any linguistic preprocessing (e.g., named entities). Like traditional methods, these approaches also suffer from the mismatch between the learning objective and the evaluation criterion (e.g., ROUGE) used at the test time. In comparison, our neural model globally optimizes the ROUGE evaluation metric through a reinforcement learning objective: sentences are highl"
N18-1158,P16-1046,1,0.107683,"and data are available here: https://github. com/shashiongithub/Refresh. et al., 2017; Tan and Wan, 2017; Paulus et al., 2017) is an encoder-decoder architecture modeled by recurrent neural networks. The encoder reads the source sequence into a list of continuousspace representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2015) is often used to locate the region of focus during decoding. Extractive systems create a summary by identifying (and subsequently concatenating) the most important sentences in a document. A few recent approaches (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017; Yasunaga et al., 2017) conceptualize extractive summarization as a sequence labeling task in which each label specifies whether each document sentence should be included in the summary. Existing models rely on recurrent neural networks to derive a meaning representation of the document which is then used to label each sentence, taking the previously labeled sentences into account. These models are typically trained using cross-entropy loss in order to maximize the likelihood of the ground-truth labels and do not necessarily learn to rank sentence"
N18-1158,J10-3005,1,0.920121,"Missing"
N18-1158,W04-1017,0,0.0911027,"EAD is considered better than See et al. (2017) in the QA evaluation, whereas we find the opposite when participants are asked to rank systems. We hypothesize that L EAD is indeed more informative than See et al. (2017) but humans prefer shorter summaries. The average length of L EAD summaries is 105.7 words compared to 61.6 for See et al. (2017). 7 Related Work Traditional summarization methods manually define features to rank sentences for their salience in order to identify the most important sentences in a document or set of documents (Kupiec et al., 1995; Mani, 2001; Radev et al., 2004; Filatova and Hatzivassiloglou, 2004; Nenkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 20"
N18-1158,D15-1042,0,0.163937,"Missing"
N18-1158,P14-1062,0,0.054469,"lecting m sentences with top p(1|si , D, θ) scores. Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017). The main components include a sentence encoder, a document encoder, and a sentence extractor (see the left block of Figure 1) which we describe in more detail below. Sentence Encoder A core component of our model is a convolutional sentence encoder which encodes sentences into continuous representations. In recent years, CNNs have proven useful for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lei et al., 2015; Kim et al., 2016; Cheng and Lapata, 2016) because of their effectiveness in identifying salient patterns in the input (Xu et al., 2015). In the case of summarization, CNNs can identify named-entities and events that correlate with the gold summary. We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature. This filter is applied to each possible window of words in s to produce a feature map f ∈ Rk−h+1 where k is the sentence length. We then apply max-pooling over time over th"
N18-1158,D14-1181,0,0.00830447,"ary S by selecting m sentences with top p(1|si , D, θ) scores. Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017). The main components include a sentence encoder, a document encoder, and a sentence extractor (see the left block of Figure 1) which we describe in more detail below. Sentence Encoder A core component of our model is a convolutional sentence encoder which encodes sentences into continuous representations. In recent years, CNNs have proven useful for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lei et al., 2015; Kim et al., 2016; Cheng and Lapata, 2016) because of their effectiveness in identifying salient patterns in the input (Xu et al., 2015). In the case of summarization, CNNs can identify named-entities and events that correlate with the gold summary. We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature. This filter is applied to each possible window of words in s to produce a feature map f ∈ Rk−h+1 where k is the sentence length. We then apply ma"
N18-1158,W14-1504,0,0.061479,"Missing"
N18-1158,D15-1180,0,0.0182507,"res. Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017). The main components include a sentence encoder, a document encoder, and a sentence extractor (see the left block of Figure 1) which we describe in more detail below. Sentence Encoder A core component of our model is a convolutional sentence encoder which encodes sentences into continuous representations. In recent years, CNNs have proven useful for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lei et al., 2015; Kim et al., 2016; Cheng and Lapata, 2016) because of their effectiveness in identifying salient patterns in the input (Xu et al., 2015). In the case of summarization, CNNs can identify named-entities and events that correlate with the gold summary. We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature. This filter is applied to each possible window of words in s to produce a feature map f ∈ Rk−h+1 where k is the sentence length. We then apply max-pooling over time over the feature map f and take the maximum v"
N18-1158,P15-1107,0,0.019581,"plied three times each. Max-pooling over time yields two feature lists f K2 and f K4 ∈ R3 . The final sentence embeddings have six dimensions. Document Encoder The document encoder composes a sequence of sentences to obtain a document representation. We use a recurrent neural network with Long Short-Term Memory (LSTM) cells to avoid the vanishing gradient problem when training long sequences (Hochreiter and Schmidhuber, 1997). Given a document D consisting of a sequence of sentences (s1 , s2 , . . . , sn ), we follow common practice and feed sentences in reverse order (Sutskever et al., 2014; Li et al., 2015; Filippova et al., 2015; Narayan et al., 2017). This way we make sure that the network also considers the top sentences of the document which are particularly important for summarization (Rush et al., 2015; Nallapati et al., 2016). Sentence Extractor Our sentence extractor sequentially labels each sentence in a document with 1 (relevant for the summary) or 0 (otherwise). 1748 Sentence extractor y5 y4 y3 y2 y1 Candidate summary Gold summary Sentence encoder REWARD s4 Police are still hunting for the driver s3 s5 s2 s1 s4 s3 s2 s1 Document encoder REINFORCE [convolution] [max pooling] Update ag"
N18-1158,D16-1127,0,0.240915,"-entropy loss using ground truth labels and then follows a curriculum learning strategy (Bengio et al., 2015) to gradually teach the model to produce stable predictions on its own. In our experiments MIXER performed worse than the model of Nallapati et al. (2017) just trained on collective labels. We conjecture that this is due to the unbounded nature of our ranking problem. Recall that our model assigns relevance scores to sentences rather than words. The space of sentential representations is vast and fairly unconstrained compared to other prediction tasks operating with fixed vocabularies (Li et al., 2016; Paulus et al., 2017; Zhang and Lapata, 2017). Moreover, our approximation of the gradient allows the model to 1751 In this section we present our experimental setup for assessing the performance of our model which we call R EFRESH as a shorthand for REinFoRcement Learning-based Extractive Summarization. We describe our datasets, discuss implementation details, our evaluation protocol, and the systems used for comparison. Summarization Datasets We evaluated our models on the CNN and DailyMail news highlights datasets (Hermann et al., 2015). We used the standard splits of Hermann et al. (2015)"
N18-1158,N03-1020,0,0.870659,"ce should be included in the summary. Existing models rely on recurrent neural networks to derive a meaning representation of the document which is then used to label each sentence, taking the previously labeled sentences into account. These models are typically trained using cross-entropy loss in order to maximize the likelihood of the ground-truth labels and do not necessarily learn to rank sentences based on their importance due to the absence of a ranking-based objective. Another discrepancy comes from the mismatch between the learning objective and the evaluation criterion, namely ROUGE (Lin and Hovy, 2003), which takes the entire summary into account. In this paper we argue that cross-entropy training is not optimal for extractive summarization. Models trained this way are prone to generating verbose summaries with unnecessarily long sentences and redundant information. We propose to overcome these difficulties by globally optimizing the ROUGE evaluation metric and learning to rank sentences for summary generation through a reinforcement learning objective. Similar to previous work (Cheng and Lapata, 2016; Narayan et al., 2017; Nallapati et al., 2017), our neural summarization model consists of"
N18-1158,U17-1012,0,0.040784,"Missing"
N18-1158,D17-1061,0,0.0259843,"zation, in our case states are documents (not summaries) and actions are relevance scores which lead to sentence ranking (not sentence-to-sentence transitions). Rather than employing reinforcement learning for sentence selection, our algorithm performs sentence ranking using ROUGE as the reward function. The REINFORCE algorithm (Williams, 1992) has been shown to improve encoder-decoder textrewriting systems by allowing to directly optimize a non-differentiable objective (Ranzato et al., 2015; Li et al., 2016; Paulus et al., 2017) or to inject task-specific constraints (Zhang and Lapata, 2017; Nogueira and Cho, 2017). However, we are not aware of any attempts to use reinforcement learning for training a sentence ranker in the context of extractive summarization. 8 Conclusions In this work we developed an extractive summarization model which is globally trained by optimizing the ROUGE evaluation metric. Our training algorithm explores the space of candidate summaries while learning to optimize a reward function which is relevant for the task at hand. Experimental results show that reinforcement learning offers a great means to steer our model towards generating informative, fluent, and concise summaries ou"
N18-1158,D15-1226,0,0.0592465,"core each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any linguistic preprocessing (e.g., named entities). Like traditional methods, these approaches also suffer from the mismatch between the learning objective and the evaluation criterion (e.g., ROUGE) used at the test time. In comparison, our neural model globally optimizes the ROUGE evaluation metric through a reinforcement learning objective: sentences are highly ranked if they occur in highly scoring summaries. Reinforcement learning ha"
N18-1158,radev-etal-2004-mead,0,0.386489,"Missing"
N18-1158,D14-1075,0,0.0947591,"Missing"
N18-1158,D15-1044,0,0.594398,"f summarization tasks that have been identified over the years (see Nenkova and McKeown, 2011 for a comprehensive overview). Modern approaches to single document summarization are data-driven, taking advantage of the success of neural network architectures and their ability to learn continuous features without recourse to preprocessing tools or linguistic annotations. Abstractive summarization involves various text rewriting operations (e.g., substitution, deletion, reordering) and has been recently framed as a sequence-to-sequence problem (Sutskever et al., 2014). Central in most approaches (Rush et al., 2015; Chen et al., 2016; Nallapati et al., 2016; See 1 Our code and data are available here: https://github. com/shashiongithub/Refresh. et al., 2017; Tan and Wan, 2017; Paulus et al., 2017) is an encoder-decoder architecture modeled by recurrent neural networks. The encoder reads the source sequence into a list of continuousspace representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2015) is often used to locate the region of focus during decoding. Extractive systems create a summary by identifying (and subsequently concatenating) the most"
N18-1158,D12-1024,0,0.0672832,"istic preprocessing (e.g., named entities). Like traditional methods, these approaches also suffer from the mismatch between the learning objective and the evaluation criterion (e.g., ROUGE) used at the test time. In comparison, our neural model globally optimizes the ROUGE evaluation metric through a reinforcement learning objective: sentences are highly ranked if they occur in highly scoring summaries. Reinforcement learning has been previously used in the context of traditional multi-document summarization as a means of selecting a sentence or a subset of sentences from a document cluster. Ryang and Abekawa (2012) cast the sentence selection task as a search problem. Their agent observes a state (e.g., a candidate summary), executes an action (a transition operation that produces a new state selecting a not-yet-selected sentence), and then receives a delayed reward based on tf ∗ idf. Follow-on work (Rioux et al., 2014) extends this approach by employing ROUGE as part of the reward function, while Henß et al. (2015) further experiment with Q-learning. Moll´aAliod (2017) has adapt this approach to queryfocused summarization. Our model differs from these approaches both in application and formulation. We"
N18-1158,P08-2052,0,0.0921083,"f L EAD summaries is 105.7 words compared to 61.6 for See et al. (2017). 7 Related Work Traditional summarization methods manually define features to rank sentences for their salience in order to identify the most important sentences in a document or set of documents (Kupiec et al., 1995; Mani, 2001; Radev et al., 2004; Filatova and Hatzivassiloglou, 2004; Nenkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any"
N18-1158,E17-2007,0,0.0672798,".2 We compared R EFRESH against a baseline which simply selects the first m leading sentences from each document (L EAD) and two neural models similar to ours (see left block in Figure 1), both trained with cross-entropy loss. Cheng and Lapata (2016) train on individual labels, while Nallapati et al. (2017) use collective labels. We also compared our model against the abstractive systems of Chen et al. (2016), Nallapati et al. (2016), See et al. (2017), and Tan and Wan (2017).3 In addition to ROUGE which can be misleading when used as the only means to assess the informativeness of summaries (Schluter, 2017), we also evaluated system output by eliciting human judgments in two ways. In our first experiment, participants were presented with a news article and summaries generated by three systems: the L EAD baseline, abstracts from See et al. (2017), and extracts from R EFRESH. We also included the human-authored highlights.4 Participants read the articles and were asked to rank the summaries from best (1) to worst (4) in order of informativeness (does the summary capture important information in the article?) and fluency (is the summary written in well-formed English?). We did not allow any ties. W"
N18-1158,P17-1099,0,0.458809,"nFoRcement Learning-based Extractive Summarization. We describe our datasets, discuss implementation details, our evaluation protocol, and the systems used for comparison. Summarization Datasets We evaluated our models on the CNN and DailyMail news highlights datasets (Hermann et al., 2015). We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 documents for CNN and 196,961/12,148/10,397 for DailyMail). We did not anonymize entities or lower case tokens. We followed previous studies (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017) in assuming that the “story highlights” associated with each article are gold-standard abstractive summaries. During training we use these to generate high scoring extracts and to estimate rewards for them, but during testing, they are used as reference summaries to evaluate our models. Implementation Details We generated extracts by selecting three sentences (m = 3) for CNN articles and four sentences (m = 4) for DailyMail articles. These decisions were informed by the fact that gold highlights in the CNN/DailyMail validation sets are 2.6/4.2 sentences long. For both data"
N18-1158,P16-1159,0,0.0290088,"mbeddings with the skip-gram model (Mikolov et al., 2013) using context window size 6, negative sampling size 10, and hierarchical softmax 1. Known words were initialized with pre-trained embeddings of size 200. Embeddings for unknown words were initialized to zero, but estimated during training. L EAD R EFRESH Experimental Setup G OLD 5 See et al. converge much faster to an optimal policy. Advantageously, we do not require an online reward estiˆ which leads to a signifimator, we pre-compute Y, cant speedup during training compared to MIXER (Ranzato et al., 2015) and related training schemes (Shen et al., 2016). Q1 Q2 Q3 • A SkyWest Airlines flight made an emergency landing in Buffalo, New York, on Wednesday after a passenger lost consciousness, officials said. • The passenger received medical attention before being released, according to Marissa Snow, spokeswoman for SkyWest. • She said the airliner expects to accommodate the 75 passengers on another aircraft to their original destination – Hartford, Connecticut – later Wednesday afternoon. • Skywest Airlines flight made an emergency landing in Buffalo, New York, on Wednesday after a passenger lost consciousness. • She said the airliner expects to"
N18-1158,D07-1047,0,0.129055,"Missing"
N18-1158,P17-1108,0,0.457427,"mmarization are data-driven, taking advantage of the success of neural network architectures and their ability to learn continuous features without recourse to preprocessing tools or linguistic annotations. Abstractive summarization involves various text rewriting operations (e.g., substitution, deletion, reordering) and has been recently framed as a sequence-to-sequence problem (Sutskever et al., 2014). Central in most approaches (Rush et al., 2015; Chen et al., 2016; Nallapati et al., 2016; See 1 Our code and data are available here: https://github. com/shashiongithub/Refresh. et al., 2017; Tan and Wan, 2017; Paulus et al., 2017) is an encoder-decoder architecture modeled by recurrent neural networks. The encoder reads the source sequence into a list of continuousspace representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2015) is often used to locate the region of focus during decoding. Extractive systems create a summary by identifying (and subsequently concatenating) the most important sentences in a document. A few recent approaches (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017; Yasunaga et al., 2017) conceptuali"
N18-1158,W97-0710,0,0.602162,"informative than See et al. (2017) but humans prefer shorter summaries. The average length of L EAD summaries is 105.7 words compared to 61.6 for See et al. (2017). 7 Related Work Traditional summarization methods manually define features to rank sentences for their salience in order to identify the most important sentences in a document or set of documents (Kupiec et al., 1995; Mani, 2001; Radev et al., 2004; Filatova and Hatzivassiloglou, 2004; Nenkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin an"
N18-1158,C10-1128,0,0.0490031,"ds compared to 61.6 for See et al. (2017). 7 Related Work Traditional summarization methods manually define features to rank sentences for their salience in order to identify the most important sentences in a document or set of documents (Kupiec et al., 1995; Mani, 2001; Radev et al., 2004; Filatova and Hatzivassiloglou, 2004; Nenkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any linguistic"
N18-1158,P10-1058,1,0.933128,"n the task definition and the training objective. While MLE in Equation (1) aims to maximize the likelihood of the ground-truth labels, the model is (a) expected to rank sentences to generate a summary and (b) evaluated using ROUGE at test time. The second discrepancy comes from the reliance on ground-truth labels. Document collections for training summarization systems do not naturally contain labels indicating which sentences should be extracted. Instead, they are typically accompanied by abstractive summaries from which sentence-level labels are extrapolated. Cheng and Lapata (2016) follow Woodsend and Lapata (2010) in adopting a rule-based method which assigns labels to each sentence in the document individually based on their semantic correspondence with the gold summary (see the fourth column in Table 1). An alternative method (Svore et al., 2007; Cao et al., 2016; Nallapati et al., 2017) identifies the set of sentences which collectively gives the highest ROUGE with respect to the gold summary. Sentences in this set are labeled with 1 and 0 otherwise (see the column 5 in Table 1). Labeling sentences individually often generates too many positive labels causing the model to 1749 Collective Oracle ROUG"
N18-1158,D12-1022,1,0.736525,"es, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any linguistic preprocessing (e.g., named entities). Like traditional methods, these approaches also suffer from the mismatch between the learning objective and the evaluation criterion (e.g., ROUGE) used at the test time. In comparison, our neural model globally optimizes the ROUGE evaluation metric through a reinforcement learning objective: sentences are highly ranked if they occur in h"
N18-1158,K17-1045,0,0.055014,"h. et al., 2017; Tan and Wan, 2017; Paulus et al., 2017) is an encoder-decoder architecture modeled by recurrent neural networks. The encoder reads the source sequence into a list of continuousspace representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2015) is often used to locate the region of focus during decoding. Extractive systems create a summary by identifying (and subsequently concatenating) the most important sentences in a document. A few recent approaches (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017; Yasunaga et al., 2017) conceptualize extractive summarization as a sequence labeling task in which each label specifies whether each document sentence should be included in the summary. Existing models rely on recurrent neural networks to derive a meaning representation of the document which is then used to label each sentence, taking the previously labeled sentences into account. These models are typically trained using cross-entropy loss in order to maximize the likelihood of the ground-truth labels and do not necessarily learn to rank sentences based on their importance due to the absence of a ranking-based obje"
N18-1158,D17-1062,1,0.913736,"s and then follows a curriculum learning strategy (Bengio et al., 2015) to gradually teach the model to produce stable predictions on its own. In our experiments MIXER performed worse than the model of Nallapati et al. (2017) just trained on collective labels. We conjecture that this is due to the unbounded nature of our ranking problem. Recall that our model assigns relevance scores to sentences rather than words. The space of sentential representations is vast and fairly unconstrained compared to other prediction tasks operating with fixed vocabularies (Li et al., 2016; Paulus et al., 2017; Zhang and Lapata, 2017). Moreover, our approximation of the gradient allows the model to 1751 In this section we present our experimental setup for assessing the performance of our model which we call R EFRESH as a shorthand for REinFoRcement Learning-based Extractive Summarization. We describe our datasets, discuss implementation details, our evaluation protocol, and the systems used for comparison. Summarization Datasets We evaluated our models on the CNN and DailyMail news highlights datasets (Hermann et al., 2015). We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266"
N18-1158,W04-3252,0,\N,Missing
N18-1158,W01-0100,0,\N,Missing
N18-6002,C10-1128,0,0.0697397,"Missing"
N18-6002,N16-1015,0,0.0416556,"Missing"
N18-6002,D15-1199,0,0.0770728,"Missing"
N19-1397,P13-1020,1,0.874328,"r methods rely on an abstractive approach with strongly conditioned generation on the source document (See et al., 2017). In fact, the best results for abstractive summarization have been achieved with models that are more extractive in nature than abstractive, since most of the words in the summary are copied from the document (Gehrmann et al., 2018). Due to the lack of training corpora, there is almost no work on neural architectures for compressive summarization. Most compressive summarization work has been applied to smaller datasets (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Almeida and Martins, 2013). Other non-neural summarization systems apply this idea to select and compress the summary. Dorr et al. (2003) introduced a method to first extract the first sentence of a news article and then use linguistically-motivated heuristics to iteratively trim parts of it. Durrett et al. (2016) also learns a system that selects textual units to include in the summary and compresses them by deleting word spans guided by anaphoric constraints to improve coherence. Recently, Zhang et al. (2018) trained an abstractive sentence compression model using attention-based sequence-to-sequence architecture (Ru"
N19-1397,P11-1049,0,0.175783,"Missing"
N19-1397,P18-1063,0,0.127952,"Missing"
N19-1397,P16-1046,0,0.661102,"nt summarization—the task of generating a short summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive systems are wasteful since they cannot trim the original sentences to fit into the summary, and they lack a mechanism to ensure overall coherence. In contrast, abstractive systems require natural language generation and semantic representation, problems that are inherently harder to solve than just extracting sentences from the original document. I"
N19-1397,R13-1027,0,0.0258907,"Missing"
N19-1397,W03-0501,0,0.109136,"Missing"
N19-1397,P16-1188,0,0.084033,"ummary are copied from the document (Gehrmann et al., 2018). Due to the lack of training corpora, there is almost no work on neural architectures for compressive summarization. Most compressive summarization work has been applied to smaller datasets (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Almeida and Martins, 2013). Other non-neural summarization systems apply this idea to select and compress the summary. Dorr et al. (2003) introduced a method to first extract the first sentence of a news article and then use linguistically-motivated heuristics to iteratively trim parts of it. Durrett et al. (2016) also learns a system that selects textual units to include in the summary and compresses them by deleting word spans guided by anaphoric constraints to improve coherence. Recently, Zhang et al. (2018) trained an abstractive sentence compression model using attention-based sequence-to-sequence architecture (Rush et al., 2015) to map a sentence in the document selected by the extractive model to a sentence in the summary. However, as the sentences in the document and in the summary are not aligned for compression, their compression model is significantly inferior to the extractive model. In thi"
N19-1397,W18-2706,0,0.021044,"ummaries derived automatically from the CNN/DailyMail 1 reference summaries. 1 Figure 1: Summaries produced by our model. For illustration, the compressive summary shows the removed spans strike-through. Introduction Text summarization is an important NLP problem with a wide range of applications in data-driven industries (e.g., news, health, and defense). Single document summarization—the task of generating a short summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from signif"
N19-1397,D15-1042,0,0.0902161,"Missing"
N19-1397,D13-1155,0,0.0222192,"rayan et al., 2018c). To build a compressive oracle, we trained a supervised sentence labeling classifier, adapted from 3959 Oracle Extractive Oracle Compressive Oracle R1 54.67 57.12 R2 30.37 32.59 RL 50.81 53.27 Table 1: Oracle scores obtained for the CNN and DailyMail testsets. We report ROUGE-1 (R1), ROUGE-2 (R2) and ROUGE-L (RL) F1 scores. the Transition-Based Chunking Model (Lample et al., 2016), to annotate spans in every sentence that can be dropped in the final summary. We used the publicly released set of 10,000 sentencecompression pairs from the Google sentence compression dataset (Filippova and Altun, 2013; Filippova et al., 2015) for training. After tagging all sentences in the CNN and DailyMail corpora using this compression model, we generated oracle compressive summaries based on the best average of ROUGE-1 (R1) and ROUGE-2 (R2) F1 scores from the combination of all possible sentences and all removals of the marked compression chunks. To verify the adequacy of our proposed oracles, we show in Table 1 a comparison of their scores. Our compressive oracle achieves much better scores than the extractive oracle, because of its capability to make summaries concise. Moreover, the linguistic qualit"
N19-1397,D18-1443,0,0.100831,". They select sentences based on an LSTM classifier that predicts a binary label for each sentence (Cheng and Lapata, 2016), based on ranking using reinforcement learning (Narayan et al., 2018c), or even by training an extractive latent model (Zhang et al., 2018). Other methods rely on an abstractive approach with strongly conditioned generation on the source document (See et al., 2017). In fact, the best results for abstractive summarization have been achieved with models that are more extractive in nature than abstractive, since most of the words in the summary are copied from the document (Gehrmann et al., 2018). Due to the lack of training corpora, there is almost no work on neural architectures for compressive summarization. Most compressive summarization work has been applied to smaller datasets (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Almeida and Martins, 2013). Other non-neural summarization systems apply this idea to select and compress the summary. Dorr et al. (2003) introduced a method to first extract the first sentence of a news article and then use linguistically-motivated heuristics to iteratively trim parts of it. Durrett et al. (2016) also learns a system that selects tex"
N19-1397,N18-1065,0,0.319626,"Missing"
N19-1397,P18-1013,0,0.0974388,"Missing"
N19-1397,N16-1030,0,0.0472101,"mmaries prior to training using two types of oracles. We used an extractive oracle to identify the set of sentences which collectively gives the highest ROUGE (Lin and Hovy, 2003) with respect to the gold summary (Narayan et al., 2018c). To build a compressive oracle, we trained a supervised sentence labeling classifier, adapted from 3959 Oracle Extractive Oracle Compressive Oracle R1 54.67 57.12 R2 30.37 32.59 RL 50.81 53.27 Table 1: Oracle scores obtained for the CNN and DailyMail testsets. We report ROUGE-1 (R1), ROUGE-2 (R2) and ROUGE-L (RL) F1 scores. the Transition-Based Chunking Model (Lample et al., 2016), to annotate spans in every sentence that can be dropped in the final summary. We used the publicly released set of 10,000 sentencecompression pairs from the Google sentence compression dataset (Filippova and Altun, 2013; Filippova et al., 2015) for training. After tagging all sentences in the CNN and DailyMail corpora using this compression model, we generated oracle compressive summaries based on the best average of ROUGE-1 (R1) and ROUGE-2 (R2) F1 scores from the combination of all possible sentences and all removals of the marked compression chunks. To verify the adequacy of our proposed"
N19-1397,N18-2009,0,0.063515,"Missing"
N19-1397,N03-1020,0,0.659093,"Missing"
N19-1397,W09-1801,1,0.936062,"2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive systems are wasteful since they cannot trim the original sentences to fit into the summary, and they lack a mechanism to ensure overall coherence. In contrast, abstractive systems require natural language generation and semantic representation, problems that are inherently harder to solve than just extracting sentences from the original document. In this paper, we present a novel architecture that attempts to mitigate the problems above via a middle ground, compressive summarization (Martins and Smith, 2009). Our model selects a set of sentences from the input document, and 3955 Proceedings of NAACL-HLT 2019, pages 3955–3966 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics compresses them by removing unnecessary words, while keeping the summaries informative, concise and grammatical. We achieve this by dynamically modeling the generated summary using a Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to produce summary state representations. This state provides crucial information to iteratively increment summaries based on previously"
N19-1397,E06-1038,0,0.169088,"Missing"
N19-1397,K16-1028,0,0.0784861,"Missing"
N19-1397,P18-1188,1,0.119729,"summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive systems are wasteful since they cannot trim the original sentences to fit into the summary, and they lack a mechanism to ensure overall coherence. In contrast, abstractive systems require natural language generation and semantic representation, problems that are inherently harder to solve than just extracting sentences from the original document. In this paper, we present a novel architecture"
N19-1397,D18-1206,1,0.135235,"summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive systems are wasteful since they cannot trim the original sentences to fit into the summary, and they lack a mechanism to ensure overall coherence. In contrast, abstractive systems require natural language generation and semantic representation, problems that are inherently harder to solve than just extracting sentences from the original document. In this paper, we present a novel architecture"
N19-1397,N18-1158,1,0.111916,"summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive systems are wasteful since they cannot trim the original sentences to fit into the summary, and they lack a mechanism to ensure overall coherence. In contrast, abstractive systems require natural language generation and semantic representation, problems that are inherently harder to solve than just extracting sentences from the original document. In this paper, we present a novel architecture"
N19-1397,N18-2102,0,0.127757,"utomatically from the CNN/DailyMail 1 reference summaries. 1 Figure 1: Summaries produced by our model. For illustration, the compressive summary shows the removed spans strike-through. Introduction Text summarization is an important NLP problem with a wide range of applications in data-driven industries (e.g., news, health, and defense). Single document summarization—the task of generating a short summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive"
N19-1397,D15-1044,0,0.825797,"for each sentence, and a set of compressed oracle summaries (see §4). Experimental results show that when evaluated automatically, both the extractive and compressive variants of our model provide state-of-the-art results. Human evaluation further shows that our model is better than previous state-of-the-art systems at generating informative and concise summaries. 2 Related Work Recent work on neural summarization has mainly focused on sequence-to-sequence (seq2seq) architectures (Sutskever et al., 2014), a formulation particularly suited and initially employed for abstractive summarization (Rush et al., 2015). However, state-of-the-art results have been achieved by RNN-based methods which are extractive. They select sentences based on an LSTM classifier that predicts a binary label for each sentence (Cheng and Lapata, 2016), based on ranking using reinforcement learning (Narayan et al., 2018c), or even by training an extractive latent model (Zhang et al., 2018). Other methods rely on an abstractive approach with strongly conditioned generation on the source document (See et al., 2017). In fact, the best results for abstractive summarization have been achieved with models that are more extractive i"
N19-1397,E17-2007,0,0.0363789,"after every 5,000 batches. We trained with Adam (Kingma and Ba, 2015) with an initial learning rate of 0.001. Our system was implemented using DyNet (Neubig et al., 2017). 4.3 Model Evaluation We evaluated summarization quality using F1 ROUGE (Lin and Hovy, 2003). We report results 4 We show examples of both oracles in Appendix §A.1. in terms of unigram and bigram overlap (R1) and (R2) as a means of assessing informativeness, and the longest common subsequence (RL) as a means 5 of assessing fluency. In addition to ROUGE, which can be misleading when used as the only means to assess summaries (Schluter, 2017), we also conducted a question-answering based human evaluation to assess the informativeness of our summaries in their ability to preserve key informa6 tion from the document (Narayan et al., 2018c). First, questions are written using the gold summary, we then examined how many questions participants were able to answer by reading system 7 summaries alone, without access to the article. Figure 5 shows a set of candidate summaries along with questions used for this evaluation. 4.4 Model and Baselines We evaluated our model E X C ON S UMM in two settings: Extractive (selects sentences to assemb"
N19-1397,P17-1099,0,0.476379,"acle compressive summaries derived automatically from the CNN/DailyMail 1 reference summaries. 1 Figure 1: Summaries produced by our model. For illustration, the compressive summary shows the removed spans strike-through. Introduction Text summarization is an important NLP problem with a wide range of applications in data-driven industries (e.g., news, health, and defense). Single document summarization—the task of generating a short summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods"
N19-1397,P17-1108,0,0.102084,"Missing"
N19-1397,D18-1088,0,0.509737,"presses them by removing unnecessary words, while keeping the summaries informative, concise and grammatical. We achieve this by dynamically modeling the generated summary using a Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to produce summary state representations. This state provides crucial information to iteratively increment summaries based on previously extracted information. It also facilitates the generation of variable length summaries as opposed to fixed lengths, in previous extractive systems (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018c; Zhang et al., 2018). Our model can be trained in both extractive (labeling sentences for extraction) or compressive (labeling words for extraction) settings. Figure 1 shows a summary example generated by our model. Our contributions in this paper are three-fold: • we present the first end-to-end neural architecture for EXtractive and COmpressive Neural SUMMarization (dubbed E X C ON S UMM, see §3), • we validate this architecture on the CNN/DailyMail and the Newsroom datasets (Hermann et al., 2015; Grusky et al., 2018), showing that our model generates variablelength summaries which correlate well with gold summ"
P12-1062,W11-2832,0,0.0681279,"results on parsing output and shown to help improve the large scale symbolic grammars and lexicons used by the parser. However the techniques they use (e.g., suffix arrays) to enumerate and count n-grams builds on the sequential nature of a text corpus and cannot easily extend to structured data. There are some NLP applications though where the processed data is structured data such as trees or graphs and which would benefit from error mining. For instance, when generating sentences from dependency trees, as was proposed recently in the Generation Challenge Surface Realisation Task (SR Task, (Belz et al., 2011)), it would be useful to be able to apply error mining on the input trees to find the most likely causes of generation failure. In this paper, we address this issue and propose an approach that supports error mining on trees. We adapt an existing algorithm for tree mining which we then use to mine the Generation Challenge dependency trees and identify the most likely causes of generation failure. We show in particular, that this tree mining algorithm permits identifying not only errors in the grammar and the lexicon used by generation but also a few idiosyncrasies/error in the input data as we"
P12-1062,W09-2609,0,0.485183,"Missing"
P12-1062,C10-1042,1,0.850978,"nodes and word forms was provided by the organisers. The surface realiser used is a system based on a Feature-Based Lexicalised Tree Adjoining Grammar (FB-LTAG) for English extended with a unification based compositional semantics. Both the grammars and the lexicon were developed in view of the Generation Challenge and the data provided by this challenge was used as a means to debug and extend the system. Unknown words are assigned a default TAG family/tree based on the part of speech they are associated with in the SR data. The surface realisation algorithm extends the algorithm proposed in (Gardent and Perez-Beltrachini, 2010) and adapts it to work on the SR dependency input rather than on flat semantic representations. using different minimal support thresholds, different display modes (sorted first by size and second by suspicion rate vs sorted by suspicion rate) and different labels (part of speech, words and part of speech, dependency, dependency and part of speech). 4.2 Mining on a single label permits (i) assessing the relative impact of each category in a given label category and (ii) identifying different sources of errors depending on the type of label considered (POS tag, dependency or word form). Experim"
P12-1062,C10-2039,1,0.506587,"from incorrect items using surface realisation and targets the most likely sources of errors rather than the absolute ones. More generally, our approach is the first to our knowledge, which mines a surface realiser for undergeneration. Indeed, apart from (Gardent and Kow, 2007), most previous work on surface realisation evaluation has focused on evaluating the performance and the coverage of surface realisers. Approaches based on reversible grammars (Carroll et al., 1999) have used the semantic formulae output by parsing to evaluate the coverage and performance of their realiser; similarly, (Gardent et al., 2010) developed a tool called GenSem which traverses the grammar to produce flat semantic representations and thereby provide a benchmark for performance and coverage evaluation. In both cases however, because it is produced using the grammar exploited by the surface realiser, the input produced can only be used to test for overgeneration (and performance) . (Callaway, 2003) avoids this shortcoming by converting the Penn Treebank to the format expected by his realiser. However, this involves manually identifying the mismatches between two formats much like symbolic systems did in the Generation Cha"
P12-1062,W07-2416,0,0.0259542,"allows us to process e.g., all NP chunks of size 4 and 6 present in the SR data (roughly 60 000 trees) in roughly 20 minutes on a PC. 4 Experiment and Results Using the input data provided by the Generation Challenge SR Task, we applied the error mining algorithm described in the preceding Section to debug and extend a symbolic surface realiser developed for this task. 4.1 Input Data and Surface Realisation System The shallow input data provided by the SR Task was obtained from the Penn Treebank using the LTH Constituent-to-Dependency Conversion Tool for Penn-style Treebanks (Pennconverter, (Johansson and Nugues, 2007)). It consists of a set of unordered labelled syntactic dependency trees whose nodes are labelled with word forms, part of speech categories, partial morphosyntactic information such as tense and number and, in some cases, a sense tag identifier. The edges are labelled with the syntactic labels provided by the Pennconverter. All words (including punctuation) of the original sen4 ETM needs to store all (n-1)-node trees in queues before producing n-node trees. tence are represented by a node in the tree and the alignment between nodes and word forms was provided by the organisers. The surface re"
P12-1062,W11-2836,0,0.0630859,"dicate that there are unresolved issues with, in decreasing order of suspicion rate, cardinal numbers (CD), proper names (NNP), nouns (NN), prepositions (IN) and determiners (DT). The highest ranking category (POS5 ) points to a mismatch between the representation of genitive NPs (e.g., John’s father) in the SR Task data and in the grammar. While our generator expects the representation of ‘John’s father’ to be FA THER (“ S ”( JOHN )), the structure provided by the SR Task is FATHER ( JOHN (“ S ”)). Hence whenever a possessive appears in the input data, generation fails. This is in line with (Rajkumar et al., 2011)’s finding that the logical forms expected by their system for possessives differed from the shared task inputs. 5 In the Penn Treebank, the POS tag is the category assigned to possessive ’s. POS POS CC CD NNP NN IN DT Sus 0.99 0.99 0.39 0.35 0.30 0.30 0.09 Sup 0.38 0.21 0.16 0.32 0.81 0.16 0.12 Fail 3237 1774 1419 2749 6798 1355 1079 Pass 1 9 2148 5014 15663 3128 10254 Table 1: Error Mining on POS tags with frequency cutoff 0.1 and displaying only trees of size 1 sorted by decreasing suspicion rate (Sus) The second highest ranked category is CC for coordinations. In this case, error mining un"
P12-1062,P06-1042,0,0.472439,"red data. In this paper, we propose an algorithm for mining trees and apply it to detect the most likely sources of generation failure. We show that this tree mining algorithm permits identifying not only errors in the generation system (grammar, lexicon) but also mismatches between the structures contained in the input and the input structures expected by our generator as well as a few idiosyncrasies/error in the input data. 1 Introduction In recent years, error mining techniques have been developed to help identify the most likely sources of parsing failure (van Noord, 2004; Sagot and de la Clergerie, 2006; de Kok et al., 2009). First, the input data (text) is separated into two subcorpora, a corpus of sentences that could be parsed (PASS) and a corpus of sentences that failed to be parsed (FAIL). For each n-gram of words (and/or part of speech tag) occurring in the corpus to be parsed, a suspicion rate is then computed which, in essence, captures the likelihood that this n-gram causes parsing to fail. These error mining techniques have been applied with good results on parsing output and shown to help improve the large scale symbolic grammars and lexicons used by the parser. However the techni"
P12-1062,P04-1057,0,0.155782,"Missing"
P12-1062,W07-2306,1,\N,Missing
P14-1041,P05-1074,0,0.0389166,"d and Lapata (2011) will often fail to appropriately reconstruct the shared phrase and introduce agreement mismatches because the alignment or rules they learn are based on syntax alone. For instance, in example (2), Zhu et al. (2010) fails to copy the shared argument “The judge” to the second clause whereas Woodsend and Lapata (2011) learns a synchronous rule matching (VP and VP) to (VP. NP(It) VP) thereby failing to produce the correct subject pronoun (“he” or “she”) for the antecedent “The judge”. Substitution and Reordering SMT based approaches to paraphrasing (Barzilay and Elhadad, 2003; Bannard and Callison-Burch, 2005) and to sentence simplification (Wubben et al., 2012) have shown that by utilising knowledge about alignment and translation probabilities, SMT systems can account for the substitutions and the reorderings occurring in sentence simplification. Following on these approaches, we therefore rely on phrase based SMT to learn substitutions and reordering. In addition, the language model we integrate in the SMT module helps ensuring better fluency and grammaticality. 3.1 An Example Figure 1 shows how our approach simplifies (4C) into (4S). (4) C. In 1964 Peter Higgs published his second paper in Phys"
P14-1041,W03-1004,0,0.0185815,"hu et al. (2010) and Woodsend and Lapata (2011) will often fail to appropriately reconstruct the shared phrase and introduce agreement mismatches because the alignment or rules they learn are based on syntax alone. For instance, in example (2), Zhu et al. (2010) fails to copy the shared argument “The judge” to the second clause whereas Woodsend and Lapata (2011) learns a synchronous rule matching (VP and VP) to (VP. NP(It) VP) thereby failing to produce the correct subject pronoun (“he” or “she”) for the antecedent “The judge”. Substitution and Reordering SMT based approaches to paraphrasing (Barzilay and Elhadad, 2003; Bannard and Callison-Burch, 2005) and to sentence simplification (Wubben et al., 2012) have shown that by utilising knowledge about alignment and translation probabilities, SMT systems can account for the substitutions and the reorderings occurring in sentence simplification. Following on these approaches, we therefore rely on phrase based SMT to learn substitutions and reordering. In addition, the language model we integrate in the SMT module helps ensuring better fluency and grammaticality. 3.1 An Example Figure 1 shows how our approach simplifies (4C) into (4S). (4) C. In 1964 Peter Higgs"
P14-1041,bott-etal-2012-text,0,0.134071,"Missing"
P14-1041,E99-1042,0,0.864244,"Missing"
P14-1041,C96-2183,0,0.631146,"has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011). Machine Translation systems have been adapted to translate complex sentences into simple ones (Zhu et al., 2010; Wubben et al., 2012; Coster and Kauchak, 2011). And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996). In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways. First, it combines a model encoding probabilities for splitting and deletion with a monolingual machine translation module which handles reordering and substitution. In this way, we exploit the ability of statistical machine translation (SMT) systems to capture phrasal/lexical substitution and reordering while relying on a dedicated probabilistic module to capture the splitting and deletion operations which are less well (deletion) or not at all (splitting) captured"
P14-1041,W11-1601,0,0.672279,"application as a reading aid for people with aphasis (Carroll et al., 1999), for low literacy readers (Watanabe et al., 2009) and for non native speakers (Siddharthan, 2002). There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011). Machine Translation systems have been adapted to translate complex sentences into simple ones (Zhu et al., 2010; Wubben et al., 2012; Coster and Kauchak, 2011). And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996). In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways. First, it combines a model encoding probabilities for splitting and deletion with a monolingual machine translation module which handles reordering and substitution. In this way, we exploit the ability of statistical machine translation (SMT) systems to capture phrasal/lexical substitutio"
P14-1041,P07-2009,0,0.20616,"his way, we exploit the ability of statistical machine translation (SMT) systems to capture phrasal/lexical substitution and reordering while relying on a dedicated probabilistic module to capture the splitting and deletion operations which are less well (deletion) or not at all (splitting) captured by SMT approaches. Second, our approach is semantic based. While previous simplification approaches starts from either the input sentence or its parse tree, our model takes as input a deep semantic representation namely, the Discourse Representation Structure (DRS, (Kamp, 1981)) assigned by Boxer (Curran et al., 2007) to the input complex sentence. As we We present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones. The approach differs from previous work in two main ways. First, it is semantic based in that it takes as input a deep semantic representation rather than e.g., a sentence or a parse tree. Second, it combines a simplification model for splitting and deletion with a monolingual translation model for phrase substitution and reordering. When compared against current state of the art methods, ou"
P14-1041,W08-1105,0,0.285564,"or apposition); and fewer modifiers (e.g., He slept vs. He also slept). In practice, simplification is thus often modeled using four main operations: splitting a complex sentence into several simpler sentences; dropping and reordering phrases or constituents; substituting words/phrases with simpler ones. As has been argued in previous work, sentence simplification has many potential applications. It is useful as a preprocessing step for a variety of NLP systems such as parsers and machine translation systems (Chandrasekar et al., 1996), summarisation (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008) and semantic 435 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 435–445, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics pler ones by greedily selecting the output sentence with highest probability. Using both the PWKP corpus developed by Zhu et al. (2010) and the edit history of Simple Wikipedia, Woodsend and Lapata (2011) learn a quasi synchronous grammar (Smith and Eisner, 2006) describing a loose alignment between parse trees of complex and of simple sentences. Following Dras (1999), they then gene"
P14-1041,kow-belz-2012-lg,0,0.0255635,"Missing"
P14-1041,C04-1129,0,0.156259,"on native speakers (Siddharthan, 2002). There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011). Machine Translation systems have been adapted to translate complex sentences into simple ones (Zhu et al., 2010; Wubben et al., 2012; Coster and Kauchak, 2011). And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996). In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways. First, it combines a model encoding probabilities for splitting and deletion with a monolingual machine translation module which handles reordering and substitution. In this way, we exploit the ability of statistical machine translation (SMT) systems to capture phrasal/lexical substitution and reordering while relying on a dedicated probabilistic module to capture the splitting and deletion operations which are less well"
P14-1041,W10-4213,0,0.0931557,"eleting obligatory arguments. When compared against current state of the art methods (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012), our model yields significantly simpler output that is both grammatical and meaning preserving. 2 Related Work Earlier work on sentence simplification relied on handcrafted rules to capture syntactic simplification e.g., to split coordinated and subordinated sentences into several, simpler clauses or to model active/passive transformations (Siddharthan, 2002; Chandrasekar and Srinivas, 1997; Bott et al., 2012; Canning, 2002; Siddharthan, 2011; Siddharthan, 2010). While these handcrafted approaches can encode precise and linguistically well-informed syntactic transformation (using e.g., detailed morphological and syntactic information), they are limited in scope to purely syntactic rules and do not account for lexical simplifications and their interaction with the sentential context. Using the parallel dataset formed by Simple English Wikipedia (SWKP)1 and traditional English Wikipedia (EWKP)2 , more recent work has focused on developing machine learning approaches to sentence simplification. Zhu et al. (2010) constructed a parallel corpus (PWKP) of 1"
P14-1041,W11-2802,0,0.221329,"rthan, 2002). There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011). Machine Translation systems have been adapted to translate complex sentences into simple ones (Zhu et al., 2010; Wubben et al., 2012; Coster and Kauchak, 2011). And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996). In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways. First, it combines a model encoding probabilities for splitting and deletion with a monolingual machine translation module which handles reordering and substitution. In this way, we exploit the ability of statistical machine translation (SMT) systems to capture phrasal/lexical substitution and reordering while relying on a dedicated probabilistic module to capture the splitting and deletion operations which are less well (deletion) or not"
P14-1041,W06-3104,0,0.020902,"and machine translation systems (Chandrasekar et al., 1996), summarisation (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008) and semantic 435 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 435–445, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics pler ones by greedily selecting the output sentence with highest probability. Using both the PWKP corpus developed by Zhu et al. (2010) and the edit history of Simple Wikipedia, Woodsend and Lapata (2011) learn a quasi synchronous grammar (Smith and Eisner, 2006) describing a loose alignment between parse trees of complex and of simple sentences. Following Dras (1999), they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate simplification. They evaluate their model on the same dataset used by Zhu et al. (2010) namely, an aligned corpus of 100/131 EWKP/SWKP sentences and show that they achieve better BLEU score. They also conducted a human evaluation on 64 of the 100 test sentences and showed again a better performance in terms of simplicity, grammaticality and meaning preservation. I"
P14-1041,P08-1040,0,0.34684,"Missing"
P14-1041,D11-1038,0,0.459744,"503 Vandoeuvre-l`es-Nancy, F-54500, France claire.gardent@loria.fr Abstract role labelling (Vickrey and Koller, 2008). It also has wide ranging potential societal application as a reading aid for people with aphasis (Carroll et al., 1999), for low literacy readers (Watanabe et al., 2009) and for non native speakers (Siddharthan, 2002). There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011). Machine Translation systems have been adapted to translate complex sentences into simple ones (Zhu et al., 2010; Wubben et al., 2012; Coster and Kauchak, 2011). And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996). In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways. First, it combines a model encoding probabilities for splitting and deletion with a monolingual machine translation module which"
P14-1041,P12-1107,0,0.599521,"Missing"
P14-1041,P01-1067,0,0.0961322,"mited in scope to purely syntactic rules and do not account for lexical simplifications and their interaction with the sentential context. Using the parallel dataset formed by Simple English Wikipedia (SWKP)1 and traditional English Wikipedia (EWKP)2 , more recent work has focused on developing machine learning approaches to sentence simplification. Zhu et al. (2010) constructed a parallel corpus (PWKP) of 108,016/114,924 complex/simple sentences by aligning sentences from EWKP and SWKP and used the resulting bitext to train a simplification model inspired by syntax-based machine translation (Yamada and Knight, 2001). Their simplification model encodes the probabilities for four rewriting operations on the parse tree of an input sentences namely, substitution, reordering, splitting and deletion. It is combined with a language model to improve grammaticality and the decoder translates sentences into sim3 Simplification Framework We start by motivating our approach and explaining how it relates to previous proposals w.r.t., the four main operations involved in simplification namely, splitting, deletion, substitution and reordering. We then introduce our framework. 1 SWKP (http://simple.wikipedia.org) is a c"
P14-1041,C10-1152,0,0.116745,"lso has wide ranging potential societal application as a reading aid for people with aphasis (Carroll et al., 1999), for low literacy readers (Watanabe et al., 2009) and for non native speakers (Siddharthan, 2002). There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011). Machine Translation systems have been adapted to translate complex sentences into simple ones (Zhu et al., 2010; Wubben et al., 2012; Coster and Kauchak, 2011). And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996). In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways. First, it combines a model encoding probabilities for splitting and deletion with a monolingual machine translation module which handles reordering and substitution. In this way, we exploit the ability of statistical machine translation (SMT"
P16-1146,W14-6110,0,0.141144,"Missing"
P16-1146,H91-1060,0,0.754221,"nterminals and their latent state numbers in the training data. They use the EM algorithm to split and merge nonterminals using the latent states, and optimize the number of latent states for each nonterminal such that it maximizes the likelihood of a training treebank. Their refined grammar successfully splits nonterminals to various degrees to capture their complexity. We take the analogous step with spectral methods. We propose an algorithm where we first compute Ωa on the training data and then we optimize the number of latent states for each nonterminal by optimizing the PARSEVAL metric (Black et al., 1991) on a development set. Our optimization algorithm appears in Figure 2. The input to the algorithm is training and development data in the form of parse trees, a basic spectral estimation algorithm S in its default setting, an upper bound m on the number of latent states that can be used for the different nonterminals and a beam size k which gives a maximal queue size for the beam. The algorithm aims to learn a function f that maps each nonterminal a to the number of latent states. It initializes f by estimating a default grammar GS : (N , I, P, fS , n) using S and setting f = fS . It then iter"
P16-1146,P05-1022,0,0.769903,"van (rep)” are vanilla estimations (i.e., each nonterminal is mapped to fixed number of latent states) replacing rare words by POS or POS+morphological signatures, respectively. The best of these two models is used with our optimization algorithm in “opt”. For Sp, “van” uses the best setting for unknown words as Cl. Best result in each column from the first seven rows is in bold. In addition, our best performing models from rows 3-7 are marked with ∗ . “Bk multiple” shows the best results with the multiple models using product-of-grammars procedure (Petrov, 2010) and discriminative reranking (Charniak and Johnson, 2005). “Cl multiple” gives the results with multiple models generated using the noise induction and decoded using the hierarchical decoding (Narayan and Cohen, 2015). Bk results are not available on the development dataset for German-N. For others, we report Bk results from Bj¨orkelund et al. (2013). We also include results from Hall et al. (2014) and Crabb´e (2015). Sp Cl lang. Bk van opt van opt Bk multiple Cl multiple Hall et al. ’14 F&M ’15 Crabb´e ’15 Basque 74.7 79.6 81.4∗ 79.9 80.5 87.9 83.4 83.4 85.9 84.9 French 80.4 74.3 75.6 78.7 79.1∗ 82.9 80.4 79.7 78.8 80.8 German-N 80.1 76.4 78.0 78.4"
P16-1146,P14-1099,1,0.848693,"e-to-fine fashion: merging and splitting nonterminals using the latent states to optimize the number of latent states for each nonterminal. Cohen et al. (2012) presented a so-called spectral algorithm to estimate L-PCFGs. This algorithm uses linear-algebraic procedures such as singular value decomposition (SVD) during learning. The spectral algorithm of Cohen et al. builds on an estimation algorithm for HMMs by Hsu et al. (2009).1 Cohen et al. (2013) experimented with this spectral algorithm for parsing English. A different variant of a spectral learning algorithm for L-PCFGs was developed by Cohen and Collins (2014). It breaks the problem of L-PCFG estimation into multiple convex optimization problems which are solved using EM. The family of L-PCFG spectral learning algorithms was further extended by Narayan and Cohen (2015). They presented a simplified version of the algorithm of Cohen et al. (2012) that estimates sparse grammars and assigns probabilities (instead of weights) to the rules in the grammar, and as such does not suffer from the problem of negative probabilities that arise with the original spectral algorithm (see discussion in Cohen et al., 2013). In this paper, we use the algorithms by Nar"
P16-1146,P12-1024,1,0.888932,"t the EM algorithm does not estimate state-of-the-art parsing models for English is that the EM algorithm does not control well for the model size used in the parser – the number of latent states associated with the various nonterminals in the grammar. As such, they introduced a coarse-to-fine technique to estimate the grammar. It splits and merges nonterminals (with latent state information) with the aim to optimize the likelihood of the training data. Together with other types of fine tuning of the parsing model, this led to state-of-the-art results for English parsing. In more recent work, Cohen et al. (2012) described a different family of estimation algorithms for L-PCFGs. This so-called “spectral” family of learning algorithms is compelling because it offers a rigorous theoretical analysis of statistical convergence, and sidesteps local maxima issues that arise with the EM algorithm. While spectral algorithms for L-PCFGs are compelling from a theoretical perspective, they have been lagging behind in their empirical results on the problem of parsing. In this paper we show that one of the main reasons for that is that spectral algorithms require a more careful tuning procedure for the number of l"
P16-1146,N13-1015,1,0.0708295,"s compelling because it offers a rigorous theoretical analysis of statistical convergence, and sidesteps local maxima issues that arise with the EM algorithm. While spectral algorithms for L-PCFGs are compelling from a theoretical perspective, they have been lagging behind in their empirical results on the problem of parsing. In this paper we show that one of the main reasons for that is that spectral algorithms require a more careful tuning procedure for the number of latent states than that which has been advocated for until now. In a sense, the relationship between our work and the work of Cohen et al. (2013) is analogous to the relationship between the work by Petrov et al. (2006) and the work by Matsuzaki et al. (2005): we suggest a technique for optimizing the number of latent states for spectral algorithms, and test it on eight languages. Our results show that when the number of latent states is optimized using our technique, the parsing models the spectral algorithms yield perform significantly better than the vanilla-estimated models, and for most of the languages – better than the Berkeley parser of Petrov et al. (2006). As such, the contributions of this parser are twofold: • We describe a"
P16-1146,D15-1212,0,0.0326805,"Missing"
P16-1146,P15-1147,0,0.161358,"Missing"
P16-1146,P14-1022,0,0.229121,"rd, and information is lost in this conversion. See also (Rabusseau et al., 2016). 1547 VP S V chased NP NP VP D N D N the cat the mouse Figure 1: The inside tree (left) and outside tree (right) for the nonterminal VP in the parse tree (S (NP (D the) (N mouse)) (VP (V chased) (NP (D the) (N cat)))) for the sentence “the mouse chased the cat.” et al. (2012), and we compare them against stateof-the-art L-PCFG parsers such as the Berkeley parser (Petrov et al., 2006). We also compare our algorithms to other state-of-the-art parsers where elaborate linguistically-motivated feature specifications (Hall et al., 2014), annotations (Crabb´e, 2015) and formalism conversions (Fern´andezGonz´alez and Martins, 2015) are used. 3 Optimizing Spectral Estimation In this section, we describe our optimization algorithm and its motivation. 3.1 Spectral Learning of L-PCFGs and Model Size The family of spectral algorithms for latentvariable PCFGs rely on feature functions that are defined for inside and outside trees. Given a tree, the inside tree for a node contains the entire subtree below that node; the outside tree contains everything in the tree excluding the inside tree. Figure 1 shows an example of inside and out"
P16-1146,D10-1004,0,0.0306015,"for the spectral algorithm of Narayan and Cohen (2015). We experimented with several versions of k-means, and discovered that the version that works best in a set of preliminary experiments is hard k-means.5 Decoding and evaluation For efficiency, we use a base PCFG without latent states to prune marginals which receive a value less than 0.00005 in the dynamic programming chart. This is just a bare-bones PCFG that is estimated using maximum likelihood estimation (with frequency count). The parser takes part-of-speech tagged sentences as input. We tag the German-N data using the Turbo Tagger (Martins et al., 2010). For the languages in the SPMRL data we use the MarMot tagger of M¨ueller et al. (2013) to jointly predict the POS and morphological tags.6 The parser itself can assign different part-of-speech tags to words to avoid parse failure. This is also particularly important for constituency parsing with morphologically rich languages. It helps mitigate the problem of the taggers to assign correct tags when long-distance dependencies are present. For all results, we report the F1 measure of the PARSEVAL metric (Black et al., 1991). We use the EVALB program7 with the parameter file COLLINS.prm (Collin"
P16-1146,P05-1010,0,0.721121,"between the different nonterminals. In addition, we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish. Our results show that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages. 1 Introduction Latent-variable probabilistic context-free grammars (L-PCFGs) have been used in the natural language processing community (NLP) for syntactic parsing for over a decade. They were introduced in the NLP community by Matsuzaki et al. (2005) and Prescher (2005), with Matsuzaki et al. using the expectation-maximization (EM) algorithm to estimate them. Their performance on syntactic parsing of English at that stage lagged behind state-of-the-art parsers. Petrov et al. (2006) showed that one of the reasons that the EM algorithm does not estimate state-of-the-art parsing models for English is that the EM algorithm does not control well for the model size used in the parser – the number of latent states associated with the various nonterminals in the grammar. As such, they introduced a coarse-to-fine technique to estimate the grammar."
P16-1146,D13-1032,0,0.0855828,"Missing"
P16-1146,D15-1214,1,0.0799359,"L-PCFGs. This algorithm uses linear-algebraic procedures such as singular value decomposition (SVD) during learning. The spectral algorithm of Cohen et al. builds on an estimation algorithm for HMMs by Hsu et al. (2009).1 Cohen et al. (2013) experimented with this spectral algorithm for parsing English. A different variant of a spectral learning algorithm for L-PCFGs was developed by Cohen and Collins (2014). It breaks the problem of L-PCFG estimation into multiple convex optimization problems which are solved using EM. The family of L-PCFG spectral learning algorithms was further extended by Narayan and Cohen (2015). They presented a simplified version of the algorithm of Cohen et al. (2012) that estimates sparse grammars and assigns probabilities (instead of weights) to the rules in the grammar, and as such does not suffer from the problem of negative probabilities that arise with the original spectral algorithm (see discussion in Cohen et al., 2013). In this paper, we use the algorithms by Narayan and Cohen (2015) and Cohen 1 A related algorithm for weighted tree automata (WTA) was developed by Bailly et al. (2010). However, the conversion from L-PCFGs to WTA is not straightforward, and information is"
P16-1146,P06-1055,0,0.168249,"hat our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages. 1 Introduction Latent-variable probabilistic context-free grammars (L-PCFGs) have been used in the natural language processing community (NLP) for syntactic parsing for over a decade. They were introduced in the NLP community by Matsuzaki et al. (2005) and Prescher (2005), with Matsuzaki et al. using the expectation-maximization (EM) algorithm to estimate them. Their performance on syntactic parsing of English at that stage lagged behind state-of-the-art parsers. Petrov et al. (2006) showed that one of the reasons that the EM algorithm does not estimate state-of-the-art parsing models for English is that the EM algorithm does not control well for the model size used in the parser – the number of latent states associated with the various nonterminals in the grammar. As such, they introduced a coarse-to-fine technique to estimate the grammar. It splits and merges nonterminals (with latent state information) with the aim to optimize the likelihood of the training data. Together with other types of fine tuning of the parsing model, this led to state-of-the-art results for Eng"
P16-1146,N10-1003,0,0.378638,"ohen et al. (2013). In Cl, “van (pos)” and “van (rep)” are vanilla estimations (i.e., each nonterminal is mapped to fixed number of latent states) replacing rare words by POS or POS+morphological signatures, respectively. The best of these two models is used with our optimization algorithm in “opt”. For Sp, “van” uses the best setting for unknown words as Cl. Best result in each column from the first seven rows is in bold. In addition, our best performing models from rows 3-7 are marked with ∗ . “Bk multiple” shows the best results with the multiple models using product-of-grammars procedure (Petrov, 2010) and discriminative reranking (Charniak and Johnson, 2005). “Cl multiple” gives the results with multiple models generated using the noise induction and decoded using the hierarchical decoding (Narayan and Cohen, 2015). Bk results are not available on the development dataset for German-N. For others, we report Bk results from Bj¨orkelund et al. (2013). We also include results from Hall et al. (2014) and Crabb´e (2015). Sp Cl lang. Bk van opt van opt Bk multiple Cl multiple Hall et al. ’14 F&M ’15 Crabb´e ’15 Basque 74.7 79.6 81.4∗ 79.9 80.5 87.9 83.4 83.4 85.9 84.9 French 80.4 74.3 75.6 78.7 7"
P16-1146,W05-1512,0,0.790877,"rminals. In addition, we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish. Our results show that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages. 1 Introduction Latent-variable probabilistic context-free grammars (L-PCFGs) have been used in the natural language processing community (NLP) for syntactic parsing for over a decade. They were introduced in the NLP community by Matsuzaki et al. (2005) and Prescher (2005), with Matsuzaki et al. using the expectation-maximization (EM) algorithm to estimate them. Their performance on syntactic parsing of English at that stage lagged behind state-of-the-art parsers. Petrov et al. (2006) showed that one of the reasons that the EM algorithm does not estimate state-of-the-art parsing models for English is that the EM algorithm does not control well for the model size used in the parser – the number of latent states associated with the various nonterminals in the grammar. As such, they introduced a coarse-to-fine technique to estimate the grammar. It splits and merge"
P16-1146,A97-1014,0,0.284431,"t sets. Eight out of the nine datasets (Basque, French, German-T, Hebrew, Hungarian, Korean, Polish 2 It has been documented in several papers that the family of spectral estimation algorithms is faster than algorithms such as EM, not just for L-PCFGs. See, for example, Parikh et al. (2012). and Swedish) are taken from the workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL; Seddah et al., 2013). The German corpus in the SPMRL workshop is taken from the TiGer corpus (German-T, Brants et al., 2004). We also experiment with another German corpus, the NEGRA corpus (German-N, Skut et al., 1997), in a standard evaluation split.3 Words in the SPMRL datasets are annotated with their morphological signatures, whereas the NEGRA corpus does not contain any morphological information. Data preprocessing and treatment of rare words We convert all trees in the treebanks to a binary form, train and run the parser in that form, and then transform back the trees when doing evaluation using the PARSEVAL metric. In addition, we collapse unary rules into unary chains, so that our trees are fully binarized. The column “#nts” in Table 1 shows the number of nonterminals after binarization in the vario"
P16-1146,E14-1015,0,0.0383947,"Missing"
P16-1146,J03-4003,0,\N,Missing
P16-1146,W13-4916,0,\N,Missing
P17-1017,W13-2111,1,0.858118,"Several domain specific data-text corpora have been built by researchers to train and evaluate NLG systems. In the sports domain, Chen and Mooney (2008) constructed a dataset mapping soccer games events to text which consists of 1,539 data-text pairs and a vocabulary of 214 words. For weather forecast generation, the dataset of Liang et al. (2009) includes 29,528 data-text pairs with a vocabulary of 345 words. For the air travel domain, Ratnaparkhi (2000) created a dataset consisting of 5,426 datatext pairs with a richer vocabulary (927 words) and in the biology domain, the KBGen shared task (Banik et al., 2013) made available 284 data-text pairs where the data was extracted from an existing knowledge base and the text was authored by biology experts. An important limitation of these datasets is that, because they are domain specific, systems learned from them are restricted to generating domain specific, often strongly stereotyped text (e.g., weather forecast or soccer game commentator reports). Arguably, training corpora for NLG should support the learning of more generic systems capable of handling a much wider range of linguistic interactions than is present in stereotyped texts. By nature howeve"
P17-1017,C16-1105,0,0.104502,"ecently, data-to-text benchmarks have also been created by associating data units with text using crowdsourcing. Wen et al. (2016) first created data by enumerating all possible combinations of 14 dialog act types (e.g., request, inform) and attribute-value pairs present in four small-size, hand-written ontologies about TVs, laptops, restaurants and hotels. They then use crowdsourcing to associate each data unit with a text. The resulting dataset is both large and varied (4 domains) and was successfully exploited to train neural and imitation learning data-to-text generator (Wen et al., 2016; Lampouras and Vlachos, 2016). Similarly, Novikova and Rieser (2016) described a framework for collecting data-text pairs using automatic quality control measures and evaluating how the type of the input representations (text vs pictures) impacts the quality of crowdsourced text. The crowdsourcing approach to creating inputtext corpora has several advantages. First, it is low cost in that the data is produced automatically and the text is authored by a crowdworker. This is in stark contrast with the previous approach where expert linguists are required to align text with data. Benchmarks constructed from “expert” linguist"
P17-1017,D16-1128,0,0.130034,"an ours, it is less diverse both in terms of input and in terms of text. We thus propose our corpus generation framework as a novel method for creating challenging data sets from which NLG models can be learned which are capable of handling the complex interactions occurring during in micro-planning between lexicalisation, aggregation, surface realisation, referring expression generation and sentence segmentation. To encourage researchers to take up this challenge, we recently made available a dataset created using this framework in the context of the W EB NLG shared task. 1 1 We ignore here (Lebret et al., 2016)’s dataset which was created fully automatically from Wikipedia by associating infoboxes with text because this dataset fails to ensure an adequate match between data and text. We manually examined 50 input/output pairs randomly extracted from this dataset and did not find a single example where data and text matched. As such, this dataset is ill-suited for training microplanners. Moreover, since its texts contain both missing and additional information, it cannot be used to train joint models for content selection and micro-planning either. Introduction To train Natural Language Generation (N"
P17-1017,P09-1011,0,0.0280802,"than on the existing ones. We thus propose our corpus generation framework as a novel method for creating challenging data sets from which NLG models can be learned which are capable of generating complex texts from KB data. 2 NLG Benchmarks Domain specific benchmarks. Several domain specific data-text corpora have been built by researchers to train and evaluate NLG systems. In the sports domain, Chen and Mooney (2008) constructed a dataset mapping soccer games events to text which consists of 1,539 data-text pairs and a vocabulary of 214 words. For weather forecast generation, the dataset of Liang et al. (2009) includes 29,528 data-text pairs with a vocabulary of 345 words. For the air travel domain, Ratnaparkhi (2000) created a dataset consisting of 5,426 datatext pairs with a richer vocabulary (927 words) and in the biology domain, the KBGen shared task (Banik et al., 2013) made available 284 data-text pairs where the data was extracted from an existing knowledge base and the text was authored by biology experts. An important limitation of these datasets is that, because they are domain specific, systems learned from them are restricted to generating domain specific, often strongly stereotyped tex"
P17-1017,P14-5010,0,0.00200382,"a Bachelor of Science degree at the University of Texas at Austin in 1955 and was chosen by NASA in 1963. 10 Recall from section 3 that input patterns are inputs where subjects and objects have been remove thus, in essence, an input pattern is the set of all the attributes occurring in a given input. 185 As illustrated by the contrast between Examples (6) and (7) above, text length (number of tokens per text) and the number of sentences per text are strong indicators of the complexity of the generation task. We use the Stanford Part-Of-Speech Tagger and Parser version 3.5.2 (dated 2015-0420, Manning et al. 2014) to tokenize and to perform sentence segmentation on text. As shown in Table 4, W EB NLG’s texts are longer both in terms of tokens and in terms of number of sentences per text. Another difference between the two datasets is that W EB NLG contains a higher number of text per input thereby providing a better basis for learning paraphrases. Nb. Text / Input Text Length (avg/median/min/max) Nb. Sentence / Text (avg/median/min/max) Nb. Tokens Nb. Types Lexical Sophistication CTTR W EB NLG 2.63 24.36/23/4/80 RNNLG 1.38 18.37/19/1/76 1.45/1/1/6 1.25/1/1/6 290479 2992 0.69 3.93 531871 3524 0.54 3.42"
P17-1017,mendes-etal-2012-dbpedia,0,0.0189715,"using the SRILM toolkit (Stolcke, 2002): one model (S-Model) for bigrams occurring in sibling triples (triples with a shared subject); one model (C-Model) for bigrams occurring in chained triples (the object of one triple is the subject of the other); and one model (M-Model) which is a linear interpolation of the sibling and the chain model. The intuition is that these sibDBPedia To illustrate the functioning of our benchmark creation framework, we apply it to DBPedia. DBPedia is a multilingual knowledge base that was built from various kinds of structured information contained in Wikipedia (Mendes et al., 2012). This data is stored as RDF (Resource Description Format) triples of the form (subject, property, object) where the subject is a URI (Uniform Resource Identifier), the property is a binary relation and the object is either a URI or a literal value such as a string, a date or a number. We use an English version of the DBPedia knowledge base which encompasses 6.2M entities, 739 classes, 1,099 properties with reference values and 1,596 properties 6 http://wiki.dbpedia.org/ dbpedia-dataset-version-2015-10 7 An entity graph for some entity e is a graph obtained by traversing the DBPedia graph star"
P17-1017,W16-6627,0,0.0281422,"o been created by associating data units with text using crowdsourcing. Wen et al. (2016) first created data by enumerating all possible combinations of 14 dialog act types (e.g., request, inform) and attribute-value pairs present in four small-size, hand-written ontologies about TVs, laptops, restaurants and hotels. They then use crowdsourcing to associate each data unit with a text. The resulting dataset is both large and varied (4 domains) and was successfully exploited to train neural and imitation learning data-to-text generator (Wen et al., 2016; Lampouras and Vlachos, 2016). Similarly, Novikova and Rieser (2016) described a framework for collecting data-text pairs using automatic quality control measures and evaluating how the type of the input representations (text vs pictures) impacts the quality of crowdsourced text. The crowdsourcing approach to creating inputtext corpora has several advantages. First, it is low cost in that the data is produced automatically and the text is authored by a crowdworker. This is in stark contrast with the previous approach where expert linguists are required to align text with data. Benchmarks constructed from “expert” linguistic annotations. NLG benchmarks have als"
P17-1017,P02-1040,0,0.118522,"Missing"
P17-1017,C16-1141,1,0.547657,"ercising the main subtasks of microplanning. For instance, in Example (1) above, given the input shown in (1a), generating (1b) involves lexicalising the occupation property as the phrase worked as (lexicalisation); using PP coordination (born in San Antonio on 1942-08-26) to avoid repeating the word born (aggregation); and verbalising the three triples using a single complex sentence including an apposition, a PP coordination and a transitive verb construction (sentence segmentation and surface realisation). 3.1 3.2 Selecting Content To create data units, we adapted the procedure outlined by Perez-Beltrachini et al. (2016) and sketched in Figure 2. This method can be summarised as follows. First, DBPedia category graphs are extracted from DBPedia by retrieving up to 500 entity graphs for entities of the same category.7 For example, we build a category graph for the Astronaut category by collecting, graphs of depth five for 500 entities of types astronaut. Next, category graphs are used to learn bi-gram models of DBPedia properties which specify the probability of two properties co-occuring together. Three types of bi-gram models are extracted from category graphs using the SRILM toolkit (Stolcke, 2002): one mod"
P17-1017,A00-2026,0,0.102559,"enging data sets from which NLG models can be learned which are capable of generating complex texts from KB data. 2 NLG Benchmarks Domain specific benchmarks. Several domain specific data-text corpora have been built by researchers to train and evaluate NLG systems. In the sports domain, Chen and Mooney (2008) constructed a dataset mapping soccer games events to text which consists of 1,539 data-text pairs and a vocabulary of 214 words. For weather forecast generation, the dataset of Liang et al. (2009) includes 29,528 data-text pairs with a vocabulary of 345 words. For the air travel domain, Ratnaparkhi (2000) created a dataset consisting of 5,426 datatext pairs with a richer vocabulary (927 words) and in the biology domain, the KBGen shared task (Banik et al., 2013) made available 284 data-text pairs where the data was extracted from an existing knowledge base and the text was authored by biology experts. An important limitation of these datasets is that, because they are domain specific, systems learned from them are restricted to generating domain specific, often strongly stereotyped text (e.g., weather forecast or soccer game commentator reports). Arguably, training corpora for NLG should suppo"
P17-1017,N16-1015,0,0.0437239,"Missing"
P17-1017,W11-2832,0,\N,Missing
P18-1188,P16-1046,1,0.880852,"re large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence"
P18-1188,D15-1042,0,0.0909022,"Missing"
P18-1188,D14-1181,0,0.010422,"y and 0 otherwise. Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 2017). The main components include a sentence encoder, a document encoder, and a novel sentence extractor (see Figure 1) that we describe in more detail below. The novel characteristics of our model are that each sentence is labeled by implicitly estimating its (local and global) relevance to the document and by directly attending to some external information for importance cues. Sentence Encoder A core component of our model is a convolutional sentence encoder (Kim, 2014; Kim et al., 2016) which encodes sentences into continuous representations. We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature. This filter is applied to each possible window of words in s to produce a feature map f ∈ Rk−h+1 where k is the sentence length. We then apply max-pooling over time over the feature map f and take the maximum value as the feature corresponding to this particular filter K. We use multiple kernels of various sizes and each kernel multiple times to construct the representation of a se"
P18-1188,P15-1107,0,0.0268866,"e times each. The max-pooling over time operation yields two feature lists f K2 and f K4 ∈ R3 . The final sentence embeddings have six dimensions. Document encoder Document Encoder The document encoder composes a sequence of sentences to obtain a document representation. We use a recurrent neural network with LSTM cells to avoid the vanishing gradient problem when training long sequences (Hochreiter and Schmidhuber, 1997). Given a document D consisting of a sequence of sentences (s1 , s2 , . . . , sn ), we follow common practice and feed the sentences in reverse order (Sutskever et al., 2014; Li et al., 2015; Filippova et al., 2015). Sentence Extractor Our sentence extractor sequentially labels each sentence in a document with 1 or 0 by implicitly estimating its relevance in the document and by directly attending to the external information for importance cues. It is implemented with another RNN with LSTM cells with an attention mechanism (Bahdanau et al., 2015) and a softmax layer. Our attention mechanism differs from the standard practice of attending intermediate states of the input (encoder). Instead, our extractor attends to a sequence of p pieces of external information E : (e1 , e2 , ...,"
P18-1188,N03-1020,0,0.198902,"e table present different variants of XN ET. We experimented with three types of external information: title (TITLE), image captions (CAPTION) and the first sentence (FS) of the document. The bottom block of the table presents models with more than one type of external information. The best performing model (highlighted in boldface) is used on the test set. ata (2016) report only on the DailyMail dataset. We used their code (https://github.com/ cheng6076/NeuralSum) to produce results on the CNN dataset.5 Automatic Evaluation To automatically assess the quality of our summaries, we used ROUGE (Lin and Hovy, 2003), a recall-oriented metric, to compare our model-generated summaries to manually-written highlights.6 Previous work has reported ROUGE-1 (R1) and ROUGE-2 (R2) scores to access informativeness, and ROUGE-L (RL) to access fluency. In addition to R1, R2 and RL, we also report ROUGE-3 (R3) and ROUGE-4 (R4) capturing higher order n-grams overlap to assess informativeness and fluency simultaneously. teresting direction of research but we do not pursue it here. It requires decoding with multiple types of attentions and this is not the focus of this paper. 5 We are unable to compare our results to the"
P18-1188,D15-1106,0,0.0230143,"es in problems such as language modeling. However, document modeling, a key to many natural language ∗ The first three authors made equal contributions to this paper. The work was done when the second author was visiting Edinburgh. 1 Our TensorFlow code and datasets are publicly available at https://github.com/shashiongithub/ Document-Models-with-Ext-Information. understanding tasks, is still an open challenge. Recently, some neural network architectures were proposed to capture large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehensio"
P18-1188,D16-1147,0,0.349286,"posed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016) and Narayan et al. (2018) in that it derives the do"
P18-1188,N18-1158,1,0.835886,"on (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016) and Narayan et al. (2018) in that it derives the document meaning representation from its sentences and their constituent words. Our novel sentence extractor combines this document meaning representation with an attention mechanism (Bahdanau et al., 2015) over the external information to label sentences from the input document. Our model explicitly biases the extractor with external cues and 2020 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2020–2030 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics implicitly bias"
P18-1188,D16-1264,0,0.264849,"each sentence in the document and the question and return the sentence with highest score in an isolated manner (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016). Our model with ISF and IDF scores as external features achieves competitive results for answer selection. Our ensemble model combining scores from our model and word overlap scores using a logistic regression layer achieves state-ofthe-art results on the popular question answering datasets WikiQA (Yang et al., 2015) and NewsQA (Trischler et al., 2016), and it obtains comparable results to the state of the art for SQuAD (Rajpurkar et al., 2016). We also evaluate our approach on the MSMarco dataset (Nguyen et al., 2016) and elaborate on the behavior of our machine reader in a scenario where each candidate answer sentence is contextually independent of each other. 2 Document Modeling For Sentence Extraction Given a document D consisting of a sequence of n sentences (s1 , s2 , ..., sn ) , we aim at labeling each sentence si in D with a label yi ∈ {0, 1} where yi = 1 indicates that si is extraction-worthy and 0 otherwise. Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 20"
P18-1188,D15-1044,0,0.105793,"Missing"
P18-1188,P17-1099,0,0.479627,"et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the ar"
P18-1188,P17-1108,0,0.515521,"t al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the architectures proposed"
P18-1188,Q16-1019,0,0.271774,"do not use this information. We also conduct a human evaluation to judge which type of summary participants prefer. Our results overwhelmingly show that human subjects find our summaries more informative and complete. Lastly, with the machine reading capabilities of our model, we confirm that a full document needs to be “read” to produce high quality extracts allowing a rich contextual reasoning, in contrast to previous answer selection approaches that often measure a score between each sentence in the document and the question and return the sentence with highest score in an isolated manner (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016). Our model with ISF and IDF scores as external features achieves competitive results for answer selection. Our ensemble model combining scores from our model and word overlap scores using a logistic regression layer achieves state-ofthe-art results on the popular question answering datasets WikiQA (Yang et al., 2015) and NewsQA (Trischler et al., 2016), and it obtains comparable results to the state of the art for SQuAD (Rajpurkar et al., 2016). We also evaluate our approach on the MSMarco dataset (Nguyen et al., 2016) and elaborate on the behavior"
P18-1188,N16-1090,0,0.0300827,"e second author was visiting Edinburgh. 1 Our TensorFlow code and datasets are publicly available at https://github.com/shashiongithub/ Document-Models-with-Ext-Information. understanding tasks, is still an open challenge. Recently, some neural network architectures were proposed to capture large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide docum"
P18-1188,P16-1125,0,0.0303368,"long-term dependencies in problems such as language modeling. However, document modeling, a key to many natural language ∗ The first three authors made equal contributions to this paper. The work was done when the second author was visiting Edinburgh. 1 Our TensorFlow code and datasets are publicly available at https://github.com/shashiongithub/ Document-Models-with-Ext-Information. understanding tasks, is still an open challenge. Recently, some neural network architectures were proposed to capture large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine r"
P18-1188,P17-1018,0,0.0276416,"well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016) and Narayan et al. (2018) in that it derives the document meaning representation from its sentences and their cons"
P18-1188,C16-1127,0,0.0908422,"t a human evaluation to judge which type of summary participants prefer. Our results overwhelmingly show that human subjects find our summaries more informative and complete. Lastly, with the machine reading capabilities of our model, we confirm that a full document needs to be “read” to produce high quality extracts allowing a rich contextual reasoning, in contrast to previous answer selection approaches that often measure a score between each sentence in the document and the question and return the sentence with highest score in an isolated manner (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016). Our model with ISF and IDF scores as external features achieves competitive results for answer selection. Our ensemble model combining scores from our model and word overlap scores using a logistic regression layer achieves state-ofthe-art results on the popular question answering datasets WikiQA (Yang et al., 2015) and NewsQA (Trischler et al., 2016), and it obtains comparable results to the state of the art for SQuAD (Rajpurkar et al., 2016). We also evaluate our approach on the MSMarco dataset (Nguyen et al., 2016) and elaborate on the behavior of our machine reader in a scenario where ea"
P18-1188,K17-1028,0,0.0234859,"RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016) and Narayan et al. (2018) in that it derives the document meaning representat"
P18-1188,D15-1237,0,0.0852545,"Missing"
P18-1188,N16-1174,0,0.0542042,"language modeling. However, document modeling, a key to many natural language ∗ The first three authors made equal contributions to this paper. The work was done when the second author was visiting Edinburgh. 1 Our TensorFlow code and datasets are publicly available at https://github.com/shashiongithub/ Document-Models-with-Ext-Information. understanding tasks, is still an open challenge. Recently, some neural network architectures were proposed to capture large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 20"
P19-1330,N18-1064,0,0.0407251,"Missing"
P19-1330,W11-2101,0,0.0860651,"Missing"
P19-1330,P18-1015,0,0.0342298,", 2005) must be measured separately from “fluency”, as judgments for them had low correlation. Finally, we make the highlighted XS UM dataset, codebase to replicate the crowd-sourcing experiments and all other materials produced in our study publicly available. 2 Literature Review In recent years, summarization literature has investigated different means of conducting manual evaluation. We study a sample of 26 recent papers from major ACL conferences and outline the trends of manual evaluation in summarization in Table 1. From 26 papers, 11 papers (e.g., See et al., 2017; Kedzie et al., 2018; Cao et al., 2018) did not conduct any manual evaluation. Following the Document Understanding Conference (DUC, Dang, 2005), a majority of work has focused on evaluating the content and the linguistic quality of summaries (Nenkova, 2005). However, there seems to be a lack of consensus on how a summary should be evaluated: (i) Should it be evaluated relative to other summaries or standalone in absolute terms? and (ii) What would be a good source of comparison: the input document or the reference summary? The disagreements on these issues result in authors evaluating their summaries often (11 out of 26 papers) us"
P19-1330,N18-1150,0,0.337783,"ning multiple ones increases dataset creation cost, thus evaluation against them is likely to exhibit reference bias (Louis and Nenkova, 2013; Fomicheva and Specia, 2016), penalizing summaries containing salient content different from the reference. For the above reasons manual evaluation is considered necessary for measuring progress in summarization. However, the intrinsic difficulty of the task has led to research without manual evaluation or only fluency being assessed manually. Those that conduct manual assessment of the content, typically use a single reference summary, either directly (Celikyilmaz et al., 2018; Tan et al., 2017) or through questions (Narayan et al., 2018b,c) and thus are also likely to exhibit reference bias. In this paper we propose a novel approach for manual evaluation, H IGHlight-based Referenceless Evaluation of document Summarization (H IGH RES), in which a summary is assessed against the source document via manually highlighted salient content in the latter (see Figure 1 for an example). Our approach avoids reference bias, as the multiple highlights obtained help consider more content than what is contained in a single reference. The highlights are not dependent on the summa"
P19-1330,N18-2097,0,0.0682798,"Missing"
P19-1330,P16-2013,0,0.172488,"g of the Association for Computational Linguistics, pages 3381–3392 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics automatic measures are unlikely to be sufficient to measure performance in summarization (Schluter, 2017), also known for other tasks in which the goal is to generate natural language (Novikova et al., 2017). Furthermore, the datasets typically considered have a single reference summary, as obtaining multiple ones increases dataset creation cost, thus evaluation against them is likely to exhibit reference bias (Louis and Nenkova, 2013; Fomicheva and Specia, 2016), penalizing summaries containing salient content different from the reference. For the above reasons manual evaluation is considered necessary for measuring progress in summarization. However, the intrinsic difficulty of the task has led to research without manual evaluation or only fluency being assessed manually. Those that conduct manual assessment of the content, typically use a single reference summary, either directly (Celikyilmaz et al., 2018; Tan et al., 2017) or through questions (Narayan et al., 2018b,c) and thus are also likely to exhibit reference bias. In this paper we propose a"
P19-1330,N18-1065,0,0.0767735,"Missing"
P19-1330,P18-1064,0,0.0146091,"s annotators. Finally, a small number of work evaluates the ”Correctness” (Chen and Bansal, 2018; Li et al., 2018b; Chen and Bansal, 2018) of the summary, similar to fact checking (Vlachos and Riedel, 2014), which can be a challenging task in its own right. The linguistic quality of a summary encompasses many different qualities such as fluency, grammatically, readability, formatting, naturalness and coherence. Most recent work uses a single human judgment to capture all linguistic qualities of the summary (Hsu et al., 2018; Kry´sci´nski et al., 2018; Narayan et al., 2018b; Song et al., 2018; Guo et al., 2018); we group them under “Fluency” in Table 1 with an exception of “Clarity” which 3383 was evaluated in the DUC evaluation campaigns (Dang, 2005). The “Clarity” metric puts emphasis in easy identification of noun and pronoun phrases in the summary which is a different dimension than “Fluency”, as a summary may be fluent but difficult to be understood due to poor clarity. et al., 2018; Narayan et al., 2018a; Hsu et al., 2018; Kry´sci´nski et al., 2018), asking judges to assess the summary after reading the source document. However this requires more effort and is known to lead to low inter-annota"
P19-1330,D18-1086,1,0.833109,"In relative assessment of summarization, annotators are shown two or more summaries and are asked to rank them according to the dimension at question (Yang et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Guo et al., 2018; Krishna and Srinivasan, 2018). The relative assessment is often done using the paired comparison (Thurstone, 1994) or the best-worst scaling (Woodworth and G, 1991; Louviere et al., 2015), to improve inter-annotator agreement. On the other hand, absolute assessment of summarization (Li et al., 2018b; Song et al., 2018; Kry´sci´nski et al., 2018; Hsu et al., 2018; Hardy and Vlachos, 2018) is often done using the Likert rating scale (Likert, 1932) where a summary is assessed on a numerical scale. Absolute assessment was also employed in combination with the question answering approach for content evaluation (Narayan et al., 2018b; Mendes et al., 2019). Both approaches, relative ranking and absolute assessment, have been investigated extensively in Machine Translation (Bojar et al., 2016, 2017). Absolute assessment correlates highly with the relative assessment without the bias introduced by having a simultaneous assessment of several models (Bojar et al., 2011). 3 Choice of Ref"
P19-1330,P18-1063,0,0.395164,"nd (ii) What would be a good source of comparison: the input document or the reference summary? The disagreements on these issues result in authors evaluating their summaries often (11 out of 26 papers) using automatic measures such as ROUGE (Lin, 2004) despite of its limitations (Schluter, 2017). In what follows, we discuss previously proposed approaches along three axes: evaluation metrics, relative vs. absolute, and the choice of reference. Evaluation Metrics Despite differences in the exact definitions, the majority (e.g., Hsu et al., 2018; Celikyilmaz et al., 2018; Narayan et al., 2018b; Chen and Bansal, 2018; Peyrard and Gurevych, 2018) agree on both or either one of two broad quality definitions: coverage determines how much of the salient content of the source document is captured in the summary, and informativeness, how much of the content captured in the summary is salient with regards to the original document. These measures correspond to “recall” and “precision” metrics respectively in Table 1, notions that are commonly used 3382 X X X X X X X X X With Ref. & Doc. Relative X With Document Absolute X X X With Reference Precision X X Recall Clarity Fluency Correctness QA Pyramid No Manual Eva"
P19-1330,P18-1013,0,0.384447,"ated relative to other summaries or standalone in absolute terms? and (ii) What would be a good source of comparison: the input document or the reference summary? The disagreements on these issues result in authors evaluating their summaries often (11 out of 26 papers) using automatic measures such as ROUGE (Lin, 2004) despite of its limitations (Schluter, 2017). In what follows, we discuss previously proposed approaches along three axes: evaluation metrics, relative vs. absolute, and the choice of reference. Evaluation Metrics Despite differences in the exact definitions, the majority (e.g., Hsu et al., 2018; Celikyilmaz et al., 2018; Narayan et al., 2018b; Chen and Bansal, 2018; Peyrard and Gurevych, 2018) agree on both or either one of two broad quality definitions: coverage determines how much of the salient content of the source document is captured in the summary, and informativeness, how much of the content captured in the summary is salient with regards to the original document. These measures correspond to “recall” and “precision” metrics respectively in Table 1, notions that are commonly used 3382 X X X X X X X X X With Ref. & Doc. Relative X With Document Absolute X X X With Reference P"
P19-1330,J10-3005,0,0.114096,"presents papers that do not report on human evaluation; the second column identifies matrices used for evaluating content (“Pyramid”, “QA”, “Correctness”, “Recall” and “Precision”) and quality (“Clarity”, “Fluency”) of summaries; the third column focuses if the system ranking reported by humans on content evaluation were “Absolute” or “Relative”; and finally, the fourth column evaluates if summaries were evaluated against the input document (“With Document”), the reference summary (“With Reference”) or both (“With Ref. & Doc.”). in information retrieval and information extraction literature. Clarke and Lapata (2010) proposed a question-answering based approach to improve the agreement among human evaluations for the quality of summary content, which was recently employed by Narayan et al. (2018b) and Narayan et al. (2018c) (QA in Table 1). In this approach, questions were created first from the reference summary and then the system summaries were judged with regards to whether they enabled humans to answer those questions correctly. ShafieiBavani et al. (2018), on the other hand, used the “Pyramid” method (Nenkova and Passonneau, 2004) which requires summaries to be annotated by experts for salient infor"
P19-1330,P18-1014,0,0.0550599,"Missing"
P19-1330,D18-1208,0,0.130462,"Missing"
P19-1330,N18-1153,0,0.0428051,"Missing"
P19-1330,D18-1207,0,0.0443902,"Missing"
P19-1330,N18-2009,0,0.139967,"ani et al. (2018), on the other hand, used the “Pyramid” method (Nenkova and Passonneau, 2004) which requires summaries to be annotated by experts for salient information. A similar evaluation approach is the factoids analysis by Teufel and Van Halteren (2004) which evaluates the system summary against factoids, a representation based on atomic units of information, that are extracted from multiple gold summaries. However, as in the case of the “Pyramid” method, extracting factoids requires experts annotators. Finally, a small number of work evaluates the ”Correctness” (Chen and Bansal, 2018; Li et al., 2018b; Chen and Bansal, 2018) of the summary, similar to fact checking (Vlachos and Riedel, 2014), which can be a challenging task in its own right. The linguistic quality of a summary encompasses many different qualities such as fluency, grammatically, readability, formatting, naturalness and coherence. Most recent work uses a single human judgment to capture all linguistic qualities of the summary (Hsu et al., 2018; Kry´sci´nski et al., 2018; Narayan et al., 2018b; Song et al., 2018; Guo et al., 2018); we group them under “Fluency” in Table 1 with an exception of “Clarity” which 3383 was evaluat"
P19-1330,C18-1121,0,0.145098,"ani et al. (2018), on the other hand, used the “Pyramid” method (Nenkova and Passonneau, 2004) which requires summaries to be annotated by experts for salient information. A similar evaluation approach is the factoids analysis by Teufel and Van Halteren (2004) which evaluates the system summary against factoids, a representation based on atomic units of information, that are extracted from multiple gold summaries. However, as in the case of the “Pyramid” method, extracting factoids requires experts annotators. Finally, a small number of work evaluates the ”Correctness” (Chen and Bansal, 2018; Li et al., 2018b; Chen and Bansal, 2018) of the summary, similar to fact checking (Vlachos and Riedel, 2014), which can be a challenging task in its own right. The linguistic quality of a summary encompasses many different qualities such as fluency, grammatically, readability, formatting, naturalness and coherence. Most recent work uses a single human judgment to capture all linguistic qualities of the summary (Hsu et al., 2018; Kry´sci´nski et al., 2018; Narayan et al., 2018b; Song et al., 2018; Guo et al., 2018); we group them under “Fluency” in Table 1 with an exception of “Clarity” which 3383 was evaluat"
P19-1330,C18-1101,0,0.0506976,"Missing"
P19-1330,W04-1013,0,0.581662,"as input a source document consisting of multiple sentences and methods need to generate a shorter text that expresses the salient information of the source fluently and succinctly. Thus there can be multiple equally good summaries for the same source document as not all salient information can fit in a given summary length, while even extractive methods that select complete sentences are not guaranteed to produce a coherent summary overall. The most consistently used evaluation approach is comparison of the summaries produces against reference summaries via automatic measures such as ROUGE (Lin, 2004) and its variants. However, 3381 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3381–3392 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics automatic measures are unlikely to be sufficient to measure performance in summarization (Schluter, 2017), also known for other tasks in which the goal is to generate natural language (Novikova et al., 2017). Furthermore, the datasets typically considered have a single reference summary, as obtaining multiple ones increases dataset creation cost, thus evaluation again"
P19-1330,P18-2027,0,0.0675982,"Missing"
P19-1330,J13-2002,0,0.179231,"of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3381–3392 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics automatic measures are unlikely to be sufficient to measure performance in summarization (Schluter, 2017), also known for other tasks in which the goal is to generate natural language (Novikova et al., 2017). Furthermore, the datasets typically considered have a single reference summary, as obtaining multiple ones increases dataset creation cost, thus evaluation against them is likely to exhibit reference bias (Louis and Nenkova, 2013; Fomicheva and Specia, 2016), penalizing summaries containing salient content different from the reference. For the above reasons manual evaluation is considered necessary for measuring progress in summarization. However, the intrinsic difficulty of the task has led to research without manual evaluation or only fluency being assessed manually. Those that conduct manual assessment of the content, typically use a single reference summary, either directly (Celikyilmaz et al., 2018; Tan et al., 2017) or through questions (Narayan et al., 2018b,c) and thus are also likely to exhibit reference bias"
P19-1330,N19-1397,1,0.811344,"lative assessment is often done using the paired comparison (Thurstone, 1994) or the best-worst scaling (Woodworth and G, 1991; Louviere et al., 2015), to improve inter-annotator agreement. On the other hand, absolute assessment of summarization (Li et al., 2018b; Song et al., 2018; Kry´sci´nski et al., 2018; Hsu et al., 2018; Hardy and Vlachos, 2018) is often done using the Likert rating scale (Likert, 1932) where a summary is assessed on a numerical scale. Absolute assessment was also employed in combination with the question answering approach for content evaluation (Narayan et al., 2018b; Mendes et al., 2019). Both approaches, relative ranking and absolute assessment, have been investigated extensively in Machine Translation (Bojar et al., 2016, 2017). Absolute assessment correlates highly with the relative assessment without the bias introduced by having a simultaneous assessment of several models (Bojar et al., 2011). 3 Choice of Reference. The most convenient way to evaluate a system summary is to assess it against the reference summary (Celikyilmaz et al., 2018; Yang et al., 2017; Peyrard and Gurevych, 2018), as this typically requires less effort than reading the source document. The question"
P19-1330,P18-1188,1,0.0544469,"hasize differences among systems that would be ignored under other evaluation approaches.1 1 Figure 1: Highlight-based evaluation of a summary. Annotators to evaluate a summary (bottom) against the highlighted source document (top) presented with a heat map marking the salient content in the document; the darker the colour, the more annotators deemed the highlighted text salient. Introduction Research in automatic summarization has made headway over the years with single document summarization as the front-runner due to the availability of large datasets (Sandhaus, 2008; Hermann et al., 2015; Narayan et al., 2018b) which has enabled the development of novel methods, many of them employing recent advances in neural networks (See et al., 2017; Narayan et al., 2018c; Pasunuru and Bansal, 2018, inter alia). ∗ The work was primarily done while Shashi was still at School of Informatics, University of Edinburgh. 1 Our dataset and code are available at https:// github.com/sheffieldnlp/highres Measuring progress in summarization is difficult, as the task has as input a source document consisting of multiple sentences and methods need to generate a shorter text that expresses the salient information of the sour"
P19-1330,D18-1206,1,0.0465765,"hasize differences among systems that would be ignored under other evaluation approaches.1 1 Figure 1: Highlight-based evaluation of a summary. Annotators to evaluate a summary (bottom) against the highlighted source document (top) presented with a heat map marking the salient content in the document; the darker the colour, the more annotators deemed the highlighted text salient. Introduction Research in automatic summarization has made headway over the years with single document summarization as the front-runner due to the availability of large datasets (Sandhaus, 2008; Hermann et al., 2015; Narayan et al., 2018b) which has enabled the development of novel methods, many of them employing recent advances in neural networks (See et al., 2017; Narayan et al., 2018c; Pasunuru and Bansal, 2018, inter alia). ∗ The work was primarily done while Shashi was still at School of Informatics, University of Edinburgh. 1 Our dataset and code are available at https:// github.com/sheffieldnlp/highres Measuring progress in summarization is difficult, as the task has as input a source document consisting of multiple sentences and methods need to generate a shorter text that expresses the salient information of the sour"
P19-1330,N18-1158,1,0.0535864,"hasize differences among systems that would be ignored under other evaluation approaches.1 1 Figure 1: Highlight-based evaluation of a summary. Annotators to evaluate a summary (bottom) against the highlighted source document (top) presented with a heat map marking the salient content in the document; the darker the colour, the more annotators deemed the highlighted text salient. Introduction Research in automatic summarization has made headway over the years with single document summarization as the front-runner due to the availability of large datasets (Sandhaus, 2008; Hermann et al., 2015; Narayan et al., 2018b) which has enabled the development of novel methods, many of them employing recent advances in neural networks (See et al., 2017; Narayan et al., 2018c; Pasunuru and Bansal, 2018, inter alia). ∗ The work was primarily done while Shashi was still at School of Informatics, University of Edinburgh. 1 Our dataset and code are available at https:// github.com/sheffieldnlp/highres Measuring progress in summarization is difficult, as the task has as input a source document consisting of multiple sentences and methods need to generate a shorter text that expresses the salient information of the sour"
P19-1330,N04-1019,0,0.879652,"& Doc.”). in information retrieval and information extraction literature. Clarke and Lapata (2010) proposed a question-answering based approach to improve the agreement among human evaluations for the quality of summary content, which was recently employed by Narayan et al. (2018b) and Narayan et al. (2018c) (QA in Table 1). In this approach, questions were created first from the reference summary and then the system summaries were judged with regards to whether they enabled humans to answer those questions correctly. ShafieiBavani et al. (2018), on the other hand, used the “Pyramid” method (Nenkova and Passonneau, 2004) which requires summaries to be annotated by experts for salient information. A similar evaluation approach is the factoids analysis by Teufel and Van Halteren (2004) which evaluates the system summary against factoids, a representation based on atomic units of information, that are extracted from multiple gold summaries. However, as in the case of the “Pyramid” method, extracting factoids requires experts annotators. Finally, a small number of work evaluates the ”Correctness” (Chen and Bansal, 2018; Li et al., 2018b; Chen and Bansal, 2018) of the summary, similar to fact checking (Vlachos and"
P19-1330,D17-1238,0,0.0658811,"Missing"
P19-1330,N18-2102,0,0.0458397,"y (bottom) against the highlighted source document (top) presented with a heat map marking the salient content in the document; the darker the colour, the more annotators deemed the highlighted text salient. Introduction Research in automatic summarization has made headway over the years with single document summarization as the front-runner due to the availability of large datasets (Sandhaus, 2008; Hermann et al., 2015; Narayan et al., 2018b) which has enabled the development of novel methods, many of them employing recent advances in neural networks (See et al., 2017; Narayan et al., 2018c; Pasunuru and Bansal, 2018, inter alia). ∗ The work was primarily done while Shashi was still at School of Informatics, University of Edinburgh. 1 Our dataset and code are available at https:// github.com/sheffieldnlp/highres Measuring progress in summarization is difficult, as the task has as input a source document consisting of multiple sentences and methods need to generate a shorter text that expresses the salient information of the source fluently and succinctly. Thus there can be multiple equally good summaries for the same source document as not all salient information can fit in a given summary length, while e"
P19-1330,N18-2103,0,0.0928385,"good source of comparison: the input document or the reference summary? The disagreements on these issues result in authors evaluating their summaries often (11 out of 26 papers) using automatic measures such as ROUGE (Lin, 2004) despite of its limitations (Schluter, 2017). In what follows, we discuss previously proposed approaches along three axes: evaluation metrics, relative vs. absolute, and the choice of reference. Evaluation Metrics Despite differences in the exact definitions, the majority (e.g., Hsu et al., 2018; Celikyilmaz et al., 2018; Narayan et al., 2018b; Chen and Bansal, 2018; Peyrard and Gurevych, 2018) agree on both or either one of two broad quality definitions: coverage determines how much of the salient content of the source document is captured in the summary, and informativeness, how much of the content captured in the summary is salient with regards to the original document. These measures correspond to “recall” and “precision” metrics respectively in Table 1, notions that are commonly used 3382 X X X X X X X X X With Ref. & Doc. Relative X With Document Absolute X X X With Reference Precision X X Recall Clarity Fluency Correctness QA Pyramid No Manual Eval Systems See et al. (2017) L"
P19-1330,N18-1157,0,0.0605052,"Missing"
P19-1330,W14-2508,1,0.832103,"neau, 2004) which requires summaries to be annotated by experts for salient information. A similar evaluation approach is the factoids analysis by Teufel and Van Halteren (2004) which evaluates the system summary against factoids, a representation based on atomic units of information, that are extracted from multiple gold summaries. However, as in the case of the “Pyramid” method, extracting factoids requires experts annotators. Finally, a small number of work evaluates the ”Correctness” (Chen and Bansal, 2018; Li et al., 2018b; Chen and Bansal, 2018) of the summary, similar to fact checking (Vlachos and Riedel, 2014), which can be a challenging task in its own right. The linguistic quality of a summary encompasses many different qualities such as fluency, grammatically, readability, formatting, naturalness and coherence. Most recent work uses a single human judgment to capture all linguistic qualities of the summary (Hsu et al., 2018; Kry´sci´nski et al., 2018; Narayan et al., 2018b; Song et al., 2018; Guo et al., 2018); we group them under “Fluency” in Table 1 with an exception of “Clarity” which 3383 was evaluated in the DUC evaluation campaigns (Dang, 2005). The “Clarity” metric puts emphasis in easy i"
P19-1330,E17-2112,0,0.0192472,"he summary which is a different dimension than “Fluency”, as a summary may be fluent but difficult to be understood due to poor clarity. et al., 2018; Narayan et al., 2018a; Hsu et al., 2018; Kry´sci´nski et al., 2018), asking judges to assess the summary after reading the source document. However this requires more effort and is known to lead to low inter-annotator agreement (Nenkova and Passonneau, 2004). Absolute vs Relative Summary Ranking. In relative assessment of summarization, annotators are shown two or more summaries and are asked to rank them according to the dimension at question (Yang et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Guo et al., 2018; Krishna and Srinivasan, 2018). The relative assessment is often done using the paired comparison (Thurstone, 1994) or the best-worst scaling (Woodworth and G, 1991; Louviere et al., 2015), to improve inter-annotator agreement. On the other hand, absolute assessment of summarization (Li et al., 2018b; Song et al., 2018; Kry´sci´nski et al., 2018; Hsu et al., 2018; Hardy and Vlachos, 2018) is often done using the Likert rating scale (Likert, 1932) where a summary is assessed on a numerical scale. Absolute assessment was also emplo"
P19-1330,E17-2007,0,0.283044,"ile even extractive methods that select complete sentences are not guaranteed to produce a coherent summary overall. The most consistently used evaluation approach is comparison of the summaries produces against reference summaries via automatic measures such as ROUGE (Lin, 2004) and its variants. However, 3381 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3381–3392 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics automatic measures are unlikely to be sufficient to measure performance in summarization (Schluter, 2017), also known for other tasks in which the goal is to generate natural language (Novikova et al., 2017). Furthermore, the datasets typically considered have a single reference summary, as obtaining multiple ones increases dataset creation cost, thus evaluation against them is likely to exhibit reference bias (Louis and Nenkova, 2013; Fomicheva and Specia, 2016), penalizing summaries containing salient content different from the reference. For the above reasons manual evaluation is considered necessary for measuring progress in summarization. However, the intrinsic difficulty of the task has led"
P19-1330,P17-1099,0,0.779884,"summary. Annotators to evaluate a summary (bottom) against the highlighted source document (top) presented with a heat map marking the salient content in the document; the darker the colour, the more annotators deemed the highlighted text salient. Introduction Research in automatic summarization has made headway over the years with single document summarization as the front-runner due to the availability of large datasets (Sandhaus, 2008; Hermann et al., 2015; Narayan et al., 2018b) which has enabled the development of novel methods, many of them employing recent advances in neural networks (See et al., 2017; Narayan et al., 2018c; Pasunuru and Bansal, 2018, inter alia). ∗ The work was primarily done while Shashi was still at School of Informatics, University of Edinburgh. 1 Our dataset and code are available at https:// github.com/sheffieldnlp/highres Measuring progress in summarization is difficult, as the task has as input a source document consisting of multiple sentences and methods need to generate a shorter text that expresses the salient information of the source fluently and succinctly. Thus there can be multiple equally good summaries for the same source document as not all salient info"
P19-1330,C18-1077,0,0.251682,"eference Precision X X Recall Clarity Fluency Correctness QA Pyramid No Manual Eval Systems See et al. (2017) Lin et al. (2018) Cohan et al. (2018) Liao et al. (2018) Kedzie et al. (2018) Amplayo et al. (2018) Jadhav and Rajan (2018) Li et al. (2018a) Pasunuru and Bansal (2018) Cao et al. (2018) Sakaue et al. (2018) Celikyilmaz et al. (2018) Chen and Bansal (2018) Guo et al. (2018) Hardy and Vlachos (2018) Hsu et al. (2018) Krishna and Srinivasan (2018) Kry´sci´nski et al. (2018) Li et al. (2018b) Narayan et al. (2018a) Narayan et al. (2018b) Narayan et al. (2018c) Peyrard and Gurevych (2018) ShafieiBavani et al. (2018) Song et al. (2018) Yang et al. (2017) H IGH RES (ours) X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X Table 1: Overview of manual evaluations conducted in recent summarization systems. We categorize them in four dimensions: the first columns presents papers that do not report on human evaluation; the second column identifies matrices used for evaluating content (“Pyramid”, “QA”, “Correctness”, “Recall” and “Precision”) and quality (“Clarity”, “Fluency”) of summaries; the third column focuses if the system ranking r"
P19-1330,C18-1146,0,0.050614,"Missing"
P19-1330,P17-1108,0,0.0299993,"es dataset creation cost, thus evaluation against them is likely to exhibit reference bias (Louis and Nenkova, 2013; Fomicheva and Specia, 2016), penalizing summaries containing salient content different from the reference. For the above reasons manual evaluation is considered necessary for measuring progress in summarization. However, the intrinsic difficulty of the task has led to research without manual evaluation or only fluency being assessed manually. Those that conduct manual assessment of the content, typically use a single reference summary, either directly (Celikyilmaz et al., 2018; Tan et al., 2017) or through questions (Narayan et al., 2018b,c) and thus are also likely to exhibit reference bias. In this paper we propose a novel approach for manual evaluation, H IGHlight-based Referenceless Evaluation of document Summarization (H IGH RES), in which a summary is assessed against the source document via manually highlighted salient content in the latter (see Figure 1 for an example). Our approach avoids reference bias, as the multiple highlights obtained help consider more content than what is contained in a single reference. The highlights are not dependent on the summaries being evaluate"
P19-1330,W04-3254,0,0.548402,"Missing"
P19-1330,W16-2301,0,\N,Missing
P19-1330,W17-4717,0,\N,Missing
Q16-1030,N09-1003,0,0.20501,"Missing"
Q16-1030,P98-1013,0,0.651476,"; Mikolov et al., 2013c) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2015). While these general purpose word embeddings have achieved significant improvement in various tasks in NLP, it has been discovered that further tuning of these continuous word representations for specific tasks improves their performance by a larger margin. For example, in dependency parsing, word embeddings could be tailored to capture similarity in terms of context within syntactic parses (Bansal et al., 2014) or they could be refined using semantic lexicons such as WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013) to improve various similarity tasks (Yu and Dredze, 2014; Faruqui et al., 2015; Rothe and Sch¨utze, 2015). This paper proposes a method to encode prior semantic knowledge in spectral word embeddings (Dhillon et al., 2015). Spectral learning algorithms are of great interest for their speed, scalability, theoretical guarantees and performance in various NLP applications. These algorithms are no strangers to word embeddings either. In latent semantic analysis (LSA, (Deerwester et al., 1990; Landauer et al., 1998)), word embeddings are learn"
Q16-1030,D14-1034,0,0.0417964,"ors of words which are adjacent in the prior knowledge graph to be close to each other in the new 424 5.3 Evaluation Benchmarks We evaluated the quality of our eigenword embeddings on three different tasks: word similarity, geographic analogies and NP bracketing. 7 https://github.com/mfaruqui/ retrofitting. sampled from words that occur at least 700 times in a large web corpus. The datasets, MTurk-287 (Radinsky et al., 2011) and MTurk-771 (Halawi et al., 2012), were scored by Amazon Mechanical Turk workers for relatedness of English word pairs. The YP-130 (Yang and Powers, 2005) and Verb-143 (Baker et al., 2014) datasets were developed for verb similarity predictions. The last two datasets, MC-30 (Miller and Charles, 1991) and RG-65 (Rubenstein and Goodenough, 1965) consist of 30 and 65 noun pairs respectively. For each dataset, we calculate the cosine similarity between the vectors of word pairs and measure Spearman’s rank correlation coefficient between the scores produced by the embeddings and human ratings. We report the average of the correlations on all 11 datasets. Each word similarity task in the above list represents a different aspect of word similarity, and as such, averaging the results p"
Q16-1030,P14-2131,0,0.0661786,"word embeddings and evaluating them on a myriad of datasets. 1 Introduction In recent years there has been an immense interest in representing words as low-dimensional continuous real-vectors, namely word embeddings. Word embeddings aim to capture lexico-semantic information such that regularities in the vocabulary are topologically represented in a Euclidean space. Such word embeddings have achieved state-of-theart performance on many natural language processing (NLP) tasks, e.g., syntactic parsing (Socher et al., 2013), word or phrase similarity (Mikolov et al., 2013b), dependency parsing (Bansal et al., 2014), unsupervised learning (Parikh et al., 2014) and others. Since the discovery that word embeddings are useful as features for various NLP tasks, research on word embeddings has taken on a life of its own, with a vibrant community searching for better word representations in a variety of problems and datasets. These word embeddings are often induced from large raw text capturing distributional co-occurrence information via neural networks (Bengio et al., 2003; Mikolov et al., 2013b; Mikolov et al., 2013c) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2015). While these general p"
Q16-1030,D13-1167,0,0.0146295,"atrix can be constructed to encode prior knowledge directly to improve the quality of word vectors. Wang et al. (2015) investigate the notion of relatedness in embedding models by incorporating syntactic and lexicographic knowledge. In spectral learning, Yih et al. (2012) augment the word co-occurrence matrix on which LSA operates with relational information such that synonyms will tend to have positive cosine similarity, and antonyms will tend to have negative similarities. Their vector space representation successfully projects synonyms and antonyms on opposite sides in the projected space. Chang et al. (2013) further generalize this approach to encode multiple relations (and not just opposing relations, such as synonyms and antonyms) using multi-relational LSA. In spectral learning, most of the studies on incorporating prior knowledge in word vectors focus on LSA based word embeddings (Yih et al., 2012; Chang et al., 2013; Turney and Littman, 2005; Turney, 2006; Turney and Pantel, 2010). From the technical perspective, our work is also related to that of Jagarlamudi et al. (2011), who showed how to generalize CCA so that it uses locality preserving projections (He and Niyogi, 2004). They also assu"
Q16-1030,W15-1523,0,0.0458809,"Missing"
Q16-1030,E14-1049,0,0.0513205,"4 81.0 81.3 81.8 81.4 80.3 81.2 81.3 80.7 81.4 81.7 81.7 81.1 81.9 81.4 81.0 80.8 81.0 80.7 C F I FN 78.7 80.5 80.2 82.0 80.7 81.0 80.7 80.9 80.8 81.0 81.2 80.6 80.0 80.4 80.4 Table 1: Results for the word similarity datasets, geographic analogies and NP bracketing. The first upper blocks (A–C) present the results with retrofitting. NPK stands for no prior knowledge (no retrofitting is used), WN for WordNet, PD for PPDB and FN for FrameNet. Glove, Skip-Gram, Global Context, Multilingual and Eigen are the word embeddings of Pennington et al. (2014), Mikolov et al. (2013b), Huang et al. (2012), Faruqui and Dyer (2014) and Dhillon et al. (2015) respectively. The second middle blocks (D–F) show the results of our eigenword embeddings encoded with prior knowledge using our method. Each row in the block corresponds to a specific use of an α value (smoothing factor), as described in Figure 3. In the lower blocks (G–I) we take the word embeddings from the second block, and retrofit them using the method of Faruqui et al. (2015). Best results in each block are in bold. art word embeddings, such as Glove (Pennington et al., 2014), Skip-Gram (Mikolov et al., 2013b), Global Context (Huang et al., 2012) and Multiling"
Q16-1030,P15-2076,0,0.0259764,"s to . This dataset consists of 506 word pairs. For given word pairs, a:b c:d where d is unknown, we use the vector offset method (Mikolov et al., 2013b), i.e., we compute a vector v = vb − va + vc where va , vb and vc are vector representations of the words a, b and c respectively; we then return the word d with the greatest cosine similarity to v. NP Bracketing Here the goal is to identify the correct bracketing of a three-word noun (Lazaridou et al., 2013). For example, the bracketing of annual (price growth) is “right,” while the bracketing of (entry level) machine is “left.” Similarly to Faruqui and Dyer (2015), we concatenate the word vectors of the three words, and use this vector for binary classification into left or right. Since most of the datasets that we evaluate on in this paper are not standardly separated into development and test sets, we report all results we calculated (with respect to hyperparameter differences) and do 425 not select just a subset of the results. 5.4 Evaluation Preliminary Experiments In our first set of experiments, we vary the dimension of the word embedding vectors. We try m ∈ {50, 100, 200, 300}. Our experiments showed that the results consistently improve when th"
Q16-1030,N15-1184,0,0.120857,"ngs have achieved significant improvement in various tasks in NLP, it has been discovered that further tuning of these continuous word representations for specific tasks improves their performance by a larger margin. For example, in dependency parsing, word embeddings could be tailored to capture similarity in terms of context within syntactic parses (Bansal et al., 2014) or they could be refined using semantic lexicons such as WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013) to improve various similarity tasks (Yu and Dredze, 2014; Faruqui et al., 2015; Rothe and Sch¨utze, 2015). This paper proposes a method to encode prior semantic knowledge in spectral word embeddings (Dhillon et al., 2015). Spectral learning algorithms are of great interest for their speed, scalability, theoretical guarantees and performance in various NLP applications. These algorithms are no strangers to word embeddings either. In latent semantic analysis (LSA, (Deerwester et al., 1990; Landauer et al., 1998)), word embeddings are learned by performing SVD on the word by document matrix. Recently, Dhillon et al. (2015) have proposed to use canonical correlation analysi"
Q16-1030,N13-1092,0,0.254469,"Missing"
Q16-1030,J15-4004,0,0.139688,"introduced in the off-theshelf embeddings as a post-processing step (Faruqui et al., 2015; Rothe and Sch¨utze, 2015). In this paper, we focus on the retrofitting approach of Faruqui et al. (2015). Word Similarity For the word similarity task we experimented with 11 different widely used benchmarks. The WS-353-ALL dataset (Finkelstein et al., 2002) consists of 353 pairs of English words with their human similarity ratings. Later, Agirre et al. (2009) re-annotated WS-353-ALL for similarity (WS-353-SIM) and relatedness (WS-353-REL) with specific distinctions between them. The SimLex999 dataset (Hill et al., 2015) was built to measure how well models capture similarity, rather than relatedness or association. The MEN-TR-3000 dataset (Bruni et al., 2014) consists of 3000 word pairs Retrofitting works by optimizing an objective function which has two terms: one that tries to keep the distance between the word vectors close to the original distances, and the other which enforces the vectors of words which are adjacent in the prior knowledge graph to be close to each other in the new 424 5.3 Evaluation Benchmarks We evaluated the quality of our eigenword embeddings on three different tasks: word similarity"
Q16-1030,P12-1092,0,0.0566106,".7 81.7 81.2 81.0 82.4 81.0 81.3 81.8 81.4 80.3 81.2 81.3 80.7 81.4 81.7 81.7 81.1 81.9 81.4 81.0 80.8 81.0 80.7 C F I FN 78.7 80.5 80.2 82.0 80.7 81.0 80.7 80.9 80.8 81.0 81.2 80.6 80.0 80.4 80.4 Table 1: Results for the word similarity datasets, geographic analogies and NP bracketing. The first upper blocks (A–C) present the results with retrofitting. NPK stands for no prior knowledge (no retrofitting is used), WN for WordNet, PD for PPDB and FN for FrameNet. Glove, Skip-Gram, Global Context, Multilingual and Eigen are the word embeddings of Pennington et al. (2014), Mikolov et al. (2013b), Huang et al. (2012), Faruqui and Dyer (2014) and Dhillon et al. (2015) respectively. The second middle blocks (D–F) show the results of our eigenword embeddings encoded with prior knowledge using our method. Each row in the block corresponds to a specific use of an α value (smoothing factor), as described in Figure 3. In the lower blocks (G–I) we take the word embeddings from the second block, and retrofit them using the method of Faruqui et al. (2015). Best results in each block are in bold. art word embeddings, such as Glove (Pennington et al., 2014), Skip-Gram (Mikolov et al., 2013b), Global Context (Huang et"
Q16-1030,D12-1002,0,0.067271,"Missing"
Q16-1030,D13-1196,0,0.0198438,"o ” where d is unknown. We report results on a subset of this dataset which focuses on finding capitals of common countries, e.g., Greece is to Athens as Iraq is to . This dataset consists of 506 word pairs. For given word pairs, a:b c:d where d is unknown, we use the vector offset method (Mikolov et al., 2013b), i.e., we compute a vector v = vb − va + vc where va , vb and vc are vector representations of the words a, b and c respectively; we then return the word d with the greatest cosine similarity to v. NP Bracketing Here the goal is to identify the correct bracketing of a three-word noun (Lazaridou et al., 2013). For example, the bracketing of annual (price growth) is “right,” while the bracketing of (entry level) machine is “left.” Similarly to Faruqui and Dyer (2015), we concatenate the word vectors of the three words, and use this vector for binary classification into left or right. Since most of the datasets that we evaluate on in this paper are not standardly separated into development and test sets, we report all results we calculated (with respect to hyperparameter differences) and do 425 not select just a subset of the results. 5.4 Evaluation Preliminary Experiments In our first set of experi"
Q16-1030,N13-1090,0,0.38127,"ustification for it, and test it by deriving word embeddings and evaluating them on a myriad of datasets. 1 Introduction In recent years there has been an immense interest in representing words as low-dimensional continuous real-vectors, namely word embeddings. Word embeddings aim to capture lexico-semantic information such that regularities in the vocabulary are topologically represented in a Euclidean space. Such word embeddings have achieved state-of-theart performance on many natural language processing (NLP) tasks, e.g., syntactic parsing (Socher et al., 2013), word or phrase similarity (Mikolov et al., 2013b), dependency parsing (Bansal et al., 2014), unsupervised learning (Parikh et al., 2014) and others. Since the discovery that word embeddings are useful as features for various NLP tasks, research on word embeddings has taken on a life of its own, with a vibrant community searching for better word representations in a variety of problems and datasets. These word embeddings are often induced from large raw text capturing distributional co-occurrence information via neural networks (Bengio et al., 2003; Mikolov et al., 2013b; Mikolov et al., 2013c) or spectral methods (Deerwester et al., 1990;"
Q16-1030,P16-1146,1,0.776071,"h et al., 2012; Chang et al., 2013; Turney and Littman, 2005; Turney, 2006; Turney and Pantel, 2010). From the technical perspective, our work is also related to that of Jagarlamudi et al. (2011), who showed how to generalize CCA so that it uses locality preserving projections (He and Niyogi, 2004). They also assume the existence of a weight matrix in a multi-view setting that describes the distances between pairs of points in the two views. More generally, CCA is an important component for spectral learning algorithms in the unsupervised setting and with latent variables (Cohen et al., 2014; Narayan and Cohen, 2016; Stratos et al., 2016). Our method for incorporating prior knowledge into CCA could potentially be transferred to these algorithms. 7 Conclusion We described a method for incorporating prior knowledge into CCA. Our method requires a relatively simple change to the original canonical correlation analysis, where extra counts are added to the matrix on which singular value decomposition is performed. We used our method to derive word embeddings in the style of eigenwords, and tested them on a set of datasets. Our results demonstrate several advantages of encoding prior knowledge into eigenword e"
Q16-1030,P14-1100,1,0.803585,"iad of datasets. 1 Introduction In recent years there has been an immense interest in representing words as low-dimensional continuous real-vectors, namely word embeddings. Word embeddings aim to capture lexico-semantic information such that regularities in the vocabulary are topologically represented in a Euclidean space. Such word embeddings have achieved state-of-theart performance on many natural language processing (NLP) tasks, e.g., syntactic parsing (Socher et al., 2013), word or phrase similarity (Mikolov et al., 2013b), dependency parsing (Bansal et al., 2014), unsupervised learning (Parikh et al., 2014) and others. Since the discovery that word embeddings are useful as features for various NLP tasks, research on word embeddings has taken on a life of its own, with a vibrant community searching for better word representations in a variety of problems and datasets. These word embeddings are often induced from large raw text capturing distributional co-occurrence information via neural networks (Bengio et al., 2003; Mikolov et al., 2013b; Mikolov et al., 2013c) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2015). While these general purpose word embeddings have achieved signific"
Q16-1030,D14-1162,0,0.0836635,"keting WN PD 79.5 79.4 80.4 81.5 79.1 80.5 81.8 82.7 81.7 81.2 81.0 82.4 81.0 81.3 81.8 81.4 80.3 81.2 81.3 80.7 81.4 81.7 81.7 81.1 81.9 81.4 81.0 80.8 81.0 80.7 C F I FN 78.7 80.5 80.2 82.0 80.7 81.0 80.7 80.9 80.8 81.0 81.2 80.6 80.0 80.4 80.4 Table 1: Results for the word similarity datasets, geographic analogies and NP bracketing. The first upper blocks (A–C) present the results with retrofitting. NPK stands for no prior knowledge (no retrofitting is used), WN for WordNet, PD for PPDB and FN for FrameNet. Glove, Skip-Gram, Global Context, Multilingual and Eigen are the word embeddings of Pennington et al. (2014), Mikolov et al. (2013b), Huang et al. (2012), Faruqui and Dyer (2014) and Dhillon et al. (2015) respectively. The second middle blocks (D–F) show the results of our eigenword embeddings encoded with prior knowledge using our method. Each row in the block corresponds to a specific use of an α value (smoothing factor), as described in Figure 3. In the lower blocks (G–I) we take the word embeddings from the second block, and retrofit them using the method of Faruqui et al. (2015). Best results in each block are in bold. art word embeddings, such as Glove (Pennington et al., 2014), Skip-Gram (Mik"
Q16-1030,P15-1173,0,0.124156,"Missing"
Q16-1030,P13-1056,0,0.0282838,"r other problems. It follows a similar idea to the one proposed by Koren and Carmel (2003) for improving the visualization of principal vectors with principal component analysis (PCA). Our derivation represents the solution to CCA as that of an optimization problem which maximizes the distance between the two view projections of training examples, while weighting these distances using the external source of prior knowledge. As such, our approach applies to other uses of CCA in the NLP literature, such as the one of Jagarlamudi and Daum´e (2012), who used CCA for transliteration, or the one of Silberer et al. (2013), who used CCA for semantically representing visual attributes. 2 Background and Notation For an integer n, we denote by [n] the set of integers {1, . . . , n}. We assume the existence of a vocabulary of words, usually taken from a corpus. This set of words is denoted by H = {h1 , . . . , h|H |}. For a square matrix A, we denote by diag(A) a diagonal matrix B which has the same dimensions as A such that Bii = Aii for all i. For vector vq∈ Rd , we dePd 2 note its `2 norm by ||v||, i.e. ||v ||= i=1 vi . We also denote by vj or [v]j the jth coordinate of v. For a pair of vectors u and v, we denot"
Q16-1030,P13-1045,0,0.019109,"te prior knowledge into CCA, give a theoretical justification for it, and test it by deriving word embeddings and evaluating them on a myriad of datasets. 1 Introduction In recent years there has been an immense interest in representing words as low-dimensional continuous real-vectors, namely word embeddings. Word embeddings aim to capture lexico-semantic information such that regularities in the vocabulary are topologically represented in a Euclidean space. Such word embeddings have achieved state-of-theart performance on many natural language processing (NLP) tasks, e.g., syntactic parsing (Socher et al., 2013), word or phrase similarity (Mikolov et al., 2013b), dependency parsing (Bansal et al., 2014), unsupervised learning (Parikh et al., 2014) and others. Since the discovery that word embeddings are useful as features for various NLP tasks, research on word embeddings has taken on a life of its own, with a vibrant community searching for better word representations in a variety of problems and datasets. These word embeddings are often induced from large raw text capturing distributional co-occurrence information via neural networks (Bengio et al., 2003; Mikolov et al., 2013b; Mikolov et al., 2013"
Q16-1030,P15-1124,0,0.0251709,"our word embeddings with existing state-of-the2 We downloaded the data from https://dumps. wikimedia.org/, and preprocessed it using the tool available at http://mattmahoney.net/dc/textdata. html. 3 We use the XL subset of the PPDB. 4 https://github.com/paramveerdhillon/ swell. 5 Our implementation and the word embeddings that we calculated are available at http://cohort.inf.ed.ac. uk/cohort/eigen/. 6 We also use the square-root transformation as mentioned in Dhillon et al. (2015) which controls the variance in the counts accumulated from the corpus. See a justification for this transform in Stratos et al. (2015). Retrofitting CCAPrior CCAPrior+RF Glove Skip-Gram Global Context Multilingual Eigen (CCA) α = 0.1 α = 0.2 α = 0.5 α = 0.7 α = 0.9 α = 0.1 α = 0.2 α = 0.5 α = 0.7 α = 0.9 Word similarity average NPK WN PD FN 59.7 63.1 64.6 57.5 64.1 65.5 68.6 62.3 44.4 50.0 50.4 47.3 62.3 66.9 68.2 62.8 59.5 62.2 63.6 61.4 59.1 59.6 59.5 59.9 60.6 60.0 59.9 59.7 59.6 60.7 59.3 59.5 60.6 59.6 58.9 61.9 63.6 61.5 62.6 64.9 61.6 62.7 63.7 61.4 63.3 63.0 61.0 62.0 63.3 60.4 A D G Geographic analogies NPK WN PD FN 94.8 75.3 80.4 94.8 87.3 72.3 70.5 87.7 7.3 4.5 18.2 7.3 70.7 46.2 53.7 72.7 89.9 79.2 73.5 89.9 88.9"
Q16-1030,Q16-1018,0,0.0277759,"al., 2013; Turney and Littman, 2005; Turney, 2006; Turney and Pantel, 2010). From the technical perspective, our work is also related to that of Jagarlamudi et al. (2011), who showed how to generalize CCA so that it uses locality preserving projections (He and Niyogi, 2004). They also assume the existence of a weight matrix in a multi-view setting that describes the distances between pairs of points in the two views. More generally, CCA is an important component for spectral learning algorithms in the unsupervised setting and with latent variables (Cohen et al., 2014; Narayan and Cohen, 2016; Stratos et al., 2016). Our method for incorporating prior knowledge into CCA could potentially be transferred to these algorithms. 7 Conclusion We described a method for incorporating prior knowledge into CCA. Our method requires a relatively simple change to the original canonical correlation analysis, where extra counts are added to the matrix on which singular value decomposition is performed. We used our method to derive word embeddings in the style of eigenwords, and tested them on a set of datasets. Our results demonstrate several advantages of encoding prior knowledge into eigenword embeddings. m X d X i=1"
Q16-1030,J06-3003,0,0.03946,"that synonyms will tend to have positive cosine similarity, and antonyms will tend to have negative similarities. Their vector space representation successfully projects synonyms and antonyms on opposite sides in the projected space. Chang et al. (2013) further generalize this approach to encode multiple relations (and not just opposing relations, such as synonyms and antonyms) using multi-relational LSA. In spectral learning, most of the studies on incorporating prior knowledge in word vectors focus on LSA based word embeddings (Yih et al., 2012; Chang et al., 2013; Turney and Littman, 2005; Turney, 2006; Turney and Pantel, 2010). From the technical perspective, our work is also related to that of Jagarlamudi et al. (2011), who showed how to generalize CCA so that it uses locality preserving projections (He and Niyogi, 2004). They also assume the existence of a weight matrix in a multi-view setting that describes the distances between pairs of points in the two views. More generally, CCA is an important component for spectral learning algorithms in the unsupervised setting and with latent variables (Cohen et al., 2014; Narayan and Cohen, 2016; Stratos et al., 2016). Our method for incorporati"
Q16-1030,D14-1167,0,0.0298382,"tion. When using the retrofitting method by Faruqui et al. on top of these word embeddings, the results barely improved. 6 Related Work Our ideas in this paper for encoding prior knowledge in eigenword embeddings relate to three main threads in existing literature. One of the threads focuses on modifying the objective of word vector training algorithms. Yu and Dredze (2014), Xu et al. (2014), Fried and Duh (2015) and Bian et al. (2014) augment the training objective in neural language models of Mikolov et al. (2013a) to encourage semantically related word vectors to come closer to each other. Wang et al. (2014) propose a method for jointly embedding entities (from FreeBase, a large community-curated knowledge base) and words (from Wikipedia) into the same continuous vector space. Chen and de Melo (2015) propose a similar joint model to improve the word embeddings, but rather than using structured knowledge sources their model focuses on discovering stronger semantic connections in specific contexts in a text corpus. Another research thread relies on post-processing steps to encode prior knowledge from semantic lexicons in off-the-shelf word embeddings. The main intuition behind this trend is to upda"
Q16-1030,P15-2075,0,0.0117246,", SkipGram, Global Context, and Multilingual, on various word similarity tasks. Rothe and Sch¨utze (2015) also describe how standard word vectors can be extended to various data types in semantic lexicons, e.g., synsets and lexemes in WordNet. Most of the standard word vector training algorithms use co-occurrence within window-based contexts to measure relatedness among words. Several studies question the limitations of defining relatedness in this way and investigate if the word co-occurrence matrix can be constructed to encode prior knowledge directly to improve the quality of word vectors. Wang et al. (2015) investigate the notion of relatedness in embedding models by incorporating syntactic and lexicographic knowledge. In spectral learning, Yih et al. (2012) augment the word co-occurrence matrix on which LSA operates with relational information such that synonyms will tend to have positive cosine similarity, and antonyms will tend to have negative similarities. Their vector space representation successfully projects synonyms and antonyms on opposite sides in the projected space. Chang et al. (2013) further generalize this approach to encode multiple relations (and not just opposing relations, su"
Q16-1030,P95-1026,0,0.0695115,"rence statistics (Deerwester et al., 1990; Dhillon et al., 2015) to neural networks jointly learning a language model (Bengio et al., 2003; Mikolov et al., 2013a) or models for other NLP tasks (Collobert and Weston, 2008). 3 Canonical Correlation Analysis for Deriving Word Embeddings One recent approach to derive word embeddings, developed by Dhillon et al. (2015), is through the use of canonical correlation analysis, resulting in socalled “eigenwords.” CCA is a technique for multiview dimensionality reduction. It assumes the existence of two views for a set of data, similarly to co-training (Yarowsky, 1995; Blum and Mitchell, 1998), and then projects the data in the two views in a way that maximizes the correlation between the projected views. Dhillon et al. (2015) used CCA to derive word embeddings through the following procedure. They first break each document in a corpus of documents into n sequences of words of a fixed length 2k + 1, where k is a window size. For example, if k = 2, the short document “Harry Potter has been a bestseller” would be broken into “Harry Potter has been a” and “Potter has been a best-seller.” In each such sequence, the middle word is identified as a pivot. This le"
Q16-1030,D12-1111,0,0.0611095,"tended to various data types in semantic lexicons, e.g., synsets and lexemes in WordNet. Most of the standard word vector training algorithms use co-occurrence within window-based contexts to measure relatedness among words. Several studies question the limitations of defining relatedness in this way and investigate if the word co-occurrence matrix can be constructed to encode prior knowledge directly to improve the quality of word vectors. Wang et al. (2015) investigate the notion of relatedness in embedding models by incorporating syntactic and lexicographic knowledge. In spectral learning, Yih et al. (2012) augment the word co-occurrence matrix on which LSA operates with relational information such that synonyms will tend to have positive cosine similarity, and antonyms will tend to have negative similarities. Their vector space representation successfully projects synonyms and antonyms on opposite sides in the projected space. Chang et al. (2013) further generalize this approach to encode multiple relations (and not just opposing relations, such as synonyms and antonyms) using multi-relational LSA. In spectral learning, most of the studies on incorporating prior knowledge in word vectors focus"
Q16-1030,P14-2089,0,0.307726,"purpose word embeddings have achieved significant improvement in various tasks in NLP, it has been discovered that further tuning of these continuous word representations for specific tasks improves their performance by a larger margin. For example, in dependency parsing, word embeddings could be tailored to capture similarity in terms of context within syntactic parses (Bansal et al., 2014) or they could be refined using semantic lexicons such as WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013) to improve various similarity tasks (Yu and Dredze, 2014; Faruqui et al., 2015; Rothe and Sch¨utze, 2015). This paper proposes a method to encode prior semantic knowledge in spectral word embeddings (Dhillon et al., 2015). Spectral learning algorithms are of great interest for their speed, scalability, theoretical guarantees and performance in various NLP applications. These algorithms are no strangers to word embeddings either. In latent semantic analysis (LSA, (Deerwester et al., 1990; Landauer et al., 1998)), word embeddings are learned by performing SVD on the word by document matrix. Recently, Dhillon et al. (2015) have proposed to use canonic"
Q16-1030,C98-1013,0,\N,Missing
W13-2105,W04-3315,0,0.0851368,"Missing"
W13-2105,W11-2832,0,0.157824,"Missing"
W13-2105,P06-1130,0,0.060315,"Missing"
W13-2105,I05-1015,0,0.0675771,"Missing"
W13-2105,cmejrek-etal-2004-prague,0,0.030845,"Missing"
W13-2105,W09-0624,0,0.0167668,"tion, right node raising, gapping and stripping and uses dependency trees connected by rhetorical relations as input. Before these trees are mapped to sentences, repeated elements are deleted and their antecedent (thesource element) is related by a SUBORROWED relation to their governor in the elliptic clause and a SUIDENTICAL relation to their governor in the antecedent clause. This is then interpreted by the surface realiser to mean that the repeated element should be realised in the source clause, elided in the target clause and that it licenses the same syntactic structure in both clauses. Harbusch and Kempen (2009) have proposed a module called Elleipo which takes as input unreduced, non-elliptic, syntactic structures annotated with lexical identity and coreference relationships Analysing the remaining failure cases. To better assess the extent to which rewriting and the FBLTAG generation system succeed in generating elliptic coordinations, we performed error mining on the elliptic data using our error miner described in (Narayan and Gardent, 2012a). This method permits highlighting the most likely sources of error given two datasets: a set of successful cases and a set of failure cases. In this case, t"
W13-2105,W07-2416,0,0.0212058,"using functions. For instance, if the remnant is parallel to the source subject, it will be labelled GAP - SBJ (cf. Figure 3). commission SBJ COORD OBJ 4 Rewriting the SR Data He The SR Task 2011 made available two types of data for surface realisers to be tested on: shallow dependency trees and deep dependency graphs. Here we focus on the shallow dependency trees i.e., on syntactic structures. The input data provided by the SR Task were obtained from the Penn Treebank. They were derived indirectly from the LTH Constituent-toDependency Conversion Tool for Penn-style Treebanks (Pennconverter, (Johansson and Nugues, 2007)) by post-processing the CoNLL data to reand fearsome contemporary score CONJ splendidly interpret Figure 4: Subject Sharing and RNR in the SR data. “[He]j ǫj commissions ǫi and ǫj splendidly interprets ǫi [fearsome contemporary scores]i .” For right-node raising and shared subjects, the coindexation present in the PTB is dropped in the SR data. As a result, the SR representation under42 specifies the relation between the object and the coordinated verbs in RNR constructions: the object could be shared as in He commissions ǫi and splendidly interprets ǫi [fearsome contemporary scores]i . (Figu"
W13-2105,daum-etal-2004-automatic,0,0.0162925,"sed here because they can be handled by the generator by having the appropriate categories in the grammar and the lexicon e.g., in a Tree Adjoining Grammar, an auxiliary anchoring a verb phrase for VP ellipsis and question words anchoring a sentence for sluicing. Figure 2: Penn Treebank annotation for gapping “Mary [likes]i potatoes and Bill ǫi ostriches.” In dependency treebanks, headless elliptic constructs such as gapping additionally raise the is41 sue of how to represent the daughters of an empty head. Three main types of approaches have been proposed. In dependency treebanks for German (Daum et al., 2004; Hajiˇc et al., 2009) and in ˇ the Czech treebank (Cmejrek et al., 2004; Hajiˇc et al., 2009), one of the dependents of the headless phrase is declared to be the head. This is a rather undesirable solution because it hides the fact that there the clause lacks a head. In contrast, the Hungarian dependency treebank (Vincze et al., 2010) explicitly represents the elided elements in the trees by introducing phonetically empty elements that serve as attachment points to other tokens. This is the cleanest solution from a linguistic point of view. Similarly, Seeker and Kuhn (2012) present a conversi"
W13-2105,W11-2912,0,0.0407452,"Missing"
W13-2105,W02-2103,0,0.13379,"Missing"
W13-2105,W12-3624,0,0.0376725,"Missing"
W13-2105,W08-1105,0,0.0731873,"Missing"
W13-2105,A94-1002,0,0.239077,"elated work Previous work on generating elliptic sentences has mostly focused on identifying material that could be elided and on defining procedures capable of producing input structures for surface realisation that support the generation of elliptic sentences. Shaw (1998) developed a sentence planner which generates elliptic sentences in 3 steps. First, input data are grouped according to their similarities. Second, repeated elements are marked. Third, constraints are used to determine which occurrences of a marked element should be deleted. The approach is integrated in the PLANDoc system (McKeown et al., 1994) and shown to generate a wide range of elliptic constructs including RNR, VPE and NCC using FUF/SURGE (Elhadad, 1993), a realisation component based on Functional Unification Grammar. Theune et al. (2006) describe how elliptic sentences are generated in a story generation system. The approach covers conjunction reduction, right node raising, gapping and stripping and uses dependency trees connected by rhetorical relations as input. Before these trees are mapped to sentences, repeated elements are deleted and their antecedent (thesource element) is related by a SUBORROWED relation to their gove"
W13-2105,C12-1123,1,0.543286,"to map sounds onto their corresponding meanings, break down. For instance, in the sentence John eats apples and Mary pear, the Subject-Verb-Object structure which can be used in English to express a binary relation is present in the source clause but not in the elided one. In practice, the syntax of elliptical sentences often leads to a duplication of the grammatical system, one system allowing for non elliptical sentences and the other for their elided counterpart. 5 Generating Elliptic Coordination 5.1 The Surface Realiser To generate sentences from the SR data, we use our surface realiser (Narayan and Gardent, 2012b), a grammar-based generator based on a Feature-Based Lexicalised Tree Adjoining Grammar (FB-LTAG) for English. This generator first selects the elementary FB-LTAG trees associated in the lexicon with the lemmas and part of speech tags associated with each node in the input dependency tree. It then attempts to combine the selected trees bottom-up taking into account the structure of the input tree (only trees that are selected by nodes belonging to the same local input tree are tried for combination). A language model is used to implement a beam search letting through only the n most likely p"
W13-2105,W09-1201,0,0.0499899,"Missing"
W13-2105,C12-1124,1,0.929044,"to map sounds onto their corresponding meanings, break down. For instance, in the sentence John eats apples and Mary pear, the Subject-Verb-Object structure which can be used in English to express a binary relation is present in the source clause but not in the elided one. In practice, the syntax of elliptical sentences often leads to a duplication of the grammatical system, one system allowing for non elliptical sentences and the other for their elided counterpart. 5 Generating Elliptic Coordination 5.1 The Surface Realiser To generate sentences from the SR data, we use our surface realiser (Narayan and Gardent, 2012b), a grammar-based generator based on a Feature-Based Lexicalised Tree Adjoining Grammar (FB-LTAG) for English. This generator first selects the elementary FB-LTAG trees associated in the lexicon with the lemmas and part of speech tags associated with each node in the input dependency tree. It then attempts to combine the selected trees bottom-up taking into account the structure of the input tree (only trees that are selected by nodes belonging to the same local input tree are tried for combination). A language model is used to implement a beam search letting through only the n most likely p"
W13-2105,C96-2103,0,0.282891,"Missing"
W13-2105,W08-2311,0,0.0156757,"he structure of the input tree (only trees that are selected by nodes belonging to the same local input tree are tried for combination). A language model is used to implement a beam search letting through only the n most likely phrases at each bottom up combination step. In this experiment, we set n to 5. The generator thus outputs at most 5 sentences for each input. For parsing with TAG, two main methods have been proposed for processing elliptical sentences. (Sarkar and Joshi, 1996) introduces an additional operation for combining TAG trees which yields derivation graphs rather than trees. (Seddah, 2008) uses Multi-Component TAG and proposes to associate each elementary verb tree with an elliptic tree with different pairs representing different types of ellipses. We could use either of these approaches for generation. The first approach however has the drawback that it leads to a non standard notion of derivation (the derivation trees become derivation graphs). The second on the other hand, induces a proliferation of trees in the grammar and impacts efficiency. Instead, we show that, given an input enriched with empty categories as proposed in the previous section, neither the grammar nor the"
W13-2105,seeker-kuhn-2012-making,0,0.0154984,"y treebanks for German (Daum et al., 2004; Hajiˇc et al., 2009) and in ˇ the Czech treebank (Cmejrek et al., 2004; Hajiˇc et al., 2009), one of the dependents of the headless phrase is declared to be the head. This is a rather undesirable solution because it hides the fact that there the clause lacks a head. In contrast, the Hungarian dependency treebank (Vincze et al., 2010) explicitly represents the elided elements in the trees by introducing phonetically empty elements that serve as attachment points to other tokens. This is the cleanest solution from a linguistic point of view. Similarly, Seeker and Kuhn (2012) present a conversion of the German Tiger treebank which introduces empty nodes for verb ellipses if a phrase normally headed by a verb is lacking a head. They compare the performance of two statistical dependency parsers on the canonical version and the CoNLL 2009 Shared Task data and show that the converted dependency treebank they propose yields better parsing results than the treebank not containing empty heads. In sum, while some linguists have argued for an approach where ellipsis has no syntactic representation, many have provided strong empirical evidence for positing empty nodes as pl"
W13-2105,P98-2199,0,0.0462157,"overall increase of 0.09 points on the set of elliptic sentences. Moreover, the consistent improvement in terms of BLEU score for generated sentences (COV column) shows that rewriting simultaneously improves both coverage and precision that is, that for those sentences that are generated, rewriting consistently improves precision. 6 Related work Previous work on generating elliptic sentences has mostly focused on identifying material that could be elided and on defining procedures capable of producing input structures for surface realisation that support the generation of elliptic sentences. Shaw (1998) developed a sentence planner which generates elliptic sentences in 3 steps. First, input data are grouped according to their similarities. Second, repeated elements are marked. Third, constraints are used to determine which occurrences of a marked element should be deleted. The approach is integrated in the PLANDoc system (McKeown et al., 1994) and shown to generate a wide range of elliptic constructs including RNR, VPE and NCC using FUF/SURGE (Elhadad, 1993), a realisation component based on Functional Unification Grammar. Theune et al. (2006) describe how elliptic sentences are generated in"
W13-2105,vincze-etal-2010-hungarian,0,0.0661275,"Missing"
W13-2105,D09-1043,0,0.0645666,"Missing"
W13-2105,J03-4003,0,\N,Missing
W13-2105,C98-2194,0,\N,Missing
W13-2105,W04-3316,0,\N,Missing
W16-6620,P11-2087,0,0.513673,"Physical Review Letters explaining Higgs mechanism which predicted a new massive elementary particle for the first time. S2 (Split). In 1964 Peter Higgs wrote his second paper in Physical Review Letters explaining Higgs mechanism. Higgs mechanism predicted a new massive elementary particle for the first time. S (Deletion). In 1964 Peter Higgs wrote his paper explaining Higgs mechanism. Higgs mechanism predicted a new elementary particle. First, the input (1C) is rewritten as (1S1 ) by replacing standard words with simpler ones using the context aware lexical simplification method proposed in (Biran et al., 2011). Splitting is then applied to the semantic representation of (1S1 ). Following Narayan and Gardent (2014), we use Boxer 3 (Curran et al., 2007) to map the output sentence from the lexical simplification step (here S1 ) to a Discourse Representation Structure (DRS, (Kamp, 1981)). The DRS for 3 http://svn.ask.it.usyd.edu.au/trac/ candc, Version 1.00 In 1964 Peter Higgs published his second paper in Physical Review Letters describing Higgs mechanism which predicted a new massive spin-zero w boson for the first time . w w wLex Simpl.  In 1964 Peter Higgs wrote his second paper in Physical Review"
W16-6620,E99-1042,0,0.442378,"Work 1 Introduction Sentence simplification maps a sentence to a simpler, more readable one approximating its content. As has been argued in (Shardlow, 2014), sentence simplification has many potential applications. It is useful as a preprocessing step for a variety of NLP systems such as parsers and machine translation systems (Chandrasekar et al., 1996), summarisation (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008) and semantic role labelling (Vickrey and Koller, 2008). It also has wide ranging potential societal applications as a reading aid for people with aphasia (Carroll et al., 1999), for low literacy readers (Watanabe et al., 2009) and for non native speakers (Siddharthan, 2002). In this paper, we present a novel approach to sentence simplification which departs from previous work in two main ways. First, it requires neither hand written rules nor a training corpus of aligned standard and simplified sentences. Instead, we exploit non aligned Simple and English Wikipedia to learn the probability of lexical simplifications, of the semantics of simple sentences and of optional phrases i.e., phrase which may be Earlier work on sentence simplification relied on handcrafted ru"
W16-6620,C96-2183,0,0.82716,"es on deep semantic structure. We show (i) that the unsupervised framework we propose is competitive with four state-of-the-art supervised systems and (ii) that our semantic based approach allows for a principled and effective handling of sentence splitting. 2 Related Work 1 Introduction Sentence simplification maps a sentence to a simpler, more readable one approximating its content. As has been argued in (Shardlow, 2014), sentence simplification has many potential applications. It is useful as a preprocessing step for a variety of NLP systems such as parsers and machine translation systems (Chandrasekar et al., 1996), summarisation (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008) and semantic role labelling (Vickrey and Koller, 2008). It also has wide ranging potential societal applications as a reading aid for people with aphasia (Carroll et al., 1999), for low literacy readers (Watanabe et al., 2009) and for non native speakers (Siddharthan, 2002). In this paper, we present a novel approach to sentence simplification which departs from previous work in two main ways. First, it requires neither hand written rules nor a training corpus of aligned standard and simplified sentences. In"
W16-6620,W11-1601,0,0.247902,"probability. Using both the PWKP corpus developed by Zhu et al. (2010) and the edit history of simple Wikipedia, Woodsend and Lapata (2011) learn a quasi synchronous grammar (Smith and Eisner, 2006) describing a loose alignment between parse trees of complex and of simple sentences. Following Dras (1999), they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate simplification. They evaluate their model on the same dataset used by Zhu et al. (2010) namely, an aligned corpus of 100/131 EWKP/SWKP sentences. Wubben et al. (2012), Coster and Kauchak (2011) and Xu et al. (2016) saw simplification as a monolingual translation task where the complex sentence is the source and the simpler one is the target. To account for deletions, reordering and substitution, Coster and Kauchak (2011) trained a phrase based machine translation system on the PWKP corpus while modifying the word alignment output by GIZA++ in Moses to allow for null phrasal alignments. In this way, they allow for phrases to be deleted during translation. Similarly, Wubben et al. (2012) used Moses and the PWKP data to train a phrase based machine translation system augmented with a p"
W16-6620,P07-2009,0,0.0477558,"ter Higgs wrote his second paper in Physical Review Letters explaining Higgs mechanism. Higgs mechanism predicted a new massive elementary particle for the first time. S (Deletion). In 1964 Peter Higgs wrote his paper explaining Higgs mechanism. Higgs mechanism predicted a new elementary particle. First, the input (1C) is rewritten as (1S1 ) by replacing standard words with simpler ones using the context aware lexical simplification method proposed in (Biran et al., 2011). Splitting is then applied to the semantic representation of (1S1 ). Following Narayan and Gardent (2014), we use Boxer 3 (Curran et al., 2007) to map the output sentence from the lexical simplification step (here S1 ) to a Discourse Representation Structure (DRS, (Kamp, 1981)). The DRS for 3 http://svn.ask.it.usyd.edu.au/trac/ candc, Version 1.00 In 1964 Peter Higgs published his second paper in Physical Review Letters describing Higgs mechanism which predicted a new massive spin-zero w boson for the first time . w w wLex Simpl.  In 1964 Peter Higgs wrote his second paper in Physical Review Letters explaining Higgs mechanism which predicted a new massive elementary particle for the first time . X2 X0 X1 (( named(X0 , higgs, per) ∧("
W16-6620,W08-1105,0,0.510671,"is competitive with four state-of-the-art supervised systems and (ii) that our semantic based approach allows for a principled and effective handling of sentence splitting. 2 Related Work 1 Introduction Sentence simplification maps a sentence to a simpler, more readable one approximating its content. As has been argued in (Shardlow, 2014), sentence simplification has many potential applications. It is useful as a preprocessing step for a variety of NLP systems such as parsers and machine translation systems (Chandrasekar et al., 1996), summarisation (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008) and semantic role labelling (Vickrey and Koller, 2008). It also has wide ranging potential societal applications as a reading aid for people with aphasia (Carroll et al., 1999), for low literacy readers (Watanabe et al., 2009) and for non native speakers (Siddharthan, 2002). In this paper, we present a novel approach to sentence simplification which departs from previous work in two main ways. First, it requires neither hand written rules nor a training corpus of aligned standard and simplified sentences. Instead, we exploit non aligned Simple and English Wikipedia to learn the probability of"
W16-6620,kow-belz-2012-lg,0,0.0130013,"us: the corresponding simple (Gold) 9 Moses support tools: multi-bleu http://www. statmt.org/moses/?n=Moses.SupportTools. 116 sentence from Zhu’s test corpus, the output of our system (UNSUP) and the output of the other four systems (Zhu, Woodsend, Narayan and Wubben) which were provided to us by the system authors10 . We collected ratings from 18 participants. All were either native speakers or proficient in English, having taken part in a Master taught in English or lived in an English speaking country for an extended period of time. The evaluation was done online using the LG-Eval toolkit (Kow and Belz, 2012)11 and a Latin Square Experimental Design (LSED) was used to ensure a fair distribution of the systems and the data across raters. Systems GOLD Zhu Woodsend Wubben Narayan UNSUP Simplicity 3.62 2.62 1.69 1.52 2.30 2.83 Fluency 4.69 2.56 3.15 3.05 3.03 3.56 Adequacy 3.80 2.47 3.15 3.38 3.35 2.83 Table 4: Average Human Ratings for simplicity, fluency and adequacy. Table 4 shows the average ratings of the human evaluation on a scale from 0 to 5. Pairwise comparisons between all models and their statistical significance were carried out using a one-way ANOVA with post-hoc Tukey HSD tests. 10 We up"
W16-6620,P14-1041,1,0.246458,"or phrases to be deleted during translation. Similarly, Wubben et al. (2012) used Moses and the PWKP data to train a phrase based machine translation system augmented with a post-hoc reranking procedure designed to rank the output based on their dissimilarity from the source sentence. Unlinke Wubben et al. (2012) and Coster and Kauchak (2011) who used machine translation as a black box, Xu et al. (2016) proposed to modify the optimization function of SMT systems by tuning them for the sentence simplification task. However, in their work they primarily focus on lexical simplification. Finally, Narayan and Gardent (2014) present a hybrid approach combining a probabilistic model for sentence splitting and deletion with a statistical machine translation system trained on PWKP for substitution and reordering. Our proposal differs from all these approaches in that it does not use the parallel PWKP corpus for training. Nor do we use hand-written rules. Another difference is that we use a deep semantic 112 representation as input for simplification. While a similar approach was proposed in (Narayan and Gardent, 2014), the probabilistic models differ in that we determine splitting points based on the maximum likelih"
W16-6620,E14-1076,0,0.794255,"ptional phrases i.e., phrase which may be Earlier work on sentence simplification relied on handcrafted rules to capture syntactic simplification e.g., to split coordinated and subordinated sentences into several, simpler clauses or to model e.g., active/passive transformations (Siddharthan, 2002; Chandrasekar and Srinivas, 1997; Canning, 2002; Siddharthan, 2011; Siddharthan, 2010). While these hand-crafted approaches can encode precise and linguistically well-informed syntactic transformations, they do not account for lexical simplifications and their interaction with the sentential context. Siddharthan and Mandya (2014) therefore propose an approach where hand-crafted syntactic simplification rules are combined with lexical simplification rules extracted from aligned English and simple English sentences, and revision histories of Simple Wikipedia. Using the parallel dataset formed by Simple English Wikipedia (SWKP)1 and traditional English Wikipedia (EWKP)2 , further work has focused on developing machine learning approaches to sentence simplification. Zhu et al. (2010) constructed a parallel Wikipedia corpus (PWKP) of 108,016/114,924 complex/simple sentences by aligning sentences from EWKP and SWKP and used"
W16-6620,W10-4213,0,0.0787579,"rules nor a training corpus of aligned standard and simplified sentences. Instead, we exploit non aligned Simple and English Wikipedia to learn the probability of lexical simplifications, of the semantics of simple sentences and of optional phrases i.e., phrase which may be Earlier work on sentence simplification relied on handcrafted rules to capture syntactic simplification e.g., to split coordinated and subordinated sentences into several, simpler clauses or to model e.g., active/passive transformations (Siddharthan, 2002; Chandrasekar and Srinivas, 1997; Canning, 2002; Siddharthan, 2011; Siddharthan, 2010). While these hand-crafted approaches can encode precise and linguistically well-informed syntactic transformations, they do not account for lexical simplifications and their interaction with the sentential context. Siddharthan and Mandya (2014) therefore propose an approach where hand-crafted syntactic simplification rules are combined with lexical simplification rules extracted from aligned English and simple English sentences, and revision histories of Simple Wikipedia. Using the parallel dataset formed by Simple English Wikipedia (SWKP)1 and traditional English Wikipedia (EWKP)2 , further"
W16-6620,W11-2802,0,0.243619,"either hand written rules nor a training corpus of aligned standard and simplified sentences. Instead, we exploit non aligned Simple and English Wikipedia to learn the probability of lexical simplifications, of the semantics of simple sentences and of optional phrases i.e., phrase which may be Earlier work on sentence simplification relied on handcrafted rules to capture syntactic simplification e.g., to split coordinated and subordinated sentences into several, simpler clauses or to model e.g., active/passive transformations (Siddharthan, 2002; Chandrasekar and Srinivas, 1997; Canning, 2002; Siddharthan, 2011; Siddharthan, 2010). While these hand-crafted approaches can encode precise and linguistically well-informed syntactic transformations, they do not account for lexical simplifications and their interaction with the sentential context. Siddharthan and Mandya (2014) therefore propose an approach where hand-crafted syntactic simplification rules are combined with lexical simplification rules extracted from aligned English and simple English sentences, and revision histories of Simple Wikipedia. Using the parallel dataset formed by Simple English Wikipedia (SWKP)1 and traditional English Wikipedi"
W16-6620,W06-3104,0,0.156549,"ia.org 111 Proceedings of The 9th International Natural Language Generation conference, pages 111–120, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics tences namely, substitution, reordering, splitting and deletion. It is combined with a language model to improve grammaticality and the decoder translates sentences into simpler ones by greedily selecting the output sentence with highest probability. Using both the PWKP corpus developed by Zhu et al. (2010) and the edit history of simple Wikipedia, Woodsend and Lapata (2011) learn a quasi synchronous grammar (Smith and Eisner, 2006) describing a loose alignment between parse trees of complex and of simple sentences. Following Dras (1999), they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate simplification. They evaluate their model on the same dataset used by Zhu et al. (2010) namely, an aligned corpus of 100/131 EWKP/SWKP sentences. Wubben et al. (2012), Coster and Kauchak (2011) and Xu et al. (2016) saw simplification as a monolingual translation task where the complex sentence is the source and the simpler one is the target. To account for deletio"
W16-6620,P08-1040,0,0.175511,"tems and (ii) that our semantic based approach allows for a principled and effective handling of sentence splitting. 2 Related Work 1 Introduction Sentence simplification maps a sentence to a simpler, more readable one approximating its content. As has been argued in (Shardlow, 2014), sentence simplification has many potential applications. It is useful as a preprocessing step for a variety of NLP systems such as parsers and machine translation systems (Chandrasekar et al., 1996), summarisation (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008) and semantic role labelling (Vickrey and Koller, 2008). It also has wide ranging potential societal applications as a reading aid for people with aphasia (Carroll et al., 1999), for low literacy readers (Watanabe et al., 2009) and for non native speakers (Siddharthan, 2002). In this paper, we present a novel approach to sentence simplification which departs from previous work in two main ways. First, it requires neither hand written rules nor a training corpus of aligned standard and simplified sentences. Instead, we exploit non aligned Simple and English Wikipedia to learn the probability of lexical simplifications, of the semantics of simple se"
W16-6620,D11-1038,0,0.892011,"an input sen1 2 http://simple.wikipedia.org http://en.wikipedia.org 111 Proceedings of The 9th International Natural Language Generation conference, pages 111–120, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics tences namely, substitution, reordering, splitting and deletion. It is combined with a language model to improve grammaticality and the decoder translates sentences into simpler ones by greedily selecting the output sentence with highest probability. Using both the PWKP corpus developed by Zhu et al. (2010) and the edit history of simple Wikipedia, Woodsend and Lapata (2011) learn a quasi synchronous grammar (Smith and Eisner, 2006) describing a loose alignment between parse trees of complex and of simple sentences. Following Dras (1999), they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate simplification. They evaluate their model on the same dataset used by Zhu et al. (2010) namely, an aligned corpus of 100/131 EWKP/SWKP sentences. Wubben et al. (2012), Coster and Kauchak (2011) and Xu et al. (2016) saw simplification as a monolingual translation task where the complex sentence is the sourc"
W16-6620,P12-1107,0,0.429341,"Missing"
W16-6620,P01-1067,0,0.0233067,"exical simplification rules extracted from aligned English and simple English sentences, and revision histories of Simple Wikipedia. Using the parallel dataset formed by Simple English Wikipedia (SWKP)1 and traditional English Wikipedia (EWKP)2 , further work has focused on developing machine learning approaches to sentence simplification. Zhu et al. (2010) constructed a parallel Wikipedia corpus (PWKP) of 108,016/114,924 complex/simple sentences by aligning sentences from EWKP and SWKP and used the resulting bitext to train a simplification model inspired by syntax-based machine translation (Yamada and Knight, 2001). Their simplification model encodes the probabilities for four rewriting operations on the parse tree of an input sen1 2 http://simple.wikipedia.org http://en.wikipedia.org 111 Proceedings of The 9th International Natural Language Generation conference, pages 111–120, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics tences namely, substitution, reordering, splitting and deletion. It is combined with a language model to improve grammaticality and the decoder translates sentences into simpler ones by greedily selecting the output sentence with highest probabil"
W16-6620,C10-1152,0,0.560218,"ed syntactic transformations, they do not account for lexical simplifications and their interaction with the sentential context. Siddharthan and Mandya (2014) therefore propose an approach where hand-crafted syntactic simplification rules are combined with lexical simplification rules extracted from aligned English and simple English sentences, and revision histories of Simple Wikipedia. Using the parallel dataset formed by Simple English Wikipedia (SWKP)1 and traditional English Wikipedia (EWKP)2 , further work has focused on developing machine learning approaches to sentence simplification. Zhu et al. (2010) constructed a parallel Wikipedia corpus (PWKP) of 108,016/114,924 complex/simple sentences by aligning sentences from EWKP and SWKP and used the resulting bitext to train a simplification model inspired by syntax-based machine translation (Yamada and Knight, 2001). Their simplification model encodes the probabilities for four rewriting operations on the parse tree of an input sen1 2 http://simple.wikipedia.org http://en.wikipedia.org 111 Proceedings of The 9th International Natural Language Generation conference, pages 111–120, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computa"
W16-6620,bott-etal-2012-text,0,\N,Missing
W16-6625,N03-1003,0,0.171655,"l corpus, containing 18 million pairs of question paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for opendomain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser (Charniak and Johnson, 2005).3 Given the treebank, we use the spectral algorithm of Narayan and Cohen (2015) to learn an L-PCFG 1 Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). We use an unweighted variant of word lattices in our algorithm. 2 For our experiments, we extract rules from the PPDBSmall to maintain the high precision (Ganitkevitch et al., 2013). 3 We ignore the Paralex alignments for training Gsyn . people speak what kind members of the public what exactly what what sort talking about Czech Czech Republic just what language linguistic do human beings people 's in Republic Czech the Czech Republic Cze express itself talk about ? to talk the population is speaking the citizens Figure 1: An example word lattice for t"
W16-6625,P14-1133,0,0.181583,"robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical paraphrases without much syntactic diversity, whereas our grammar-based approach is capable of producing both lexically and syntactically diverse paraphrases. Unlike MT based approach"
W16-6625,Q15-1039,0,0.0619899,"Missing"
W16-6625,D13-1160,0,0.0680962,"k dataset show that the performance of the semantic parser improves over strong baselines. 1 Introduction Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-ba"
W16-6625,C04-1180,0,0.0518088,"st find the set of oracle grounded graphs—Freebase subgraphs which when executed yield the correct answer—derivable from the question’s ungrounded graphs. These oracle graphs are then used to train a structured perceptron model. These steps are discussed in detail below. 157 3.1 Ungrounded Graphs from Paraphrases We use G RAPH PARSER (Reddy et al., 2014) to convert paraphrases to ungrounded graphs. This conversion involves three steps: 1) parsing the paraphrase using a CCG parser to extract syntactic derivations (Lewis and Steedman, 2014), 2) extracting logical forms from the CCG derivations (Bos et al., 2004), and 3) converting the logical forms to an ungrounded graph.8 The ungrounded graph for the example question and its paraphrases are shown in Figure 3(a), Figure 3(b) and Figure 3(c), respectively. 3.2 Grounded Graphs from Ungrounded Graphs The ungrounded graphs are grounded to Freebase subgraphs by mapping entity nodes, entity-entity edges and entity type nodes in the ungrounded graph to Freebase entities, relations and types, respectively. For example, the graph in Figure 3(b) can be converted to a Freebase graph in Figure 3(d) by replacing the entity node Czech Republic with the Freebase en"
W16-6625,P05-1022,0,0.0827711,"araphrases of the input question q. These paraphrases are further filtered with a classifier to improve the precision of the generated paraphrases. L-PCFG Estimation We train the L-PCFG Gsyn on the Paralex corpus (Fader et al., 2013). Paralex is a large monolingual parallel corpus, containing 18 million pairs of question paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for opendomain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser (Charniak and Johnson, 2005).3 Given the treebank, we use the spectral algorithm of Narayan and Cohen (2015) to learn an L-PCFG 1 Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). We use an unweighted variant of word lattices in our algorithm. 2 For our experiments, we extract rules from the PPDBSmall to maintain the high precision (Ganitkevitch et al., 2013). 3 We ignore the Paralex alignments for training Gsyn . people speak what kind members of the public what ex"
W16-6625,N13-1015,1,0.837008,"ase logical forms (§3). We build on a strong baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources. 2 Paraphrase Generation Using Grammars Our paraphrase generation algorithm is based on a model in the form of an L-PCFG. L-PCFGs are PCFGs where the nonterminals are refined with latent states that provide some contextual information about each node in a given derivation. L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016). In our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al. (2006). The spectral method we use enables the choice of a set of feature functions that indicate the latent states, which proves to be useful in our case. It also leads to sparse grammar estimates and compact models. The spectral method works by identifying feature functions for “inside” and “outside” trees, and then clusters them into latent states. Then it foll"
W16-6625,W02-1001,0,0.0392051,"gˆ) under the model θ ∈ Rn : (ˆ p, u ˆ, gˆ) = arg max θ · Φ(p, u, g, q, K) , (p,u,g) where Φ(p, u, g, q, K) ∈ Rn denotes the features for the tuple of paraphrase, ungrounded and grounded graphs. The feature function has access to the paraphrase, ungrounded and grounded graphs, the original question, as well as to the content of the knowledge base and the denotation |g|K (the denotation of a grounded graph is defined as the set of entities or attributes reachable at its TARGET node). See §4.3 for the features employed. The model parameters are estimated with the averaged structured perceptron (Collins, 2002). Given a training questionanswer pair (q, A), the update is: θt+1 ← θt +Φ(p+ , u+ , g + , q, K)−Φ(ˆ p, u ˆ, gˆ, q, K) , where (p+ , u+ , g + ) denotes the tuple of gold paraphrase, gold ungrounded and grounded graphs for 158 q. Since we do not have direct access to the gold paraphrase and graphs, we instead rely on the set of oracle tuples, OK,A (q), as a proxy: (p+ , u+ , g + ) = arg max (p,u,g)∈OK,A (q) θ · Φ(p, u, g, q, K) , where OK,A (q) is defined as the set of tuples (p, u, g) derivable from the question q, whose denotation |g|K has minimal F1 -loss against the gold answer A. We find t"
W16-6625,N06-2009,0,0.358515,"ebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical paraphrases without much syntactic diversity, whereas our grammar-based approach is capable of producing both lexically and syntactically diverse paraphrases. Unlike MT based approaches, our system does not require aligned parallel paraphrase corpora. In addition we do not require hand annotated grammars for paraphrase generation but instead learn the grammar directly from a large scale question corpus. 15"
W16-6625,P13-1158,0,0.615591,"ney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical paraphrases without much syntactic diversity, whereas our grammar-based approach is capable of producing both lexically and syntactically diverse paraphrases."
W16-6625,N13-1092,0,0.134832,"Missing"
W16-6625,P96-1027,0,0.368333,"First, the sampling from an L-PCFG grammar lessens the lexical ambiguity problem evident in lexicalized grammars such as tree adjoining grammars (Narayan and Gardent, 2012) and combinatory categorial grammars (White, 2004). Our grammar is not lexicalized, only unary context-free rules are lexicalized. Second, the top-down sampling restricts the combinatorics inherent to bottom-up search (Shieber et al., 1990). Third, we do not restrict the generation by the order information in the input. The lack of order information in the input often raises the high combinatorics in lexicalist approaches (Kay, 1996). In our case, however, we use sampling to reduce this problem, and it allows us to produce syntactically diverse questions. And fourth, we impose no constraints on the grammar thereby making it easier to maintain bi-directional (recursive) grammars that can be used both for parsing and for generation (Shieber, 1988). 2.2 Bi-Layered L-PCFGs As mentioned earlier, one of our lattice types is based on bi-layered PCFGs introduced here. In their traditional use, the latent states in LPCFGs aim to capture syntactic information. We introduce here the use of an L-PCFG with two layers of latent states:"
W16-6625,P07-2045,0,0.00417424,"Missing"
W16-6625,P02-1003,0,0.0603275,"e and outside trees as they use, capturing contextual syntactic information about nonterminals. We refer the reader to Narayan and Cohen (2015) for more detailed description of these features. In our experiments, we set the number of latent states to 24. Once we estimate Gsyn from the Paralex corpus, we restrict it for each question to a grammar G0syn by keeping only the rules that could lead to a derivation over the lattice. This step is similar to lexical pruning in standard grammar-based generation process to avoid an intermediate derivation which can never lead to a successful derivation (Koller and Striegnitz, 2002; Narayan and Gardent, 2012). Paraphrase Sampling Sampling a question from the grammar G0syn is done by recursively sampling nodes in the derivation tree, together with their latent states, in a top-down breadth-first fashion. Sampling from the pruned grammar G0syn raises an issue of oversampling words that are more frequent in the training data. To lessen this problem, we follow a controlled sampling approach where sampling is guided by the word lattice Wq . Once a word w from a path e in Wq is sampled, all other parallel or conflicting paths to e are removed from Wq . For example, generating"
W16-6625,D12-1069,0,0.0277247,"y idea is to leverage the flexibility and scalability of latent-variable probabilistic context-free grammars to sample paraphrases. We do an extrinsic evaluation of our paraphrases by plugging them into a semantic parser for Freebase. Our evaluation experiments on the WebQuestions benchmark dataset show that the performance of the semantic parser improves over strong baselines. 1 Introduction Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasi"
W16-6625,D13-1161,0,0.0181592,"parser improves over strong baselines. 1 Introduction Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mai"
W16-6625,P98-1116,0,0.119099,"a large monolingual parallel corpus, containing 18 million pairs of question paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for opendomain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser (Charniak and Johnson, 2005).3 Given the treebank, we use the spectral algorithm of Narayan and Cohen (2015) to learn an L-PCFG 1 Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). We use an unweighted variant of word lattices in our algorithm. 2 For our experiments, we extract rules from the PPDBSmall to maintain the high precision (Ganitkevitch et al., 2013). 3 We ignore the Paralex alignments for training Gsyn . people speak what kind members of the public what exactly what what sort talking about Czech Czech Republic just what language linguistic do human beings people 's in Republic Czech the Czech Republic Cze express itself talk about ? to talk the population is speaking the citizens Figure 1: An ex"
W16-6625,D14-1107,0,0.0240241,"ly on manually assembled questionanswer pairs. For each training question, we first find the set of oracle grounded graphs—Freebase subgraphs which when executed yield the correct answer—derivable from the question’s ungrounded graphs. These oracle graphs are then used to train a structured perceptron model. These steps are discussed in detail below. 157 3.1 Ungrounded Graphs from Paraphrases We use G RAPH PARSER (Reddy et al., 2014) to convert paraphrases to ungrounded graphs. This conversion involves three steps: 1) parsing the paraphrase using a CCG parser to extract syntactic derivations (Lewis and Steedman, 2014), 2) extracting logical forms from the CCG derivations (Bos et al., 2004), and 3) converting the logical forms to an ungrounded graph.8 The ungrounded graph for the example question and its paraphrases are shown in Figure 3(a), Figure 3(b) and Figure 3(c), respectively. 3.2 Grounded Graphs from Ungrounded Graphs The ungrounded graphs are grounded to Freebase subgraphs by mapping entity nodes, entity-entity edges and entity type nodes in the ungrounded graph to Freebase entities, relations and types, respectively. For example, the graph in Figure 3(b) can be converted to a Freebase graph in Fig"
W16-6625,N12-1019,0,0.0463653,"ar is not fine-grained enough and often merges different paraphrase information into the same latent state. This problem is often severe for nonterminals at the top level of the bilayered tree. Hence, we rely only on unary lexical rules (the rules that produce terminal nodes) to extract paraphrase patterns in our experiments. 6 We have 154 positive and 846 negative paraphrase pairs. 7 We do not use the paraphrase pairs from the Paralex corpus to train our classifier, as they do not represent the distribution of our sampled paraphrases and the classifier trained on them performs poorly. follow Madnani et al. (2012), who used MT metrics for paraphrase identification, and experiment with 8 MT metrics as features for our binary classifier. In addition, we experiment with a binary feature which checks if the sampled paraphrase preserves named entities from the input sentence. We use WEKA (Hall et al., 2009) to replicate the classifier of Madnani et al. (2012) with our new feature. We tune the feature set for our classifier on the development data. 3 Semantic Parsing using Paraphrasing In this section we describe how the paraphrase algorithm is used for converting natural language to Freebase queries. Follow"
W16-6625,P14-5010,0,0.00325599,"r paraphrase generation (Quirk et al., 2004; Wubben et al., 2010). In particular, we use Moses (Koehn et al., 2007) to train a monolingual phrase-based MT system on the Paralex corpus. Finally, we use Moses decoder to generate 10-best distinct paraphrases for the test questions. MT 4.3 Implementation Details Entity Resolution For WebQuestions, we use 8 handcrafted part-of-speech patterns (e.g., the pattern (DT)?(JJ.?|NN.?){0,2}NN.? matches the noun phrase the big lebowski) to identify candidate named entity mention spans. We use the Stanford CoreNLP caseless tagger for part-of-speech tagging (Manning et al., 2014). For each candidate mention span, we retrieve the top 10 entities according to the Freebase API.10 We then create a lattice in which the nodes correspond to mention-entity pairs, scored by their Freebase API scores, and the edges encode the fact that no joint assignment of entities to mentions can contain overlapping spans. We take the top 10 paths through the lattice as possible entity disambiguations. For each possibility, we generate n-best paraphrases that contains the entity mention spans. In the end, this process creates a total of 10n paraphrases. We generate ungrounded graphs for thes"
W16-6625,P05-1010,0,0.02558,"rove semantic parsing of questions into Freebase logical forms (§3). We build on a strong baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources. 2 Paraphrase Generation Using Grammars Our paraphrase generation algorithm is based on a model in the form of an L-PCFG. L-PCFGs are PCFGs where the nonterminals are refined with latent states that provide some contextual information about each node in a given derivation. L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016). In our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al. (2006). The spectral method we use enables the choice of a set of feature functions that indicate the latent states, which proves to be useful in our case. It also leads to sparse grammar estimates and compact models. The spectral method works by identifying feature functions for “inside” and “outside” trees, and then clust"
W16-6625,P15-1126,0,0.0215356,"latent states for each nonterminals ensures that we retrieve the correct syntactic representation of the sentence. Here, however, we are more interested in the second latent states assigned to each nonterminals which capture the paraphrase information of the sentence at various levels. For example, we have a unary lexical rule (NN-*-142 day) indicating that we observe day with NN of the paraphrase type 142. We could use this information to extract unary rules of the form (NN-*-142 w) in the treebank that will generate 4 For other cases of separating syntax from semantics in a similar way, see Mitchell and Steedman (2015). 156 words w which are paraphrases to day. Similarly, any node WHNP-*-291 in the treebank will generate paraphrases for what day, SBARQ-*-403, for what day is nochebuena. This way we will be able to generate paraphrases when is nochebuena and when is nochebuena celebrated as they both have SBARQ-*-403 as their roots.5 To generate a word lattice Wq for a given question q, we parse q with the bi-layered grammar Glayered . For each rule of the form X-m1 -m2 → w in the bilayered tree with X ∈ P, m1 ∈ {1, . . . , 24}, m2 ∈ {1, . . . , 1000} and w a word in q, we extract rules of the form X-∗-m2 →"
W16-6625,D15-1214,1,0.808172,". Unlike MT based approaches, our system does not require aligned parallel paraphrase corpora. In addition we do not require hand annotated grammars for paraphrase generation but instead learn the grammar directly from a large scale question corpus. 153 Proceedings of The 9th International Natural Language Generation conference, pages 153–162, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics The main contributions of this paper are two fold. First, we present an algorithm (§2) to generate paraphrases using latent-variable PCFGs. We use the spectral method of Narayan and Cohen (2015) to estimate L-PCFGs on a large scale question treebank. Our grammar model leads to a robust and an efficient system for paraphrase generation in opendomain question answering. While CFGs have been explored for paraphrasing using bilingual parallel corpus (Ganitkevitch et al., 2013), ours is the first implementation of CFG that uses only monolingual data. Second, we show that generated paraphrases can be used to improve semantic parsing of questions into Freebase logical forms (§3). We build on a strong baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline e"
W16-6625,P16-1146,1,0.817238,"baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources. 2 Paraphrase Generation Using Grammars Our paraphrase generation algorithm is based on a model in the form of an L-PCFG. L-PCFGs are PCFGs where the nonterminals are refined with latent states that provide some contextual information about each node in a given derivation. L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016). In our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al. (2006). The spectral method we use enables the choice of a set of feature functions that indicate the latent states, which proves to be useful in our case. It also leads to sparse grammar estimates and compact models. The spectral method works by identifying feature functions for “inside” and “outside” trees, and then clusters them into latent states. Then it follows with a maximum likelihood estimation step, that"
W16-6625,C12-1124,1,0.916187,"se, capturing contextual syntactic information about nonterminals. We refer the reader to Narayan and Cohen (2015) for more detailed description of these features. In our experiments, we set the number of latent states to 24. Once we estimate Gsyn from the Paralex corpus, we restrict it for each question to a grammar G0syn by keeping only the rules that could lead to a derivation over the lattice. This step is similar to lexical pruning in standard grammar-based generation process to avoid an intermediate derivation which can never lead to a successful derivation (Koller and Striegnitz, 2002; Narayan and Gardent, 2012). Paraphrase Sampling Sampling a question from the grammar G0syn is done by recursively sampling nodes in the derivation tree, together with their latent states, in a top-down breadth-first fashion. Sampling from the pruned grammar G0syn raises an issue of oversampling words that are more frequent in the training data. To lessen this problem, we follow a controlled sampling approach where sampling is guided by the word lattice Wq . Once a word w from a path e in Wq is sampled, all other parallel or conflicting paths to e are removed from Wq . For example, generating for the word lattice in Fig"
W16-6625,N03-1024,0,0.0794827,"million pairs of question paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for opendomain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser (Charniak and Johnson, 2005).3 Given the treebank, we use the spectral algorithm of Narayan and Cohen (2015) to learn an L-PCFG 1 Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). We use an unweighted variant of word lattices in our algorithm. 2 For our experiments, we extract rules from the PPDBSmall to maintain the high precision (Ganitkevitch et al., 2013). 3 We ignore the Paralex alignments for training Gsyn . people speak what kind members of the public what exactly what what sort talking about Czech Czech Republic just what language linguistic do human beings people 's in Republic Czech the Czech Republic Cze express itself talk about ? to talk the population is speaking the citizens Figure 1: An example word lattice for the question What la"
W16-6625,P06-1055,0,0.0144064,"questions into Freebase logical forms (§3). We build on a strong baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources. 2 Paraphrase Generation Using Grammars Our paraphrase generation algorithm is based on a model in the form of an L-PCFG. L-PCFGs are PCFGs where the nonterminals are refined with latent states that provide some contextual information about each node in a given derivation. L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016). In our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al. (2006). The spectral method we use enables the choice of a set of feature functions that indicate the latent states, which proves to be useful in our case. It also leads to sparse grammar estimates and compact models. The spectral method works by identifying feature functions for “inside” and “outside” trees, and then clusters them into latent"
W16-6625,W05-1512,0,0.00962666,"n be used to improve semantic parsing of questions into Freebase logical forms (§3). We build on a strong baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources. 2 Paraphrase Generation Using Grammars Our paraphrase generation algorithm is based on a model in the form of an L-PCFG. L-PCFGs are PCFGs where the nonterminals are refined with latent states that provide some contextual information about each node in a given derivation. L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016). In our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al. (2006). The spectral method we use enables the choice of a set of feature functions that indicate the latent states, which proves to be useful in our case. It also leads to sparse grammar estimates and compact models. The spectral method works by identifying feature functions for “inside” and “outsid"
W16-6625,W04-3219,0,0.273262,"estion paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for opendomain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser (Charniak and Johnson, 2005).3 Given the treebank, we use the spectral algorithm of Narayan and Cohen (2015) to learn an L-PCFG 1 Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). We use an unweighted variant of word lattices in our algorithm. 2 For our experiments, we extract rules from the PPDBSmall to maintain the high precision (Ganitkevitch et al., 2013). 3 We ignore the Paralex alignments for training Gsyn . people speak what kind members of the public what exactly what what sort talking about Czech Czech Republic just what language linguistic do human beings people 's in Republic Czech the Czech Republic Cze express itself talk about ? to talk the population is speaking the citizens Figure 1: An example word lattice for the question What language do people in C"
W16-6625,Q14-1030,1,0.410455,"ng baselines. 1 Introduction Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical p"
W16-6625,P07-1059,0,0.0439532,"al of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical paraphrases without much syntactic diversity, whereas our grammar-based approach is capable of producing both lexically and syntactically diverse paraphrases. Unlike MT based approaches, our system does not require aligned parallel paraphrase corpora. In addition we do not require hand annotated grammars for paraphrase generation but instead learn the grammar directly from a large scale question corpus. 153 Proceedings of The 9t"
W16-6625,C88-2128,0,0.670481,"e top-down sampling restricts the combinatorics inherent to bottom-up search (Shieber et al., 1990). Third, we do not restrict the generation by the order information in the input. The lack of order information in the input often raises the high combinatorics in lexicalist approaches (Kay, 1996). In our case, however, we use sampling to reduce this problem, and it allows us to produce syntactically diverse questions. And fourth, we impose no constraints on the grammar thereby making it easier to maintain bi-directional (recursive) grammars that can be used both for parsing and for generation (Shieber, 1988). 2.2 Bi-Layered L-PCFGs As mentioned earlier, one of our lattice types is based on bi-layered PCFGs introduced here. In their traditional use, the latent states in LPCFGs aim to capture syntactic information. We introduce here the use of an L-PCFG with two layers of latent states: one layer is intended to capture the usual syntactic information, and the other aims to capture semantic and topical information by using a SBARQ-33-403 SBARQ-30-403 SQ-8-925 WHNP-7-291 WP-7-254 NN-45-142 AUX-22-300 NN-41-854 what day is nochebuena SBARQ-24-403 WRB-42-707 SQ-8-709 WRB-42-707 when AUX-12-300 NN-41-85"
W16-6625,P15-1129,0,0.0272823,", 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical paraphrases without much syntactic diversity, whereas our grammar-based approach is capable of producing both lexically and syntactically diverse paraphrases. Unlike MT based approaches, our system does"
W16-6625,N06-1056,0,0.0309204,"phrase generation that does not require any sentence-aligned paraphrase corpus. Our key idea is to leverage the flexibility and scalability of latent-variable probabilistic context-free grammars to sample paraphrases. We do an extrinsic evaluation of our paraphrases by plugging them into a semantic parser for Freebase. Our evaluation experiments on the WebQuestions benchmark dataset show that the performance of the semantic parser improves over strong baselines. 1 Introduction Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et a"
W16-6625,W10-4223,0,0.165589,"Missing"
W16-6625,P16-1220,1,0.837668,"lts on the test data. We get similar results on the test data as we reported on the development data. Again, the PPDB model performs best with an F1 score of 47.7. The baselines, ORIGINAL and MT, lag with scores of 45.0 and 47.1, respectively. We also present the results of existing literature on this dataset. Among these, Berant and Liang (2014) also uses paraphrasing but unlike ours it is based on a template grammar (containing 8 grammar rules) and requires logical forms beforehand to generate paraphrases. Our PPDB outperforms Berant and Liang’s model by 7.8 F1 points. Yih et al. (2015) and Xu et al. (2016) use neural network models for semantic parsing, in addition to using sophisticated entity resolution (Yang and Chang, 2015) and a very large unsupervised corpus as additional training data. Note that we use G RAPH PARSER as our semantic parsing 160 avg oracle F1 # oracle graphs avg F1 65.1 71.5 71.2 71.8 71.6 11.0 77.2 53.6 59.8 55.0 44.7 47.0 47.5 47.9 47.1 ORIGINAL MT NAIVE PPDB BILAYERED Table 1: Oracle statistics and results on the WebQuestions development set. Method avg P. Berant and Liang ’14 40.5 Bast and Haussmann ’15 49.8 Berant and Liang ’15 50.4 49.0 Reddy et al. ’16 Yih et al. ’1"
W16-6625,P15-1049,0,0.0148143,"odel performs best with an F1 score of 47.7. The baselines, ORIGINAL and MT, lag with scores of 45.0 and 47.1, respectively. We also present the results of existing literature on this dataset. Among these, Berant and Liang (2014) also uses paraphrasing but unlike ours it is based on a template grammar (containing 8 grammar rules) and requires logical forms beforehand to generate paraphrases. Our PPDB outperforms Berant and Liang’s model by 7.8 F1 points. Yih et al. (2015) and Xu et al. (2016) use neural network models for semantic parsing, in addition to using sophisticated entity resolution (Yang and Chang, 2015) and a very large unsupervised corpus as additional training data. Note that we use G RAPH PARSER as our semantic parsing 160 avg oracle F1 # oracle graphs avg F1 65.1 71.5 71.2 71.8 71.6 11.0 77.2 53.6 59.8 55.0 44.7 47.0 47.5 47.9 47.1 ORIGINAL MT NAIVE PPDB BILAYERED Table 1: Oracle statistics and results on the WebQuestions development set. Method avg P. Berant and Liang ’14 40.5 Bast and Haussmann ’15 49.8 Berant and Liang ’15 50.4 49.0 Reddy et al. ’16 Yih et al. ’15 52.8 Xu et al. ’16 53.1 This paper ORIGINAL 53.2 48.0 MT NAIVE 48.1 PPDB 48.4 BILAYERED 47.0 avg R. 46.6 60.4 55.7 61.1 60"
W16-6625,P15-1128,0,0.0431321,"2 shows our final results on the test data. We get similar results on the test data as we reported on the development data. Again, the PPDB model performs best with an F1 score of 47.7. The baselines, ORIGINAL and MT, lag with scores of 45.0 and 47.1, respectively. We also present the results of existing literature on this dataset. Among these, Berant and Liang (2014) also uses paraphrasing but unlike ours it is based on a template grammar (containing 8 grammar rules) and requires logical forms beforehand to generate paraphrases. Our PPDB outperforms Berant and Liang’s model by 7.8 F1 points. Yih et al. (2015) and Xu et al. (2016) use neural network models for semantic parsing, in addition to using sophisticated entity resolution (Yang and Chang, 2015) and a very large unsupervised corpus as additional training data. Note that we use G RAPH PARSER as our semantic parsing 160 avg oracle F1 # oracle graphs avg F1 65.1 71.5 71.2 71.8 71.6 11.0 77.2 53.6 59.8 55.0 44.7 47.0 47.5 47.9 47.1 ORIGINAL MT NAIVE PPDB BILAYERED Table 1: Oracle statistics and results on the WebQuestions development set. Method avg P. Berant and Liang ’14 40.5 Bast and Haussmann ’15 49.8 Berant and Liang ’15 50.4 49.0 Reddy et"
W16-6625,C98-1112,0,\N,Missing
W16-6625,P14-1091,0,\N,Missing
W16-6625,N15-3014,0,\N,Missing
W16-6625,P15-1026,0,\N,Missing
W16-6626,W11-2832,0,0.0220477,"cusing on different categories. Moreover, the set of relations on both KBs pose different challenges for generation, while the AURA KB contains n-ary relations DBPedia contains relations names challenging for the lexicalisation subtask. A last difference with our task is the content selection method. Our method is completely automatic and thus permits the inexpensive generation of a large benchmark. Moreover, it can be used to select content ranging from a single triple to several triples and with different shapes. The Surface Realisation Shared Task (SR’11). The major goal of the SR’11 task (Belz et al., 2011) was to provide a common ground for the comparison of surface realisers on the task of regenerating sentences in a treebank. Two different tracks are considered with different input representations. The ’shallow’ input provides a dependency tree of the sentence to be generated and the ’deep’ input provides a graph representation where syntactic dependencies have been replaced by semantic roles and some function words have been removed. The focus of the SR’11 task was on the linguistic realisation subtask and the broad coverage of linguistic phenomena. The task we propose here starts from non-l"
W16-6626,W13-2102,0,0.0271306,"Missing"
W16-6626,W13-0108,0,0.0608974,"from DBPedia Data Emilie Colin1 Claire Gardent1 Yassine M’rabet2 Shashi Narayan3 Laura Perez-Beltrachini1 1 CNRS/LORIA and Universit´e de Lorraine, Nancy, France {emilie.colin,claire.gardent,laura.perez}@loria.fr 2 National Library of Medicine, Bethesda, USA yassine.m’rabet@nih.gov 3 School of Informatics, University of Edinburgh, UK snaraya2@inf.ed.ac.uk 1 Introduction With the emergence of the linked data initiative and the rapid development of RDF (Resource Description Format) datasets, several approaches have recently been proposed for generating text from RDF data (Sun and Mellish, 2006; Duma and Klein, 2013; Bontcheva and Wilks, 2004; Cimiano et al., 2013; Lebret et al., 2016). To support the evaluation and comparison of such systems, we propose a shared task on generating text from DBPedia data. The training data will consist of Data/Text pairs where the data is a set of triples extracted from DBPedia and the text is a verbalisation of these triples. In essence, the task consists in mapping data to text. Specific subtasks include sentence segmentation (how to chunk the input data into sentences), lexicalisation (of the DBPedia properties), aggregation (how to avoid repetitions) and surface real"
W16-6626,W08-1131,0,0.0189634,"function words have been removed. The focus of the SR’11 task was on the linguistic realisation subtask and the broad coverage of linguistic phenomena. The task we propose here starts from non-linguistic KB data and puts forward other NLG subtasks. Generating Referring Expressions (GRE). The GRE shared tasks pioneered the proposed NLG challenges. The first shared task has only focused on the selection of distinguishing attributes (Belz and Gatt, 2007) while subsequent tasks have considered the referring expression realisation subtask proposing a complete referring expression generation task (Gatt et al., 2008; Gatt et al., 2009). This tasks aimed at the unique identification of the referent and brevity of the referring expression. Slightly different, the GREC challenges (Belz et al., 2008; Belz et al., 2009; Belz et al., 2010) propose the generation of referring expressions in a discourse context. The GREC tasks use a corpus created from Wikipedia abstracts on geographic entities and people and with two referring expression annotation schemes, reference type and word strings. Rather than generating from data input these tasks consist in labelling underspecified referring expressions in a given tex"
W16-6626,W09-0629,0,0.0763856,"e been removed. The focus of the SR’11 task was on the linguistic realisation subtask and the broad coverage of linguistic phenomena. The task we propose here starts from non-linguistic KB data and puts forward other NLG subtasks. Generating Referring Expressions (GRE). The GRE shared tasks pioneered the proposed NLG challenges. The first shared task has only focused on the selection of distinguishing attributes (Belz and Gatt, 2007) while subsequent tasks have considered the referring expression realisation subtask proposing a complete referring expression generation task (Gatt et al., 2008; Gatt et al., 2009). This tasks aimed at the unique identification of the referent and brevity of the referring expression. Slightly different, the GREC challenges (Belz et al., 2008; Belz et al., 2009; Belz et al., 2010) propose the generation of referring expressions in a discourse context. The GREC tasks use a corpus created from Wikipedia abstracts on geographic entities and people and with two referring expression annotation schemes, reference type and word strings. Rather than generating from data input these tasks consist in labelling underspecified referring expressions in a given text. Our task concerns"
W16-6626,D16-1128,0,0.0549581,"Missing"
W16-6626,mendes-etal-2012-dbpedia,0,0.0200225,"Missing"
W16-6626,W16-6616,1,0.65506,"trast, our proposal targets all generation subtasks involved in content realisation. 4 Data As illustrated in Example 1 above, the training corpus consists of (D, T ) pairs such that D is a set of DBPedia triples and T is an English text (possibly consisting of a single sentence). This corpus will be constructed in two steps by first, extracting from DBPedia content units that are both coherent and diverse and second, associating these content units with English text verbalising their content. Data To extract content units from DBPedia, we will use the content selection procedure sketched in (Mohammed et al., 2016). This procedure consists of two steps. First, bigram models of DBPedia properties specific to a given DBPedia category (e.g., Astronaut) are learned from the DBPedia graphs associated with entities of that category. Second, an 165 ILP program is used to extract from DBPedia, subtrees that maximise bigram probability. In effect, the extracted DBPedia trees are coherent entity descriptions in that the property bigram they contain often cooccur together in the DBPedia graphs associated with entities of a given DBPedia category. The method can be parameterised to produce content units for differe"
W16-6626,W16-3506,1,0.809821,"contained in the Lemon English Lexicon for DBPedia7 (Walter et al., 2013; Walter et al., 2014a; Walter et al., 2014b) and by manually filtering the lexicalisations produced by the lexicalisation method described in (PerezBeltrachini and Gardent, 2016) and by the relation extraction and clustering method described in (c.f. (Nakashole et al., 2012))8 . We will then ask crowdsourcers to verbalise sets of DBPEdia triples in which properties have already been lexicalised (e.g., C REW 1U P will be lexicalised as commander of ). Second, we will exploit the data-to-text alignment method presented in (Mrabet et al., 2016) to semiautomatically align Wikipedia text with sets of DBPedia triples. The method consists in (i) automatically annotating phrases with DBPedia entities, (ii) associating sentences with DBPedia triples relating entities annotating these sentences and (iii) using crowdsourcing to align sentences with triples. In the third step, annotators are asked to “align” triples and sentences that is, to remove from the sentence all material that is irrelevant to express the associated triples and vice versa, to remove any triples that is not expressed by the sentence. Statistics, Schedule and Funding Th"
W16-6626,S16-2027,1,0.87081,"Missing"
W16-6626,D15-1199,0,0.0380916,"Missing"
W16-6626,2007.mtsummit-ucnlg.14,0,\N,Missing
W16-6626,W13-2111,1,\N,Missing
W17-3518,W13-2111,1,0.909881,"Missing"
W17-3518,2007.mtsummit-ucnlg.14,0,0.0931338,"Missing"
W17-3518,W09-2817,0,0.0289064,"ur evaluation methodology, analyse participant results and provide a brief description of the participating systems. (1) a. Data: (J OHN E B LAHA BIRTH DATE 1942 08 26) (J OHN E B LAHA S AN A NTONIO ) (J OHN E B LAHA OCCUPATION F IGHTER PILOT ) b. Text: John E Blaha, born in San Antonio on 194208-26, worked as a fighter pilot 2 1 BIRTH P LACE Introduction Previous Natural Language Generation (NLG) challenges have focused on surface realisation (Banik et al., 2013; Belz et al., 2011), referring expression generation (Belz and Gatt, 2007; Gatt et al., 2008; Gatt et al., 2009; Belz et al., 2008; Belz et al., 2009; Belz et al., 2010) and content selection (BouayadAgha et al., 2013). In contrast, the WebNLG challenge focuses on microplanning, that subtask of NLG which consists in mapping a given content to a text verbalising this content. Microplanning is a complex choice problem involving several subtasks referred to in the literature as referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. For instance, given the WebNLG data unit shown in (1a), generating the text in (1b) involves choosing to lexicalise the JOHN E BLAHA Data As illustrated by the"
W17-3518,W11-2832,0,0.104147,"Missing"
W17-3518,W13-2112,0,0.0508518,"Missing"
W17-3518,P16-1054,0,0.0250493,"UT ILBURG -SMT Tilburg University UIT-VNU-HCM University of Information Technology UPF-FORG E Universitat Pompeu Fabra SMT Systems UT ILBURG -SMT Tilburg University NMT Systems ADAPTC ENTRE ADAPT Centre, Ireland UM ELBOURNE University of Melbourne UT ILBURG -NMT Tilburg University PKUW RITER Peking University BASELINE Table 2: Categorisation of participating systems. from, UT ILBURG -P IPELINE first ordered triples to maintain discourse order. Extracted rules were then applied to generate a delexicalised text. Missing entities were added using a referring expression generation module (Castro Ferreira et al., 2016). Finally, a 6-gram language model trained on the Gigaword corpus was used to rank the system output. UIT-VNU-HCM did not resort to delexicalisation in their rules. Instead of using the text to extract templates, it used the typed-dependency structure of the text to facilitate rule extraction from the training data. In addition, at run time, WordNet was used to estimate similarity between predicates in the test and train sets. UPF-FORG E mostly focused on sentence planning with predicate-argument (PredArg) templates. For each of the DBPedia properties found in the training and evaluation data,"
W17-3518,W14-3348,0,0.0163047,"Missing"
W17-3518,P17-1017,1,0.291717,"o in the literature as referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. For instance, given the WebNLG data unit shown in (1a), generating the text in (1b) involves choosing to lexicalise the JOHN E BLAHA Data As illustrated by the above example, the WebNLG dataset was designed to exercise the ability of NLG systems to handle the whole range of microplanning operations and their interactions. It was created using a content selection procedure specifically designed to enhance data and text variety (Perez-Beltrachini et al., 2016). In (Gardent et al., 2017), we compared a dataset created using the WebNLG process with existing benchmarks in particular, (Wen et al., 2016)’s dataset (RNNLG) which was produced using a similar process. In what follows, we give various statistics about the WebNLG dataset using the RNNLG dataset as a reference point. Size. The WebNLG dataset consists of 25,298 (data,text) pairs and 9,674 distinct data units. The data units are sets of RDF triples extracted from DBPedia and the texts are sequences of one or more sentences verbalising these data units. 124 Proceedings of The 10th International Natural Language Generation"
W17-3518,W08-1131,0,0.151063,"Missing"
W17-3518,W09-0629,0,0.0265947,"describe data preparation, introduce our evaluation methodology, analyse participant results and provide a brief description of the participating systems. (1) a. Data: (J OHN E B LAHA BIRTH DATE 1942 08 26) (J OHN E B LAHA S AN A NTONIO ) (J OHN E B LAHA OCCUPATION F IGHTER PILOT ) b. Text: John E Blaha, born in San Antonio on 194208-26, worked as a fighter pilot 2 1 BIRTH P LACE Introduction Previous Natural Language Generation (NLG) challenges have focused on surface realisation (Banik et al., 2013; Belz et al., 2011), referring expression generation (Belz and Gatt, 2007; Gatt et al., 2008; Gatt et al., 2009; Belz et al., 2008; Belz et al., 2009; Belz et al., 2010) and content selection (BouayadAgha et al., 2013). In contrast, the WebNLG challenge focuses on microplanning, that subtask of NLG which consists in mapping a given content to a text verbalising this content. Microplanning is a complex choice problem involving several subtasks referred to in the literature as referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. For instance, given the WebNLG data unit shown in (1a), generating the text in (1b) involves choosing to lexicalise the JO"
W17-3518,P17-4012,0,0.00506614,"extracted from seen categories. (4) a. Set of triples: (I NDONESIA J USUF K ALLA ) (BAKSO LEADER NAME INGREDIENT N OODLE ) (BAKSO COUNTRY I NDONESIA ) b. Text: Bakso is a food containing noodles; it is found in Indonesia where Jusuf Kalla is the leader. (5) a. Source: (COUNTRY LEADER NAME LEADERNAME) (FOOD INGREDIENT INGREDIENT) (FOOD COUNTRY COUNTRY) b. Target: FOOD is a food containing noodles ; it is found in COUNTRY where LEADERNAME is the leader . On this delexicalised data-to-text corpus, we trained a vanilla sequence-to-sequence model with attention mechanism using the OpenNMT toolkit (Klein et al., 2017) with default parameters for training and decoding. The network consists of a twolayered bidirectional encoder-decoder model with LSTM units. We use a batch size of 64 and a starting learning rate of 1.0. The size of the hidden layers is 500. The network was trained for 13 epochs with a stochastic gradient descent optimisation method and a dropout probability of 0.3. We used the entire vocabulary for the baseline due to its rather small size. Source Target Total Original 2703 5374 8077 Delexicalised 1300 5013 6313 Table 4: Vocabulary size in tokens. After training we relexicalised sentences wi"
W17-3518,W06-3114,0,0.0350981,"resented in this paper. The results of the humanbased evaluation will be provided on the WebNLG website2 in October 2017. 2 http://talc1.loria.fr/webnlg/stories/ challenge.html 4.1 Automatic Evaluation test set, but not in the training and development set. Three automatic metrics were used to evaluate the participating systems: • BLEU-43 (Papineni et al., 2002). BLEU scores were computed using up to three references. • METEOR (v1.5)4 (Denkowski and Lavie, 2014); Prop. Obj. • TER5 (Snover et al., 2006). For statistical significance testing, we followed the bootstrapping algorithm described in (Koehn and Monz, 2006). To assess the ability of the participating systems to generalise to out of domain data, the test dataset consists of two sets of roughly equal size: a test set containing inputs created for entities belonging to DBpedia categories that were seen in the training data (Astronaut, University, Monument, Building, ComicsCharacter, Food, Airport, SportsTeam, City, and WrittenWork), and a test set containing inputs extracted for entities belonging to 5 unseen categories (Athlete, Artist, MeanOfTransportation, CelestialBody, Politician). We call the first type of data seen categories, the second, un"
W17-3518,S17-2158,0,0.041301,"cipating systems and our baseline (BASELINE) system. These can be grouped into three categories: pipeline systems, statistical machine translation (SMT) and neural machine translation (NMT) systems. Table 3 shows the system categorisations. Pipeline Systems. Three submissions used a template or grammar-based pipeline framework with some NLG module: UT ILBURG -P IPELINE, UITVNU-HCM and UPF-FORG E. The first two systems, UT ILBURG -P IPELINE and UIT-VNU-HCM, extracted rules or templates from the training data for surface realisation, whereas the third system, UPF-FORG E, used the FORGe grammar (Mille et al., 2017). UT ILBURG -P IPELINE extracted rules mapping a triple (or a triple set) to a text observed in the training data; both the triple and the associated text were delexicalised. Given a RDF triple set to generate 126 System ID Institution P IPELINE Systems UT ILBURG -SMT Tilburg University UIT-VNU-HCM University of Information Technology UPF-FORG E Universitat Pompeu Fabra SMT Systems UT ILBURG -SMT Tilburg University NMT Systems ADAPTC ENTRE ADAPT Centre, Ireland UM ELBOURNE University of Melbourne UT ILBURG -NMT Tilburg University PKUW RITER Peking University BASELINE Table 2: Categorisation of"
W17-3518,D17-1064,1,0.810278,"aset consisting of 40,049 (data, text) pairs, 15,095 distinct data input and 15 DBpedia categories is also available. Both datasets are under the creative common licence “CC Attribution-Noncommercial-Share Alike 4.0 International license”. We hope that these resources will enable a long and fruitful strand of research on microplanning. The usefulness of the WebNLG dataset reaches far beyond the WebNLG challenge. It can be used for instance to train a semantic parser which would convert a sentence into a set of RDF triples. It can also be used to derive new datasets for related tasks. Thus in (Narayan et al., 2017), we show how to derive from the WebNLG dataset, a dataset for sentence simplification which we call the Split-andRephrase dataset. In this dataset, each pair consists of (i) a single, complex sentence with its meaning representation in terms of RDF triples and (ii) a sequence of at least two sentences and their corresponding sets of RDF triples whereby these sets form a partition on the set of RDF triples associated with the input complex sentence. In other words, the Split-and-Rephrase dataset associates a complex sentence with a sequence of at least two sentences whose meaning is the same a"
W17-3518,P02-1040,0,0.113293,"Missing"
W17-3518,W17-3537,1,0.797882,"e naturally give rise to object relative clauses or participials. Another factor impacting syntactic variation is the set of properties (input patterns) cooccuring in a given input. This is illustrated by the examples in (3) where two inputs of the same length (3 triples hence 3 properties) result in text with different syntax. That is, a larger number of input patterns is more likely to induce texts with greater syntactic variety. By extracting data units from a large number of distinct domains (DBPedia categories), we seeked to produce a large number of distinct input patterns. 1 Following (Perez-Beltrachini and Gardent, 2017), we use (Lu, 2008)’s system to compute the CTTR (Carroll, 1964). 125 (3) a. C A participated in C operated B mission A SIBLING operator B mission n (2) X TITLE Y ⇒ X served as Y Verb X NATIONALITY Y ⇒ X’s nationality is Y Relational noun X COUNTRY Y ⇒ X is in Y Preposition X NATIONALITY USA ⇒ X is American Adjective To promote diverse lexicalisation patterns, we extracted data from 15 DBPedia categories (Astronaut, University, Monument, Building, ComicsCharacter, Food, Airport, SportsTeam, WrittenWork, Athlete, Artist, City, MeanOfTransportation, CelestialBody, Politician) resulting in a set"
W17-3518,C16-1141,1,0.698605,"involving several subtasks referred to in the literature as referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. For instance, given the WebNLG data unit shown in (1a), generating the text in (1b) involves choosing to lexicalise the JOHN E BLAHA Data As illustrated by the above example, the WebNLG dataset was designed to exercise the ability of NLG systems to handle the whole range of microplanning operations and their interactions. It was created using a content selection procedure specifically designed to enhance data and text variety (Perez-Beltrachini et al., 2016). In (Gardent et al., 2017), we compared a dataset created using the WebNLG process with existing benchmarks in particular, (Wen et al., 2016)’s dataset (RNNLG) which was produced using a similar process. In what follows, we give various statistics about the WebNLG dataset using the RNNLG dataset as a reference point. Size. The WebNLG dataset consists of 25,298 (data,text) pairs and 9,674 distinct data units. The data units are sets of RDF triples extracted from DBPedia and the texts are sequences of one or more sentences verbalising these data units. 124 Proceedings of The 10th International"
W17-3518,E17-3017,0,0.0162677,"s were tuned using 60batch MIRA with BLEU as the evaluation metric. Similar to UT ILBURG -P IPELINE, the system used a 6-gram language model trained on the Gigaword corpus using KenLM. NMT Systems. Four systems (ADAPTC ENTRE, UM ELBOURNE, UT ILBURG -NMT and PKUW RITER) build upon the attention-based encoder-decoder architecture proposed in (Bahdanau et al., 2014). Most of them make use of existing NMT frameworks. There are however important differences among systems with respect to both the concrete architecture and the sequence representations they use. ADAPTC ENTRE makes use of the Nematus (Sennrich et al., 2017) system. They opt for subword representations rather than delexicalisation to deal with rare words and sparsity. They linearise the input sequence and insert tuple separation special tokens. 127 UM ELBOURNE does a combined delexicalisation procedure and enrichment of the input sequence. Entities are delexicalised using an entity identifier (ENTITY- ID). When available, the DBPedia type of the entity is appended. An n-gram search is used to assure the most accurate target sequence delexicalisation. They use a standard encoder-decoder with attention model. UT ILBURG -NMT is based on the Edinburg"
W17-3518,2006.amta-papers.25,0,0.128663,"Missing"
W17-3518,N16-1015,0,0.00976834,"Missing"
W17-3518,P07-2045,0,\N,Missing
