2020.acl-main.718,P98-1013,0,0.479484,"Missing"
2020.acl-main.718,P10-1160,0,0.639694,"we recognize the possible existence of fillers for these roles, for example, the place of the particular Air∗ Equal Contribution Data and code at http://nlp.jhu.edu/rams/. 2  would indicate there is no explicit referent in the text. 1 strikeMissileStrike event. These implicit arguments are linked to explicit arguments in the document (i.e., text spans). We refer to the task of finding explicit argument(s) to fill each role for an event as argument linking. Prior annotation of cross-sentence argument links has produced small datasets, with a focus either on a small number of predicate types (Gerber and Chai, 2010, 2012; Feizabadi and Pad´o, 2014) or on a small number of documents (Ruppenhofer et al., 2010). To enable the development of a neural model for argument linking, we produce Roles Across Multiple Sentences (RAMS), a dataset of 9,124 annotated events from news based on an ontology of 139 event types and 65 roles. In a 5sentence window around each event trigger, we annotate the closest argument span for each role. Our model builds on recent ideas in span selection models (Lee et al., 2018; He et al., 2018; Ouchi et al., 2018), used in this work for the multisentence argument linking task for RAM"
2020.acl-main.718,J12-4003,0,0.201031,"ll number of documents (Ruppenhofer et al., 2010). To enable the development of a neural model for argument linking, we produce Roles Across Multiple Sentences (RAMS), a dataset of 9,124 annotated events from news based on an ontology of 139 event types and 65 roles. In a 5sentence window around each event trigger, we annotate the closest argument span for each role. Our model builds on recent ideas in span selection models (Lee et al., 2018; He et al., 2018; Ouchi et al., 2018), used in this work for the multisentence argument linking task for RAMS and for several other event-based datasets (Gerber and Chai, 2012; Pradhan et al., 2013; Pavlick et al., 2016, AIDA Phase 1). On RAMS our best model achieves 68.3 F1 , and it achieves 73.3 F1 when event types are also known, outperforming strong baselines. We also demonstrate effective use of RAMS as pre-training for a related dataset. 8057 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8057–8077 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Our main contributions are a novel model for argument linking and a new large-scale dataset for the task. Our dataset is annotated for arguments acro"
2020.acl-main.718,P18-2058,0,0.131827,"as produced small datasets, with a focus either on a small number of predicate types (Gerber and Chai, 2010, 2012; Feizabadi and Pad´o, 2014) or on a small number of documents (Ruppenhofer et al., 2010). To enable the development of a neural model for argument linking, we produce Roles Across Multiple Sentences (RAMS), a dataset of 9,124 annotated events from news based on an ontology of 139 event types and 65 roles. In a 5sentence window around each event trigger, we annotate the closest argument span for each role. Our model builds on recent ideas in span selection models (Lee et al., 2018; He et al., 2018; Ouchi et al., 2018), used in this work for the multisentence argument linking task for RAMS and for several other event-based datasets (Gerber and Chai, 2012; Pradhan et al., 2013; Pavlick et al., 2016, AIDA Phase 1). On RAMS our best model achieves 68.3 F1 , and it achieves 73.3 F1 when event types are also known, outperforming strong baselines. We also demonstrate effective use of RAMS as pre-training for a related dataset. 8057 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8057–8077 c July 5 - 10, 2020. 2020 Association for Computational Li"
2020.acl-main.718,C18-1313,0,0.0399872,"Missing"
2020.acl-main.718,D18-1191,0,0.437212,"datasets, with a focus either on a small number of predicate types (Gerber and Chai, 2010, 2012; Feizabadi and Pad´o, 2014) or on a small number of documents (Ruppenhofer et al., 2010). To enable the development of a neural model for argument linking, we produce Roles Across Multiple Sentences (RAMS), a dataset of 9,124 annotated events from news based on an ontology of 139 event types and 65 roles. In a 5sentence window around each event trigger, we annotate the closest argument span for each role. Our model builds on recent ideas in span selection models (Lee et al., 2018; He et al., 2018; Ouchi et al., 2018), used in this work for the multisentence argument linking task for RAMS and for several other event-based datasets (Gerber and Chai, 2012; Pradhan et al., 2013; Pavlick et al., 2016, AIDA Phase 1). On RAMS our best model achieves 68.3 F1 , and it achieves 73.3 F1 when event types are also known, outperforming strong baselines. We also demonstrate effective use of RAMS as pre-training for a related dataset. 8057 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8057–8077 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Our main co"
2020.acl-main.718,J05-1004,0,0.255144,"Missing"
2020.acl-main.718,D16-1106,0,0.128769,"010). To enable the development of a neural model for argument linking, we produce Roles Across Multiple Sentences (RAMS), a dataset of 9,124 annotated events from news based on an ontology of 139 event types and 65 roles. In a 5sentence window around each event trigger, we annotate the closest argument span for each role. Our model builds on recent ideas in span selection models (Lee et al., 2018; He et al., 2018; Ouchi et al., 2018), used in this work for the multisentence argument linking task for RAMS and for several other event-based datasets (Gerber and Chai, 2012; Pradhan et al., 2013; Pavlick et al., 2016, AIDA Phase 1). On RAMS our best model achieves 68.3 F1 , and it achieves 73.3 F1 when event types are also known, outperforming strong baselines. We also demonstrate effective use of RAMS as pre-training for a related dataset. 8057 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8057–8077 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Our main contributions are a novel model for argument linking and a new large-scale dataset for the task. Our dataset is annotated for arguments across multiple sentences and has broader covera"
2020.acl-main.718,D14-1162,0,0.0839459,"r) for each event e ∈ E and role r ∈ Re . Representations To represent text spans, we adopt the convention from Lee et al. (2017) that has been used for a broad suite of core NLP tasks (Swayamdipta et al., 2018; He et al., 2018; Tenney et al., 2019b). A bidirectional LSTM encodes each sentence’s contextualized embeddings (Peters et al., 2018; Devlin et al., 2018). The hidden states at the start and end of the span are concatenated along with a feature vector for the size of the span and a soft head word vector produced by a learned attention mask over the word vectors 8060 (GloVe embeddings (Pennington et al., 2014) and character-level convolutions) within the span. We use this method to form representations of trigger spans, e, and of candidate argument spans, a. We learn a separate embedding, r, for each role in the ontology, r ∈ R. Since our objective is to link candidate arguments to event-role pairs, we construct an event-role representation10 by applying a feed-forward neural network (Fa˜ ) to the event trigger span and role embedding: ˜e,r = Fa˜ ([e; r]) a (1) argument span is to participate in an event, which can be determined irrespective of a role: sc (e, a) = e> Wc a + sA (a) + sE (e) + φc (e,"
2020.acl-main.718,N18-1202,0,0.682091,"tualized text embeddings are used to form candidate argument span representations, A. These are then pruned and scored alongside the trigger span and learned role embeddings to determine the best argument span (possibly none) for each event and role, i.e., argmaxa∈A P (a |e, r) for each event e ∈ E and role r ∈ Re . Representations To represent text spans, we adopt the convention from Lee et al. (2017) that has been used for a broad suite of core NLP tasks (Swayamdipta et al., 2018; He et al., 2018; Tenney et al., 2019b). A bidirectional LSTM encodes each sentence’s contextualized embeddings (Peters et al., 2018; Devlin et al., 2018). The hidden states at the start and end of the span are concatenated along with a feature vector for the size of the span and a soft head word vector produced by a learned attention mask over the word vectors 8060 (GloVe embeddings (Pennington et al., 2014) and character-level convolutions) within the span. We use this method to form representations of trigger spans, e, and of candidate argument spans, a. We learn a separate embedding, r, for each role in the ontology, r ∈ R. Since our objective is to link candidate arguments to event-role pairs, we construct an event-ro"
2020.acl-main.718,W13-3516,0,0.0716894,"Missing"
2020.acl-main.746,P13-1023,0,0.023631,"nsductive parser performs comparably while additionally performing attribute prediction. By analyzing the attribute prediction errors, we find the model captures natural relationships between attribute groups. 1 Introduction A structured account of compositional meaning has been longstanding goal for both natural language understanding and computational semantics. To this end, a number of efforts have focused on encoding semantic relationships and attributes in a semantic graph—e.g. Abstract Meaning Representation (AMR; Banarescu et al., 2013), Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013), and Semantic Dependency Parsing (SDP; Oepen et al., 2014, 2015, 2016). In these formalisms, semantic information is typically encoded discretely, using nominal category labels for nodes and edges. This categorical encoding can make such formalisms brittle when presented with non-prototypical instances, and leads to challenges in coping with changing label ontologies and new datasets (White et al., 2019). Furthermore, they are difficult to annotate, often requiring trained linguists and large annotation manuals. The Decompositional Semantics framework presents an alternative to categorical fo"
2020.acl-main.746,P81-1022,0,0.68989,"Missing"
2020.acl-main.746,P13-2009,0,0.018191,"ucture of the graph. Zhang et al. (2018) develop the first model for performing both graph parsing and UDS attribute prediction in a cross-lingual setting, where Chinese input sentences were transduced into UDS graphs derived from UD parses of the input’s English translation. This represents the first application of the parsing-as-transduction paradigm to a subset of UDS data as well as the introduction of a novel graph evaluation metric, S which we describe in further detail in Section 5. In contrast to the end-to-end approach presented here, Zhang et al. take a pipeline approach to parsing. Andreas et al. (2013) recast semantic parsing in a tree formalism as a sequence-to-sequence problem. Parsing-as-transduction, which extends this approach to directed acyclic graphs, has proven to be applicable in a variety of settings: Zhang et al. (2019a) use it to achieve state-of-the-art re8428 sults in AMR parsing. These results are improved upon and shown to generalize to two other semantic formalisms (UCCA and SDP) by Zhang et al. (2019b), which set new state-of-the-art benchmarks for AMR and UCCA. The former result was subsequently surpassed by Cai and Lam (2020), which applies a similar transductive approa"
2020.acl-main.746,S16-1180,0,0.0236123,"shown to generalize to two other semantic formalisms (UCCA and SDP) by Zhang et al. (2019b), which set new state-of-the-art benchmarks for AMR and UCCA. The former result was subsequently surpassed by Cai and Lam (2020), which applies a similar transductive approach, while the latter was surpassed by Jiang et al. (2019). Having both been subjects of SemEval tasks (May, 2016; May and Priyadarshi, 2017; Oepen et al., 2019; Hershcovich et al., 2019), there are a number of contrasting methods for both AMR and UCCA parsing. These include transition-based parsing system for AMR (Wang et al., 2018; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017) and for UCCA (Hershcovich et al., 2017). In a similar vein to Zhang et al. (2019b), Hershcovich et al. (2018a) convert multiple formalisms into a unified formalism and use multitask learning for improved UCCA parsing; however, the latter does so at a loss to performance on the other formalisms, while Zhang et al. achieve state-of-the-art results in AMR and UCCA simultaneously. UCCA has also been shown to transfer to syntactic parsing: by converting UD parse trees into a format resembling UCCA, Hershcovich et al. (2018b) are able to appl"
2020.acl-main.746,D17-1130,0,0.0251187,"ormalisms (UCCA and SDP) by Zhang et al. (2019b), which set new state-of-the-art benchmarks for AMR and UCCA. The former result was subsequently surpassed by Cai and Lam (2020), which applies a similar transductive approach, while the latter was surpassed by Jiang et al. (2019). Having both been subjects of SemEval tasks (May, 2016; May and Priyadarshi, 2017; Oepen et al., 2019; Hershcovich et al., 2019), there are a number of contrasting methods for both AMR and UCCA parsing. These include transition-based parsing system for AMR (Wang et al., 2018; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017) and for UCCA (Hershcovich et al., 2017). In a similar vein to Zhang et al. (2019b), Hershcovich et al. (2018a) convert multiple formalisms into a unified formalism and use multitask learning for improved UCCA parsing; however, the latter does so at a loss to performance on the other formalisms, while Zhang et al. achieve state-of-the-art results in AMR and UCCA simultaneously. UCCA has also been shown to transfer to syntactic parsing: by converting UD parse trees into a format resembling UCCA, Hershcovich et al. (2018b) are able to apply a UCCA parser to both standard UD parses as well as enh"
2020.acl-main.746,W13-2322,0,0.0352826,"line model for parsing into the UDS graph structure, and show that our transductive parser performs comparably while additionally performing attribute prediction. By analyzing the attribute prediction errors, we find the model captures natural relationships between attribute groups. 1 Introduction A structured account of compositional meaning has been longstanding goal for both natural language understanding and computational semantics. To this end, a number of efforts have focused on encoding semantic relationships and attributes in a semantic graph—e.g. Abstract Meaning Representation (AMR; Banarescu et al., 2013), Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013), and Semantic Dependency Parsing (SDP; Oepen et al., 2014, 2015, 2016). In these formalisms, semantic information is typically encoded discretely, using nominal category labels for nodes and edges. This categorical encoding can make such formalisms brittle when presented with non-prototypical instances, and leads to challenges in coping with changing label ontologies and new datasets (White et al., 2019). Furthermore, they are difficult to annotate, often requiring trained linguists and large annotation manuals. The"
2020.acl-main.746,2020.acl-main.119,0,0.130825,"al. take a pipeline approach to parsing. Andreas et al. (2013) recast semantic parsing in a tree formalism as a sequence-to-sequence problem. Parsing-as-transduction, which extends this approach to directed acyclic graphs, has proven to be applicable in a variety of settings: Zhang et al. (2019a) use it to achieve state-of-the-art re8428 sults in AMR parsing. These results are improved upon and shown to generalize to two other semantic formalisms (UCCA and SDP) by Zhang et al. (2019b), which set new state-of-the-art benchmarks for AMR and UCCA. The former result was subsequently surpassed by Cai and Lam (2020), which applies a similar transductive approach, while the latter was surpassed by Jiang et al. (2019). Having both been subjects of SemEval tasks (May, 2016; May and Priyadarshi, 2017; Oepen et al., 2019; Hershcovich et al., 2019), there are a number of contrasting methods for both AMR and UCCA parsing. These include transition-based parsing system for AMR (Wang et al., 2018; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017) and for UCCA (Hershcovich et al., 2017). In a similar vein to Zhang et al. (2019b), Hershcovich et al. (2018a) convert multiple formalisms int"
2020.acl-main.746,P13-2131,0,0.117211,"Manning, 2014)—and passing its output through PredPatt to create a structure.4 For this baseline, 4 This structure is missing the core decompositional attributes but has both predicate and argument nodes. Additionally, the pipeline model fails to capture nominal heads of copular predicates (e.g. Jo is a doctor), which are not returned by PredPatt but are added to the dataset as a preprocessing step in the genericity annotation task. the only source of error is the UD parsing model, which for English performs very highly. S Metric For evaluating the quality of output graph structures, Smatch (Cai and Knight, 2013), a hill-climbing approach to approximating the optimal matching between variables in two graphs, is commonly used. While Smatch can match categorial variables such as those found in meaning representations like AMR, it lacks a matching function for continuous variables such as decompositional attributes. To remedy this, Zhang et al. (2018) introduce the S metric, an extension to Smatch that allows for attribute matching. Using hill-climbing, we are able to match instance and attribute nodes and edges; instance nodes are matched via string match, while attribute   ν −ν 2 similarity is given"
2020.acl-main.746,D14-1082,0,0.0405845,"in §7. Following Zhang et al. (2019b) we train the structural parsing modules with coverage loss (See et al., 2017). All models were trained to convergence using the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.001. 5 P 84.83 83.52 84.97 Experiments Pipeline Model Recall from Section 3 that the semantic graph structure in UDS graphs is deterministically generated from PredPatt, which takes as input a UD parse and outputs a semantic graph structure. This leads to a strong pipeline model for the graph structure alone: running a high-performing UD parser—the Stanford UD parser (Chen and Manning, 2014)—and passing its output through PredPatt to create a structure.4 For this baseline, 4 This structure is missing the core decompositional attributes but has both predicate and argument nodes. Additionally, the pipeline model fails to capture nominal heads of copular predicates (e.g. Jo is a doctor), which are not returned by PredPatt but are added to the dataset as a preprocessing step in the genericity annotation task. the only source of error is the UD parsing model, which for English performs very highly. S Metric For evaluating the quality of output graph structures, Smatch (Cai and Knight,"
2020.acl-main.746,E17-1051,0,0.0858745,"o two other semantic formalisms (UCCA and SDP) by Zhang et al. (2019b), which set new state-of-the-art benchmarks for AMR and UCCA. The former result was subsequently surpassed by Cai and Lam (2020), which applies a similar transductive approach, while the latter was surpassed by Jiang et al. (2019). Having both been subjects of SemEval tasks (May, 2016; May and Priyadarshi, 2017; Oepen et al., 2019; Hershcovich et al., 2019), there are a number of contrasting methods for both AMR and UCCA parsing. These include transition-based parsing system for AMR (Wang et al., 2018; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017) and for UCCA (Hershcovich et al., 2017). In a similar vein to Zhang et al. (2019b), Hershcovich et al. (2018a) convert multiple formalisms into a unified formalism and use multitask learning for improved UCCA parsing; however, the latter does so at a loss to performance on the other formalisms, while Zhang et al. achieve state-of-the-art results in AMR and UCCA simultaneously. UCCA has also been shown to transfer to syntactic parsing: by converting UD parse trees into a format resembling UCCA, Hershcovich et al. (2018b) are able to apply a UCCA parser to bot"
2020.acl-main.746,N19-1423,0,0.0158881,"be consulted for further details on the encoder, decoder, and pointer-generator modules. The original parser is composed of six major modules: the encoder, the decoder embedding module, the target node module, the target label module, the head module, and the relation module. In this work we introduce two new modules: the node attribute module and the edge attribute module, as well a loss function for attributes. Encoder The encoder module takes a concatenation of multiple input features: GloVe token embeddings (Pennington et al., 2014), POS tag embeddings, character CNN embeddings, and BERT (Devlin et al., 2019) contextual embeddings (meanpooled over subwords). These representations are passed through a stacked bidirectional LSTM encoder, which has the following definition: →  ""− −−− → l−1 t # − l s LSTM(s ,s ) t slt = ← = ← −−− − tl−1 t−1 − l st LSTM(st , stt+1 ) where arrows denote the LSTM direction, t denotes the timestep, and l denotes the layer of the stack. Decoder embedding module In order to generate new semantic nodes and relationships, a method of embedding categorical semantic information is required. More formally, a semantic relation is given by a tuple hui , dui , ri , vi , dvi i, wh"
2020.acl-main.746,P17-1104,0,0.0218422,", which set new state-of-the-art benchmarks for AMR and UCCA. The former result was subsequently surpassed by Cai and Lam (2020), which applies a similar transductive approach, while the latter was surpassed by Jiang et al. (2019). Having both been subjects of SemEval tasks (May, 2016; May and Priyadarshi, 2017; Oepen et al., 2019; Hershcovich et al., 2019), there are a number of contrasting methods for both AMR and UCCA parsing. These include transition-based parsing system for AMR (Wang et al., 2018; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017) and for UCCA (Hershcovich et al., 2017). In a similar vein to Zhang et al. (2019b), Hershcovich et al. (2018a) convert multiple formalisms into a unified formalism and use multitask learning for improved UCCA parsing; however, the latter does so at a loss to performance on the other formalisms, while Zhang et al. achieve state-of-the-art results in AMR and UCCA simultaneously. UCCA has also been shown to transfer to syntactic parsing: by converting UD parse trees into a format resembling UCCA, Hershcovich et al. (2018b) are able to apply a UCCA parser to both standard UD parses as well as enhanced UD parses, which contain re-entran"
2020.acl-main.746,P18-1035,0,0.0168351,"er result was subsequently surpassed by Cai and Lam (2020), which applies a similar transductive approach, while the latter was surpassed by Jiang et al. (2019). Having both been subjects of SemEval tasks (May, 2016; May and Priyadarshi, 2017; Oepen et al., 2019; Hershcovich et al., 2019), there are a number of contrasting methods for both AMR and UCCA parsing. These include transition-based parsing system for AMR (Wang et al., 2018; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017) and for UCCA (Hershcovich et al., 2017). In a similar vein to Zhang et al. (2019b), Hershcovich et al. (2018a) convert multiple formalisms into a unified formalism and use multitask learning for improved UCCA parsing; however, the latter does so at a loss to performance on the other formalisms, while Zhang et al. achieve state-of-the-art results in AMR and UCCA simultaneously. UCCA has also been shown to transfer to syntactic parsing: by converting UD parse trees into a format resembling UCCA, Hershcovich et al. (2018b) are able to apply a UCCA parser to both standard UD parses as well as enhanced UD parses, which contain re-entrant nodes. 3 Data The UDS1.0 dataset is built on top of the UD-EWT data"
2020.acl-main.746,K18-2010,0,0.0204984,"er result was subsequently surpassed by Cai and Lam (2020), which applies a similar transductive approach, while the latter was surpassed by Jiang et al. (2019). Having both been subjects of SemEval tasks (May, 2016; May and Priyadarshi, 2017; Oepen et al., 2019; Hershcovich et al., 2019), there are a number of contrasting methods for both AMR and UCCA parsing. These include transition-based parsing system for AMR (Wang et al., 2018; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017) and for UCCA (Hershcovich et al., 2017). In a similar vein to Zhang et al. (2019b), Hershcovich et al. (2018a) convert multiple formalisms into a unified formalism and use multitask learning for improved UCCA parsing; however, the latter does so at a loss to performance on the other formalisms, while Zhang et al. achieve state-of-the-art results in AMR and UCCA simultaneously. UCCA has also been shown to transfer to syntactic parsing: by converting UD parse trees into a format resembling UCCA, Hershcovich et al. (2018b) are able to apply a UCCA parser to both standard UD parses as well as enhanced UD parses, which contain re-entrant nodes. 3 Data The UDS1.0 dataset is built on top of the UD-EWT data"
2020.acl-main.746,S19-2001,0,0.0135011,"has proven to be applicable in a variety of settings: Zhang et al. (2019a) use it to achieve state-of-the-art re8428 sults in AMR parsing. These results are improved upon and shown to generalize to two other semantic formalisms (UCCA and SDP) by Zhang et al. (2019b), which set new state-of-the-art benchmarks for AMR and UCCA. The former result was subsequently surpassed by Cai and Lam (2020), which applies a similar transductive approach, while the latter was surpassed by Jiang et al. (2019). Having both been subjects of SemEval tasks (May, 2016; May and Priyadarshi, 2017; Oepen et al., 2019; Hershcovich et al., 2019), there are a number of contrasting methods for both AMR and UCCA parsing. These include transition-based parsing system for AMR (Wang et al., 2018; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017) and for UCCA (Hershcovich et al., 2017). In a similar vein to Zhang et al. (2019b), Hershcovich et al. (2018a) convert multiple formalisms into a unified formalism and use multitask learning for improved UCCA parsing; however, the latter does so at a loss to performance on the other formalisms, while Zhang et al. achieve state-of-the-art results in AMR and UCCA simultane"
2020.acl-main.746,S19-2002,0,0.017671,"malism as a sequence-to-sequence problem. Parsing-as-transduction, which extends this approach to directed acyclic graphs, has proven to be applicable in a variety of settings: Zhang et al. (2019a) use it to achieve state-of-the-art re8428 sults in AMR parsing. These results are improved upon and shown to generalize to two other semantic formalisms (UCCA and SDP) by Zhang et al. (2019b), which set new state-of-the-art benchmarks for AMR and UCCA. The former result was subsequently surpassed by Cai and Lam (2020), which applies a similar transductive approach, while the latter was surpassed by Jiang et al. (2019). Having both been subjects of SemEval tasks (May, 2016; May and Priyadarshi, 2017; Oepen et al., 2019; Hershcovich et al., 2019), there are a number of contrasting methods for both AMR and UCCA parsing. These include transition-based parsing system for AMR (Wang et al., 2018; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017) and for UCCA (Hershcovich et al., 2017). In a similar vein to Zhang et al. (2019b), Hershcovich et al. (2018a) convert multiple formalisms into a unified formalism and use multitask learning for improved UCCA parsing; however, the latter does s"
2020.acl-main.746,S16-1166,0,0.0138471,", which extends this approach to directed acyclic graphs, has proven to be applicable in a variety of settings: Zhang et al. (2019a) use it to achieve state-of-the-art re8428 sults in AMR parsing. These results are improved upon and shown to generalize to two other semantic formalisms (UCCA and SDP) by Zhang et al. (2019b), which set new state-of-the-art benchmarks for AMR and UCCA. The former result was subsequently surpassed by Cai and Lam (2020), which applies a similar transductive approach, while the latter was surpassed by Jiang et al. (2019). Having both been subjects of SemEval tasks (May, 2016; May and Priyadarshi, 2017; Oepen et al., 2019; Hershcovich et al., 2019), there are a number of contrasting methods for both AMR and UCCA parsing. These include transition-based parsing system for AMR (Wang et al., 2018; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017) and for UCCA (Hershcovich et al., 2017). In a similar vein to Zhang et al. (2019b), Hershcovich et al. (2018a) convert multiple formalisms into a unified formalism and use multitask learning for improved UCCA parsing; however, the latter does so at a loss to performance on the other formalisms, whi"
2020.acl-main.746,S17-2090,0,0.0165582,"ends this approach to directed acyclic graphs, has proven to be applicable in a variety of settings: Zhang et al. (2019a) use it to achieve state-of-the-art re8428 sults in AMR parsing. These results are improved upon and shown to generalize to two other semantic formalisms (UCCA and SDP) by Zhang et al. (2019b), which set new state-of-the-art benchmarks for AMR and UCCA. The former result was subsequently surpassed by Cai and Lam (2020), which applies a similar transductive approach, while the latter was surpassed by Jiang et al. (2019). Having both been subjects of SemEval tasks (May, 2016; May and Priyadarshi, 2017; Oepen et al., 2019; Hershcovich et al., 2019), there are a number of contrasting methods for both AMR and UCCA parsing. These include transition-based parsing system for AMR (Wang et al., 2018; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017) and for UCCA (Hershcovich et al., 2017). In a similar vein to Zhang et al. (2019b), Hershcovich et al. (2018a) convert multiple formalisms into a unified formalism and use multitask learning for improved UCCA parsing; however, the latter does so at a loss to performance on the other formalisms, while Zhang et al. achieve sta"
2020.acl-main.746,K19-2001,0,0.0239153,"Missing"
2020.acl-main.746,L16-1630,0,0.113554,"handled natively by the transductive paradigm. We compare our end-to-end transductive parser against a strong pipeline system, finding that the parser slightly outperforms the pipeline while additionally learning to produce decompositional attribute scores. Our results are reflected in the UDS1.0 leaderboard at http://decomp.io/ leaderboards/. 2 Related Work Datasets Reisinger et al. (2015) introduce the Decompositional Semantics framework in the context of a corpus-based verification of Dowty’s seminal proto-role theory of semantic roles. This work was substantially expanded by White et al. (2016), who annotate for semantic proto-roles (SPR), wordsense, and temporal properties on top of semantic graphs extracted from English Web Treebank (EWT; Bies et al., 2012) UD parses using PredPatt (White et al., 2016; Zhang et al., 2017). White et al.’s EWT annotations are modeled by Teichert et al. (2017), who present a CRF-based multi-label classifier for proto-role labelling, and Rudinger et al. (2018a), who make use of an eventdriven neural model. More recently, the annotation coverage for the same EWT data was expanded by Vashishtha et al. (2019) who annotate and model fine-grained temporal"
2020.acl-main.746,S15-2153,0,0.0646547,"Missing"
2020.acl-main.746,S14-2008,0,0.0701843,"Missing"
2020.acl-main.746,D14-1162,0,0.0840291,"sductive broadcoverage parsing model presented in Zhang et al. (2019b), which can be consulted for further details on the encoder, decoder, and pointer-generator modules. The original parser is composed of six major modules: the encoder, the decoder embedding module, the target node module, the target label module, the head module, and the relation module. In this work we introduce two new modules: the node attribute module and the edge attribute module, as well a loss function for attributes. Encoder The encoder module takes a concatenation of multiple input features: GloVe token embeddings (Pennington et al., 2014), POS tag embeddings, character CNN embeddings, and BERT (Devlin et al., 2019) contextual embeddings (meanpooled over subwords). These representations are passed through a stacked bidirectional LSTM encoder, which has the following definition: →  ""− −−− → l−1 t # − l s LSTM(s ,s ) t slt = ← = ← −−− − tl−1 t−1 − l st LSTM(st , stt+1 ) where arrows denote the LSTM direction, t denotes the timestep, and l denotes the layer of the stack. Decoder embedding module In order to generate new semantic nodes and relationships, a method of embedding categorical semantic information is required. More for"
2020.acl-main.746,D18-1114,1,0.90479,"Missing"
2020.acl-main.746,N18-1067,1,0.878102,"Missing"
2020.acl-main.746,P17-1099,0,0.0451571,"← s l;→ sl] 0 1 n respectively. Further, let ci be a context vector over encoder states sl1:n , defined as  = softmax MLP(enc) ([hli ; sl1:n ]) a(enc) i 8430 ci = aTi sl1:n Let hli and zi be defined as follows: zi = MLP(relation) ([hli ; ci ; ri ; ui ; dui ]) l hli = LSTM(hl−1 i , hi−1 ) where zi can be thought as a representation of node i in the graph, conditioned on previous nodes (via hli as well as the input text via ci , the graph token (via ui and dui ) and the relation type (via ri ). Using this representation zi , Zhang et al. (2019b) introduce an extended pointer-generator network (See et al., 2017) which computes the distribution over the next node label vi+1 : P(vi+1 ) = pgen p(vocab) ⊕ penc a(enc) ⊕ pdec a(dec) i i i From this last equation, we have that the generation of a new node is decomposed into three options: (1) generate a new node from a vocabulary of node labels, (2) copy a node label directly from the input sequence (lexicalization), or (3) copy a node label from a previously generated node (re-entrancy). Parsing modules To obtain a parse from the node states h1:n , a head node and relation type must be assigned to each node 1 : n. In order to assign a head node, we instant"
2020.acl-main.746,D17-3004,1,0.678827,"Missing"
2020.acl-main.746,P19-1280,1,0.90031,"Missing"
2020.acl-main.746,D16-1177,1,0.904966,"Missing"
2020.acl-main.746,2020.lrec-1.699,1,0.799604,"Missing"
2020.acl-main.746,P19-1009,1,0.892405,"Missing"
2020.acl-main.746,D19-1392,1,0.826676,"Missing"
2020.acl-main.746,D18-1194,1,0.886522,"Missing"
2020.acl-main.746,Q15-1034,1,\N,Missing
2020.acl-main.746,W17-6944,1,\N,Missing
2020.acl-main.749,E17-1075,0,0.0937255,"Missing"
2020.acl-main.749,P18-1009,0,0.660012,"itive types are highlighted. Introduction Entity typing is the assignment of a semantic label to a span of text, where that span is usually a mention of some entity in the real world. Named entity recognition (NER) is a canonical information extraction task, commonly considered a form of entity typing that assigns spans to one of a handful of types, such as PER, ORG, GPE, and so on. Fine-grained entity typing (FET) seeks to classify spans into types according to more diverse, semantically richer ontologies (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015; Choi et al., 2018), and has begun to be used in downstream models for entity linking (Gupta et al., 2017; Raiman and Raiman, 2018). Consider the example in Figure 1 from the FET dataset, FIGER (Ling and Weld, 2012). The mention of interest, Hollywood Hills, will be typed with the single label LOC in traditional NER, but may be typed with a set of types {/location, /geography, /geography/mountain} under a fine-grained typing scheme. In these finergrained typing schemes, types usually form a hierarchy: there are a set of coarse types that lies on 1 Code can be found at https://github.com/ ctongfei/hierarchical-ty"
2020.acl-main.749,D19-1643,0,0.562899,"ns (Lin and Ji, 2019). Our approach builds upon these developments and uses state-of-theart mention encoders. • Incorporating the hierarchy: Most prior works approach the hierarchical typing problem Researchers have proposed alternative FET formulations whose types are not formed in a type hierarchy, in particular Ultra-fine entity typing (Choi et al., 2018; Xiong et al., 2019; Onoe and Durrett, 2019), with a very large set of types derived from phrases mined from a corpus. FET in KB (Jin et al., 2019) labels mentions to types in a knowledge base with multiple relations, forming a type graph. Dai et al. (2019) augments the task with entity linking to KBs. 3 Problem Formulation We denote a mention as a tuple x = (w, l, r), where w = (w1, · · · , wn ) is the sentential context and the span [l : r] marks a mention of interest in sentence w. That is, the mention of interest is (wl, · · · , wr ). Given x, a hierarchical entity typing model outputs 8466 a set of types Y in the type ontology Y, i.e. Y ⊆ Y. Type hierarchies take the form of a forest, where each tree is rooted by a top-level supertype (e.g. /person, /location, etc.). We add a dummy parent node ENTITY = “/”, the supertype of all entity types"
2020.acl-main.749,D15-1103,0,0.111972,"FIGER ontology. Positive types are highlighted. Introduction Entity typing is the assignment of a semantic label to a span of text, where that span is usually a mention of some entity in the real world. Named entity recognition (NER) is a canonical information extraction task, commonly considered a form of entity typing that assigns spans to one of a handful of types, such as PER, ORG, GPE, and so on. Fine-grained entity typing (FET) seeks to classify spans into types according to more diverse, semantically richer ontologies (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015; Choi et al., 2018), and has begun to be used in downstream models for entity linking (Gupta et al., 2017; Raiman and Raiman, 2018). Consider the example in Figure 1 from the FET dataset, FIGER (Ling and Weld, 2012). The mention of interest, Hollywood Hills, will be typed with the single label LOC in traditional NER, but may be typed with a set of types {/location, /geography, /geography/mountain} under a fine-grained typing scheme. In these finergrained typing schemes, types usually form a hierarchy: there are a set of coarse types that lies on 1 Code can be found at https://github.com/ cton"
2020.acl-main.749,N19-1423,0,0.0158898,"d from the mention vector m. We use the multiplicative attention of Luong et al. (2015): ai ∝ exp(mT Qwi ) c= N X i=1 ai w i ∈ R d w (2) (3) The final representation for an entity mention is generated via concatenation of the mention and context vector: [m ; c] ∈ R2dw . 4.2 Type Scorer We learn a type embedding y ∈ Rdt for each type y ∈ Y. To score an instance with representation [m ; c], we pass it through a 2-layer feed-forward network that maps into the same space as the type space Rdt , with tanh as the nonlinearity. The final 3 Lin and Ji (2019) found that ELMo performs better than BERT (Devlin et al., 2019) for FET. Our internal experiments also confirm this finding. We hypothesize that this is due to the richer character-level information contained in lowerlevel ELMo representations that are useful for FET. 4 Lin and Ji (2019) proposed an attentive pooler with a learned global query vector. We found out that a simple max pooling layer achieves similar performance. 8467 score is an inner product between the transformed feature vector and the type embedding: F(x, y) = FFNN([m ; c]) · y . 4.3 (4) Hierarchical Learning-to-Rank We introduce our novel hierarchical learning-torank loss that (1) allows"
2020.acl-main.749,W18-2501,0,0.0262513,"ogether with their respective hyperparameters.8 5.2 Setup To best compare to recent prior work, we follow Lin and Ji (2019) where the ELMo encodings of words are fixed and not updated. We use all 3 layers of ELMo output, so the initial embedding has dimension dw = 3072. We set the type embedding dimensionality to be dt = 1024. The initial learning rate is 10−5 and the batch size is 256. Hyperparameter choices are tuned on dev sets, and are listed in Table 1. We employ early stopping: choosing the model that yields the best micro F1 score on dev sets. Our models are implemented using AllenNLP (Gardner et al., 2018), with implementation for subtyping relation constraints from OpenKE (Han et al., 2018). 5.3 Baselines We compare our approach to major prior work in FET that are capable of multi-path entity typing.9 For AIDA, since there are no prior work on this dataset to our knowledge, we also implemented multi-label classification as set of binary classifier models (similar to Lin and Ji (2019)) as a baseline, with our mention feature extractor. The results are shown in Table 2 as “Multi-label”. 5.4 Metrics We follow prior work and use strict accuracy (Acc), macro F1 (MaF), and micro F1 (MiF) scores. Giv"
2020.acl-main.749,D17-1284,0,0.0434305,"c label to a span of text, where that span is usually a mention of some entity in the real world. Named entity recognition (NER) is a canonical information extraction task, commonly considered a form of entity typing that assigns spans to one of a handful of types, such as PER, ORG, GPE, and so on. Fine-grained entity typing (FET) seeks to classify spans into types according to more diverse, semantically richer ontologies (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015; Choi et al., 2018), and has begun to be used in downstream models for entity linking (Gupta et al., 2017; Raiman and Raiman, 2018). Consider the example in Figure 1 from the FET dataset, FIGER (Ling and Weld, 2012). The mention of interest, Hollywood Hills, will be typed with the single label LOC in traditional NER, but may be typed with a set of types {/location, /geography, /geography/mountain} under a fine-grained typing scheme. In these finergrained typing schemes, types usually form a hierarchy: there are a set of coarse types that lies on 1 Code can be found at https://github.com/ ctongfei/hierarchical-typing. the top level—these are similar to traditional NER types, e.g. /person; addition"
2020.acl-main.749,D18-2024,0,0.0201845,"ork, we follow Lin and Ji (2019) where the ELMo encodings of words are fixed and not updated. We use all 3 layers of ELMo output, so the initial embedding has dimension dw = 3072. We set the type embedding dimensionality to be dt = 1024. The initial learning rate is 10−5 and the batch size is 256. Hyperparameter choices are tuned on dev sets, and are listed in Table 1. We employ early stopping: choosing the model that yields the best micro F1 score on dev sets. Our models are implemented using AllenNLP (Gardner et al., 2018), with implementation for subtyping relation constraints from OpenKE (Han et al., 2018). 5.3 Baselines We compare our approach to major prior work in FET that are capable of multi-path entity typing.9 For AIDA, since there are no prior work on this dataset to our knowledge, we also implemented multi-label classification as set of binary classifier models (similar to Lin and Ji (2019)) as a baseline, with our mention feature extractor. The results are shown in Table 2 as “Multi-label”. 5.4 Metrics We follow prior work and use strict accuracy (Acc), macro F1 (MaF), and micro F1 (MiF) scores. Given instance xi , we denote the gold type set as Yi and the predicted type set Yˆi . The"
2020.acl-main.749,D19-1502,0,0.0416245,"to employing pre-trained language models like ELMo (Peters et al., 2018) to generate ever better representations (Lin and Ji, 2019). Our approach builds upon these developments and uses state-of-theart mention encoders. • Incorporating the hierarchy: Most prior works approach the hierarchical typing problem Researchers have proposed alternative FET formulations whose types are not formed in a type hierarchy, in particular Ultra-fine entity typing (Choi et al., 2018; Xiong et al., 2019; Onoe and Durrett, 2019), with a very large set of types derived from phrases mined from a corpus. FET in KB (Jin et al., 2019) labels mentions to types in a knowledge base with multiple relations, forming a type graph. Dai et al. (2019) augments the task with entity linking to KBs. 3 Problem Formulation We denote a mention as a tuple x = (w, l, r), where w = (w1, · · · , wn ) is the sentential context and the span [l : r] marks a mention of interest in sentence w. That is, the mention of interest is (wl, · · · , wr ). Given x, a hierarchical entity typing model outputs 8466 a set of types Y in the type ontology Y, i.e. Y ⊆ Y. Type hierarchies take the form of a forest, where each tree is rooted by a top-level superty"
2020.acl-main.749,D19-1641,0,0.281639,"archical-typing. the top level—these are similar to traditional NER types, e.g. /person; additionally, there are finer types that are subtypes of these top-level types, e.g. /person/artist or /person/doctor. Most prior work concerning fine-grained entity typing has approached the problem as a multilabel classification problem: given an entity mention together with its context, the classifier seeks to output a set of types, where each type is a node in the hierarchy. Approaches to FET include handcrafted sparse features to various neural architectures (Ren et al., 2016a; Shimaoka et al., 2017; Lin and Ji, 2019, inter alia, see section 2). Perhaps owing to the historical transition from “flat” NER types, there has been relatively little work in FET that exploits ontological tree structure, where type labels satisfy the hierarchical property: a subtype is valid only if its parent supertype is also valid. We propose a novel method that takes the explicit ontology structure into account, by a multi-level learning to rank approach that ranks the candidate types conditioned on the given entity mention. Intuitively, coarser types are easier whereas finer types are harder to classify: we capture this intui"
2020.acl-main.749,D15-1166,0,0.0402719,"Missing"
2020.acl-main.749,P18-1010,0,0.53594,"er person vehicle aircraft wea entity weapon L2 veh entity BBN Figure 2: Various type ontologies. Different levels of the types are shown in different shades, from L0 to L3. The ENTITY and OTHER special nodes are discussed in section 3. as multi-label classification, without using information in the hierarchical structure, but there are a few exceptions. Ren et al. (2016a) proposed an adaptive margin for learning-to-rank so that similar types have a smaller margin; Xu and Barbosa (2018) proposed hierarchical loss normalization that penalizes output that violates the hierarchical property; and Murty et al. (2018) proposed to learn a subtyping relation to constrain the type embeddings in the type space. In contrast to these approaches, our coarse-tofine decoding approach strictly guarantees that the output does not violate the hierarchical property, leading to better performance. HYENA (Yosef et al., 2012) applied ranking to sibling types in a type hierarchy, but the number of predicted positive types are trained separately with a meta-model, hence does not support neural end-to-end training. Coupled with a novel coarse-to-fine decoder that searches on the type hierarchy, our approach guarantees that p"
2020.acl-main.749,N19-1250,0,0.422427,"oka et al., 2017) or CNNs (Murty et al., 2018), with mention-to-context attention (Zhang et al., 2018), then to employing pre-trained language models like ELMo (Peters et al., 2018) to generate ever better representations (Lin and Ji, 2019). Our approach builds upon these developments and uses state-of-theart mention encoders. • Incorporating the hierarchy: Most prior works approach the hierarchical typing problem Researchers have proposed alternative FET formulations whose types are not formed in a type hierarchy, in particular Ultra-fine entity typing (Choi et al., 2018; Xiong et al., 2019; Onoe and Durrett, 2019), with a very large set of types derived from phrases mined from a corpus. FET in KB (Jin et al., 2019) labels mentions to types in a knowledge base with multiple relations, forming a type graph. Dai et al. (2019) augments the task with entity linking to KBs. 3 Problem Formulation We denote a mention as a tuple x = (w, l, r), where w = (w1, · · · , wn ) is the sentential context and the span [l : r] marks a mention of interest in sentence w. That is, the mention of interest is (wl, · · · , wr ). Given x, a hierarchical entity typing model outputs 8466 a set of types Y in the type ontology Y, i"
2020.acl-main.749,N18-1202,0,0.349586,"we focus on the sentence-level variant for best contrast to prior work. Progress in FET has focused primarily on: • Better mention representations: Starting from sparse hand-crafted binary features (Ling and Weld, 2012; Gillick et al., 2014), the community has moved to distributed representations (Yogatama et al., 2015), to pre-trained word embeddings with LSTMs (Ren et al., 2016a,b; Shimaoka et al., 2016; Abhishek et al., 2017; Shimaoka et al., 2017) or CNNs (Murty et al., 2018), with mention-to-context attention (Zhang et al., 2018), then to employing pre-trained language models like ELMo (Peters et al., 2018) to generate ever better representations (Lin and Ji, 2019). Our approach builds upon these developments and uses state-of-theart mention encoders. • Incorporating the hierarchy: Most prior works approach the hierarchical typing problem Researchers have proposed alternative FET formulations whose types are not formed in a type hierarchy, in particular Ultra-fine entity typing (Choi et al., 2018; Xiong et al., 2019; Onoe and Durrett, 2019), with a very large set of types derived from phrases mined from a corpus. FET in KB (Jin et al., 2019) labels mentions to types in a knowledge base with mult"
2020.acl-main.749,D16-1144,0,0.321119,"found at https://github.com/ ctongfei/hierarchical-typing. the top level—these are similar to traditional NER types, e.g. /person; additionally, there are finer types that are subtypes of these top-level types, e.g. /person/artist or /person/doctor. Most prior work concerning fine-grained entity typing has approached the problem as a multilabel classification problem: given an entity mention together with its context, the classifier seeks to output a set of types, where each type is a node in the hierarchy. Approaches to FET include handcrafted sparse features to various neural architectures (Ren et al., 2016a; Shimaoka et al., 2017; Lin and Ji, 2019, inter alia, see section 2). Perhaps owing to the historical transition from “flat” NER types, there has been relatively little work in FET that exploits ontological tree structure, where type labels satisfy the hierarchical property: a subtype is valid only if its parent supertype is also valid. We propose a novel method that takes the explicit ontology structure into account, by a multi-level learning to rank approach that ranks the candidate types conditioned on the given entity mention. Intuitively, coarser types are easier whereas finer types are"
2020.acl-main.749,W16-1313,0,0.0420908,"eated the commonly used FIGER and OntoNotes datasets for FET. While researchers have considered the benefits of document-level (Zhang et al., 2018), and corpuslevel (Yaghoobzadeh and Sch¨utze, 2015) context, here we focus on the sentence-level variant for best contrast to prior work. Progress in FET has focused primarily on: • Better mention representations: Starting from sparse hand-crafted binary features (Ling and Weld, 2012; Gillick et al., 2014), the community has moved to distributed representations (Yogatama et al., 2015), to pre-trained word embeddings with LSTMs (Ren et al., 2016a,b; Shimaoka et al., 2016; Abhishek et al., 2017; Shimaoka et al., 2017) or CNNs (Murty et al., 2018), with mention-to-context attention (Zhang et al., 2018), then to employing pre-trained language models like ELMo (Peters et al., 2018) to generate ever better representations (Lin and Ji, 2019). Our approach builds upon these developments and uses state-of-theart mention encoders. • Incorporating the hierarchy: Most prior works approach the hierarchical typing problem Researchers have proposed alternative FET formulations whose types are not formed in a type hierarchy, in particular Ultra-fine entity typing (Choi et a"
2020.acl-main.749,E17-1119,0,0.441698,"thub.com/ ctongfei/hierarchical-typing. the top level—these are similar to traditional NER types, e.g. /person; additionally, there are finer types that are subtypes of these top-level types, e.g. /person/artist or /person/doctor. Most prior work concerning fine-grained entity typing has approached the problem as a multilabel classification problem: given an entity mention together with its context, the classifier seeks to output a set of types, where each type is a node in the hierarchy. Approaches to FET include handcrafted sparse features to various neural architectures (Ren et al., 2016a; Shimaoka et al., 2017; Lin and Ji, 2019, inter alia, see section 2). Perhaps owing to the historical transition from “flat” NER types, there has been relatively little work in FET that exploits ontological tree structure, where type labels satisfy the hierarchical property: a subtype is valid only if its parent supertype is also valid. We propose a novel method that takes the explicit ontology structure into account, by a multi-level learning to rank approach that ranks the candidate types conditioned on the given entity mention. Intuitively, coarser types are easier whereas finer types are harder to classify: we"
2020.acl-main.749,N19-1084,0,0.279046,"et al., 2017; Shimaoka et al., 2017) or CNNs (Murty et al., 2018), with mention-to-context attention (Zhang et al., 2018), then to employing pre-trained language models like ELMo (Peters et al., 2018) to generate ever better representations (Lin and Ji, 2019). Our approach builds upon these developments and uses state-of-theart mention encoders. • Incorporating the hierarchy: Most prior works approach the hierarchical typing problem Researchers have proposed alternative FET formulations whose types are not formed in a type hierarchy, in particular Ultra-fine entity typing (Choi et al., 2018; Xiong et al., 2019; Onoe and Durrett, 2019), with a very large set of types derived from phrases mined from a corpus. FET in KB (Jin et al., 2019) labels mentions to types in a knowledge base with multiple relations, forming a type graph. Dai et al. (2019) augments the task with entity linking to KBs. 3 Problem Formulation We denote a mention as a tuple x = (w, l, r), where w = (w1, · · · , wn ) is the sentential context and the span [l : r] marks a mention of interest in sentence w. That is, the mention of interest is (wl, · · · , wr ). Given x, a hierarchical entity typing model outputs 8466 a set of types Y"
2020.acl-main.749,N18-1002,0,0.0943234,"her ammunition other molotov cocktail other airplane AIDA other product organization educational other bomb bullets bullets L3 other person vehicle aircraft wea entity weapon L2 veh entity BBN Figure 2: Various type ontologies. Different levels of the types are shown in different shades, from L0 to L3. The ENTITY and OTHER special nodes are discussed in section 3. as multi-label classification, without using information in the hierarchical structure, but there are a few exceptions. Ren et al. (2016a) proposed an adaptive margin for learning-to-rank so that similar types have a smaller margin; Xu and Barbosa (2018) proposed hierarchical loss normalization that penalizes output that violates the hierarchical property; and Murty et al. (2018) proposed to learn a subtyping relation to constrain the type embeddings in the type space. In contrast to these approaches, our coarse-tofine decoding approach strictly guarantees that the output does not violate the hierarchical property, leading to better performance. HYENA (Yosef et al., 2012) applied ranking to sibling types in a type hierarchy, but the number of predicted positive types are trained separately with a meta-model, hence does not support neural end-"
2020.acl-main.749,D15-1083,0,0.020672,"h probability pD to the ELMo vectors. Our mention encoder largely follows Lin and Ji (2019). First a mention representation is derived using the representations of the words in the mention. We apply a max pooling layer atop the mention after a linear transformation:4 m = MaxPool(Twl, · · · , Twr ) ∈ Rdw . (1) Then we employ mention-to-context attention first described in Zhang et al. (2018) and later employed by Lin and Ji (2019): a context vector c is generated by attending the sentence with a query vector derived from the mention vector m. We use the multiplicative attention of Luong et al. (2015): ai ∝ exp(mT Qwi ) c= N X i=1 ai w i ∈ R d w (2) (3) The final representation for an entity mention is generated via concatenation of the mention and context vector: [m ; c] ∈ R2dw . 4.2 Type Scorer We learn a type embedding y ∈ Rdt for each type y ∈ Y. To score an instance with representation [m ; c], we pass it through a 2-layer feed-forward network that maps into the same space as the type space Rdt , with tanh as the nonlinearity. The final 3 Lin and Ji (2019) found that ELMo performs better than BERT (Devlin et al., 2019) for FET. Our internal experiments also confirm this finding. We hy"
2020.acl-main.749,P15-2048,0,0.0685813,"ons, notably starting with Ling and Weld (2012) and Gillick et al. (2014), where they created the commonly used FIGER and OntoNotes datasets for FET. While researchers have considered the benefits of document-level (Zhang et al., 2018), and corpuslevel (Yaghoobzadeh and Sch¨utze, 2015) context, here we focus on the sentence-level variant for best contrast to prior work. Progress in FET has focused primarily on: • Better mention representations: Starting from sparse hand-crafted binary features (Ling and Weld, 2012; Gillick et al., 2014), the community has moved to distributed representations (Yogatama et al., 2015), to pre-trained word embeddings with LSTMs (Ren et al., 2016a,b; Shimaoka et al., 2016; Abhishek et al., 2017; Shimaoka et al., 2017) or CNNs (Murty et al., 2018), with mention-to-context attention (Zhang et al., 2018), then to employing pre-trained language models like ELMo (Peters et al., 2018) to generate ever better representations (Lin and Ji, 2019). Our approach builds upon these developments and uses state-of-theart mention encoders. • Incorporating the hierarchy: Most prior works approach the hierarchical typing problem Researchers have proposed alternative FET formulations whose type"
2020.acl-main.749,C12-2133,0,0.415931,"2) applied ranking to sibling types in a type hierarchy, but the number of predicted positive types are trained separately with a meta-model, hence does not support neural end-to-end training. Coupled with a novel coarse-to-fine decoder that searches on the type hierarchy, our approach guarantees that predictions do not violate the hierarchical property, and achieves state-of-the-art results according to multiple measures across various commonly used datasets. 2 Related Work FET is usually studied as allowing for sentencelevel context in making predictions, notably starting with Ling and Weld (2012) and Gillick et al. (2014), where they created the commonly used FIGER and OntoNotes datasets for FET. While researchers have considered the benefits of document-level (Zhang et al., 2018), and corpuslevel (Yaghoobzadeh and Sch¨utze, 2015) context, here we focus on the sentence-level variant for best contrast to prior work. Progress in FET has focused primarily on: • Better mention representations: Starting from sparse hand-crafted binary features (Ling and Weld, 2012; Gillick et al., 2014), the community has moved to distributed representations (Yogatama et al., 2015), to pre-trained word emb"
2020.acl-main.749,S18-2022,1,0.451699,"other molotov cocktail other airplane AIDA other product organization educational other bomb bullets bullets L3 other person vehicle aircraft wea entity weapon L2 veh entity BBN Figure 2: Various type ontologies. Different levels of the types are shown in different shades, from L0 to L3. The ENTITY and OTHER special nodes are discussed in section 3. as multi-label classification, without using information in the hierarchical structure, but there are a few exceptions. Ren et al. (2016a) proposed an adaptive margin for learning-to-rank so that similar types have a smaller margin; Xu and Barbosa (2018) proposed hierarchical loss normalization that penalizes output that violates the hierarchical property; and Murty et al. (2018) proposed to learn a subtyping relation to constrain the type embeddings in the type space. In contrast to these approaches, our coarse-tofine decoding approach strictly guarantees that the output does not violate the hierarchical property, leading to better performance. HYENA (Yosef et al., 2012) applied ranking to sibling types in a type hierarchy, but the number of predicted positive types are trained separately with a meta-model, hence does not support neural end-"
2020.acl-main.774,D18-1500,0,0.290334,"Missing"
2020.acl-main.774,D15-1189,0,0.0294371,"tion of plausible human judgments” as plausible human judgments cause inherent disagreements. Our concern is different as we are interested in the uncertain and probabilistic nature of NLI. We are the first to propose a method for direct elicitation of subjective probability judgments on NLI pairs and direct prediction of these scalars, as opposed to reducing to categorical classification. Recent work have also modeled the uncertainty of other semantic phenomena as direct scalar regression (and collected scalar versions of data for them) instead of categorical classification, e.g. factuality (Lee et al., 2015; Stanovsky et al., 2017; Rudinger et al., 2018), and semantic proto-roles (Teichert et al., 2017). Plausiblity tasks such as COPA (Roemmele et al., 2011) and ROCStories (Mostafazadeh et al., 2016) ask models to choose the most probable examples given a context, capturing relative uncertainty between examples, but do not force a model to predict the probability of ℎ given ?. Li et al. (2019) viewed the plausibility task of COPA as a learning to rank problem, where the model is trained to assign the highest scalar score to the most plausible alternative given context. Our work can be viewed as"
2020.acl-main.774,P19-1475,1,0.880089,"Missing"
2020.acl-main.774,N16-1098,0,0.0113898,"I. We are the first to propose a method for direct elicitation of subjective probability judgments on NLI pairs and direct prediction of these scalars, as opposed to reducing to categorical classification. Recent work have also modeled the uncertainty of other semantic phenomena as direct scalar regression (and collected scalar versions of data for them) instead of categorical classification, e.g. factuality (Lee et al., 2015; Stanovsky et al., 2017; Rudinger et al., 2018), and semantic proto-roles (Teichert et al., 2017). Plausiblity tasks such as COPA (Roemmele et al., 2011) and ROCStories (Mostafazadeh et al., 2016) ask models to choose the most probable examples given a context, capturing relative uncertainty between examples, but do not force a model to predict the probability of ℎ given ?. Li et al. (2019) viewed the plausibility task of COPA as a learning to rank problem, where the model is trained to assign the highest scalar score to the most plausible alternative given context. Our work can be viewed as a variant to this, with the score being an explicit human probability judgment instead. Linguists such as van Eijck and Lappin (2014), Goodman and Lassiter (2015), Cooper et al. (2015) and Bernardy"
2020.acl-main.774,P16-1204,0,0.0675445,"LI. We use this mapping to pre-train a regression model on the SNLI training examples not included in u-SNLI. We also fine-tune the model on uSNLI’s training set. Table 5 reports the results evaluated on u-SNLI’s dev and test sets. The model trained on the roughly 500? mapped SNLI examples, performs much worse than when trained on just about 55? u-SNLI examples. When we pretrain the model on the mapped SNLI and fine-tune on u-SNLI, results noticeably improve. This improvement is akin to the Phang et al. (2018)’s finding that many NLI datasets cover informative signal 11 This is similar to how Pavlick and Callison-Burch (2016) pre-train on SNLI, then fine-tune the model using their AddOne pairs. 12 That the score of ENT &gt; score of NEU &gt; score of CON . 13 ? : { ENT ↦→ 0.9272; NEU ↦→ 0.4250; CON ↦→ 0.0209}. Related Work The probabilistic nature and the uncertainty of NLI has been considered from a variety of perspectives. Glickman et al. (2005) modified the task to explicitly include the probabilistic aspect of NLI, stating that “? probabilistically entails ℎ ... if ? increases the likelihood of ℎ being true,” while Lai and Hockenmaier (2017) noted how predicting the conditional probability of one phrase given anothe"
2020.acl-main.774,Q19-1043,0,0.0756004,"or work has elicited ordinal annotations (e.g. Likert scale) reflecting likelihood judgments (Pavlick and Callison-Burch, 2016; Zhang et al., 2017), but then collapsed the annotations into coarse categorical labels for modeling. Vuli´c et al. (2017) proposed graded lexical entailment, which is similar to our idea but applied to lexical-level inference, asking “to what degree ? is a type of ?.” Additionally, Lalor et al. (2016, 2018) tried capturing the uncertainty of each inference pair by item response theory (IRT), showing fine-grained 8775 differences in discriminative power in each label. Pavlick and Kwiatkowski (2019) recently argued that models should “explicitly capture the full distribution of plausible human judgments” as plausible human judgments cause inherent disagreements. Our concern is different as we are interested in the uncertain and probabilistic nature of NLI. We are the first to propose a method for direct elicitation of subjective probability judgments on NLI pairs and direct prediction of these scalars, as opposed to reducing to categorical classification. Recent work have also modeled the uncertainty of other semantic phenomena as direct scalar regression (and collected scalar versions o"
2020.acl-main.774,S18-2023,1,0.883712,"Missing"
2020.acl-main.774,Q17-1027,1,0.873571,"Missing"
2020.acl-main.774,N18-1067,1,0.89602,"Missing"
2020.acl-main.774,P18-1020,1,0.894744,"Missing"
2020.acl-main.774,P17-2056,0,0.019011,"human judgments” as plausible human judgments cause inherent disagreements. Our concern is different as we are interested in the uncertain and probabilistic nature of NLI. We are the first to propose a method for direct elicitation of subjective probability judgments on NLI pairs and direct prediction of these scalars, as opposed to reducing to categorical classification. Recent work have also modeled the uncertainty of other semantic phenomena as direct scalar regression (and collected scalar versions of data for them) instead of categorical classification, e.g. factuality (Lee et al., 2015; Stanovsky et al., 2017; Rudinger et al., 2018), and semantic proto-roles (Teichert et al., 2017). Plausiblity tasks such as COPA (Roemmele et al., 2011) and ROCStories (Mostafazadeh et al., 2016) ask models to choose the most probable examples given a context, capturing relative uncertainty between examples, but do not force a model to predict the probability of ℎ given ?. Li et al. (2019) viewed the plausibility task of COPA as a learning to rank problem, where the model is trained to assign the highest scalar score to the most plausible alternative given context. Our work can be viewed as a variant to this, with"
2020.acl-main.774,L18-1239,0,0.0610431,"Missing"
2020.acl-main.774,J17-4004,0,0.0545373,"Missing"
2020.acl-main.774,N18-1101,0,0.0604904,"l output ?ˆ . Owing to the concerns raised with annotation artifacts in SNLI (Gururangan et al., 2018; Tsuchiya, 2018; Poliak et al., 2018), we include a hypothesis-only baseline.8 Metrics We compute Pearson correlation (?), the Spearman rank correlation (?), and the mean square error (MSE) between y and ?ˆ as the metrics to measure the to performance of UNLI models. Pearson ? measures the linear correlation between the gold probability assessments and model’s output; Spearman ? measures the ability of the model ranking the premise-hypothesis pairs with 6 The neural architecture for MultiNLI (Williams et al., 2018) in Devlin et al. (2019). 7 No significant difference is observed with an ? loss. 2 8 See Appendix D for additional training details. ? ? MSE Full-model Dev Test Dev Test 0.3759 0.3853 0.1086 0.4120 0.4165 0.1055 0.6383 0.6408 0.0751 0.6271 0.6346 0.0777 Table 4: Metrics for training on u-SNLI. Human Performance We elicit additional annotations on u-SNLI dev set to establish a randomly sampled human performance. We use the same annotators as before but ensure each annotator has not previously seen the pair they are annotating. We average the scores from three-way redundant elicitation,10 yield"
2020.codi-1.10,N19-1423,0,0.0127169,". Prior work has proposed pipeline system of the subtasks (Ji and Grishman, 2008; Li et al., 2013; Yang and Mitchell, 2016, inter alia), or as a joint model over the three tasks (Nguyen and Nguyen, 2019; Lin et al., 2020, inter alia). Our work could be seen as a version of argument role prediction, but which operates beyond sentence boundaries. 4 Approach Argument and trigger representation We compute a fixed-length vector with dimension ? for each argument and trigger span as their representations. To compute this, we first pass the document through a pre-trained contextualizing model (BERT (Devlin et al., 2019) here).5 We split documents into sentences and feed each sentence to BERT for encoding. Each token ? ? might be split into more than 1 subword units—in this case we take the average of these subword representations so that each token ? ? has 1 vector representation w? ∈ R?tok , following Zhang et al. (2019). For an argument span ? = (? ? , · · · , ? ? ), we follow Lee et al. (2017) to generate a span embedding.6 The span embedding m for mention span ? comprises of three parts, the representation of its left boundary, its right boundary, and a learned pooling over the tokens in the span. This l"
2020.codi-1.10,P17-1045,0,0.0286705,"attention query vector q ∈ R?tok , and computes the weighted sum of all tokens with respect to the attention scores derived from q: Frame-based SLU In dialogue systems, semantic frame based spoken language understanding (SLU) is one of the most commonly applied SLU technologies for human-computer interaction. Such systems often output an interpretation of dialogues represented as intents and slots (Wang et al., 2011). C ¸ elikyilmaz et al. (2014) and Bapna et al. (2017) proposed models to resolve references to slots in the dialogue, tracking conversation states across multiple dialogue turns. Dhingra et al. (2017) augmented such methods with external knowledge bases (KBs) to create a multi-turn dialogue agent which helps users search KBs. Chen et al. (2019) proposed joint models over potential slots in dialogue to output which contextual slots should be carried over to the most recent utterance. Our approach is inspired by this work, by drawing analogies between concepts in SLU (intents / slots) and those in IE (events / arguments) (see Table 1). exp qT w? ? ? = X? ; Tw exp q ? ?=? 4 c= ? X ? ? · w? , (1) ?=? For example, in the ACE 2005 dataset, ?( ATTACK) = { ATTACKER, TARGET, INSTRUMENT, TIME, PLACE"
2020.codi-1.10,2020.acl-main.718,1,0.753342,"Missing"
2020.codi-1.10,P00-1065,0,0.306187,"ments. The input to this Transformer is no longer tokens but spans: given the Transformer output of each span, a classification loss is utilized to perform argument role classification. We demonstrate this leads to state-of-theart performance on the RAMS argument linking dataset introduced by Ebner et al. (2020),3 showing the benefits of joint modeling when linking arguments to roles of events. Given an event recognized in text, we are concerned with finding its associated arguments. Significant work has focused at the level of single sentence contexts, such as in semantic role labeling (SRL; Gildea and Jurafsky, 2000; He et al., 2017; Ouchi et al., 2018, inter alia). Unfortunately even perfect performance in SRL will be limited by the existence of arguments outside the sentence boundary, leading to prior work (Das et al., 2010; Silberer and Frank, 2012; Ebner et al., 2020) on an alternative paradigm variously called implicit role resolution or argument linking, where an event trigger (e.g. “attack”) evokes a set of roles (e.g. ATTACKER , TARGET) to be filled, and they are linked to explicit argument mentions found in text. In argument linking, possible candidate arguments are first detected, then linked t"
2020.codi-1.10,P17-1044,0,0.0291769,"ansformer is no longer tokens but spans: given the Transformer output of each span, a classification loss is utilized to perform argument role classification. We demonstrate this leads to state-of-theart performance on the RAMS argument linking dataset introduced by Ebner et al. (2020),3 showing the benefits of joint modeling when linking arguments to roles of events. Given an event recognized in text, we are concerned with finding its associated arguments. Significant work has focused at the level of single sentence contexts, such as in semantic role labeling (SRL; Gildea and Jurafsky, 2000; He et al., 2017; Ouchi et al., 2018, inter alia). Unfortunately even perfect performance in SRL will be limited by the existence of arguments outside the sentence boundary, leading to prior work (Das et al., 2010; Silberer and Frank, 2012; Ebner et al., 2020) on an alternative paradigm variously called implicit role resolution or argument linking, where an event trigger (e.g. “attack”) evokes a set of roles (e.g. ATTACKER , TARGET) to be filled, and they are linked to explicit argument mentions found in text. In argument linking, possible candidate arguments are first detected, then linked to specific roles"
2020.codi-1.10,W17-5514,0,0.0560647,"Missing"
2020.codi-1.10,D14-1223,0,0.0603661,"Missing"
2020.codi-1.10,S10-1059,0,0.0232463,"logy can be formulated as a set of event types T , where each type ? ∈ T is associated with a set of roles ?(?),4 while other roles are nonpermissible. We denote the union of all roles for all event types, plus an empty ? role (a dummy role denoting an argument is not part of the event [ structure) as R = ?(?) ∪ {?}. ? ∈T Implicit role resolution Palmer et al. (1986) treated unfilled semantic roles as special cases of anaphora and coreference resolution. Starting from the SemEval 2010 Task 10: Linking Roles (Ruppenhofer et al., 2010), there have been more recent modeling efforts on this task. Chen et al. (2010) approached this with their SRL system S E MAFOR (Das et al., 2010), casting the task as extended SRL by admitting constituents (potential arguments) from context larger than sentence boundaries. Silberer and Frank (2012) considered the problem as an anaphora resolution task within the discourse context. Ebner et al. (2020) similarly considered the task as related to anaphora resolution, and introduced a new dataset, RAMS, for exploring non-local argument linking. See O’Gorman (2019) and Ebner et al. (2020) for further background. Event extraction In event extraction there are historically thr"
2020.codi-1.10,W19-4111,1,0.906125,"BOOK ATTACK Slot key Role type NAME , AUTHOR ATTACKER , TARGET Slot value 1984, George Orwell Argument Russia, Ukraine Table 1: Mapping between terminologies in intent slot resolution and event argument linking, with examples. Introduction NAME , AUTHOR , PUBLISHER , etc.). Even more than in event argument linking, in dialogue systems the sentence-level (utterance-level) context often fails to contain all salient arguments (slots): slots from previous rounds of dialogue may often be relevant to the current intent.2 We propose a novel model for joint modeling of potential arguments inspired by Chen et al. (2019) for slot-filling in dialogue systems, which proposed to jointly predict spans that are relevant to the intent of the current round of dialogue. Over detected arguments, a Transformer (Vaswani et al., 2017) encoder is placed upon the event trigger and potential arguments to jointly learn the relations between the event trigger and its arguments. The input to this Transformer is no longer tokens but spans: given the Transformer output of each span, a classification loss is utilized to perform argument role classification. We demonstrate this leads to state-of-theart performance on the RAMS argu"
2020.codi-1.10,N10-1138,0,0.263679,"-theart performance on the RAMS argument linking dataset introduced by Ebner et al. (2020),3 showing the benefits of joint modeling when linking arguments to roles of events. Given an event recognized in text, we are concerned with finding its associated arguments. Significant work has focused at the level of single sentence contexts, such as in semantic role labeling (SRL; Gildea and Jurafsky, 2000; He et al., 2017; Ouchi et al., 2018, inter alia). Unfortunately even perfect performance in SRL will be limited by the existence of arguments outside the sentence boundary, leading to prior work (Das et al., 2010; Silberer and Frank, 2012; Ebner et al., 2020) on an alternative paradigm variously called implicit role resolution or argument linking, where an event trigger (e.g. “attack”) evokes a set of roles (e.g. ATTACKER , TARGET) to be filled, and they are linked to explicit argument mentions found in text. In argument linking, possible candidate arguments are first detected, then linked to specific roles of detected events. This bears similarity to coreference resolution, where document-level context can be aptly utilized. For an example, see Figure 1. This formulation is similar to the resolution"
2020.codi-1.10,P08-1030,0,0.0494314,"ext. Ebner et al. (2020) similarly considered the task as related to anaphora resolution, and introduced a new dataset, RAMS, for exploring non-local argument linking. See O’Gorman (2019) and Ebner et al. (2020) for further background. Event extraction In event extraction there are historically three subtasks: detecting event triggers, detecting entity mentions, and then argument role prediction, where relations between mentions and triggers are predicted in accordance to the event type’s predefined set of roles under a closed ontology. Prior work has proposed pipeline system of the subtasks (Ji and Grishman, 2008; Li et al., 2013; Yang and Mitchell, 2016, inter alia), or as a joint model over the three tasks (Nguyen and Nguyen, 2019; Lin et al., 2020, inter alia). Our work could be seen as a version of argument role prediction, but which operates beyond sentence boundaries. 4 Approach Argument and trigger representation We compute a fixed-length vector with dimension ? for each argument and trigger span as their representations. To compute this, we first pass the document through a pre-trained contextualizing model (BERT (Devlin et al., 2019) here).5 We split documents into sentences and feed each sen"
2020.codi-1.10,D17-1018,0,0.167848,"tation We compute a fixed-length vector with dimension ? for each argument and trigger span as their representations. To compute this, we first pass the document through a pre-trained contextualizing model (BERT (Devlin et al., 2019) here).5 We split documents into sentences and feed each sentence to BERT for encoding. Each token ? ? might be split into more than 1 subword units—in this case we take the average of these subword representations so that each token ? ? has 1 vector representation w? ∈ R?tok , following Zhang et al. (2019). For an argument span ? = (? ? , · · · , ? ? ), we follow Lee et al. (2017) to generate a span embedding.6 The span embedding m for mention span ? comprises of three parts, the representation of its left boundary, its right boundary, and a learned pooling over the tokens in the span. This learned pooling utilized a global attention query vector q ∈ R?tok , and computes the weighted sum of all tokens with respect to the attention scores derived from q: Frame-based SLU In dialogue systems, semantic frame based spoken language understanding (SLU) is one of the most commonly applied SLU technologies for human-computer interaction. Such systems often output an interpretat"
2020.codi-1.10,P13-1008,0,0.0336801,") similarly considered the task as related to anaphora resolution, and introduced a new dataset, RAMS, for exploring non-local argument linking. See O’Gorman (2019) and Ebner et al. (2020) for further background. Event extraction In event extraction there are historically three subtasks: detecting event triggers, detecting entity mentions, and then argument role prediction, where relations between mentions and triggers are predicted in accordance to the event type’s predefined set of roles under a closed ontology. Prior work has proposed pipeline system of the subtasks (Ji and Grishman, 2008; Li et al., 2013; Yang and Mitchell, 2016, inter alia), or as a joint model over the three tasks (Nguyen and Nguyen, 2019; Lin et al., 2020, inter alia). Our work could be seen as a version of argument role prediction, but which operates beyond sentence boundaries. 4 Approach Argument and trigger representation We compute a fixed-length vector with dimension ? for each argument and trigger span as their representations. To compute this, we first pass the document through a pre-trained contextualizing model (BERT (Devlin et al., 2019) here).5 We split documents into sentences and feed each sentence to BERT for"
2020.codi-1.10,2020.acl-main.713,0,0.460718,"posed to jointly predict spans that are relevant to the intent of the current round of dialogue. Over detected arguments, a Transformer (Vaswani et al., 2017) encoder is placed upon the event trigger and potential arguments to jointly learn the relations between the event trigger and its arguments. The input to this Transformer is no longer tokens but spans: given the Transformer output of each span, a classification loss is utilized to perform argument role classification. We demonstrate this leads to state-of-theart performance on the RAMS argument linking dataset introduced by Ebner et al. (2020),3 showing the benefits of joint modeling when linking arguments to roles of events. Given an event recognized in text, we are concerned with finding its associated arguments. Significant work has focused at the level of single sentence contexts, such as in semantic role labeling (SRL; Gildea and Jurafsky, 2000; He et al., 2017; Ouchi et al., 2018, inter alia). Unfortunately even perfect performance in SRL will be limited by the existence of arguments outside the sentence boundary, leading to prior work (Das et al., 2010; Silberer and Frank, 2012; Ebner et al., 2020) on an alternative paradigm"
2020.codi-1.10,D18-1191,0,0.0190735,"onger tokens but spans: given the Transformer output of each span, a classification loss is utilized to perform argument role classification. We demonstrate this leads to state-of-theart performance on the RAMS argument linking dataset introduced by Ebner et al. (2020),3 showing the benefits of joint modeling when linking arguments to roles of events. Given an event recognized in text, we are concerned with finding its associated arguments. Significant work has focused at the level of single sentence contexts, such as in semantic role labeling (SRL; Gildea and Jurafsky, 2000; He et al., 2017; Ouchi et al., 2018, inter alia). Unfortunately even perfect performance in SRL will be limited by the existence of arguments outside the sentence boundary, leading to prior work (Das et al., 2010; Silberer and Frank, 2012; Ebner et al., 2020) on an alternative paradigm variously called implicit role resolution or argument linking, where an event trigger (e.g. “attack”) evokes a set of roles (e.g. ATTACKER , TARGET) to be filled, and they are linked to explicit argument mentions found in text. In argument linking, possible candidate arguments are first detected, then linked to specific roles of detected events."
2020.codi-1.10,P86-1004,0,0.552585,"? [?? : ? ? ] ∈ ? where ?? and ? ? demarcates the left and right boundary (both inclusive), and a event trigger span ? = ? [? ? : ? ? ], an argument linking model predicts the role (or absence) of each mention with respect to the event. An event ontology can be formulated as a set of event types T , where each type ? ∈ T is associated with a set of roles ?(?),4 while other roles are nonpermissible. We denote the union of all roles for all event types, plus an empty ? role (a dummy role denoting an argument is not part of the event [ structure) as R = ?(?) ∪ {?}. ? ∈T Implicit role resolution Palmer et al. (1986) treated unfilled semantic roles as special cases of anaphora and coreference resolution. Starting from the SemEval 2010 Task 10: Linking Roles (Ruppenhofer et al., 2010), there have been more recent modeling efforts on this task. Chen et al. (2010) approached this with their SRL system S E MAFOR (Das et al., 2010), casting the task as extended SRL by admitting constituents (potential arguments) from context larger than sentence boundaries. Silberer and Frank (2012) considered the problem as an anaphora resolution task within the discourse context. Ebner et al. (2020) similarly considered the"
2020.codi-1.10,S10-1008,0,0.0399088,"redicts the role (or absence) of each mention with respect to the event. An event ontology can be formulated as a set of event types T , where each type ? ∈ T is associated with a set of roles ?(?),4 while other roles are nonpermissible. We denote the union of all roles for all event types, plus an empty ? role (a dummy role denoting an argument is not part of the event [ structure) as R = ?(?) ∪ {?}. ? ∈T Implicit role resolution Palmer et al. (1986) treated unfilled semantic roles as special cases of anaphora and coreference resolution. Starting from the SemEval 2010 Task 10: Linking Roles (Ruppenhofer et al., 2010), there have been more recent modeling efforts on this task. Chen et al. (2010) approached this with their SRL system S E MAFOR (Das et al., 2010), casting the task as extended SRL by admitting constituents (potential arguments) from context larger than sentence boundaries. Silberer and Frank (2012) considered the problem as an anaphora resolution task within the discourse context. Ebner et al. (2020) similarly considered the task as related to anaphora resolution, and introduced a new dataset, RAMS, for exploring non-local argument linking. See O’Gorman (2019) and Ebner et al. (2020) for furt"
2020.codi-1.10,S12-1001,0,0.151231,"e on the RAMS argument linking dataset introduced by Ebner et al. (2020),3 showing the benefits of joint modeling when linking arguments to roles of events. Given an event recognized in text, we are concerned with finding its associated arguments. Significant work has focused at the level of single sentence contexts, such as in semantic role labeling (SRL; Gildea and Jurafsky, 2000; He et al., 2017; Ouchi et al., 2018, inter alia). Unfortunately even perfect performance in SRL will be limited by the existence of arguments outside the sentence boundary, leading to prior work (Das et al., 2010; Silberer and Frank, 2012; Ebner et al., 2020) on an alternative paradigm variously called implicit role resolution or argument linking, where an event trigger (e.g. “attack”) evokes a set of roles (e.g. ATTACKER , TARGET) to be filled, and they are linked to explicit argument mentions found in text. In argument linking, possible candidate arguments are first detected, then linked to specific roles of detected events. This bears similarity to coreference resolution, where document-level context can be aptly utilized. For an example, see Figure 1. This formulation is similar to the resolution of referring expressions i"
2020.codi-1.10,N16-1033,0,0.0222262,"dered the task as related to anaphora resolution, and introduced a new dataset, RAMS, for exploring non-local argument linking. See O’Gorman (2019) and Ebner et al. (2020) for further background. Event extraction In event extraction there are historically three subtasks: detecting event triggers, detecting entity mentions, and then argument role prediction, where relations between mentions and triggers are predicted in accordance to the event type’s predefined set of roles under a closed ontology. Prior work has proposed pipeline system of the subtasks (Ji and Grishman, 2008; Li et al., 2013; Yang and Mitchell, 2016, inter alia), or as a joint model over the three tasks (Nguyen and Nguyen, 2019; Lin et al., 2020, inter alia). Our work could be seen as a version of argument role prediction, but which operates beyond sentence boundaries. 4 Approach Argument and trigger representation We compute a fixed-length vector with dimension ? for each argument and trigger span as their representations. To compute this, we first pass the document through a pre-trained contextualizing model (BERT (Devlin et al., 2019) here).5 We split documents into sentences and feed each sentence to BERT for encoding. Each token ? ?"
2020.codi-1.10,P19-1009,1,0.848746,"Missing"
2020.emnlp-main.421,Q17-1010,0,0.0192561,"ingual machine translation (MT) (Ha et al., 2016) and domain adaptation (Chu and Dabre, 2019) in which a language code (e.g. en, de) is prepended to the target to guide generation. Our method for encoding sentence diversity is closely related to MT work by Shu et al. (2019), who condition generation on prefixed sentence codes. They improve the syntactic diversity of sampled translations using codes produced from improved semantic hashing (Kaiser and Bengio, 2018) with a TreeLSTMbased autoencoder. Their experiments with semantic coding via clustering of BERT (Devlin et al., 2019) and FastText (Bojanowski et al., 2017) embeddings lead to negligible or negative effects. Outside of MT, Keskar et al. (2019) in a similar vein condition on manually categorized “control codes” that specify style and content, and Mallinson and Lapata (2019) condition on annotated syntactic or lexical change markers that can be learnt from data. We refer readers to Ippolito et al. (2019) for an overview of diverse decoding methods. Few to our knowledge explicitly and effectively encode open-domain semantic diversity. Text-based causal knowledge acquisition is a well-studied challenge in NLP (Radinsky et al., 2012). Recent efforts h"
2020.emnlp-main.421,P19-1470,0,0.0204067,"Keskar et al. (2019) in a similar vein condition on manually categorized “control codes” that specify style and content, and Mallinson and Lapata (2019) condition on annotated syntactic or lexical change markers that can be learnt from data. We refer readers to Ippolito et al. (2019) for an overview of diverse decoding methods. Few to our knowledge explicitly and effectively encode open-domain semantic diversity. Text-based causal knowledge acquisition is a well-studied challenge in NLP (Radinsky et al., 2012). Recent efforts have investigated open ended causal generation using neural models (Bosselut et al., 2019; Li et al., 2020). The latter train a conditional generation model to propose cause or effect statements for a given proposition. The model is trained on the co-released corpus CausalBank, which comprises causal statements harvested from English Common Crawl (Buck et al., 2014). Applications of LSH (Indyk and Motwani, 1998; Charikar, 2002) in NLP began with Ravichandran et al. (2005) who demonstrated its use in fast lexical similarity comparison; later, Van Durme and Lall (2010) showed such hashing could be performed online. More similar to our use case, Petrovi´c et al. (2010) binned tweets"
2020.emnlp-main.421,buck-etal-2014-n,0,0.0273844,"Missing"
2020.emnlp-main.421,S17-2001,0,0.020904,"cosine similarity approximates the semantic textual similarity (STS) of the underlying sentences. We select this single sentence encoder over other popular encoders, e.g. BERT, which best encode concatenations of pairs of sentences and therefore do not produce individual embeddings that encode semantic difference retrievable under vector similarity metrics (Reimers and Gurevych, 2019; Shu et al., 2019). The cosine similarity of embeddings from SRoBERTa-L, the instance of SBERT that we use as our COD 3 S encoder, has a Spearman ρ correlation of .863 with human STS judgements from STSbenchmark (Cer et al., 2017).1 We provide a list of cosine/STS correlations using other models in Appendix E.2 Discretization via LSH Locality-sensitive hashing (LSH; Indyk and Motwani, 1998) maps highdimensional vectors into low-dimensional sketches for quick and accurate similarity comparison under measures such as cosine or Euclidean distance. We use the popular variant by Charikar (2002), which computes a discrete b-bit signature LSH(~v) = [LSH1 (~v), . . . LSHb (~v)]. Appendix A provides an overview of this approach. The Hamming distance between two LSH signatures approximates the cosine distance of the underyling v"
2020.emnlp-main.421,N19-1423,0,0.0181328,"piration from recent work in multilingual machine translation (MT) (Ha et al., 2016) and domain adaptation (Chu and Dabre, 2019) in which a language code (e.g. en, de) is prepended to the target to guide generation. Our method for encoding sentence diversity is closely related to MT work by Shu et al. (2019), who condition generation on prefixed sentence codes. They improve the syntactic diversity of sampled translations using codes produced from improved semantic hashing (Kaiser and Bengio, 2018) with a TreeLSTMbased autoencoder. Their experiments with semantic coding via clustering of BERT (Devlin et al., 2019) and FastText (Bojanowski et al., 2017) embeddings lead to negligible or negative effects. Outside of MT, Keskar et al. (2019) in a similar vein condition on manually categorized “control codes” that specify style and content, and Mallinson and Lapata (2019) condition on annotated syntactic or lexical change markers that can be learnt from data. We refer readers to Ippolito et al. (2019) for an overview of diverse decoding methods. Few to our knowledge explicitly and effectively encode open-domain semantic diversity. Text-based causal knowledge acquisition is a well-studied challenge in NLP (R"
2020.emnlp-main.421,Q18-1031,0,0.0251276,"cause or effect statements for a given proposition. The model is trained on the co-released corpus CausalBank, which comprises causal statements harvested from English Common Crawl (Buck et al., 2014). Applications of LSH (Indyk and Motwani, 1998; Charikar, 2002) in NLP began with Ravichandran et al. (2005) who demonstrated its use in fast lexical similarity comparison; later, Van Durme and Lall (2010) showed such hashing could be performed online. More similar to our use case, Petrovi´c et al. (2010) binned tweets via LSH to enable fast first story detection. Most related to ours is work by Guu et al. (2018), who describe a generative sentence model that edits a ‘prototype’ sentence using lexically similar ones retrieved via LSH. 3 COD 3 S Approach Our signature construction method, depicted in Figure 1(a), produces a sequence of bits that collectively imply a highly specific bin of sentences with similar semantic meaning. This is accomplished by encoding sentences into high-dimensional vectors that encode degrees of semantic difference and then discretizing the vectors in a way that approximately preserves the difference. Semantic Embedding Model We embed a sentence using the contextual encoder"
2020.emnlp-main.421,N19-1090,1,0.888347,"Missing"
2020.emnlp-main.421,K19-1005,1,0.849696,"Missing"
2020.emnlp-main.421,P19-1365,1,0.801868,"ic diversity of sampled translations using codes produced from improved semantic hashing (Kaiser and Bengio, 2018) with a TreeLSTMbased autoencoder. Their experiments with semantic coding via clustering of BERT (Devlin et al., 2019) and FastText (Bojanowski et al., 2017) embeddings lead to negligible or negative effects. Outside of MT, Keskar et al. (2019) in a similar vein condition on manually categorized “control codes” that specify style and content, and Mallinson and Lapata (2019) condition on annotated syntactic or lexical change markers that can be learnt from data. We refer readers to Ippolito et al. (2019) for an overview of diverse decoding methods. Few to our knowledge explicitly and effectively encode open-domain semantic diversity. Text-based causal knowledge acquisition is a well-studied challenge in NLP (Radinsky et al., 2012). Recent efforts have investigated open ended causal generation using neural models (Bosselut et al., 2019; Li et al., 2020). The latter train a conditional generation model to propose cause or effect statements for a given proposition. The model is trained on the co-released corpus CausalBank, which comprises causal statements harvested from English Common Crawl (Bu"
2020.emnlp-main.421,P05-1077,0,0.0263769,"de open-domain semantic diversity. Text-based causal knowledge acquisition is a well-studied challenge in NLP (Radinsky et al., 2012). Recent efforts have investigated open ended causal generation using neural models (Bosselut et al., 2019; Li et al., 2020). The latter train a conditional generation model to propose cause or effect statements for a given proposition. The model is trained on the co-released corpus CausalBank, which comprises causal statements harvested from English Common Crawl (Buck et al., 2014). Applications of LSH (Indyk and Motwani, 1998; Charikar, 2002) in NLP began with Ravichandran et al. (2005) who demonstrated its use in fast lexical similarity comparison; later, Van Durme and Lall (2010) showed such hashing could be performed online. More similar to our use case, Petrovi´c et al. (2010) binned tweets via LSH to enable fast first story detection. Most related to ours is work by Guu et al. (2018), who describe a generative sentence model that edits a ‘prototype’ sentence using lexically similar ones retrieved via LSH. 3 COD 3 S Approach Our signature construction method, depicted in Figure 1(a), produces a sequence of bits that collectively imply a highly specific bin of sentences w"
2020.emnlp-main.421,N16-1014,0,0.13187,"decoded signatures. Introduction Open-ended sequence generation problems such as dialog, story generation, image captioning, or causal generation pose a practical challenge to neural sequence-to-sequence (seq2seq) models, as they necessitate a diverse set of predicted outputs. The typical sampling method for seq2seq decoding is beam search, which produces a set of candidate sequences that generally have high syntactic, lexical, and semantic overlap. Recent methods for improved diversity generation make slight modifications to the neural architecture or beam search algorithm (Xu et al., 2018; Li et al., 2016b), or impose lexical constraints during decoding (Post and Vilar, 2018; Hu et al., 2019a). Shu et al. (2019) propose the use of sentence codes, a technique in which generation is conditioned on a discrete code that aims to induce diversity in syntax or semantics. While their approach is effective for syntactic codes, it is less so for semantics. In this work, we introduce an improved method for diverse generation conditioned on inferred sentence codes that explicitly capture meaningful semantic differences. We use the contextual sentence embeddings from Sentence-BERT (SBERT; Reimers and Gurev"
2020.emnlp-main.421,N19-4009,0,0.0220296,"train models on sentence pairs from Li et al.’s released dataset, CausalBank, which is scraped from Common Crawl using templatic causal patterns. Following their work, we use 10 million sentence pairs that match the patterns “X, so Y” to train cause-to-effect models and “X because Y” for effect-to-cause models. We experiment with 16-bit LSH signatures of SBERT embeddings.5 After prepending targetside bit signatures, pairs are encoded with bytepair encoding (BPE; Sennrich et al., 2016) using a vocabulary size of 10K. We train Transformer models (Vaswani et al., 2017) using the FAIRSEQ library (Ott et al., 2019). Appendix B provides details for reproducibility.6 Evaluation We show that COD 3 S induces sensible inference of diverse but relevant semantic bins and causal statements. Examples of generation are shown in Table 3 and additionally Appendix C. We quantitatively compare COD 3 S against the out4 We find the threshold t = 2 best for 16-bit COD 3 S. describing the distribution of the 10M training targets into signature bins are given in Appendix E. 6 Our code and pretrained models are available at https:// github.com/nweir127/COD3S 5 Statistics 5201 COPA 3-Sets Baselines S2S S2S + Sigs C→E E→C BL"
2020.emnlp-main.421,N10-1021,0,0.163732,"Missing"
2020.emnlp-main.421,D19-1410,0,0.181371,"; Li et al., 2016b), or impose lexical constraints during decoding (Post and Vilar, 2018; Hu et al., 2019a). Shu et al. (2019) propose the use of sentence codes, a technique in which generation is conditioned on a discrete code that aims to induce diversity in syntax or semantics. While their approach is effective for syntactic codes, it is less so for semantics. In this work, we introduce an improved method for diverse generation conditioned on inferred sentence codes that explicitly capture meaningful semantic differences. We use the contextual sentence embeddings from Sentence-BERT (SBERT; Reimers and Gurevych, 2019), the cosine distances between which correlate highly with human scalar judgments of semantic textual similarity (STS). We construct discrete codes from these embeddings using locality-sensitive hashing (Indyk and Motwani, 1998; Charikar, 2002), producing short binary signatures whose Hamming distances well-preserves the cosine distances between inputs. Our method induces a bitwise hierarchy of semantic bins whose similarities in signature imply similarities in semantics. Conditioning generation on a signature as a target-side prefix indicates the bin into which the generated sequence falls. W"
2020.emnlp-main.421,P16-1162,0,0.0338403,"as considered by Li et al. (2020). Given an input statement, the model must suggest a diverse set of possible causes or effects. We train models on sentence pairs from Li et al.’s released dataset, CausalBank, which is scraped from Common Crawl using templatic causal patterns. Following their work, we use 10 million sentence pairs that match the patterns “X, so Y” to train cause-to-effect models and “X because Y” for effect-to-cause models. We experiment with 16-bit LSH signatures of SBERT embeddings.5 After prepending targetside bit signatures, pairs are encoded with bytepair encoding (BPE; Sennrich et al., 2016) using a vocabulary size of 10K. We train Transformer models (Vaswani et al., 2017) using the FAIRSEQ library (Ott et al., 2019). Appendix B provides details for reproducibility.6 Evaluation We show that COD 3 S induces sensible inference of diverse but relevant semantic bins and causal statements. Examples of generation are shown in Table 3 and additionally Appendix C. We quantitatively compare COD 3 S against the out4 We find the threshold t = 2 best for 16-bit COD 3 S. describing the distribution of the 10M training targets into signature bins are given in Appendix E. 6 Our code and pretrai"
2020.emnlp-main.421,P19-1177,0,0.199238,"image captioning, or causal generation pose a practical challenge to neural sequence-to-sequence (seq2seq) models, as they necessitate a diverse set of predicted outputs. The typical sampling method for seq2seq decoding is beam search, which produces a set of candidate sequences that generally have high syntactic, lexical, and semantic overlap. Recent methods for improved diversity generation make slight modifications to the neural architecture or beam search algorithm (Xu et al., 2018; Li et al., 2016b), or impose lexical constraints during decoding (Post and Vilar, 2018; Hu et al., 2019a). Shu et al. (2019) propose the use of sentence codes, a technique in which generation is conditioned on a discrete code that aims to induce diversity in syntax or semantics. While their approach is effective for syntactic codes, it is less so for semantics. In this work, we introduce an improved method for diverse generation conditioned on inferred sentence codes that explicitly capture meaningful semantic differences. We use the contextual sentence embeddings from Sentence-BERT (SBERT; Reimers and Gurevych, 2019), the cosine distances between which correlate highly with human scalar judgments of semantic textu"
2020.emnlp-main.421,P10-2043,1,0.810806,"Missing"
2020.emnlp-main.421,W18-6319,0,0.0119333,"set ∆(y, y0 ) to be the sentences’ inverse (100 minus) BLEU-1 and -2 scores.8 To measure semantic diversity, we set ∆ to be the cosine distance between their SBERT embeddings. Higher scores imply greater diversity. Following Li et al., we evaluate on 100 examples from an out-of-distribution dev split of the Choice of Plausible Alternatives dataset (COPA; Gordon et al., 2012), with results shown in Table 2.9 In both cases, COD 3 S outperforms all other methods except 7 We also compare against our own S2S-RS using the same model as the COD 3 S methods. 8 Implemented using the SacreBLEU toolkit (Post, 2018). 9 Results over 10 outputs and over a within-distribution train split from CausalBank are shown in Appendix Table 4. Figure 2: Results of human evaluation of plausibility. Ratings are shown in comparison to the gold answer and less plausible alternative from COPA. Mean/max ratings per input are presented for 1, 3-best outputs ranked by forward score (PPL). To demonstrate that COD 3 S produces plausible response from many semantic bins, we also show max ratings from top-10 outputs. random sampling, the addition of which also improves the diversity of COD 3 S itself.10 We also use the SBERT div"
2020.emnlp-main.421,N18-1119,0,0.0557503,"roblems such as dialog, story generation, image captioning, or causal generation pose a practical challenge to neural sequence-to-sequence (seq2seq) models, as they necessitate a diverse set of predicted outputs. The typical sampling method for seq2seq decoding is beam search, which produces a set of candidate sequences that generally have high syntactic, lexical, and semantic overlap. Recent methods for improved diversity generation make slight modifications to the neural architecture or beam search algorithm (Xu et al., 2018; Li et al., 2016b), or impose lexical constraints during decoding (Post and Vilar, 2018; Hu et al., 2019a). Shu et al. (2019) propose the use of sentence codes, a technique in which generation is conditioned on a discrete code that aims to induce diversity in syntax or semantics. While their approach is effective for syntactic codes, it is less so for semantics. In this work, we introduce an improved method for diverse generation conditioned on inferred sentence codes that explicitly capture meaningful semantic differences. We use the contextual sentence embeddings from Sentence-BERT (SBERT; Reimers and Gurevych, 2019), the cosine distances between which correlate highly with hu"
2020.emnlp-main.482,N15-1170,0,0.0177933,"vily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings a bilingual speaker in the loop to actively provide cross-lingual knowledge, which is more reliable in low-resource settings. Concurrent 5991 to our work, Karamanolakis et al. (2020) also show that keyword translation is very useful for crosslingual document classification. 6 Conclusion and Future Work is an interactive system that enhances for a task by asking a bilingual speaker for word-level similarity annotations. We test CLIME on cross-lingual information triage in internat"
2020.emnlp-main.482,P18-1073,0,0.0259999,"propriate for languages with more data. Future work can extend the interactive component of CLIME to multilingual transformers. 5 Related Work Cross-Lingual Word Embeddings. Ruder et al. (2019) summarize previous CLWE methods. These methods learn from existing resources such as dictionaries, parallel text, and monolingual corpora. Therefore, the availability and quality of training data primarily determines the success of these methods (Søgaard et al., 2018). To improve the suitability of CLWE methods in low-resource settings, recent work focuses on learning without cross-lingual supervision (Artetxe et al., 2018; Hoshen and Wolf, 2018) and normalizing monolingual embeddings before alignment (Zhang et al., 2019). In contrast, we design a human-in-the-loop system to efficiently improve CLWE. Moreover, previous CLWE methods are heavily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al"
2020.emnlp-main.482,Q17-1010,0,0.0604242,"To simplify the task, we consider a binary classification problem of detecting whether the documents are associated with medical needs. Table 1 shows an example document. To balance the label distribution, we sample an equal number of negative examples. Word Embeddings. To transfer knowledge between languages, we build CLWE between English and each target language. We experiment with two methods to pre-train CLWE: (1) train monolingual embeddings with word2vec (Mikolov et al., 2013b) and align with CCA (Faruqui et al., 2015; Ammar et al., 2016), (2) train monolingual embeddings with fastText (Bojanowski et al., 2017) and align with RCSLS (Joulin et al., 2018). The English em2 Download from https://www.ldc.upenn.edu beddings are trained on Wikipedia and the target language embeddings are trained on unlabeled documents from the LORELEI language packs. For alignment, we use the small English dictionary in each pack. Low-resource language speakers are hard to find, so we do not try all combinations of languages and CLWE: we use CCA embeddings for Tigrinya and Uyghur, RCSLS embeddings for Ilocano. Since Sinhalese speakers are easier to find, we experiment with both CLWE for Sinhalese. Text Classifier. Our clas"
2020.emnlp-main.482,Q18-1039,0,0.0909114,"Missing"
2020.emnlp-main.482,2020.acl-main.747,0,0.0165428,"increase test accuracy on identifying health-related documents in less than an hour. CLIME is related to active learning (Settles, 2009), which also improves a classifier through user interaction. Therefore, we compare CLIME with an active learning baseline that asks a user to label target language documents. Under the same annotation time constraint, CLIME often has higher accuracy. Furthermore, the two methods are complementary. Combining active learning with CLIME increases accuracy even more, and the user-adapted model is competitive with a large, resource-hungry multilingual transformer (Conneau et al., 2020). 2 Interactive Neighborhood Reshaping This section introduces the interface designed to solicit human feedback on neighborhoods of CLWE and our keyword selection criterion. Suppose that we have two languages with vocabulary V1 and V2 . Let E be a pre-trained CLWE matrix, where Ew is the vector representation of word type w in the joint vocabulary V = V1 ∪ V2 . Our goal is to help a bilingual novice (i.e., not a machine learning expert) improve the CLWE E for a downstream task through inspection of neighboring words. 2.1 Keyword Selection With limited annotation time, users cannot vet the enti"
2020.emnlp-main.482,D09-1009,0,0.277055,"to help a bilingual novice (i.e., not a machine learning expert) improve the CLWE E for a downstream task through inspection of neighboring words. 2.1 Keyword Selection With limited annotation time, users cannot vet the entire vocabulary. Instead, we need to find a small salient subset of keywords K ⊆ V whose embeddings, if vetted, would most improve a downstream task. For example, if the downstream task is sentiment analysis, our keywords set should include sentiment words such as “good” and “bad”. Prior work in active learning solicits keywords using information gain (Raghavan et al., 2006; Druck et al., 2009; Settles, 2011), but this cannot be applied to continuous embeddings. Li et al. (2016) suggest that the contribution of one dimension of a word embedding to the loss function can be approximated by the absolute value of its partial derivative, and therefore they use partial derivatives to visualize the behavior of neural models. However, rather than understanding the importance of individual dimensions, we want to compute the salience of an entire word vector. Therefore, we extend their idea by defining the salience of a word embedding as the magnitude of the loss function’s gradient. This sc"
2020.emnlp-main.482,D17-1063,0,0.01969,"17). Therefore, future advances in these areas may also improve CLIME. Another line of future work is to investigate alternative user interfaces. For example, we could ask bilingual users to rank nearest neighbors (Sakaguchi and Van Durme, 2018) or provide scalar grades (Hill et al., 2015) instead of accepting/rejecting individual neighbors. We also explore a simple combination of active learning and CLIME. Simultaneously applying both methods is better than using either alone. In the future, we plan to train a policy that dynamically combines the two interactions with reinforcement learning (Fang et al., 2017). CLIME CLWE Human-in-the-Loop Multilingual Systems. CLIME is inspired by human-in-the-loop systems that bridge language gaps. Brown and Grinter (2016) build an interactive translation platform to help refugee resettlement. Yuan et al. (2018) interactively align topic models across languages. Active Learning. A common solution to data scarcity is active learning, the framework in which the learner iteratively queries an oracle (often a human) to receive annotations on unlabeled data. Settles (2009) summarizes popular active learning methods. Most active learning methods solicit labels for trai"
2020.emnlp-main.482,D18-1407,1,0.853298,"curacy: active learning and CLIME are complementary. Singlesample t-tests confirm that CLIME is significantly better than Base and A+C is significantly better than Active (Appendix A.1). Keyword Detection. We inspect the list of the fifty most salient keywords (Section 2.1). Most keywords have obvious connections to our classification task of detecting medical emergencies, such as “ambulance”, “hospitals”, and “disease”. However, the list also contains some words that are unrelated to a medical emergency, including “over” and “given”. These words may be biases or artifacts from training data (Feng et al., 2018). 5989 CLIME −−−→ (a) Neighborhood of “plague” CLIME −−−→ (b) Neighborhood of “ill” Figure 5: T- SNE visualization of embeddings before (left) and after (right) CLIME updates. From one Sinhalese user study, we inspect two keywords, “ill” and “plague”, and their five closest neighbors in English (blue) and Sinhalese (green). The Sinhalese words are labeled with English translations. Shape denotes the type of feedback: “+” for positive neighbors and “x” for negative neighbors. Number of Keywords. To evaluate how feedback quantity changes accuracy, we vary the number of keywords and compare test"
2020.emnlp-main.482,P19-1070,0,0.0667672,"y and quality of training data primarily determines the success of these methods (Søgaard et al., 2018). To improve the suitability of CLWE methods in low-resource settings, recent work focuses on learning without cross-lingual supervision (Artetxe et al., 2018; Hoshen and Wolf, 2018) and normalizing monolingual embeddings before alignment (Zhang et al., 2019). In contrast, we design a human-in-the-loop system to efficiently improve CLWE. Moreover, previous CLWE methods are heavily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings"
2020.emnlp-main.482,P18-1004,0,0.038408,"Missing"
2020.emnlp-main.482,J15-4004,0,0.03786,"ME further improves the system. CLIME has a modular design with three components: keyword ranking, user interface, and embedding refinement. The keyword ranking and the embedding refinement modules build upon existing methods for interpreting neural networks (Li et al., 2016) and fine-tuning word embeddings (Mrkši´c et al., 2017). Therefore, future advances in these areas may also improve CLIME. Another line of future work is to investigate alternative user interfaces. For example, we could ask bilingual users to rank nearest neighbors (Sakaguchi and Van Durme, 2018) or provide scalar grades (Hill et al., 2015) instead of accepting/rejecting individual neighbors. We also explore a simple combination of active learning and CLIME. Simultaneously applying both methods is better than using either alone. In the future, we plan to train a policy that dynamically combines the two interactions with reinforcement learning (Fang et al., 2017). CLIME CLWE Human-in-the-Loop Multilingual Systems. CLIME is inspired by human-in-the-loop systems that bridge language gaps. Brown and Grinter (2016) build an interactive translation platform to help refugee resettlement. Yuan et al. (2018) interactively align topic mod"
2020.emnlp-main.482,D18-1043,0,0.0187236,"s with more data. Future work can extend the interactive component of CLIME to multilingual transformers. 5 Related Work Cross-Lingual Word Embeddings. Ruder et al. (2019) summarize previous CLWE methods. These methods learn from existing resources such as dictionaries, parallel text, and monolingual corpora. Therefore, the availability and quality of training data primarily determines the success of these methods (Søgaard et al., 2018). To improve the suitability of CLWE methods in low-resource settings, recent work focuses on learning without cross-lingual supervision (Artetxe et al., 2018; Hoshen and Wolf, 2018) and normalizing monolingual embeddings before alignment (Zhang et al., 2019). In contrast, we design a human-in-the-loop system to efficiently improve CLWE. Moreover, previous CLWE methods are heavily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 201"
2020.emnlp-main.482,P15-1162,1,0.784673,"often improves accuracy faster than an active learning baseline and can be easily combined with active learning to improve results. 1 mediocre disappointing Figure 1: A hypothetical topographic map of an English–French embedding space tailored for sentiment analysis. Dots are English words, and squares are French words. Positive sentiment words are grouped in a clime (red), while negative sentiment words are grouped in another clime (blue). These climes help sentiment analysis. Introduction Modern text classification requires large labeled datasets and pre-trained word embeddings (Kim, 2014; Iyyer et al., 2015; Joulin et al., 2017). However, scarcity of both labeled and unlabeled data holds back applications in low-resource languages. Cross-lingual word embeddings (Mikolov et al., 2013a, CLWE) can bridge the gap by mapping words from different languages to a shared vector space. Using CLWE features, models trained in a resource-rich language (e.g., English) can predict labels for other languages. The success of CLWE relies on the domain and quality of training data (Søgaard et al., 2018). While these methods have impressive word translation accuracy, they are not tailored for downstream tasks such"
2020.emnlp-main.482,D18-1330,0,0.0246625,"sification problem of detecting whether the documents are associated with medical needs. Table 1 shows an example document. To balance the label distribution, we sample an equal number of negative examples. Word Embeddings. To transfer knowledge between languages, we build CLWE between English and each target language. We experiment with two methods to pre-train CLWE: (1) train monolingual embeddings with word2vec (Mikolov et al., 2013b) and align with CCA (Faruqui et al., 2015; Ammar et al., 2016), (2) train monolingual embeddings with fastText (Bojanowski et al., 2017) and align with RCSLS (Joulin et al., 2018). The English em2 Download from https://www.ldc.upenn.edu beddings are trained on Wikipedia and the target language embeddings are trained on unlabeled documents from the LORELEI language packs. For alignment, we use the small English dictionary in each pack. Low-resource language speakers are hard to find, so we do not try all combinations of languages and CLWE: we use CCA embeddings for Tigrinya and Uyghur, RCSLS embeddings for Ilocano. Since Sinhalese speakers are easier to find, we experiment with both CLWE for Sinhalese. Text Classifier. Our classifier is a convolutional neural network (K"
2020.emnlp-main.482,E17-2068,0,0.026799,"racy faster than an active learning baseline and can be easily combined with active learning to improve results. 1 mediocre disappointing Figure 1: A hypothetical topographic map of an English–French embedding space tailored for sentiment analysis. Dots are English words, and squares are French words. Positive sentiment words are grouped in a clime (red), while negative sentiment words are grouped in another clime (blue). These climes help sentiment analysis. Introduction Modern text classification requires large labeled datasets and pre-trained word embeddings (Kim, 2014; Iyyer et al., 2015; Joulin et al., 2017). However, scarcity of both labeled and unlabeled data holds back applications in low-resource languages. Cross-lingual word embeddings (Mikolov et al., 2013a, CLWE) can bridge the gap by mapping words from different languages to a shared vector space. Using CLWE features, models trained in a resource-rich language (e.g., English) can predict labels for other languages. The success of CLWE relies on the domain and quality of training data (Søgaard et al., 2018). While these methods have impressive word translation accuracy, they are not tailored for downstream tasks such as text classification"
2020.emnlp-main.482,2020.findings-emnlp.323,0,0.0221149,"ansfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings a bilingual speaker in the loop to actively provide cross-lingual knowledge, which is more reliable in low-resource settings. Concurrent 5991 to our work, Karamanolakis et al. (2020) also show that keyword translation is very useful for crosslingual document classification. 6 Conclusion and Future Work is an interactive system that enhances for a task by asking a bilingual speaker for word-level similarity annotations. We test CLIME on cross-lingual information triage in international health emergencies for four low-resource languages. Bilingual users can quickly improve a model with the help of CLIME at a faster rate than an active learning baseline. Combining active learning with CLIME further improves the system. CLIME has a modular design with three components: keywor"
2020.emnlp-main.482,D14-1181,0,0.00697457,"ings. CLIME often improves accuracy faster than an active learning baseline and can be easily combined with active learning to improve results. 1 mediocre disappointing Figure 1: A hypothetical topographic map of an English–French embedding space tailored for sentiment analysis. Dots are English words, and squares are French words. Positive sentiment words are grouped in a clime (red), while negative sentiment words are grouped in another clime (blue). These climes help sentiment analysis. Introduction Modern text classification requires large labeled datasets and pre-trained word embeddings (Kim, 2014; Iyyer et al., 2015; Joulin et al., 2017). However, scarcity of both labeled and unlabeled data holds back applications in low-resource languages. Cross-lingual word embeddings (Mikolov et al., 2013a, CLWE) can bridge the gap by mapping words from different languages to a shared vector space. Using CLWE features, models trained in a resource-rich language (e.g., English) can predict labels for other languages. The success of CLWE relies on the domain and quality of training data (Søgaard et al., 2018). While these methods have impressive word translation accuracy, they are not tailored for do"
2020.emnlp-main.482,C12-1089,0,0.276539,"too far ˆ away from the original embeddings E. The final cost function combines the feedback cost (Equation 3) and the regularizer (Equation 4): C(E) = Cf (E) + λR(E), (5) where the hyperparameter λ controls the strength of the regularizer. The updated embeddings enforce constraints from user feedback while preserving other structures from the original embeddings. After tuning in a pilot user study, we set λ to one. We use the Adam optimizer (Kingma and Ba, 2015) with default hyperparameters. 4 Cross-Lingual Classification Experiments We evaluate CLIME on cross-lingual documentclassification (Klementiev et al., 2012), where we build a text classifier for a low-resource target Ilocano ... Nagtalinaed dagiti pito a balod ti Bureau of Jail Management and Penology (BJMP) ditoy ciudad ti Laoag iti isolation room gapo iti tuko ... English ... Seven inmates from the Bureau of Jail Management and Penology (BJMP), Laoag City, have been transferred to the isolation room due to chicken pox ... Table 1: Excerpt of a positive Ilocano test example (top) and its English translation (bottom) that describes a medical emergency. language using labeled data in a high-resource source language through CLWE. Our task identifie"
2020.emnlp-main.482,N16-1082,0,0.287384,"downstream task through inspection of neighboring words. 2.1 Keyword Selection With limited annotation time, users cannot vet the entire vocabulary. Instead, we need to find a small salient subset of keywords K ⊆ V whose embeddings, if vetted, would most improve a downstream task. For example, if the downstream task is sentiment analysis, our keywords set should include sentiment words such as “good” and “bad”. Prior work in active learning solicits keywords using information gain (Raghavan et al., 2006; Druck et al., 2009; Settles, 2011), but this cannot be applied to continuous embeddings. Li et al. (2016) suggest that the contribution of one dimension of a word embedding to the loss function can be approximated by the absolute value of its partial derivative, and therefore they use partial derivatives to visualize the behavior of neural models. However, rather than understanding the importance of individual dimensions, we want to compute the salience of an entire word vector. Therefore, we extend their idea by defining the salience of a word embedding as the magnitude of the loss function’s gradient. This score summarizes salience of all dimensions from a word embedding. Formally, let x = hx1"
2020.emnlp-main.482,P18-1020,1,0.880351,"Missing"
2020.emnlp-main.482,L18-1560,0,0.193578,"Missing"
2020.emnlp-main.482,D11-1136,0,0.582572,"novice (i.e., not a machine learning expert) improve the CLWE E for a downstream task through inspection of neighboring words. 2.1 Keyword Selection With limited annotation time, users cannot vet the entire vocabulary. Instead, we need to find a small salient subset of keywords K ⊆ V whose embeddings, if vetted, would most improve a downstream task. For example, if the downstream task is sentiment analysis, our keywords set should include sentiment words such as “good” and “bad”. Prior work in active learning solicits keywords using information gain (Raghavan et al., 2006; Druck et al., 2009; Settles, 2011), but this cannot be applied to continuous embeddings. Li et al. (2016) suggest that the contribution of one dimension of a word embedding to the loss function can be approximated by the absolute value of its partial derivative, and therefore they use partial derivatives to visualize the behavior of neural models. However, rather than understanding the importance of individual dimensions, we want to compute the salience of an entire word vector. Therefore, we extend their idea by defining the salience of a word embedding as the magnitude of the loss function’s gradient. This score summarizes s"
2020.emnlp-main.482,N16-1018,0,0.0630818,"Missing"
2020.emnlp-main.482,Q17-1022,0,0.0346062,"Missing"
2020.emnlp-main.482,P10-1023,0,0.0137763,"otations, CLIME updates the embeddings to reflect their feedback. The algorithm reshapes the neighborhood so that words near a keyword share similar semantic attributes. Together, these embeddings form desired taskspecific connections between words across languages. Our update equations are inspired by ATTRACT- REPEL (Mrkši´c et al., 2017), which fine-tunes word embeddings with synonym and antonym constraints. The objective in ATTRACTREPEL pulls synonyms closer to and pushes antonyms further away from their nearest neighbors. This objective is useful for large lexical resources like BabelNet (Navigli and Ponzetto, 2010) with hundreds of thousands linguistic constraints, but our pilot experiment suggests that the method is not suitable for smaller constraint sets. Since CLIME is designed for low-resource languages, we optimize an objective that reshapes the neighborhood more drastically than ATTRACT- REPEL. 3.1 Feedback Cost For each keyword k ∈ K, we collect a positive set Pk and a negative set Nk (Section 2.2). To refine 5986 embeddings E with human feedback, we increase the similarity between k and each positive word p ∈ Pk , and decrease the similarity between k and each negative word n ∈ Nk . Formally, w"
2020.emnlp-main.482,D10-1103,0,0.0284959,"and Wolf, 2018) and normalizing monolingual embeddings before alignment (Zhang et al., 2019). In contrast, we design a human-in-the-loop system to efficiently improve CLWE. Moreover, previous CLWE methods are heavily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings a bilingual speaker in the loop to actively provide cross-lingual knowledge, which is more reliable in low-resource settings. Concurrent 5991 to our work, Karamanolakis et al. (2020) also show that keyword translation is very useful for crosslingual document classific"
2020.emnlp-main.482,P18-1072,0,0.132892,"Missing"
2020.emnlp-main.482,L16-1521,0,0.0816641,"ti pito a balod ti Bureau of Jail Management and Penology (BJMP) ditoy ciudad ti Laoag iti isolation room gapo iti tuko ... English ... Seven inmates from the Bureau of Jail Management and Penology (BJMP), Laoag City, have been transferred to the isolation room due to chicken pox ... Table 1: Excerpt of a positive Ilocano test example (top) and its English translation (bottom) that describes a medical emergency. language using labeled data in a high-resource source language through CLWE. Our task identifies whether a document describes a medical emergency, useful for planning disaster relief (Strassel and Tracey, 2016). The source language is English and the four low-resource target languages are Ilocano, Sinhalese, Tigrinya, and Uyghur. Our experiments confirm that a bilingual user can quickly improve the test accuracy of crosslingual models through CLIME. Alternatively, we can ask an annotator to improve the model by labeling more training documents in the target language. Therefore, we compare CLIME to an active learning baseline that queries the user for document labels; CLIME often improves accuracy faster. Then, we combine CLIME and active learning to show an even faster improvement of test accuracy."
2020.emnlp-main.482,P09-1027,0,0.0243158,"of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings a bilingual speaker in the loop to actively provide cross-lingual knowledge, which is more reliable in low-resource settings. Concurrent 5991 to our work, Karamanolakis et al. (2020) also show that keyword translation is very useful for crosslingual document classification. 6 Conclusion and Future Work is an interactive system that enhances for a task by asking a bilingual speaker for word-level similarity annotations. We test CLIME on cross-lingual information triage in international health emergencies for fou"
2020.emnlp-main.482,I08-1022,0,0.0386376,"l., 2018; Hoshen and Wolf, 2018) and normalizing monolingual embeddings before alignment (Zhang et al., 2019). In contrast, we design a human-in-the-loop system to efficiently improve CLWE. Moreover, previous CLWE methods are heavily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings a bilingual speaker in the loop to actively provide cross-lingual knowledge, which is more reliable in low-resource settings. Concurrent 5991 to our work, Karamanolakis et al. (2020) also show that keyword translation is very useful for crosslingual"
2020.emnlp-main.482,P10-1114,0,0.0231499,"efficiently improve CLWE. Moreover, previous CLWE methods are heavily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings a bilingual speaker in the loop to actively provide cross-lingual knowledge, which is more reliable in low-resource settings. Concurrent 5991 to our work, Karamanolakis et al. (2020) also show that keyword translation is very useful for crosslingual document classification. 6 Conclusion and Future Work is an interactive system that enhances for a task by asking a bilingual speaker for word-level similarity annotat"
2020.emnlp-main.482,D19-1077,0,0.0685283,"Missing"
2020.emnlp-main.482,P17-1130,0,0.0173083,"nolingual embeddings before alignment (Zhang et al., 2019). In contrast, we design a human-in-the-loop system to efficiently improve CLWE. Moreover, previous CLWE methods are heavily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings a bilingual speaker in the loop to actively provide cross-lingual knowledge, which is more reliable in low-resource settings. Concurrent 5991 to our work, Karamanolakis et al. (2020) also show that keyword translation is very useful for crosslingual document classification. 6 Conclusion and Future Work"
2020.emnlp-main.482,2020.emnlp-main.637,1,0.733454,"Active Learning. A common solution to data scarcity is active learning, the framework in which the learner iteratively queries an oracle (often a human) to receive annotations on unlabeled data. Settles (2009) summarizes popular active learning methods. Most active learning methods solicit labels for training examples/documents, while CLIME asks for word-level annotation. Previous active learning methods that use feature-level annotation (Raghavan et al., 2006; Zaidan et al., 2007; Druck et al., 2009; Settles, 2011) are not applicable to neural networks and CLWE. Closely related to our work, Yuan et al. (2020) propose an active learning strategy that selects examples based on language modeling pre-training. Neural Network Interpretation. Our keyword detection algorithm expands upon prior work in interpreting neural networks. Li et al. (2016) uses the gradient of the objective function to linearly approximate salience of one dimension, which helps interpret and visualize word compositionality in neural networks. Their ideas are inspired by visual salience in computer vision (Simonyan et al., 2013; Zeiler and Fergus, 2014). We further extend the idea to compute the global salience of an entire word v"
2020.emnlp-main.482,N07-1033,0,0.0365299,"interactive translation platform to help refugee resettlement. Yuan et al. (2018) interactively align topic models across languages. Active Learning. A common solution to data scarcity is active learning, the framework in which the learner iteratively queries an oracle (often a human) to receive annotations on unlabeled data. Settles (2009) summarizes popular active learning methods. Most active learning methods solicit labels for training examples/documents, while CLIME asks for word-level annotation. Previous active learning methods that use feature-level annotation (Raghavan et al., 2006; Zaidan et al., 2007; Druck et al., 2009; Settles, 2011) are not applicable to neural networks and CLWE. Closely related to our work, Yuan et al. (2020) propose an active learning strategy that selects examples based on language modeling pre-training. Neural Network Interpretation. Our keyword detection algorithm expands upon prior work in interpreting neural networks. Li et al. (2016) uses the gradient of the objective function to linearly approximate salience of one dimension, which helps interpret and visualize word compositionality in neural networks. Their ideas are inspired by visual salience in computer vi"
2020.emnlp-main.482,2020.acl-main.201,1,0.837488,"al word embeddings (Mikolov et al., 2013a, CLWE) can bridge the gap by mapping words from different languages to a shared vector space. Using CLWE features, models trained in a resource-rich language (e.g., English) can predict labels for other languages. The success of CLWE relies on the domain and quality of training data (Søgaard et al., 2018). While these methods have impressive word translation accuracy, they are not tailored for downstream tasks such as text classification (Glavas ? ∗ indicates equal contribution Benjamin Van Durme Johns Hopkins University vandurme@jhu.edu et al., 2019; Zhang et al., 2020a). We develop CLassifying Interactively with Multilingual Embeddings (CLIME), that efficiently specializes CLWE with human interaction.1 Given a pre-trained CLWE , a bilingual speaker in the loop reviews the nearest-neighbor words. CLIME capitalizes on the intuition that neighboring words in an ideal embedding space should have similar semantic attributes. In an analogy to geographic climes—zones with distinctive meteorological features—we call areas in the embedding space where words share similar semantic features climes. Our goal is to convert neighborhoods in the embedding space into clas"
2020.emnlp-main.482,P19-1307,1,0.831817,"ultilingual transformers. 5 Related Work Cross-Lingual Word Embeddings. Ruder et al. (2019) summarize previous CLWE methods. These methods learn from existing resources such as dictionaries, parallel text, and monolingual corpora. Therefore, the availability and quality of training data primarily determines the success of these methods (Søgaard et al., 2018). To improve the suitability of CLWE methods in low-resource settings, recent work focuses on learning without cross-lingual supervision (Artetxe et al., 2018; Hoshen and Wolf, 2018) and normalizing monolingual embeddings before alignment (Zhang et al., 2019). In contrast, we design a human-in-the-loop system to efficiently improve CLWE. Moreover, previous CLWE methods are heavily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Z"
2020.emnlp-main.482,P16-1133,0,0.0243915,"ary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings a bilingual speaker in the loop to actively provide cross-lingual knowledge, which is more reliable in low-resource settings. Concurrent 5991 to our work, Karamanolakis et al. (2020) also show that keyword translation is very useful for crosslingual document classification. 6 Conclusion and Future Work is an interactive system that enhances for a task by asking a bilingual speaker for word-level similarity annotations. We test CLIME on cross-lingual information triage in international health emergencies for four low-resource langu"
2020.emnlp-main.482,W06-0605,0,\N,Missing
2020.emnlp-main.482,D14-1162,0,\N,Missing
2020.emnlp-main.482,P15-1027,0,\N,Missing
2020.emnlp-main.482,N15-1104,0,\N,Missing
2020.emnlp-main.482,N15-1028,0,\N,Missing
2020.emnlp-main.482,N16-1156,0,\N,Missing
2020.emnlp-main.482,D16-1250,0,\N,Missing
2020.emnlp-main.482,D18-1027,0,\N,Missing
2020.emnlp-main.482,D18-1042,0,\N,Missing
2020.emnlp-main.482,E14-1049,0,\N,Missing
2020.emnlp-main.608,2020.acl-main.236,0,0.056003,"Missing"
2020.emnlp-main.608,2020.acl-main.421,0,0.0291495,"have devised multilingual benchmarks mirroring those for NLU in English (Conneau et al., 2018b; Liang et al., 2020; Hu et al., 2020). Surprisingly, without any explicit cross-lingual signal, these models achieve strong zero-shot cross-lingual performance, outperforming prior cross-lingual word embedding-based methods (Wu and Dredze, 2019; Pires et al., 2019). A natural follow-up question to ask is why these models learn cross-lingual representations. Some answers include the shared subword vocabulary (Pires et al., 2019; Wu and Dredze, 2019), shared Transformer layers (Conneau et al., 2020b; Artetxe et al., 2020) across languages, and depth of the network (K et al., 2020). Studies have also found the geometry of representations of different languages in the multilingual encoders can be aligned with linear transformations (Schuster et al., 2019; Wang et al., 2019e, 2020c; Liu et al., 2019b), which has also been observed in independent monolingual encoders (Conneau et al., 2020b). These alignments can be further improved (Cao et al., 2020b). 7.1 Evaluating Multilinguality All of the areas discussed in this paper are applicable to multilingual encoders. However, progress in training, architecture, datase"
2020.emnlp-main.608,D19-1539,0,0.094854,"ext word has historically been equivalent to the task of language modeling. Large language models perform impressively on a variety of language understanding tasks while maintaining their generative capabilities (Radford et al., 2018, 2019; Keskar et al., 2019; Brown et al., 2020), often outperforming contemporaneous models that use additional training objectives. ELMo (Peters et al., 2018) is a BiLSTM model with a language modeling objective for the next (or previous) token given the forward (or backward) history. This idea of looking at the full context was further refined as a cloze3 task (Baevski et al., 2019), or as a denoising Masked Language Modeling (MLM) objective (Devlin et al., 2019, BERT). MLM replaces some tokens with a [mask] symbol and provides both right and left contexts (bidirectional context) for predicting the masked tokens. The bidirectionality is key to outperforming a unidirectional language model on a large suite of natural language understanding benchmarks (Devlin et al., 2019; Raffel et al., 2019). The MLM objective is far from perfect, as the use of [mask] introduces a pretrain/finetune vo7517 2 3 See Raffel et al. (2019) for comprehensive experiments. A cloze task is a fill-"
2020.emnlp-main.608,Q19-1004,0,0.0275005,"ed data can contain incorrect facts about living people; while webpages can be edited or retracted, publicly released “language” model are frozen, which can raise privacy concerns (Feyisetan et al., 2020). 6 Area IV: Interpretability While it is clear that the performance of text encoders surpass human baselines, it is less clear what knowledge is stored in these models; how do they make decisions? In their survey, Rogers et al. (2020) find answers to the first question and also 7521 12 How was the data generated, curated, and processed? raise the second. Inspired by prior work (Lipton, 2018; Belinkov and Glass, 2019; Alishahi et al., 2019), we organize here the major probing methods that are applicable to all encoders in hopes that future work will use comparable techniques. 6.1 Probing with Tasks One technique uses the learned model as initialization for a model trained on a probing task consisting of a set of targeted natural language examples. The probing task’s format is flexible as additional, (simple) diagnostic classifiers are trained on top of a typically frozen model (Ettinger et al., 2016; Hupkes et al., 2018; Poliak et al., 2018; Tenney et al., 2019b). Task probing can also be applied to the e"
2020.emnlp-main.608,2020.acl-main.463,0,0.162743,"2019; Sun et al., 2019b). This idea can be simplified to random spans of texts (Yang et al., 2019; Song et al., 2019). Specifically, Joshi et al. (2020) add a reconstruction objective which predicts the masked tokens using only the span boundaries. They find that masking random spans is more effective than masking linguistic units. An alternative architecture uses an encoderdecoder framework (or denoising autoencoder) where the input is a corrupted (masked) sequence the output is the full original sequence (Wang et al., 2019d; Lewis et al., 2020; Raffel et al., 2019). 3.2 Nontoken Prediction Bender and Koller (2020) argue that for the goal of natural language understanding, we cannot rely purely on a language modeling objective; there must be some grounding or external information that relates the text to each other or to the world. One solution is to introduce a secondary objective to directly learn these biases. Self-supervised discourse structure objectives, such as text order, has garnered significant attention. To capture relationships between two sentences,5 Devlin et al. (2019) introduce the next 4 5 Clark et al. (2020) report negative results for rarer words. Sentence unfortunately refers to a te"
2020.emnlp-main.608,2020.acl-main.411,0,0.0813069,"(Liu et al., 2018a) and Transformer (Fan et al., 2020) based encoders. These also have a regularization effect resulting in more stable training and improved performance. There are additional novel pruning methods that can be performed during training (Guo et al., 2019; Qiu et al., 2019). These successful results are corroborated by other efforts (Gordon et al., 2020) showing that low levels of pruning do not substantially affect pretrained representations. Additional successful efforts in model pruning directly target a downstream task (Sun et al., 2019a; Michel et al., 2019; McCarley, 2019; Cao et al., 2020a). Note that pruning does not always lead to speedups in practice as sparse operations may be hard to parallelize. Knowledge distillation (KD) uses an overparameterized teacher model to rapidly train a smaller student model with minimal loss in performance (Hinton et al., 2015) and has been used for translation (Kim and Rush, 2016), computer vision (Howard et al., 2017), and adversarial examples (Carlini and Wagner, 2016). This has been applied to ELMo (Li et al., 2019) and BERT (Tang et al., 2019; Sanh et al., 2019; Sun et al., 2020b, inter alia). KD can also be combined with adaptive infere"
2020.emnlp-main.608,2020.acl-main.747,0,0.0454328,"Missing"
2020.emnlp-main.608,N19-1357,0,0.066778,"Missing"
2020.emnlp-main.608,2020.acl-main.197,0,0.0219162,"Missing"
2020.emnlp-main.608,2020.tacl-1.5,0,0.0223057,"hitecturally (Song et al., 2020; Bao et al., 2020) and mathematically (Kong et al., 2020). ELECTRA (Clark et al., 2020) replaces [mask] through the use of a small generator (trained with MLM) to sample a real token from the vocabulary. The main encoder, a discriminator, then determines whether each token was replaced. A natural extension would mask units that are more linguistically meaningful, such as rarer words,4 whole words, or named entities (Devlin et al., 2019; Sun et al., 2019b). This idea can be simplified to random spans of texts (Yang et al., 2019; Song et al., 2019). Specifically, Joshi et al. (2020) add a reconstruction objective which predicts the masked tokens using only the span boundaries. They find that masking random spans is more effective than masking linguistic units. An alternative architecture uses an encoderdecoder framework (or denoising autoencoder) where the input is a corrupted (masked) sequence the output is the full original sequence (Wang et al., 2019d; Lewis et al., 2020; Raffel et al., 2019). 3.2 Nontoken Prediction Bender and Koller (2020) argue that for the goal of natural language understanding, we cannot rely purely on a language modeling objective; there must be"
2020.emnlp-main.608,D17-1082,0,0.0270858,"bi et al., 2018; Zhang et al., 2018; Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo 1 Unlike traditional word-level tokenization, most works decompose text into subtokens from a fixed vocabulary using some variation of byte pair encoding (Gage, 1994; Schuster and Nakajima, 2012; Sennrich et al., 2016) et al., 2007; Bentivogli et al., 2009; Pilehvar and Camacho-Collados, 2019; Rudinger et al., 2018; Poliak et al., 2018; Levesque et al., 2011); (2) crowdsourced questions derived from Wikipedia articles (Rajpurkar et al., 2016, 2018, SQuAD); and (3) multiple-choice reading comprehension (Lai et al., 2017, RACE). 3 Area I: Pretraining Tasks To utilize data at scale, pretraining tasks are typically self-supervised. We categorize the contributions into two types: token prediction (over a large vocabulary space) and nontoken prediction (over a handful of labels). In this section, we discuss several empirical observations. While token prediction is clearly important, less clear is which variation of the token prediction task is the best (or whether it even matters). Nontoken prediction tasks appear to offer orthogonal contributions that marginally improve the language representations. We emphasize"
2020.emnlp-main.608,D16-1139,0,0.0320905,"Missing"
2020.emnlp-main.608,Q19-1039,0,0.0374153,"Missing"
2020.emnlp-main.608,W19-4825,0,0.052677,"Missing"
2020.emnlp-main.608,D18-1153,0,0.0240175,"be cautious in drawing conclusions from these methods. 6.2 Model Inspection Model inspection directly opens the metaphorical black box and studies the model weights without additional training. For examples, the embeddings themselves can be analyzed as points in a vector space (Ethayarajh, 2019). Through visualization, attention heads have been matched to linguistic functions (Vig, 2019; Clark et al., 2019b). These works suggest inspection is a viable path to debugging specific examples. In the future, methods for analyzing and manipulating attention in machine translation (Lee et al., 2017; Liu et al., 2018b; Bau et al., 2019; Voita et al., 2019) can also be applied to text encoders. Recently, interpreting attention as explanation has been questioned (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Clark et al., 2019b). The ongoing discussion suggests that this method may still be insufficient for uncovering the rationale for predictions, which is critical for real-world applications. 6.3 Input Manipulation13 Input manipulation draws conclusions by recasting the probing task format into the form of the pre13 This is analogous to the “few-shot“ and “zero-shot” analysi"
2020.emnlp-main.608,N19-1112,0,0.121407,"ined language model outputs as initialization, McCann et al. (2017) use pretrained outputs from translation as frozen word embeddings, and Howard and Ruder (2018) and Radford et al. (2018) demonstrate the effectiveness of finetuning to different target tasks by updating the full (pretrained) model for each task. We refer to the embeddings produced by the pretrained models (or encoders) as contextualized text representations. As our goal is to discuss the encoders and their representations, we do not cover the innovations in finetuning (Liu et al., 2015; Ruder et al., 2019; Phang et al., 2018; Liu et al., 2019c; Zhu et al., 2020, inter alia). Evaluation Widely adopted evaluations of text representations relate them to downstream natural language understanding (NLU) benchmarks. This full-stack process necessarily conflates representation power with finetuning strategies. Common language understanding benchmarks include (1) a diverse suite of sentence-level tasks covering paraphrasing, natural language inference, sentiment, and linguistic acceptability (GLUE) and its more challenging counterpart with additional commonsense and linguistic reasoning tasks (SuperGLUE) (Wang et al., 2019c,b; Clark et al."
2020.emnlp-main.608,K19-1004,0,0.114098,"Missing"
2020.emnlp-main.608,D18-2007,0,0.0147861,"be cautious in drawing conclusions from these methods. 6.2 Model Inspection Model inspection directly opens the metaphorical black box and studies the model weights without additional training. For examples, the embeddings themselves can be analyzed as points in a vector space (Ethayarajh, 2019). Through visualization, attention heads have been matched to linguistic functions (Vig, 2019; Clark et al., 2019b). These works suggest inspection is a viable path to debugging specific examples. In the future, methods for analyzing and manipulating attention in machine translation (Lee et al., 2017; Liu et al., 2018b; Bau et al., 2019; Voita et al., 2019) can also be applied to text encoders. Recently, interpreting attention as explanation has been questioned (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Clark et al., 2019b). The ongoing discussion suggests that this method may still be insufficient for uncovering the rationale for predictions, which is critical for real-world applications. 6.3 Input Manipulation13 Input manipulation draws conclusions by recasting the probing task format into the form of the pre13 This is analogous to the “few-shot“ and “zero-shot” analysi"
2020.emnlp-main.608,2020.acl-main.537,0,0.137961,"ted, and present considerations for practitioners and researchers when choosing an encoder. This survey does not intend to compare specific model metrics, as tables from other works provide comprehensive insight. For example, Table 16 in Raffel et al. (2019) compares the scores on a large suite of tasks of different model architectures, training objectives, and hyperparameters, and Table 1 in Rogers et al. (2020) details early efforts in model compression and distillation. We also recommend other closely related surveys on contextualized word representations (Smith, 2019; Rogers et al., 2020; Liu et al., 2020a), transfer learning in NLP (Ruder et al., 2019), and integrating encoders into NLP applications (Wolf et al., 2019). Complementing these existing bodies of work, we look at the ideas and progress in the scientific discourse for text representations from the perspective of discerning their differences. We organize this paper as follows. §2 provides brief background on encoding, training, and evaluating text representations. §3 identifies and analyzes two classes of pretraining objectives. In §4, we explore faster and smaller models and architectures in both training and inference. §5 notes th"
2020.emnlp-main.608,N15-1092,0,0.0249739,"c models. Dai and Le (2015) are credited with using pretrained language model outputs as initialization, McCann et al. (2017) use pretrained outputs from translation as frozen word embeddings, and Howard and Ruder (2018) and Radford et al. (2018) demonstrate the effectiveness of finetuning to different target tasks by updating the full (pretrained) model for each task. We refer to the embeddings produced by the pretrained models (or encoders) as contextualized text representations. As our goal is to discuss the encoders and their representations, we do not cover the innovations in finetuning (Liu et al., 2015; Ruder et al., 2019; Phang et al., 2018; Liu et al., 2019c; Zhu et al., 2020, inter alia). Evaluation Widely adopted evaluations of text representations relate them to downstream natural language understanding (NLU) benchmarks. This full-stack process necessarily conflates representation power with finetuning strategies. Common language understanding benchmarks include (1) a diverse suite of sentence-level tasks covering paraphrasing, natural language inference, sentiment, and linguistic acceptability (GLUE) and its more challenging counterpart with additional commonsense and linguistic reaso"
2020.emnlp-main.608,P19-1441,0,0.0766356,"ined language model outputs as initialization, McCann et al. (2017) use pretrained outputs from translation as frozen word embeddings, and Howard and Ruder (2018) and Radford et al. (2018) demonstrate the effectiveness of finetuning to different target tasks by updating the full (pretrained) model for each task. We refer to the embeddings produced by the pretrained models (or encoders) as contextualized text representations. As our goal is to discuss the encoders and their representations, we do not cover the innovations in finetuning (Liu et al., 2015; Ruder et al., 2019; Phang et al., 2018; Liu et al., 2019c; Zhu et al., 2020, inter alia). Evaluation Widely adopted evaluations of text representations relate them to downstream natural language understanding (NLU) benchmarks. This full-stack process necessarily conflates representation power with finetuning strategies. Common language understanding benchmarks include (1) a diverse suite of sentence-level tasks covering paraphrasing, natural language inference, sentiment, and linguistic acceptability (GLUE) and its more challenging counterpart with additional commonsense and linguistic reasoning tasks (SuperGLUE) (Wang et al., 2019c,b; Clark et al."
2020.emnlp-main.608,2020.tacl-1.47,0,0.196122,"ted, and present considerations for practitioners and researchers when choosing an encoder. This survey does not intend to compare specific model metrics, as tables from other works provide comprehensive insight. For example, Table 16 in Raffel et al. (2019) compares the scores on a large suite of tasks of different model architectures, training objectives, and hyperparameters, and Table 1 in Rogers et al. (2020) details early efforts in model compression and distillation. We also recommend other closely related surveys on contextualized word representations (Smith, 2019; Rogers et al., 2020; Liu et al., 2020a), transfer learning in NLP (Ruder et al., 2019), and integrating encoders into NLP applications (Wolf et al., 2019). Complementing these existing bodies of work, we look at the ideas and progress in the scientific discourse for text representations from the perspective of discerning their differences. We organize this paper as follows. §2 provides brief background on encoding, training, and evaluating text representations. §3 identifies and analyzes two classes of pretraining objectives. In §4, we explore faster and smaller models and architectures in both training and inference. §5 notes th"
2020.emnlp-main.608,2021.ccl-1.108,0,0.0819911,"Missing"
2020.emnlp-main.608,N19-1063,0,0.0254244,"pecially for deployed models. We are not aware of a comprehensive study that explores the effect of leaving out targeted subsets of the pretraining data. We hope future models note the domains of pretraining and evaluation benchmarks, and for future language understanding benchmarks to focus on more diverse genres in addition to diverse tasks. As we improve models by training on increasing sizes of crawled data, these models are also being picked up by NLP practitioners who deploy them in real-world software. These models learn biases found in their pretraining data (Gonen and Goldberg, 2019; May et al., 2019, inter alia). It is critical to clearly state the source12 of the pretraining data and clarify appropriate uses of the released models. For example, crawled data can contain incorrect facts about living people; while webpages can be edited or retracted, publicly released “language” model are frozen, which can raise privacy concerns (Feyisetan et al., 2020). 6 Area IV: Interpretability While it is clear that the performance of text encoders surpass human baselines, it is less clear what knowledge is stored in these models; how do they make decisions? In their survey, Rogers et al. (2020) find"
2020.emnlp-main.608,P10-2041,0,0.0240542,"0). We are not aware of any work which probes identical models trained with decreasingly less data. How much (and which) data is necessary for high performance on probing tasks?11 5.2 Data Quality While text encoders should be trained on language, large-scale datasets may contain web-scraped and uncurated content (like code). Raffel et al. (2019) ablate different types of data for text representations and find that naively increasing dataset size does not always improve performance, partially due to data quality. This realization is not new. Parallel data and alignment in machine translation (Moore and Lewis, 2010; Duh et al., 2013; Xu and Koehn, 2017; Koehn et al., 2018, inter alia) and speech (Peddinti et al., 2016) often use language models to filter out misaligned or poor data. Sun et al. (2017) use automatic data filtering in vision. These successes on other tasks suggest that improved automated methods of data cleaning would let future models consume more high-quality data. In addition to high quality, data uniqueness appears to be advantageous. Raffel et al. (2019) show that increasing the repetitions (number of epochs) of the pretraining corpus hurts performance. This is corroborated by Liu et"
2020.emnlp-main.608,N19-1392,0,0.0211092,"ld look more across (simpler) models to see how and why specific knowledge is picked up as our models both become increasingly complex and perform better on a wide set of tasks. For example, how many parameters does a Transformerbased model need to outperform ELMo or even rule-based baselines? 7522 14 A definition is given in §3 of Bender and Koller (2020). 7 Area V: Multilinguality The majority of research on text encoders has been in English.15 Cross-lingual shared representations have been proposed as an efficient way to target multiple languages by using multilingual text for pretraining (Mulcaire et al., 2019; Devlin et al., 2019; Lample and Conneau, 2019; Liu et al., 2020c, inter alia). For evaluation, researchers have devised multilingual benchmarks mirroring those for NLU in English (Conneau et al., 2018b; Liang et al., 2020; Hu et al., 2020). Surprisingly, without any explicit cross-lingual signal, these models achieve strong zero-shot cross-lingual performance, outperforming prior cross-lingual word embedding-based methods (Wu and Dredze, 2019; Pires et al., 2019). A natural follow-up question to ask is why these models learn cross-lingual representations. Some answers include the shared subw"
2020.emnlp-main.608,K16-1028,0,0.0692066,"Missing"
2020.emnlp-main.608,P19-1442,0,0.0160763,"into the set of pretraining tasks from word to sentence to document level tasks. Encoders using visual features (and evaluated only on visual tasks) jointly optimize multiple different masking objectives over both token sequences and regions of interests in the image (Tan and Bansal, 2019).6 Prior to token prediction, discourse information has been used in training sentence representations. Conneau et al. (2017, 2018a) use natural language inference sentence pairs, Jernite et al. (2017) use discourse-based objectives of sentence order, conjunction classifier, and next sentence selection, and Nie et al. (2019) use discourse markers. While there is weak evidence suggesting that these types of objectives are less effective than language modeling (Wang et al., 2019a), we lack fair studies comparing the relative influence between the two categories of objectives. 3.3 Comments on Evaluation We reviewed the progress on pretraining tasks, finding that token prediction is powerful but can be improved further by other objectives. Currently, successful techniques like span masking or arbitrarily sized “sentences” are linguistically unmotivated. We anticipate future work to further incorporate no more than a"
2020.emnlp-main.608,P16-1144,0,0.0290026,"Missing"
2020.emnlp-main.608,N18-1202,0,0.340467,"etrained contextualized text encoders are now a staple of the NLP community. We present a survey on language representation learning with the aim of consolidating a series of shared lessons learned across a variety of recent efforts. While significant advancements continue at a rapid pace, we find that enough has now been discovered, in different directions, that we can begin to organize advances according to common themes. Through this organization, we highlight important considerations when interpreting recent contributions and choosing which model to use. 1 Introduction A couple years ago, Peters et al. (2018, ELMo) won the NAACL Best Paper Award for creating strong performing, task-agnostic sentence representations due to large scale unsupervised pretraining. Days later, its high level of performance was surpassed by Radford et al. (2018) which boasted representations beyond a single sentence and finetuning flexibility. This instability and competition between models has been a recurring theme for researchers and practitioners who have watched the rapidly narrowing gap between text representations and language understanding benchmarks. However, it has not discouraged research. Given the recent fl"
2020.emnlp-main.608,N19-1128,0,0.0351551,"Missing"
2020.emnlp-main.608,P19-1493,0,0.0345912,"representations have been proposed as an efficient way to target multiple languages by using multilingual text for pretraining (Mulcaire et al., 2019; Devlin et al., 2019; Lample and Conneau, 2019; Liu et al., 2020c, inter alia). For evaluation, researchers have devised multilingual benchmarks mirroring those for NLU in English (Conneau et al., 2018b; Liang et al., 2020; Hu et al., 2020). Surprisingly, without any explicit cross-lingual signal, these models achieve strong zero-shot cross-lingual performance, outperforming prior cross-lingual word embedding-based methods (Wu and Dredze, 2019; Pires et al., 2019). A natural follow-up question to ask is why these models learn cross-lingual representations. Some answers include the shared subword vocabulary (Pires et al., 2019; Wu and Dredze, 2019), shared Transformer layers (Conneau et al., 2020b; Artetxe et al., 2020) across languages, and depth of the network (K et al., 2020). Studies have also found the geometry of representations of different languages in the multilingual encoders can be aligned with linear transformations (Schuster et al., 2019; Wang et al., 2019e, 2020c; Liu et al., 2019b), which has also been observed in independent monolingual"
2020.emnlp-main.608,W18-5441,1,0.824289,"Missing"
2020.emnlp-main.608,2020.acl-demos.15,0,0.0163211,"release (data) ablated versions of their models. We also raise a concern about reproducibility and accessibility of evaluation. Already, several papers focused on model compression do not report full GLUE results, possibly due to the expensive finetuning process for each of the nine datasets. Finetuning currently requires additional compute and infrastructure,17 and the specific methods used impact task performance. As long as finetuning is still an essential component of evaluating encoders, de16 An EMNLP 2020 workshop is motivated by better science (https://insights-workshop.github.io/). 17 Pruksachatkun et al. (2020) is a library that reduces some infrastructural overhead of finetuning. 7523 vising cheap, accessible, and reproducible metrics for encoders is an open problem. Ribeiro et al. (2020) suggest a practical solution to both probing model errors and reproducible evaluations by creating tools that quickly generate test cases for linguistic capabilities and find bugs in models. This task-agnostic methodology may be extensible to both challenging tasks and probing specific linguistic phenomenon. 8.2 Which *BERT should we use? Here, we discuss tradeoffs between metrics and synthesize the previous secti"
2020.emnlp-main.608,P18-2124,0,0.0501841,"Missing"
2020.emnlp-main.608,D16-1264,0,0.15336,"2019c,b; Clark et al., 2019a; De Marneffe et al., 2019; Roemmele et al., 2011; Khashabi et al., 2018; Zhang et al., 2018; Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo 1 Unlike traditional word-level tokenization, most works decompose text into subtokens from a fixed vocabulary using some variation of byte pair encoding (Gage, 1994; Schuster and Nakajima, 2012; Sennrich et al., 2016) et al., 2007; Bentivogli et al., 2009; Pilehvar and Camacho-Collados, 2019; Rudinger et al., 2018; Poliak et al., 2018; Levesque et al., 2011); (2) crowdsourced questions derived from Wikipedia articles (Rajpurkar et al., 2016, 2018, SQuAD); and (3) multiple-choice reading comprehension (Lai et al., 2017, RACE). 3 Area I: Pretraining Tasks To utilize data at scale, pretraining tasks are typically self-supervised. We categorize the contributions into two types: token prediction (over a large vocabulary space) and nontoken prediction (over a handful of labels). In this section, we discuss several empirical observations. While token prediction is clearly important, less clear is which variation of the token prediction task is the best (or whether it even matters). Nontoken prediction tasks appear to offer orthogonal c"
2020.emnlp-main.608,2020.acl-main.442,0,0.0125832,"report full GLUE results, possibly due to the expensive finetuning process for each of the nine datasets. Finetuning currently requires additional compute and infrastructure,17 and the specific methods used impact task performance. As long as finetuning is still an essential component of evaluating encoders, de16 An EMNLP 2020 workshop is motivated by better science (https://insights-workshop.github.io/). 17 Pruksachatkun et al. (2020) is a library that reduces some infrastructural overhead of finetuning. 7523 vising cheap, accessible, and reproducible metrics for encoders is an open problem. Ribeiro et al. (2020) suggest a practical solution to both probing model errors and reproducible evaluations by creating tools that quickly generate test cases for linguistic capabilities and find bugs in models. This task-agnostic methodology may be extensible to both challenging tasks and probing specific linguistic phenomenon. 8.2 Which *BERT should we use? Here, we discuss tradeoffs between metrics and synthesize the previous sections. We provide a series of questions to consider when working with encoders for research or application development. Task performance vs. efficiency An increasingly popular line of"
2020.emnlp-main.608,N19-5004,0,0.181415,"ers and researchers when choosing an encoder. This survey does not intend to compare specific model metrics, as tables from other works provide comprehensive insight. For example, Table 16 in Raffel et al. (2019) compares the scores on a large suite of tasks of different model architectures, training objectives, and hyperparameters, and Table 1 in Rogers et al. (2020) details early efforts in model compression and distillation. We also recommend other closely related surveys on contextualized word representations (Smith, 2019; Rogers et al., 2020; Liu et al., 2020a), transfer learning in NLP (Ruder et al., 2019), and integrating encoders into NLP applications (Wolf et al., 2019). Complementing these existing bodies of work, we look at the ideas and progress in the scientific discourse for text representations from the perspective of discerning their differences. We organize this paper as follows. §2 provides brief background on encoding, training, and evaluating text representations. §3 identifies and analyzes two classes of pretraining objectives. In §4, we explore faster and smaller models and architectures in both training and inference. §5 notes the impact of both quality and quantity of pretrain"
2020.emnlp-main.608,N18-2002,1,0.795325,"Missing"
2020.emnlp-main.608,N19-1162,0,0.020179,"-lingual performance, outperforming prior cross-lingual word embedding-based methods (Wu and Dredze, 2019; Pires et al., 2019). A natural follow-up question to ask is why these models learn cross-lingual representations. Some answers include the shared subword vocabulary (Pires et al., 2019; Wu and Dredze, 2019), shared Transformer layers (Conneau et al., 2020b; Artetxe et al., 2020) across languages, and depth of the network (K et al., 2020). Studies have also found the geometry of representations of different languages in the multilingual encoders can be aligned with linear transformations (Schuster et al., 2019; Wang et al., 2019e, 2020c; Liu et al., 2019b), which has also been observed in independent monolingual encoders (Conneau et al., 2020b). These alignments can be further improved (Cao et al., 2020b). 7.1 Evaluating Multilinguality All of the areas discussed in this paper are applicable to multilingual encoders. However, progress in training, architecture, datasets, and evaluations are occurring concurrently, making it difficult to draw conclusions. We need more comparisons between competitive multilingual and monolingual systems or datasets. To this end, Wu and Dredze (2020) find that monolin"
2020.emnlp-main.608,P19-1355,0,0.0184862,"se interested in the best performance should first carefully investigate metrics on their specific task. Even if models are finetuned on an older encoder,7 it may be more cost-efficient and enable fairer future comparisons to reuse those over restarting the finetuning or reintegrating new encoders into existing models when doing so does not necessarily guarantee improved performance. 4 Area II: Efficiency As models perform better but cost more to train, some have called for research into efficient models to improve deployability, accessibility, and reproducibility (Amodei and Hernandez, 2018; Strubell et al., 2019; Schwartz et al., 2019). Encoders tend to scale effectively (Lan et al., 2020; Raffel et al., 2019; Brown et al., 2020), so efficient models will also result in improvements over inefficient ones of the same size. In this section, we give an overview of several efforts aimed to decrease the computation budget (time and memory usage) during training and inference of text encoders. While these two axes are correlated, reductions in one axis do not always lead to reductions in the other. 4.1 Training One area of research decreases wall-clock training time through more compute and larger batches."
2020.emnlp-main.608,P19-1032,0,0.0142162,"oduce smaller and faster models, while occasionally improving performance. Rogers et al. (2020) survey BERT-like models and present in Table 1 the differences in sizes and performance across several models focused on inference efficiency. Architectural changes have been explored as one avenue for reducing either the model size or inference time. In Transformers, the self-attention pattern scales quadratically in sequence length. To reduce the asymptotic complexity, the self-attention can be sparsified: each token only attending to a small “local” set (Vaswani et al., 2017; Child et al., 2019; Sukhbaatar et al., 2019). This has further been applied to pretraining on longer sequences, resulting in sparse contextualized encoders (Qiu et al., 2019; Ye et al., 2019; Kitaev et al., 2020; Beltagy et al., 2020, inter alia). Efficient Transformers is an emerging subfield with applications beyond NLP; Tay et al. (2020) survey 17 Transformers that have implications on efficiency. Another class of approaches carefully selects weights to reduce model size. Lan et al. (2020) use low-rank factorization to reduce the size of the embedding matrices, while Wang et al. (2019f) factorize other weight matrices. Additionally,"
2020.emnlp-main.608,P16-1162,0,0.00739612,"tasks covering paraphrasing, natural language inference, sentiment, and linguistic acceptability (GLUE) and its more challenging counterpart with additional commonsense and linguistic reasoning tasks (SuperGLUE) (Wang et al., 2019c,b; Clark et al., 2019a; De Marneffe et al., 2019; Roemmele et al., 2011; Khashabi et al., 2018; Zhang et al., 2018; Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo 1 Unlike traditional word-level tokenization, most works decompose text into subtokens from a fixed vocabulary using some variation of byte pair encoding (Gage, 1994; Schuster and Nakajima, 2012; Sennrich et al., 2016) et al., 2007; Bentivogli et al., 2009; Pilehvar and Camacho-Collados, 2019; Rudinger et al., 2018; Poliak et al., 2018; Levesque et al., 2011); (2) crowdsourced questions derived from Wikipedia articles (Rajpurkar et al., 2016, 2018, SQuAD); and (3) multiple-choice reading comprehension (Lai et al., 2017, RACE). 3 Area I: Pretraining Tasks To utilize data at scale, pretraining tasks are typically self-supervised. We categorize the contributions into two types: token prediction (over a large vocabulary space) and nontoken prediction (over a handful of labels). In this section, we discuss sever"
2020.emnlp-main.608,D19-1441,0,0.0486959,"Missing"
2020.emnlp-main.608,P19-1282,0,0.0265159,"s without additional training. For examples, the embeddings themselves can be analyzed as points in a vector space (Ethayarajh, 2019). Through visualization, attention heads have been matched to linguistic functions (Vig, 2019; Clark et al., 2019b). These works suggest inspection is a viable path to debugging specific examples. In the future, methods for analyzing and manipulating attention in machine translation (Lee et al., 2017; Liu et al., 2018b; Bau et al., 2019; Voita et al., 2019) can also be applied to text encoders. Recently, interpreting attention as explanation has been questioned (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Clark et al., 2019b). The ongoing discussion suggests that this method may still be insufficient for uncovering the rationale for predictions, which is critical for real-world applications. 6.3 Input Manipulation13 Input manipulation draws conclusions by recasting the probing task format into the form of the pre13 This is analogous to the “few-shot“ and “zero-shot” analysis in Brown et al. (2020). training task and observing the model’s predictions. As discussed in §3, word prediction (cloze task) is a popular objective. This method has bee"
2020.emnlp-main.608,2020.acl-main.195,0,0.0120162,"dicting whether A is before, after, or unrelated to B, and Wang et al. (2020b) and Lan et al. (2020) use it for pretraining encoders. They report that (1) understanding text order does contribute to improved language understanding; and (2) harder-to-learn pretraining objectives are more powerful, as both modified tasks have lower intrinsic performance than NSP. It is still unclear, however, if this is the best way to incorporate discourse structure, especially since these works do not use real sentences. Additional work has focused on effectively incorporating multiple pretraining objectives. Sun et al. (2020a) use multi-task learning with continual pretraining (Hashimoto et al., 2017), which incrementally introduces newer tasks into the set of pretraining tasks from word to sentence to document level tasks. Encoders using visual features (and evaluated only on visual tasks) jointly optimize multiple different masking objectives over both token sequences and regions of interests in the image (Tan and Bansal, 2019).6 Prior to token prediction, discourse information has been used in training sentence representations. Conneau et al. (2017, 2018a) use natural language inference sentence pairs, Jernite"
2020.emnlp-main.608,D19-1514,0,0.0157334,"best way to incorporate discourse structure, especially since these works do not use real sentences. Additional work has focused on effectively incorporating multiple pretraining objectives. Sun et al. (2020a) use multi-task learning with continual pretraining (Hashimoto et al., 2017), which incrementally introduces newer tasks into the set of pretraining tasks from word to sentence to document level tasks. Encoders using visual features (and evaluated only on visual tasks) jointly optimize multiple different masking objectives over both token sequences and regions of interests in the image (Tan and Bansal, 2019).6 Prior to token prediction, discourse information has been used in training sentence representations. Conneau et al. (2017, 2018a) use natural language inference sentence pairs, Jernite et al. (2017) use discourse-based objectives of sentence order, conjunction classifier, and next sentence selection, and Nie et al. (2019) use discourse markers. While there is weak evidence suggesting that these types of objectives are less effective than language modeling (Wang et al., 2019a), we lack fair studies comparing the relative influence between the two categories of objectives. 3.3 Comments on Eva"
2020.emnlp-main.608,D19-6122,0,0.0433119,"Missing"
2020.emnlp-main.608,P19-1452,0,0.0223769,". Inspired by prior work (Lipton, 2018; Belinkov and Glass, 2019; Alishahi et al., 2019), we organize here the major probing methods that are applicable to all encoders in hopes that future work will use comparable techniques. 6.1 Probing with Tasks One technique uses the learned model as initialization for a model trained on a probing task consisting of a set of targeted natural language examples. The probing task’s format is flexible as additional, (simple) diagnostic classifiers are trained on top of a typically frozen model (Ettinger et al., 2016; Hupkes et al., 2018; Poliak et al., 2018; Tenney et al., 2019b). Task probing can also be applied to the embeddings at various layers to explore the knowledge captured at each layer (Tenney et al., 2019a; Lin et al., 2019; Liu et al., 2019a). Hewitt and Liang (2019) warn that expressive (nonlinear) diagnostic classifiers can learn more arbitrary information than constrained (linear) ones. This revelation, combined with the differences in probing task format and the need to train, leads us to be cautious in drawing conclusions from these methods. 6.2 Model Inspection Model inspection directly opens the metaphorical black box and studies the model weights"
2020.emnlp-main.608,P19-3007,0,0.0199295,"expressive (nonlinear) diagnostic classifiers can learn more arbitrary information than constrained (linear) ones. This revelation, combined with the differences in probing task format and the need to train, leads us to be cautious in drawing conclusions from these methods. 6.2 Model Inspection Model inspection directly opens the metaphorical black box and studies the model weights without additional training. For examples, the embeddings themselves can be analyzed as points in a vector space (Ethayarajh, 2019). Through visualization, attention heads have been matched to linguistic functions (Vig, 2019; Clark et al., 2019b). These works suggest inspection is a viable path to debugging specific examples. In the future, methods for analyzing and manipulating attention in machine translation (Lee et al., 2017; Liu et al., 2018b; Bau et al., 2019; Voita et al., 2019) can also be applied to text encoders. Recently, interpreting attention as explanation has been questioned (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Clark et al., 2019b). The ongoing discussion suggests that this method may still be insufficient for uncovering the rationale for predictions, which"
2020.emnlp-main.608,D19-1448,0,0.0207949,"rom these methods. 6.2 Model Inspection Model inspection directly opens the metaphorical black box and studies the model weights without additional training. For examples, the embeddings themselves can be analyzed as points in a vector space (Ethayarajh, 2019). Through visualization, attention heads have been matched to linguistic functions (Vig, 2019; Clark et al., 2019b). These works suggest inspection is a viable path to debugging specific examples. In the future, methods for analyzing and manipulating attention in machine translation (Lee et al., 2017; Liu et al., 2018b; Bau et al., 2019; Voita et al., 2019) can also be applied to text encoders. Recently, interpreting attention as explanation has been questioned (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Clark et al., 2019b). The ongoing discussion suggests that this method may still be insufficient for uncovering the rationale for predictions, which is critical for real-world applications. 6.3 Input Manipulation13 Input manipulation draws conclusions by recasting the probing task format into the form of the pre13 This is analogous to the “few-shot“ and “zero-shot” analysis in Brown et al. (2020). training task"
2020.emnlp-main.608,P19-1439,1,0.893476,"Missing"
2020.emnlp-main.608,D19-1412,0,0.122203,"ang et al., 2018; Liu et al., 2019c; Zhu et al., 2020, inter alia). Evaluation Widely adopted evaluations of text representations relate them to downstream natural language understanding (NLU) benchmarks. This full-stack process necessarily conflates representation power with finetuning strategies. Common language understanding benchmarks include (1) a diverse suite of sentence-level tasks covering paraphrasing, natural language inference, sentiment, and linguistic acceptability (GLUE) and its more challenging counterpart with additional commonsense and linguistic reasoning tasks (SuperGLUE) (Wang et al., 2019c,b; Clark et al., 2019a; De Marneffe et al., 2019; Roemmele et al., 2011; Khashabi et al., 2018; Zhang et al., 2018; Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo 1 Unlike traditional word-level tokenization, most works decompose text into subtokens from a fixed vocabulary using some variation of byte pair encoding (Gage, 1994; Schuster and Nakajima, 2012; Sennrich et al., 2016) et al., 2007; Bentivogli et al., 2009; Pilehvar and Camacho-Collados, 2019; Rudinger et al., 2018; Poliak et al., 2018; Levesque et al., 2011); (2) crowdsourced questions derived from Wikipedia articles (Rajp"
2020.emnlp-main.608,2020.acl-main.200,0,0.192524,"t attention. To capture relationships between two sentences,5 Devlin et al. (2019) introduce the next 4 5 Clark et al. (2020) report negative results for rarer words. Sentence unfortunately refers to a text segment containing sentence prediction (NSP) objective. In this task, either sentence B follows sentence A or B is a random negative sample. Subsequent works showed that this was not effective, suggesting the model simply learned topic (Yang et al., 2019; Liu et al., 2019d). Jernite et al. (2017) propose a sentence order task of predicting whether A is before, after, or unrelated to B, and Wang et al. (2020b) and Lan et al. (2020) use it for pretraining encoders. They report that (1) understanding text order does contribute to improved language understanding; and (2) harder-to-learn pretraining objectives are more powerful, as both modified tasks have lower intrinsic performance than NSP. It is still unclear, however, if this is the best way to incorporate discourse structure, especially since these works do not use real sentences. Additional work has focused on effectively incorporating multiple pretraining objectives. Sun et al. (2020a) use multi-task learning with continual pretraining (Hashi"
2020.emnlp-main.608,D19-1575,0,0.181821,"ang et al., 2018; Liu et al., 2019c; Zhu et al., 2020, inter alia). Evaluation Widely adopted evaluations of text representations relate them to downstream natural language understanding (NLU) benchmarks. This full-stack process necessarily conflates representation power with finetuning strategies. Common language understanding benchmarks include (1) a diverse suite of sentence-level tasks covering paraphrasing, natural language inference, sentiment, and linguistic acceptability (GLUE) and its more challenging counterpart with additional commonsense and linguistic reasoning tasks (SuperGLUE) (Wang et al., 2019c,b; Clark et al., 2019a; De Marneffe et al., 2019; Roemmele et al., 2011; Khashabi et al., 2018; Zhang et al., 2018; Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo 1 Unlike traditional word-level tokenization, most works decompose text into subtokens from a fixed vocabulary using some variation of byte pair encoding (Gage, 1994; Schuster and Nakajima, 2012; Sennrich et al., 2016) et al., 2007; Bentivogli et al., 2009; Pilehvar and Camacho-Collados, 2019; Rudinger et al., 2018; Poliak et al., 2018; Levesque et al., 2011); (2) crowdsourced questions derived from Wikipedia articles (Rajp"
2020.emnlp-main.608,2020.emnlp-main.496,0,0.0679194,"Missing"
2020.emnlp-main.608,D19-1286,0,0.0257324,"ient for uncovering the rationale for predictions, which is critical for real-world applications. 6.3 Input Manipulation13 Input manipulation draws conclusions by recasting the probing task format into the form of the pre13 This is analogous to the “few-shot“ and “zero-shot” analysis in Brown et al. (2020). training task and observing the model’s predictions. As discussed in §3, word prediction (cloze task) is a popular objective. This method has been used to investigate syntactic and semantic knowledge (Goldberg, 2019; Ettinger, 2020; Kassner and Sch¨utze, 2019). For a specific probing task, Warstadt et al. (2019) show that cloze and diagnostic classifiers draw similar conclusions. As input manipulation is not affected by variables introduced by probing tasks and is as interpretable than inspection, we suggest more focus on this method: either by creating new datasets (Warstadt et al., 2020) or recasting existing ones (Brown et al., 2020) into this format. A disadvantage of this method (especially for smaller models) is the dependence on both the pattern used to elicit an answer from the model and, in the few-shot case where a couple examples are provided first, highly dependent on the examples (Schick"
2020.emnlp-main.608,2020.tacl-1.25,0,0.0109889,"s in Brown et al. (2020). training task and observing the model’s predictions. As discussed in §3, word prediction (cloze task) is a popular objective. This method has been used to investigate syntactic and semantic knowledge (Goldberg, 2019; Ettinger, 2020; Kassner and Sch¨utze, 2019). For a specific probing task, Warstadt et al. (2019) show that cloze and diagnostic classifiers draw similar conclusions. As input manipulation is not affected by variables introduced by probing tasks and is as interpretable than inspection, we suggest more focus on this method: either by creating new datasets (Warstadt et al., 2020) or recasting existing ones (Brown et al., 2020) into this format. A disadvantage of this method (especially for smaller models) is the dependence on both the pattern used to elicit an answer from the model and, in the few-shot case where a couple examples are provided first, highly dependent on the examples (Schick and Sch¨utze, 2020). 6.4 Future Directions in Model Analysis Most probing efforts have relied on diagnostic classifiers, yet these results are being questioned. Inspection of model weights has discovered what the models learn, but cannot explain their causal structure. We suggest r"
2020.emnlp-main.608,2020.lrec-1.494,0,0.0309642,"Missing"
2020.emnlp-main.608,D19-1002,0,0.0182493,"embeddings themselves can be analyzed as points in a vector space (Ethayarajh, 2019). Through visualization, attention heads have been matched to linguistic functions (Vig, 2019; Clark et al., 2019b). These works suggest inspection is a viable path to debugging specific examples. In the future, methods for analyzing and manipulating attention in machine translation (Lee et al., 2017; Liu et al., 2018b; Bau et al., 2019; Voita et al., 2019) can also be applied to text encoders. Recently, interpreting attention as explanation has been questioned (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Clark et al., 2019b). The ongoing discussion suggests that this method may still be insufficient for uncovering the rationale for predictions, which is critical for real-world applications. 6.3 Input Manipulation13 Input manipulation draws conclusions by recasting the probing task format into the form of the pre13 This is analogous to the “few-shot“ and “zero-shot” analysis in Brown et al. (2020). training task and observing the model’s predictions. As discussed in §3, word prediction (cloze task) is a popular objective. This method has been used to investigate syntactic and semantic knowled"
2020.emnlp-main.608,2020.emnlp-main.633,0,0.042485,"Missing"
2020.emnlp-main.608,D17-1319,0,0.0120329,"obes identical models trained with decreasingly less data. How much (and which) data is necessary for high performance on probing tasks?11 5.2 Data Quality While text encoders should be trained on language, large-scale datasets may contain web-scraped and uncurated content (like code). Raffel et al. (2019) ablate different types of data for text representations and find that naively increasing dataset size does not always improve performance, partially due to data quality. This realization is not new. Parallel data and alignment in machine translation (Moore and Lewis, 2010; Duh et al., 2013; Xu and Koehn, 2017; Koehn et al., 2018, inter alia) and speech (Peddinti et al., 2016) often use language models to filter out misaligned or poor data. Sun et al. (2017) use automatic data filtering in vision. These successes on other tasks suggest that improved automated methods of data cleaning would let future models consume more high-quality data. In addition to high quality, data uniqueness appears to be advantageous. Raffel et al. (2019) show that increasing the repetitions (number of epochs) of the pretraining corpus hurts performance. This is corroborated by Liu et al. (2019d), who find that random, uni"
2020.emnlp-main.608,D19-1077,1,0.851096,"Cross-lingual shared representations have been proposed as an efficient way to target multiple languages by using multilingual text for pretraining (Mulcaire et al., 2019; Devlin et al., 2019; Lample and Conneau, 2019; Liu et al., 2020c, inter alia). For evaluation, researchers have devised multilingual benchmarks mirroring those for NLU in English (Conneau et al., 2018b; Liang et al., 2020; Hu et al., 2020). Surprisingly, without any explicit cross-lingual signal, these models achieve strong zero-shot cross-lingual performance, outperforming prior cross-lingual word embedding-based methods (Wu and Dredze, 2019; Pires et al., 2019). A natural follow-up question to ask is why these models learn cross-lingual representations. Some answers include the shared subword vocabulary (Pires et al., 2019; Wu and Dredze, 2019), shared Transformer layers (Conneau et al., 2020b; Artetxe et al., 2020) across languages, and depth of the network (K et al., 2020). Studies have also found the geometry of representations of different languages in the multilingual encoders can be aligned with linear transformations (Schuster et al., 2019; Wang et al., 2019e, 2020c; Liu et al., 2019b), which has also been observed in ind"
2020.emnlp-main.608,2020.repl4nlp-1.16,1,0.836104,"transformations (Schuster et al., 2019; Wang et al., 2019e, 2020c; Liu et al., 2019b), which has also been observed in independent monolingual encoders (Conneau et al., 2020b). These alignments can be further improved (Cao et al., 2020b). 7.1 Evaluating Multilinguality All of the areas discussed in this paper are applicable to multilingual encoders. However, progress in training, architecture, datasets, and evaluations are occurring concurrently, making it difficult to draw conclusions. We need more comparisons between competitive multilingual and monolingual systems or datasets. To this end, Wu and Dredze (2020) find that monolingual BERTs in low-resource languages are outperformed by multilingual BERT. Additionally, as zero-shot (or few-shot) cross-lingual transfer has inherently high variance (Keung et al., 2020), the variance of models should also be reported. We anticipate cross-lingual performance being a new dimension to consider when evaluating text representations. For example, it will be exciting to discover how a small, highly-performant mono15 Of the monolingual encoders in other languages, core research in modeling has only been performed so far for a few non-English languages (Sun et al."
2020.emnlp-main.612,N13-1104,0,0.0749217,"th the results of the human evaluation, give further evidence towards the relative robustness of the Causal model in extracting informative core events. The precipitous drop in performance of the LM further underscores problems that a naive automatic cloze evaluation may cover up. 5 Related Work Our work looks at script like associations between events in a manner similar to Chambers and Jurafsky (2008), and works along similar lines (Jans et al., 2012; Pichotta and Mooney, 2016). Related lines of work exist, such as work using generative models to induce probabilistic schemas(Chambers, 2013; Cheung et al., 2013; Ferraro and Van Durme, 2016), work showing how script knowledge may be mined from user elicited event sequences (Regneri et al., 2010; Orr et al., 2014), and approaches take advantage of hand coded schematic knowledge (Mooney and DeJong, 1985; Raskin et al., 2003). The cognitive linguistics literature is rich with work studying the role of causal semantics in linguistic constructions and argument structure (Talmy, 1988; Croft, 1991, 2012), as well as the causal semantics of lexical items themselves (Wolff and Song, 2003). Work in the NLP literature on extracting causal relations has benefite"
2020.emnlp-main.612,W03-1210,0,0.129539,"al., 2014), and approaches take advantage of hand coded schematic knowledge (Mooney and DeJong, 1985; Raskin et al., 2003). The cognitive linguistics literature is rich with work studying the role of causal semantics in linguistic constructions and argument structure (Talmy, 1988; Croft, 1991, 2012), as well as the causal semantics of lexical items themselves (Wolff and Song, 2003). Work in the NLP literature on extracting causal relations has benefited from this line of work, utilizing the systematic way in which causation in expressed in language to mine relations (Girju and Moldovan, 2002; Girju, 2003; Riaz and Girju, 2013; Blanco et al., 2008; Do et al., 2011; Bosselut et al., 2019). This line work aims to extract 7590 Method Causal LM PMI &lt;0 5.60 65.3 1.80 &lt; 50 7.10 28.1 3.30 Exclusion Threshold &lt; 100 &lt; 125 &lt; 150 7.00 7.49 7.20 9.70 6.30 3.60 3.36 4.10 4.00 &lt; 200 8.20 1.70 4.90 &lt; 500 9.10 0.25 7.00 Table 6: Recall@100 Narrative Cloze Results. &lt; C indicates that instances whose cloze answer is one of the top C most frequent events are not evaluated on causal relations between events that are in some way explicitly expressed in the text (e.g. through the use of particular constructions).Ta"
2020.emnlp-main.612,W04-3205,0,0.0809127,"from pD = p(Di−1 i i−1 , Di−1 , Ti−1 ), then we would have access to a dataset with samples drawn from the full joint. In order to ‘draw’ samples from pD we employ human annotation. Annotators are presented with I ,T 9 a human readable form of (ei , ei−1 , Di−1 i−1 ) and are asked to annotate for possible events beO . Rather than opt for noisy annotalonging in Di−1 tions obtained via freeform elicitation, we instead provide users with a set of 6 candidate choices for O . The candidates are obtained members of Di−1 from various knowledge sources: ConceptNet (Speer and Havasi, 2012), VerbOcean (Chklovski and Pantel, 2004), and high PMI events from the NYT Gigaword corpus (Graff et al., 2003). The top two candidates are selected from each source. In a scheme similar to Zhang et al. (2017), we ask users to rate candidates on an ordinal scale and consider candidates rated at or above a 3 (out of 4) O . We found annotator to be considered within Di−1 agreement to be quite high, with a Krippendorf’s α of 0.79. Under this scheme, we crowdsourced a dataset of 2000 fully annotated examples on the Mechanical Turk platform. An image of our annotation interface is provided in the Appendix. The Conditional Model We use ne"
2020.emnlp-main.612,J90-1003,0,0.506167,"Missing"
2020.emnlp-main.612,D16-1245,0,0.0280822,"Missing"
2020.emnlp-main.612,P16-1061,0,0.0379642,"Missing"
2020.emnlp-main.612,E12-1034,0,0.224224,"Missing"
2020.emnlp-main.612,2020.acl-main.474,0,0.094048,"Missing"
2020.emnlp-main.612,P14-5010,0,0.00531774,"Missing"
2020.emnlp-main.612,K16-1008,0,0.209241,"Missing"
2020.emnlp-main.612,W14-1606,0,0.0170828,"rs and Jurafsky (2008; 2009) adopted point-wise mutual information (PMI) (Church and 1 For simplicity we will refer to these ‘prototypical event sequences’ as scripts throughout the paper, though it should be noted scripts as originally proposed contain further structure not captured in this definition. Hanks, 1990) between event mentions. Others employed probabilities from a language model over event sequences (Jans et al., 2012; Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016; Weber et al., 2018b), or other measures of event co-occurrence (Balasubramanian et al., 2013; Modi and Titov, 2014). In this work we ask: do measures rooted in cooccurrence best capture the notion of whether one event should follow another in a script? We posit that it does not, that while observed correlations between events indicate relatedness, relatedness is not the only factor in determining whether events form a meaningful script. Consider the example of Ge et al. (2016): hurricane events are prototypically connected with events of donations coming in. Likewise, hurricane events are connected to evacuation events. However, while donation and evacuation events are not conceptually connected in the sam"
2020.emnlp-main.612,1985.tmi-1.17,0,0.146038,"automatic cloze evaluation may cover up. 5 Related Work Our work looks at script like associations between events in a manner similar to Chambers and Jurafsky (2008), and works along similar lines (Jans et al., 2012; Pichotta and Mooney, 2016). Related lines of work exist, such as work using generative models to induce probabilistic schemas(Chambers, 2013; Cheung et al., 2013; Ferraro and Van Durme, 2016), work showing how script knowledge may be mined from user elicited event sequences (Regneri et al., 2010; Orr et al., 2014), and approaches take advantage of hand coded schematic knowledge (Mooney and DeJong, 1985; Raskin et al., 2003). The cognitive linguistics literature is rich with work studying the role of causal semantics in linguistic constructions and argument structure (Talmy, 1988; Croft, 1991, 2012), as well as the causal semantics of lexical items themselves (Wolff and Song, 2003). Work in the NLP literature on extracting causal relations has benefited from this line of work, utilizing the systematic way in which causation in expressed in language to mine relations (Girju and Moldovan, 2002; Girju, 2003; Riaz and Girju, 2013; Blanco et al., 2008; Do et al., 2011; Bosselut et al., 2019). Thi"
2020.emnlp-main.612,N04-1041,0,0.0276825,"and Jurafsky (2008, 2009)) and LMs over event sequences (Rudinger et al., 2015; Pichotta and Mooney, 2016). We defer definitions for these models to the cited papers, below we provide the relevant details for each baseline, with further training details provided in the Appendix. For computing PMI we follow many of the details from (Jans et al., 2012). Due to the nature of the evaluations, we utilize their ‘ordered’ PMIvariant. Also like Jans et al. (2012), we use skipbigrams with a window of 2 to deal with count sparsity. Consistent with prior work we additionally employ the discount score of Pantel and Ravichandran (2004). For the LM, we use a standard, 2 layer, GRU-based neural network language model, with 512 dimensional hidden states, trained on a log-likelihood objective. Average Score 49.71 35.95 34.92 Average Rank (1-6) 4.10 3.39 3.02 Table 1: Average Annotator Scores in Pairwise annotation experiment Causal X tripped X lit X aimed X poured X radioed LM X came X sat X came X nodded X made PMI X featured X laboured X alarmed X credited X fostered Target X fell X inhaled X fired X refilled X ordered Table 2: Examples from each system, each of which outputs a previous event that maximizes the score/likeliho"
2020.emnlp-main.612,P16-1028,0,0.109753,"we compute over the raw presence or absence of events in documents that is most useful for script induction? Chambers and Jurafsky (2008; 2009) adopted point-wise mutual information (PMI) (Church and 1 For simplicity we will refer to these ‘prototypical event sequences’ as scripts throughout the paper, though it should be noted scripts as originally proposed contain further structure not captured in this definition. Hanks, 1990) between event mentions. Others employed probabilities from a language model over event sequences (Jans et al., 2012; Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016; Weber et al., 2018b), or other measures of event co-occurrence (Balasubramanian et al., 2013; Modi and Titov, 2014). In this work we ask: do measures rooted in cooccurrence best capture the notion of whether one event should follow another in a script? We posit that it does not, that while observed correlations between events indicate relatedness, relatedness is not the only factor in determining whether events form a meaningful script. Consider the example of Ge et al. (2016): hurricane events are prototypically connected with events of donations coming in. Likewise, hurricane events are co"
2020.emnlp-main.612,W12-4501,0,0.0158492,"Missing"
2020.emnlp-main.612,W03-0905,0,0.159262,"on may cover up. 5 Related Work Our work looks at script like associations between events in a manner similar to Chambers and Jurafsky (2008), and works along similar lines (Jans et al., 2012; Pichotta and Mooney, 2016). Related lines of work exist, such as work using generative models to induce probabilistic schemas(Chambers, 2013; Cheung et al., 2013; Ferraro and Van Durme, 2016), work showing how script knowledge may be mined from user elicited event sequences (Regneri et al., 2010; Orr et al., 2014), and approaches take advantage of hand coded schematic knowledge (Mooney and DeJong, 1985; Raskin et al., 2003). The cognitive linguistics literature is rich with work studying the role of causal semantics in linguistic constructions and argument structure (Talmy, 1988; Croft, 1991, 2012), as well as the causal semantics of lexical items themselves (Wolff and Song, 2003). Work in the NLP literature on extracting causal relations has benefited from this line of work, utilizing the systematic way in which causation in expressed in language to mine relations (Girju and Moldovan, 2002; Girju, 2003; Riaz and Girju, 2013; Blanco et al., 2008; Do et al., 2011; Bosselut et al., 2019). This line work aims to ex"
2020.emnlp-main.612,P10-1100,0,0.0410613,"ative core events. The precipitous drop in performance of the LM further underscores problems that a naive automatic cloze evaluation may cover up. 5 Related Work Our work looks at script like associations between events in a manner similar to Chambers and Jurafsky (2008), and works along similar lines (Jans et al., 2012; Pichotta and Mooney, 2016). Related lines of work exist, such as work using generative models to induce probabilistic schemas(Chambers, 2013; Cheung et al., 2013; Ferraro and Van Durme, 2016), work showing how script knowledge may be mined from user elicited event sequences (Regneri et al., 2010; Orr et al., 2014), and approaches take advantage of hand coded schematic knowledge (Mooney and DeJong, 1985; Raskin et al., 2003). The cognitive linguistics literature is rich with work studying the role of causal semantics in linguistic constructions and argument structure (Talmy, 1988; Croft, 1991, 2012), as well as the causal semantics of lexical items themselves (Wolff and Song, 2003). Work in the NLP literature on extracting causal relations has benefited from this line of work, utilizing the systematic way in which causation in expressed in language to mine relations (Girju and Moldova"
2020.emnlp-main.612,W13-4004,0,0.0221693,"nd approaches take advantage of hand coded schematic knowledge (Mooney and DeJong, 1985; Raskin et al., 2003). The cognitive linguistics literature is rich with work studying the role of causal semantics in linguistic constructions and argument structure (Talmy, 1988; Croft, 1991, 2012), as well as the causal semantics of lexical items themselves (Wolff and Song, 2003). Work in the NLP literature on extracting causal relations has benefited from this line of work, utilizing the systematic way in which causation in expressed in language to mine relations (Girju and Moldovan, 2002; Girju, 2003; Riaz and Girju, 2013; Blanco et al., 2008; Do et al., 2011; Bosselut et al., 2019). This line work aims to extract 7590 Method Causal LM PMI &lt;0 5.60 65.3 1.80 &lt; 50 7.10 28.1 3.30 Exclusion Threshold &lt; 100 &lt; 125 &lt; 150 7.00 7.49 7.20 9.70 6.30 3.60 3.36 4.10 4.00 &lt; 200 8.20 1.70 4.90 &lt; 500 9.10 0.25 7.00 Table 6: Recall@100 Narrative Cloze Results. &lt; C indicates that instances whose cloze answer is one of the top C most frequent events are not evaluated on causal relations between events that are in some way explicitly expressed in the text (e.g. through the use of particular constructions).Taking advantage of how"
2020.emnlp-main.612,D15-1195,1,0.94423,"Missing"
2020.emnlp-main.612,D18-1114,1,0.881378,"Missing"
2020.emnlp-main.612,N18-1067,1,0.876942,"Missing"
2020.emnlp-main.612,P18-1020,1,0.797399,"Missing"
2020.emnlp-main.612,speer-havasi-2012-representing,0,0.0146716,"t, we could draw samples O |e , e I from pD = p(Di−1 i i−1 , Di−1 , Ti−1 ), then we would have access to a dataset with samples drawn from the full joint. In order to ‘draw’ samples from pD we employ human annotation. Annotators are presented with I ,T 9 a human readable form of (ei , ei−1 , Di−1 i−1 ) and are asked to annotate for possible events beO . Rather than opt for noisy annotalonging in Di−1 tions obtained via freeform elicitation, we instead provide users with a set of 6 candidate choices for O . The candidates are obtained members of Di−1 from various knowledge sources: ConceptNet (Speer and Havasi, 2012), VerbOcean (Chklovski and Pantel, 2004), and high PMI events from the NYT Gigaword corpus (Graff et al., 2003). The top two candidates are selected from each source. In a scheme similar to Zhang et al. (2017), we ask users to rate candidates on an ordinal scale and consider candidates rated at or above a 3 (out of 4) O . We found annotator to be considered within Di−1 agreement to be quite high, with a Krippendorf’s α of 0.79. Under this scheme, we crowdsourced a dataset of 2000 fully annotated examples on the Mechanical Turk platform. An image of our annotation interface is provided in the A"
2020.emnlp-main.612,N03-1033,0,0.0234654,"Missing"
2020.emnlp-main.612,D18-1413,1,0.88652,"aw presence or absence of events in documents that is most useful for script induction? Chambers and Jurafsky (2008; 2009) adopted point-wise mutual information (PMI) (Church and 1 For simplicity we will refer to these ‘prototypical event sequences’ as scripts throughout the paper, though it should be noted scripts as originally proposed contain further structure not captured in this definition. Hanks, 1990) between event mentions. Others employed probabilities from a language model over event sequences (Jans et al., 2012; Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016; Weber et al., 2018b), or other measures of event co-occurrence (Balasubramanian et al., 2013; Modi and Titov, 2014). In this work we ask: do measures rooted in cooccurrence best capture the notion of whether one event should follow another in a script? We posit that it does not, that while observed correlations between events indicate relatedness, relatedness is not the only factor in determining whether events form a meaningful script. Consider the example of Ge et al. (2016): hurricane events are prototypically connected with events of donations coming in. Likewise, hurricane events are connected to evacuatio"
2020.emnlp-main.612,D16-1177,1,0.898949,"Missing"
2020.emnlp-main.612,D18-1488,0,0.161379,"Missing"
2020.emnlp-main.612,Q17-1027,1,0.927583,"Missing"
2020.emnlp-main.612,D11-1027,0,\N,Missing
2020.emnlp-main.612,P08-1090,0,\N,Missing
2020.emnlp-main.612,P09-1068,0,\N,Missing
2020.emnlp-main.612,D13-1185,0,\N,Missing
2020.emnlp-main.612,D14-1082,0,\N,Missing
2020.emnlp-main.612,D13-1178,0,\N,Missing
2020.emnlp-main.612,L16-1262,0,\N,Missing
2020.emnlp-main.612,W17-0905,0,\N,Missing
2020.emnlp-main.612,P19-1470,0,\N,Missing
2020.emnlp-main.612,blanco-etal-2008-causal,0,\N,Missing
2020.emnlp-main.695,P14-1035,0,0.0839304,"Missing"
2020.emnlp-main.695,D16-1245,0,0.0810519,"Missing"
2020.emnlp-main.695,P16-1061,0,0.0466598,"nd information extraction. At the sentence level, ambiguities in pronoun coreference can be used to probe a model for common sense (Levesque et al., 2012; Sakaguchi et al., 2020) or gender biases (Rudinger et al., 2018; Zhao et al., 2018). At the document level, coreference resolution is commonly used in information extraction pipelines, but can be applied to reading comprehension (Dasigi et al., 2019) or literature analysis (Bamman et al., 2014). Models for this task typically encode the entire text before scoring and subsequently clustering candidate mention spans, either found by a parser (Clark and Manning, 2016b) or learned jointly (Lee et al., 2017). Prior work has primarily focused on improving pairwise span scoring functions (Raghunathan et al., 2010; Clark and Manning, 2016a; Wu et al., 2020) and methods for decoding into globally consistent clusters (Wiseman et al., 2016; Lee et al., 2018; Kantor and Globerson, 2019; Xu and Choi, 2020). Recent models have also benefited from pretrained encoders used to create high-dimensional input text (and span) representations, and improvements in contextualized encoders appear to translate directly to coreference resolution (Lee et al., 2018; Joshi et al.,"
2020.emnlp-main.695,D19-1606,0,0.0715814,"Missing"
2020.emnlp-main.695,N19-1423,0,0.0444494,"PDATE(top e, m) else A DD NEW ENTITY(E, m) E VICT(E) return E Our algorithm revisits the approach taken by Webster and Curran (2014) for incrementally making coreference resolution decisions (online clustering). The major differences lie in explicit entity representations, neural components, and learning. Baseline First, we summarize the coreference resolution model described by Joshi et al. (2019), which itself extends from earlier work (Lee et al., 2017, 2018). For each document, this model enumerates and scores all spans up to a chosen width. The span representations are formed using BERT (Devlin et al., 2019) encodings of input text by concatenating the first, last, and an attention-weighted average of the token representations within the span. These spans are ranked and pruned to the top Θ(n) mentions. Both the maximum span width and fraction of remaining spans are hyperparameters. For each remaining span, the model learns a distribution over its possible antecedents (via a pairwise scorer) and the training objective maximizes the probability of its gold labeled antecedents. The entire model (including finetuning the encoder) is trained end-to-end over OntoNotes 5.0. This model is further improve"
2020.emnlp-main.695,2020.tacl-1.5,0,0.39169,"dings of input text by concatenating the first, last, and an attention-weighted average of the token representations within the span. These spans are ranked and pruned to the top Θ(n) mentions. Both the maximum span width and fraction of remaining spans are hyperparameters. For each remaining span, the model learns a distribution over its possible antecedents (via a pairwise scorer) and the training objective maximizes the probability of its gold labeled antecedents. The entire model (including finetuning the encoder) is trained end-to-end over OntoNotes 5.0. This model is further improved by Joshi et al. (2020), who introduces SpanBERT and uses it as the underlying encoder instead. The SpanBERTlarge version of Joshi et al. (2019) is the baseline model used in this paper. Inference Our method (Algorithm 1) stores a permanent list of entities (clusters), each with its own representation. For a given sentence or segment, the model proposes a candidate set of spans. For each span, a scorer scores the span representation against all the cluster representations. This is used to determine to which (if any) of the pre-existing clusters the current span should be added. Upon inclusion of the span in the clus"
2020.emnlp-main.695,D19-1588,0,0.317657,"Manning, 2016b) or learned jointly (Lee et al., 2017). Prior work has primarily focused on improving pairwise span scoring functions (Raghunathan et al., 2010; Clark and Manning, 2016a; Wu et al., 2020) and methods for decoding into globally consistent clusters (Wiseman et al., 2016; Lee et al., 2018; Kantor and Globerson, 2019; Xu and Choi, 2020). Recent models have also benefited from pretrained encoders used to create high-dimensional input text (and span) representations, and improvements in contextualized encoders appear to translate directly to coreference resolution (Lee et al., 2018; Joshi et al., 2019, 2020). These models typically rely on simultaneous access to all spans – Θ(n) for a document with length n – for scoring and all scores – up to Θ(n2 ) – for decoding. As the dimensionality of contextualized encoders, and therefore the size of span representations, increases, this becomes computationally intractable for long documents or under limited memory. Given these constraints, expensive scoring functions are increasingly difficult to explore. Further, prior models depart from how humans incrementally read and reason about coreferent mentions; Webster and Curran (2014) argue in favor of"
2020.emnlp-main.695,P19-1066,0,0.182395,"tion extraction pipelines, but can be applied to reading comprehension (Dasigi et al., 2019) or literature analysis (Bamman et al., 2014). Models for this task typically encode the entire text before scoring and subsequently clustering candidate mention spans, either found by a parser (Clark and Manning, 2016b) or learned jointly (Lee et al., 2017). Prior work has primarily focused on improving pairwise span scoring functions (Raghunathan et al., 2010; Clark and Manning, 2016a; Wu et al., 2020) and methods for decoding into globally consistent clusters (Wiseman et al., 2016; Lee et al., 2018; Kantor and Globerson, 2019; Xu and Choi, 2020). Recent models have also benefited from pretrained encoders used to create high-dimensional input text (and span) representations, and improvements in contextualized encoders appear to translate directly to coreference resolution (Lee et al., 2018; Joshi et al., 2019, 2020). These models typically rely on simultaneous access to all spans – Θ(n) for a document with length n – for scoring and all scores – up to Θ(n2 ) – for decoding. As the dimensionality of contextualized encoders, and therefore the size of span representations, increases, this becomes computationally intra"
2020.emnlp-main.695,D17-1018,0,0.620658,"el, ambiguities in pronoun coreference can be used to probe a model for common sense (Levesque et al., 2012; Sakaguchi et al., 2020) or gender biases (Rudinger et al., 2018; Zhao et al., 2018). At the document level, coreference resolution is commonly used in information extraction pipelines, but can be applied to reading comprehension (Dasigi et al., 2019) or literature analysis (Bamman et al., 2014). Models for this task typically encode the entire text before scoring and subsequently clustering candidate mention spans, either found by a parser (Clark and Manning, 2016b) or learned jointly (Lee et al., 2017). Prior work has primarily focused on improving pairwise span scoring functions (Raghunathan et al., 2010; Clark and Manning, 2016a; Wu et al., 2020) and methods for decoding into globally consistent clusters (Wiseman et al., 2016; Lee et al., 2018; Kantor and Globerson, 2019; Xu and Choi, 2020). Recent models have also benefited from pretrained encoders used to create high-dimensional input text (and span) representations, and improvements in contextualized encoders appear to translate directly to coreference resolution (Lee et al., 2018; Joshi et al., 2019, 2020). These models typically rely"
2020.emnlp-main.695,N18-2108,0,0.383415,"Missing"
2020.emnlp-main.695,H05-1004,0,0.270332,"wswire, magazines, weblogs, and the Bible). Implementation We use the model dimensions and training hyperparameters from the baseline model, a publicly available coreference resolution model by Joshi et al. (2019, 2020). We also reuse their (trained) parameters for the encoder, span 5 The implementation of Joshi et al. (2020, 2019) was the most amenable to extension and experimentation and therefore serves as our illustrative example. Results 4.1 Performance Table 1 presents the OntoNotes 5.0 test set scores for the metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005) using the official CoNLL-2012 scorer. We reevaluated the baseline, and we report the scores for CorefQA directly from Wu et al. (2020). We observe a small drop in performance compared to the baseline and apparently no drop with eviction. 4.2 Document Length Our goal is a constant-memory model that is comparable to the baseline. We showed above that our model is competitive with and without eviction, the key to constant memory. In Table 2, we report the average F1 broken down based on the length (in subtokens)6 of the document and number of speakers. Our model is competitive on most document s"
2020.emnlp-main.695,D10-1048,0,0.0669479,"al., 2012; Sakaguchi et al., 2020) or gender biases (Rudinger et al., 2018; Zhao et al., 2018). At the document level, coreference resolution is commonly used in information extraction pipelines, but can be applied to reading comprehension (Dasigi et al., 2019) or literature analysis (Bamman et al., 2014). Models for this task typically encode the entire text before scoring and subsequently clustering candidate mention spans, either found by a parser (Clark and Manning, 2016b) or learned jointly (Lee et al., 2017). Prior work has primarily focused on improving pairwise span scoring functions (Raghunathan et al., 2010; Clark and Manning, 2016a; Wu et al., 2020) and methods for decoding into globally consistent clusters (Wiseman et al., 2016; Lee et al., 2018; Kantor and Globerson, 2019; Xu and Choi, 2020). Recent models have also benefited from pretrained encoders used to create high-dimensional input text (and span) representations, and improvements in contextualized encoders appear to translate directly to coreference resolution (Lee et al., 2018; Joshi et al., 2019, 2020). These models typically rely on simultaneous access to all spans – Θ(n) for a document with length n – for scoring and all scores – u"
2020.emnlp-main.695,N18-2002,1,0.87159,"Missing"
2020.emnlp-main.695,M95-1005,0,0.391245,"d telephone conversations) and those without (broadcast news, newswire, magazines, weblogs, and the Bible). Implementation We use the model dimensions and training hyperparameters from the baseline model, a publicly available coreference resolution model by Joshi et al. (2019, 2020). We also reuse their (trained) parameters for the encoder, span 5 The implementation of Joshi et al. (2020, 2019) was the most amenable to extension and experimentation and therefore serves as our illustrative example. Results 4.1 Performance Table 1 presents the OntoNotes 5.0 test set scores for the metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005) using the official CoNLL-2012 scorer. We reevaluated the baseline, and we report the scores for CorefQA directly from Wu et al. (2020). We observe a small drop in performance compared to the baseline and apparently no drop with eviction. 4.2 Document Length Our goal is a constant-memory model that is comparable to the baseline. We showed above that our model is competitive with and without eviction, the key to constant memory. In Table 2, we report the average F1 broken down based on the length (in subtokens)6 of the document and number of"
2020.emnlp-main.695,C14-1201,0,0.0951762,"ution (Lee et al., 2018; Joshi et al., 2019, 2020). These models typically rely on simultaneous access to all spans – Θ(n) for a document with length n – for scoring and all scores – up to Θ(n2 ) – for decoding. As the dimensionality of contextualized encoders, and therefore the size of span representations, increases, this becomes computationally intractable for long documents or under limited memory. Given these constraints, expensive scoring functions are increasingly difficult to explore. Further, prior models depart from how humans incrementally read and reason about coreferent mentions; Webster and Curran (2014) argue in favor of a limited memory constraint as a more psycholinguistically plausible approach to reading and model coreference resolution via shift-reduce parsing. Motivated by scalability and armed with advances in neural architectures, we revisit that intuition. Following prior work, our model begins with a SpanBERT encoding of a text segment to form a list of proposed mention spans (Joshi et al., 2019, 2020). Clustering is performed online: each span either attaches to an existing cluster or begins a new one. We substantially minimize memory usage during inference by storing only the emb"
2020.emnlp-main.695,N16-1114,0,0.137195,"Missing"
2020.emnlp-main.695,2020.acl-main.622,0,0.552768,"(Rudinger et al., 2018; Zhao et al., 2018). At the document level, coreference resolution is commonly used in information extraction pipelines, but can be applied to reading comprehension (Dasigi et al., 2019) or literature analysis (Bamman et al., 2014). Models for this task typically encode the entire text before scoring and subsequently clustering candidate mention spans, either found by a parser (Clark and Manning, 2016b) or learned jointly (Lee et al., 2017). Prior work has primarily focused on improving pairwise span scoring functions (Raghunathan et al., 2010; Clark and Manning, 2016a; Wu et al., 2020) and methods for decoding into globally consistent clusters (Wiseman et al., 2016; Lee et al., 2018; Kantor and Globerson, 2019; Xu and Choi, 2020). Recent models have also benefited from pretrained encoders used to create high-dimensional input text (and span) representations, and improvements in contextualized encoders appear to translate directly to coreference resolution (Lee et al., 2018; Joshi et al., 2019, 2020). These models typically rely on simultaneous access to all spans – Θ(n) for a document with length n – for scoring and all scores – up to Θ(n2 ) – for decoding. As the dimension"
2020.emnlp-main.695,2020.emnlp-main.686,0,0.602195,"ut can be applied to reading comprehension (Dasigi et al., 2019) or literature analysis (Bamman et al., 2014). Models for this task typically encode the entire text before scoring and subsequently clustering candidate mention spans, either found by a parser (Clark and Manning, 2016b) or learned jointly (Lee et al., 2017). Prior work has primarily focused on improving pairwise span scoring functions (Raghunathan et al., 2010; Clark and Manning, 2016a; Wu et al., 2020) and methods for decoding into globally consistent clusters (Wiseman et al., 2016; Lee et al., 2018; Kantor and Globerson, 2019; Xu and Choi, 2020). Recent models have also benefited from pretrained encoders used to create high-dimensional input text (and span) representations, and improvements in contextualized encoders appear to translate directly to coreference resolution (Lee et al., 2018; Joshi et al., 2019, 2020). These models typically rely on simultaneous access to all spans – Θ(n) for a document with length n – for scoring and all scores – up to Θ(n2 ) – for decoding. As the dimensionality of contextualized encoders, and therefore the size of span representations, increases, this becomes computationally intractable for long docu"
2020.emnlp-main.695,N18-2003,0,0.0302762,"Missing"
2020.findings-emnlp.363,L18-1293,0,0.035144,"Missing"
2020.findings-emnlp.363,2021.ccl-1.108,0,0.178101,"Missing"
2020.findings-emnlp.363,S15-2134,0,0.0233489,"i and Varzi, 1996; Zacks and Tversky, 2001; Radvansky and Zacks, 2014), and modeling such temporal reasoning has been central to many classical AI approaches (McCarthy and Hayes, 1987; Kahn and Gorry, 1977; McDermott, 1982; Allen, 1984; Kowalski and Sergot, 1989; Pani and Bhattacharjee, 2001). Natural language supports various forms of temporal reasoning, including reasoning about the chronology and duration of events, and many Natural Language Understanding (NLU) tasks and models have been employed for understanding and capturing different aspects of temporal reasoning (UzZaman et al., 2013; Llorens et al., 2015; Mostafazadeh et al., 2016; Reimers et al., 2016; Tourille et al., 2017; Ning et al., 2017, 2018a; Meng and Rumshisky, 2018; Ning et al., 2018b; Han et al., 2019; Naik et al., 2019; Vashishtha et al., 2019; Zhou et al., 2019, 2020). More broadly, the ability to perform temporal reasoning is important for understanding narratives (Nakhimovsky, 1987; Jung et al., 2011; Cheng et al., 2013), answering questions (Bruce, 1972; Khashabi, 2019; Ning et al., 2020), and summarizing events (Jung et al., 2011; Wang et al., 2018). Given that temporal reasoning is integral to natural language understanding"
2020.findings-emnlp.363,P18-1049,0,0.10862,"ntral to many classical AI approaches (McCarthy and Hayes, 1987; Kahn and Gorry, 1977; McDermott, 1982; Allen, 1984; Kowalski and Sergot, 1989; Pani and Bhattacharjee, 2001). Natural language supports various forms of temporal reasoning, including reasoning about the chronology and duration of events, and many Natural Language Understanding (NLU) tasks and models have been employed for understanding and capturing different aspects of temporal reasoning (UzZaman et al., 2013; Llorens et al., 2015; Mostafazadeh et al., 2016; Reimers et al., 2016; Tourille et al., 2017; Ning et al., 2017, 2018a; Meng and Rumshisky, 2018; Ning et al., 2018b; Han et al., 2019; Naik et al., 2019; Vashishtha et al., 2019; Zhou et al., 2019, 2020). More broadly, the ability to perform temporal reasoning is important for understanding narratives (Nakhimovsky, 1987; Jung et al., 2011; Cheng et al., 2013), answering questions (Bruce, 1972; Khashabi, 2019; Ning et al., 2020), and summarizing events (Jung et al., 2011; Wang et al., 2018). Given that temporal reasoning is integral to natural language understanding (NLU) and that Natural Language Inference (NLI) is a common framework for evaluating how well models capture semantic pheno"
2020.findings-emnlp.363,W16-1007,0,0.0277985,"ks and Tversky, 2001; Radvansky and Zacks, 2014), and modeling such temporal reasoning has been central to many classical AI approaches (McCarthy and Hayes, 1987; Kahn and Gorry, 1977; McDermott, 1982; Allen, 1984; Kowalski and Sergot, 1989; Pani and Bhattacharjee, 2001). Natural language supports various forms of temporal reasoning, including reasoning about the chronology and duration of events, and many Natural Language Understanding (NLU) tasks and models have been employed for understanding and capturing different aspects of temporal reasoning (UzZaman et al., 2013; Llorens et al., 2015; Mostafazadeh et al., 2016; Reimers et al., 2016; Tourille et al., 2017; Ning et al., 2017, 2018a; Meng and Rumshisky, 2018; Ning et al., 2018b; Han et al., 2019; Naik et al., 2019; Vashishtha et al., 2019; Zhou et al., 2019, 2020). More broadly, the ability to perform temporal reasoning is important for understanding narratives (Nakhimovsky, 1987; Jung et al., 2011; Cheng et al., 2013), answering questions (Bruce, 1972; Khashabi, 2019; Ning et al., 2020), and summarizing events (Jung et al., 2011; Wang et al., 2018). Given that temporal reasoning is integral to natural language understanding (NLU) and that Natural Lan"
2020.findings-emnlp.363,P16-2022,0,0.0309644,"ng. Specifically, we use three types of models: (i) neural bag of words (NBOW; Iyyer et al., 2015) (ii) InferSent (Conneau et al., 2017), and (iii) RoBERTa (Liu et al., 2019).4 Our NBOW model represents contexts and hypotheses as an average of GloVe embeddings (Pennington et al., 2014). The concatenation of these representations is fed to a MLP with one hidden layer. The InferSent model encodes contexts and hypotheses independently with a BiLSTM and sentence representations are extracted using max-pooling. The concatenation of these sentences, their difference, and their element-wise product (Mou et al., 2016) are then fed to a MLP. For Roberta, we use a classification head on top of the pooled output of roberta-large to predict the labels.5 In our experiments, we train and test these models on each recast temporal dataset. For each model, we include a hypothesis-only baseline to evaluate how much the datasets test NLI as opposed to just the likely duration and order of events in general. Additionally, we train each model on Multi-genre NLI (MNLI, Williams et al., 2018) and test the model on our datasets to see if the model learns temporal reasoning from a generic NLI dataset that does not necessar"
2020.findings-emnlp.363,W19-5929,0,0.0549069,"Kahn and Gorry, 1977; McDermott, 1982; Allen, 1984; Kowalski and Sergot, 1989; Pani and Bhattacharjee, 2001). Natural language supports various forms of temporal reasoning, including reasoning about the chronology and duration of events, and many Natural Language Understanding (NLU) tasks and models have been employed for understanding and capturing different aspects of temporal reasoning (UzZaman et al., 2013; Llorens et al., 2015; Mostafazadeh et al., 2016; Reimers et al., 2016; Tourille et al., 2017; Ning et al., 2017, 2018a; Meng and Rumshisky, 2018; Ning et al., 2018b; Han et al., 2019; Naik et al., 2019; Vashishtha et al., 2019; Zhou et al., 2019, 2020). More broadly, the ability to perform temporal reasoning is important for understanding narratives (Nakhimovsky, 1987; Jung et al., 2011; Cheng et al., 2013), answering questions (Bruce, 1972; Khashabi, 2019; Ning et al., 2020), and summarizing events (Jung et al., 2011; Wang et al., 2018). Given that temporal reasoning is integral to natural language understanding (NLU) and that Natural Language Inference (NLI) is a common framework for evaluating how well models capture semantic phenomena integral to NLU (Cooper et al., 1996; Dagan et al.,"
2020.findings-emnlp.363,E87-1042,0,0.570666,"ing, including reasoning about the chronology and duration of events, and many Natural Language Understanding (NLU) tasks and models have been employed for understanding and capturing different aspects of temporal reasoning (UzZaman et al., 2013; Llorens et al., 2015; Mostafazadeh et al., 2016; Reimers et al., 2016; Tourille et al., 2017; Ning et al., 2017, 2018a; Meng and Rumshisky, 2018; Ning et al., 2018b; Han et al., 2019; Naik et al., 2019; Vashishtha et al., 2019; Zhou et al., 2019, 2020). More broadly, the ability to perform temporal reasoning is important for understanding narratives (Nakhimovsky, 1987; Jung et al., 2011; Cheng et al., 2013), answering questions (Bruce, 1972; Khashabi, 2019; Ning et al., 2020), and summarizing events (Jung et al., 2011; Wang et al., 2018). Given that temporal reasoning is integral to natural language understanding (NLU) and that Natural Language Inference (NLI) is a common framework for evaluating how well models capture semantic phenomena integral to NLU (Cooper et al., 1996; Dagan et al., 2006; White et al., 2017; Poliak et al., 2018), it is important to evaluate how well different classes of NLI models trained on common generic NLI datasets capture tempo"
2020.findings-emnlp.363,D17-1108,0,0.043143,"oral reasoning has been central to many classical AI approaches (McCarthy and Hayes, 1987; Kahn and Gorry, 1977; McDermott, 1982; Allen, 1984; Kowalski and Sergot, 1989; Pani and Bhattacharjee, 2001). Natural language supports various forms of temporal reasoning, including reasoning about the chronology and duration of events, and many Natural Language Understanding (NLU) tasks and models have been employed for understanding and capturing different aspects of temporal reasoning (UzZaman et al., 2013; Llorens et al., 2015; Mostafazadeh et al., 2016; Reimers et al., 2016; Tourille et al., 2017; Ning et al., 2017, 2018a; Meng and Rumshisky, 2018; Ning et al., 2018b; Han et al., 2019; Naik et al., 2019; Vashishtha et al., 2019; Zhou et al., 2019, 2020). More broadly, the ability to perform temporal reasoning is important for understanding narratives (Nakhimovsky, 1987; Jung et al., 2011; Cheng et al., 2013), answering questions (Bruce, 1972; Khashabi, 2019; Ning et al., 2020), and summarizing events (Jung et al., 2011; Wang et al., 2018). Given that temporal reasoning is integral to natural language understanding (NLU) and that Natural Language Inference (NLI) is a common framework for evaluating how w"
2020.findings-emnlp.363,P18-1212,0,0.194393,"Missing"
2020.findings-emnlp.363,2020.emnlp-main.88,0,0.201678,"(NLU) tasks and models have been employed for understanding and capturing different aspects of temporal reasoning (UzZaman et al., 2013; Llorens et al., 2015; Mostafazadeh et al., 2016; Reimers et al., 2016; Tourille et al., 2017; Ning et al., 2017, 2018a; Meng and Rumshisky, 2018; Ning et al., 2018b; Han et al., 2019; Naik et al., 2019; Vashishtha et al., 2019; Zhou et al., 2019, 2020). More broadly, the ability to perform temporal reasoning is important for understanding narratives (Nakhimovsky, 1987; Jung et al., 2011; Cheng et al., 2013), answering questions (Bruce, 1972; Khashabi, 2019; Ning et al., 2020), and summarizing events (Jung et al., 2011; Wang et al., 2018). Given that temporal reasoning is integral to natural language understanding (NLU) and that Natural Language Inference (NLI) is a common framework for evaluating how well models capture semantic phenomena integral to NLU (Cooper et al., 1996; Dagan et al., 2006; White et al., 2017; Poliak et al., 2018), it is important to evaluate how well different classes of NLI models trained on common generic NLI datasets capture temporal reasoning. We present five new NLI datasets recasted from four existing temporal reasoning datasets: (i) T"
2020.findings-emnlp.363,P18-1122,0,0.0362535,"approaches (McCarthy and Hayes, 1987; Kahn and Gorry, 1977; McDermott, 1982; Allen, 1984; Kowalski and Sergot, 1989; Pani and Bhattacharjee, 2001). Natural language supports various forms of temporal reasoning, including reasoning about the chronology and duration of events, and many Natural Language Understanding (NLU) tasks and models have been employed for understanding and capturing different aspects of temporal reasoning (UzZaman et al., 2013; Llorens et al., 2015; Mostafazadeh et al., 2016; Reimers et al., 2016; Tourille et al., 2017; Ning et al., 2017, 2018a; Meng and Rumshisky, 2018; Ning et al., 2018b; Han et al., 2019; Naik et al., 2019; Vashishtha et al., 2019; Zhou et al., 2019, 2020). More broadly, the ability to perform temporal reasoning is important for understanding narratives (Nakhimovsky, 1987; Jung et al., 2011; Cheng et al., 2013), answering questions (Bruce, 1972; Khashabi, 2019; Ning et al., 2020), and summarizing events (Jung et al., 2011; Wang et al., 2018). Given that temporal reasoning is integral to natural language understanding (NLU) and that Natural Language Inference (NLI) is a common framework for evaluating how well models capture semantic phenomena integral to NL"
2020.findings-emnlp.363,W16-5706,0,0.14975,"Missing"
2020.findings-emnlp.363,D14-1162,0,0.0825854,"Missing"
2020.findings-emnlp.363,W18-5441,1,0.887283,"Missing"
2020.findings-emnlp.363,2020.acl-demos.14,0,0.0306861,"attach the direct object modifier of the event to make the reference unambiguous in the context. For example, to refer to the highlighted predicate in the context – ‘we cleaned the apartment .... and they cleaned the washroom ...’ – we use the hypothesis ‘the cleaning the apartment started ...’. We use the gold dependency trees of each context to obtain these modifiers of the predicate. We do not consider predicates with AUX and DET POS tags for our recasting. TE3, TB-Dense, and RED do not have gold dependency trees. Hence, we process and tokenize each sentence in these corpora using Stanza (Qi et al., 2020) to predict the POS, lemma and dependency trees for all the sentences. To tag the copular predicates in these corpora, we use PredPatt (White et al., 2016; Zhang et al., 2017b) that uses the predicted dependency trees from Stanza. To get the inflection on each verb, we use the Universal Morphology corpora (UniMorph, Sylak-Glassman et al., 2015; Kirov et al., 2018) for English and back-off to LemmInflect for tokens not found in UniMorph.3 4 Dataset Validation To assess whether the recast NLI pairs are correct, we conduct a validation experiment by randomly sampling 100 NLI pairs from the train"
2020.findings-emnlp.363,P16-1207,0,0.036715,"nsky and Zacks, 2014), and modeling such temporal reasoning has been central to many classical AI approaches (McCarthy and Hayes, 1987; Kahn and Gorry, 1977; McDermott, 1982; Allen, 1984; Kowalski and Sergot, 1989; Pani and Bhattacharjee, 2001). Natural language supports various forms of temporal reasoning, including reasoning about the chronology and duration of events, and many Natural Language Understanding (NLU) tasks and models have been employed for understanding and capturing different aspects of temporal reasoning (UzZaman et al., 2013; Llorens et al., 2015; Mostafazadeh et al., 2016; Reimers et al., 2016; Tourille et al., 2017; Ning et al., 2017, 2018a; Meng and Rumshisky, 2018; Ning et al., 2018b; Han et al., 2019; Naik et al., 2019; Vashishtha et al., 2019; Zhou et al., 2019, 2020). More broadly, the ability to perform temporal reasoning is important for understanding narratives (Nakhimovsky, 1987; Jung et al., 2011; Cheng et al., 2013), answering questions (Bruce, 1972; Khashabi, 2019; Ning et al., 2020), and summarizing events (Jung et al., 2011; Wang et al., 2018). Given that temporal reasoning is integral to natural language understanding (NLU) and that Natural Language Inference (NLI)"
2020.findings-emnlp.363,D19-1228,0,0.0799923,"it hard to learn using common language modeling-based methods. Popular NLI datasets contain hypotheses which are elicited by humans (Bowman et al., 2015; Williams et al., 2018). Although the context sentences for these datasets come from multiple genres, the constructed hypotheses do not necessarily capture semantic phenomenon which are essential for any robust NLU inference system. Recent work has catered to the lack of such inference capabilities by focusing on semantic phenomenon such as paraphrastic inference and anaphora resolution (White et al., 2017), veridicality (Poliak et al., 2018; Ross and Pavlick, 2019), and various other implicatures and presuppositions (Jeretic et al., 2020). Even though temporal reasoning is crucial for event understanding, no datasets focused on temporal reasoning exist in the NLI format. To fill this lacuna, we recast four existing datasets to create NLI pairs that explicitly require reasoning about event duration and chronological ordering. Table 1 shows examples from two of our recasted datasets. 3 Phenomenon Dataset Creation We construct five new NLI datasets recast from four existing datasets that focus on two key aspects of temporal reasoning: (a) temporal ordering"
2020.findings-emnlp.363,P15-2111,0,0.0154311,"each context to obtain these modifiers of the predicate. We do not consider predicates with AUX and DET POS tags for our recasting. TE3, TB-Dense, and RED do not have gold dependency trees. Hence, we process and tokenize each sentence in these corpora using Stanza (Qi et al., 2020) to predict the POS, lemma and dependency trees for all the sentences. To tag the copular predicates in these corpora, we use PredPatt (White et al., 2016; Zhang et al., 2017b) that uses the predicted dependency trees from Stanza. To get the inflection on each verb, we use the Universal Morphology corpora (UniMorph, Sylak-Glassman et al., 2015; Kirov et al., 2018) for English and back-off to LemmInflect for tokens not found in UniMorph.3 4 Dataset Validation To assess whether the recast NLI pairs are correct, we conduct a validation experiment by randomly sampling 100 NLI pairs from the train split of each dataset. For each NLI pair, we ask the annotators to answer the question – How likely is it that the second sentence is true if the first sentence is true? We provide 5 options to choose from – extremely likely, very likely, even chance, very unlikely, extremely unlikely. We recruited 48 annotators from Amazon Mechanical Turk to"
2020.findings-emnlp.363,D18-1006,0,0.0996488,"Missing"
2020.findings-emnlp.363,P17-2035,0,0.0260165,"and modeling such temporal reasoning has been central to many classical AI approaches (McCarthy and Hayes, 1987; Kahn and Gorry, 1977; McDermott, 1982; Allen, 1984; Kowalski and Sergot, 1989; Pani and Bhattacharjee, 2001). Natural language supports various forms of temporal reasoning, including reasoning about the chronology and duration of events, and many Natural Language Understanding (NLU) tasks and models have been employed for understanding and capturing different aspects of temporal reasoning (UzZaman et al., 2013; Llorens et al., 2015; Mostafazadeh et al., 2016; Reimers et al., 2016; Tourille et al., 2017; Ning et al., 2017, 2018a; Meng and Rumshisky, 2018; Ning et al., 2018b; Han et al., 2019; Naik et al., 2019; Vashishtha et al., 2019; Zhou et al., 2019, 2020). More broadly, the ability to perform temporal reasoning is important for understanding narratives (Nakhimovsky, 1987; Jung et al., 2011; Cheng et al., 2013), answering questions (Bruce, 1972; Khashabi, 2019; Ning et al., 2020), and summarizing events (Jung et al., 2011; Wang et al., 2018). Given that temporal reasoning is integral to natural language understanding (NLU) and that Natural Language Inference (NLI) is a common framework f"
2020.findings-emnlp.363,S13-2001,0,0.064557,"about the world (Casati and Varzi, 1996; Zacks and Tversky, 2001; Radvansky and Zacks, 2014), and modeling such temporal reasoning has been central to many classical AI approaches (McCarthy and Hayes, 1987; Kahn and Gorry, 1977; McDermott, 1982; Allen, 1984; Kowalski and Sergot, 1989; Pani and Bhattacharjee, 2001). Natural language supports various forms of temporal reasoning, including reasoning about the chronology and duration of events, and many Natural Language Understanding (NLU) tasks and models have been employed for understanding and capturing different aspects of temporal reasoning (UzZaman et al., 2013; Llorens et al., 2015; Mostafazadeh et al., 2016; Reimers et al., 2016; Tourille et al., 2017; Ning et al., 2017, 2018a; Meng and Rumshisky, 2018; Ning et al., 2018b; Han et al., 2019; Naik et al., 2019; Vashishtha et al., 2019; Zhou et al., 2019, 2020). More broadly, the ability to perform temporal reasoning is important for understanding narratives (Nakhimovsky, 1987; Jung et al., 2011; Cheng et al., 2013), answering questions (Bruce, 1972; Khashabi, 2019; Ning et al., 2020), and summarizing events (Jung et al., 2011; Wang et al., 2018). Given that temporal reasoning is integral to natural"
2020.findings-emnlp.363,P19-1280,1,0.890188,"Missing"
2020.findings-emnlp.363,N18-1101,0,0.150056,"not explicitly mention how long the waiting lasted, one can reasonably guess that it lasted somewhere between minutes to hours— definitely not months or years. Zhou et al. (2020) note that common sense inference is required to come to such conclusions about an event’s duration and text might even contain reporting biases when highlighting rarities (Schubert, 2002; Van Durme, 2011; Zhang et al., 2017a; Tandon et al., 2018), potentially making it hard to learn using common language modeling-based methods. Popular NLI datasets contain hypotheses which are elicited by humans (Bowman et al., 2015; Williams et al., 2018). Although the context sentences for these datasets come from multiple genres, the constructed hypotheses do not necessarily capture semantic phenomenon which are essential for any robust NLU inference system. Recent work has catered to the lack of such inference capabilities by focusing on semantic phenomenon such as paraphrastic inference and anaphora resolution (White et al., 2017), veridicality (Poliak et al., 2018; Ross and Pavlick, 2019), and various other implicatures and presuppositions (Jeretic et al., 2020). Even though temporal reasoning is crucial for event understanding, no datase"
2020.findings-emnlp.363,Q17-1027,1,0.759302,"Missing"
2020.findings-emnlp.363,W17-6944,1,0.865979,"Missing"
2020.findings-emnlp.363,D19-1332,0,0.0983652,"n, 1984; Kowalski and Sergot, 1989; Pani and Bhattacharjee, 2001). Natural language supports various forms of temporal reasoning, including reasoning about the chronology and duration of events, and many Natural Language Understanding (NLU) tasks and models have been employed for understanding and capturing different aspects of temporal reasoning (UzZaman et al., 2013; Llorens et al., 2015; Mostafazadeh et al., 2016; Reimers et al., 2016; Tourille et al., 2017; Ning et al., 2017, 2018a; Meng and Rumshisky, 2018; Ning et al., 2018b; Han et al., 2019; Naik et al., 2019; Vashishtha et al., 2019; Zhou et al., 2019, 2020). More broadly, the ability to perform temporal reasoning is important for understanding narratives (Nakhimovsky, 1987; Jung et al., 2011; Cheng et al., 2013), answering questions (Bruce, 1972; Khashabi, 2019; Ning et al., 2020), and summarizing events (Jung et al., 2011; Wang et al., 2018). Given that temporal reasoning is integral to natural language understanding (NLU) and that Natural Language Inference (NLI) is a common framework for evaluating how well models capture semantic phenomena integral to NLU (Cooper et al., 1996; Dagan et al., 2006; White et al., 2017; Poliak et al., 201"
2020.findings-emnlp.363,2020.acl-main.678,0,0.141052,", 2020. 2020 Association for Computational Linguistics 2 Motivation A text often does not contain explicit mentions of how long events last or whether some events are contained within another. Consider (1). (1) We waited until 2:25 pm and then left. Dataset duration order order order order UDS-Time UDS-Time TempEval3 TimeBank-Dense RED # NLI Pairs 504,136 562,944 27,240 11,910 5,578 Table 2: Recast datasets statistics Although (1) does not explicitly mention how long the waiting lasted, one can reasonably guess that it lasted somewhere between minutes to hours— definitely not months or years. Zhou et al. (2020) note that common sense inference is required to come to such conclusions about an event’s duration and text might even contain reporting biases when highlighting rarities (Schubert, 2002; Van Durme, 2011; Zhang et al., 2017a; Tandon et al., 2018), potentially making it hard to learn using common language modeling-based methods. Popular NLI datasets contain hypotheses which are elicited by humans (Bowman et al., 2015; Williams et al., 2018). Although the context sentences for these datasets come from multiple genres, the constructed hypotheses do not necessarily capture semantic phenomenon whi"
2020.findings-emnlp.363,I17-1100,1,0.903017,"Missing"
2020.findings-emnlp.363,D16-1177,1,0.863641,"Missing"
2020.lrec-1.699,P13-1023,0,0.257132,"Missing"
2020.lrec-1.699,W17-6901,0,0.0233458,"alization procedures. The Decomp toolkit provides a suite of Python 3 tools for querying UDS graphs using SPARQL. Both UDS1.0 and Decomp0.1 are publicly available at http://decomp.io. Keywords: semantics, semantic roles, factuality, genericity, temporal duration, entity typing 1. Introduction Traditional semantic annotation frameworks generally define complex, often exclusive category systems that require highly trained annotators to build (Palmer et al., 2005; Banarescu et al., 2013; Abend and Rappoport, 2013; Oepen et al., 2014; Oepen et al., 2015; Bos et al., 2017; Abzianidze et al., 2017; Abzianidze and Bos, 2017; Schneider et al., 2018). And in spite of their high quality for the cases they are designed to handle, these frameworks can be brittle to cases that (i) deviate from prototypical instances of a category; (ii) are equally good instances of multiple categories; or (iii) fall under a category that was erroneously excluded from the framework’s ontology.1 An alternative approach to semantic annotation that addresses these issues has been growing in popularity: decompositional semantics (Reisinger et al., 2015; White et al., 2016). In this approach, which is rooted in a long tradition of theoretic"
2020.lrec-1.699,E17-2039,0,0.0539466,"Missing"
2020.lrec-1.699,W13-2322,0,0.0964851,"y the predicative patterns produced by the PredPatt tool and real-valued node and edge attributes constructed using sophisticated normalization procedures. The Decomp toolkit provides a suite of Python 3 tools for querying UDS graphs using SPARQL. Both UDS1.0 and Decomp0.1 are publicly available at http://decomp.io. Keywords: semantics, semantic roles, factuality, genericity, temporal duration, entity typing 1. Introduction Traditional semantic annotation frameworks generally define complex, often exclusive category systems that require highly trained annotators to build (Palmer et al., 2005; Banarescu et al., 2013; Abend and Rappoport, 2013; Oepen et al., 2014; Oepen et al., 2015; Bos et al., 2017; Abzianidze et al., 2017; Abzianidze and Bos, 2017; Schneider et al., 2018). And in spite of their high quality for the cases they are designed to handle, these frameworks can be brittle to cases that (i) deviate from prototypical instances of a category; (ii) are equally good instances of multiple categories; or (iii) fall under a category that was erroneously excluded from the framework’s ontology.1 An alternative approach to semantic annotation that addresses these issues has been growing in popularity: de"
2020.lrec-1.699,W03-1022,0,0.0646411,") rai Entity type The UDS-WordSense dataset (v1.0) annotates (the nominal heads of) arguments for the WordNet 3.0 (Miller, 1995; Fellbaum, 1998) senses that those (nominal heads of) arguments can have. For any particular argument, annotators were presented with all of the definitions of senses listed in WordNet for the head of that argument and asked to select all that were applicable using check boxes. After MEM-based normalization of the sense responses (described below), we extract entity types for these annotations by mapping the selected senses to their supersenses/lexicographer classes (Ciaramita and Johnson, 2003) and deriving a real-valued attribute value for each 5703 supersense from the normalized values associated with the senses that fall under it. To normalize the sense responses, we use a logistic mixed effects model with real-valued fixed effects βik for each annotated argument (head) token i and potential sense k and real-valued random intercepts ua for each annotator a:8 X X X  L= log Bern yaρa (i)k ; paik i + k∈π(i) a∈α(i) X log N (ua ; 0, Var(u)) a where π(i) is the set of potential senses for argument (head) token i, and paik = logit−1 (βik + ua ). We then take βik as the attribute value"
2020.lrec-1.699,S14-2008,0,0.243138,"Missing"
2020.lrec-1.699,S15-2153,0,0.283487,"d node and edge attributes constructed using sophisticated normalization procedures. The Decomp toolkit provides a suite of Python 3 tools for querying UDS graphs using SPARQL. Both UDS1.0 and Decomp0.1 are publicly available at http://decomp.io. Keywords: semantics, semantic roles, factuality, genericity, temporal duration, entity typing 1. Introduction Traditional semantic annotation frameworks generally define complex, often exclusive category systems that require highly trained annotators to build (Palmer et al., 2005; Banarescu et al., 2013; Abend and Rappoport, 2013; Oepen et al., 2014; Oepen et al., 2015; Bos et al., 2017; Abzianidze et al., 2017; Abzianidze and Bos, 2017; Schneider et al., 2018). And in spite of their high quality for the cases they are designed to handle, these frameworks can be brittle to cases that (i) deviate from prototypical instances of a category; (ii) are equally good instances of multiple categories; or (iii) fall under a category that was erroneously excluded from the framework’s ontology.1 An alternative approach to semantic annotation that addresses these issues has been growing in popularity: decompositional semantics (Reisinger et al., 2015; White et al., 2016"
2020.lrec-1.699,J05-1004,0,0.304583,"structures defined by the predicative patterns produced by the PredPatt tool and real-valued node and edge attributes constructed using sophisticated normalization procedures. The Decomp toolkit provides a suite of Python 3 tools for querying UDS graphs using SPARQL. Both UDS1.0 and Decomp0.1 are publicly available at http://decomp.io. Keywords: semantics, semantic roles, factuality, genericity, temporal duration, entity typing 1. Introduction Traditional semantic annotation frameworks generally define complex, often exclusive category systems that require highly trained annotators to build (Palmer et al., 2005; Banarescu et al., 2013; Abend and Rappoport, 2013; Oepen et al., 2014; Oepen et al., 2015; Bos et al., 2017; Abzianidze et al., 2017; Abzianidze and Bos, 2017; Schneider et al., 2018). And in spite of their high quality for the cases they are designed to handle, these frameworks can be brittle to cases that (i) deviate from prototypical instances of a category; (ii) are equally good instances of multiple categories; or (iii) fall under a category that was erroneously excluded from the framework’s ontology.1 An alternative approach to semantic annotation that addresses these issues has been g"
2020.lrec-1.699,N18-1067,1,0.864544,"Missing"
2020.lrec-1.699,P18-1018,0,0.01412,"Decomp toolkit provides a suite of Python 3 tools for querying UDS graphs using SPARQL. Both UDS1.0 and Decomp0.1 are publicly available at http://decomp.io. Keywords: semantics, semantic roles, factuality, genericity, temporal duration, entity typing 1. Introduction Traditional semantic annotation frameworks generally define complex, often exclusive category systems that require highly trained annotators to build (Palmer et al., 2005; Banarescu et al., 2013; Abend and Rappoport, 2013; Oepen et al., 2014; Oepen et al., 2015; Bos et al., 2017; Abzianidze et al., 2017; Abzianidze and Bos, 2017; Schneider et al., 2018). And in spite of their high quality for the cases they are designed to handle, these frameworks can be brittle to cases that (i) deviate from prototypical instances of a category; (ii) are equally good instances of multiple categories; or (iii) fall under a category that was erroneously excluded from the framework’s ontology.1 An alternative approach to semantic annotation that addresses these issues has been growing in popularity: decompositional semantics (Reisinger et al., 2015; White et al., 2016). In this approach, which is rooted in a long tradition of theoretical approaches to lexical"
2020.lrec-1.699,W19-3316,0,0.0105732,"ntic feature. Common feature configurations often correspond to categories in a traditional framework (Reisinger et al., 2015; Govindarajan et al., 2019); but unlike such frameworks, a decompositional approach retains the ability to capture configurations that were not considered at design time. Further, unlike a categorical framework, reannotation after an overhaul of the framework’s ontology is never necessary, since additional annotations simply accrue to sharpen the framework’s ability to capture fine-grained semantic phenomena. A variety of semantic annotation datasets that take a decom1 Shalev et al. (2019) discuss multiple recent, instructive examples of such brittleness. positional approach now exist, including ones that target semantic roles (Reisinger et al., 2015; White et al., 2016), entity types (White et al., 2016), event factuality (White et al., 2016; Rudinger et al., 2018), linguistic expressions of generalizations about entities and events (Govindarajan et al., 2019), and temporal properties of and relations between events (Vashishtha et al., 2019). But despite the potential benefits of a decompositional approach—as well as the broad coverage of linguistic phenomena it has been shown"
2020.lrec-1.699,P19-1280,1,0.828602,"Missing"
2020.lrec-1.699,D16-1177,1,0.914879,"Missing"
2020.lrec-1.699,E17-2015,1,0.905118,"Missing"
2020.nuse-1.7,boytcheva-etal-2017-mining,0,0.026574,"age for discovering interesting patterns in consumer purchases. The applicability of ARM extends far beyond this specific scenario, where examples of ARM usage for NLP applications include detecting annotation inconsistencies (Nov´ak and Raz´ımov´a, 2009), discovering strongly-related events (Shibata and Kurohashi, 2011), adding missing knowledge to the KB 55 Proceedings of the 1st Joint Workshop on Narrative Understanding, Storylines, and Events, pages 55–62 c July 9, 2020. 2020 Association for Computational Linguistics (Gal´arraga et al., 2013), as well as understanding clinical narratives (Boytcheva et al., 2017). ARM aims to extract interesting patterns from a transactional database D. A transaction is a set of items, and a non-empty subset of a transaction is called an itemset. We define support as the number of transactions we observe an itemset I in: sup(I) = |{t|t ∈ D, I ⊆ t}|. (1) We say that an itemset I is frequent, if its support (on a given database D) exceeeds a user-defined threshold tsup : sup(I) ≥ tsup . A pair of itemsets A, B is called a rule if A ∩ B = ∅ and is denoted as A → B. We say that a rule A → B is interesting if 1) both A and B are frequent, 2) the interestingess of the rule"
2020.nuse-1.7,P08-1090,0,0.447774,"is not to establish new state of the art results in the area of SI. Rather, our primary contribution is retrospective, drawing a connection between a sub-topic in Computational Linguistics (CL) with a major pre-existing area of Computer Science, i.e., Data Mining. In the case one approached SI through counting co-occurrence statistics, then the existing tools of ARM lead naturally to solutions that had not been previously considered within CL. Introduction The goal of this paper is to demonstrate how the efforts in Script Induction (SI), up until recently dominated by statistical approaches (Chambers and Jurafsky, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015a,b), can be productively framed and extended as a special case of Association Rule Mining (ARM), a wellestablished problem in Data Mining (Agrawal et al., 1993, 1994; Han et al., 2000). We start by introducing SI and ARM and then demonstrate a unification under a general chain likelihood maximization framework. We discuss how the existing count-based SI models tackle this maximization problem using na¨ıve Bayes assumptions. We provide an alternative: mining higherorder count statistics using ARM and picking the most reliable"
2020.nuse-1.7,P09-1068,0,0.13751,"Missing"
2020.nuse-1.7,D15-1195,1,0.928623,"Missing"
2020.nuse-1.7,E12-1034,0,0.32747,"Missing"
2020.nuse-1.7,I11-1115,0,0.0272427,"s. We conclude with a discussion on the implications and potential extensions of the proposed framework. 2 2.1 Background Association Rule Mining ARM is a prevalent problem in Data Mining, introduced by Agrawal et al. (1993). The task is often referred to as market basket analysis due to its widespread usage for discovering interesting patterns in consumer purchases. The applicability of ARM extends far beyond this specific scenario, where examples of ARM usage for NLP applications include detecting annotation inconsistencies (Nov´ak and Raz´ımov´a, 2009), discovering strongly-related events (Shibata and Kurohashi, 2011), adding missing knowledge to the KB 55 Proceedings of the 1st Joint Workshop on Narrative Understanding, Storylines, and Events, pages 55–62 c July 9, 2020. 2020 Association for Computational Linguistics (Gal´arraga et al., 2013), as well as understanding clinical narratives (Boytcheva et al., 2017). ARM aims to extract interesting patterns from a transactional database D. A transaction is a set of items, and a non-empty subset of a transaction is called an itemset. We define support as the number of transactions we observe an itemset I in: sup(I) = |{t|t ∈ D, I ⊆ t}|. (1) We say that an item"
2020.nuse-1.7,D18-1413,0,0.497873,"t of narrative events that share a common actor”, where the partial ordering typically represents temporal or causal order of events, and a narrative event is “a tuple of an event and its participants, represented as typed dependencies”. Formally, we define a narrative event e := (v, d), where v is a verb lemma, and d is a dependency arc between the verb and the common actor (dobj or nsubj). An example of a narrative chain is given in Figure 1. {e1 , e2 , . . . , ek , eˆ, ek+1 , . . . eL }. Although the recent work in SI (Rudinger et al., 2015b; Pichotta and Mooney, 2016; Peng and Roth, 2016; Weber et al., 2018) has focused on a Language Modeling (LM) approach for the narrative cloze test, it is fundamentally different from ARM in that it makes use of the total ordering of events and is thus incomparable to ARM, which does not assume any ordering of events within a chain. In the next section, we survey two of the most influential count-based SI models, showing how each of them is related to ARM. 56 3 3.1 Count-based Script Induction 3.2 The bigram probability model was proposed by Jans et al. (2012) and was also used by Pichotta and Mooney (2014). It utilizes positional information between co-occurri"
2020.nuse-1.7,1985.tmi-1.17,0,0.313891,"s and Jurafsky, 2008) is a standard extrinsic evaluation procedure for Task 1 of SI. In this test, a sequence of narrative events is automatically extracted from a document, and one event is removed; the goal is to predict the missing event. Formally, given an incomplete narrative chain {e1 , e2 , . . . , eL } and an insertion point k ∈ [L], we would like to predict the most likely missing event eˆ to complete the chain: Script Induction The concept of script knowledge in AI, along with early knowledge-based methods to learn scripts were introduced by Minsky (1974); Schank and Abelson (1977); Mooney and DeJong (1985). With the rise of statistical methods, the next generation of algorithms made use of co-occurrence statistics and distributional semantics for script learning (Chambers and Jurafsky, 2008, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). Our primary focus is on drawing connections between ARM and this body of work. Following Chambers and Jurafsky (2008), we define a narrative chain as “a partially ordered set of narrative events that share a common actor”, where the partial ordering typically represents temporal or causal order of events, and a narrative event is “a tuple of an event and"
2020.nuse-1.7,W12-3018,1,0.802909,"Missing"
2020.nuse-1.7,W09-3024,0,0.0992676,"Missing"
2020.nuse-1.7,P16-1028,0,0.0126083,"partially ordered set of narrative events that share a common actor”, where the partial ordering typically represents temporal or causal order of events, and a narrative event is “a tuple of an event and its participants, represented as typed dependencies”. Formally, we define a narrative event e := (v, d), where v is a verb lemma, and d is a dependency arc between the verb and the common actor (dobj or nsubj). An example of a narrative chain is given in Figure 1. {e1 , e2 , . . . , ek , eˆ, ek+1 , . . . eL }. Although the recent work in SI (Rudinger et al., 2015b; Pichotta and Mooney, 2016; Peng and Roth, 2016; Weber et al., 2018) has focused on a Language Modeling (LM) approach for the narrative cloze test, it is fundamentally different from ARM in that it makes use of the total ordering of events and is thus incomparable to ARM, which does not assume any ordering of events within a chain. In the next section, we survey two of the most influential count-based SI models, showing how each of them is related to ARM. 56 3 3.1 Count-based Script Induction 3.2 The bigram probability model was proposed by Jans et al. (2012) and was also used by Pichotta and Mooney (2014). It utilizes positional informati"
2020.nuse-1.7,E14-1024,0,0.67446,"s in the area of SI. Rather, our primary contribution is retrospective, drawing a connection between a sub-topic in Computational Linguistics (CL) with a major pre-existing area of Computer Science, i.e., Data Mining. In the case one approached SI through counting co-occurrence statistics, then the existing tools of ARM lead naturally to solutions that had not been previously considered within CL. Introduction The goal of this paper is to demonstrate how the efforts in Script Induction (SI), up until recently dominated by statistical approaches (Chambers and Jurafsky, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015a,b), can be productively framed and extended as a special case of Association Rule Mining (ARM), a wellestablished problem in Data Mining (Agrawal et al., 1993, 1994; Han et al., 2000). We start by introducing SI and ARM and then demonstrate a unification under a general chain likelihood maximization framework. We discuss how the existing count-based SI models tackle this maximization problem using na¨ıve Bayes assumptions. We provide an alternative: mining higherorder count statistics using ARM and picking the most reliable rules using the weighted set cover algorithm."
2020.spnlp-1.2,P16-1154,0,0.0284652,"(a) seq2seq, (b) pointer network, (c) Copy-only, and (d) CopyNext model. The numbers are predictions of indices corresponding to the tokens in the input sequence. CN refers to the CopyNext symbol, our proposed method of denoting the operation that copies the next token from the input. In (d), the next token from token 4 would be 5. Specifically, we extend pointer (or copy) networks. Unlike the algorithmic tasks originally targeted by Vinyals et al. (2015), tasks in NLP tend to copy spans from the input rather than discontiguous tokens. This is prevalent for copying named entities in dialogue (Gu et al., 2016; Eric and Manning, 2017), entire sentences in summarization (See et al., 2017; Song et al., 2018), or even single words (if subtokenized). The need to efficiently copy spans motivates our introduction of an inductive bias that copies contiguous tokens. Like a pointer network, our model copies the first token of a span. However, for subsequent timesteps, our model generates a “CopyNext” symbol (CN) instead of copying another token from source. CopyNext represents the operation of copying the word following the last predicted word from the input sequence. Figure 1 highlights the difference betw"
2020.spnlp-1.2,P16-1014,0,0.016636,"gnition (NNER). Prior work uses several approaches to model NNER: machine reading comprehension (Li et al., 2019), transition-based methods (Wang et al., 2018), mention hypergraphs (Lu and Roth, 2015; Wang and Lu, 2018; Katiyar and Cardie, 2018), and seq2seq models (Strakov´a et al., 2019). Related Work Pointer networks (Vinyals et al., 2015; Jia and Liang, 2016; Merity et al., 2016) are seq2seq models that employ a soft attention distribution (Bahdanau et al., 2014) to produce an output sequence consisting of values from the input sequence. Pointer-generator networks (Miao and Blunsom, 2016; Gulcehre et al., 2016, inter alia) extend the range of output types by combining the distribution from the pointer with a vocabulary distribution from a generator. Thus, these models operate on the type level. In contrast, our model operates at the token level. Instead of using soft attention distribution of the encoder states, we use hard attention, resulting in a single encoder state, or a single token, to feed to the decoder. This enables explicit copying of span offsets. Closest to our work, Zhou et al. (2018) and Panthaplackel et al. (2020) have tackled span copying by extending pointer-generator networks and"
2020.spnlp-1.2,2020.tacl-1.5,0,0.0466275,"Missing"
2020.spnlp-1.2,D19-1393,0,0.0225068,"Missing"
2020.spnlp-1.2,N18-1079,0,0.0203709,"search over the space of potential labels, a solution that does not scale to large complex label sets. 2 generation, and editing. In contrast, we are concerned with information extraction tasks as transduction, where hard alignments to the input sentence are crucial and output sequences must represent a valid linearized structure. Specifically, we study nested named entity recognition (NNER). Prior work uses several approaches to model NNER: machine reading comprehension (Li et al., 2019), transition-based methods (Wang et al., 2018), mention hypergraphs (Lu and Roth, 2015; Wang and Lu, 2018; Katiyar and Cardie, 2018), and seq2seq models (Strakov´a et al., 2019). Related Work Pointer networks (Vinyals et al., 2015; Jia and Liang, 2016; Merity et al., 2016) are seq2seq models that employ a soft attention distribution (Bahdanau et al., 2014) to produce an output sequence consisting of values from the input sequence. Pointer-generator networks (Miao and Blunsom, 2016; Gulcehre et al., 2016, inter alia) extend the range of output types by combining the distribution from the pointer with a vocabulary distribution from a generator. Thus, these models operate on the type level. In contrast, our model operates at"
2020.spnlp-1.2,N19-1423,0,0.0374639,"Missing"
2020.spnlp-1.2,2021.ccl-1.108,0,0.091546,"Missing"
2020.spnlp-1.2,D15-1102,0,0.0259234,"wer model that performs an exhaustive search over the space of potential labels, a solution that does not scale to large complex label sets. 2 generation, and editing. In contrast, we are concerned with information extraction tasks as transduction, where hard alignments to the input sentence are crucial and output sequences must represent a valid linearized structure. Specifically, we study nested named entity recognition (NNER). Prior work uses several approaches to model NNER: machine reading comprehension (Li et al., 2019), transition-based methods (Wang et al., 2018), mention hypergraphs (Lu and Roth, 2015; Wang and Lu, 2018; Katiyar and Cardie, 2018), and seq2seq models (Strakov´a et al., 2019). Related Work Pointer networks (Vinyals et al., 2015; Jia and Liang, 2016; Merity et al., 2016) are seq2seq models that employ a soft attention distribution (Bahdanau et al., 2014) to produce an output sequence consisting of values from the input sequence. Pointer-generator networks (Miao and Blunsom, 2016; Gulcehre et al., 2016, inter alia) extend the range of output types by combining the distribution from the pointer with a vocabulary distribution from a generator. Thus, these models operate on the t"
2020.spnlp-1.2,D18-1019,0,0.255797,"orms an exhaustive search over the space of potential labels, a solution that does not scale to large complex label sets. 2 generation, and editing. In contrast, we are concerned with information extraction tasks as transduction, where hard alignments to the input sentence are crucial and output sequences must represent a valid linearized structure. Specifically, we study nested named entity recognition (NNER). Prior work uses several approaches to model NNER: machine reading comprehension (Li et al., 2019), transition-based methods (Wang et al., 2018), mention hypergraphs (Lu and Roth, 2015; Wang and Lu, 2018; Katiyar and Cardie, 2018), and seq2seq models (Strakov´a et al., 2019). Related Work Pointer networks (Vinyals et al., 2015; Jia and Liang, 2016; Merity et al., 2016) are seq2seq models that employ a soft attention distribution (Bahdanau et al., 2014) to produce an output sequence consisting of values from the input sequence. Pointer-generator networks (Miao and Blunsom, 2016; Gulcehre et al., 2016, inter alia) extend the range of output types by combining the distribution from the pointer with a vocabulary distribution from a generator. Thus, these models operate on the type level. In contr"
2020.spnlp-1.2,D18-1124,0,0.0402483,"Missing"
2020.spnlp-1.2,D16-1031,0,0.0178083,"nested named entity recognition (NNER). Prior work uses several approaches to model NNER: machine reading comprehension (Li et al., 2019), transition-based methods (Wang et al., 2018), mention hypergraphs (Lu and Roth, 2015; Wang and Lu, 2018; Katiyar and Cardie, 2018), and seq2seq models (Strakov´a et al., 2019). Related Work Pointer networks (Vinyals et al., 2015; Jia and Liang, 2016; Merity et al., 2016) are seq2seq models that employ a soft attention distribution (Bahdanau et al., 2014) to produce an output sequence consisting of values from the input sequence. Pointer-generator networks (Miao and Blunsom, 2016; Gulcehre et al., 2016, inter alia) extend the range of output types by combining the distribution from the pointer with a vocabulary distribution from a generator. Thus, these models operate on the type level. In contrast, our model operates at the token level. Instead of using soft attention distribution of the encoder states, we use hard attention, resulting in a single encoder state, or a single token, to feed to the decoder. This enables explicit copying of span offsets. Closest to our work, Zhou et al. (2018) and Panthaplackel et al. (2020) have tackled span copying by extending pointer"
2020.spnlp-1.2,D14-1162,0,0.0881134,"Missing"
2020.spnlp-1.2,P19-1009,1,0.889916,"Missing"
2020.spnlp-1.2,N18-1202,0,0.107835,"Missing"
2020.spnlp-1.2,P19-1510,0,0.173562,"a linear output based on an encoded (linear) representation of the input. As IE is traditionally considered a structured prediction task, it remains today that IE systems are assumed to produce an annotation on the input text. That is, predicting which specific tokens of an input string led to, e.g., the label of P ERSON. This is in contrast to text generation which rarely, if ever, needs hard alignments between the input and the desired output. Our work explores a novel extension to seq2seq that provides such alignments. We apply our model for the Nested Named Entity Recognition (NNER) task (Ringland et al., 2019). Unlike traditional named entity recognition, named entity mentions in NNER may be subsequences of 1 Our source code: https://github.com/ abhinonymous/copynext 11 Proceedings of 4th Workshop on Structured Prediction for NLP, pages 11–16 c November 20, 2020. 2020 Association for Computational Linguistics Figure 2: For each decoder timestep a decision vector chooses between labeling, a CopyNext operation, or pointing to an input token. The decoder input comes from either an encoder state or a label embedding. other named entity mentions (such as [[last] year] in Figure 1). We find that both exp"
2020.spnlp-1.2,P17-1099,0,0.0267895,"numbers are predictions of indices corresponding to the tokens in the input sequence. CN refers to the CopyNext symbol, our proposed method of denoting the operation that copies the next token from the input. In (d), the next token from token 4 would be 5. Specifically, we extend pointer (or copy) networks. Unlike the algorithmic tasks originally targeted by Vinyals et al. (2015), tasks in NLP tend to copy spans from the input rather than discontiguous tokens. This is prevalent for copying named entities in dialogue (Gu et al., 2016; Eric and Manning, 2017), entire sentences in summarization (See et al., 2017; Song et al., 2018), or even single words (if subtokenized). The need to efficiently copy spans motivates our introduction of an inductive bias that copies contiguous tokens. Like a pointer network, our model copies the first token of a span. However, for subsequent timesteps, our model generates a “CopyNext” symbol (CN) instead of copying another token from source. CopyNext represents the operation of copying the word following the last predicted word from the input sequence. Figure 1 highlights the difference between output sequences for several transductive models, including our CopyNext m"
2020.spnlp-1.2,C18-1146,0,0.0136437,"tions of indices corresponding to the tokens in the input sequence. CN refers to the CopyNext symbol, our proposed method of denoting the operation that copies the next token from the input. In (d), the next token from token 4 would be 5. Specifically, we extend pointer (or copy) networks. Unlike the algorithmic tasks originally targeted by Vinyals et al. (2015), tasks in NLP tend to copy spans from the input rather than discontiguous tokens. This is prevalent for copying named entities in dialogue (Gu et al., 2016; Eric and Manning, 2017), entire sentences in summarization (See et al., 2017; Song et al., 2018), or even single words (if subtokenized). The need to efficiently copy spans motivates our introduction of an inductive bias that copies contiguous tokens. Like a pointer network, our model copies the first token of a span. However, for subsequent timesteps, our model generates a “CopyNext” symbol (CN) instead of copying another token from source. CopyNext represents the operation of copying the word following the last predicted word from the input sequence. Figure 1 highlights the difference between output sequences for several transductive models, including our CopyNext model. Introduction S"
2020.spnlp-1.2,P19-1527,0,0.0223325,"Missing"
2020.spnlp-1.2,P19-1452,0,0.0475972,"Missing"
2020.spnlp-1.9,P18-1201,0,0.0411471,"8, inter alia). Because pipelined approaches suffer from error propagation in which the error from earlier subtasks (e.g. entity mention detection) is inherited by later subtasks, joint modeling of the 3 subtasks has been attempted. Yang and Mitchell (2016) attempts to jointly model the three components with hand-crafted features, but still need to detect entity mentions and event triggers separately. Nguyen and Nguyen (2019) jointly models the three tasks using neural networks with shared underlying representations. The models proposed in these two works are the baselines used in this paper. Huang et al. (2018) approach zero-shot event extraction by stipulating a graph structure for each event type and finding the event type graph structure whose learned representation most closely matches the learned representation of the parsed AMR (Banarescu et al., 2013) structure of a text. In contrast, our approach forgoes explicit graphstructured semantic representations such as AMR. Background Event extraction is traditionally viewed as three subtasks: (1) event trigger detection, where triggers of events (words that most clearly express the occurrences of events) are detected; (2) entity menResearchers have"
2020.spnlp-1.9,P08-1030,0,0.186391,"statements; (ii) A multiple-span selection model that demonstrates the feasibility of the approach for event extraction as well as for zero- and few-shot settings. 2 tion detection, where all potential arguments (entity mentions) to events are detected; and (3) argument role prediction, where relations between detected arguments and trigger words are recognized with respect to each event type’s defined set of roles. Much prior work adopts a pipelined approach to these 3 subtasks or focuses on a subset of the subtasks based on gold entity mention spans. These include feature-based approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013, inter alia) and neural approaches (Nguyen and Grishman, 2015; Chen et al., 2015, 2017; Nguyen and Grishman, 2018; Sha et al., 2018, inter alia). Because pipelined approaches suffer from error propagation in which the error from earlier subtasks (e.g. entity mention detection) is inherited by later subtasks, joint modeling of the 3 subtasks has been attempted. Yang and Mitchell (2016) attempts to jointly model the three components with hand-crafted features, but still need to detect entity mentions and ev"
2020.spnlp-1.9,W13-2322,0,0.0275695,"ll (2016) attempts to jointly model the three components with hand-crafted features, but still need to detect entity mentions and event triggers separately. Nguyen and Nguyen (2019) jointly models the three tasks using neural networks with shared underlying representations. The models proposed in these two works are the baselines used in this paper. Huang et al. (2018) approach zero-shot event extraction by stipulating a graph structure for each event type and finding the event type graph structure whose learned representation most closely matches the learned representation of the parsed AMR (Banarescu et al., 2013) structure of a text. In contrast, our approach forgoes explicit graphstructured semantic representations such as AMR. Background Event extraction is traditionally viewed as three subtasks: (1) event trigger detection, where triggers of events (words that most clearly express the occurrences of events) are detected; (2) entity menResearchers have introduced large question an75 swering (QA) / machine reading comprehension (MRC) datasets in a cloze style (Hermann et al., 2015; Onishi et al., 2016), where a query sentence contains a placeholder and the model fills the blank. Our work can be viewe"
2020.spnlp-1.9,K17-1034,0,0.0412697,"ed; (2) entity menResearchers have introduced large question an75 swering (QA) / machine reading comprehension (MRC) datasets in a cloze style (Hermann et al., 2015; Onishi et al., 2016), where a query sentence contains a placeholder and the model fills the blank. Our work can be viewed as an extension to such work, where multiple placeholders are extracted. Li et al. (2019) casts relation extraction as multiturn QA with natural language questions, where in each turn one argument of the relation is found.. The method requires writing a question for each entity type and each relation type. In (Levy et al., 2017), sets of crowdsourced paraphrastic questions are written for each relation type in the ontology. In contrast, for each event type we use a single declarative bleached statement derived from the annotation guidelines. Soares et al. (2019) proposes a model for relation extraction by filling in two blanks given a contextual relation statement. These three methods focus on binary relation extraction, and do not readily generalize to ?-ary events or relations. Our approach naturally supports variable arity events and relations. 3 The event extraction task as defined in the ACE 2005 dataset also re"
2020.spnlp-1.9,P17-1038,0,0.0848019,"Missing"
2020.spnlp-1.9,P13-1008,0,0.584059,"oach for event extraction as well as for zero- and few-shot settings. 2 tion detection, where all potential arguments (entity mentions) to events are detected; and (3) argument role prediction, where relations between detected arguments and trigger words are recognized with respect to each event type’s defined set of roles. Much prior work adopts a pipelined approach to these 3 subtasks or focuses on a subset of the subtasks based on gold entity mention spans. These include feature-based approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013, inter alia) and neural approaches (Nguyen and Grishman, 2015; Chen et al., 2015, 2017; Nguyen and Grishman, 2018; Sha et al., 2018, inter alia). Because pipelined approaches suffer from error propagation in which the error from earlier subtasks (e.g. entity mention detection) is inherited by later subtasks, joint modeling of the 3 subtasks has been attempted. Yang and Mitchell (2016) attempts to jointly model the three components with hand-crafted features, but still need to detect entity mentions and event triggers separately. Nguyen and Nguyen (2019) jointly models the three tasks using ne"
2020.spnlp-1.9,P15-1017,0,0.118654,"tection, where all potential arguments (entity mentions) to events are detected; and (3) argument role prediction, where relations between detected arguments and trigger words are recognized with respect to each event type’s defined set of roles. Much prior work adopts a pipelined approach to these 3 subtasks or focuses on a subset of the subtasks based on gold entity mention spans. These include feature-based approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013, inter alia) and neural approaches (Nguyen and Grishman, 2015; Chen et al., 2015, 2017; Nguyen and Grishman, 2018; Sha et al., 2018, inter alia). Because pipelined approaches suffer from error propagation in which the error from earlier subtasks (e.g. entity mention detection) is inherited by later subtasks, joint modeling of the 3 subtasks has been attempted. Yang and Mitchell (2016) attempts to jointly model the three components with hand-crafted features, but still need to detect entity mentions and event triggers separately. Nguyen and Nguyen (2019) jointly models the three tasks using neural networks with shared underlying representations. The models proposed in thes"
2020.spnlp-1.9,P19-1129,0,0.0302646,"Missing"
2020.spnlp-1.9,N19-1423,0,0.00622577,"∪ ( TRIGGER : ?) E ← E ∪ ?ˆ end for end for return E ⊲ all events extracted from ? end function “Trigger Identification” section below) specified in the ontology. For those event types whose trigger is found, we proceed with argument extraction (Algorithm 1). ? = {“Kim”, “Pat”}, i.e. a set containing multiple extracted arguments, we replace the placeholder with all arguments, concatenated with the “and” token, and shift the focus to the next placeholder, creating the refined statement: 4.1 Architecture for MRC In light of recent advancements in NLP from large-scale pre-training, we use BERT (Devlin et al., 2019) as our sequence encoder. We first review the answer selector architecture for machine reading comprehension (MRC) used in BERT, then extend it for our approach. Under the formulation of MRC, each training data point is of the form (?, ?) where ? is a natural language question with tokens ? = (?1 , · · · , ? ? ) and ? is the text to extract answers from, with tokens ? = (?1 , · · · , ? ? ). The model returns a span in ? or predicts that the question is not answerable, in which case an empty span is returned. To perform MRC, Devlin et al. (2019) proposed the following architecture. First the qu"
2020.spnlp-1.9,P10-1081,0,0.185147,"tiple-span selection model that demonstrates the feasibility of the approach for event extraction as well as for zero- and few-shot settings. 2 tion detection, where all potential arguments (entity mentions) to events are detected; and (3) argument role prediction, where relations between detected arguments and trigger words are recognized with respect to each event type’s defined set of roles. Much prior work adopts a pipelined approach to these 3 subtasks or focuses on a subset of the subtasks based on gold entity mention spans. These include feature-based approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013, inter alia) and neural approaches (Nguyen and Grishman, 2015; Chen et al., 2015, 2017; Nguyen and Grishman, 2018; Sha et al., 2018, inter alia). Because pipelined approaches suffer from error propagation in which the error from earlier subtasks (e.g. entity mention detection) is inherited by later subtasks, joint modeling of the 3 subtasks has been attempted. Yang and Mitchell (2016) attempts to jointly model the three components with hand-crafted features, but still need to detect entity mentions and event triggers separately."
2020.spnlp-1.9,D15-1166,0,0.0180111,"s (see BERT ( [ CLS, ?1 , · · · , ? ? , SEP, ?1 , · · · , ? ? , SEP]) , where CLS is a special sentinel token whose embedding encompasses the whole string, and SEP is a sentence separator. We denote the output encoding of each question token ?? (1 ≤ ? ≤ ?) as s? ∈ R? , and the encoding of each text token ? ? (1 ≤ ? ≤ ?) as t ? ∈ R? . Additionally, two vectors, bleft and bright , for the left and right boundaries of the answer span are learned. The probability of 77 each token ? ? (1 ≤ ? ≤ ?) being the left or right boundary of the answer span is computed as the attention mechanism proposed by Luong et al. (2015), since the placeholder is of variable length but we desire a fixed-size vector representation:  exp s? · t ? (2) ?? ? = X  exp s?0 · t ? ?0 ∈? X s˜ ? = ? ? ? s? (3) ?left (? ? ) ∝ exp(bleft ·t ? ); ?right (? ? ) ∝ exp(bright ·t ? ) The two vectors bleft and bright act as attention query vectors to the text, resulting in a soft pointer over the text tokens. ? ∈? Multiple Argument Selector Our scenario is fundamentally different from MRC in two ways: (1) Our query is not formulated as a natural language question; instead, it is a cloze-style problem with a natural language statement and a hig"
2020.spnlp-1.9,W18-2501,0,0.0255381,"Missing"
2020.spnlp-1.9,J93-2004,0,0.0696642,"Missing"
2020.spnlp-1.9,P11-1163,0,0.210378,"Missing"
2020.spnlp-1.9,P16-2022,0,0.0246317,"le-sentence contexts, so information available in other sentences is not considered. We use the BIO tagging scheme (Ramshaw and Marcus, 1995), where each token in the text is tagged with B (beginning), I (inside), or O (outside). In a linear-chain CRF, the probability of an output tag sequence ? 1 , · · · , ? ? (for each ?, ? ? ∈ {B, I, O}) given the text ? = (?1 , · · · , ? ? ) is ?(? 1 , · · ·, ? ? |?1 , · · ·, ? ? ) ∝ ? Y Then the attentive placeholder representation s˜ ? , together with its corresponding text token representation t ? , are joined using various matching methods proposed in Mou et al. (2016):5   x ? = s˜ ? ; t ? ; |˜s ? − t ? |; s˜ ? t ? (4) yielding the joined feature vector x ? ∈ R4? . Finally the joined feature vector x ? is passed through a multi-layer feed-forward neural network to get the final potential function for each token and each predicted tag type ? ? ∈ {B, I, O}: ?(? ?−1 , ? ? , ?) = FFNN ? ? (x ? ) (5) In our experiments, we pass x ? through 4 layers, with output dimensions 2?, ?, ?, and 1, respectively, and tanh as the nonlinearity function between layers. Trigger Identification Triggers of events can be thought as a special argument, which usually is the main"
2020.spnlp-1.9,N16-1033,0,0.579574,"3 subtasks or focuses on a subset of the subtasks based on gold entity mention spans. These include feature-based approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013, inter alia) and neural approaches (Nguyen and Grishman, 2015; Chen et al., 2015, 2017; Nguyen and Grishman, 2018; Sha et al., 2018, inter alia). Because pipelined approaches suffer from error propagation in which the error from earlier subtasks (e.g. entity mention detection) is inherited by later subtasks, joint modeling of the 3 subtasks has been attempted. Yang and Mitchell (2016) attempts to jointly model the three components with hand-crafted features, but still need to detect entity mentions and event triggers separately. Nguyen and Nguyen (2019) jointly models the three tasks using neural networks with shared underlying representations. The models proposed in these two works are the baselines used in this paper. Huang et al. (2018) approach zero-shot event extraction by stipulating a graph structure for each event type and finding the event type graph structure whose learned representation most closely matches the learned representation of the parsed AMR (Banarescu"
2020.spnlp-1.9,P15-2060,0,0.0352255,"ew-shot settings. 2 tion detection, where all potential arguments (entity mentions) to events are detected; and (3) argument role prediction, where relations between detected arguments and trigger words are recognized with respect to each event type’s defined set of roles. Much prior work adopts a pipelined approach to these 3 subtasks or focuses on a subset of the subtasks based on gold entity mention spans. These include feature-based approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013, inter alia) and neural approaches (Nguyen and Grishman, 2015; Chen et al., 2015, 2017; Nguyen and Grishman, 2018; Sha et al., 2018, inter alia). Because pipelined approaches suffer from error propagation in which the error from earlier subtasks (e.g. entity mention detection) is inherited by later subtasks, joint modeling of the 3 subtasks has been attempted. Yang and Mitchell (2016) attempts to jointly model the three components with hand-crafted features, but still need to detect entity mentions and event triggers separately. Nguyen and Nguyen (2019) jointly models the three tasks using neural networks with shared underlying representations. The mode"
2020.spnlp-1.9,N13-1106,1,0.834066,"Missing"
2020.spnlp-1.9,D16-1241,0,0.0159971,"whose learned representation most closely matches the learned representation of the parsed AMR (Banarescu et al., 2013) structure of a text. In contrast, our approach forgoes explicit graphstructured semantic representations such as AMR. Background Event extraction is traditionally viewed as three subtasks: (1) event trigger detection, where triggers of events (words that most clearly express the occurrences of events) are detected; (2) entity menResearchers have introduced large question an75 swering (QA) / machine reading comprehension (MRC) datasets in a cloze style (Hermann et al., 2015; Onishi et al., 2016), where a query sentence contains a placeholder and the model fills the blank. Our work can be viewed as an extension to such work, where multiple placeholders are extracted. Li et al. (2019) casts relation extraction as multiturn QA with natural language questions, where in each turn one argument of the relation is found.. The method requires writing a question for each entity type and each relation type. In (Levy et al., 2017), sets of crowdsourced paraphrastic questions are written for each relation type in the ontology. In contrast, for each event type we use a single declarative bleached"
2020.spnlp-1.9,P18-2124,0,0.0388815,"Missing"
2020.spnlp-1.9,D16-1264,0,0.101594,"Missing"
2020.spnlp-1.9,W95-0107,0,0.159501,"an, we consider the problem of answer span selection as a tagging problem, first proposed in Yao et al. (2013), where answer spans are tagged using a linear chain CRF (Lafferty et al., 2001). By considering answer span selection as tagging, our model selects potentially multiple spans for a query. We enforce the constraint that all extracted spans come from the same sentence in the text, but in general this constraint need not be enforced. Additionally, our model operates on single-sentence contexts, so information available in other sentences is not considered. We use the BIO tagging scheme (Ramshaw and Marcus, 1995), where each token in the text is tagged with B (beginning), I (inside), or O (outside). In a linear-chain CRF, the probability of an output tag sequence ? 1 , · · · , ? ? (for each ?, ? ? ∈ {B, I, O}) given the text ? = (?1 , · · · , ? ? ) is ?(? 1 , · · ·, ? ? |?1 , · · ·, ? ? ) ∝ ? Y Then the attentive placeholder representation s˜ ? , together with its corresponding text token representation t ? , are joined using various matching methods proposed in Mou et al. (2016):5   x ? = s˜ ? ; t ? ; |˜s ? − t ? |; s˜ ? t ? (4) yielding the joined feature vector x ? ∈ R4? . Finally the joined feat"
2020.spnlp-1.9,P19-1279,0,0.0146141,"d the model fills the blank. Our work can be viewed as an extension to such work, where multiple placeholders are extracted. Li et al. (2019) casts relation extraction as multiturn QA with natural language questions, where in each turn one argument of the relation is found.. The method requires writing a question for each entity type and each relation type. In (Levy et al., 2017), sets of crowdsourced paraphrastic questions are written for each relation type in the ontology. In contrast, for each event type we use a single declarative bleached statement derived from the annotation guidelines. Soares et al. (2019) proposes a model for relation extraction by filling in two blanks given a contextual relation statement. These three methods focus on binary relation extraction, and do not readily generalize to ?-ary events or relations. Our approach naturally supports variable arity events and relations. 3 The event extraction task as defined in the ACE 2005 dataset also requires finding an event trigger— a span in the text that most clearly expresses the event’s occurrence. In this example, the trigger is the word “killed”. For a consistent implementation, we consider the trigger to be a special argument o"
2020.spnlp-1.9,P17-1076,0,0.019571,"Missing"
2021.acl-long.213,C18-1139,0,0.0124865,"2012) collect how-to instructions in a variety of domains, while Wambsganss and Fromm (2019) focus on automotive repair instructions. Branavan et al. (2012) exploit instructions in a game manual to improve an agent’s performance. Dalvi et al. (2019) and Amini et al. (2020) turn to modeling textual descriptions of physical and biological mechanisms. Weller et al. (2020) propose models that generalize to new task descriptions. The tasks proposed in this work are germane to standard NLP tasks, such as named entity recognition (Ratinov and Roth, 2009), part-of-speech tagging (Petrov et al., 2012; Akbik et al., 2018), and coreference resolution (Pradhan et al., 2014). Structure extraction is conceptually similar to syntactic (Socher et al., 2013) and semantic parsing (Berant et al., 2013), which Pertierra et al. (2017) attempt for a subsection of tax law. Argument instantiation is closest to the task of aligning predicate argument structures (Roth and Frank, 2012; Wolfe et al., 2013). We frame argument instantiation as iteratively completing a statement in natural language. Chen et al. (2020) refine generic statements by copying strings from input text, with the goal of detecting events. Chan et al. (2019"
2021.acl-long.213,D13-1160,0,0.0446489,"ons in a game manual to improve an agent’s performance. Dalvi et al. (2019) and Amini et al. (2020) turn to modeling textual descriptions of physical and biological mechanisms. Weller et al. (2020) propose models that generalize to new task descriptions. The tasks proposed in this work are germane to standard NLP tasks, such as named entity recognition (Ratinov and Roth, 2009), part-of-speech tagging (Petrov et al., 2012; Akbik et al., 2018), and coreference resolution (Pradhan et al., 2014). Structure extraction is conceptually similar to syntactic (Socher et al., 2013) and semantic parsing (Berant et al., 2013), which Pertierra et al. (2017) attempt for a subsection of tax law. Argument instantiation is closest to the task of aligning predicate argument structures (Roth and Frank, 2012; Wolfe et al., 2013). We frame argument instantiation as iteratively completing a statement in natural language. Chen et al. (2020) refine generic statements by copying strings from input text, with the goal of detecting events. Chan et al. (2019) extend transformer-based language models to permit inserting tokens anywhere in a sequence, thus allowing to modify an existing sequence. For argument instantiation, we make"
2021.acl-long.213,P12-1014,0,0.0347514,"veral NLP and specialized reasoning techniques, with contextualized language models playing a major role. Clark et al. (2019) take the route of sorting questions into different types, and working on specialized solvers. In contrast, our approach is to treat each question identically, but to decompose the process of answering into a sequence of subtasks. The language of statutes is related to procedural language, which describes steps in a process. Zhang et al. (2012) collect how-to instructions in a variety of domains, while Wambsganss and Fromm (2019) focus on automotive repair instructions. Branavan et al. (2012) exploit instructions in a game manual to improve an agent’s performance. Dalvi et al. (2019) and Amini et al. (2020) turn to modeling textual descriptions of physical and biological mechanisms. Weller et al. (2020) propose models that generalize to new task descriptions. The tasks proposed in this work are germane to standard NLP tasks, such as named entity recognition (Ratinov and Roth, 2009), part-of-speech tagging (Petrov et al., 2012; Akbik et al., 2018), and coreference resolution (Pradhan et al., 2014). Structure extraction is conceptually similar to syntactic (Socher et al., 2013) and"
2021.acl-long.213,P19-1424,0,0.0631751,"Missing"
2021.acl-long.213,2020.spnlp-1.9,1,0.762085,"Missing"
2021.acl-long.213,P18-1078,0,0.0499073,"Missing"
2021.acl-long.213,petrov-etal-2012-universal,0,0.0167148,"ocess. Zhang et al. (2012) collect how-to instructions in a variety of domains, while Wambsganss and Fromm (2019) focus on automotive repair instructions. Branavan et al. (2012) exploit instructions in a game manual to improve an agent’s performance. Dalvi et al. (2019) and Amini et al. (2020) turn to modeling textual descriptions of physical and biological mechanisms. Weller et al. (2020) propose models that generalize to new task descriptions. The tasks proposed in this work are germane to standard NLP tasks, such as named entity recognition (Ratinov and Roth, 2009), part-of-speech tagging (Petrov et al., 2012; Akbik et al., 2018), and coreference resolution (Pradhan et al., 2014). Structure extraction is conceptually similar to syntactic (Socher et al., 2013) and semantic parsing (Berant et al., 2013), which Pertierra et al. (2017) attempt for a subsection of tax law. Argument instantiation is closest to the task of aligning predicate argument structures (Roth and Frank, 2012; Wolfe et al., 2013). We frame argument instantiation as iteratively completing a statement in natural language. Chen et al. (2020) refine generic statements by copying strings from input text, with the goal of detecting even"
2021.acl-long.213,P13-1045,0,0.0806304,"creases the amount of available training data by a factor of 210. 3 Baseline models We provide baselines for three tasks, omitting structure extraction because it is the one task with the highest return on human annotation effort2 . In other words, if humans could annotate for any of these four tasks, structure extraction is where we posit their involvement would be the most worthwhile. Further, Pertierra et al. (2017) have shown that the related task of semantic parsing of legal statutes is a difficult task, calling for a complex model. 3.1 Argument identification We run the Stanford parser (Socher et al., 2013) on the statutes, and extract all noun phrases as spans – specifically, all NNP, NNPS, PRP$, NP and NML constituents. While de-formatting legal text can boost parser performance (Morgenstern, 2014), we found it made little difference in our case. As an orthogonal approach, we train a BERTbased CRF model for the task of BIO tagging. With the 9 sections in the SARA v2 statutes, we create 7 equally-sized splits by grouping §68, 3301 and 7703 into a single split. We run a 7-fold crossvalidation, using 1 split as a dev set, 1 split as a test set, and the remaining as training data. We embed each pa"
2021.acl-long.213,P14-2006,0,0.0397419,"adient descent to maximize the log-likelihood of the sequence of gold tags. We experiment with using Legal BERT (Holzenberger et al., 2020) and BERT-base-cased (Devlin et al., 2019) as our BERT model. We freeze its parameters and optionally unfreeze the last layer. We use a batch size of 32 paragraphs, a learning rate of 10−3 and the Adam optimizer (Kingma and Ba, 2015). Based on F1 score measured on the dev set, the best model uses Legal BERT and unfreezes its last layer. Test results are shown in Table 1. 3.2 Argument coreference Argument coreference differs from the usual coreference task (Pradhan et al., 2014), even though we are using similar terminology, and frame it in a similar way. In argument coreference, it is equally 2 Code for the experiments can be found under https: //github.com/SgfdDttt/sara_v2 avg ± stddev 17.6 ± 4.4 77.9 ± 5.0 28.6 ± 6.2 avg ± stddev 64.7 ± 15.0 69.0 ± 24.2 66.2 ± 20.5 macro 16.6 77.3 27.3 macro 65.1 59.8 62.4 Table 1: Argument identification results. Average and standard deviations are computed across test splits. as important to link two coreferent argument mentions as it is not to link two different arguments. In contrast, regular coreference emphasizes the predict"
2021.acl-long.213,W09-1119,0,0.0794777,"rocedural language, which describes steps in a process. Zhang et al. (2012) collect how-to instructions in a variety of domains, while Wambsganss and Fromm (2019) focus on automotive repair instructions. Branavan et al. (2012) exploit instructions in a game manual to improve an agent’s performance. Dalvi et al. (2019) and Amini et al. (2020) turn to modeling textual descriptions of physical and biological mechanisms. Weller et al. (2020) propose models that generalize to new task descriptions. The tasks proposed in this work are germane to standard NLP tasks, such as named entity recognition (Ratinov and Roth, 2009), part-of-speech tagging (Petrov et al., 2012; Akbik et al., 2018), and coreference resolution (Pradhan et al., 2014). Structure extraction is conceptually similar to syntactic (Socher et al., 2013) and semantic parsing (Berant et al., 2013), which Pertierra et al. (2017) attempt for a subsection of tax law. Argument instantiation is closest to the task of aligning predicate argument structures (Roth and Frank, 2012; Wolfe et al., 2013). We frame argument instantiation as iteratively completing a statement in natural language. Chen et al. (2020) refine generic statements by copying strings fro"
2021.acl-long.213,2020.emnlp-main.105,0,0.0418721,"s. In contrast, our approach is to treat each question identically, but to decompose the process of answering into a sequence of subtasks. The language of statutes is related to procedural language, which describes steps in a process. Zhang et al. (2012) collect how-to instructions in a variety of domains, while Wambsganss and Fromm (2019) focus on automotive repair instructions. Branavan et al. (2012) exploit instructions in a game manual to improve an agent’s performance. Dalvi et al. (2019) and Amini et al. (2020) turn to modeling textual descriptions of physical and biological mechanisms. Weller et al. (2020) propose models that generalize to new task descriptions. The tasks proposed in this work are germane to standard NLP tasks, such as named entity recognition (Ratinov and Roth, 2009), part-of-speech tagging (Petrov et al., 2012; Akbik et al., 2018), and coreference resolution (Pradhan et al., 2014). Structure extraction is conceptually similar to syntactic (Socher et al., 2013) and semantic parsing (Berant et al., 2013), which Pertierra et al. (2017) attempt for a subsection of tax law. Argument instantiation is closest to the task of aligning predicate argument structures (Roth and Frank, 201"
2021.acl-long.213,P13-2012,1,0.742795,"Missing"
2021.acl-long.213,2020.tacl-1.13,0,0.0268727,"reasoning. We refer to this as structure extraction. This mapping can be trivial, with the taxpayer and taxable year generally staying the same across subsections. Some mappings are more involved, such as the taxpayer from §152(b)(1) becoming the dependent in §152(a). Providing annotations for this task in general requires expert knowledge, as many references are implicit, and some must be resolved using guidance from Treasury Regulations. Our approach contrasts with recent efforts in breaking down complex questions into atomic questions, with the possibility of referring to previous answers (Wolfson et al., 2020). Statutes contain their own breakdown into atomic questions. In addition, our structure is interpretable by a Prolog engine. We provide structure extraction annotations for SARA in the style of Horn clauses (Horn, 1951), using common logical operators, as shown in the bottom left of Figure 1. We also provide character offsets for the start and end of each subsection. Argument identification and coreference, and structure extraction can be done with the statutes only. They correspond to extracting a shallow version of the symbolic solver of Holzenberger et al. (2020). Argument instantiation We"
2021.acl-long.213,zhang-etal-2012-automatically,0,0.0358797,"ence domain, with the goal of using the prescriptive knowledge from science textbooks to answer exam questions. The core of their model relies on several NLP and specialized reasoning techniques, with contextualized language models playing a major role. Clark et al. (2019) take the route of sorting questions into different types, and working on specialized solvers. In contrast, our approach is to treat each question identically, but to decompose the process of answering into a sequence of subtasks. The language of statutes is related to procedural language, which describes steps in a process. Zhang et al. (2012) collect how-to instructions in a variety of domains, while Wambsganss and Fromm (2019) focus on automotive repair instructions. Branavan et al. (2012) exploit instructions in a game manual to improve an agent’s performance. Dalvi et al. (2019) and Amini et al. (2020) turn to modeling textual descriptions of physical and biological mechanisms. Weller et al. (2020) propose models that generalize to new task descriptions. The tasks proposed in this work are germane to standard NLP tasks, such as named entity recognition (Ratinov and Roth, 2009), part-of-speech tagging (Petrov et al., 2012; Akbik"
2021.adaptnlp-1.22,N19-1423,0,0.0900422,"Missing"
2021.adaptnlp-1.22,2020.acl-main.627,0,0.055485,"Missing"
2021.adaptnlp-1.22,D19-5543,0,0.0536501,"Missing"
2021.adaptnlp-1.22,P17-2081,0,0.42471,"ons in which one wants to learn a model for a task in a particular domain with too few instances of in-domain data to directly learn a model. Common approaches for domain adaptation make use of fine-tuning (Dabre et al., 2019; Li and Specia, 2019; Imankulova et al., 2019), in which a model is pretrained on a large amount of out-of-domain but task-relevant data and then refined toward the target domain by subsequently training on in-domain data. This fine-tuning procedure is often performed in one stage: the pretrained model is trained on the in-domain data until convergence (Chu et al., 2017; Min et al., 2017a). We propose a gradual finetuning approach, in which a model is iteratively trained to convergence on data whose distribution progressively approaches that of the in-domain data. Intuitively, the model is eased toward the target domain rather than abruptly shifting to it. Inspired by the general approach of curriculum learning of training a model on a trajectory from easier instances to more difficult instances (Bengio et al., 2009), we train a model on a sequence of datasets, each of which would be increasingly difficult to learn on its own due to its size. Each dataset in the sequence inte"
2021.adaptnlp-1.22,2020.acl-main.740,0,0.0396179,"Missing"
2021.adaptnlp-1.22,P18-1031,0,0.018911,"tives, so it can be applied to any system. Domain adaptation can also be achieved using curriculum learning. Zhang et al. (2019) use a curriculum learning approach to adapt a generaldomain machine translation model to a target domain while also using data whose domain is unknown. Inspired by curriculum learning (Bengio et al., 2009), which highlights the importance of the order of training instances, we propose a multistage fine-tuning strategy for domain adaptation. In this work, we order a sequence of fine-tuning datasets from least similar to the target domain to most similar. Related Work Howard and Ruder (2018) propose an effective inductive transfer learning method for language model fine-tuning and demonstrate improvements on text classification tasks. Gururangan et al. (2020) also show improvements on target task performance by fine-tuning pretrained language models on in-domain data and on the target task’s training data. In this work, we focus on adapting the entire model, not just the underlying language model encoder. Our approach is a form of transductive transfer (Pan and Yang, 2009), in which the pretraining and fine-tuning tasks are the same. In the transductive transfer setting, we hope"
2021.adaptnlp-1.22,W19-6613,0,0.0538881,"Missing"
2021.adaptnlp-1.22,P07-1034,0,0.17006,"omputational Linguistics slot classification in the target domain. We then consider (2) an event extraction task from the ACE 2005 dataset (LDC2006T06) for which we augment the Arabic target domain data with English data. Gradual fine-tuning is also simple to implement. If fine-tuning is already supported by the code, then only new configuration files need to be created to specify the (amount of) data used at each iteration.1 No adjustments to model or training code are needed. Just by modifying the training approach, one can obtain substantial improvements.2 2 Frank, 2019; Fei et al., 2020). Jiang and Zhai (2007) propose a method for upweighting the importance of target domain instances relative to source domain instances to improve domain adaptation. The iterative increase in concentration of target domain data in the mixed domain data used in gradual fine-tuning can be seen as analogous to giving target domain instances more weight. In contrast to all the aforementioned approaches, gradual fine-tuning requires no modification to existing models or learning objectives, so it can be applied to any system. Domain adaptation can also be achieved using curriculum learning. Zhang et al. (2019) use a curri"
2021.adaptnlp-1.22,P16-1009,0,0.0946469,"Missing"
2021.adaptnlp-1.22,D19-1030,0,0.0235903,"Missing"
2021.adaptnlp-1.22,D19-1585,0,0.0470243,"Missing"
2021.adaptnlp-1.22,P19-1078,0,0.151864,"odel encoder. Our approach is a form of transductive transfer (Pan and Yang, 2009), in which the pretraining and fine-tuning tasks are the same. In the transductive transfer setting, we hope to learn task-specific information by training on a large (potentially out-of-domain) dataset, and then subsequently adjust model parameters based on domainspecific information learned from in-domain data. Transductive transfer has been effective for tasks such as question answering (Min et al., 2017b), machine translation (Sennrich et al., 2015), and open information extraction (Sarhan and Spruit, 2020). Wu et al. (2019) fine-tune toward a target domain for dialogue state tracking using Gradient Episodic Memory (Lopez-Paz and Ranzato, 2017) to avoid catastrophic forgetting (McCloskey and Cohen, 1989). Ahn et al. (2019) introduce an uncertaintybased regularization method to overcome catastrophic forgetting in the continual learning setting. There have also been successful approaches for cross-lingual information extraction and semantic role labeling using no target language data (Subburathinam et al., 2019), mixed source and translated target language data (Fei et al., 2020), and language-independent model tra"
2021.adaptnlp-1.22,N19-1189,0,0.061234,"Missing"
2021.eacl-demos.19,P14-2082,0,0.0154406,"d clinical data (Savova et al., 2010; Soysal et al., 2018), text summarization (Glavaˇs ˇ and Snajder, 2014; Kedzie et al., 2015), questionanswering (Llorens et al., 2015; Zhou et al., 2019), etc. The task is most commonly viewed as a classification task where given a pair of events and its textual context, the temporal relation between them needs to be identified. The construction of the TimeBank corpus (Pustejovsky et al., 2003) largely spurred the research in temporal relation extraction. It included 14 temporal relation labels. Other corpora (Verhagen et al., 2007, 2010; Sun et al., 2013; Cassidy et al., 2014) reduced the number of labels to a smaller number owing to lower inter-annotator agreements and sparse annotations. Various types of models (Chambers et al., 2014; Cheng and Miyao, 2017; Leeuwenberg and Moens, 2017; Ning et al., 2017; Vashishtha et al., 2019; Zhou et al., 2021) have been used in the recent years to extract temporal relations from text. 4 UltraFine is slightly different in that the types are bucketed into 3 categories of different granularity, but without explicit subtyping relations. 151 In this work, we use Vashishtha et al. (2019)’s best model and retrain it using XLM-R. We"
2021.eacl-demos.19,2020.acl-main.749,1,0.89307,"Missing"
2021.eacl-demos.19,P18-1009,0,0.020115,"ction step (which is left to the FrameNet parser). Instead, both training and inference assumes given mentions, and the task we are concerned about in this paper is mention linking. 2.3 Entity Typing Entity typing assigns a fine-grained semantic label to a span of text, where the span is a mention of some entity found by the FrameNet parser. Traditionally, labels include PER, GPE, ORG, etc., but recent work in fine-grained entity typing seek to classify spans into types defined by hierarchical type ontologies (e.g. BBN (Weischedel and Brunstein, 2005), FIGER (Ling and Weld, 2012), UltraFine4 (Choi et al., 2018), COLLIE (Allen et al., 2020)). Such ontologies refine coarse types like PER to fine-grained types such as /person/artist/singer that sits on a type hierarchy. A portion of the AIDA ontology (LDC2019E07) is illustrated in Figure 2. To support fine-grained ontologies, we employ a recent coarse-to-fine-decoding entity typing model (Chen et al., 2020a) that is specifically designed to assign types that are defined by hierarchical ontologies. The use of a coarse-to-fine model also allows users to select between coarse- and finegrained types. We swap the underlying encoder from ELMo (Peters et al.,"
2021.eacl-demos.19,2020.lrec-1.423,0,0.0206723,"Unless there are annotation dependencies, individual modules can be inserted, replaced, merged, or bypassed depending on the application. We discuss two example applications of our C ONCRETE-based modules, one of which further extracts relations and the other performs cross-sentence argument linking for events. 2 Tasks The overarching application of LOME is to extract an entity- and event-centric knowledge graph from a textual document. In particular, we are interested in using these graphs to support a multilingual schema learning task (KAIROS3 ) for which data has been annotated by the LDC (Cieri et al., 2020). As a result, some parts of LOME are designed for compatibility with the KAIROS event and entity ontology. Nonetheless, there is significant overlap with publicly available datasets, which we describe for those tasks. Figure 1 presents the architecture of our pipeline. Besides the FrameNet parser, which is run first, the remaining modules can be run in any order, if at all. In addition, our use of a standardized data schema for communication allows for the integration of third-party systems. In this section, we will go into 3 This goal is to develop a system that identifies, links, and tempor"
2021.eacl-demos.19,2020.acl-main.747,0,0.0808112,"Missing"
2021.eacl-demos.19,J14-1002,0,0.239207,"ning-over-schemas. 150 further detail for each task. 2.1 FrameNet Parsing FrameNet parsing is a semantic role labeling style task. The goal is to find all the frames and their roles, as well as the trigger spans associated with them in a sentence. Frames are concepts, such as events or entities, in a sentences. Every frame is associated with some roles, and both of them are triggered by spans in the sentence. Unlike most previous work (Yang and Mitchell, 2017; Peng et al., 2018; Swayamdipta et al., 2018), our system is not conditioned on the trigger spans or frames. We perform “full parsing” (Das et al., 2014), where the input is a raw sentence, and the output is the complete structure predictions. As the first model in the whole pipeline system, the trigger spans found by the FrameNet parser will be used as candidate spans for all other tasks. 2.2 Entity Coreference Resolution In coreference resolution, the goal is to cluster spans in the text that refer to the same entity. Neural models for doing so typically encode the text first before identifying possible mentions (Lee et al., 2017; Joshi et al., 2019, 2020). These spans are scored pairwise to determine whether two spans refer to each other. T"
2021.eacl-demos.19,P14-1136,0,0.0618737,"Missing"
2021.eacl-demos.19,2020.tacl-1.5,0,0.012868,"Budnikov et al., 2019). We benchmark the performance of our model on each language. We report the average F1 of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005) by language in Table 3. We can compare the model’s performance to monolingual gold-only baselines, where they exist. For 5 https://github.com/aboSamoor/polyglot 6 http://hltcoe.github.io/concrete/ 152 7 A role is considered to be correctly predicted only when its frame is precisely predicted. 8 We use AdamW and a learning rate of 5 × 10−6 . English, we trained an identical model but instead use SpanBERT (Joshi et al., 2020), an English-only encoder finetuned for English OntoNotes coreference. That model achieves 92.2 average (dev.) F1, compared to our 92.7. There is also a comparable system for Russian AnCor from Le et al. (2019), which achieves 79.9 F1 using the model from Lee et al. (2018) and RuBERT (Kuratov and Arkhipov, 2019). This shows that our single, multilingual model, can perform similarly to monolingual models, with the advantage that our model does not need to perform language ID. This finding mirrors prior findings showing multilingual encoders are strong cross-lingually (Wu and Dredze, 2019). Lang"
2021.eacl-demos.19,D19-1588,0,0.0118609,"018), our system is not conditioned on the trigger spans or frames. We perform “full parsing” (Das et al., 2014), where the input is a raw sentence, and the output is the complete structure predictions. As the first model in the whole pipeline system, the trigger spans found by the FrameNet parser will be used as candidate spans for all other tasks. 2.2 Entity Coreference Resolution In coreference resolution, the goal is to cluster spans in the text that refer to the same entity. Neural models for doing so typically encode the text first before identifying possible mentions (Lee et al., 2017; Joshi et al., 2019, 2020). These spans are scored pairwise to determine whether two spans refer to each other. These scores then determine coreference clusters by decoding under a variety of strategies (Lee et al., 2018; Xu and Choi, 2020). In this work, we choose a constant-memory variant of that model which also achieves high perentity veh aircraft guages using a single multilingual model, and to enable transfer between languages. wea other bomb bullets other other ammunition other molotov cocktail bullets other Figure 2: A portion of the AIDA entity type ontology. formance (Xia et al., 2020). The motivation"
2021.eacl-demos.19,P15-1155,0,0.0299357,"being typed as type t as si,t , we perform Borda counting Pto select the most confident type t∗ = arg maxt i r(i, t) over all t’s in a specific type level, where r(i, t) = 1/rankt (si,t ) is the ranking relevance score used in Borda counting. 2.4 Temporal Relation Extraction The task of temporal relation extraction focuses on finding the chronology of events (e.g., Before, After, Overlaps) in text. Extracting temporal relation is useful for various downstream tasks – curating structured clinical data (Savova et al., 2010; Soysal et al., 2018), text summarization (Glavaˇs ˇ and Snajder, 2014; Kedzie et al., 2015), questionanswering (Llorens et al., 2015; Zhou et al., 2019), etc. The task is most commonly viewed as a classification task where given a pair of events and its textual context, the temporal relation between them needs to be identified. The construction of the TimeBank corpus (Pustejovsky et al., 2003) largely spurred the research in temporal relation extraction. It included 14 temporal relation labels. Other corpora (Verhagen et al., 2007, 2010; Sun et al., 2013; Cassidy et al., 2014) reduced the number of labels to a smaller number owing to lower inter-annotator agreements and sparse annot"
2021.eacl-demos.19,D18-2012,0,0.0123658,"are then labeled by the typing module. To parse a sentence, we run the model to find all frames, and then find their roles conditioned on the frames. # Sentences # Frames # Roles 3120 311 1333 18604 2209 6687 32419 3853 11277 Table 1: Statistics of FrameNet v1.7 System Inputs and Outputs The system can consume, as input, either tokenized or untokenized text, which is first tokenized either by whitespace or with a multilingual tokenizer, PolyGlot.5 However, this tokenization is not necessarily used by all modules, which may choose to either operate on the raw text itself or on a SentencePiece (Kudo and Richardson, 2018) retokenization. The system outputs a C ONCRETE communication file for each input document. This output file contains annotations including entities, events, coreference, entity types, and temporal relations. This schema used is entirely self-contained and the well-documented library also contains tools for visualizing and inspecting C ONCRETE files.6 For the web demo, the output is displayed in the browser. 4 We train the FrameNet parser on the FrameNet v1.7 corpus following Das et al. (2014), with statistics in Table 1. We evaluate the results with exact matching as our metric,7 and get 56.3"
2021.eacl-demos.19,D17-1018,0,0.0250409,"yamdipta et al., 2018), our system is not conditioned on the trigger spans or frames. We perform “full parsing” (Das et al., 2014), where the input is a raw sentence, and the output is the complete structure predictions. As the first model in the whole pipeline system, the trigger spans found by the FrameNet parser will be used as candidate spans for all other tasks. 2.2 Entity Coreference Resolution In coreference resolution, the goal is to cluster spans in the text that refer to the same entity. Neural models for doing so typically encode the text first before identifying possible mentions (Lee et al., 2017; Joshi et al., 2019, 2020). These spans are scored pairwise to determine whether two spans refer to each other. These scores then determine coreference clusters by decoding under a variety of strategies (Lee et al., 2018; Xu and Choi, 2020). In this work, we choose a constant-memory variant of that model which also achieves high perentity veh aircraft guages using a single multilingual model, and to enable transfer between languages. wea other bomb bullets other other ammunition other molotov cocktail bullets other Figure 2: A portion of the AIDA entity type ontology. formance (Xia et al., 20"
2021.eacl-demos.19,N18-2108,0,0.0535114,"Missing"
2021.eacl-demos.19,E17-1108,0,0.0614311,"Missing"
2021.eacl-demos.19,N19-4019,0,0.0171683,"ion on using the Docker container, web demo, and demo video at https://nlp.jhu.edu/demos. by following a multilingual input example. A sentence-level parser identifies both INGESTION events and their arguments. To connect these events cross-sententially, the system clusters coreferent mentions and predicts the temporal relations between the events. LOME, which supports finegrained entity types, additionally labels entities like the rabbit with LIVING THING / ANIMAL. Several prior packages have also used advances in state-of-the-art models to build comprehensive information extraction systems. Li et al. (2019) present an event, relation, and entity extraction and coreference system for three languages: English, Russian, and Ukrainian. Li et al. (2020, GAIA) extend that work to support cross-media documents. However, both of these systems consist of languagespecific models that operate on monolingual documents after first identifying the language. On the other hand, work prioritizing coverage across tens or hundreds of languages is limited in their scope in extraction (Akbik and Li, 2016; Pan et al., 2017). Like prior work, LOME is focused on extracting entities and events from raw text documents. H"
2021.eacl-demos.19,2020.acl-demos.11,0,0.449761,"are competitive with the (monolingual) state-of-the-art. We achieve this through the use of multilingual encoders like XLM-R (Conneau et al., 2020) and leveraging multilingual training data. LOME is available as a Docker container on Docker Hub. In addition, a lightweight version of the system is accessible as a web demo. 1 Introduction As information extraction capabilities continue to improve due to advances in modeling, encoders, and data collection, we can now look (back) toward making richer predictions at the documentlevel, with a large ontology, and across multiple languages. Recently, Li et al. (2020) noted that despite a growth of open-source NLP software in general, there is still a lack of available software for knowledge extraction. We wish to provide a starting point that allows others to build increasingly comprehensive document-level knowledge graphs of events and entities from text in many languages.1 Therefore, we demonstrate LOME, a system for multilingual information extraction with large ontologies. Figure 1 shows the high-level pipeline ∗ Equal Contribution Information on using the Docker container, web demo, and demo video at https://nlp.jhu.edu/demos. by following a multilin"
2021.eacl-demos.19,C16-1137,0,0.0639288,"and evaluate on their combined test set.11 Results on the combined test set are reported in Table 6. We use this model as the default temporal relation extraction model in LOME. 9 Please refer to Chen et al. (2020a) for the exact definitions of the evaluation metric. 153 10 The train and dev set of TBD has a total of 4,590 instances and the test set has 1,405 instances of event-event relations. 11 We consider only event-event relations and the combined dataset has 5,987 (1,249) instances in the train (test) set. We also test our default model on a Chinese temporal relation extraction dataset (Li et al., 2016).12 In the zero-shot setting, we get a micro F1 score of 52.6 on the provided dataset, as compared to a majority baseline of 37.5.13 Similar to the default temporal system in LOME, we use the XLM-R version of Vashishtha et al. (2019)’s model obtaining relation embeddings for the Chinese dataset and train an SVM model using the transfer learning approach to get a micro F1 score of 64.4.14 Relation Precision Recall F1 before after includes is included 68 74 83 44 89 69 5 15 77 71 10 22 Table 6: Result on the combined test set of TempEval3 and TimeBank-Dense when trained with just 4 temporal rela"
2021.eacl-demos.19,D19-1641,0,0.0276022,"Missing"
2021.eacl-demos.19,2020.acl-main.713,0,0.039008,"on the combined test set of TempEval3 and TimeBank-Dense when trained with just 4 temporal relation labels 5 5.1 Extensions Incorporating third-party systems Besides the core components described above, we also discuss the viability of including additional modules that may not fit directly in the core pipeline but can be included depending on the downstream application. For example, the system described above does not predict any relation information, which is needed for the motivating application of downstream schema inference. To do so, we wrote a C ONCRETE and Docker wrapper around OneIE (Lin et al., 2020) and attached it at the end of the pipeline. With our C ONCRETE based design, the integration of any third-party module can be done via implementing the AnnotateCommunicationService service interface, which can ensure compatibility between LOME and external modules. The OneIE wrapper is one example of an external module. 5.2 evaluation, which aims to produce document-level knowledge graphs.15 Each given document may be in English, Russian, or Spanish. On a development set consisting solely of text-only documents,16 we started with initial predictions made by GAIA (Li et al., 2020), for entity"
2021.eacl-demos.19,S15-2134,0,0.0176977,"rm Borda counting Pto select the most confident type t∗ = arg maxt i r(i, t) over all t’s in a specific type level, where r(i, t) = 1/rankt (si,t ) is the ranking relevance score used in Borda counting. 2.4 Temporal Relation Extraction The task of temporal relation extraction focuses on finding the chronology of events (e.g., Before, After, Overlaps) in text. Extracting temporal relation is useful for various downstream tasks – curating structured clinical data (Savova et al., 2010; Soysal et al., 2018), text summarization (Glavaˇs ˇ and Snajder, 2014; Kedzie et al., 2015), questionanswering (Llorens et al., 2015; Zhou et al., 2019), etc. The task is most commonly viewed as a classification task where given a pair of events and its textual context, the temporal relation between them needs to be identified. The construction of the TimeBank corpus (Pustejovsky et al., 2003) largely spurred the research in temporal relation extraction. It included 14 temporal relation labels. Other corpora (Verhagen et al., 2007, 2010; Sun et al., 2013; Cassidy et al., 2014) reduced the number of labels to a smaller number owing to lower inter-annotator agreements and sparse annotations. Various types of models (Chambers"
2021.eacl-demos.19,H05-1004,0,0.0964284,"that we are provided gold spans. This is motivated by the location of coreference in LOME. In addition, while they use a frozen encoder, we found that finetuning improves performance.8 Finally, we train on the full OntoNotes 5.0 (Weischedel et al., 2013; Pradhan et al., 2013), a subset of SemEval 2010 Task 1 (Recasens et al., 2010), and two additional sources of Russian data, RuCor (Toldova et al., 2014) and AnCor (Budnikov et al., 2019). We benchmark the performance of our model on each language. We report the average F1 of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005) by language in Table 3. We can compare the model’s performance to monolingual gold-only baselines, where they exist. For 5 https://github.com/aboSamoor/polyglot 6 http://hltcoe.github.io/concrete/ 152 7 A role is considered to be correctly predicted only when its frame is precisely predicted. 8 We use AdamW and a learning rate of 5 × 10−6 . English, we trained an identical model but instead use SpanBERT (Joshi et al., 2020), an English-only encoder finetuned for English OntoNotes coreference. That model achieves 92.2 average (dev.) F1, compared to our 92.7. There is also a comparable system f"
2021.eacl-demos.19,D17-1108,0,0.0119294,"assification task where given a pair of events and its textual context, the temporal relation between them needs to be identified. The construction of the TimeBank corpus (Pustejovsky et al., 2003) largely spurred the research in temporal relation extraction. It included 14 temporal relation labels. Other corpora (Verhagen et al., 2007, 2010; Sun et al., 2013; Cassidy et al., 2014) reduced the number of labels to a smaller number owing to lower inter-annotator agreements and sparse annotations. Various types of models (Chambers et al., 2014; Cheng and Miyao, 2017; Leeuwenberg and Moens, 2017; Ning et al., 2017; Vashishtha et al., 2019; Zhou et al., 2021) have been used in the recent years to extract temporal relations from text. 4 UltraFine is slightly different in that the types are bucketed into 3 categories of different granularity, but without explicit subtyping relations. 151 In this work, we use Vashishtha et al. (2019)’s best model and retrain it using XLM-R. We evaluate their model using the transfer learning approach described in their work and retrain it on TimeBankDense (TBD) (Cassidy et al., 2014). TBD uses a reduced set of 5 temporal relation labels – before, after, includes, is includ"
2021.eacl-demos.19,N19-1250,0,0.0212056,"Missing"
2021.eacl-demos.19,P17-1178,0,0.0515866,"used advances in state-of-the-art models to build comprehensive information extraction systems. Li et al. (2019) present an event, relation, and entity extraction and coreference system for three languages: English, Russian, and Ukrainian. Li et al. (2020, GAIA) extend that work to support cross-media documents. However, both of these systems consist of languagespecific models that operate on monolingual documents after first identifying the language. On the other hand, work prioritizing coverage across tens or hundreds of languages is limited in their scope in extraction (Akbik and Li, 2016; Pan et al., 2017). Like prior work, LOME is focused on extracting entities and events from raw text documents. However, LOME is language-agnostic; all components prioritize multilinguality. Using XLM-R (Conneau et al., 2020) as the underlying encoder paves the way for both training on multilingual data (where it exists) and inference in many languages.2 Our pipeline includes a full FrameNet parser for events and their arguments, neural coreference resolution, an entity typing model over large ontologies, and temporal resolution between events. Our system is designed to be modular: each component is trained ind"
2021.eacl-demos.19,N18-1135,0,0.0159434,"d temporally sequences complex events. More information at https://www.darpa.mil/program/knowledgedirected-artificial-intelligencereasoning-over-schemas. 150 further detail for each task. 2.1 FrameNet Parsing FrameNet parsing is a semantic role labeling style task. The goal is to find all the frames and their roles, as well as the trigger spans associated with them in a sentence. Frames are concepts, such as events or entities, in a sentences. Every frame is associated with some roles, and both of them are triggered by spans in the sentence. Unlike most previous work (Yang and Mitchell, 2017; Peng et al., 2018; Swayamdipta et al., 2018), our system is not conditioned on the trigger spans or frames. We perform “full parsing” (Das et al., 2014), where the input is a raw sentence, and the output is the complete structure predictions. As the first model in the whole pipeline system, the trigger spans found by the FrameNet parser will be used as candidate spans for all other tasks. 2.2 Entity Coreference Resolution In coreference resolution, the goal is to cluster spans in the text that refer to the same entity. Neural models for doing so typically encode the text first before identifying possible menti"
2021.eacl-demos.19,N15-3018,1,0.836599,"Missing"
2021.eacl-demos.19,N18-1202,0,0.0229946,"et al., 2018), COLLIE (Allen et al., 2020)). Such ontologies refine coarse types like PER to fine-grained types such as /person/artist/singer that sits on a type hierarchy. A portion of the AIDA ontology (LDC2019E07) is illustrated in Figure 2. To support fine-grained ontologies, we employ a recent coarse-to-fine-decoding entity typing model (Chen et al., 2020a) that is specifically designed to assign types that are defined by hierarchical ontologies. The use of a coarse-to-fine model also allows users to select between coarse- and finegrained types. We swap the underlying encoder from ELMo (Peters et al., 2018) to XLM-R to be able to assign types over mentions in different lanThe base typing model in Chen et al. (2020a) supports entity typing on entity mentions. We extend this model to gain the ability to perform entity typing on entities, i.e. clusters of entity mentions. Since our decoder is coarse-to-fine and predicts a type at each level of the type hierarchy, we employ Borda voting on each level. Specifically, given a coreference chain comprising mentions m1,··· ,n , and the score for mention mi being typed as type t as si,t , we perform Borda counting Pto select the most confident type t∗ = ar"
2021.eacl-demos.19,W13-3516,0,0.0883972,"Missing"
2021.eacl-demos.19,W09-2411,0,0.0948432,"Missing"
2021.eacl-demos.19,D18-1412,0,0.0138533,"ces complex events. More information at https://www.darpa.mil/program/knowledgedirected-artificial-intelligencereasoning-over-schemas. 150 further detail for each task. 2.1 FrameNet Parsing FrameNet parsing is a semantic role labeling style task. The goal is to find all the frames and their roles, as well as the trigger spans associated with them in a sentence. Frames are concepts, such as events or entities, in a sentences. Every frame is associated with some roles, and both of them are triggered by spans in the sentence. Unlike most previous work (Yang and Mitchell, 2017; Peng et al., 2018; Swayamdipta et al., 2018), our system is not conditioned on the trigger spans or frames. We perform “full parsing” (Das et al., 2014), where the input is a raw sentence, and the output is the complete structure predictions. As the first model in the whole pipeline system, the trigger spans found by the FrameNet parser will be used as candidate spans for all other tasks. 2.2 Entity Coreference Resolution In coreference resolution, the goal is to cluster spans in the text that refer to the same entity. Neural models for doing so typically encode the text first before identifying possible mentions (Lee et al., 2017; Josh"
2021.eacl-demos.19,S13-2001,0,0.0757837,"Missing"
2021.eacl-demos.19,P19-1280,1,0.901648,"Missing"
2021.eacl-demos.19,S07-1014,0,0.039708,"r various downstream tasks – curating structured clinical data (Savova et al., 2010; Soysal et al., 2018), text summarization (Glavaˇs ˇ and Snajder, 2014; Kedzie et al., 2015), questionanswering (Llorens et al., 2015; Zhou et al., 2019), etc. The task is most commonly viewed as a classification task where given a pair of events and its textual context, the temporal relation between them needs to be identified. The construction of the TimeBank corpus (Pustejovsky et al., 2003) largely spurred the research in temporal relation extraction. It included 14 temporal relation labels. Other corpora (Verhagen et al., 2007, 2010; Sun et al., 2013; Cassidy et al., 2014) reduced the number of labels to a smaller number owing to lower inter-annotator agreements and sparse annotations. Various types of models (Chambers et al., 2014; Cheng and Miyao, 2017; Leeuwenberg and Moens, 2017; Ning et al., 2017; Vashishtha et al., 2019; Zhou et al., 2021) have been used in the recent years to extract temporal relations from text. 4 UltraFine is slightly different in that the types are bucketed into 3 categories of different granularity, but without explicit subtyping relations. 151 In this work, we use Vashishtha et al. (201"
2021.eacl-demos.19,M95-1005,0,0.613928,"ning details. Unlike that work, we operate under the assumption that we are provided gold spans. This is motivated by the location of coreference in LOME. In addition, while they use a frozen encoder, we found that finetuning improves performance.8 Finally, we train on the full OntoNotes 5.0 (Weischedel et al., 2013; Pradhan et al., 2013), a subset of SemEval 2010 Task 1 (Recasens et al., 2010), and two additional sources of Russian data, RuCor (Toldova et al., 2014) and AnCor (Budnikov et al., 2019). We benchmark the performance of our model on each language. We report the average F1 of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005) by language in Table 3. We can compare the model’s performance to monolingual gold-only baselines, where they exist. For 5 https://github.com/aboSamoor/polyglot 6 http://hltcoe.github.io/concrete/ 152 7 A role is considered to be correctly predicted only when its frame is precisely predicted. 8 We use AdamW and a learning rate of 5 × 10−6 . English, we trained an identical model but instead use SpanBERT (Joshi et al., 2020), an English-only encoder finetuned for English OntoNotes coreference. That model achieves 92.2 average (dev.) F1, com"
2021.eacl-demos.19,D19-1077,0,0.0355343,"nBERT (Joshi et al., 2020), an English-only encoder finetuned for English OntoNotes coreference. That model achieves 92.2 average (dev.) F1, compared to our 92.7. There is also a comparable system for Russian AnCor from Le et al. (2019), which achieves 79.9 F1 using the model from Lee et al. (2018) and RuBERT (Kuratov and Arkhipov, 2019). This shows that our single, multilingual model, can perform similarly to monolingual models, with the advantage that our model does not need to perform language ID. This finding mirrors prior findings showing multilingual encoders are strong cross-lingually (Wu and Dredze, 2019). Language O Arabic CatalanS ChineseO DutchS EnglishO ItalianS RussianA SpanishS # Training # Eval Docs Avg. F1 359 829 1810 145 2802 80 573 875 44 142 252 23 343 17 127 140 71.3 58.7 90.8 63.5 92.7 47.2 77.3 63.5 AIDA M18 LDC2019E07 AIDA M36 LDC2020E29 GAIA Language # of entities English Russian Ukrainian 4,433 4,826 4,261 English Spanish Russian 703 557 729 English Spanish Russian 42.8M 11.1M 2.4M Table 4: Statistics of the datasets used for training our entity typing model. commonly used fine-grained entity type ontologies. We report the results in micro-F1 in Table 5. Ontology Prior state-"
2021.eacl-demos.19,2020.emnlp-main.695,1,0.890542,"Missing"
2021.eacl-demos.19,2020.emnlp-main.686,0,0.0138037,"in the whole pipeline system, the trigger spans found by the FrameNet parser will be used as candidate spans for all other tasks. 2.2 Entity Coreference Resolution In coreference resolution, the goal is to cluster spans in the text that refer to the same entity. Neural models for doing so typically encode the text first before identifying possible mentions (Lee et al., 2017; Joshi et al., 2019, 2020). These spans are scored pairwise to determine whether two spans refer to each other. These scores then determine coreference clusters by decoding under a variety of strategies (Lee et al., 2018; Xu and Choi, 2020). In this work, we choose a constant-memory variant of that model which also achieves high perentity veh aircraft guages using a single multilingual model, and to enable transfer between languages. wea other bomb bullets other other ammunition other molotov cocktail bullets other Figure 2: A portion of the AIDA entity type ontology. formance (Xia et al., 2020). The motivation here is robustness: we prioritize the ability to soundly run on all document lengths over slightly better performing but fragile systems. In addition, because this coreference resolution model is part of a broader entity-"
2021.eacl-demos.19,D17-1128,0,0.0242829,"hat identifies, links, and temporally sequences complex events. More information at https://www.darpa.mil/program/knowledgedirected-artificial-intelligencereasoning-over-schemas. 150 further detail for each task. 2.1 FrameNet Parsing FrameNet parsing is a semantic role labeling style task. The goal is to find all the frames and their roles, as well as the trigger spans associated with them in a sentence. Frames are concepts, such as events or entities, in a sentences. Every frame is associated with some roles, and both of them are triggered by spans in the sentence. Unlike most previous work (Yang and Mitchell, 2017; Peng et al., 2018; Swayamdipta et al., 2018), our system is not conditioned on the trigger spans or frames. We perform “full parsing” (Das et al., 2014), where the input is a raw sentence, and the output is the complete structure predictions. As the first model in the whole pipeline system, the trigger spans found by the FrameNet parser will be used as candidate spans for all other tasks. 2.2 Entity Coreference Resolution In coreference resolution, the goal is to cluster spans in the text that refer to the same entity. Neural models for doing so typically encode the text first before identif"
2021.eacl-demos.19,D19-1332,0,0.0152026,"select the most confident type t∗ = arg maxt i r(i, t) over all t’s in a specific type level, where r(i, t) = 1/rankt (si,t ) is the ranking relevance score used in Borda counting. 2.4 Temporal Relation Extraction The task of temporal relation extraction focuses on finding the chronology of events (e.g., Before, After, Overlaps) in text. Extracting temporal relation is useful for various downstream tasks – curating structured clinical data (Savova et al., 2010; Soysal et al., 2018), text summarization (Glavaˇs ˇ and Snajder, 2014; Kedzie et al., 2015), questionanswering (Llorens et al., 2015; Zhou et al., 2019), etc. The task is most commonly viewed as a classification task where given a pair of events and its textual context, the temporal relation between them needs to be identified. The construction of the TimeBank corpus (Pustejovsky et al., 2003) largely spurred the research in temporal relation extraction. It included 14 temporal relation labels. Other corpora (Verhagen et al., 2007, 2010; Sun et al., 2013; Cassidy et al., 2014) reduced the number of labels to a smaller number owing to lower inter-annotator agreements and sparse annotations. Various types of models (Chambers et al., 2014; Cheng"
2021.emnlp-main.149,K19-1035,0,0.0237412,"projection and self-training use the same translated text and differ only by label source, the results indicate that the external knowledge from frozen mBERTbased alignment is worse than what the model learns from source language data alone. Thus, further performance improvement could be achieved with an improved aligner. 7 Related Work tors (Platanios et al., 2018), or language-specific semantic spaces (Luo et al., 2021). Conversely, adversarial training (Ganin et al., 2016) has been used to discourage models from learning language-specific information (Chen et al., 2018; Keung et al., 2019; Ahmad et al., 2019). 8 Conclusion In this paper, we explore data projection and the use of silver data in zero-shot cross-lingual IE, facilitated by neural machine translation and word alignment. Recent advances in pretrained encoders have improved machine translation systems and word aligners in terms of intrinsic evaluation. We conduct an extensive extrinsic evaluation and study how the encoders themselves—and components containing them—impact performance on a range of downstream tasks and languages. With a test bed of English–Arabic IE tasks, we find that adding projected silver training data overall yields i"
2021.emnlp-main.149,P15-1039,0,0.374766,"size and the number of supported languages. 3 Data Projection We create silver versions of the data by automatically projecting annotations from source English gold data to their corresponding machine translations in the target language.3 Data projection transfers word-level annotations in a source language to a target language via word-to-word alignments (Yarowsky et al., 2001). The technique has been used to create cross-lingual datasets for a variety of structured natural language processing tasks, including named entity recognition (Stengel-Eskin et al., 2019) and semantic role labeling (Akbik et al., 2015; Aminian et al., 2017; Fei et al., 2020). To create silver data, as shown in Figure 1, we: (1) translate the source text to the target language using the MT system described in Section 5.2, (2) obtain word alignments between the original and translated parallel text using a word alignment tool, and (3) project the annotations along the word alignments. We then combine silver target data with gold source data to augment the training set for the structured prediction task. For step (1), we rely on a variety of source-toWhile massively multilingual encoders like mBERT and XLM-R enable strong zer"
2021.emnlp-main.149,I17-2003,0,0.3029,"of supported languages. 3 Data Projection We create silver versions of the data by automatically projecting annotations from source English gold data to their corresponding machine translations in the target language.3 Data projection transfers word-level annotations in a source language to a target language via word-to-word alignments (Yarowsky et al., 2001). The technique has been used to create cross-lingual datasets for a variety of structured natural language processing tasks, including named entity recognition (Stengel-Eskin et al., 2019) and semantic role labeling (Akbik et al., 2015; Aminian et al., 2017; Fei et al., 2020). To create silver data, as shown in Figure 1, we: (1) translate the source text to the target language using the MT system described in Section 5.2, (2) obtain word alignments between the original and translated parallel text using a word alignment tool, and (3) project the annotations along the word alignments. We then combine silver target data with gold source data to augment the training set for the structured prediction task. For step (1), we rely on a variety of source-toWhile massively multilingual encoders like mBERT and XLM-R enable strong zero-shot crosslingual pe"
2021.emnlp-main.149,W19-0417,0,0.0206532,"separately for each desired language pair instead of in joint multilingual training. We also examine self-training with translated text to assess when data projection helps crosslingual transfer, and find it to be another viable option for obtaining labels for some tasks. In future work, we will explore how to improve alignment quality and how to combine data projection and self-training techniques. Although projected data may be of lower quality than the original source data due to errors in translation or alignment, it is useful for tasks such as semantic role labeling (Akbik et al., 2015; Aminian et al., 2019), information extraction (Riloff et al., 2002), POS tagging (Yarowsky and Ngai, 2001), and dependency parsing (Ozaki et al., 2021). The intuition is that although the projected data may be noisy, training on it gives a model useful information about the statistics of the target language. Akbik et al. (2015) and Aminian et al. (2017) use bootstrapping algorithms to iteratively construct projected datasets for semantic role labeling. Akbik et al. (2015) additionally use manually defined filters to maintain high data quality, which results in a projected dataset that has low recall with respect t"
2021.emnlp-main.149,J93-2003,0,0.0838433,"Missing"
2021.emnlp-main.149,C18-1233,0,0.0598207,"Missing"
2021.emnlp-main.149,W11-2103,0,0.0721601,"Missing"
2021.emnlp-main.149,Q18-1039,0,0.0224463,"with the exception of parsing. As data projection and self-training use the same translated text and differ only by label source, the results indicate that the external knowledge from frozen mBERTbased alignment is worse than what the model learns from source language data alone. Thus, further performance improvement could be achieved with an improved aligner. 7 Related Work tors (Platanios et al., 2018), or language-specific semantic spaces (Luo et al., 2021). Conversely, adversarial training (Ganin et al., 2016) has been used to discourage models from learning language-specific information (Chen et al., 2018; Keung et al., 2019; Ahmad et al., 2019). 8 Conclusion In this paper, we explore data projection and the use of silver data in zero-shot cross-lingual IE, facilitated by neural machine translation and word alignment. Recent advances in pretrained encoders have improved machine translation systems and word aligners in terms of intrinsic evaluation. We conduct an extensive extrinsic evaluation and study how the encoders themselves—and components containing them—impact performance on a range of downstream tasks and languages. With a test bed of English–Arabic IE tasks, we find that adding projec"
2021.emnlp-main.149,2020.acl-main.747,0,0.284752,"tilingual contextualized encoders 1 Introduction on each data projection component, and the use We consider zero-shot cross-lingual information of different encoders in task-specific models. We extraction (IE), in which training data exists in also offer suggestions for practitioners operating a source language but not in a target language. under different computation budgets on four tasks: Massively multilingual encoders like Multilingual event extraction, named entity recognition, part-ofBERT (mBERT; Devlin et al., 2019) and XLM- speech tagging, and dependency parsing, followRoBERTa (XLM-R; Conneau et al., 2020a) allow ing recent work that uses English-to-Arabic tasks for a strategy of training only on the source lan- as a test bed (Lan et al., 2020). We then apply guage data, trusting entirely in a shared underly- data projection and self-training to three structured ing feature representation across languages (Wu prediction tasks—named entity recognition, partand Dredze, 2019; Conneau et al., 2020b). How- of-speech tagging, and dependency parsing—in ever, in meta-benchmarks like XTREME (Hu et al., multiple target languages. Additionally, we use 2020), such cross-lingual performance on struc- self-"
2021.emnlp-main.149,2020.emnlp-main.382,0,0.285569,"Missing"
2021.emnlp-main.149,2020.findings-emnlp.147,0,0.0184073,". Our transducer uses the same architecture and hyperparameters as our baseline MT system, but with 1k BPE operations instead of 32k. On an internal held-out test set, we get a BLEU score of 96.9 with a unigram score of 98.6, implying few errors will propagate due to the denormalization process.13 Encoder Word Alignment Until recently, alignments have typically been obtained using unsupervised statistical models such as GIZA++ (Och and Ney, 2003) and fast-align (Dyer et al., 2013). Recent work has focused on using the similarities between contextualized embeddings to obtain alignments (Jalili Sabet et al., 2020; Daza and Frank, 2020; Dou and Neubig, 2021), achieving state-of-the-art performance. We use two automatic word alignment tools: fast-align, a widely used statistical alignment tool based on IBM models (Brown et al., 1993); and Awesome-align (Dou and Neubig, 2021), a contextualized embedding-based word aligner that extracts word alignments based on similarities of the tokens’ contextualized embeddings. Awesomealign achieves state-of-the-art performance on five language pairs. Optionally, Awesome-align can be fine-tuned on parallel text with objectives suitable for word alignment and on gold a"
2021.emnlp-main.149,li-etal-2012-parallel,0,0.0142817,"tool based on IBM models (Brown et al., 1993); and Awesome-align (Dou and Neubig, 2021), a contextualized embedding-based word aligner that extracts word alignments based on similarities of the tokens’ contextualized embeddings. Awesomealign achieves state-of-the-art performance on five language pairs. Optionally, Awesome-align can be fine-tuned on parallel text with objectives suitable for word alignment and on gold alignment data. We benchmark the word aligners on the gold standard alignments in the GALE Arabic–English Intrinsic Evaluation Table 2 shows the denor- Parallel Aligned Treebank (Li et al., 2012). We use malized and detokenized BLEU scores for English– the same data splits as Stengel-Eskin et al. (2019), Arabic MT systems with different encoders on the containing 1687, 299, and 315 sentence pairs in the 13 Denormalization code available at https://github. train, dev, and test splits, respectively. To obtain com/KentonMurray/ArabicDetokenizer alignments using fast-align, we append the test 1954 Model fast-align* Layer† AER P R F n/a 47.4 53.9 51.4 52.6 Awesome-align w/o FT mBERT GBv4 8 8 35.6 32.7 78.5 85.6 54.5 55.4 64.4 67.3 XLM-R L64K L128K 16 17 17 40.1 34.0 35.1 78.6 81.5 80.0 48."
2021.emnlp-main.149,2020.acl-main.156,0,0.350417,"Missing"
2021.emnlp-main.149,N19-4009,0,0.0382122,"Missing"
2021.emnlp-main.149,2021.eacl-main.221,0,0.0351332,"text to assess when data projection helps crosslingual transfer, and find it to be another viable option for obtaining labels for some tasks. In future work, we will explore how to improve alignment quality and how to combine data projection and self-training techniques. Although projected data may be of lower quality than the original source data due to errors in translation or alignment, it is useful for tasks such as semantic role labeling (Akbik et al., 2015; Aminian et al., 2019), information extraction (Riloff et al., 2002), POS tagging (Yarowsky and Ngai, 2001), and dependency parsing (Ozaki et al., 2021). The intuition is that although the projected data may be noisy, training on it gives a model useful information about the statistics of the target language. Akbik et al. (2015) and Aminian et al. (2017) use bootstrapping algorithms to iteratively construct projected datasets for semantic role labeling. Akbik et al. (2015) additionally use manually defined filters to maintain high data quality, which results in a projected dataset that has low recall with respect to the source corpus. Fei et al. (2020) and Daza and Frank (2020) find that a non-bootstrapped approach works well for cross-lingua"
2021.emnlp-main.149,P17-1178,0,0.0258177,"arguments, aligned to the source span in the middle. Three and argument roles. For English, we use the same of the IE tasks we consider—ACE, named entity recognition, and BETTER—use span-based projec- English document splits as (Lin et al., 2020). That work does not consider Arabic, so for Arabic we tion, and we filter out projected target spans that are five times longer than the source spans. Two syntac- use the document splits from (Lan et al., 2020). tic tasks—POS tagging and dependency parsing— use token-based projection. For dependency pars- 4.2 Named Entity Recognition We use WikiAnn (Pan et al., 2017) for English– ing, following Tiedemann et al. (2014), we adapt the disambiguation of many-to-one mappings by Arabic and multilingual experiments. The labeling scheme is BIO with 3 types of named entities: PER, choosing as the head the node that is highest up in LOC, and ORG. On top of the encoder, we use a the dependency tree. In the case of a non-aligned linear classification layer with softmax to obtain dependency head, we choose the closest aligned word-level predictions. The labeling is word-level ancestor as the head. while the encoders operate at subword-level, thus, To address issues li"
2021.emnlp-main.149,Y18-1061,0,0.0278057,"d on an alignment of predicted and reference event structures. To find all events in a sentence and their corresponding arguments, we model the structure of the events as a tree, where event triggers are children of the “virtual root” of the sentence and arguments are children of event triggers (Cai et al., 2018). Each node is associated with a span in the text and is labeled with an event or argument type label. We use a model for event structure prediction that has three major components: a contextualized encoder, tagger, and typer (Xia et al., 2021).8 The tagger is a BiLSTM-CRF BIO tagger (Panchendrarajan and Amaresan, 2018) trained to predict child spans conditioned on parent spans and labels. 6 We use the following treebanks: Arabic-PADT, GermanGSD, English-EWT, Spanish-GSD, French-GSD, HindiHDTB, Russian-GSD, Vietnamese-VTB, and Chinese-GSD. 7 https://www.iarpa.gov/index.php/ research-programs/better 8 Code available at https://github.com/ hiaoxui/span-finder The typer is a feedforward network whose inputs are a parent span representation, parent label embedding, and child span representation. The tree is produced level-wise at inference time, first predicting event triggers, typing them, and then predicting a"
2021.emnlp-main.149,D18-1039,0,0.0271163,"roj). This could be the result of noise from alignments of various quality mutually interfering. In fact, selftraining with the same translated text (+Self) outperforms data projection and zero-shot scenarios, again with the exception of parsing. As data projection and self-training use the same translated text and differ only by label source, the results indicate that the external knowledge from frozen mBERTbased alignment is worse than what the model learns from source language data alone. Thus, further performance improvement could be achieved with an improved aligner. 7 Related Work tors (Platanios et al., 2018), or language-specific semantic spaces (Luo et al., 2021). Conversely, adversarial training (Ganin et al., 2016) has been used to discourage models from learning language-specific information (Chen et al., 2018; Keung et al., 2019; Ahmad et al., 2019). 8 Conclusion In this paper, we explore data projection and the use of silver data in zero-shot cross-lingual IE, facilitated by neural machine translation and word alignment. Recent advances in pretrained encoders have improved machine translation systems and word aligners in terms of intrinsic evaluation. We conduct an extensive extrinsic evalu"
2021.emnlp-main.149,W18-6319,0,0.0122623,"gradient updates are applied to them during MT training, whereas the randomly initialized baselines are updated. A preliminary experiment in Zhu et al. (2020) uses a related system that leverages the last layer of BERT. However, that experiment was monolingual, and our hypothesis is that the shared embedding space of a multilingual encoder will aid in training a translation system. BLEU Public 12.7 None 14.9 mBERT GBv4 15.7 15.7 XLM-R L64K L128K 16.0 16.2 15.8 Table 2: BLEU scores of MT systems with different pre-trained encoders on English–Arabic IWSLT’17. IWLST’17 test set using sacreBLEU (Post, 2018). The use of contextualized embeddings from pretrained encoders results in better performance than using a standard randomly initialized MT model regardless of which encoder is used. The best performing system uses our bilingual L64K encoder, but all pretrained encoder-based systems perform well and within 0.5 BLEU points of each other. We hypothesize that the MT systems are able to leverage the shared embedding spaces of the pretrained language models in order to assist with translation. 5.3 Denormalization System Generating text in Arabic is a notoriously difficult problem due to data sparsi"
2021.emnlp-main.149,L16-1144,0,0.0288817,"train with 9.2B words of Arabic text and 26.8B words of English text, more than either XLM-R (2.9B words/23.6B words) or GBv4 (4.3B words/6.1B words).10 We build two English–Arabic joint vocabularies using SentencePiece (Kudo and Richardson, 2018), resulting in two encoders: L64K and L128K. For the latter, we additionally enforce coverage of all Arabic characters after normalization. 5.2 Machine Translation For all of our MT experiments, we use a dataset of 2M sentences from publicly available data including the UN corpus, Global Voices, wikimatrix, and newscommentary11 (Ziemski et al., 2016; Prokopidis et al., 2016; Schwenk et al., 2021; CallisonBurch et al., 2011). We pre-filtered the data using LASER scores to ensure high quality translations are used for our bitext (Schwenk and Douze, 2017; Thompson and Post, 2020). All of our systems are based on the Transformer architecture (Vaswani et al., 2017).11 Our baseline system uses a joint English–Arabic vocabulary with 32k BPE operations (Sennrich et al., 2016). The public system is a publicly released model that has been demonstrated to perform well (Tiedemann, 2020).12 The other systems use contextualized embeddings from frozen pretrained language model"
2021.emnlp-main.149,C02-1070,0,0.280509,"ead of in joint multilingual training. We also examine self-training with translated text to assess when data projection helps crosslingual transfer, and find it to be another viable option for obtaining labels for some tasks. In future work, we will explore how to improve alignment quality and how to combine data projection and self-training techniques. Although projected data may be of lower quality than the original source data due to errors in translation or alignment, it is useful for tasks such as semantic role labeling (Akbik et al., 2015; Aminian et al., 2019), information extraction (Riloff et al., 2002), POS tagging (Yarowsky and Ngai, 2001), and dependency parsing (Ozaki et al., 2021). The intuition is that although the projected data may be noisy, training on it gives a model useful information about the statistics of the target language. Akbik et al. (2015) and Aminian et al. (2017) use bootstrapping algorithms to iteratively construct projected datasets for semantic role labeling. Akbik et al. (2015) additionally use manually defined filters to maintain high data quality, which results in a projected dataset that has low recall with respect to the source corpus. Fei et al. (2020) and Daz"
2021.emnlp-main.149,2021.emnlp-main.802,0,0.0698842,"Missing"
2021.emnlp-main.149,2013.iwslt-evaluation.8,1,0.754834,"gual L64K encoder, but all pretrained encoder-based systems perform well and within 0.5 BLEU points of each other. We hypothesize that the MT systems are able to leverage the shared embedding spaces of the pretrained language models in order to assist with translation. 5.3 Denormalization System Generating text in Arabic is a notoriously difficult problem due to data sparsity problems arising from the morphological richness of the language, frequently necessitating destructive normalization schemes during training that must be heuristically undone in postprocessing to ensure well-formed text (Sajjad et al., 2013). All of the most common multilingual pretrained encoders use a form of destructive normalization which removes diacritics, which causes MT systems to translate into normalized Arabic text. To generate valid Arabic text, we train a sequenceto-sequence model that transduces normalized text into unnormalized text using the Arabic side of our bitext, before and after normalization. Our transducer uses the same architecture and hyperparameters as our baseline MT system, but with 1k BPE operations instead of 32k. On an internal held-out test set, we get a BLEU score of 96.9 with a unigram score of"
2021.emnlp-main.149,2021.eacl-main.115,0,0.037193,"Arabic text and 26.8B words of English text, more than either XLM-R (2.9B words/23.6B words) or GBv4 (4.3B words/6.1B words).10 We build two English–Arabic joint vocabularies using SentencePiece (Kudo and Richardson, 2018), resulting in two encoders: L64K and L128K. For the latter, we additionally enforce coverage of all Arabic characters after normalization. 5.2 Machine Translation For all of our MT experiments, we use a dataset of 2M sentences from publicly available data including the UN corpus, Global Voices, wikimatrix, and newscommentary11 (Ziemski et al., 2016; Prokopidis et al., 2016; Schwenk et al., 2021; CallisonBurch et al., 2011). We pre-filtered the data using LASER scores to ensure high quality translations are used for our bitext (Schwenk and Douze, 2017; Thompson and Post, 2020). All of our systems are based on the Transformer architecture (Vaswani et al., 2017).11 Our baseline system uses a joint English–Arabic vocabulary with 32k BPE operations (Sennrich et al., 2016). The public system is a publicly released model that has been demonstrated to perform well (Tiedemann, 2020).12 The other systems use contextualized embeddings from frozen pretrained language models 9 Details of pretrai"
2021.emnlp-main.149,W17-2619,0,0.0166884,"joint vocabularies using SentencePiece (Kudo and Richardson, 2018), resulting in two encoders: L64K and L128K. For the latter, we additionally enforce coverage of all Arabic characters after normalization. 5.2 Machine Translation For all of our MT experiments, we use a dataset of 2M sentences from publicly available data including the UN corpus, Global Voices, wikimatrix, and newscommentary11 (Ziemski et al., 2016; Prokopidis et al., 2016; Schwenk et al., 2021; CallisonBurch et al., 2011). We pre-filtered the data using LASER scores to ensure high quality translations are used for our bitext (Schwenk and Douze, 2017; Thompson and Post, 2020). All of our systems are based on the Transformer architecture (Vaswani et al., 2017).11 Our baseline system uses a joint English–Arabic vocabulary with 32k BPE operations (Sennrich et al., 2016). The public system is a publicly released model that has been demonstrated to perform well (Tiedemann, 2020).12 The other systems use contextualized embeddings from frozen pretrained language models 9 Details of pretraining can be found in Appendix B. We measure word count with wc -w. 11 See Appendix C for a full list of hyperparameters. 12 The public MT model is available ht"
2021.emnlp-main.149,P16-1162,0,0.00515715,"slation For all of our MT experiments, we use a dataset of 2M sentences from publicly available data including the UN corpus, Global Voices, wikimatrix, and newscommentary11 (Ziemski et al., 2016; Prokopidis et al., 2016; Schwenk et al., 2021; CallisonBurch et al., 2011). We pre-filtered the data using LASER scores to ensure high quality translations are used for our bitext (Schwenk and Douze, 2017; Thompson and Post, 2020). All of our systems are based on the Transformer architecture (Vaswani et al., 2017).11 Our baseline system uses a joint English–Arabic vocabulary with 32k BPE operations (Sennrich et al., 2016). The public system is a publicly released model that has been demonstrated to perform well (Tiedemann, 2020).12 The other systems use contextualized embeddings from frozen pretrained language models 9 Details of pretraining can be found in Appendix B. We measure word count with wc -w. 11 See Appendix C for a full list of hyperparameters. 12 The public MT model is available https://huggingface.co/Helsinki-NLP/ opus-mt-en-ar 1953 10 at as inputs to the encoder. For the decoder vocabulary, these systems all use the GBv4 vocabulary regardless of which pretrained language model was used to augment"
2021.emnlp-main.149,D19-1084,1,0.845728,"Missing"
2021.emnlp-main.149,2020.emnlp-main.8,0,0.0191688,"SentencePiece (Kudo and Richardson, 2018), resulting in two encoders: L64K and L128K. For the latter, we additionally enforce coverage of all Arabic characters after normalization. 5.2 Machine Translation For all of our MT experiments, we use a dataset of 2M sentences from publicly available data including the UN corpus, Global Voices, wikimatrix, and newscommentary11 (Ziemski et al., 2016; Prokopidis et al., 2016; Schwenk et al., 2021; CallisonBurch et al., 2011). We pre-filtered the data using LASER scores to ensure high quality translations are used for our bitext (Schwenk and Douze, 2017; Thompson and Post, 2020). All of our systems are based on the Transformer architecture (Vaswani et al., 2017).11 Our baseline system uses a joint English–Arabic vocabulary with 32k BPE operations (Sennrich et al., 2016). The public system is a publicly released model that has been demonstrated to perform well (Tiedemann, 2020).12 The other systems use contextualized embeddings from frozen pretrained language models 9 Details of pretraining can be found in Appendix B. We measure word count with wc -w. 11 See Appendix C for a full list of hyperparameters. 12 The public MT model is available https://huggingface.co/Helsi"
2021.emnlp-main.149,2020.wmt-1.139,0,0.407731,"UN corpus, Global Voices, wikimatrix, and newscommentary11 (Ziemski et al., 2016; Prokopidis et al., 2016; Schwenk et al., 2021; CallisonBurch et al., 2011). We pre-filtered the data using LASER scores to ensure high quality translations are used for our bitext (Schwenk and Douze, 2017; Thompson and Post, 2020). All of our systems are based on the Transformer architecture (Vaswani et al., 2017).11 Our baseline system uses a joint English–Arabic vocabulary with 32k BPE operations (Sennrich et al., 2016). The public system is a publicly released model that has been demonstrated to perform well (Tiedemann, 2020).12 The other systems use contextualized embeddings from frozen pretrained language models 9 Details of pretraining can be found in Appendix B. We measure word count with wc -w. 11 See Appendix C for a full list of hyperparameters. 12 The public MT model is available https://huggingface.co/Helsinki-NLP/ opus-mt-en-ar 1953 10 at as inputs to the encoder. For the decoder vocabulary, these systems all use the GBv4 vocabulary regardless of which pretrained language model was used to augment the encoder. Incorporating Pretrained LMs In order to make use of the pretrained language models, we use the"
2021.emnlp-main.149,W14-1614,0,0.0771516,"Missing"
2021.emnlp-main.149,D19-1077,1,0.786563,"., 2020). To create silver data, as shown in Figure 1, we: (1) translate the source text to the target language using the MT system described in Section 5.2, (2) obtain word alignments between the original and translated parallel text using a word alignment tool, and (3) project the annotations along the word alignments. We then combine silver target data with gold source data to augment the training set for the structured prediction task. For step (1), we rely on a variety of source-toWhile massively multilingual encoders like mBERT and XLM-R enable strong zero-shot crosslingual performance (Wu and Dredze, 2019; Conneau et al., 2020a), they suffer from the curse of multilinguality (Conneau et al., 2020a): crosslingual effectiveness suffers as the number of supported languages increases for a fixed model size. We would therefore expect that when restricted to only the source and target languages, a bilingual model should perform better than (or at least on par with) a multilingual model of the same size, assuming both languages have sufficient corpora (Wu and Dredze, 2020a). If a practitioner is interested 1 We do not include multilingual T5 (Xue et al., 2021) as in only a small subset of the support"
2021.emnlp-main.149,2020.repl4nlp-1.16,1,0.925623,"a variety of source-toWhile massively multilingual encoders like mBERT and XLM-R enable strong zero-shot crosslingual performance (Wu and Dredze, 2019; Conneau et al., 2020a), they suffer from the curse of multilinguality (Conneau et al., 2020a): crosslingual effectiveness suffers as the number of supported languages increases for a fixed model size. We would therefore expect that when restricted to only the source and target languages, a bilingual model should perform better than (or at least on par with) a multilingual model of the same size, assuming both languages have sufficient corpora (Wu and Dredze, 2020a). If a practitioner is interested 1 We do not include multilingual T5 (Xue et al., 2021) as in only a small subset of the supported languages, it is still an open question on how to best utilize text-to-text models for structured prediction tasks (Ruder et al., 2021). is the multilingual model still the best option? 2 L128K available at https://huggingface.co/ To answer this question, we use English and jhu-clsp/roberta-large-eng-ara-128k 3 Arabic as a test bed. In Table 1, we summarize exCode available at https://github.com/ isting publicly available encoders that support both shijie-wu/cro"
2021.emnlp-main.149,2020.emnlp-main.362,1,0.895717,"a variety of source-toWhile massively multilingual encoders like mBERT and XLM-R enable strong zero-shot crosslingual performance (Wu and Dredze, 2019; Conneau et al., 2020a), they suffer from the curse of multilinguality (Conneau et al., 2020a): crosslingual effectiveness suffers as the number of supported languages increases for a fixed model size. We would therefore expect that when restricted to only the source and target languages, a bilingual model should perform better than (or at least on par with) a multilingual model of the same size, assuming both languages have sufficient corpora (Wu and Dredze, 2020a). If a practitioner is interested 1 We do not include multilingual T5 (Xue et al., 2021) as in only a small subset of the supported languages, it is still an open question on how to best utilize text-to-text models for structured prediction tasks (Ruder et al., 2021). is the multilingual model still the best option? 2 L128K available at https://huggingface.co/ To answer this question, we use English and jhu-clsp/roberta-large-eng-ara-128k 3 Arabic as a test bed. In Table 1, we summarize exCode available at https://github.com/ isting publicly available encoders that support both shijie-wu/cro"
2021.emnlp-main.149,2021.eacl-demos.19,1,0.828272,"Missing"
2021.emnlp-main.149,2021.adaptnlp-1.22,1,0.328699,"Missing"
2021.emnlp-main.149,2021.naacl-main.41,0,0.0220796,"ng zero-shot crosslingual performance (Wu and Dredze, 2019; Conneau et al., 2020a), they suffer from the curse of multilinguality (Conneau et al., 2020a): crosslingual effectiveness suffers as the number of supported languages increases for a fixed model size. We would therefore expect that when restricted to only the source and target languages, a bilingual model should perform better than (or at least on par with) a multilingual model of the same size, assuming both languages have sufficient corpora (Wu and Dredze, 2020a). If a practitioner is interested 1 We do not include multilingual T5 (Xue et al., 2021) as in only a small subset of the supported languages, it is still an open question on how to best utilize text-to-text models for structured prediction tasks (Ruder et al., 2021). is the multilingual model still the best option? 2 L128K available at https://huggingface.co/ To answer this question, we use English and jhu-clsp/roberta-large-eng-ara-128k 3 Arabic as a test bed. In Table 1, we summarize exCode available at https://github.com/ isting publicly available encoders that support both shijie-wu/crosslingual-nlp 1951 Multilingual Bilingual Base Large mBERT (Devlin et al.) GBv4 (Lan et al"
2021.emnlp-main.149,P95-1026,0,0.837635,"ord alignment. If bilingual encoders exist, using them in aligners requires little additional computation. options should be explored if one’s budget allows. In terms of computation budget, using pretrained encoders in a custom MT system requires medium additional computation. Impact of Label Source To assess the quality of Impact of Encoder on MT By comparing the projected annotations in the silver data, we congroups C and E, we observe the performance differ- sider a different way to automatically label transence between the bilingual encoder based MT and lated sentences: self-training (ST; Yarowsky, 1995). the public MT depends on the task and encoder, and For self-training, we translate the source data to the neither MT system clearly outperforms the other in target language, label the translated data using a all settings, despite the bilingual encoder having a zero-shot model trained on source data, and combetter BLEU score. The results suggest that both bine the labeled translations with the source data to 1956 Encoder Data ar de en es fr hi ru vi zh Average NER (F1) mBERT Zero-shot + Self + Proj + Proj (Bi) 41.6 +7.7 -5.8 +0.3 78.8 -0.5 -0.6 -0.7 83.9 +0.4 +0.3 +0.1 73.1 +4.8 +3.6 +5.2 79."
2021.emnlp-main.425,2020.findings-emnlp.222,0,0.099892,"Missing"
2021.emnlp-main.425,2020.crac-1.11,0,0.0222987,"0 40 30 30 80 70 75 60 65 70 All 60 50 55 0 6 12 18 24 0 6 12 18 Top k layers are trainable 24 Figure 5: Average F1 across different models and number of trainable layers. Low vs. All describes the number of documents used for the first fold of LitBank (10 vs. 80) and QBCoref (15 vs. 240). 4.4 Which encoder layers are important? parameters of the encoder and training the top-k layers, along with the rest of the model, for each of the “large” encoders. We investigate LitBank and QBCoref under low and high(er) data conditions. This is motivated by prior work which uses just the top four layers (Aloraini et al., 2020) and by findings from encoder probing that higher layers are more salient for coreference (Tenney et al., 2019). Figure 5 shows that there are gains to training some layers, but it is not always necessary to train the full model. In particular, for transferred models, we observe that unfreezing more layers of the encoder could even lead to worse performance. On the other hand, untrained models generally benefit from training more of the encoder. These trends are observed in both datasets and data quantities.10 This demonstrates that continued training allows us to freeze a substantial fraction"
2021.emnlp-main.425,S10-1022,0,0.0267431,"Missing"
2021.emnlp-main.425,2020.lrec-1.6,0,0.544457,"ing (Joshi et al., 2019, 2020) and additional pretraining (Wu et al., 2020). At the same 2 Coreference Resolution time, the number of parameters used in these modEntity coreference resolution is the task of finding els have increased, raising questions of overfitting clusters of mentions within a document that all reour research to a specific dataset. Several studies show that fully-trained neural models on preex- fer to the same entity. It still remains a difficult challenge in NLP due to several factors like ambiisting large datasets do not transfer well to new domains (Akta¸s et al., 2020; Bamman et al., 2020; guity (Poesio and Artstein, 2008) and dependence on real-world knowledge (Levesque et al., 2012). Timmapathini et al., 2021), and that rule-based There are several large annotated datasets for baselines can still be superior (Poot and van Cracoreference resolution. Annotation guidelines for nenburgh, 2020). Further, while prior work has analyzed fully-trained models for mention pairs, coref differ across these datasets based on the intended goals of the creators, resulting in differlike gender bias (Rudinger et al., 2018; Webster 1 et al., 2018; Zhao et al., 2019), there has not been We use"
2021.emnlp-main.425,2021.eacl-demos.19,1,0.761211,"Missing"
2021.emnlp-main.425,2020.emnlp-main.695,1,0.765956,"Missing"
2021.emnlp-main.425,2021.adaptnlp-1.22,1,0.814453,"Missing"
2021.emnlp-main.425,2020.coling-main.538,0,0.0372811,"d an algorithm for decoding clusters. The incremental coreference (IC OREF) model (Xia et al., 2020) used in this work is a constant-memory adaptation of the end-to-end neural coreference resolution model (Lee et al., 2017) with improvements from subsequent work that incorporates stronger encoders (Joshi et al., 2019, 2020). By creating explicit clusters and performing mention-cluster linking instead of mention-pair linking, IC OREF naturally produces clusters from linking scores. This memory-efficient model is conceptually similar to other recent cluster-based models (Toshniwal et al., 2020; Yu et al., 2020b). This model was chosen because of its competitive performance against the line of end-to-end neural coreference resolution models (Joshi et al., 2019) and memory efficiency, which allows for experiments on longer documents. However, IC OREF, like the models before it, is designed around OntoNotes. As a result, we make minor modifications for compatibility with other datasets by ignoring genre-specific embeddings and implementing an auxiliary objective for entity mention detection, similar to the one adopted by Zhang et al. (2018). For completion, we reformulate the IC OREF model to more pre"
2021.emnlp-main.534,2014.iwslt-evaluation.1,0,0.0110934,": https://huggingface. co/jhu-clsp/bibert-ende. Inspired by the superior performance of BERT on many other tasks, researchers have investigated leveraging using this pre-trained masked language model to enhance translation models, e.g., initializing the parameters of the model’s encoder with BERT parameters (Rothe et al., 2020), and incorporating the output of BERT to each layer of the encoder (Zhu et al., 2020; Weng et al., 2020). In this paper, we demonstrate simply using the output of a pre-trained language model as the input of NMT systems can achieve state-of-the-art results on IWLST’14 (Cettolo et al., 2014) and WMT’14 (Bojar et al., 2014) English↔German (En↔De) translation tasks in the case of without using back translation (Sennrich et al., 2016; Edunov et al., 2018)3 . After conducting a thor3 Though we use a language model that has been trained 6663 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6663–6675 c November 7–11, 2021. 2021 Association for Computational Linguistics ough evaluation of numerous pre-trained language models, we demonstrate that specialized bilingual models perform the best. We then introduce two further refinements, stochast"
2021.emnlp-main.534,D19-5611,0,0.0277221,"XLM-R (Conneau et al., 2020), a model learning cross-lingual representation at scale achieved state-of-the-art results on multiple cross-lingual benchmarks. Recently, an English-Arabic bilingual BERT (Lan et al., 2020) outperformed A RA BERT, M BERT and XLM-R on supervised and zero-shot transfer settings. 6.2 MT with Context-Aware Representations Imamura and Sumita (2019) removed the NMT encoder part and directly fed the output of BERT to the attention mechanism in the decoder. They train the model with two optimization stages, i.e., only training the decoder and fine-tuning BERT. Similarly, Clinchant et al. (2019) have incorporated BERT into NMT models by replacing the embedding layer with BERT parameters and initializing encoder with BERT, but they still notice that NMT model with BERT is not as robust as expected. Rothe et al. (2020) also leveraged pretrained checkpoints (e.g., BERT and GPT) to ini6 Related Work tialize 12-layer NMT encoder and decoder and 6.1 Pre-Trained Embeddings achieved state-of-the-art results. Interestingly, they Traditional pre-trained embeddings are investigated showed that the models with decoder initialized by in type level, e.g., word2vec (Mikolov et al., 2013), GPT fail"
2021.emnlp-main.534,2020.acl-main.747,0,0.306166,"er of a frozen pre-trained language model and feeding them to the embedding layer of the NMT encoder. Rather than randomly initializing the source embedding layer, we use the output of these pre-trained models and do not allow these parameters to update during training. To allow for a deep analysis, we concentrate on one language pair, English↔German (En↔De). In the XLM-R (base) A transformer-based (Vaswani et al., 2017) masked language model trained on 100 languages, using more than two terabytes of filtered CommonCrawl data, which outperforms M BERT on a variety of cross-lingual benchmarks (Conneau et al., 2020). 2.3 How Do Pre-Trained LMs Affect NMT? First we investigate how contextualized embeddings of aforementioned pre-trained language models help NMT models, and explore possible positive and negative factors that may affect NMT models. Dataset We initially consider a low-resource scenario and then show further experiments in a highresource scenario in Section 5. We conduct experiwith additional monolingual data, we only use the provided bitexts during machine translation training. 6664 4 5 https://deepset.ai/german-bert https://github.com/dbmdz/berts ments on the IWSLT’14 English-German dataset,"
2021.emnlp-main.534,N19-1423,0,0.189787,"etrained language models, we show that our B I BERT, a bilingual English-German language model, vastly outperforms all other methods (Section 2). Adding stochastic layer selection to B I BERT improves performance (Section 3). Finally, innovative dual-directional training and fine-tuning with the previous two methods yield around 2 BLEU point gains over the previous state-of-the-art result (Wu et al., 2021) (Section 4). Introduction Pre-trained language models (LMs), trained on a large-scale unlabeled data to capture rich representations of the input, such as ELM O (Peters et al., 2018), BERT (Devlin et al., 2019), XLN ET (Yang et al., 2019) and XLM (Conneau and Lample, 2019) have increasingly attracted attention in various NLP tasks. Either utilizing context-aware representations of input tokens (Peters et al., 2018) or fine-tuning the pre-trained parameters (Devlin 1 Code is available at: https://github.com/ fe1ixxu/BiBERT. 2 Our BiBERT is released at: https://huggingface. co/jhu-clsp/bibert-ende. Inspired by the superior performance of BERT on many other tasks, researchers have investigated leveraging using this pre-trained masked language model to enhance translation models, e.g., initializing the"
2021.emnlp-main.534,D18-1045,0,0.0229336,"pre-trained masked language model to enhance translation models, e.g., initializing the parameters of the model’s encoder with BERT parameters (Rothe et al., 2020), and incorporating the output of BERT to each layer of the encoder (Zhu et al., 2020; Weng et al., 2020). In this paper, we demonstrate simply using the output of a pre-trained language model as the input of NMT systems can achieve state-of-the-art results on IWLST’14 (Cettolo et al., 2014) and WMT’14 (Bojar et al., 2014) English↔German (En↔De) translation tasks in the case of without using back translation (Sennrich et al., 2016; Edunov et al., 2018)3 . After conducting a thor3 Though we use a language model that has been trained 6663 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6663–6675 c November 7–11, 2021. 2021 Association for Computational Linguistics ough evaluation of numerous pre-trained language models, we demonstrate that specialized bilingual models perform the best. We then introduce two further refinements, stochastic layer selection and dual-directional training that yield further improvements. The overview of methods are shown in Figure 1. Overall, our best systems beat publ"
2021.emnlp-main.534,2020.findings-emnlp.352,0,0.22048,"same number of update steps on German texts as G OTT BERT6 . We train a unified 52K vocabulary using the WordPiece tokenizer (Wu et al., 2016), with 67GB English and 67GB German texts which are randomly sampled from the training set. B I BERT EN - DE is trained on TPU v3-8 for four weeks. More details about optimization for B I BERT EN - DE are described in Appendix B. 2.5 Vocabulary Size Selection The vocabulary is fixed for the encoder but still indeterminate for the decoder. In a low-resource machine translation setting, performance is highly sensitive to decoder vocabulary size selection. Gowda and May (2020) demonstrated that a decoder vocabulary using 8K BPE operations performed best across a large grid search. To ensure that 8K vocabulary size is also a suitable choice for the IWSLT’14 (160K parallel sentences) dataset when combined with our method, we search over four candidate decoder vocabulary sizes (8K, 16K, 24K, and 32K) for all aforementioned pre-trained language models. As shown in Figure 2, 8K yields the highest BLEU score for all of our NMT models for De↔En. Thus we select 8K as the vocabulary size of the decoder and use this for all subsequent experiments on IWSLT’14 unless otherwise"
2021.emnlp-main.534,D19-5603,0,0.0334617,"up, e.g., C AMEM BERT for French (Martin et al., 2020) and A RA BERT for Arabic (Baly et al., 2020). Multilingual representations, e.g. M BERT and XLM S (Conneau and Lample, 2019) have been shown to be effective to facilitate cross-lingual learning. XLM-R (Conneau et al., 2020), a model learning cross-lingual representation at scale achieved state-of-the-art results on multiple cross-lingual benchmarks. Recently, an English-Arabic bilingual BERT (Lan et al., 2020) outperformed A RA BERT, M BERT and XLM-R on supervised and zero-shot transfer settings. 6.2 MT with Context-Aware Representations Imamura and Sumita (2019) removed the NMT encoder part and directly fed the output of BERT to the attention mechanism in the decoder. They train the model with two optimization stages, i.e., only training the decoder and fine-tuning BERT. Similarly, Clinchant et al. (2019) have incorporated BERT into NMT models by replacing the embedding layer with BERT parameters and initializing encoder with BERT, but they still notice that NMT model with BERT is not as robust as expected. Rothe et al. (2020) also leveraged pretrained checkpoints (e.g., BERT and GPT) to ini6 Related Work tialize 12-layer NMT encoder and decoder and"
2021.emnlp-main.534,P19-1356,0,0.0190161,"target embeddings to help translation. 2) Embeddings of overlapping En-De sub-word units7 fed to NMT encoders may facilitate translation by bilingual information. Algorithms Adversarial MLE (Wang et al., 2019) DynamicConv(Wu et al., 2019) Macaron Net (Lu* et al., 2020) BERT-Fuse (Zhu et al., 2020) MAT (Fan et al., 2020) Mixed Representations (Wu et al., 2020) UniDrop (Wu et al., 2021) Ours, G OTT BERT Ours, B I BERT De → En 35.18 35.20 35.40 36.11 36.22 36.41 36.88 36.32 37.58 Table 2: Comparison of our work and most recent existing methods on IWSLT’14 De→En. 2.7 Comparison with Existing Work Jawahar et al. (2019) demonstrates that different layers of BERT capture differing linguistic information in a rich, hierarchical structure that mimics classical, compositional tree-like structures. Information in the lower layer (e.g., phrase-level information) gets gradually diluted in higher layers. Thus, to potentially leverage more information encapsulated in the pre-trained language models, we are also interested in exploring how other layers of contextualized embeddings can improve NMT models — rather than simply using the last layer. We denote X as the collection of source language sentences. For each sour"
2021.emnlp-main.534,W17-3204,0,0.0764443,"Missing"
2021.emnlp-main.534,2020.emnlp-main.382,0,0.0656855,"Missing"
2021.emnlp-main.534,2021.ccl-1.108,0,0.0537826,"Missing"
2021.emnlp-main.534,2020.acl-main.156,0,0.0286413,"ion (Liu et al., 2019). This model matches or exceeds the performance of BERT on multiple NLP tasks. • We introduce dual-directional translation models which leverages the inherent bilingual nature of B I BERT with mixed domain training and fine-tuning. When combined with stochastic layer selection, it achieves state-of-the-art performance, i.e., 30.45 for En→De and 38.61 for De→En on the IWSLT’14 dataset, and 31.26 for En→De on the WMT’14 dataset (Section 4). GottBERT A state-of-the-art pure German Roberta model (Scheible et al., 2020) trained on 145G German text data portion of OSCAR (Ortiz Suárez et al., 2020), a huge multilingual corpus extracted from Common Crawl. This has been shown to outperform the other two existing German monolingual models (i.e., German BERT4 from deepset and dbmz BERT5 ) on NER and text classification tasks. Contextualized Embeddings for NMT M BERT (cased) A multilingual BERT (Devlin et al., 2019) pre-trained on 104 highest-resource languages in Wikipedia. 2 2.1 Method In this section, we focus on investigating the effectiveness of using the output (contextualized embeddings) of the last layer of pre-trained language models on building NMT models. Our basic NMT models are"
2021.emnlp-main.534,N19-4009,0,0.0120394,"translation perMT encoder is randomly initialized, as opposed formance if its training data is composed of a to using the pre-trained language model, we ob- mixture of texts in both source and target lan6665 guages. In other words, we expect the source and target language data to enrich the contextualized information for each other to better facilitate translation for both directions (En↔De). Therefore, we propose our bilingual pre-trained language models, dubbed B I BERT. Our B I BERT EN - DE is based on the RoBERTa architecture (Liu et al., 2019) and implemented using the fairseq framework (Ott et al., 2019). In order to make a direct comparison, B I BERT EN - DE is trained on the same German texts as G OTT BERT – just with an additional 146GB of English texts. These are a subset of the English portion in OSCAR – the same dataset the German texts come from. We combine English and German data and shuffle them before training. We train the model using the same number of update steps on German texts as G OTT BERT6 . We train a unified 52K vocabulary using the WordPiece tokenizer (Wu et al., 2016), with 67GB English and 67GB German texts which are randomly sampled from the training set. B I BERT EN -"
2021.emnlp-main.534,W18-6301,0,0.0624074,"Missing"
2021.emnlp-main.534,P02-1040,0,0.109613,"the provided bitexts during machine translation training. 6664 4 5 https://deepset.ai/german-bert https://github.com/dbmdz/berts ments on the IWSLT’14 English-German dataset, which has 160K parallel bilingual sentence pairs. Settings Our model configuration is transformer_iwslt_de_en, a six-layer transformer architecture (Vaswani et al., 2017), with FFN dimension size 1024 and 4 attention heads. We use an embedding dimension of 768 to match the dimension of pre-trained language models. For a consistent comparison with previous works, the evaluation metric is the commonly used tokenized BLEU (Papineni et al., 2002) score calculated with the multi-bleu.perl script. More training details are described in Appendix A. Methods random RO BERTA pre-trained random G OTT BERT pre-trained random M BERT pre-trained random XLM-R pre-trained random B I BERT pre-trained En→De 27.3 28.74(+1.44) 27.80 27.37(−0.43) 27.87 27.85(−0.02) 27.53 29.65(+2.12) De→En 33.56 36.32(+2.76) 34.01 34.26(+0.25) 33.67 35.38 (+1.71) 33.52 37.58(+4.06) Table 1: IWSLT’14 En↔De BLEU scores utilizing contextualized embeddings from various pre-trained language models. random represents the embedding layer of the NMT encoder that is randomly i"
2021.emnlp-main.534,D14-1162,0,0.0895,"lacing the embedding layer with BERT parameters and initializing encoder with BERT, but they still notice that NMT model with BERT is not as robust as expected. Rothe et al. (2020) also leveraged pretrained checkpoints (e.g., BERT and GPT) to ini6 Related Work tialize 12-layer NMT encoder and decoder and 6.1 Pre-Trained Embeddings achieved state-of-the-art results. Interestingly, they Traditional pre-trained embeddings are investigated showed that the models with decoder initialized by in type level, e.g., word2vec (Mikolov et al., 2013), GPT fail to improve the translation performance glove (Pennington et al., 2014) and fastText (Bo- and are even worse than the one whose decoder is janowski et al., 2017). Peters et al. (2018) moved randomly initialized. Similarly, Ma et al. (2020) further from this line and proposed context-aware initialize both transformer encoder and decoder embeddings output from pre-trained bidirectional by XLM-R but fine-tune it on multiple bilingual LSTM (ELM O). Following the attention-based corpora to obtain a multilingual translation model. transformer module (Vaswani et al., 2017), the ar- The preliminary experiments from Zhu et al. (2020) chitectures of GPT models (Radford et"
2021.emnlp-main.534,N18-1202,0,0.156945,"xperimenting over various pretrained language models, we show that our B I BERT, a bilingual English-German language model, vastly outperforms all other methods (Section 2). Adding stochastic layer selection to B I BERT improves performance (Section 3). Finally, innovative dual-directional training and fine-tuning with the previous two methods yield around 2 BLEU point gains over the previous state-of-the-art result (Wu et al., 2021) (Section 4). Introduction Pre-trained language models (LMs), trained on a large-scale unlabeled data to capture rich representations of the input, such as ELM O (Peters et al., 2018), BERT (Devlin et al., 2019), XLN ET (Yang et al., 2019) and XLM (Conneau and Lample, 2019) have increasingly attracted attention in various NLP tasks. Either utilizing context-aware representations of input tokens (Peters et al., 2018) or fine-tuning the pre-trained parameters (Devlin 1 Code is available at: https://github.com/ fe1ixxu/BiBERT. 2 Our BiBERT is released at: https://huggingface. co/jhu-clsp/bibert-ende. Inspired by the superior performance of BERT on many other tasks, researchers have investigated leveraging using this pre-trained masked language model to enhance translation mod"
2021.emnlp-main.534,2020.tacl-1.18,0,0.418261,", 2019) have increasingly attracted attention in various NLP tasks. Either utilizing context-aware representations of input tokens (Peters et al., 2018) or fine-tuning the pre-trained parameters (Devlin 1 Code is available at: https://github.com/ fe1ixxu/BiBERT. 2 Our BiBERT is released at: https://huggingface. co/jhu-clsp/bibert-ende. Inspired by the superior performance of BERT on many other tasks, researchers have investigated leveraging using this pre-trained masked language model to enhance translation models, e.g., initializing the parameters of the model’s encoder with BERT parameters (Rothe et al., 2020), and incorporating the output of BERT to each layer of the encoder (Zhu et al., 2020; Weng et al., 2020). In this paper, we demonstrate simply using the output of a pre-trained language model as the input of NMT systems can achieve state-of-the-art results on IWLST’14 (Cettolo et al., 2014) and WMT’14 (Bojar et al., 2014) English↔German (En↔De) translation tasks in the case of without using back translation (Sennrich et al., 2016; Edunov et al., 2018)3 . After conducting a thor3 Though we use a language model that has been trained 6663 Proceedings of the 2021 Conference on Empirical Methods i"
2021.emnlp-main.534,P16-1009,0,0.0340782,"d leveraging using this pre-trained masked language model to enhance translation models, e.g., initializing the parameters of the model’s encoder with BERT parameters (Rothe et al., 2020), and incorporating the output of BERT to each layer of the encoder (Zhu et al., 2020; Weng et al., 2020). In this paper, we demonstrate simply using the output of a pre-trained language model as the input of NMT systems can achieve state-of-the-art results on IWLST’14 (Cettolo et al., 2014) and WMT’14 (Bojar et al., 2014) English↔German (En↔De) translation tasks in the case of without using back translation (Sennrich et al., 2016; Edunov et al., 2018)3 . After conducting a thor3 Though we use a language model that has been trained 6663 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6663–6675 c November 7–11, 2021. 2021 Association for Computational Linguistics ough evaluation of numerous pre-trained language models, we demonstrate that specialized bilingual models perform the best. We then introduce two further refinements, stochastic layer selection and dual-directional training that yield further improvements. The overview of methods are shown in Figure 1. Overall, our"
2021.emnlp-main.534,2021.naacl-main.302,0,0.0708261,"Missing"
2021.emnlp-main.534,2021.adaptnlp-1.22,1,0.837831,"Missing"
2021.emnlp-main.534,2021.emnlp-main.149,1,0.790635,"Missing"
2021.scil-1.42,N19-1423,0,0.0633391,"Missing"
2021.scil-1.42,C18-1152,0,0.0646405,"Missing"
2021.scil-1.42,2021.ccl-1.108,0,0.0604062,"Missing"
2021.scil-1.42,D19-1514,0,0.0819742,"the mapping between inputs and outputs is unclear, represent an edge case for the assumptions made by the supervised paradigm, and result in systematic divergences between human and model behavior. To demonstrate this, we begin by identifying a set of canonically vague terms in the binary question subset of the Visual Question Answering (VQA) and GQA datasets (Antol et al., 2015; Goyal et al., 2017; Hudson and Manning, 2019) and isolating a subset of images, questions, and answers from these datasets centered around these terms. Using this subset, we show that although the accuracy of LXMERT (Tan and Bansal, 2019) on non-borderline cases is very high, its performance drops—sometimes dramatically—on borderline cases. We then compare the behavior of the model against that of human annotators, finding that while humans display behavior which aligns with theories of meaning for vague terms, model “Is the sky cloudy?” “Is it cloudy?” “Is the sky cloudy?” Figure 1: Given a binary question involving a vague term (in this case, cloudy) humans hedge between “yes” and “no,” following a sigmoid curve with borderline examples falling in the middle. Standard error (grey band) shows that annotator agree even in bord"
2021.starsem-1.12,P98-1013,0,0.192641,"ing infilling models, a user can insert or rewrite text spans at any position in a story. With the proposed extension, generation can be guided via explicit frame semantic constraints, either provided manually or suggested by the model based on surrounding context. Introduction A popular strategy for automatic story generation is to proceed in a coarse-to-fine manner: first by proposing a story plan, and then realizing it into natural language form using large pretrained neural language models (Fan et al., 2018; GoldfarbTarrant et al., 2019). In this work, we study the use of FrameNet frames (Baker et al., 1998) as representational units for such plan guidance. In Frame Semantics (Fillmore, 1976; Fillmore and Baker, 2010), words evoke structural situation types (frames) that describe the common schematic relationships between lexical items. We hypothesize that these structured types can be used to effectively induce the semantic content of text generated by increasingly powerful pretrained language models, yielding a flexible, controllable and domaingeneral model for surface realization of story plans with a variety of dimensions for user guidance. † Corresponding authors. Work done during an interns"
2021.starsem-1.12,N16-1098,0,0.0661808,"t leverages LUs as ordered disjunctive constraint sets. Given a possibly multi-frame sequence and a generative model, our method en129 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 129–142 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics forces the generation of one of the associated LUs for each frame in the sequence. This decoding method is implemented as a plug-and-play module that can be imposed on top of any standard generative language model.2 We evaluate through a sentence-infilling task based on ROCStories (Mostafazadeh et al., 2016), assessing performance on two dimensions: 1) the quality of generation, as measured through perplexity and human evaluation; and 2) the fidelity, which scores whether generated text evokes the frames used as guidance. We demonstrate that our methods utilize guidance to generate frame-evoking surface realizations without meaningfully detracting from the contextual narrative coherence. We also demonstrate the practical applicability of frame-guided generation in a variety of example use cases. 2 Related work Controlled Generation Existing work employs a variety of pretraining strategies to guid"
2021.starsem-1.12,D14-1162,0,0.0860835,"Missing"
2021.starsem-1.12,N18-1119,0,0.0971239,"ed content and style. PPLM (Dathathri et al., 2020) makes use of lightweight attribute classifiers that guide generation without requiring language model retraining. For diverse generation of sentences in a more general scenario, Weir et al. (2020) train models to condition on semantic bit codes obtained from hashing sentence embeddings. Constrained Generation Separate lines of work employ lexical constraints to achieve the same goal of guided and diverse generation. As such, lexically constrained beam search methods such as Grid Beam Search (Hokamp and Liu, 2017) and Dynamic Beam Allocation (Post and Vilar, 2018; Hu et al., 2019a) were proposed as the decoding methods for causal generation with disjunctive positive constraints (Li et al., 2020b), paraphrasing (Hu et al., 2019b; Culkin et al., 2020), machine translation (Zhang et al., 2021), and abstractive summarization (Mao et al., 2020). Lu et al. (2020) generalize beam-search based methods with an algorithm that supports lexical constraints in the conjunctive normal form. Parallel are the approaches that handle lexical constraints in an editing manner: starting with a sequence of keyword constraints and fleshing out a 2 Fairseq-based implementatio"
2021.starsem-1.12,D19-1509,0,0.0306875,"Missing"
2021.starsem-1.12,D18-1462,0,0.0284992,"fy lexical constraints in a soft manner as external memories (Li et al., 2020a, 2019) or constructing constraint-aware training data (Chen et al., 2020). Story Generation Inspired by the traditional pipeline of Reiter and Dale (2000), recent work tackles generation of stories in a coarse-to-fine manner (Fan et al., 2018): based on a premise, a structured outline is generated first, and then an outline-condition model generates the full story. To represent the story outline, existing approaches typically either model it as a latent variable, or use symbolic representations such as key phrases (Xu et al., 2018; Yao et al., 2019; Goldfarb-Tarrant et al., 2019; Gupta et al., 2019; Rashkin et al., 2020), short summaries (Jain et al., 2017; Chen et al., 2019), verb-argument tuples (Martin et al., 2018), or PropBank predicates and arguments (Fan et al., 2019; Goldfarb-Tarrant et al., 2020). Our work can be viewed as an extension of this direction, where a Content Planner model generates an outline as a sequence of FrameNet frames, and our methods generate a surface form story. 3 Data FrameNet FrameNet is a lexical database of English based on Fillmore’s theory of Frame Semantics. It defines more than 12"
2021.starsem-1.12,2020.emnlp-main.349,0,0.0446871,"Missing"
2021.starsem-1.12,P16-1162,0,0.010822,"ered Disjunctive Constraint Sets We develop a disjunctive lexically constrained decoding method (LCD) that extends implementations in Post and Vilar (2018); Hu et al. (2019a) and Li et al. (2020b). We also use Dynamic Beam Allocation (DBA) (Post and Vilar, 2018; Hu et al., 2019a) for beam assignment and next token selection, but we track our constraints differently. As shown in Figure 3, LCD represents a sequence of disjunctive constraint sets as a list of tries, one per frame, each covering a set of disjunctive lexical units (with morphological variants) based on the Byte Pair Encoding (BPE, Sennrich et al., 2016) adopted by GPT-2. Based on this representation, we develop two versions of LCD: LCD-ordered and -unordered, the former of which requires that the constraint sets be completed in the order that the corresponding frame ID tokens are specified. By providing these two versions, we offer the user the flexibility to either enforce the frame-evoking narration being triggered in their desire order, or leave it to be determined by the generative model and decoder. To track the generation progress through constraint sets, we use a global pointer to the currently active disjunctive set. Whenever the act"
2021.starsem-1.12,2020.emnlp-main.701,0,0.0279854,"., 2020b), paraphrasing (Hu et al., 2019b; Culkin et al., 2020), machine translation (Zhang et al., 2021), and abstractive summarization (Mao et al., 2020). Lu et al. (2020) generalize beam-search based methods with an algorithm that supports lexical constraints in the conjunctive normal form. Parallel are the approaches that handle lexical constraints in an editing manner: starting with a sequence of keyword constraints and fleshing out a 2 Fairseq-based implementation and data to be released. sentence via editing operations such as insertion or deletion (Miao et al., 2019; Liu et al., 2019; Sha, 2020; Susanto et al., 2020; ?; Zhang et al., 2020). Finally, it is possible to satisfy lexical constraints in a soft manner as external memories (Li et al., 2020a, 2019) or constructing constraint-aware training data (Chen et al., 2020). Story Generation Inspired by the traditional pipeline of Reiter and Dale (2000), recent work tackles generation of stories in a coarse-to-fine manner (Fan et al., 2018): based on a premise, a structured outline is generated first, and then an outline-condition model generates the full story. To represent the story outline, existing approaches typically either mode"
2021.starsem-1.12,2020.acl-main.325,0,0.0116726,"paraphrasing (Hu et al., 2019b; Culkin et al., 2020), machine translation (Zhang et al., 2021), and abstractive summarization (Mao et al., 2020). Lu et al. (2020) generalize beam-search based methods with an algorithm that supports lexical constraints in the conjunctive normal form. Parallel are the approaches that handle lexical constraints in an editing manner: starting with a sequence of keyword constraints and fleshing out a 2 Fairseq-based implementation and data to be released. sentence via editing operations such as insertion or deletion (Miao et al., 2019; Liu et al., 2019; Sha, 2020; Susanto et al., 2020; ?; Zhang et al., 2020). Finally, it is possible to satisfy lexical constraints in a soft manner as external memories (Li et al., 2020a, 2019) or constructing constraint-aware training data (Chen et al., 2020). Story Generation Inspired by the traditional pipeline of Reiter and Dale (2000), recent work tackles generation of stories in a coarse-to-fine manner (Fan et al., 2018): based on a premise, a structured outline is generated first, and then an outline-condition model generates the full story. To represent the story outline, existing approaches typically either model it as a latent varia"
2021.starsem-1.12,2020.emnlp-main.421,1,0.832102,"Missing"
2021.starsem-1.12,2021.eacl-demos.19,1,0.709222,"Missing"
2021.starsem-1.12,2020.emnlp-main.698,0,0.0463764,"Missing"
2021.unimplicit-1.6,C18-1152,0,0.0626713,"Missing"
C08-1116,H05-1071,0,0.026429,"Missing"
C08-1116,W04-3221,0,0.126528,"e generation of abstracted logical forms through compositional linguistic analysis. The following provides an overview of K NEXT and its target knowledge representation, Episodic Logic. 2.1 Episodic Logic Automatically acquiring general world knowledge from text is not a task that provides an immediate solution to any real world problem.2 Rather, the motivation for acquiring large stores of background knowledge is to enable research within other areas of artificial intelligence, e.g., the construction of systems that can engage in dialogues about everyday topics in unrestricted English, use 1 Almuhareb and Poesio (2004) treat unary attributes as values of binary attributes; e.g., illegal might be the value of a legality attribute. But for many unary attributes, this is a stretch. 2 Unless one regularly needs reminding of facts such as, A WOMAN MAY BOIL A GOAT. 921 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 921–928 Manchester, August 2008 (Some e0: [e0 at-about Now0] [(Many.det x : [x ((attr athletic.a) (plur youngster.n))] [x want.v (Ka (become.v (plur ((attr professional.a) athlete.n))))]) ** e0]) Figure 1: Example EL formula; square brackets indicate"
C08-1116,P97-1003,0,0.035938,"r the class Drug. We extracted such unary attributes by focusing on verbalizations of the form, A(N) X CAN BE Y as in AN ANIMAL CAN BE WARM-BLOODED. 3 3.1 Experimental Setting Corpus Processing Initial reports on the use of K NEXT were focused on the processing of manually created parse trees, on a corpus of limited size (the Brown corpus of Kucera and Francis (1967)). Since that time the system has been modified into a fully automatic extraction system, making use of syntactic parse trees generated by parsers trained on the Penn Treebank. For our studies here, the parser employed was that of Collins (1997) applied to the sentences of the British National Corpus (BNC Consortium, 2001). Our choice of the BNC was motivated by its breadth of genre, its substantial size (100 million words) and its familiarity (and accessibility) to the community. 3.2 p/a ratio (r) r ≥ .06 0 &lt; r &lt; .06 otherwise Gazetteers KNEXT’s gazetteers were used as-is, and were defined based on a variety of sources: miscellaneous publicly available lists, as well as manual enumeration. The classes covered can be seen in the Results section in table 2, where the minimum, maximum and mean size were 2, 249, and 41, respectively. 3."
C08-1116,N03-1011,0,0.0210997,"ized propositions, e.g.: A CANDY-COMPANY MAY HAVE BARS, and A CAR-COMPANY MAY HAVE CUSTOMERS. These examples point to additional areas for improvement beyond sense disambiguation: noncompositional phrase filtering for all NPs, rather than just in the cases of adjectival modification (Mars bar is a Wikipedia topic); and relative discounting of patterns used in the extraction process4 . This later technique is commonly used in specialized extraction systems, such as constructed by Snow et al. (2005) who fit a logistic regression model for hypernym (X is-a Y) classification based on WordNet, and Girju et al. (2003) who trained a classifier to look specifically for part-whole relations. 4 For example, (NP (NNP X) (NNS Y)) may be more semantically ambiguous than, e.g., the possessive construction (NP (NP (NNP X) (POS ’s)) (NP (NNS Y))). 4.2 Unary Attributes Table 4 shows how filtering non-compositional phrases from CAN BE propositions affects extraction volume. Table 5 shows the difference between such post-filtered propositions and those that were deleted. As our filter lists were not built fully automatically, evaluation was performed exclusively by an author with negligible direct involvement in the li"
C08-1116,P99-1041,0,0.0864005,"Missing"
D11-1108,P05-1074,1,0.945191,"rity signal they use is noisier than the sentence-level correspondency in parallel corpora and additionally suffers from problems such as mistaking cousin expressions or antonyms (such as {boy, girl } or {rise, fall }) for paraphrases. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1168–1179, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Abundantly available bilingual parallel corpora have been shown to address both these issues, obtaining paraphrases via a pivoting step over foreign language phrases (Bannard and Callison-Burch, 2005). The coverage of paraphrase lexica extracted from bitexts has been shown to outperform that obtained from other sources (Zhao et al., 2008a). While there have been efforts pursuing the extraction of more powerful paraphrases (Madnani et al., 2007; Callison-Burch, 2008; Cohn and Lapata, 2008; Zhao et al., 2008b), it is not yet clear to what extent sentential paraphrases can be induced from bitexts. In this paper we: phrase table, English paraphrases are obtained by pivoting through foreign language phrases. Since many paraphrases can be extracted for a phrase, Bannard and Callison-Burch rank t"
D11-1108,W03-1004,0,0.0534231,"istical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-"
D11-1108,N03-1003,0,0.352985,"Describe how training paradigms for syntactic/sentential paraphrase models should be tailored to different text-to-text generation tasks. • Demonstrate our framework’s suitability for a variety of text-to-text generation tasks by obtaining state-of-the-art results on the example task of sentence compression. 2 Related Work Madnani and Dorr (2010) survey a variety of datadriven paraphrasing techniques, categorizing them based on the type of data that they use. These include large monolingual texts (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), comparable corpora (Barzilay and Lee, 2003; Dolan et al., 2004), monolingual parallel corpora (Barzilay and McKeown, 2001; Pang et al., 2003), and bilingual parallel corpora (Bannard and Callison-Burch, 2005; Madnani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKe"
D11-1108,P01-1008,0,0.863018,"tated with syntactic constraints: the NP 1 ’s NP 2 the NP 2 of the NP 1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A variety of different types of corpora (and semantic equivalence cues) have been used to automatically induce paraphrase collections for English (Madnani and Dorr, 2010). Perhaps the most natural type of corpus for this task is a monolingual parallel text, which allows sentential paraphrases to be extracted since the sentence pairs in such corpora are perfect paraphrases of each other (Barzilay and McKeown, 2001; Pang et al., 2003). While rich syntactic paraphrases have been learned from monolingual parallel corpora, they suffer from very limited data availability and thus have poor coverage. Other methods obtain paraphrases from raw monolingual text by relying on distributional similarity (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). While vast amounts of data are readily available for these approaches, the distributional similarity signal they use is noisier than the sentence-level correspondency in parallel corpora and additionally suffers from problems such as mistaking cousin expression"
D11-1108,P99-1071,0,0.227065,"scuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems. 1 Introduction Paraphrases are alternative ways of expressing the same information (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). 1168 Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meanin"
D11-1108,P08-1077,0,0.373054,"ollections for English (Madnani and Dorr, 2010). Perhaps the most natural type of corpus for this task is a monolingual parallel text, which allows sentential paraphrases to be extracted since the sentence pairs in such corpora are perfect paraphrases of each other (Barzilay and McKeown, 2001; Pang et al., 2003). While rich syntactic paraphrases have been learned from monolingual parallel corpora, they suffer from very limited data availability and thus have poor coverage. Other methods obtain paraphrases from raw monolingual text by relying on distributional similarity (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). While vast amounts of data are readily available for these approaches, the distributional similarity signal they use is noisier than the sentence-level correspondency in parallel corpora and additionally suffers from problems such as mistaking cousin expressions or antonyms (such as {boy, girl } or {rise, fall }) for paraphrases. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1168–1179, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Abundantly available bilingual parallel corpora have been shown to ad"
D11-1108,J93-2003,0,0.0293765,"with an equal number of nonterminals cNT (γ) = cNT (α) and ∼: {1 . . . cNT (γ)} → {1 . . . cNT (α)} constitutes a one-to-one correspondency function between the nonterminals in γ and α. A nonnegative weight w ≥ 0 is assigned to each rule, reflecting the likelihood of the rule. Rule Extraction Phrase-based approaches to statistical machine translation (and their successors) extract pairs of (e, f ) phrases from automatically word-aligned parallel sentences. Och (2003b) described various heuristics for extracting phrase alignments from the Viterbi word-level alignments that are estimated using Brown et al. (1993) wordalignment models. These phrase extraction heuristics have been extended so that they extract synchronous grammar rules (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006). Most of these extraction methods require that one side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word align"
D11-1108,D08-1021,1,0.937898,"l Methods in Natural Language Processing, pages 1168–1179, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Abundantly available bilingual parallel corpora have been shown to address both these issues, obtaining paraphrases via a pivoting step over foreign language phrases (Bannard and Callison-Burch, 2005). The coverage of paraphrase lexica extracted from bitexts has been shown to outperform that obtained from other sources (Zhao et al., 2008a). While there have been efforts pursuing the extraction of more powerful paraphrases (Madnani et al., 2007; Callison-Burch, 2008; Cohn and Lapata, 2008; Zhao et al., 2008b), it is not yet clear to what extent sentential paraphrases can be induced from bitexts. In this paper we: phrase table, English paraphrases are obtained by pivoting through foreign language phrases. Since many paraphrases can be extracted for a phrase, Bannard and Callison-Burch rank them using a paraphrase probability defined in terms of the translation model probabilities p(f |e) and p(e|f ): X p(e2 |e1 ) = p(e2 , f |e1 ) (1) • Extend the bilingual pivoting approach to paraphrase induction to produce rich syntactic paraphrases. Several subsequent"
D11-1108,P05-1033,0,0.851368,"rases can be extracted for a phrase, Bannard and Callison-Burch rank them using a paraphrase probability defined in terms of the translation model probabilities p(f |e) and p(e|f ): X p(e2 |e1 ) = p(e2 , f |e1 ) (1) • Extend the bilingual pivoting approach to paraphrase induction to produce rich syntactic paraphrases. Several subsequent efforts extended the bilingual pivoting technique, many of which introduced elements of more contemporary syntax-based approaches to statistical machine translation. Madnani et al. (2007) extended the technique to hierarchical phrase-based machine translation (Chiang, 2005), which is formally a synchronous context-free grammar (SCFG) and thus can be thought of as a paraphrase grammar. The paraphrase grammar can paraphrase (or “decode”) input sentences using an SCFG decoder, like the Hiero, Joshua or cdec MT systems (Chiang, 2007; Li et al., 2009; Dyer et al., 2010). Like Hiero, Madnani’s model uses just one nonterminal X instead of linguistic nonterminals. Three additional efforts incorporated linguistic syntax. Callison-Burch (2008) introduced syntactic constraints by labeling all phrases and paraphrases (even non-constituent phrases) with CCGinspired slash cat"
D11-1108,J07-2003,0,0.556159,"araphrase induction to produce rich syntactic paraphrases. Several subsequent efforts extended the bilingual pivoting technique, many of which introduced elements of more contemporary syntax-based approaches to statistical machine translation. Madnani et al. (2007) extended the technique to hierarchical phrase-based machine translation (Chiang, 2005), which is formally a synchronous context-free grammar (SCFG) and thus can be thought of as a paraphrase grammar. The paraphrase grammar can paraphrase (or “decode”) input sentences using an SCFG decoder, like the Hiero, Joshua or cdec MT systems (Chiang, 2007; Li et al., 2009; Dyer et al., 2010). Like Hiero, Madnani’s model uses just one nonterminal X instead of linguistic nonterminals. Three additional efforts incorporated linguistic syntax. Callison-Burch (2008) introduced syntactic constraints by labeling all phrases and paraphrases (even non-constituent phrases) with CCGinspired slash categories (Steedman and Baldridge, 2011), an approach similar to Zollmann and Venugopal (2006)’s syntax-augmented machine translation (SAMT). Callison-Burch did not formally define a synchronous grammar, nor discuss decoding, since his presentation did not inclu"
D11-1108,D07-1008,0,0.150486,"ere the compression rate cr falls in the range 0.5 < cr ≤ 0.8. From these, we randomly select 936 sentences for the development set, as well as 560 sentences for a test set that we use to gauge the performance of our system. 6.4 Grammar Augmentations As we discussed in Section 5, the paraphrase grammar we induce is capable of representing a wide variety of transformations. However, the formalism and extraction method are not explicitly geared towards a compression application. For instance, the synchronous nature of our grammar does not allow us to perform deletions of constituents as done by Cohn and Lapata (2007)’s tree transducers. One way to extend the grammar’s capabilities towards the requirements of a given task is by injecting additional rules designed to capture appropriate operations. For the compression task, this could include adding rules to delete target-side nonterminals: JJ → JJ |ε This would render the grammar asynchronous and require adjustments to the decoding process. Alternatively, we can generate rules that specifically delete particular adjectives from the corpus: JJ → superfluous |ε . In our experiments we evaluate the latter approach by generating optional deletion rules for all"
D11-1108,C08-1018,0,0.256051,"anguage Processing, pages 1168–1179, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Abundantly available bilingual parallel corpora have been shown to address both these issues, obtaining paraphrases via a pivoting step over foreign language phrases (Bannard and Callison-Burch, 2005). The coverage of paraphrase lexica extracted from bitexts has been shown to outperform that obtained from other sources (Zhao et al., 2008a). While there have been efforts pursuing the extraction of more powerful paraphrases (Madnani et al., 2007; Callison-Burch, 2008; Cohn and Lapata, 2008; Zhao et al., 2008b), it is not yet clear to what extent sentential paraphrases can be induced from bitexts. In this paper we: phrase table, English paraphrases are obtained by pivoting through foreign language phrases. Since many paraphrases can be extracted for a phrase, Bannard and Callison-Burch rank them using a paraphrase probability defined in terms of the translation model probabilities p(f |e) and p(e|f ): X p(e2 |e1 ) = p(e2 , f |e1 ) (1) • Extend the bilingual pivoting approach to paraphrase induction to produce rich syntactic paraphrases. Several subsequent efforts extended the bi"
D11-1108,C04-1051,0,0.198539,"radigms for syntactic/sentential paraphrase models should be tailored to different text-to-text generation tasks. • Demonstrate our framework’s suitability for a variety of text-to-text generation tasks by obtaining state-of-the-art results on the example task of sentence compression. 2 Related Work Madnani and Dorr (2010) survey a variety of datadriven paraphrasing techniques, categorizing them based on the type of data that they use. These include large monolingual texts (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), comparable corpora (Barzilay and Lee, 2003; Dolan et al., 2004), monolingual parallel corpora (Barzilay and McKeown, 2001; Pang et al., 2003), and bilingual parallel corpora (Bannard and Callison-Burch, 2005; Madnani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 19"
D11-1108,P10-4002,1,0.175165,"rich syntactic paraphrases. Several subsequent efforts extended the bilingual pivoting technique, many of which introduced elements of more contemporary syntax-based approaches to statistical machine translation. Madnani et al. (2007) extended the technique to hierarchical phrase-based machine translation (Chiang, 2005), which is formally a synchronous context-free grammar (SCFG) and thus can be thought of as a paraphrase grammar. The paraphrase grammar can paraphrase (or “decode”) input sentences using an SCFG decoder, like the Hiero, Joshua or cdec MT systems (Chiang, 2007; Li et al., 2009; Dyer et al., 2010). Like Hiero, Madnani’s model uses just one nonterminal X instead of linguistic nonterminals. Three additional efforts incorporated linguistic syntax. Callison-Burch (2008) introduced syntactic constraints by labeling all phrases and paraphrases (even non-constituent phrases) with CCGinspired slash categories (Steedman and Baldridge, 2011), an approach similar to Zollmann and Venugopal (2006)’s syntax-augmented machine translation (SAMT). Callison-Burch did not formally define a synchronous grammar, nor discuss decoding, since his presentation did not include hierarchical rules. Cohn and Lapat"
D11-1108,J93-1004,0,0.146258,"from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contribution"
D11-1108,N04-1035,0,0.279686,"e nonterminal X instead of linguistic nonterminals. Three additional efforts incorporated linguistic syntax. Callison-Burch (2008) introduced syntactic constraints by labeling all phrases and paraphrases (even non-constituent phrases) with CCGinspired slash categories (Steedman and Baldridge, 2011), an approach similar to Zollmann and Venugopal (2006)’s syntax-augmented machine translation (SAMT). Callison-Burch did not formally define a synchronous grammar, nor discuss decoding, since his presentation did not include hierarchical rules. Cohn and Lapata (2008) used the GHKM extraction method (Galley et al., 2004), which is limited to constituent phrases and thus produces a reasonably small set of syntactic rules. Zhao et al. (2008b) added slots to bilingually extracted paraphrase patterns that were labeled with part-ofspeech tags, but not larger syntactic constituents. Before the shift to statistical natural language processing, paraphrasing was often treated as syntactic transformations or by parsing and then generating • Perform a thorough analysis of the types of paraphrases we obtain and discuss the paraphrastic transformations we are capable of capturing. • Describe how training paradigms for syn"
D11-1108,D10-1051,0,0.0136359,"cision estimate of B LEU with an additional “verbosity penalty” that is applied to compressions that fail to meet a given target compression rate ϕ. We rely on the B LEU brevity penalty to prevent the system from producing overly aggressive compressions. The scaling term λ determines how severely we penalize deviations from ϕ. In our experiments we use λ = 10. It is straightforward to find similar adaptations for other tasks. For text simplification, for instance, the penalty term can include a readability metric. For poetry generation we can analogously penalize outputs that break the meter (Greene et al., 2010). 6.3 Development Data In Section 4 we discussed phrasal probabilities. While these help quantify how good a paraphrase is in general, they do not make any statement on task-specific things such as the change in language complexity or text length. To make this information available to the decoder, we enhance our paraphrases with four compression-targeted features. We add the count features csrc and ctgt , indicating the number of words on either side of the rule as well as two difference features: cdcount = ctgt − csrc and the analogously computed difference in the average word To tune the par"
D11-1108,P07-1019,0,0.00898414,"inals, which enables us to promote more complex paraphrases that require structural reordering. Decoding With this, paraphrasing becomes an English-to-English translation problem which can be formulated similarly to Equation 5 as: max p(d, e2 |e1 )). Figure 3 shows an example derivation produced as a result of applying our paraphrase rules in the decoding process. Another advantage of using the decoder from statistical machine translation is that n-gram language models, which have been shown to be useful in natural language generation (Langkilde and Knight, 1998), are already well integrated (Huang and Chiang, 2007). 5 Analysis CD NNS JJ twelve cartoons insulting CD C → hα1 , α2 , ∼, ϕ ~ i, d∈D(e2 ,e1 ) DT+NNP DT NNP the prophet mohammad 12 of the cartoons that are offensive to the prophet mohammad we create a paraphrase rule: eˆ2 ≈ yield (arg NP JJ NNS DT NNP DT+NNP NP NP VP NP Paraphrase Rule Foreign Pivot Phrase Lexical paraphrase: JJ → offensive |insulting JJ -&gt; beleidigend |offensive JJ -&gt; beleidigend |insulting Reduced relative clause: NP → NP that VP |NP VP NP -&gt; NP die VP |NP VP NP -&gt; NP die VP |NP that VP Pred. adjective copula deletion: VP → are JJ to NP |JJ NP VP → sind JJ für NP |are JJ to NP"
D11-1108,N06-1058,0,0.0649688,"1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). 1168 Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic constraints: the NP 1 ’s NP 2 the NP 2 of the NP 1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A variety of different types of"
D11-1108,N03-1017,0,0.0243958,"ese include large monolingual texts (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), comparable corpora (Barzilay and Lee, 2003; Dolan et al., 2004), monolingual parallel corpora (Barzilay and McKeown, 2001; Pang et al., 2003), and bilingual parallel corpora (Bannard and Callison-Burch, 2005; Madnani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and"
D11-1108,W03-1601,0,0.0356656,"the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and"
D11-1108,W98-1426,0,0.0720656,"reorder , fires if the rule swaps the order of two nonterminals, which enables us to promote more complex paraphrases that require structural reordering. Decoding With this, paraphrasing becomes an English-to-English translation problem which can be formulated similarly to Equation 5 as: max p(d, e2 |e1 )). Figure 3 shows an example derivation produced as a result of applying our paraphrase rules in the decoding process. Another advantage of using the decoder from statistical machine translation is that n-gram language models, which have been shown to be useful in natural language generation (Langkilde and Knight, 1998), are already well integrated (Huang and Chiang, 2007). 5 Analysis CD NNS JJ twelve cartoons insulting CD C → hα1 , α2 , ∼, ϕ ~ i, d∈D(e2 ,e1 ) DT+NNP DT NNP the prophet mohammad 12 of the cartoons that are offensive to the prophet mohammad we create a paraphrase rule: eˆ2 ≈ yield (arg NP JJ NNS DT NNP DT+NNP NP NP VP NP Paraphrase Rule Foreign Pivot Phrase Lexical paraphrase: JJ → offensive |insulting JJ -&gt; beleidigend |offensive JJ -&gt; beleidigend |insulting Reduced relative clause: NP → NP that VP |NP VP NP -&gt; NP die VP |NP VP NP -&gt; NP die VP |NP that VP Pred. adjective copula deletion: VP →"
D11-1108,W09-0424,1,0.517892,"ction to produce rich syntactic paraphrases. Several subsequent efforts extended the bilingual pivoting technique, many of which introduced elements of more contemporary syntax-based approaches to statistical machine translation. Madnani et al. (2007) extended the technique to hierarchical phrase-based machine translation (Chiang, 2005), which is formally a synchronous context-free grammar (SCFG) and thus can be thought of as a paraphrase grammar. The paraphrase grammar can paraphrase (or “decode”) input sentences using an SCFG decoder, like the Hiero, Joshua or cdec MT systems (Chiang, 2007; Li et al., 2009; Dyer et al., 2010). Like Hiero, Madnani’s model uses just one nonterminal X instead of linguistic nonterminals. Three additional efforts incorporated linguistic syntax. Callison-Burch (2008) introduced syntactic constraints by labeling all phrases and paraphrases (even non-constituent phrases) with CCGinspired slash categories (Steedman and Baldridge, 2011), an approach similar to Zollmann and Venugopal (2006)’s syntax-augmented machine translation (SAMT). Callison-Burch did not formally define a synchronous grammar, nor discuss decoding, since his presentation did not include hierarchical r"
D11-1108,W10-1718,1,0.81311,"toolkit (Venugopal and Zollmann, 2009). The grammars we extract tend to be extremely large. To keep their size manageable, we only consider translation rules that have been seen more than 3 times and whose translation probability exceeds 10−4 for pivot recombination. Additionally, we only retain the top 25 most likely paraphrases of each phrase, ranked by a uniformly weighted combination of phrasal and lexical paraphrase probabilities. We tuned the model parameters to our P R E´ CIS objective function, implemented in the Z-MERT toolkit (Zaidan, 2009). For decoding we used the Joshua decoder (Li et al., 2010). The language model used in our paraphraser and the Clarke and Lapata (2008) baseline system is a Kneser-Ney discounted 5-gram model estimated on the Gigaword corpus using the SRILM toolkit (Stolcke, 2002). 6.6 Evaluation To assess the output quality of the resulting sentence compression system, we compare it to two state-ofthe-art sentence compression systems. Specifically, we compare against our implementation of Clarke and Lapata (2008)’s compression model which uses a series of constraints in an integer linear programming (ILP) solver, and Cohn and Lapata (2007)’s tree transducer toolkit"
D11-1108,P06-1077,0,0.0471752,"≥ 0 is assigned to each rule, reflecting the likelihood of the rule. Rule Extraction Phrase-based approaches to statistical machine translation (and their successors) extract pairs of (e, f ) phrases from automatically word-aligned parallel sentences. Och (2003b) described various heuristics for extracting phrase alignments from the Viterbi word-level alignments that are estimated using Brown et al. (1993) wordalignment models. These phrase extraction heuristics have been extended so that they extract synchronous grammar rules (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006). Most of these extraction methods require that one side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word alignment (in this case the leak is consistent with our f ). The nonterminal symbol that is the left-hand side of the SCFG rule is then determined by the syntactic constituent that dominates e (in this case NP)."
D11-1108,J10-3003,0,0.435482,"raphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic constraints: the NP 1 ’s NP 2 the NP 2 of the NP 1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A variety of different types of corpora (and semantic equivalence cues) have been used to automatically induce paraphrase collections for English (Madnani and Dorr, 2010). Perhaps the most natural type of corpus for this task is a monolingual parallel text, which allows sentential paraphrases to be extracted since the sentence pairs in such corpora are perfect paraphrases of each other (Barzilay and McKeown, 2001; Pang et al., 2003). While rich syntactic paraphrases have been learned from monolingual parallel corpora, they suffer from very limited data availability and thus have poor coverage. Other methods obtain paraphrases from raw monolingual text by relying on distributional similarity (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). While vast amou"
D11-1108,W07-0716,0,0.353926,"ing and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). 1168 Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic constraints: the NP 1 ’s NP 2 the NP 2 of the NP 1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A variety of different types of corpora (and semantic"
D11-1108,E06-1038,0,0.0138678,"allows changes to the tree topology. Cohn and Lapata argue that this is a natural fit for sentence compression, since deletions introduce structural mismatches. We trained the T3 software2 on the 936 hfull, compressedi sentence pairs that comprise our development set. This is equivalent in size to the training corpora that Cohn and Lapata (2007) used (their training corpora ranged from 2 www.dcs.shef.ac.uk/people/T.Cohn/t3/ 882–1020 sentence pairs), and has the advantage of being in-domain with respect to our test set. Both these systems reported results outperforming previous systems such as McDonald (2006). To showcase the value of the adaptations discussed above, we also compare variants of our paraphrase-based compression systems: using Hiero instead of syntax, using syntax with or without compression features, using an augmented grammar with optional deletion rules. We solicit human judgments of the compressions along two five-point scales: grammaticality and meaning. Judges are instructed to decide how much the meaning from a reference translation is retained in the compressed sentence, with a score of 5 indicating that all of the important information is present, and 1 being that the compr"
D11-1108,P79-1016,0,0.845325,"illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems. 1 Introduction Paraphrases are alternative ways of expressing the same information (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). 1168 Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing no"
D11-1108,P04-1083,0,0.0345897,"se of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008; Cohn and Lap"
D11-1108,C88-2088,0,0.562127,"nolingual parallel corpora (Barzilay and McKeown, 2001; Pang et al., 2003), and bilingual parallel corpora (Bannard and Callison-Burch, 2005; Madnani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 20"
D11-1108,C82-1038,0,0.678767,"al., 2004), monolingual parallel corpora (Barzilay and McKeown, 2001; Pang et al., 2003), and bilingual parallel corpora (Bannard and Callison-Burch, 2005; Madnani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase"
D11-1108,W11-1611,1,0.464043,"Missing"
D11-1108,P03-1021,0,0.472333,"g (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the"
D11-1108,W06-3112,0,0.0538655,"Missing"
D11-1108,N03-1024,0,0.109714,"ints: the NP 1 ’s NP 2 the NP 2 of the NP 1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A variety of different types of corpora (and semantic equivalence cues) have been used to automatically induce paraphrase collections for English (Madnani and Dorr, 2010). Perhaps the most natural type of corpus for this task is a monolingual parallel text, which allows sentential paraphrases to be extracted since the sentence pairs in such corpora are perfect paraphrases of each other (Barzilay and McKeown, 2001; Pang et al., 2003). While rich syntactic paraphrases have been learned from monolingual parallel corpora, they suffer from very limited data availability and thus have poor coverage. Other methods obtain paraphrases from raw monolingual text by relying on distributional similarity (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). While vast amounts of data are readily available for these approaches, the distributional similarity signal they use is noisier than the sentence-level correspondency in parallel corpora and additionally suffers from problems such as mistaking cousin expressions or antonyms (such"
D11-1108,P02-1040,0,0.0951856,"ohn and Lapata, 2008) or whether they include arbitrary phrases, including non-constituent phrases (Zollmann and Venugopal, 2006; Callison-Burch, 2008). We adopt the extraction for all phrases, including non-constituents, since it allows us to cover a much greater set of phrases, both in translation and paraphrasing. Feature Functions Rather than assigning a single weight w, we define a set of feature functions ϕ ~ = {ϕ1 ...ϕN } that are combined in a log-linear model: w=− N X λi log ϕi . (4) i=1 The weights ~λ of these feature functions are set to maximize some objective function like B LEU (Papineni et al., 2002) using a procedure called minimum error rate training (MERT), owing to Och (2003a). MERT iteratively adjusts the weights until the decoder produces output that best matches reference translations in a development set, according to the objective function. We will examine appropriate objective functions for text-to-text generation tasks in Section 6.2. Typical features used in statistical machine translation include phrase translation probabilities (calculated using maximum likelihood estimation over all phrase pairs enumerable in the parallel corpus), word-for-word lexical translation probabili"
D11-1108,W04-3219,0,0.110365,"≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be ext"
D11-1108,P05-1034,0,0.0285203,"al machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008; Cohn and Lapata, 2009). 3 SCFGs i"
D11-1108,P02-1006,0,0.0251685,"raphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems. 1 Introduction Paraphrases are alternative ways of expressing the same information (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). 1168 Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic"
D11-1108,P07-1059,0,0.0351686,"f sentence compression and achieve results competitive with state-of-the-art compression systems. 1 Introduction Paraphrases are alternative ways of expressing the same information (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). 1168 Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic constraints: the NP 1 ’"
D11-1108,C96-2155,0,0.0278403,"a (Barzilay and McKeown, 2001; Pang et al., 2003), and bilingual parallel corpora (Bannard and Callison-Burch, 2005; Madnani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et"
D11-1108,W90-0102,0,0.41483,"nani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here synta"
D11-1108,W04-3206,0,0.0482543,"nd discuss the paraphrastic transformations we are capable of capturing. • Describe how training paradigms for syntactic/sentential paraphrase models should be tailored to different text-to-text generation tasks. • Demonstrate our framework’s suitability for a variety of text-to-text generation tasks by obtaining state-of-the-art results on the example task of sentence compression. 2 Related Work Madnani and Dorr (2010) survey a variety of datadriven paraphrasing techniques, categorizing them based on the type of data that they use. These include large monolingual texts (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), comparable corpora (Barzilay and Lee, 2003; Dolan et al., 2004), monolingual parallel corpora (Barzilay and McKeown, 2001; Pang et al., 2003), and bilingual parallel corpora (Bannard and Callison-Burch, 2005; Madnani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1"
D11-1108,J97-3002,0,0.246935,"zlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2"
D11-1108,P01-1067,0,0.0230038,"al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 200"
D11-1108,C02-1163,0,0.0325514,"McKeown, 2001; Pang et al., 2003), and bilingual parallel corpora (Bannard and Callison-Burch, 2005; Madnani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-l"
D11-1108,P08-1116,0,0.54245,"expressions or antonyms (such as {boy, girl } or {rise, fall }) for paraphrases. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1168–1179, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Abundantly available bilingual parallel corpora have been shown to address both these issues, obtaining paraphrases via a pivoting step over foreign language phrases (Bannard and Callison-Burch, 2005). The coverage of paraphrase lexica extracted from bitexts has been shown to outperform that obtained from other sources (Zhao et al., 2008a). While there have been efforts pursuing the extraction of more powerful paraphrases (Madnani et al., 2007; Callison-Burch, 2008; Cohn and Lapata, 2008; Zhao et al., 2008b), it is not yet clear to what extent sentential paraphrases can be induced from bitexts. In this paper we: phrase table, English paraphrases are obtained by pivoting through foreign language phrases. Since many paraphrases can be extracted for a phrase, Bannard and Callison-Burch rank them using a paraphrase probability defined in terms of the translation model probabilities p(f |e) and p(e|f ): X p(e2 |e1 ) = p(e2 , f |e1"
D11-1108,P08-1089,0,0.71213,"expressions or antonyms (such as {boy, girl } or {rise, fall }) for paraphrases. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1168–1179, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Abundantly available bilingual parallel corpora have been shown to address both these issues, obtaining paraphrases via a pivoting step over foreign language phrases (Bannard and Callison-Burch, 2005). The coverage of paraphrase lexica extracted from bitexts has been shown to outperform that obtained from other sources (Zhao et al., 2008a). While there have been efforts pursuing the extraction of more powerful paraphrases (Madnani et al., 2007; Callison-Burch, 2008; Cohn and Lapata, 2008; Zhao et al., 2008b), it is not yet clear to what extent sentential paraphrases can be induced from bitexts. In this paper we: phrase table, English paraphrases are obtained by pivoting through foreign language phrases. Since many paraphrases can be extracted for a phrase, Bannard and Callison-Burch rank them using a paraphrase probability defined in terms of the translation model probabilities p(f |e) and p(e|f ): X p(e2 |e1 ) = p(e2 , f |e1"
D11-1108,P09-1094,0,0.0403077,"ear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008; Cohn and Lapata, 2009). 3 SCFGs in Translation The model we use in our paraphrasing approach is a syntactically informed synchronous context-free grammar (SCFG). The SCFG formalism (Aho and Ullman, 1972) was repopularized for statistical machine translation by Chiang (2005). Formally, a probabilistic SCFG G is defined by specifying G = hN , TS , TT , R, Si, where N is a set of nonterminal symbols, TS and TT are the source and target lang"
D11-1108,N06-1057,0,0.0338613,"mation (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). 1168 Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic constraints: the NP 1 ’s NP 2 the NP 2 of the NP 1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A va"
D11-1108,W06-3119,0,0.275665,"nd thus can be thought of as a paraphrase grammar. The paraphrase grammar can paraphrase (or “decode”) input sentences using an SCFG decoder, like the Hiero, Joshua or cdec MT systems (Chiang, 2007; Li et al., 2009; Dyer et al., 2010). Like Hiero, Madnani’s model uses just one nonterminal X instead of linguistic nonterminals. Three additional efforts incorporated linguistic syntax. Callison-Burch (2008) introduced syntactic constraints by labeling all phrases and paraphrases (even non-constituent phrases) with CCGinspired slash categories (Steedman and Baldridge, 2011), an approach similar to Zollmann and Venugopal (2006)’s syntax-augmented machine translation (SAMT). Callison-Burch did not formally define a synchronous grammar, nor discuss decoding, since his presentation did not include hierarchical rules. Cohn and Lapata (2008) used the GHKM extraction method (Galley et al., 2004), which is limited to constituent phrases and thus produces a reasonably small set of syntactic rules. Zhao et al. (2008b) added slots to bilingually extracted paraphrase patterns that were labeled with part-ofspeech tags, but not larger syntactic constituents. Before the shift to statistical natural language processing, paraphrasi"
D11-1108,P97-1070,0,\N,Missing
D11-1108,C88-1016,0,\N,Missing
D11-1108,W10-0701,1,\N,Missing
D11-1108,P99-1039,0,\N,Missing
D11-1108,D08-1076,0,\N,Missing
D12-1005,P08-1077,0,0.0131831,"oaches is a more recent development; we provide here examples of representative work, beyond those described in previous sections. Levenberg and Osborne (2009) gave a streaming variant of the earlier perfect hashing language model of Talbot and Brants (2008), which operated in batch-mode. Using a similar decomposition to that here, Van Durme and Lall (2010) showed that Locality Sensitive Hash (LSH) signatures (Indyk and Motwani, 1998; Charikar, 2002) built using count-based feature vectors can be maintained online, as compared to their earlier uses in the community (Ravichandran et al., 2005; Bhagat and Ravichandran, 2008). Finally, Goyal et al. (2009) applied the frequent items8 algorithm of Manku and Motwani (2002) to language modeling. For further background in predicting author attributes such as gender, see (Garera and Yarowsky, 2009) for an overview of previous work and (nonstreaming) methodology. 7 Conclusions and Future Work We have taken the predominately batch-oriented process of analyzing communication data and shown it to be fertile territory for research in large-scale streaming algorithms. Using the example task of automatic gender detection, on both spoken transcripts and microblogs, we showed th"
D12-1005,P05-1054,0,0.0177375,"its decision with each new communication, becoming more stable in its prediction as evidence is acquired. In either setting, the state of the classifier is sufficiently captured by the pair (st , zt ), under the restrictions on f .3 2.1 ● ● ● ● ● 0.9 ● ● ● Validation ● 0.8 3 Note that some non-linear kernels can be maintained online in a similar fashion. For instance, a polynomial kernel of degree p decomposes as: (f (Cn ) · w)p = ( zsnn )p . 50 Accuracy ● As an example of a model decomposed into a stream, we revist the task of gender classification based on speech transcripts, as explored by Boulis and Ostendorf (2005) and later Garera and Yarowsky (2009). In the original problem definition, one would collect all transcribed utterances from a given speaker in a corpus such as Fisher (Cieri et al., 2004) or Switchboard (Godfrey et al., 1992), known as a side of the conversation. Then by collapsing these utterances into a single document, one could classify it as to whether it was generated by a male or female. Here we define the task as: starting from scratch, report the classifier probability of the speaker being male, as each utterance is presented. Intuitively we would expect that as more utterances are o"
D12-1005,D11-1120,0,0.385214,"tion, one would collect all transcribed utterances from a given speaker in a corpus such as Fisher (Cieri et al., 2004) or Switchboard (Godfrey et al., 1992), known as a side of the conversation. Then by collapsing these utterances into a single document, one could classify it as to whether it was generated by a male or female. Here we define the task as: starting from scratch, report the classifier probability of the speaker being male, as each utterance is presented. Intuitively we would expect that as more utterances are observed, the better our classification accuracy. Researchers such as Burger et al. (2011) have considered this point, but by comparing the classification accuracy based on the volume of batch data available per author (in that case, tweets): the more prolific the author had been, the better able they were to correctly classify their gender. We confirm here this can be reframed: as a speaker (author) continues to emit a stream of communication, a dynamic model tends to improve its online prediction. Our collection based on Switchboard consisted of 520 unique speakers (240 female, 280 male), with a total of roughly 400k utterances. Similar to Boulis and Ostendorf, we extracted unigr"
D12-1005,cieri-etal-2004-fisher,0,0.00979322,"zt ), under the restrictions on f .3 2.1 ● ● ● ● ● 0.9 ● ● ● Validation ● 0.8 3 Note that some non-linear kernels can be maintained online in a similar fashion. For instance, a polynomial kernel of degree p decomposes as: (f (Cn ) · w)p = ( zsnn )p . 50 Accuracy ● As an example of a model decomposed into a stream, we revist the task of gender classification based on speech transcripts, as explored by Boulis and Ostendorf (2005) and later Garera and Yarowsky (2009). In the original problem definition, one would collect all transcribed utterances from a given speaker in a corpus such as Fisher (Cieri et al., 2004) or Switchboard (Godfrey et al., 1992), known as a side of the conversation. Then by collapsing these utterances into a single document, one could classify it as to whether it was generated by a male or female. Here we define the task as: starting from scratch, report the classifier probability of the speaker being male, as each utterance is presented. Intuitively we would expect that as more utterances are observed, the better our classification accuracy. Researchers such as Burger et al. (2011) have considered this point, but by comparing the classification accuracy based on the volume of ba"
D12-1005,P09-1080,0,0.779307,"on, becoming more stable in its prediction as evidence is acquired. In either setting, the state of the classifier is sufficiently captured by the pair (st , zt ), under the restrictions on f .3 2.1 ● ● ● ● ● 0.9 ● ● ● Validation ● 0.8 3 Note that some non-linear kernels can be maintained online in a similar fashion. For instance, a polynomial kernel of degree p decomposes as: (f (Cn ) · w)p = ( zsnn )p . 50 Accuracy ● As an example of a model decomposed into a stream, we revist the task of gender classification based on speech transcripts, as explored by Boulis and Ostendorf (2005) and later Garera and Yarowsky (2009). In the original problem definition, one would collect all transcribed utterances from a given speaker in a corpus such as Fisher (Cieri et al., 2004) or Switchboard (Godfrey et al., 1992), known as a side of the conversation. Then by collapsing these utterances into a single document, one could classify it as to whether it was generated by a male or female. Here we define the task as: starting from scratch, report the classifier probability of the speaker being male, as each utterance is presented. Intuitively we would expect that as more utterances are observed, the better our classificatio"
D12-1005,N09-1058,0,0.039932,"Missing"
D12-1005,D09-1079,0,0.0230657,"cheers, ai n’t obrigada (thank you [1F]), hubby, husband, cute, my husband, ?, cansada (tired [1F]), hair, dress, soooo, lovely, etsy, boyfriend, jonas, loved, book, sooo, girl, s´e (I), lindo (cute), shopping, amiga (friend [2F]), yummy, ppl, cupcakes 56 mary of the streaming algorithms community is beyond the scope of this work: interested readers are directed to Muthukrishnan (2005) as a starting point. Within computational linguistics interest in streaming approaches is a more recent development; we provide here examples of representative work, beyond those described in previous sections. Levenberg and Osborne (2009) gave a streaming variant of the earlier perfect hashing language model of Talbot and Brants (2008), which operated in batch-mode. Using a similar decomposition to that here, Van Durme and Lall (2010) showed that Locality Sensitive Hash (LSH) signatures (Indyk and Motwani, 1998; Charikar, 2002) built using count-based feature vectors can be maintained online, as compared to their earlier uses in the community (Ravichandran et al., 2005; Bhagat and Ravichandran, 2008). Finally, Goyal et al. (2009) applied the frequent items8 algorithm of Manku and Motwani (2002) to language modeling. For furthe"
D12-1005,P11-1032,0,0.00912054,"e forums, can support a variety of data mining tasks. Inferring the underlying properties of those that engage with these platforms, the discourse participants, has become an active topic of research: predicting individual attributes such as age, gender, and political preferences (Rao et al., 2010), or relationships between communicants, such as organizational dominance (Diehl et al., 2007). This research can benefit areas such as: (A) commercial applications, e.g., improved models for advertising placement, or detecting fraudulent or otherwise unhelpful product reviews (Jindal and Liu, 2008; Ott et al., 2011); and (B) in enhanced models of civic discourse, e.g., inexpensive, large-scale, passive polling of popular opinion (O’Connor et al., 2010). Classification with streaming data has usually been taken in the computational linguistics community to mean individual decisions made on items that are presented over time. For example: assigning a label to each newly posted product review as to whether it contains positive or negative sentiment, or whether the latest tweet signals a novel topic that should be tagged for tracking (Petrovic et al., 2010). Here we consider a distinct form of stream-based c"
D12-1005,N10-1021,0,0.0187311,"herwise unhelpful product reviews (Jindal and Liu, 2008; Ott et al., 2011); and (B) in enhanced models of civic discourse, e.g., inexpensive, large-scale, passive polling of popular opinion (O’Connor et al., 2010). Classification with streaming data has usually been taken in the computational linguistics community to mean individual decisions made on items that are presented over time. For example: assigning a label to each newly posted product review as to whether it contains positive or negative sentiment, or whether the latest tweet signals a novel topic that should be tagged for tracking (Petrovic et al., 2010). Here we consider a distinct form of stream-based classification: we wish to assign, then dynamically update, labels on discourse participants based on their associated streaming communications. For instance, rather than classifying individual reviews as to their sentiment polarity, we might wish to classify the underlying author as to whether they are genuine or paid-advertising, and then update that decision as they continue to post new reviews. As the scale of social media continues to grow, we desire that our model be aggressively space efficient, which precludes a naive solution of stori"
D12-1005,P05-1077,0,0.0216428,"interest in streaming approaches is a more recent development; we provide here examples of representative work, beyond those described in previous sections. Levenberg and Osborne (2009) gave a streaming variant of the earlier perfect hashing language model of Talbot and Brants (2008), which operated in batch-mode. Using a similar decomposition to that here, Van Durme and Lall (2010) showed that Locality Sensitive Hash (LSH) signatures (Indyk and Motwani, 1998; Charikar, 2002) built using count-based feature vectors can be maintained online, as compared to their earlier uses in the community (Ravichandran et al., 2005; Bhagat and Ravichandran, 2008). Finally, Goyal et al. (2009) applied the frequent items8 algorithm of Manku and Motwani (2002) to language modeling. For further background in predicting author attributes such as gender, see (Garera and Yarowsky, 2009) for an overview of previous work and (nonstreaming) methodology. 7 Conclusions and Future Work We have taken the predominately batch-oriented process of analyzing communication data and shown it to be fertile territory for research in large-scale streaming algorithms. Using the example task of automatic gender detection, on both spoken transcri"
D12-1005,P08-1058,0,0.0116887,"ir, dress, soooo, lovely, etsy, boyfriend, jonas, loved, book, sooo, girl, s´e (I), lindo (cute), shopping, amiga (friend [2F]), yummy, ppl, cupcakes 56 mary of the streaming algorithms community is beyond the scope of this work: interested readers are directed to Muthukrishnan (2005) as a starting point. Within computational linguistics interest in streaming approaches is a more recent development; we provide here examples of representative work, beyond those described in previous sections. Levenberg and Osborne (2009) gave a streaming variant of the earlier perfect hashing language model of Talbot and Brants (2008), which operated in batch-mode. Using a similar decomposition to that here, Van Durme and Lall (2010) showed that Locality Sensitive Hash (LSH) signatures (Indyk and Motwani, 1998; Charikar, 2002) built using count-based feature vectors can be maintained online, as compared to their earlier uses in the community (Ravichandran et al., 2005; Bhagat and Ravichandran, 2008). Finally, Goyal et al. (2009) applied the frequent items8 algorithm of Manku and Motwani (2002) to language modeling. For further background in predicting author attributes such as gender, see (Garera and Yarowsky, 2009) for an"
D12-1005,P07-1065,0,0.0189165,"ly the sequences have different averages: 23 6= 25 , which we cannot detect based on the original counting algorithm. Finally, we restate the constraint: for the sequence to averaged, one must know m∗ ahead of time. Reservoir.Size.Bits 4 8 12 Approximate Average 50 0 −50 −40 −20 0 True Average 20 40 Figure 5: Results on averaging randomly generated sequences, with m∗ = 100, g = 100, and using an 8 bit Morris-style counter of base 2. Larger reservoir sizes lead to better approximation, at higher cost in bits. to provide a streaming extension to the Bloom-filter based count-storage mechanism of Talbot and Osborne (2007a) and Talbot and Osborne (2007b). See (Flajolet, 1985) for a detailed analysis of Morrisstyle counting. 3.3 Experiment We show through experimentation on synthetic data that this approach gives reasonable levels of accuracy at space efficient sizes of the length and sum parameter. Random sequences of 1,000 values were generated by: (1) fix a value for m∗ ; (2) draw a polarity bias term µ uniformly from the range [0,1]; then (3) for each value, x: (a) σ was positive with probability µ; (b) m was drawn from [0, m∗ ]. Figure 5 shows results for varying reservoir sizes (using 4, 8 or 12 bits) whe"
D12-1005,D07-1049,0,0.0214909,"ly the sequences have different averages: 23 6= 25 , which we cannot detect based on the original counting algorithm. Finally, we restate the constraint: for the sequence to averaged, one must know m∗ ahead of time. Reservoir.Size.Bits 4 8 12 Approximate Average 50 0 −50 −40 −20 0 True Average 20 40 Figure 5: Results on averaging randomly generated sequences, with m∗ = 100, g = 100, and using an 8 bit Morris-style counter of base 2. Larger reservoir sizes lead to better approximation, at higher cost in bits. to provide a streaming extension to the Bloom-filter based count-storage mechanism of Talbot and Osborne (2007a) and Talbot and Osborne (2007b). See (Flajolet, 1985) for a detailed analysis of Morrisstyle counting. 3.3 Experiment We show through experimentation on synthetic data that this approach gives reasonable levels of accuracy at space efficient sizes of the length and sum parameter. Random sequences of 1,000 values were generated by: (1) fix a value for m∗ ; (2) draw a polarity bias term µ uniformly from the range [0,1]; then (3) for each value, x: (a) σ was positive with probability µ; (b) m was drawn from [0, m∗ ]. Figure 5 shows results for varying reservoir sizes (using 4, 8 or 12 bits) whe"
D12-1005,P10-2043,1,0.693089,"Missing"
D12-1005,P11-2004,1,0.903297,"Missing"
D13-1056,2009.eamt-1.23,0,0.092581,"Missing"
D13-1056,D12-1032,0,0.0163741,"edness Feature is a single scaled number in [0, 1] from the best performing system (Han et al., 2013) of the *Sem 2013 Semantic Textual Similarity (STS) task. We included this feature mainly to deal with cases where “related” words cannot be well measured by either paraphrases or distributional similarities. For instance, in one alignment dataset annotators aligned married with wife. Adding a few other words as comparison, the Han et al. (2013) system gives the following similarity scores: married/wife: 0.85 married/husband: 0.84 married/child: 0.10 married/stone: 0.01 Name Phylogeny Feature (Andrews et al., 2012) is a similarity feature with a string transducer to model how one name evolves to another. Examples below show how similar is the name Bill associated with other names in log probability: Bill/Bill: -0.8 Bill/Billy: -5.2 Bill/William: -13.6 Bill/Mary: -18.6 Finally, one decision we made during feature design was not to use any parsing-based features, with a permissive assumption that the input might not be well-formed English, or even not complete sentences (such as fragmented snippets from web search). The “deepest” linguistic processing stays at the level of tagging and chunking, making the"
D13-1056,P11-1131,0,0.0343557,"Missing"
D13-1056,P06-1009,0,0.241342,"translation) rather than monolingual. Moreover, most work has considered token-based approaches over phrase-based.1 Here we seek to address this imbalance by proposing better phrase-based models for monolingual word alignment. ∗ Performed while faculty at Johns Hopkins University. In this paper we use the term token-based alignment for one-to-one alignment and phrase-based for non one-to-one alignment, and word alignment in general for both. 1 The token aligner jacana-align (Yao et al., 2013a) has achieved state-of-the-art result on the task of monolingual alignment, based on previous work of Blunsom and Cohn (2006). It employs a Conditional Random Field (Lafferty et al., 2001) to align tokens from the source sentence to tokens in the target sentence, by treating source tokens as “observation” and target tokens as “hidden states”. However, it is not designed to handle phrase-based alignment, largely due to the Markov nature of the underlying model: a state can only span one token each time, making it unable to align multiple consecutive tokens (i.e. a phrase). We extend this model by introducing semiMarkov states for phrase-based alignment: a state can instead span multiple consecutive time steps, thus a"
D13-1056,J93-2003,0,0.0472056,"e. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can be easily added, significantly improving exact alignment match rate for whole sentence pairs. Besides the common application of textual entailment and question answering, monolingual alignment has also been applied in the field of text generation (Barzilay and Lee, 2003; Pang et al., 2003). Word alignment has been more explored in machine translation. The IBM models (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2 http://code.google.com/p/jacana/ 591 bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-p"
D13-1056,W07-1427,0,0.102732,"Missing"
D13-1056,J08-4005,1,0.880184,"Missing"
D13-1056,P09-1053,0,0.2096,"Missing"
D13-1056,W11-2107,0,0.0285654,"Missing"
D13-1056,C04-1051,0,0.429672,"and phrase-only alignment (subscript p). it is another topic, we simply show in this section using just alignment scores in binary prediction problems. Specifically, we pick the tasks of recognizing textual entailment (RTE), paraphrase identification (PP), and question answering sentence ranking (QA) described in Heilman and Smith (2010): RTE: predicting whether a hypothesis can be inferred from the premise, with training data from RTE-1/2 and RTE-3 dev, and test from RTE-3 test. PP: predicting whether two sentences are paraphrases, with training and test data from the MSR Paraphrase Corpus (Dolan et al., 2004). QA: predicting whether a sentence contains the answer to the question, with training data from TREC-8 to TREC-12 and test data from TREC-13. For each aligned pair, we can compute a normalized decoding score. Following MacCartney et al. (2008), we select a threshold score and predict true if the decoding score is above this threshold. For the tasks of RTE and PP, we tuned this threshold w.r.t the maximal accuracy on the training set, then reported performance on the test set. For the task of QA, since the evaluation methods in Mean Average Precision and Mean Reciprocal Rank only need a ranked"
D13-1056,N13-1092,1,0.387664,"Missing"
D13-1056,N10-1112,0,0.0196559,"ue labels. It is only computed during training in the denominator because in the numerator cost(ay , ay ) = 0. Hamming cost is used in practice without learning the weights (i.e., uniform weights). The more inconsistence there is between ay and ˆ a, the more penalized is the decoding sequence ˆ a through the cost function. 3.2 1 6 7..14 Z(s, t) X Shops ...-... P exp( i,k λk fk (ai−1 , ai , s, t)) This assumes a first-order Conditional Random Field (Lafferty et al., 2001). Since the word alignment task is evaluated over F1 , instead of directly optimizing it, we choose a much easier objective (Gimpel and Smith, 2010) and add a cost function to the normalizing function Z(s, t) in the denominator: Z(s, t) = 0 Phrase-based Model The token-based model supports 1 : 1 alignment. We first extend it in the direction of ls : 1, where a target state spans ls words on the source side (ls source words align to 1 target word). Then we extend it in the direction of 1 : lt , where lt is the target phrase length a source word aligns to (1 source word aligns to lt target words). The final combined 592 15 shops Shops are closed up for now until March Figure 1: A semi-Markov phrase-based model example and the desired Viterb"
D13-1056,S13-1005,0,0.0116787,"s is the left hand side syntactic non-terminal symbol. We did not use the syntactic part (e.g., the N P of N N S ↔ the N N S of N P ) of PPDB as we did not make the assumption that the input sentence pairs were well-formed (and newswire-like) English, or 594 even of a language with a parser available. Also, for phrasal alignments, we ruled out those paraphrases spanning multiple syntactic structures, or of different syntactic structures (indicated as [X] in PPDB), for instance, and crazy ↔ , mad. Semantic Relatedness Feature is a single scaled number in [0, 1] from the best performing system (Han et al., 2013) of the *Sem 2013 Semantic Textual Similarity (STS) task. We included this feature mainly to deal with cases where “related” words cannot be well measured by either paraphrases or distributional similarities. For instance, in one alignment dataset annotators aligned married with wife. Adding a few other words as comparison, the Han et al. (2013) system gives the following similarity scores: married/wife: 0.85 married/husband: 0.84 married/child: 0.10 married/stone: 0.01 Name Phylogeny Feature (Andrews et al., 2012) is a similarity feature with a string transducer to model how one name evolves"
D13-1056,N10-1145,0,0.119622,"on Empirical Methods in Natural Language Processing, pages 590–600, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics CRF model on the task of phrase-based alignment, and then show a basic application in the NLP tasks of recognizing textual entailment, paraphrase identification, and question answering sentence ranking. The final phrase-based aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron lea"
D13-1056,N06-1014,0,0.0177325,"08) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens. Andr´es-Ferrer and Juan (2009) extended the HMM-based method to Hidden Semi-Markov Models (HSMM) (Ostendorf et al., 1996), allowing phrasal alignments on the source side. Finally, Bansal et al. (2011) unified the HSMM models with the alignment by agreement framework (Liang et al., 2006), achieving phrasal alignment that agreed in both directions. Despite successful usage of generative semiMarkov models in bilingual alignment, this has not been followed with models in discriminative monolingual alignment. Essentially monolingual alignment would benefit more from discriminative models with various feature extractions (just like those defined in MANLI) than generative models without any predefined feature (just like how they were used in bilingual alignment). To combine the strengths of both semi-Markov models and discriminative training, we propose to use the semi-Markov Condi"
D13-1056,C08-1066,0,0.0315246,"odels (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2 http://code.google.com/p/jacana/ 591 bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens. Andr´es-Ferrer and Juan (2009) extended the HMM-based method to Hidden Semi-Markov Models (HSMM) (Ostendorf et al., 1996), allowing phrasal alignments on the source side. Finally, Bansal et al. (2011) unified the HSMM models with the alignment by agreement framework (Lia"
D13-1056,D08-1084,0,0.228637,"d aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can b"
D13-1056,W02-1018,0,0.0337279,"sentence pairs. Besides the common application of textual entailment and question answering, monolingual alignment has also been applied in the field of text generation (Barzilay and Lee, 2003; Pang et al., 2003). Word alignment has been more explored in machine translation. The IBM models (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2 http://code.google.com/p/jacana/ 591 bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens."
D13-1056,P09-2073,0,0.0234686,"ttle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics CRF model on the task of phrase-based alignment, and then show a basic application in the NLP tasks of recognizing textual entailment, paraphrase identification, and question answering sentence ranking. The final phrase-based aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment o"
D13-1056,J03-1002,0,0.0465458,"Missing"
D13-1056,N03-1024,0,0.0311421,"nt of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can be easily added, significantly improving exact alignment match rate for whole sentence pairs. Besides the common application of textual entailment and question answering, monolingual alignment has also been applied in the field of text generation (Barzilay and Lee, 2003; Pang et al., 2003). Word alignment has been more explored in machine translation. The IBM models (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2 http://code.google.com/p/jacana/ 591 bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences"
D13-1056,D12-1016,0,0.0332604,"rase-based alignment, and then show a basic application in the NLP tasks of recognizing textual entailment, paraphrase identification, and question answering sentence ranking. The final phrase-based aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadan"
D13-1056,P11-2044,0,0.385701,"work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can be easily added, significantly improving exact a"
D13-1056,C12-2120,0,0.278948,"Missing"
D13-1056,C96-2141,0,0.435527,"essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2 http://code.google.com/p/jacana/ 591 bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens. Andr´es-Ferrer and Juan (2009) extended the HMM-based method to Hidden Semi-Markov Models (HSMM) (Ostendorf et al., 1996), allowing phrasal alignments on the source side. Finally, Bansal et al. (2011) unified the HSMM models with the alignment by agreement framework (Liang et al., 2006), achieving phrasal alignment that agreed in both directions"
D13-1056,U06-1019,0,0.0376029,"Missing"
D13-1056,C10-1131,0,0.102753,"Missing"
D13-1056,D07-1003,0,0.114867,"Missing"
D13-1056,P13-2123,1,0.662726,"Missing"
D13-1056,N13-1106,1,0.881841,"Missing"
D13-1056,N03-1003,0,\N,Missing
D13-1056,U04-1000,0,\N,Missing
D13-1171,W11-0705,0,0.263308,"g expressed is a large part of the challenge, but identifying other attributes, such as the target of the sentiment, is also crucial if the ultimate goal is to pinpoint and extract opinions. Consider the examples below, all of which contain a positive sentiment: (1) So happy that Kentucky lost to Tennessee! (2) Kentucky versus Kansas I can hardly wait... (3) Kentucky is the best alley-oop throwing team since Sherman Douglas’ Syracuse squads!! (1) An experiencer (2) An attitude (3) A target (optionally) Research in sentiment analysis often focuses on (2), predicting overall sentiment polarity (Agarwal et al., 2011; Bora, 2012). Recent work has begun to combine (2) with (3), examining how to automatically predict the sentiment polarity expressed towards a target entity (Jiang et al., 2011; Chen et al., 2012) for a fixed set of targets. This topic-dependent sentiment classification requires that the target entity be 1643 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1643–1654, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics @[user] le dijo erralo muy por lo bajo jaja un grande juancito grandes amigos mios @[use"
D13-1171,baccianella-etal-2010-sentiwordnet,0,0.0249089,"e prompt is shown in Figure 3. Each htweet, NEi pair was shown to three Turkers, and those with majority consensus on sentiment polarity were extracted. Tweets without sentiment 5 www.mturk.com/mturk 1646 ORGANIZATION PERSON Named Entity Figure 4: Targeted sentiment annotated for Spanish. Majority Minority Sentiment Lexicons We use two sentiment lexicon sources in each language. For English, we use the MPQA lexicon (Wilson et al., 2005), which identifies 12,296 manually and semi-automatically produced subjective terms along with their polarity. For the second lexicon, we use SentiWordNet 3.0 (Baccianella et al., 2010), which assigns positive and negative polarity scores to WordNet synsets. We use the majority polarity of all words with a subjectivity score above 0.5. For Spanish, the first lexicon is obtained from Volkova et al. (2013), who automatically translated strongly subjective terms from the MPQA lexicon (Wilson et al., 2005) into Spanish. The resulting Spanish lexicon contains about 65K words. The second lexicon is available from Perez-Rosas et al. (2012). This contains approximately 1000 sentiment-bearing words collected leveraging manual resources and 2000 collected leveraging automatic resource"
D13-1171,C10-2005,0,0.0167898,"tags (Brown clusters) and a method that automatically syllabifies a word based on the orthography of the language. All tools and code used for this research are released with this paper.2 2 Related Work As the scale of social media has grown, using sources such as Twitter to mine public sentiment has become increasingly promising. Commercial systems include Sentiment1403 (products and brands) and tweetfeel4 (suggests searching for popular movies, celebrities and companies). The majority of academic research has focused on supervised classification of message sentiment irrespective of target (Barbosa and Feng, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2 www.m-mitchell.com/code www.sentiment140.com 4 www.tweetfeel.com 3 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et"
D13-1171,D11-1052,0,0.0128713,"de www.sentiment140.com 4 www.tweetfeel.com 3 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2011; Wang et al., 2011; Li et al., 2012; Chen et al., 2012), have mostly focused on topic-dependent analysis. In these approaches, messages are collected on a fixed set of topics/targets, such as products or sports teams, and sentiment is learned for the given set. In contrast, we aim to predict sentiment in tweets for any named person or organization. We refer to this task as open domain targeted sentiment analysis. Within topic-dependent sentiment analysis, seve"
D13-1171,J92-4003,0,0.0558456,"dow of three words in either direction. Words seen only once are treated as out-of-vocabulary. Surface features and linguistic features are concatenated in groups of two and three to create further features. All algorithms and code that we have developed for feature extraction are available online.9 Because we aim to develop models that do not heavily rely on language-specific resources, we are interested in exploring unsupervised and lightly supervised methods for learning relevant features. Rather than use part-of-speech tags, we therefore use Brown cluster labels as unsupervised word tags (Brown et al., 1992; Koo et al., 2008). Brown clustering is a distributional similarity method that merges pairs of word clusters in the training data10 to create the smallest decrease in corpus likelihood, using a bigram language model on the clusters. For our task, we cut clusters at length 3 and length 5, and these serve as rough part-of-speech tags without the need to train additional models. For example, the word hello is tagged as belonging to cluster 011 (length 3) and 01111 (length 5). During development, we found that being able to syllabify the word (break the word into syllables) was a positive indica"
D13-1171,W10-0701,0,0.010857,", 7,105 Spanish tweets contained a total of 9,870 volitional entities and 2,350 English tweets contained a total of 3,577 volitional entities. Annotation To collect sentiment labels, we use crowdsourcing through Amazon’s Mechanical Turk.5 Annotators (“Turkers”) were shown six tweets at a time, each with a single highlighted named entity. Turkers were instructed to (1) select the sentiment being expressed towards the entity (positive, negative, or no sentiment); and (2) rate their level of confidence in their selection. Following best practices on collecting language data with Mechanical Turk (Callison-Burch and Dredze, 2010), two controls were placed among each set of six tweets to screen out unreliable judgments. An example prompt is shown in Figure 3. Each htweet, NEi pair was shown to three Turkers, and those with majority consensus on sentiment polarity were extracted. Tweets without sentiment 5 www.mturk.com/mturk 1646 ORGANIZATION PERSON Named Entity Figure 4: Targeted sentiment annotated for Spanish. Majority Minority Sentiment Lexicons We use two sentiment lexicon sources in each language. For English, we use the MPQA lexicon (Wilson et al., 2005), which identifies 12,296 manually and semi-automatically p"
D13-1171,W06-1651,0,0.00966013,"ng the span of the entity itself: Similar to how named entity recognition (NER) learns labels along the span of each word in an entity name, sentiment may be expressed along the entity as well. A small example is shown in Figure 1. We focus on people and organizations (volitional named entities), which are the primary targets of sentiment in our microblog data (see Table 1). Both NER and opinion expression extraction have achieved impressive results using conditional random fields (CRFs) (Lafferty et al., 2001) to define the conditional probability of entity categories (McCallum and Li, 2003; Choi et al., 2006; Yang and Cardie, 2013). We develop such models to jointly predict the NE and the sentiment expressed towards it using minimum risk training (Stoyanov and Eisner, 2012). We learn our models on informal Spanish and English language taken from the social network Twitter,1 where the language variety makes NLP particularly challenging (see Figure 2). Our ultimate goal is to develop models that will be useful for low resource languages, where a sentiment lexicon may be known or bootstrapped, but more sophisticated linguistic tools may not be readily available. We therefore do not rely on an extern"
D13-1171,C10-2028,0,0.0105885,"rd based on the orthography of the language. All tools and code used for this research are released with this paper.2 2 Related Work As the scale of social media has grown, using sources such as Twitter to mine public sentiment has become increasingly promising. Commercial systems include Sentiment1403 (products and brands) and tweetfeel4 (suggests searching for popular movies, celebrities and companies). The majority of academic research has focused on supervised classification of message sentiment irrespective of target (Barbosa and Feng, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2 www.m-mitchell.com/code www.sentiment140.com 4 www.tweetfeel.com 3 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learnin"
D13-1171,P10-1074,0,0.0161969,"Missing"
D13-1171,D10-1101,0,0.203319,"us to use surrounding context to determine sentiment polarity without identifying explicit opinion expressions or relying on a parser to help link expression to target. Most work in targeted sentiment outside the microblogging domain has been in relation to product review mining (e.g., Yi et al. (2003), Hu and Liu (2004), Popescu and Etzioni (2005), Qiu et al. (2011)). Rather than identify named entities (NEs), this work seeks to identify products and their features mentioned in reviews, and classify these for sentiment. Recent work by Qui et al. jointly learns targets and opinion words, and Jakob and Gurevych (2010) use CRFs to extract the targets of opinions, but do not attempt to classify the sentiment toward these targets. To the best of our knowledge, this is the first work to approach targeted sentiment in a low resource setting and to jointly predict NEs and targeted sentiment. 3 Data Twitter Collection We use the Spanish/English Twitter dataset of Etter et al. (2013) to train and test our models. Approximately 30,000 Spanish tweets and 10,000 English were labeled for named entities in BIO encoding: The start of an NE is labeled B{NE} and the rest of the NE is labeled I-{NE}. The NE PERSON ORGANIZA"
D13-1171,P11-1016,0,0.181267,"Missing"
D13-1171,N06-1026,0,0.0195787,"ask as open domain targeted sentiment analysis. Within topic-dependent sentiment analysis, several approaches have explored applying CRFs or HMMs to extract sentiment and target words from text (Jin and Ho, 2009; Li et al., 2010a). In these approaches, opinion expressions are extracted, and polarity is annotated across the opinion expression. However, as noted by many researchers in sentiment, opinion orientation towards a specific target is often not equal to the orientation of a neighboring opinion expression; and opinion expressions in one context may not be opinion expressions in another (Kim and Hovy, 2006), making open domain approaches particularly challenging. The above work by Jiang et al. (2011) is most similar to our own. They do not use joint learning, but they do incorporate a number of parse-based features designed to capture relationships between sentiment terms and topic references. In our work these relationships are captured by the CRF model, and we compare against their approach in Section 6. Recent work by Yang and Cardie (2013) is similar in spirit to our own, where the identification of opinion holders, opinion targets, and opinion expressions is modeled as a sequence tagging pr"
D13-1171,P08-1068,0,0.0180751,"n either direction. Words seen only once are treated as out-of-vocabulary. Surface features and linguistic features are concatenated in groups of two and three to create further features. All algorithms and code that we have developed for feature extraction are available online.9 Because we aim to develop models that do not heavily rely on language-specific resources, we are interested in exploring unsupervised and lightly supervised methods for learning relevant features. Rather than use part-of-speech tags, we therefore use Brown cluster labels as unsupervised word tags (Brown et al., 1992; Koo et al., 2008). Brown clustering is a distributional similarity method that merges pairs of word clusters in the training data10 to create the smallest decrease in corpus likelihood, using a bigram language model on the clusters. For our task, we cut clusters at length 3 and length 5, and these serve as rough part-of-speech tags without the need to train additional models. For example, the word hello is tagged as belonging to cluster 011 (length 3) and 01111 (length 5). During development, we found that being able to syllabify the word (break the word into syllables) was a positive indicator of people names"
D13-1171,C10-1074,0,0.0273905,"ouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2 www.m-mitchell.com/code www.sentiment140.com 4 www.tweetfeel.com 3 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2011; Wang et al., 2011; Li et al., 2012; Chen et al., 2012), have mostly focused on topic-dependent analysis. In these approaches, messages are collected on a fixed set of topics"
D13-1171,Y12-1013,0,0.0147017,"(Barbosa and Feng, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2 www.m-mitchell.com/code www.sentiment140.com 4 www.tweetfeel.com 3 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2011; Wang et al., 2011; Li et al., 2012; Chen et al., 2012), have mostly focused on"
D13-1171,W03-0430,0,0.0136585,"n a graphical model along the span of the entity itself: Similar to how named entity recognition (NER) learns labels along the span of each word in an entity name, sentiment may be expressed along the entity as well. A small example is shown in Figure 1. We focus on people and organizations (volitional named entities), which are the primary targets of sentiment in our microblog data (see Table 1). Both NER and opinion expression extraction have achieved impressive results using conditional random fields (CRFs) (Lafferty et al., 2001) to define the conditional probability of entity categories (McCallum and Li, 2003; Choi et al., 2006; Yang and Cardie, 2013). We develop such models to jointly predict the NE and the sentiment expressed towards it using minimum risk training (Stoyanov and Eisner, 2012). We learn our models on informal Spanish and English language taken from the social network Twitter,1 where the language variety makes NLP particularly challenging (see Figure 2). Our ultimate goal is to develop models that will be useful for low resource languages, where a sentiment lexicon may be known or bootstrapped, but more sophisticated linguistic tools may not be readily available. We therefore do no"
D13-1171,pak-paroubek-2010-twitter,0,0.0456523,"nd a method that automatically syllabifies a word based on the orthography of the language. All tools and code used for this research are released with this paper.2 2 Related Work As the scale of social media has grown, using sources such as Twitter to mine public sentiment has become increasingly promising. Commercial systems include Sentiment1403 (products and brands) and tweetfeel4 (suggests searching for popular movies, celebrities and companies). The majority of academic research has focused on supervised classification of message sentiment irrespective of target (Barbosa and Feng, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2 www.m-mitchell.com/code www.sentiment140.com 4 www.tweetfeel.com 3 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted"
D13-1171,perez-rosas-etal-2012-learning,0,0.0210049,"ies 12,296 manually and semi-automatically produced subjective terms along with their polarity. For the second lexicon, we use SentiWordNet 3.0 (Baccianella et al., 2010), which assigns positive and negative polarity scores to WordNet synsets. We use the majority polarity of all words with a subjectivity score above 0.5. For Spanish, the first lexicon is obtained from Volkova et al. (2013), who automatically translated strongly subjective terms from the MPQA lexicon (Wilson et al., 2005) into Spanish. The resulting Spanish lexicon contains about 65K words. The second lexicon is available from Perez-Rosas et al. (2012). This contains approximately 1000 sentiment-bearing words collected leveraging manual resources and 2000 collected leveraging automatic resources. POS NEUTRAL NEG POS NEUTRAL NEG 757 707 129 1249 2151 726 130 473 452 Table 2: Number of targeted sentiment instances where at least two of the three annotators (Majority) agreed. Common disagreements with a third annotator (Minority) were over whether no sentiment or positive sentiment was expressed, and whether no sentiment or negative sentment was expressed. consensus on all NEs were removed. In Spanish, this yielded 6,658 unique htweet, NEi pai"
D13-1171,P06-1055,0,0.0236922,"tomatically learn syllable structure.11 We learn this in an unsupervised way, using the most frequent (seen more than 1,000 times) word-initial non-vowel sequences from the Brown cluster data as allowable syllable onset consonants. Similarly, the most frequent word-final non-vowel sequences are learned as possible syllable codas. For each word, we then attempt to segment syllables using the learned onsets and codas around each vowel. If a word cannot be syllabified, it is often an initialism (e.g., CND, lsat). We follow the approach from the out-ofvocabulary assignment in the Berkeley parser (Petrov et al., 2006) to encode common surface patterns such as capitalization and lexical patterns such as verb endings as a single feature for words we have seen once or less. We also use the Jerboa toolkit (Van Durme, 2012) to extract further language-independent features from the data, such as features for emoticons and binning for repeated characters (like !!!). In addition, we include features for whether the word is three or four letters, which is often used for acronyms and initialisms in several languages (including Spanish and English); whether the word is neighbored by a punctuation mark; word identity;"
D13-1171,H05-1043,0,0.0280161,"ations to connect an opinion target to an opinion expression. In contrast, we model the expression of sentiment polarity across the sentiment target itself, extracting both the sentiment target and the sentiment expressed towards it within the same span of words. This allows us to use surrounding context to determine sentiment polarity without identifying explicit opinion expressions or relying on a parser to help link expression to target. Most work in targeted sentiment outside the microblogging domain has been in relation to product review mining (e.g., Yi et al. (2003), Hu and Liu (2004), Popescu and Etzioni (2005), Qiu et al. (2011)). Rather than identify named entities (NEs), this work seeks to identify products and their features mentioned in reviews, and classify these for sentiment. Recent work by Qui et al. jointly learns targets and opinion words, and Jakob and Gurevych (2010) use CRFs to extract the targets of opinions, but do not attempt to classify the sentiment toward these targets. To the best of our knowledge, this is the first work to approach targeted sentiment in a low resource setting and to jointly predict NEs and targeted sentiment. 3 Data Twitter Collection We use the Spanish/English"
D13-1171,J11-1002,0,0.144222,"target to an opinion expression. In contrast, we model the expression of sentiment polarity across the sentiment target itself, extracting both the sentiment target and the sentiment expressed towards it within the same span of words. This allows us to use surrounding context to determine sentiment polarity without identifying explicit opinion expressions or relying on a parser to help link expression to target. Most work in targeted sentiment outside the microblogging domain has been in relation to product review mining (e.g., Yi et al. (2003), Hu and Liu (2004), Popescu and Etzioni (2005), Qiu et al. (2011)). Rather than identify named entities (NEs), this work seeks to identify products and their features mentioned in reviews, and classify these for sentiment. Recent work by Qui et al. jointly learns targets and opinion words, and Jakob and Gurevych (2010) use CRFs to extract the targets of opinions, but do not attempt to classify the sentiment toward these targets. To the best of our knowledge, this is the first work to approach targeted sentiment in a low resource setting and to jointly predict NEs and targeted sentiment. 3 Data Twitter Collection We use the Spanish/English Twitter dataset of"
D13-1171,W11-2207,0,0.0189438,"rch has focused on supervised classification of message sentiment irrespective of target (Barbosa and Feng, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2 www.m-mitchell.com/code www.sentiment140.com 4 www.tweetfeel.com 3 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2"
D13-1171,N12-1013,0,0.0572789,"pressed along the entity as well. A small example is shown in Figure 1. We focus on people and organizations (volitional named entities), which are the primary targets of sentiment in our microblog data (see Table 1). Both NER and opinion expression extraction have achieved impressive results using conditional random fields (CRFs) (Lafferty et al., 2001) to define the conditional probability of entity categories (McCallum and Li, 2003; Choi et al., 2006; Yang and Cardie, 2013). We develop such models to jointly predict the NE and the sentiment expressed towards it using minimum risk training (Stoyanov and Eisner, 2012). We learn our models on informal Spanish and English language taken from the social network Twitter,1 where the language variety makes NLP particularly challenging (see Figure 2). Our ultimate goal is to develop models that will be useful for low resource languages, where a sentiment lexicon may be known or bootstrapped, but more sophisticated linguistic tools may not be readily available. We therefore do not rely on an external part-of-speech tagger or parser, which are often used for features in fine-grained sentiment analysis; such tools are not available in many languages, and if they are"
D13-1171,P13-2090,1,0.618506,"amed Entity Figure 4: Targeted sentiment annotated for Spanish. Majority Minority Sentiment Lexicons We use two sentiment lexicon sources in each language. For English, we use the MPQA lexicon (Wilson et al., 2005), which identifies 12,296 manually and semi-automatically produced subjective terms along with their polarity. For the second lexicon, we use SentiWordNet 3.0 (Baccianella et al., 2010), which assigns positive and negative polarity scores to WordNet synsets. We use the majority polarity of all words with a subjectivity score above 0.5. For Spanish, the first lexicon is obtained from Volkova et al. (2013), who automatically translated strongly subjective terms from the MPQA lexicon (Wilson et al., 2005) into Spanish. The resulting Spanish lexicon contains about 65K words. The second lexicon is available from Perez-Rosas et al. (2012). This contains approximately 1000 sentiment-bearing words collected leveraging manual resources and 2000 collected leveraging automatic resources. POS NEUTRAL NEG POS NEUTRAL NEG 757 707 129 1249 2151 726 130 473 452 Table 2: Number of targeted sentiment instances where at least two of the three annotators (Majority) agreed. Common disagreements with a third annot"
D13-1171,H05-1044,1,0.0830461,"collecting language data with Mechanical Turk (Callison-Burch and Dredze, 2010), two controls were placed among each set of six tweets to screen out unreliable judgments. An example prompt is shown in Figure 3. Each htweet, NEi pair was shown to three Turkers, and those with majority consensus on sentiment polarity were extracted. Tweets without sentiment 5 www.mturk.com/mturk 1646 ORGANIZATION PERSON Named Entity Figure 4: Targeted sentiment annotated for Spanish. Majority Minority Sentiment Lexicons We use two sentiment lexicon sources in each language. For English, we use the MPQA lexicon (Wilson et al., 2005), which identifies 12,296 manually and semi-automatically produced subjective terms along with their polarity. For the second lexicon, we use SentiWordNet 3.0 (Baccianella et al., 2010), which assigns positive and negative polarity scores to WordNet synsets. We use the majority polarity of all words with a subjectivity score above 0.5. For Spanish, the first lexicon is obtained from Volkova et al. (2013), who automatically translated strongly subjective terms from the MPQA lexicon (Wilson et al., 2005) into Spanish. The resulting Spanish lexicon contains about 65K words. The second lexicon is"
D13-1171,J09-3003,1,0.344722,"f sentiment for the named entities annotated by Turkers is shown in Figure 4. Neutral (no targeted sentiment) dominates, followed by positive sentiment for both organizations and people. As shown in Table 2, common disagreements were over whether or not there was targeted positive sentiment, and whether or not there was targeted negative sentiment. This is in line with previous research showing that distinguishing positive sentiment from no sentiment (and distinguishing negative sentiment from no sentiment) is often more challenging than distinguishing between positive and negative sentiment (Wilson et al., 2009). Indeed, we see that it was more common for annotators to disagree than to agree on targeted sentiment, particularly for negative targeted sentiment, where more instances had NEUTRAL/NEGATIVE disagreement than NEGATIVE three-way agreement. Figure 3: Example Tweet shown to Turkers. Variable Possible values Sentiment (s) NOT- TARG , SENT- TARG (P IPE & J OINT models) Named Entity (l) O , B - VOLITIONAL , I - VOLITIONAL (P IPE & J OINT models) Combined Sent/NE (y) O, B + NOT- TARG, I + NOT- TARG (C OLL models) B + SENT- TARG , I + SENT- TARG Table 3: Possible values for random variables, targete"
D13-1171,P13-1161,0,0.176281,"entity itself: Similar to how named entity recognition (NER) learns labels along the span of each word in an entity name, sentiment may be expressed along the entity as well. A small example is shown in Figure 1. We focus on people and organizations (volitional named entities), which are the primary targets of sentiment in our microblog data (see Table 1). Both NER and opinion expression extraction have achieved impressive results using conditional random fields (CRFs) (Lafferty et al., 2001) to define the conditional probability of entity categories (McCallum and Li, 2003; Choi et al., 2006; Yang and Cardie, 2013). We develop such models to jointly predict the NE and the sentiment expressed towards it using minimum risk training (Stoyanov and Eisner, 2012). We learn our models on informal Spanish and English language taken from the social network Twitter,1 where the language variety makes NLP particularly challenging (see Figure 2). Our ultimate goal is to develop models that will be useful for low resource languages, where a sentiment lexicon may be known or bootstrapped, but more sophisticated linguistic tools may not be readily available. We therefore do not rely on an external part-of-speech tagger"
D13-1171,H05-2017,0,\N,Missing
D15-1195,P08-1090,0,0.917472,"ibutional semantics, such as pointwise mutual information, we argue that the narrative cloze can be productively reframed as a language modeling task. By training a discriminative language model for this task, we attain improvements of up to 27 percent over prior methods on standard narrative cloze metrics. 1 2 Introduction Although the concept of scripts in artificial intelligence dates back to the 1970s (Schank and Abelson, 1977), interest in this topic has renewed with recent efforts to automatically induce scripts from text on a large scale. One particularly influential work in this area, Chambers and Jurafsky (2008), treats the problem of script induction as one of learning narrative chains, which they accomplish using simple textual co-occurrence statistics. For the novel task of learning narrative chains, they introduce a new evaluation metric, the narrative cloze test, which involves predicting a missing event from a chain of events drawn from text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and extend Chambers and Jurafsky (2008)’s methods for learning narrative chains, each using the narrative cloze to evaluate th"
D15-1195,D13-1185,0,0.408778,"ce a new evaluation metric, the narrative cloze test, which involves predicting a missing event from a chain of events drawn from text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and extend Chambers and Jurafsky (2008)’s methods for learning narrative chains, each using the narrative cloze to evaluate their work. 1 In this paper, we take the position that the narrative cloze test, which has been treated predom1 A number of related works on script induction use alternative task formulations and evaluations. (Chambers, 2013; Cheung et al., 2013; Cheung and Penn, 2013; Frermann et al., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Regneri et al., 2010) Task Definition Following the definitions of Chambers and Jurafsky (2008), a narrative chain is “a partially ordered set of narrative events that share a common actor,” where a narrative event is “a tuple of an event (most simply a verb) and its participants, represented as typed dependencies.” (De Marneffe et al., 2006) Formally, e := (v, d), where e is a narrative event, v is a verb lemma, and d is the syntactic dependency (nsubj or dobj) between v and the p"
D15-1195,P13-1039,0,0.0148236,"tive cloze test, which involves predicting a missing event from a chain of events drawn from text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and extend Chambers and Jurafsky (2008)’s methods for learning narrative chains, each using the narrative cloze to evaluate their work. 1 In this paper, we take the position that the narrative cloze test, which has been treated predom1 A number of related works on script induction use alternative task formulations and evaluations. (Chambers, 2013; Cheung et al., 2013; Cheung and Penn, 2013; Frermann et al., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Regneri et al., 2010) Task Definition Following the definitions of Chambers and Jurafsky (2008), a narrative chain is “a partially ordered set of narrative events that share a common actor,” where a narrative event is “a tuple of an event (most simply a verb) and its participants, represented as typed dependencies.” (De Marneffe et al., 2006) Formally, e := (v, d), where e is a narrative event, v is a verb lemma, and d is the syntactic dependency (nsubj or dobj) between v and the protagonist. As an example, consider the foll"
D15-1195,N13-1104,0,0.260423,"ion metric, the narrative cloze test, which involves predicting a missing event from a chain of events drawn from text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and extend Chambers and Jurafsky (2008)’s methods for learning narrative chains, each using the narrative cloze to evaluate their work. 1 In this paper, we take the position that the narrative cloze test, which has been treated predom1 A number of related works on script induction use alternative task formulations and evaluations. (Chambers, 2013; Cheung et al., 2013; Cheung and Penn, 2013; Frermann et al., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Regneri et al., 2010) Task Definition Following the definitions of Chambers and Jurafsky (2008), a narrative chain is “a partially ordered set of narrative events that share a common actor,” where a narrative event is “a tuple of an event (most simply a verb) and its participants, represented as typed dependencies.” (De Marneffe et al., 2006) Formally, e := (v, d), where e is a narrative event, v is a verb lemma, and d is the syntactic dependency (nsubj or dobj) between v and the protagonist. As an exa"
D15-1195,J90-1003,0,0.468095,"ly for overall perplexity on the development set.) All models trend toward improved performance on longer chains. Because the unigram model also improves with chain length, it appears that longer chains contain more frequent events and are thus easier to predict. However, LBL performance is also likely improving on longer chains because of additional contextual information, as is evident from LBL 4’s slight relative gains over LBL 2 on longer chains. 5 Conclusion Pointwise mutual information and other related count-based techniques have been used widely to identify semantically similar words (Church and Hanks, 1990; Lin and Pantel, 2001; Tur1684 ney and Pantel, 2010), so it is natural that these techniques have also been applied to the task of script induction. Qualitatively, PMI often identifies intuitively compelling matches; among the top 15 events to share a high PMI with (eat, nsubj) under the Unordered PMI model, for example, we find events such as (overeat, nsubj), (taste, nsubj), (smell, nsubj), (cook, nsubj), and (serve, dobj). When evaluated by the narrative cloze test, however, these count-based methods are overshadowed by the performance of a general-purpose discriminative language model. Ou"
D15-1195,de-marneffe-etal-2006-generating,0,0.018641,"Missing"
D15-1195,E14-1006,0,0.149579,"Missing"
D15-1195,E12-1034,0,0.514627,"Missing"
D15-1195,W14-1606,0,0.519514,"in of events drawn from text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and extend Chambers and Jurafsky (2008)’s methods for learning narrative chains, each using the narrative cloze to evaluate their work. 1 In this paper, we take the position that the narrative cloze test, which has been treated predom1 A number of related works on script induction use alternative task formulations and evaluations. (Chambers, 2013; Cheung et al., 2013; Cheung and Penn, 2013; Frermann et al., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Regneri et al., 2010) Task Definition Following the definitions of Chambers and Jurafsky (2008), a narrative chain is “a partially ordered set of narrative events that share a common actor,” where a narrative event is “a tuple of an event (most simply a verb) and its participants, represented as typed dependencies.” (De Marneffe et al., 2006) Formally, e := (v, d), where e is a narrative event, v is a verb lemma, and d is the syntactic dependency (nsubj or dobj) between v and the protagonist. As an example, consider the following narrative: John studied for the exam and aced it. His teacher"
D15-1195,N04-1041,0,0.00975097,") C(e1 , ∗)C(∗, e2 ) (1) Here, C(e1 , e2 ) is the number of times e1 and e2 occur in the same narrative event sequence, i.e., the number of times they “had a coreferring entity filling the values of [their] dependencies,” and the ordering of e1 and e2 is not considered. In our implementation, individual counts are defined as follows: C(e, ∗) := X C(e, e0 ) (2) e0 ∈V This model selects the best candidate event in a given cloze test according to the following score: eˆ = arg max e∈V L X pmi(e, ei ) (3) i=1 We tune this model with an option to apply a modified version of discounting for PMI from Pantel and Ravichandran (2004). Ordered PMI (OP) This model is a slight variation on Unordered PMI introduced by Jans et al. (2012). The only distinction is that C(e1 , e2 ) is treated as an asymmetric count, sensitive to the order in which e1 and e2 occur within a chain. Bigram Probability (BG) Another variant introduced by Jans et al. (2012), the “bigram probability” model uses conditional probabilities rather than PMI to compute scores. In a cloze test, this model selects the following event: eˆ = arg max e∈V k Y i=1 p(e|ei ) L Y p(ei |e) (4) i=k+1 1 ,e2 ) where p(e2 |e1 ) = C(e C(e1 ,∗) and C(e1 , e2 ) is asymmetric. W"
D15-1195,E14-1024,0,0.776425,"in this topic has renewed with recent efforts to automatically induce scripts from text on a large scale. One particularly influential work in this area, Chambers and Jurafsky (2008), treats the problem of script induction as one of learning narrative chains, which they accomplish using simple textual co-occurrence statistics. For the novel task of learning narrative chains, they introduce a new evaluation metric, the narrative cloze test, which involves predicting a missing event from a chain of events drawn from text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and extend Chambers and Jurafsky (2008)’s methods for learning narrative chains, each using the narrative cloze to evaluate their work. 1 In this paper, we take the position that the narrative cloze test, which has been treated predom1 A number of related works on script induction use alternative task formulations and evaluations. (Chambers, 2013; Cheung et al., 2013; Cheung and Penn, 2013; Frermann et al., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Regneri et al., 2010) Task Definition Following the definitions of Chambers and Jurafsky (2008), a narrati"
D15-1195,P10-1100,0,0.599611,"m text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and extend Chambers and Jurafsky (2008)’s methods for learning narrative chains, each using the narrative cloze to evaluate their work. 1 In this paper, we take the position that the narrative cloze test, which has been treated predom1 A number of related works on script induction use alternative task formulations and evaluations. (Chambers, 2013; Cheung et al., 2013; Cheung and Penn, 2013; Frermann et al., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Regneri et al., 2010) Task Definition Following the definitions of Chambers and Jurafsky (2008), a narrative chain is “a partially ordered set of narrative events that share a common actor,” where a narrative event is “a tuple of an event (most simply a verb) and its participants, represented as typed dependencies.” (De Marneffe et al., 2006) Formally, e := (v, d), where e is a narrative event, v is a verb lemma, and d is the syntactic dependency (nsubj or dobj) between v and the protagonist. As an example, consider the following narrative: John studied for the exam and aced it. His teacher congratulated him. With"
D15-1195,S15-1024,1,0.748051,"Missing"
D15-1195,P09-1068,0,\N,Missing
D15-1285,cieri-etal-2004-fisher,0,0.0470278,"t multiple types of topic models as goals in their own rights, directly comparing SAGE and LDA, Morchid et al. use LDA as a pre-processing step to Gaussian i-vectors. Second, we use triphone state cluster soft counts instead of ASR word counts, hence our representation of speech data is significantly lower-resource. Third, we also evaluate performance on text data, and where Morchid et al. limit their vocabulary (from ASR) to 166 task-specific words, we use all 26,606 words present in our training data. 3 Input Representations Our data is drawn from Part 1 of the Fisher English speech corpus (Cieri et al., 2004c), which contains audio recordings (Cieri et al., 2004a) and manual transcriptions (Cieri et al., 2004b) of telephone conversations. Specifically, we use the topic ID training and evaluation test subsets defined in prior work (Hazen et al., 2007). In each conversation in these subsets of the data, two study participants are prompted to speak on one of a predefined set of forty topics. There are 1374 training conversations and 686 test conversations. We represent each conversation by two documents, one for each side (speaker), resulting in a training set of 2748 documents and a test set of 137"
D15-1285,N12-1096,1,0.866061,"Missing"
D15-1285,Q14-1041,0,0.024071,"ide only one view of text and speech (respectively), and other input representations may yield different conclusions. The particular LSA approach we used for text, based on tf-idf weighting, is not as appropriate for our speech data, which is dense. Future work could evaluate other implementations of LSA or use a higher-level view of speech, such as triphone state cluster ngrams, that more naturally exhibits sparsity and lends to tf-idf weighting. In particular, weighting by a likelihood ratio test statistic and applying a log transform has generated better performance in several other tasks (Lapesa and Evert, 2014). Future work could also test our conclusions on higher-resource views of speech, such as ASR word counts, or lower-resource views such as melfrequency cepstral coefficients (MFCCs). We have provided a brief cross-community evaluation of learned representations on multinomial text and speech data. Some prior work has evaluated related learned representations on text data alone, surveying parameters and tasks at greater breadth (Lapesa and Evert, 2014; Levy et al., 2015). A similarly comprehensive evaluation spanning the text and speech research communities would demand great effort but provide"
D15-1285,Q15-1016,0,0.0366725,"a likelihood ratio test statistic and applying a log transform has generated better performance in several other tasks (Lapesa and Evert, 2014). Future work could also test our conclusions on higher-resource views of speech, such as ASR word counts, or lower-resource views such as melfrequency cepstral coefficients (MFCCs). We have provided a brief cross-community evaluation of learned representations on multinomial text and speech data. Some prior work has evaluated related learned representations on text data alone, surveying parameters and tasks at greater breadth (Lapesa and Evert, 2014; Levy et al., 2015). A similarly comprehensive evaluation spanning the text and speech research communities would demand great effort but provide a large and versatile resource. In complement, a detailed, case-by-case analysis of errors made by the models in our study could illuminate future modeling efforts by exposing exactly how and why each model errs or excels in each task. 7 Conclusion Topic ID and topic discovery are competing objectives in our setting: we found that the bestperforming representations per task were the same whether considering text- or speech-based communications. By evaluating learned re"
D15-1285,D11-1024,0,0.0427457,"ed representations of size K = 600. Error bars denote plus and minus one standard deviation according to the CV empirical distribution. Figure 7: V-measure on the Fisher English text and speech data, respectively, for raw and tf-idf representations and lower-dimensional learned representations at selected dimensions. As in topic ID, we see underparametrized mi-vectors perform well on the text data. chises, welfare, and professional basketball, while the mi-vector topics are less succinctly characterizable and more polluted by uninformative words. 6 We complement this qualitative analysis with Mimno et al. (2011)’s intrinsic coherence measure, a standard quantitative method. This scoring function, which correlates well with human quality judgments, averages estimates of the conditional log-likelihoods of each topic’s M highestweighted words across all topics. Using K = 600 models on text as before and picking M = 20, we compute mi-vector coherence as −453.34 and SAGE coherence (averaged over three runs) as −407.52, indicating that SAGE is more amenable to topic discovery and human interaction. mi-vector you’ve, florida, each, a-, bit hours, never, couldn’t, check, communicate pregnant, water, lifestyl"
D15-1285,D14-1051,0,0.0877326,"Missing"
D15-1285,Q15-1004,0,0.0115607,"rmalized) natural parameters of that distribution are represented in an affine subspace: probability vectors. A general formulation is then   φ(d,n) = Q H z (d) (3)   φ(d) = softmax m + Hθ (d) The hyperparameters α and η dictate the informativeness of the priors over H k and θ (d) : often (empirically optimized) symmetric hyperparameters are employed, resulting in a form of Laplace smoothing during topic estimation. In the current work, we follow this strategy, noting that there have been concerted efforts to encode domain or expert knowledge via the hyperparameters (Gormley et al., 2012; Paul and Dredze, 2015). (2) θ (d) ∼ N (0, I). We call this multinomial version of the i-vector model the mi-vector model. The latent variable θ (d) is the multinomial i-vector, or mi-vector. H is an unconstrained linear transformation. The bias term m is computed as the log of the l1 normalized background word count vector. The Gaussian prior on the mi-vector θ (d) is effectively an l2 regularizer; mi-vectors are neither nonnegative nor sparse in general. Unlike many Bayesian topic models, word occurrences in the mi-vector model are i.i.d. draws from a document-level multinomial φ(d) ; as in LSA, each latent compon"
D15-1285,D07-1043,0,0.0609104,"lower-dimensional learned representations of size K = 10. Error bars denote plus and minus one standard deviation according to the CV empirical distribution. We see underparametrized mi-vectors excel at compressing the topic label information for text, particularly in the limitedsupervision settings. LDA view text text text text text text speech speech speech speech speech Topic Discovery To quantitatively assess representations’ potential for topic discovery we compute their V-measure against the gold-standard labels. V-measure is an unsupervised measure of similarity between two partitions (Rosenberg and Hirschberg, 2007) and is equivalent to the mutual information normalized by the sum of the entropy (Becker, 2011). For all representations, we compute V-measure between a partition induced by that representation and the gold-standard topic labels on the test set. A partition is induced on a representation by assigning each document d to the cluster indexed by the coordinate of θ (d) with highest value (the argmax). Results of this analysis are displayed in Figure 7. (Selected values are listed in Table 2.) On the text data, SAGE dominates the lower-dimensional representations, LSA is next best overall, and LDA"
D16-1107,W14-2700,0,0.0518927,"Missing"
D16-1107,P14-2030,1,0.898178,"Missing"
D16-1107,W12-2108,0,0.0305403,"Missing"
D16-1107,P15-1073,0,0.0150675,"going NLP research, including dedicated workshops(NAA, 2015; ACL, 2014). Previous work has considered macroscopic properties of the entire Twitter network (Gabielkov et al., 2014), and pondered whether it is an “information” or “social” network (Myers et al., 2014). Studies have focused on determining user attributes such as gender (Li et al., 2015), political allegiance (Volkova et al., 2014), brand affinity (Pennacchiotti and Popescu, 2011a), sentiment analysis (West et al., 2014), and more abstract roles (Beller et al., 2014). Such demographic information is known to help downstream tasks (Hovy, 2015). Research involving social media communication networks has typically focused on homophily, the tendency of users to connect to others with similar properties (Barber´a, 2014). A number of papers have employed features drawn from both the content and structure of network entities in pursuit of latent user attributes (Pennacchiotti and Popescu, 2011b; Campbell et al., 2014; Suwan et al., 2015). 3 Definitions We refer to the entities that produce and consume communications as Actors, and the communications (packets of language data) as Messages. Each message occurs in a particular Language, and"
D16-1107,W14-5317,0,0.0312717,"Missing"
D16-1107,P12-3005,0,0.0369064,"Missing"
D16-1107,P14-1018,1,0.847442,"Missing"
D16-1107,Q14-1024,0,0.0137468,"6. 2016 Association for Computational Linguistics 2 Previous Work 4 Twitter and other social media platforms are a major area of ongoing NLP research, including dedicated workshops(NAA, 2015; ACL, 2014). Previous work has considered macroscopic properties of the entire Twitter network (Gabielkov et al., 2014), and pondered whether it is an “information” or “social” network (Myers et al., 2014). Studies have focused on determining user attributes such as gender (Li et al., 2015), political allegiance (Volkova et al., 2014), brand affinity (Pennacchiotti and Popescu, 2011a), sentiment analysis (West et al., 2014), and more abstract roles (Beller et al., 2014). Such demographic information is known to help downstream tasks (Hovy, 2015). Research involving social media communication networks has typically focused on homophily, the tendency of users to connect to others with similar properties (Barber´a, 2014). A number of papers have employed features drawn from both the content and structure of network entities in pursuit of latent user attributes (Pennacchiotti and Popescu, 2011b; Campbell et al., 2014; Suwan et al., 2015). 3 Definitions We refer to the entities that produce and consume communications"
D16-1177,de-marneffe-etal-2014-universal,0,0.0211358,"Missing"
D16-1177,W12-0702,0,0.0239036,"Missing"
D16-1177,N09-1057,0,0.027303,"s and (ii) general components, such as CAUSATION , that are used across the lexicon. In the domain of thematic roles, Dowty (1991) argues that notions such as AGENT should be decomposed into simpler, more primitive properties such as volitional participation in an event. Pustejovsky (1991) decomposes word meanings into qualia structures that again incorporate more primitive properties of events and individuals. In spite of this wealth of theory, existing annotation protocols rarely take the idea into account, operating at the level that the above approaches decompose with very few exceptions (Greene and Resnik, 2009; Hartshorne et al., 2013; Reisinger et al., 2015). Decomp’s premise is that a decompositional approach to large-scale annotation has benefits for both the annotation process and downstream uses of annotated data (cf. He et al. 2015; Bos et al. 2017 for recent nondecompositional approaches). To capture these benefits, Decomp incorporates semantic decomposition directly into its protocols by mapping from decompositional theories, such as Dowty’s, into straightforward questions on binary properties that are easily answered. This method of constructing annotation protocols gives rise to simplicit"
D16-1177,D13-1149,0,0.36012,"ents, such as CAUSATION , that are used across the lexicon. In the domain of thematic roles, Dowty (1991) argues that notions such as AGENT should be decomposed into simpler, more primitive properties such as volitional participation in an event. Pustejovsky (1991) decomposes word meanings into qualia structures that again incorporate more primitive properties of events and individuals. In spite of this wealth of theory, existing annotation protocols rarely take the idea into account, operating at the level that the above approaches decompose with very few exceptions (Greene and Resnik, 2009; Hartshorne et al., 2013; Reisinger et al., 2015). Decomp’s premise is that a decompositional approach to large-scale annotation has benefits for both the annotation process and downstream uses of annotated data (cf. He et al. 2015; Bos et al. 2017 for recent nondecompositional approaches). To capture these benefits, Decomp incorporates semantic decomposition directly into its protocols by mapping from decompositional theories, such as Dowty’s, into straightforward questions on binary properties that are easily answered. This method of constructing annotation protocols gives rise to simplicity in the protocol, since"
D16-1177,D15-1076,0,0.0215047,"Missing"
D16-1177,W10-0730,0,0.0491918,"thods for extending SPR2’s reach beyond this subset, resulting in SPR2.1. 4.1 SPR1 protocol In the SPR1 protocol, each core argument of a verb is annotated for the likelihood that particular properties hold of that argument’s referent as a participant in the event denoted by the verb. Property questions The properties selected for this purpose, given in Table 2, are based on those invoked by Dowty (1991) in his prototype-theoretic 5 UD itself allows for language-specific exceptions to the “universal” standard, and we therefore allow that practice here. 6 See Kako 2006; Greene and Resnik 2009; Madnani et al. 2010; Hartshorne et al. 2013 for work using similar protocols. Role property instigation volition awareness sentient change of location -exists as physical existed before existed during existed after change of possession change of state -stationary -location of event -physical contact was used -pred changed arg +was for benefit +partitive +change of state continuous How likely or unlikely is it that... ARG caused the PRED to happen? ARG chose to be involved in the PRED ? ARG was/were aware of being involved in the PRED ? ARG was/were sentient? ARG changed location during the PRED ? ARG existed as"
D16-1177,J93-2004,0,0.0606588,", 1995) in order to more efficiently gather implicit property responses. Everyday speakers can perform basic word sense disambiguation (Snow et al., 2008): this falls under the simplicity tenant of Decomp. Once a word is disambiguated in context we then can infer automatically whether an instance is, e.g., a physical object. While WordNet is a valuable resource, the selection of a specific categorical sense under an enumerated set of prespecified options is troubling in a similar way as Dowty was concerned with thematic roles (see Kilgarriff 1997). Therefore we follow a path similar to Sussna (1993) in asking annotators for zero or more senses that are appropriate.9 Candidate senses are extracted from WordNet synsets. We have grounded argument tokens in WordNet in order to make efficient use of existing lexical semantic resourses, but this protocol could in principle be used with any other lexical semantic resource. We believe these annotations will be useful already in the context of the other annotations, but in addition, future work will use these sense groundings to derive commonsense properties beyond those directly encoded in the WordNet hierarchy. Data collection A total of 18,054"
D16-1177,P06-1014,0,0.013516,"s to derive commonsense properties beyond those directly encoded in the WordNet hierarchy. Data collection A total of 18,054 word tokens (arguments) in 10,833 total sentences extracted from EUD1.2 were annotated for sense by at least three annotators recruited from Amazon Mechanical Turk. Each token had an average of 5.63 candidates senses for annotators to choose from (Figure 4). In total, 1,065 unique annotators participated. 9 Not only does this weaken the commitment to a single categorical meaning, but it also reduces concerns of annotators being confused by overly fine-grain definitions (Navigli, 2006). 1721 Data validation Inter-annotator agreement was computed by lemma by taking the Jaccard index for each pair of annotators that judged the senses for that of senses checked by both annotators lemma: ## of senses checked by either annotator . The overall inter-annotator agreement using this measure was 0.592: this is reasonably high considering the extremely low chance-level. In total, 9,317 token-sense pairs were agreed upon by all annotators. We refer to these token-sense pairs as gold word sense(s) for the token. If we relax the agreement threshold for a token-sense pair to be gold to 0."
D16-1177,J05-1004,0,0.220277,"Missing"
D16-1177,J91-4003,0,0.64869,"tin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics For example, Dowty (1979) followed by a substantial amount of research (e.g. Jackendoff 1990; Rappaport-Hovav and Levin 1998; Levin and Rappaport Hovav 2005) suggests that word meanings can be factored into (i) idiosyncratic, item-specific components and (ii) general components, such as CAUSATION , that are used across the lexicon. In the domain of thematic roles, Dowty (1991) argues that notions such as AGENT should be decomposed into simpler, more primitive properties such as volitional participation in an event. Pustejovsky (1991) decomposes word meanings into qualia structures that again incorporate more primitive properties of events and individuals. In spite of this wealth of theory, existing annotation protocols rarely take the idea into account, operating at the level that the above approaches decompose with very few exceptions (Greene and Resnik, 2009; Hartshorne et al., 2013; Reisinger et al., 2015). Decomp’s premise is that a decompositional approach to large-scale annotation has benefits for both the annotation process and downstream uses of annotated data (cf. He et al. 2015; Bos et al. 2017 for recent nondec"
D16-1177,Q15-1034,1,0.769224,"Missing"
D16-1177,W14-2908,1,0.872359,"Missing"
D16-1177,J12-2002,0,0.0156358,"Missing"
D16-1177,D08-1027,0,0.0687623,"Missing"
D17-3004,D07-1002,0,\N,Missing
D17-3004,lo-wu-2010-evaluating,0,\N,Missing
D17-3004,W05-0635,0,\N,Missing
D17-3004,W05-0628,0,\N,Missing
D17-3004,W04-3212,0,\N,Missing
D17-3004,W05-0634,0,\N,Missing
D17-3004,P10-1040,0,\N,Missing
D17-3004,J05-1004,0,\N,Missing
D17-3004,D14-1162,0,\N,Missing
D17-3004,W04-2705,0,\N,Missing
D17-3004,N07-1069,0,\N,Missing
D17-3004,W13-3520,0,\N,Missing
D17-3004,W08-0804,0,\N,Missing
D17-3004,J93-2004,0,\N,Missing
D17-3004,D12-1074,0,\N,Missing
D17-3004,de-marneffe-etal-2014-universal,0,\N,Missing
D17-3004,W08-2121,0,\N,Missing
D17-3004,D09-1059,0,\N,Missing
D17-3004,W09-1205,1,\N,Missing
D17-3004,J92-4003,0,\N,Missing
D17-3004,W09-1201,0,\N,Missing
D17-3004,petrov-etal-2012-universal,0,\N,Missing
D17-3004,P08-1063,0,\N,Missing
D17-3004,W09-1208,0,\N,Missing
D17-3004,Q15-1034,1,\N,Missing
D17-3004,P14-1111,1,\N,Missing
D17-3004,W13-5503,0,\N,Missing
D17-3004,D16-1177,1,\N,Missing
D18-1007,P17-1080,0,0.0161305,"ast test set, the decrease was not significant. We leave this investigating for a future thorough study. 5 Related Work Exploring what linguistic phenomena neural models learn Many tests have been used to probe how well neural models learn different linguistic phenomena. Linzen et al. (2016) use “number agreement in English subject-verb dependencies” to show that LSTMs learn about syntaxsensitive dependencies. In addition to syntax (Shi et al., 2016), researchers have used other labeling tasks to investigate whether neural machine translation (NMT) models learn different linguistic phenomena (Belinkov et al., 2017a,b; Dalvi et al., 2017; Marvin and Koehn, 2018). Recently, Poliak et al. (2018a) used recast NLI datasets to investigate semantics captured by NMT encoders. Targeted Tests for Natural Language Understanding We follow a long line of work focused on building datasets to test how well NLU systems perform distinct types of semantic reasoning. FraCaS uses a limited number of sentencepairs to test whether systems understand semantic phenomena, e.g. generalized quantifiers, temporal references, and (nominal) anaphora (Cooper et al., 1996). FraCas cannot be used to train neural models – it includes j"
D18-1007,I17-1001,0,0.0114653,"ast test set, the decrease was not significant. We leave this investigating for a future thorough study. 5 Related Work Exploring what linguistic phenomena neural models learn Many tests have been used to probe how well neural models learn different linguistic phenomena. Linzen et al. (2016) use “number agreement in English subject-verb dependencies” to show that LSTMs learn about syntaxsensitive dependencies. In addition to syntax (Shi et al., 2016), researchers have used other labeling tasks to investigate whether neural machine translation (NMT) models learn different linguistic phenomena (Belinkov et al., 2017a,b; Dalvi et al., 2017; Marvin and Koehn, 2018). Recently, Poliak et al. (2018a) used recast NLI datasets to investigate semantics captured by NMT encoders. Targeted Tests for Natural Language Understanding We follow a long line of work focused on building datasets to test how well NLU systems perform distinct types of semantic reasoning. FraCaS uses a limited number of sentencepairs to test whether systems understand semantic phenomena, e.g. generalized quantifiers, temporal references, and (nominal) anaphora (Cooper et al., 1996). FraCas cannot be used to train neural models – it includes j"
D18-1007,I17-1015,0,0.011537,"was not significant. We leave this investigating for a future thorough study. 5 Related Work Exploring what linguistic phenomena neural models learn Many tests have been used to probe how well neural models learn different linguistic phenomena. Linzen et al. (2016) use “number agreement in English subject-verb dependencies” to show that LSTMs learn about syntaxsensitive dependencies. In addition to syntax (Shi et al., 2016), researchers have used other labeling tasks to investigate whether neural machine translation (NMT) models learn different linguistic phenomena (Belinkov et al., 2017a,b; Dalvi et al., 2017; Marvin and Koehn, 2018). Recently, Poliak et al. (2018a) used recast NLI datasets to investigate semantics captured by NMT encoders. Targeted Tests for Natural Language Understanding We follow a long line of work focused on building datasets to test how well NLU systems perform distinct types of semantic reasoning. FraCaS uses a limited number of sentencepairs to test whether systems understand semantic phenomena, e.g. generalized quantifiers, temporal references, and (nominal) anaphora (Cooper et al., 1996). FraCas cannot be used to train neural models – it includes just roughly 300 highqua"
D18-1007,D15-1075,0,0.647225,"I Stefan had visited his son in Bulgaria Stefan was born in Bulgaria Puns I Kim heard masks have no face value Kim heard a pun I Tod heard that thrift is better than annuity Tod heard a pun 3 7 3 7 3 7 Table 1: Example sentence pairs for different semantic phenomena. I indicates the line is a context and the following line is its corresponding hypothesis. 3 and 7 respectively indicate that the context entails, or does not entail the hypothesis. Appendix A includes more recast examples. Introduction A plethora of new natural language inference (NLI)1 datasets has been created in recent years (Bowman et al., 2015; Williams et al., 2017; Lai et al., 2017; Khot et al., 2018). However, these datasets do not provide clear insight into what type of reasoning or inference a model may be performing. For example, these datasets cannot be used to evaluate whether competitive NLI models can determine if an event occurred, correctly differentiate between figurative and literal language, or accurately identify and categorize named entities. Consequently, these datasets cannot answer how well sentence representation learning models capture distinct semantic phenomena necessary for general natural language understa"
D18-1007,D11-1142,0,0.0351374,"icates were extracted directly from Wikipedia infoboxes and are not cleaned. As a result, many relations are redundant with one another (birthPlace, hometown) and some relations do not correspond to obvious natural language glosses based on the name alone (demographics1Info). Thus, we construct a template for each predicate p by manually inspecting 1) a sample of entities which are related by p 2) a sample of sentences in which those entities co-occur and 3) the most frequent natural language strings which join entities related by p according to a OpenIE triple database (Schmitz et al., 2012; Fader et al., 2011) extracted from a large text corpus. We then manually write a simple template (e.g. Mention1 was born in Mention2) for p, ignoring any unclear relations. In total, we end up with 574 unique relations, expressed by 354 unique templates. For each such hypothesis generated, we create a number of contexts. We begin with the FACC1 corpus (Gabrilovich et al., 2013) which contains natural language sentences from ClueWeb in which entities have been automatically linked to disambiguated Freebase entities, when possible. Then, given a tuple hentity1, relation, entity2i, we find every sentence which cont"
D18-1007,P15-1070,0,0.0283141,"Missing"
D18-1007,S17-2005,0,0.0126155,"onstrates natural language’s expressiveness and wide variations. Understanding and recognizing figurative language “entail[s] cognitive capabilities to abstract and meta-represent meanings beyond physical words” (Reyes et al., 2012). Puns are prime examples of figurative language that may perplex general NLU systems as they are one of the more regular uses of linguistic ambiguity (Binsted, 1996) and rely on a wide-range of phonetic, morphological, syntactic, and semantic ambiguity (Pepicello and Green, 1984; Binsted, 1996; Bekinschtein et al., 2011). We recast puns from Yang et al. (2015) and Miller et al. (2017) using templates to generate contexts (6a) and hypotheses (6b), (6c). We replace Name with names sampled from a distribution based on US census data,11 and Pun with the original sentence. If the original sentence was labeled as containing a pun, the (6a)-(6b) pair is labeled as ENTAILED and (6a)-(6c) is labeled as NOT- ENTAILED , otherwise we swap the labels. Michael swatted the fly cause(E, Agent) Agent caused the E Michael caused the swatting We use the Berkeley Parser (Petrov et al., 2006) to match tokens in an example sentence with the thematic roles and then fill in the templates with the"
D18-1007,I17-1011,0,0.0716552,"efan was born in Bulgaria Puns I Kim heard masks have no face value Kim heard a pun I Tod heard that thrift is better than annuity Tod heard a pun 3 7 3 7 3 7 Table 1: Example sentence pairs for different semantic phenomena. I indicates the line is a context and the following line is its corresponding hypothesis. 3 and 7 respectively indicate that the context entails, or does not entail the hypothesis. Appendix A includes more recast examples. Introduction A plethora of new natural language inference (NLI)1 datasets has been created in recent years (Bowman et al., 2015; Williams et al., 2017; Lai et al., 2017; Khot et al., 2018). However, these datasets do not provide clear insight into what type of reasoning or inference a model may be performing. For example, these datasets cannot be used to evaluate whether competitive NLI models can determine if an event occurred, correctly differentiate between figurative and literal language, or accurately identify and categorize named entities. Consequently, these datasets cannot answer how well sentence representation learning models capture distinct semantic phenomena necessary for general natural language understanding (NLU). To answer these questions, w"
D18-1007,P06-1055,0,0.00824722,"and Green, 1984; Binsted, 1996; Bekinschtein et al., 2011). We recast puns from Yang et al. (2015) and Miller et al. (2017) using templates to generate contexts (6a) and hypotheses (6b), (6c). We replace Name with names sampled from a distribution based on US census data,11 and Pun with the original sentence. If the original sentence was labeled as containing a pun, the (6a)-(6b) pair is labeled as ENTAILED and (6a)-(6c) is labeled as NOT- ENTAILED , otherwise we swap the labels. Michael swatted the fly cause(E, Agent) Agent caused the E Michael caused the swatting We use the Berkeley Parser (Petrov et al., 2006) to match tokens in an example sentence with the thematic roles and then fill in the templates with the matched tokens (5d). We also decompose multi-argument predicates into unary predicates to increase the number of hypotheses we generate. On average, each context is paired with 4.5 hypotheses. We generate NOT- ENTAILED hypotheses by filling in templates with incorrect thematic roles. 9 We partition the recast NLI examples into train/development/test splits such that all example sentences from a VerbNet class (which we use a NLI hypothesis) appear in only one partition of our dataset. In turn"
D18-1007,L16-1699,0,0.0478279,"Missing"
D18-1007,P09-1113,0,0.0119408,". http://www.ssa.gov/oact/babynames/ names.zip This is similar to Aharon et al. (2010)’s template matching to generate entailment rules from FrameNet (Baker et al., 1998). 11 71 (7) a. b. c. d. pairs labeled as 1 − 4 and 5 to ENTAILED and NOT- ENTAILED respectively.12 We apply pruning methods (described in Appendix B.4) to combat issues related to noisy, ungrammatical hypotheses and disagreement between multiple annotators. Name was born in Place Name is from Place Name, a Place native, . . . Name visited Place Natural language surface forms are often used in RE in a weak-supervision setting (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013). That is, if entity1 and entity2 are known to be related by relation, it is assumed that every sentence observed which mentions both entity1 and entity2 is assumed to be a realization of relation: i.e. (7d) would (falsely) be taken as evidence of the birthPlace relation. Here we first generate hypotheses and then corresponding contexts. To generate hypotheses, we begin with entity-relation triples extracted from DBPedia infoboxes: e.g. hBarack Obama, birthPlace, Hawaiii. These relation predicates were extracted directly from Wikipedia infoboxes and"
D18-1007,N18-2082,1,0.897386,"Missing"
D18-1007,C18-1198,0,0.0391277,"systems on nearly the full complexity 16 By 32.81, 31.00, and 30.83 points respectively. 74 Glockner et al. (2018) introduce a modified version of SNLI to test how well NLI models perform when requiring lexical and world knowledge. Wang et al. (2018)’s GLUE dataset is intended to evaluate and potentially train a sentence representation to perform well across different NLP tasks. This continues an aspect of the initial RTE collection, designed to be representative of downstream tasks like QA, MT, and IR (Dagan et al., 2010). While GLUE is therefore concerned with applied tasks, DNC, as well as Naik et al. (2018)’s NLI stress tests, is concerned with probing the capabilities of NLU models to capture explicitly distinguished aspects of meaning. While one may conjecture that the latter is needed to be “solved” to eventually “solve” the former, it may be that these goals only partially overlap. Some NLP researchers might focus on probing for semantic phenomena in sentence representations while others may be more interested in developing single sentence representations that can help models perform well on a wide array of downstream tasks. datasets’ training set to investigate what happens if we train our"
D18-1007,S18-2023,1,0.894734,"Missing"
D18-1007,P17-1117,0,0.0245338,"e ability to perform pronoun resolution is essential to language understanding, in many cases requiring common-sense reasoning about the world (Levesque et al., 2012). White et al. (2017) show that this task can be directly recast as an NLI problem by transforming Winograd schemas into NLI sentence pairs. Using a similar formula Rudinger et al. (2018a) introduce Winogender schemas, minimal sentence pairs that differ only by pronoun gender. With this 3 This changed as large NLI datasets have recently been used to train, or pre-train, models to perform NLI, or other tasks (Conneau et al., 2017; Pasunuru and Bansal, 2017). 4 Appendix B.1 provides an example. 5 We replace Event with the event described in the context. 6 We ensure grammatical hypotheses by appropriately conjugating “is a” when needed. 69 Sem. Phenomena Dataset # pairs Automated Event Factuality Decomp (Rudinger et al., 2018b) UW (Lee et al., 2015) MeanTime (Minard et al., 2016) 42K (41,888) 5K (5,094) .7K (738) 3 3 3 Named Entity Recognition Groningen (Bos et al., 2017) CoNLL (Tjong Kim Sang and De Meulder, 2003) 260K (261,406) 60K (59,970) 3 3 Gendered Anaphora Winogender (Rudinger et al., 2018a) .4K (464) 7 Lexicosyntactic Inference VerbCorner"
D18-1007,N13-1008,0,0.0270329,"zip This is similar to Aharon et al. (2010)’s template matching to generate entailment rules from FrameNet (Baker et al., 1998). 11 71 (7) a. b. c. d. pairs labeled as 1 − 4 and 5 to ENTAILED and NOT- ENTAILED respectively.12 We apply pruning methods (described in Appendix B.4) to combat issues related to noisy, ungrammatical hypotheses and disagreement between multiple annotators. Name was born in Place Name is from Place Name, a Place native, . . . Name visited Place Natural language surface forms are often used in RE in a weak-supervision setting (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013). That is, if entity1 and entity2 are known to be related by relation, it is assumed that every sentence observed which mentions both entity1 and entity2 is assumed to be a realization of relation: i.e. (7d) would (falsely) be taken as evidence of the birthPlace relation. Here we first generate hypotheses and then corresponding contexts. To generate hypotheses, we begin with entity-relation triples extracted from DBPedia infoboxes: e.g. hBarack Obama, birthPlace, Hawaiii. These relation predicates were extracted directly from Wikipedia infoboxes and are not cleaned. As a result, many relations"
D18-1007,P16-1204,1,0.926176,"ch that is entailed, neutral, and contradicted by a caption extracted from the Flickr30k corpus (Young et al., 2014). Although these datasets are widely used to train and evaluate sentence representations, a high accuracy is not indicative of what types of reasoning NLI models perform. Workers were free to create any type of hypothesis for each context and label. Such datasets cannot be used to determine how well an NLI model captures many desired capabilities of language understanding systems, e.g. paraphrastic inference, complex anaphora resolution (White et al., 2017), or compositionality (Pavlick and Callison-Burch, 2016; Dasgupta et al., 2018). By converting prior annotation of a specific phenomenon into NLI examples, recasting allows us to create a diverse NLI benchmark that tests a model’s ability to perform distinct types of reasoning. Why These Semantic Phenomena? A long term goal is to develop NLU systems that can achieve human levels of understanding and reasoning. Investigating how different architectures and training corpora can help a system perform human-level general NLU is an important step in this direction. DNC contains recast NLI pairs that are easily understandable by humans and can be used t"
D18-1007,P15-2067,1,0.897725,"Missing"
D18-1007,W17-1609,1,0.882467,"Missing"
D18-1007,D14-1162,0,0.0790388,"Missing"
D18-1007,N18-2002,1,0.883369,"Missing"
D18-1007,N18-1067,1,0.880757,"Missing"
D18-1007,W18-5446,0,0.0711151,"Missing"
D18-1007,I17-1100,1,0.851337,"Missing"
D18-1007,D12-1048,0,0.0309356,"i. These relation predicates were extracted directly from Wikipedia infoboxes and are not cleaned. As a result, many relations are redundant with one another (birthPlace, hometown) and some relations do not correspond to obvious natural language glosses based on the name alone (demographics1Info). Thus, we construct a template for each predicate p by manually inspecting 1) a sample of entities which are related by p 2) a sample of sentences in which those entities co-occur and 3) the most frequent natural language strings which join entities related by p according to a OpenIE triple database (Schmitz et al., 2012; Fader et al., 2011) extracted from a large text corpus. We then manually write a simple template (e.g. Mention1 was born in Mention2) for p, ignoring any unclear relations. In total, we end up with 574 unique relations, expressed by 354 unique templates. For each such hypothesis generated, we create a number of contexts. We begin with the FACC1 corpus (Gabrilovich et al., 2013) which contains natural language sentences from ClueWeb in which entities have been automatically linked to disambiguated Freebase entities, when possible. Then, given a tuple hentity1, relation, entity2i, we find ever"
D18-1007,D16-1159,0,0.0133472,"nvestigate what happens if we train our models on a subsample of each training set instead of the entire DNC. Although we noticed a slight decrease across each recast test set, the decrease was not significant. We leave this investigating for a future thorough study. 5 Related Work Exploring what linguistic phenomena neural models learn Many tests have been used to probe how well neural models learn different linguistic phenomena. Linzen et al. (2016) use “number agreement in English subject-verb dependencies” to show that LSTMs learn about syntaxsensitive dependencies. In addition to syntax (Shi et al., 2016), researchers have used other labeling tasks to investigate whether neural machine translation (NMT) models learn different linguistic phenomena (Belinkov et al., 2017a,b; Dalvi et al., 2017; Marvin and Koehn, 2018). Recently, Poliak et al. (2018a) used recast NLI datasets to investigate semantics captured by NMT encoders. Targeted Tests for Natural Language Understanding We follow a long line of work focused on building datasets to test how well NLU systems perform distinct types of semantic reasoning. FraCaS uses a limited number of sentencepairs to test whether systems understand semantic p"
D18-1007,W03-0419,0,0.503394,"Missing"
D18-1007,D15-1284,0,0.0188814,"Figurative language demonstrates natural language’s expressiveness and wide variations. Understanding and recognizing figurative language “entail[s] cognitive capabilities to abstract and meta-represent meanings beyond physical words” (Reyes et al., 2012). Puns are prime examples of figurative language that may perplex general NLU systems as they are one of the more regular uses of linguistic ambiguity (Binsted, 1996) and rely on a wide-range of phonetic, morphological, syntactic, and semantic ambiguity (Pepicello and Green, 1984; Binsted, 1996; Bekinschtein et al., 2011). We recast puns from Yang et al. (2015) and Miller et al. (2017) using templates to generate contexts (6a) and hypotheses (6b), (6c). We replace Name with names sampled from a distribution based on US census data,11 and Pun with the original sentence. If the original sentence was labeled as containing a pun, the (6a)-(6b) pair is labeled as ENTAILED and (6a)-(6c) is labeled as NOT- ENTAILED , otherwise we swap the labels. Michael swatted the fly cause(E, Agent) Agent caused the E Michael caused the swatting We use the Berkeley Parser (Petrov et al., 2006) to match tokens in an example sentence with the thematic roles and then fill"
D18-1007,L18-1239,0,0.0700741,"leveraging existing datasets to create NLI examples (Glickman, 2006; White et al., 2017). We recast annotations from a total of 13 datasets across 7 NLP tasks into labeled NLI examples. The tasks include event factuality, named entity recognition, gendered anaphora resolution, sentiment analysis, relationship extraction, pun detection, and lexicosyntactic inference. Currently, the DNC contains over half a million labeled examples. Table 1 includes NLI pairs that test specific types of reasoning. Using a hypothesis-only NLI model, with access to just hypothesis sentences, as a strong baseline (Tsuchiya, 2018; Gururangan et al., 2018; Poliak et al., 2018b), our experiments demonstrate how DNC can be used to probe a model’s ability to capture different types of semantic reasoning 1 The task of determining if a hypothesis would likely be inferred from a context, or premise; also known as Recognizing Textual Entailment (RTE) (Dagan et al., 2006, 2013). 67 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 67–81 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics We limit some biases by not relying on humans to g"
D18-1007,Q14-1006,0,0.0500487,"of semantic NLP tasks are freely available. By leveraging existing semantic annotations already invested in by the community we can generate and label NLI pairs at little cost and create large NLI datasets to train data hungry models. NLU Insights Popular NLI datasets, e.g. Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and its successor MultiNLI (Williams et al., 2017), were created by eliciting hypotheses from humans. Crowd-source workers were tasked with writing one sentence each that is entailed, neutral, and contradicted by a caption extracted from the Flickr30k corpus (Young et al., 2014). Although these datasets are widely used to train and evaluate sentence representations, a high accuracy is not indicative of what types of reasoning NLI models perform. Workers were free to create any type of hypothesis for each context and label. Such datasets cannot be used to determine how well an NLI model captures many desired capabilities of language understanding systems, e.g. paraphrastic inference, complex anaphora resolution (White et al., 2017), or compositionality (Pavlick and Callison-Burch, 2016; Dasgupta et al., 2018). By converting prior annotation of a specific phenomenon in"
D18-1007,Q17-1027,1,0.888526,"Missing"
D18-1114,P17-4012,0,0.106947,"Missing"
D18-1114,silveira-etal-2014-gold,0,0.168886,"Missing"
D18-1114,D16-1046,0,0.0529297,"Missing"
D18-1114,D17-3004,1,0.505448,"Missing"
D18-1114,D16-1177,1,0.904935,"Missing"
D18-1114,J05-1004,0,0.0550999,"Model Our proposed SPRL model (Fig. 1) determines the value of each attribute (e.g., VOLITION) on an argument (a) with respect to a particular predication (e) as a function on the latent states associated with the pair, (e, a), in the context of a full sentence. Our architecture encodes the sentence using a shared, one-layer, bidirectional LSTM (Hochreiter and Schmidhuber, 1997; Graves et al., 2013). We then obtain a continuous, vector representation hea = [he ; ha ], for each predicate-argument pair as the concatenation of the hidden BiLSTM 3 This formalism aligns with that used in PropBank (Palmer et al., 2005), which associated numbered, core arguments with each sense of a verb in their corpus annotation. 4 For example, as seen in FrameNet (Baker et al., 1998). 5 Splitting train/dev/test along Penn Treebank boundaries and casting the SPRL task as multi-label binary classification. 945 produced the best results,8 so we simplify discussion by focusing on that model. The efficacy of MT pretraining that we observe here comes as no surprise given prior work demonstrating, e.g., the utility of bitext for paraphrase (Ganitkevitch et al., 2013), that NMT pretraining yields improved contextualized word embe"
D18-1114,N18-1202,0,0.0382141,"SRL model in contrast to recent BIO-style neural models of SRL (He et al., 2017)). To evaluate this idea empirically, we experimented with a number of multi-task training strategies for SPRL. While all settings outperformed prior work in aggregate, simply initializing the BiLSTM parameters with a pretrained English-to-French machine translation encoder7 6 7 2017) trained on the 109 Fr-En corpus (Callison-Burch et al., 2009) (Appendix A). 8 e.g. this initialization resulted in raising micro-averaged F1 from 82.2 to 83.3 9 More recent discoveries on the usefulness of language model pretraining (Peters et al., 2018; Howard and Ruder, 2018) for RNN encoders suggest a promising direction for future SPRL experiments. 10 300-dimensional, uncased; glove.42B.300d from https://nlp.stanford.edu/projects/glove/; 15,533 out-of-vocabulary words across all datasets were assigned a random embedding (uniformly from [−.01, .01]). Embeddings remained fixed during training. Observed to be state-of-the-art by Zhang et al. (2017). using a modified version of OpenNMT-py (Klein et al., 946 1 2 3 4 5 6 7 80 18 15 10 14 11 5 −14 −2 −9 0 −6 −5 −2 6 −2 2 8 0 −2 1 80 21 31 12 9 6 5 −14 4 −6 0 −4 −2 −1 −10 −5 −1 0 1 0 2 Table 3:"
D18-1114,W10-0730,0,\N,Missing
D18-1114,N03-1031,0,\N,Missing
D18-1114,W09-0401,0,\N,Missing
D18-1114,P98-1013,0,\N,Missing
D18-1114,C98-1013,0,\N,Missing
D18-1114,Q15-1034,1,\N,Missing
D18-1114,N13-1092,1,\N,Missing
D18-1114,bonial-etal-2014-propbank,0,\N,Missing
D18-1114,N16-1030,0,\N,Missing
D18-1114,D17-1206,0,\N,Missing
D18-1114,L16-1376,0,\N,Missing
D18-1114,P17-1044,0,\N,Missing
D18-1114,P17-1008,0,\N,Missing
D18-1114,I17-1100,1,\N,Missing
D18-1114,W17-6944,1,\N,Missing
D18-1114,P16-1101,0,\N,Missing
D18-1194,P13-1023,0,0.590441,"), (2) an evaluation metric to measure the similarity between system output and reference semantic analysis, (3) an end-to-end model with a novel annotating mechanism that supports intra-sentential coreference, and (4) an evaluation dataset on which our model outperforms strong baselines by at least 1.75 F1 score. 1 Introduction We are concerned here with representing the semantics of multiple natural languages in a single semantic analysis. Renewed interest in semantic analysis has led to a surge of proposed new frameworks, e.g., GMB (Basile et al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), and UDS (White et al., 2016), as well as further calls to attend to existing efforts, e.g., Episodic Logic (EL) (Schubert and Hwang, 2000; Schubert, 2000; Hwang and Schubert, 1994; Schubert, 2014), or Discourse Representation Theory (Kamp, 1981; Heim, 1988). Many of these efforts are limited to the analysis of English, but with a number of exceptions, e.g., recent efforts by Bos et al. (2017), ongoing efforts in Minimal Recursion Semantics (MRS) (Copestake et al., 1995), multilingual FrameNet annotation and parsing (Fung and Chen, 2004; Pad´o and Lapata, 2005), among others. For many languag"
D18-1194,P17-1008,0,0.0755862,"alysis with a close correspondence to natural language syntax. Unlike interlingua (Mitamura et al., 1991; Dorr and Habash, 2002) that maps the source language into an intermediate analysis, and then maps it into the target language, we are not concerned with generating text from the semantic analysis. Substantial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabiliti"
D18-1194,P16-1231,0,0.167334,"en created linearized representations for these sentences using PredPatt based on their gold UD syntax. Meanwhile, the Chinese translations of these sentences were created by crowdworkers on Amazon Mechanical Turk. The test dataset will be released upon publication. For training, we first collected about 1.8M Chinese-English sentence bitexts from the GALE project (Cohen, 2007), then tokenized Chinese sentences with Stanford Word Segmenter (Chang et al., 2008). We created linearized representations for English sentences using PredPatt based on automatic UD syntax generated by SyntaxNet Parser (Andor et al., 2016), and added SPR and factuality annotations using the state-of-the-art models (Rudinger et al., 2018b,c) trained on SPR v2.x and It-happened v2.0 respectively.7 We hold out 20K training sentences for validation and indomain test. Table 2 reports the dataset statistics. 6.2 Variants We evaluate our model described in Section 5 and three variants: (a) We replace the coreference annotating mechanism by randomly choosing an an7 Train Validation In-domain Test Test Both datasets are available at http://decomp.net No. sents Source 1,879,172 10,000 10,000 270 GALE GALE GALE UD Treebank Table 2: Statis"
D18-1194,P02-1041,0,0.0281372,"ted. Unlike AMR, but akin to decisions made in PropBank (Palmer et al., 2005) (which forms the majority of the AMR ontological backbone), we target an analysis with a close correspondence to natural language syntax. Unlike interlingua (Mitamura et al., 1991; Dorr and Habash, 2002) that maps the source language into an intermediate analysis, and then maps it into the target language, we are not concerned with generating text from the semantic analysis. Substantial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig."
D18-1194,W13-2322,0,0.785265,"exity (shallow to deep analysis), (2) an evaluation metric to measure the similarity between system output and reference semantic analysis, (3) an end-to-end model with a novel annotating mechanism that supports intra-sentential coreference, and (4) an evaluation dataset on which our model outperforms strong baselines by at least 1.75 F1 score. 1 Introduction We are concerned here with representing the semantics of multiple natural languages in a single semantic analysis. Renewed interest in semantic analysis has led to a surge of proposed new frameworks, e.g., GMB (Basile et al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), and UDS (White et al., 2016), as well as further calls to attend to existing efforts, e.g., Episodic Logic (EL) (Schubert and Hwang, 2000; Schubert, 2000; Hwang and Schubert, 1994; Schubert, 2014), or Discourse Representation Theory (Kamp, 1981; Heim, 1988). Many of these efforts are limited to the analysis of English, but with a number of exceptions, e.g., recent efforts by Bos et al. (2017), ongoing efforts in Minimal Recursion Semantics (MRS) (Copestake et al., 1995), multilingual FrameNet annotation and parsing (Fung and Chen, 2004; Pad´o and Lapata, 200"
D18-1194,S16-1176,0,0.141024,"derspecified version of Dependency Minimal Recursion Semantics (DMRS) (Copestake, 2009) due to the underspecification of scope. Different from DMRS, the graph representation is linked cleanly to Universal Dependency syntax via PredPatt. 2.2 Linearized Representation The linearized representation aims to facilitate learning of semantic parsers. Recently parsers based on RNN that make use of linearized representation have achieved state-of-the-art performance in constituency parsing (Choe and Charniak, 2016), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016), and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). There was also work on predicting linearized semantic representations before RNN based approaches (Wong and Mooney, 2006). Fig. 2 shows an example of UDS linearized representation. Intra-sentential coreference occurs when an instance refers to an antecedent, where we replace the instance with a special symbol “•” and add a COREF link between “•” and its antecedent. The linearized representation can be viewed as a sequence of tokens with a list of COREF links. Brackets, parentheses, and the special symbol “•” are all considered as tokens in this representation. The COREF l"
D18-1194,basile-etal-2012-developing,0,0.030024,"levels of structural complexity (shallow to deep analysis), (2) an evaluation metric to measure the similarity between system output and reference semantic analysis, (3) an end-to-end model with a novel annotating mechanism that supports intra-sentential coreference, and (4) an evaluation dataset on which our model outperforms strong baselines by at least 1.75 F1 score. 1 Introduction We are concerned here with representing the semantics of multiple natural languages in a single semantic analysis. Renewed interest in semantic analysis has led to a surge of proposed new frameworks, e.g., GMB (Basile et al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), and UDS (White et al., 2016), as well as further calls to attend to existing efforts, e.g., Episodic Logic (EL) (Schubert and Hwang, 2000; Schubert, 2000; Hwang and Schubert, 1994; Schubert, 2014), or Discourse Representation Theory (Kamp, 1981; Heim, 1988). Many of these efforts are limited to the analysis of English, but with a number of exceptions, e.g., recent efforts by Bos et al. (2017), ongoing efforts in Minimal Recursion Semantics (MRS) (Copestake et al., 1995), multilingual FrameNet annotation and parsing (Fung and Che"
D18-1194,C04-1180,0,0.125253,"decisions made in PropBank (Palmer et al., 2005) (which forms the majority of the AMR ontological backbone), we target an analysis with a close correspondence to natural language syntax. Unlike interlingua (Mitamura et al., 1991; Dorr and Habash, 2002) that maps the source language into an intermediate analysis, and then maps it into the target language, we are not concerned with generating text from the semantic analysis. Substantial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by"
D18-1194,E17-2039,0,0.0902485,"Missing"
D18-1194,P13-2131,0,0.390925,"explore endto-end cross-lingual learning. 4 Evaluation Metric S UDS can be represented in three forms. Evaluating such forms is crucial to the development of parsing algorithms. However, there is no method directly available for evaluation. Related methods come from semantic parsing, whose results are mainly evaluated in three ways: (1) task correctness (Tang and Mooney, 2001), which evaluates on a specific NLP task that uses the parsing results; (2) whole-parse correctness (Zettlemoyer and Collins, 2005), which counts the number of parsing results that are completely correct; and (3) Smatch (Cai and Knight, 2013), which computes the number of exactly matched edges between two semantic structures. Nevertheless, our task needs an evaluation metric that can be used regardless of specific tasks or domains, and is able to differentiate two UDS graph representations with similar instances, SPR analysis, or attributes. We design an evaluation metric S that computes the similarity between two graph representations. As described in Section 2.1, the graph representation is a tuple G = (V, E). For two graphs G1 = (V1 , E1 ) and G2 = (V2 , E2 ), we define the score S as the maximum soft edge matching score betwee"
D18-1194,W08-0336,0,0.0144433,"Missing"
D18-1194,D16-1257,0,0.300107,"ty (Saur´ı and Pustejovsky, 2009) and word senses (Miller, 1995). The graph representation can be viewed as an underspecified version of Dependency Minimal Recursion Semantics (DMRS) (Copestake, 2009) due to the underspecification of scope. Different from DMRS, the graph representation is linked cleanly to Universal Dependency syntax via PredPatt. 2.2 Linearized Representation The linearized representation aims to facilitate learning of semantic parsers. Recently parsers based on RNN that make use of linearized representation have achieved state-of-the-art performance in constituency parsing (Choe and Charniak, 2016), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016), and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). There was also work on predicting linearized semantic representations before RNN based approaches (Wong and Mooney, 2006). Fig. 2 shows an example of UDS linearized representation. Intra-sentential coreference occurs when an instance refers to an antecedent, where we replace the instance with a special symbol “•” and add a COREF link between “•” and its antecedent. The linearized representation can be viewed as a sequence of tokens with a list of COREF links. B"
D18-1194,W07-1210,0,0.0403434,"s to provide a semantic analysis which can be used for various types of deep and shallow processing on the target language side. Many forms of semantic analysis are potentially suitable for this goal, e.g., AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), and Universal Decompositional Semantics (White et al., 2016). Here we choose Universal Decompositional Semantics (UDS), but note that our approach is applicable to other potential graph semantic formalisms. The reasons for choosing UDS are three-fold: (1) Compatibility: UDS relates to Robust Minimal Recursion Semantics (RMRS) (Copestake, 2007), aiming for a maximal degree of semantic compatibility. With UDS, shallow analysis, such as predicate-argument extraction (Zhang et al., 2017a), can be regarded as producing a semantics which is underspecified and reusable with respect to deeper analysis, such as lexical semantics and inference (White et al., 2016). (2) Robustness and Speed: There exists a robust framework, PredPatt (White et al., 2016), for automatically creating UDS from raw sentences and their Universal Dependencies. PredPatt has been shown to be fast and accurate enough to process large volumes of text (Zhang et al., 2017"
D18-1194,E09-1001,0,0.144899,"were reportedh ( COREF deadh (in one blockh of flats) ) ] [ was hith (by a storm surgeh) ] Figure 2: UDS linearized representation. Deeper analysis such as SPR and factuality is not shown. edges describe instances of variables in the target language. The subscript “h” indicates the syntactic head of an instance. (3) Attribute edges are unary, which describe various attributes of variables, such as event factuality (Saur´ı and Pustejovsky, 2009) and word senses (Miller, 1995). The graph representation can be viewed as an underspecified version of Dependency Minimal Recursion Semantics (DMRS) (Copestake, 2009) due to the underspecification of scope. Different from DMRS, the graph representation is linked cleanly to Universal Dependency syntax via PredPatt. 2.2 Linearized Representation The linearized representation aims to facilitate learning of semantic parsers. Recently parsers based on RNN that make use of linearized representation have achieved state-of-the-art performance in constituency parsing (Choe and Charniak, 2016), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016), and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). There was also work on predicting lineari"
D18-1194,1995.tmi-1.2,0,0.525065,"ysis has led to a surge of proposed new frameworks, e.g., GMB (Basile et al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), and UDS (White et al., 2016), as well as further calls to attend to existing efforts, e.g., Episodic Logic (EL) (Schubert and Hwang, 2000; Schubert, 2000; Hwang and Schubert, 1994; Schubert, 2014), or Discourse Representation Theory (Kamp, 1981; Heim, 1988). Many of these efforts are limited to the analysis of English, but with a number of exceptions, e.g., recent efforts by Bos et al. (2017), ongoing efforts in Minimal Recursion Semantics (MRS) (Copestake et al., 1995), multilingual FrameNet annotation and parsing (Fung and Chen, 2004; Pad´o and Lapata, 2005), among others. For many languages, semantic analysis cannot be performed directly, owing to a lack of training data. While there is active work in the community focused on rapid construction of resources for low resource Benjamin Van Durme Johns Hopkins University languages (Strassel and Tracey, 2016), it remains an expensive and perhaps infeasible solution to assume in-language annotated resources for developing semantic parsing technologies. In contrast, bitext is easier to get: it occurs often witho"
D18-1194,P16-1004,0,0.0321227,"Miller, 1995). The graph representation can be viewed as an underspecified version of Dependency Minimal Recursion Semantics (DMRS) (Copestake, 2009) due to the underspecification of scope. Different from DMRS, the graph representation is linked cleanly to Universal Dependency syntax via PredPatt. 2.2 Linearized Representation The linearized representation aims to facilitate learning of semantic parsers. Recently parsers based on RNN that make use of linearized representation have achieved state-of-the-art performance in constituency parsing (Choe and Charniak, 2016), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016), and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). There was also work on predicting linearized semantic representations before RNN based approaches (Wong and Mooney, 2006). Fig. 2 shows an example of UDS linearized representation. Intra-sentential coreference occurs when an instance refers to an antecedent, where we replace the instance with a special symbol “•” and add a COREF link between “•” and its antecedent. The linearized representation can be viewed as a sequence of tokens with a list of COREF links. Brackets, parentheses, and the special symbol “•”"
D18-1194,C16-1056,0,0.0491978,"ediate analysis, and then maps it into the target language, we are not concerned with generating text from the semantic analysis. Substantial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabilities of a given cross-lingual semantic parser. 1666 ing features or model parameters for different languages. Sudo et al. (2004); Zhang et al. (2017a,b); Mei et al. (201"
D18-1194,N15-1151,0,0.0280355,"hen maps it into the target language, we are not concerned with generating text from the semantic analysis. Substantial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabilities of a given cross-lingual semantic parser. 1666 ing features or model parameters for different languages. Sudo et al. (2004); Zhang et al. (2017a,b); Mei et al. (2018) worked on cross-lingual"
D18-1194,C04-1134,0,0.0308449,"t al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), and UDS (White et al., 2016), as well as further calls to attend to existing efforts, e.g., Episodic Logic (EL) (Schubert and Hwang, 2000; Schubert, 2000; Hwang and Schubert, 1994; Schubert, 2014), or Discourse Representation Theory (Kamp, 1981; Heim, 1988). Many of these efforts are limited to the analysis of English, but with a number of exceptions, e.g., recent efforts by Bos et al. (2017), ongoing efforts in Minimal Recursion Semantics (MRS) (Copestake et al., 1995), multilingual FrameNet annotation and parsing (Fung and Chen, 2004; Pad´o and Lapata, 2005), among others. For many languages, semantic analysis cannot be performed directly, owing to a lack of training data. While there is active work in the community focused on rapid construction of resources for low resource Benjamin Van Durme Johns Hopkins University languages (Strassel and Tracey, 2016), it remains an expensive and perhaps infeasible solution to assume in-language annotated resources for developing semantic parsing technologies. In contrast, bitext is easier to get: it occurs often without researcher involvement,1 and even when not available, it may be"
D18-1194,P09-1042,0,0.0406505,"ic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabilities of a given cross-lingual semantic parser. 1666 ing features or model parameters for different languages. Sudo et al. (2004); Zhang et al. (2017a,b); Mei et al. (2018) worked on cross-lingual information extraction and demonstrated the advantages of end-to-end learning. In this work, we explore endto-end cross-lingual learning"
D18-1194,D17-1195,0,0.0187608,"sm5 and decompositional analysis. As illustrated in Fig. 3, Encoder transforms the input sequence into hidden states; Decoder reads the hidden states, and then at each time step generates a token and creates its COREF link; Decompositional Analysis, based on the decoder output, performs SPR analysis for predicate-argument pairs, and factuality analysis for predicates. 4 Future work could consider, e.g., a modified BLEU that considers Levenshtein distance between tokens for a more robust partial-scoring in the face of transliteration errors. 5 Similar coreference mechanism has been proposed by Ji et al. (2017). 1667 Decompostional Analysis AWARENESS SENTIENT INSTIGATION … Decoder 0.8 0.9 0.1 … FACTUAL 1.0 SPR Module Factuality Module y&lt;t and c&lt;t are the preceding tokens and their antecedents. We omit y&lt;t and c&lt;t from the notation when the context is unambiguous. The decoding probability at each time step t is decomposed as P (yt , ct ) = P (yt )P (ct |yt ) COREF peopleh ) were reportedh ( deadh where P (yt ) is the token generation probability, and P (ct |yt ) is the antecedent probability. Token Generation: The probability distribution of the generated token yt is defined as Token Generation & Cor"
D18-1194,P16-1002,0,0.108668,"h representation can be viewed as an underspecified version of Dependency Minimal Recursion Semantics (DMRS) (Copestake, 2009) due to the underspecification of scope. Different from DMRS, the graph representation is linked cleanly to Universal Dependency syntax via PredPatt. 2.2 Linearized Representation The linearized representation aims to facilitate learning of semantic parsers. Recently parsers based on RNN that make use of linearized representation have achieved state-of-the-art performance in constituency parsing (Choe and Charniak, 2016), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016), and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). There was also work on predicting linearized semantic representations before RNN based approaches (Wong and Mooney, 2006). Fig. 2 shows an example of UDS linearized representation. Intra-sentential coreference occurs when an instance refers to an antecedent, where we replace the instance with a special symbol “•” and add a COREF link between “•” and its antecedent. The linearized representation can be viewed as a sequence of tokens with a list of COREF links. Brackets, parentheses, and the special symbol “•” are all considered as"
D18-1194,P17-4012,0,0.0235319,"preprocess the data by replacing the special symbol “•” with the syntactic head of its antecedent. During training and testing, we replace the coreference annotating mechanism with a heuristic that solves coreference by randomly choosing an antecedent among preceding instances which have the same syntactic head. (c) We remove the decoder-side information in the token representation γ(yt ) defined in Equation (12) and only keep the encoder-side information at . We also include a Pipeline approach where Chinese sentences are first translated into English by a neural machine translation system (Klein et al., 2017) and are then annotated by a UD parser (Andor et al., 2016). The UDS linearized representation of Pipeline are created by PredPatt based the automatic UD parses. 6.3 Results Table 1 reports the experimental results on the test set. Results on the in-domain test set are similar and shown in Appendix D. In Table 1, S metric (defined in Section 4) measures the similarity between predicted and reference graph representations. Based on the optimal variable mapping provided by the S metric, we are able to evaluate our model and the variants in different aspects: BLEUINST measures the BLEU score of a"
D18-1194,D17-1018,0,0.0230523,"s. Coref Link: The probability of yt referring to the preceding token yk , i.e., ct = yk , is defined as SCORE(yt , yk ) where yt is the decoded token at time step t, and ct is the source of the COREF link for yt , i.e., the antecedent of yt . The set of possible antecedents of yt is A(t) = {, y1 , . . . , yt−1 }: a dummy antecedent  and all preceding tokens.  represents a scenario, where the token is not a special symbol “•”, and it refers to none of the preceding tokens. αt,i hi , i = sc (yt ) + sp (yk ) + sa (yt , yk ) (8) There are three factors in this pairwise score, which is akin to Lee et al. (2017): (1) sc (yt ), whether yt should refer to a preceding instance; (2) sp (yk ), whether yk shoud be a candidate source of such a coreference; and (3) sa (yt , yk ), whether yk is an antecedent of yt . Fig. 4 shows the details of the scoring architecture. At the core of the three factors are vector representations γ(yt ) for each token yt , which is described in detail in the following section. Given 1668 COREF link score SCORE(yt, yk) Preceding token score sp(yk) SPR: Given a predicate-argument pair (yi , yj ), we (yi ,yj ) denote the score for SPR property p as DSPR . p As shown in Fig. 3, we"
D18-1194,D15-1166,0,0.0237123,"Missing"
D18-1194,D11-1006,0,0.0774706,"Missing"
D18-1194,S18-2017,1,0.868796,"Missing"
D18-1194,1991.mtsummit-papers.9,0,0.0630016,".3 Embracing underspecification in the name of tractability is exemplified by MRS (Copestake et al., 2005; Copestake, 2009), the so-called slacker semantics, and we draw inspiration from that work. Analyses such as AMR (Banarescu et al., 2013) also make use of underspecification, but usually this is only implicit: certain aspects of meaning are simply not annotated. Unlike AMR, but akin to decisions made in PropBank (Palmer et al., 2005) (which forms the majority of the AMR ontological backbone), we target an analysis with a close correspondence to natural language syntax. Unlike interlingua (Mitamura et al., 1991; Dorr and Habash, 2002) that maps the source language into an intermediate analysis, and then maps it into the target language, we are not concerned with generating text from the semantic analysis. Substantial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to"
D18-1194,P12-1066,0,0.0198033,"es (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabilities of a given cross-lingual semantic parser. 1666 ing features or model parameters for different languages. Sudo et al. (2004); Zhang et al. (2017a,b); Mei et al. (2018) worked on cross-lingual information extraction and demonstrated the advantages of end-to-end learning. In this work, we explore endto-end cross-lingual learning. 4 Evaluation Metric S UDS can be represented"
D18-1194,H05-1108,0,0.115905,"Missing"
D18-1194,J05-1004,0,0.288422,"an analysis, we are pursuing a strategy that incrementally increases the complexity of the target analysis in accordance with our ability to fashion models capable of producing it.3 Embracing underspecification in the name of tractability is exemplified by MRS (Copestake et al., 2005; Copestake, 2009), the so-called slacker semantics, and we draw inspiration from that work. Analyses such as AMR (Banarescu et al., 2013) also make use of underspecification, but usually this is only implicit: certain aspects of meaning are simply not annotated. Unlike AMR, but akin to decisions made in PropBank (Palmer et al., 2005) (which forms the majority of the AMR ontological backbone), we target an analysis with a close correspondence to natural language syntax. Unlike interlingua (Mitamura et al., 1991; Dorr and Habash, 2002) that maps the source language into an intermediate analysis, and then maps it into the target language, we are not concerned with generating text from the semantic analysis. Substantial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies ba"
D18-1194,P02-1040,0,0.104017,"Missing"
D18-1194,E17-1035,0,0.355092,"pendency Minimal Recursion Semantics (DMRS) (Copestake, 2009) due to the underspecification of scope. Different from DMRS, the graph representation is linked cleanly to Universal Dependency syntax via PredPatt. 2.2 Linearized Representation The linearized representation aims to facilitate learning of semantic parsers. Recently parsers based on RNN that make use of linearized representation have achieved state-of-the-art performance in constituency parsing (Choe and Charniak, 2016), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016), and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). There was also work on predicting linearized semantic representations before RNN based approaches (Wong and Mooney, 2006). Fig. 2 shows an example of UDS linearized representation. Intra-sentential coreference occurs when an instance refers to an antecedent, where we replace the instance with a special symbol “•” and add a COREF link between “•” and its antecedent. The linearized representation can be viewed as a sequence of tokens with a list of COREF links. Brackets, parentheses, and the special symbol “•” are all considered as tokens in this representation. The COREF links are drawn as a"
D18-1194,P14-2006,0,0.0282414,"has the access to the sentence in the source language during the encoding stage,8 the performance is comparable to the state-of-the-art monolingual model. Table 3: Coreference evaluation (MUC) based on forced decoding. 7 Coreference occurs 589 times in the test set. To evaluate the coreference accuracy of our model, we force the decoder to generate the reference target sequence, and only predict coreference via the copy mechanism, or its variants. In Table 3, we report the precision, recall, and F1 for the standard MUC using the official coreference scorer of the CoNLL-2011/2012 shared tasks (Pradhan et al., 2014). Since coreference in our setup occurs at the sentence level, our model achieves high performance. Variant (a) randomly choosing antecedents performs poorly, whereas variant (b), which solves coreference only based on syntactic heads, achieves a relatively high score. Variant (c) demonstrates that only using encoder-side information in the coreference annotating mechanism leads a significant performance drop. We introduce the task of cross-lingual decompositional semantic parsing, which maps content provided in a source language into decompositional analysis based on a target language. We pre"
D18-1194,D17-1009,0,0.0579582,"Missing"
D18-1194,Q15-1034,1,0.907465,"Missing"
D18-1194,D18-1114,1,0.898981,"Missing"
D18-1194,N18-1067,1,0.885848,"Missing"
D18-1194,W14-2411,0,0.0162147,", and (4) an evaluation dataset on which our model outperforms strong baselines by at least 1.75 F1 score. 1 Introduction We are concerned here with representing the semantics of multiple natural languages in a single semantic analysis. Renewed interest in semantic analysis has led to a surge of proposed new frameworks, e.g., GMB (Basile et al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), and UDS (White et al., 2016), as well as further calls to attend to existing efforts, e.g., Episodic Logic (EL) (Schubert and Hwang, 2000; Schubert, 2000; Hwang and Schubert, 1994; Schubert, 2014), or Discourse Representation Theory (Kamp, 1981; Heim, 1988). Many of these efforts are limited to the analysis of English, but with a number of exceptions, e.g., recent efforts by Bos et al. (2017), ongoing efforts in Minimal Recursion Semantics (MRS) (Copestake et al., 1995), multilingual FrameNet annotation and parsing (Fung and Chen, 2004; Pad´o and Lapata, 2005), among others. For many languages, semantic analysis cannot be performed directly, owing to a lack of training data. While there is active work in the community focused on rapid construction of resources for low resource Benjamin"
D18-1194,silveira-etal-2014-gold,0,0.0744862,"Missing"
D18-1194,L16-1521,0,0.0194704,"1; Heim, 1988). Many of these efforts are limited to the analysis of English, but with a number of exceptions, e.g., recent efforts by Bos et al. (2017), ongoing efforts in Minimal Recursion Semantics (MRS) (Copestake et al., 1995), multilingual FrameNet annotation and parsing (Fung and Chen, 2004; Pad´o and Lapata, 2005), among others. For many languages, semantic analysis cannot be performed directly, owing to a lack of training data. While there is active work in the community focused on rapid construction of resources for low resource Benjamin Van Durme Johns Hopkins University languages (Strassel and Tracey, 2016), it remains an expensive and perhaps infeasible solution to assume in-language annotated resources for developing semantic parsing technologies. In contrast, bitext is easier to get: it occurs often without researcher involvement,1 and even when not available, it may be easier to find bilingual speakers that can translate a text, than it is to find experts that will create in-language semantic annotations. In addition, we are simply further along in being able to automatically understand English than we are other languages, resulting from the bias in investment in English-rooted resources. Th"
D18-1194,C04-1127,0,0.0352355,"t al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabilities of a given cross-lingual semantic parser. 1666 ing features or model parameters for different languages. Sudo et al. (2004); Zhang et al. (2017a,b); Mei et al. (2018) worked on cross-lingual information extraction and demonstrated the advantages of end-to-end learning. In this work, we explore endto-end cross-lingual learning. 4 Evaluation Metric S UDS can be represented in three forms. Evaluating such forms is crucial to the development of parsing algorithms. However, there is no method directly available for evaluation. Related methods come from semantic parsing, whose results are mainly evaluated in three ways: (1) task correctness (Tang and Mooney, 2001), which evaluates on a specific NLP task that uses the pa"
D18-1194,I17-1084,1,0.895202,"Missing"
D18-1194,Q14-1005,0,0.0228795,"2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabilities of a given cross-lingual semantic parser. 1666 ing features or model parameters for different languages. Sudo et al. (2004); Zhang et al. (2017a,b); Mei et al. (2018) worked on cross-lingual information extraction and demonstrated the advantages of end-to-end learning. In this work, we explore endto-end cross-lingual learning. 4 Evaluation Metric S UDS can be represented in three forms. Evaluati"
D18-1194,D16-1177,1,0.853308,"Missing"
D18-1194,N06-1056,0,0.0592432,"RS, the graph representation is linked cleanly to Universal Dependency syntax via PredPatt. 2.2 Linearized Representation The linearized representation aims to facilitate learning of semantic parsers. Recently parsers based on RNN that make use of linearized representation have achieved state-of-the-art performance in constituency parsing (Choe and Charniak, 2016), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016), and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). There was also work on predicting linearized semantic representations before RNN based approaches (Wong and Mooney, 2006). Fig. 2 shows an example of UDS linearized representation. Intra-sentential coreference occurs when an instance refers to an antecedent, where we replace the instance with a special symbol “•” and add a COREF link between “•” and its antecedent. The linearized representation can be viewed as a sequence of tokens with a list of COREF links. Brackets, parentheses, and the special symbol “•” are all considered as tokens in this representation. The COREF links are drawn as a visual convenience, and the actual linearized representation achieves this via co-indexing, and is thus fully linear. We de"
D18-1194,H01-1035,0,0.100566,"002) that maps the source language into an intermediate analysis, and then maps it into the target language, we are not concerned with generating text from the semantic analysis. Substantial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabilities of a given cross-lingual semantic parser. 1666 ing features or model parameters for different languages. Sudo et al."
D18-1194,I08-3008,0,0.0379404,"tial prior work on semantic analyses exists, including HPSG-based analyses (Copestake et al., 2005), CCG-based analyses (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004), and Universal Dependencies based analyses (White et al., 2016; Reddy et al., 2017). See (Schubert, 2015; Abend and Rappoport, 2017) for further discussion. Cross-lingual learning has previously been applied to various NLP tasks. Yarowsky et al. (2001); Pad´o and Lapata (2009); Evang and Bos (2016); Faruqui and Kumar (2015) focused on projecting existing annotations on sourcelanguage text to the target language. Zeman and Resnik (2008); Ganchev et al. (2009); McDonald et al. (2011); Naseem et al. (2012); Wang and Manning (2014) enabled model transfer by shar3 E.g., in Fig. 1a we recognize “by a storm surge” as an initial structural unit, with multiple potential analysis, which may be further refined based on the capabilities of a given cross-lingual semantic parser. 1666 ing features or model parameters for different languages. Sudo et al. (2004); Zhang et al. (2017a,b); Mei et al. (2018) worked on cross-lingual information extraction and demonstrated the advantages of end-to-end learning. In this work, we explore endto-end"
D18-1194,E17-2011,1,0.872845,"Missing"
D18-1501,D15-1075,0,0.0779144,"artly a consequence of the different nonlinearities used for the L-biLSTM (tanh) and T-biLSTM (ReLU), and not the architectures themselves. But whether or not this pattern is due to the architectures, nonlinearities, or both, the entanglement hypothesis may still help explain the pattern of results discussed in §4. 6 Related work This work is inspired by recent work in recasting various semantic annotations into natural language inference (NLI) datasets (White et al., 2017; Poliak et al., 2018a,b; Wang et al., 2018) to gain a better understanding of which phenomena standard neural NLI models (Bowman et al., 2015; Conneau et al., 2017) can capture – a line of work with deep roots (Cooper et al., 1996). The experimental setup – specifically, the idea of UNKing the embedding verb – was inspired by recent work that uses hypothesis-only baselines for a similar purpose (Gururangan et al., 2018; Poliak et al., 2018c; Tsuchiya, 2018). This work is also related to the broader investigation of sentence representations – particularly, tasks aimed at probing these representations’ content (Pavlick and Callison-Burch, 2016; Adi et al., 2016; Conneau et al., 2018; Conneau and Kiela, 2018; Dasgupta et al., 2018). 7"
D18-1501,P16-1139,0,0.0581876,"Missing"
D18-1501,L18-1269,0,0.0181252,"na standard neural NLI models (Bowman et al., 2015; Conneau et al., 2017) can capture – a line of work with deep roots (Cooper et al., 1996). The experimental setup – specifically, the idea of UNKing the embedding verb – was inspired by recent work that uses hypothesis-only baselines for a similar purpose (Gururangan et al., 2018; Poliak et al., 2018c; Tsuchiya, 2018). This work is also related to the broader investigation of sentence representations – particularly, tasks aimed at probing these representations’ content (Pavlick and Callison-Burch, 2016; Adi et al., 2016; Conneau et al., 2018; Conneau and Kiela, 2018; Dasgupta et al., 2018). 7 Conclusion We investigated neural models’ ability to capture lexicosyntactic inference, taking the task of event factuality prediction (EFP) as a case study. We built a factuality judgment dataset for all English clause-embedding verbs in various syntactic contexts and used this dataset to probe current stateof-the-art EFP systems. We showed that these systems make certain systematic errors that are clearly visible through the lens of factuality. Acknowledgments This research was supported by the JHU HLTCOE, DARPA LORELEI and AIDA, NSF-BCS (1748969/1749025), and NSF"
D18-1501,D17-1070,0,0.0281601,"f the different nonlinearities used for the L-biLSTM (tanh) and T-biLSTM (ReLU), and not the architectures themselves. But whether or not this pattern is due to the architectures, nonlinearities, or both, the entanglement hypothesis may still help explain the pattern of results discussed in §4. 6 Related work This work is inspired by recent work in recasting various semantic annotations into natural language inference (NLI) datasets (White et al., 2017; Poliak et al., 2018a,b; Wang et al., 2018) to gain a better understanding of which phenomena standard neural NLI models (Bowman et al., 2015; Conneau et al., 2017) can capture – a line of work with deep roots (Cooper et al., 1996). The experimental setup – specifically, the idea of UNKing the embedding verb – was inspired by recent work that uses hypothesis-only baselines for a similar purpose (Gururangan et al., 2018; Poliak et al., 2018c; Tsuchiya, 2018). This work is also related to the broader investigation of sentence representations – particularly, tasks aimed at probing these representations’ content (Pavlick and Callison-Burch, 2016; Adi et al., 2016; Conneau et al., 2018; Conneau and Kiela, 2018; Dasgupta et al., 2018). 7 Conclusion We investig"
D18-1501,P18-1198,0,0.042181,"Missing"
D18-1501,C96-1079,0,0.130864,"ces are conditioned by surprising aspects of the syntactic context that a word occurs in. For example, while (3a), (3b), and (4a) trigger the inference (2b), (4b) triggers the inference (2c). (3) a. Jo remembered that Bo left. b. Jo didn’t remember that Bo left. (4) a. Bo remembered to leave. b. Bo didn’t remember to leave. Accurately capturing such interactions – e.g. between clause-embedding verbs, negation, and embedded clause type – is important for any system that aims to do general natural language inference (MacCartney et al. 2008 et seq; cf. Dagan et al. 2006) or event extraction (see Grishman and Sundheim 1996 et seq), and it seems unlikely to be a trivial phenomenon to capture, given the complexity and variability of the inferences involved (see, e.g., Karttunen, 2012, 2013; Karttunen et al., 2014; van Leusen, 2012; White, 2014; Baglini and Francez, 2016; Nadathur, 2016, on implicatives). In this paper, we investigate how well current state-of-the-art neural systems for a subtask of general event extraction – event factuality prediction (EFP; Nairn et al., 2006; Saur´ı and Pustejovsky, 2009, 2012; de Marneffe et al., 2012; Lee et al., 2015; Stanovsky et al., 2017; Rudinger et al., 2018) – capture"
D18-1501,P82-1020,0,0.789956,"Missing"
D18-1501,D14-1070,0,0.0843595,"Missing"
D18-1501,S12-1020,0,0.508558,"he inference (2c). (3) a. Jo remembered that Bo left. b. Jo didn’t remember that Bo left. (4) a. Bo remembered to leave. b. Bo didn’t remember to leave. Accurately capturing such interactions – e.g. between clause-embedding verbs, negation, and embedded clause type – is important for any system that aims to do general natural language inference (MacCartney et al. 2008 et seq; cf. Dagan et al. 2006) or event extraction (see Grishman and Sundheim 1996 et seq), and it seems unlikely to be a trivial phenomenon to capture, given the complexity and variability of the inferences involved (see, e.g., Karttunen, 2012, 2013; Karttunen et al., 2014; van Leusen, 2012; White, 2014; Baglini and Francez, 2016; Nadathur, 2016, on implicatives). In this paper, we investigate how well current state-of-the-art neural systems for a subtask of general event extraction – event factuality prediction (EFP; Nairn et al., 2006; Saur´ı and Pustejovsky, 2009, 2012; de Marneffe et al., 2012; Lee et al., 2015; Stanovsky et al., 2017; Rudinger et al., 2018) – capture inferential interactions between lexical items and syntactic context – lexicosyntactic inferences – when trained on current event factuality datasets. Probing the"
D18-1501,D15-1189,0,0.216173,"cf. Dagan et al. 2006) or event extraction (see Grishman and Sundheim 1996 et seq), and it seems unlikely to be a trivial phenomenon to capture, given the complexity and variability of the inferences involved (see, e.g., Karttunen, 2012, 2013; Karttunen et al., 2014; van Leusen, 2012; White, 2014; Baglini and Francez, 2016; Nadathur, 2016, on implicatives). In this paper, we investigate how well current state-of-the-art neural systems for a subtask of general event extraction – event factuality prediction (EFP; Nairn et al., 2006; Saur´ı and Pustejovsky, 2009, 2012; de Marneffe et al., 2012; Lee et al., 2015; Stanovsky et al., 2017; Rudinger et al., 2018) – capture inferential interactions between lexical items and syntactic context – lexicosyntactic inferences – when trained on current event factuality datasets. Probing these particular systems is useful for understanding neural systems’ behavior more generally because (i) the best performing neural models for EFP (Rudinger et al., 2018) are simple instances of common baseline models; and (ii) the task itself is relatively constrained. To do this, we substantially extend the MegaVeridicality1 dataset (White and Rawlins, 2018) to cover all Englis"
D18-1501,D08-1084,0,0.0103987,"t leave. A major finding of this literature is that lexically triggered inferences are conditioned by surprising aspects of the syntactic context that a word occurs in. For example, while (3a), (3b), and (4a) trigger the inference (2b), (4b) triggers the inference (2c). (3) a. Jo remembered that Bo left. b. Jo didn’t remember that Bo left. (4) a. Bo remembered to leave. b. Bo didn’t remember to leave. Accurately capturing such interactions – e.g. between clause-embedding verbs, negation, and embedded clause type – is important for any system that aims to do general natural language inference (MacCartney et al. 2008 et seq; cf. Dagan et al. 2006) or event extraction (see Grishman and Sundheim 1996 et seq), and it seems unlikely to be a trivial phenomenon to capture, given the complexity and variability of the inferences involved (see, e.g., Karttunen, 2012, 2013; Karttunen et al., 2014; van Leusen, 2012; White, 2014; Baglini and Francez, 2016; Nadathur, 2016, on implicatives). In this paper, we investigate how well current state-of-the-art neural systems for a subtask of general event extraction – event factuality prediction (EFP; Nairn et al., 2006; Saur´ı and Pustejovsky, 2009, 2012; de Marneffe et al."
D18-1501,J12-2003,0,0.22694,"Missing"
D18-1501,L16-1699,0,0.0737644,"Missing"
D18-1501,P16-1105,0,0.062885,"Missing"
D18-1501,W06-3907,0,0.143748,"that aims to do general natural language inference (MacCartney et al. 2008 et seq; cf. Dagan et al. 2006) or event extraction (see Grishman and Sundheim 1996 et seq), and it seems unlikely to be a trivial phenomenon to capture, given the complexity and variability of the inferences involved (see, e.g., Karttunen, 2012, 2013; Karttunen et al., 2014; van Leusen, 2012; White, 2014; Baglini and Francez, 2016; Nadathur, 2016, on implicatives). In this paper, we investigate how well current state-of-the-art neural systems for a subtask of general event extraction – event factuality prediction (EFP; Nairn et al., 2006; Saur´ı and Pustejovsky, 2009, 2012; de Marneffe et al., 2012; Lee et al., 2015; Stanovsky et al., 2017; Rudinger et al., 2018) – capture inferential interactions between lexical items and syntactic context – lexicosyntactic inferences – when trained on current event factuality datasets. Probing these particular systems is useful for understanding neural systems’ behavior more generally because (i) the best performing neural models for EFP (Rudinger et al., 2018) are simple instances of common baseline models; and (ii) the task itself is relatively constrained. To do this, we substantially ex"
D18-1501,P16-1204,0,0.0194132,"018a,b; Wang et al., 2018) to gain a better understanding of which phenomena standard neural NLI models (Bowman et al., 2015; Conneau et al., 2017) can capture – a line of work with deep roots (Cooper et al., 1996). The experimental setup – specifically, the idea of UNKing the embedding verb – was inspired by recent work that uses hypothesis-only baselines for a similar purpose (Gururangan et al., 2018; Poliak et al., 2018c; Tsuchiya, 2018). This work is also related to the broader investigation of sentence representations – particularly, tasks aimed at probing these representations’ content (Pavlick and Callison-Burch, 2016; Adi et al., 2016; Conneau et al., 2018; Conneau and Kiela, 2018; Dasgupta et al., 2018). 7 Conclusion We investigated neural models’ ability to capture lexicosyntactic inference, taking the task of event factuality prediction (EFP) as a case study. We built a factuality judgment dataset for all English clause-embedding verbs in various syntactic contexts and used this dataset to probe current stateof-the-art EFP systems. We showed that these systems make certain systematic errors that are clearly visible through the lens of factuality. Acknowledgments This research was supported by the JHU H"
D18-1501,N18-2082,1,0.824655,"Missing"
D18-1501,W18-5441,1,0.894535,"Missing"
D18-1501,S18-2023,1,0.899092,"Missing"
D18-1501,N18-1067,1,0.816425,"Missing"
D18-1501,P15-1150,0,0.146648,"Missing"
D18-1501,L18-1239,0,0.0384544,"ed work This work is inspired by recent work in recasting various semantic annotations into natural language inference (NLI) datasets (White et al., 2017; Poliak et al., 2018a,b; Wang et al., 2018) to gain a better understanding of which phenomena standard neural NLI models (Bowman et al., 2015; Conneau et al., 2017) can capture – a line of work with deep roots (Cooper et al., 1996). The experimental setup – specifically, the idea of UNKing the embedding verb – was inspired by recent work that uses hypothesis-only baselines for a similar purpose (Gururangan et al., 2018; Poliak et al., 2018c; Tsuchiya, 2018). This work is also related to the broader investigation of sentence representations – particularly, tasks aimed at probing these representations’ content (Pavlick and Callison-Burch, 2016; Adi et al., 2016; Conneau et al., 2018; Conneau and Kiela, 2018; Dasgupta et al., 2018). 7 Conclusion We investigated neural models’ ability to capture lexicosyntactic inference, taking the task of event factuality prediction (EFP) as a case study. We built a factuality judgment dataset for all English clause-embedding verbs in various syntactic contexts and used this dataset to probe current stateof-the-ar"
D18-1501,W18-5446,0,0.0252896,"rmation, observed in §4. Of note here is that the pattern seen in Figure 3 is probably at least partly a consequence of the different nonlinearities used for the L-biLSTM (tanh) and T-biLSTM (ReLU), and not the architectures themselves. But whether or not this pattern is due to the architectures, nonlinearities, or both, the entanglement hypothesis may still help explain the pattern of results discussed in §4. 6 Related work This work is inspired by recent work in recasting various semantic annotations into natural language inference (NLI) datasets (White et al., 2017; Poliak et al., 2018a,b; Wang et al., 2018) to gain a better understanding of which phenomena standard neural NLI models (Bowman et al., 2015; Conneau et al., 2017) can capture – a line of work with deep roots (Cooper et al., 1996). The experimental setup – specifically, the idea of UNKing the embedding verb – was inspired by recent work that uses hypothesis-only baselines for a similar purpose (Gururangan et al., 2018; Poliak et al., 2018c; Tsuchiya, 2018). This work is also related to the broader investigation of sentence representations – particularly, tasks aimed at probing these representations’ content (Pavlick and Callison-Burch"
D18-1501,J12-2002,0,0.113234,"Missing"
D18-1501,I17-1100,1,0.897633,"Missing"
D18-1501,Q14-1017,0,0.120514,"Missing"
D18-1501,P17-2056,0,0.186726,"2006) or event extraction (see Grishman and Sundheim 1996 et seq), and it seems unlikely to be a trivial phenomenon to capture, given the complexity and variability of the inferences involved (see, e.g., Karttunen, 2012, 2013; Karttunen et al., 2014; van Leusen, 2012; White, 2014; Baglini and Francez, 2016; Nadathur, 2016, on implicatives). In this paper, we investigate how well current state-of-the-art neural systems for a subtask of general event extraction – event factuality prediction (EFP; Nairn et al., 2006; Saur´ı and Pustejovsky, 2009, 2012; de Marneffe et al., 2012; Lee et al., 2015; Stanovsky et al., 2017; Rudinger et al., 2018) – capture inferential interactions between lexical items and syntactic context – lexicosyntactic inferences – when trained on current event factuality datasets. Probing these particular systems is useful for understanding neural systems’ behavior more generally because (i) the best performing neural models for EFP (Rudinger et al., 2018) are simple instances of common baseline models; and (ii) the task itself is relatively constrained. To do this, we substantially extend the MegaVeridicality1 dataset (White and Rawlins, 2018) to cover all English clause-embedding verbs"
D18-1501,D16-1177,1,0.911803,"Missing"
D19-1084,Q16-1022,0,0.0583752,"Missing"
D19-1084,W18-6318,0,0.118821,"attention weights towards alignments. For example, Chen et al. (2016) introduce a loss over the attention weight that penalizes the model when the attention deviates from alignments; this yields only minor BLEU score improvements.1 Neural Alignment Models Legrand et al. (2016) develop a neural alignment model that uses a convolutional encoder for the source and target sequences and a negative-sampling-based objective. Tamura et al. (2014) introduce a supervised RNNbased aligner which conditions each alignment decision on the source and target sequences as well as previous alignment decisions. Alkhouli et al. (2018) extend the Transformer architecture for alignment by adding an additional alignment head to the multi-head encoder-decoder attention, biasing the attention weights to correspond with alignments. Similarly, Zenkel et al. (2019) introduce a single-layer attention module (akin to the standard Transformer decoder) which predicts the target word conditioned on the encoder and decoder states, following the intuition that a source word should be aligned to a target word if it is highly predictive of the target word. Unlike our model, all of these models are asymmetrical and are trained without expli"
D19-1084,W16-2206,0,0.0816841,"wever, despite their intuitive analogy to word alignments, attention-based alignments typically do not correspond very closely to gold-standard human annotations and suffer from systematic errors (Koehn and Knowles, 2017; Ghader and Monz, 2017), falling short of the promise to jointly learn to align and translate. This problem has only been exacerbated by the introduction of deeper models containing multiple attention layers and heads (e.g. Vaswani et al. (2017)). There have been several attempts to mitigate this discrepancy, mostly with a view to improving the quality of output translations (Alkhouli et al., 2016, 2018; Chen et al., 2016; Liu et al., 2016) – though some work has focused specifically on alignment (Legrand et al., 2016; Zenkel et al., 2019). We confirm that the poor performance of encoder-decoder attention for Recurrent Neural Networks (RNNs) found by Koehn and Knowles (2017) and Ghader and Monz (2017) can also be observed in the self-attentional Transformer model (Vaswani et al., 2017), further motivating an 910 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 910–920,"
D19-1084,W17-4711,0,0.0634044,"n to the standard Transformer decoder) which predicts the target word conditioned on the encoder and decoder states, following the intuition that a source word should be aligned to a target word if it is highly predictive of the target word. Unlike our model, all of these models are asymmetrical and are trained without explicit alignment data. Following the generative formulation of the alignment problem, Alkhouli et al. (2016) present neural lexical translation and alignment models, which they train using silver-standard alignments obtained from GIZA++. A similar training strategy is used in Alkhouli and Ney (2017), who bias an RNN attention module with silver-standard alignments for use as a lexical model. In the same vein, Peter et al. (2017) improve the attention module by allowing it to peek at the next target word. Note that unlike our framework, none of the models mentioned make use of existing gold-standard labeled data. Following Legrand et al. (2016) and Zenkel et al. (2019) we focus exclusively on alignment quality. A similar approach is taken 911 1 This may also be a result of noisy training data, as they use unsupervised alignments as training data for their models. by Ouyang and McKeown (20"
D19-1084,D16-1162,0,0.0358275,"and attention-based alignments. FastAlign is a fast loglinear reparameterization of IBM Model 2 (Brown et al., 1993). Unlike in the RNN case, where a single attention layer intervenes between the encoder and decoder, Transformers make use of multi-head and multi-layer attention. Our attentional alignments are obtained by averaging across all heads of the final multi-head encoder-decoder attention layer. Both of these methods allow for fast online decoding, making them comparable to our model, and both have been used for word alignment; moreover, FastAlign is a standard choice for projection (Arthur et al., 2016; Fu et al., 2014; Eger et al., 2018; Agi´c et al., 2016; Sharoff, 2018). 912 Our model extends the Sockeye implementation of the Transformer model (Hieber et al., 2017).3 Following Dyer et al. (2013), FastAlign hyperparameters were tuned by training on 10K training examples, evaluating on a 100-sentence subset of the validation split. The grow-diag-final-and heuristic was used for symmetrization. Thresholds for both attention alignment and our model were tuned on the same 100-sentence validation subset. 4.2 Data Chinese character-tokenized bitext and alignments were sourced from the GALE Chin"
D19-1084,J93-2003,0,0.104894,"an be observed for Chinese, and that the amount of annotated alignment data has a far greater impact on alignment score than the amount of unlabelled bitext used for pre-training. Furthermore, our NER projection trials demonstrate a major downstream improvement when using our alignments over FastAlign. Taken together, these results motivate the further annotation of small amounts of quality alignment data in a variety of languages. We demonstrate that such annotation can be performed rapidly by untrained L2 (second language) speakers. 2 Related work Statistical Alignment Generative alignment (Brown et al., 1993) models the posterior over a target sequence t given a source sequence s as: XY p(t|s) = p(ti |ai , t<i , s) p(ai |a<i , t<i , s) {z }| {z } | a i lexical model alignment model Such models are asymmetrical: they are learned in both directions and then heuristically combined. Discriminative alignment models, on the other hand, directly model p(a|s, t), usually by extracting features from the source and target sequences and training a supervised classifier using labelled data. A comprehensive account of methods and features for discriminative alignment can be found in Tomeh (2012). Neural Machin"
D19-1084,2016.amta-researchers.10,0,0.263412,"e analogy to word alignments, attention-based alignments typically do not correspond very closely to gold-standard human annotations and suffer from systematic errors (Koehn and Knowles, 2017; Ghader and Monz, 2017), falling short of the promise to jointly learn to align and translate. This problem has only been exacerbated by the introduction of deeper models containing multiple attention layers and heads (e.g. Vaswani et al. (2017)). There have been several attempts to mitigate this discrepancy, mostly with a view to improving the quality of output translations (Alkhouli et al., 2016, 2018; Chen et al., 2016; Liu et al., 2016) – though some work has focused specifically on alignment (Legrand et al., 2016; Zenkel et al., 2019). We confirm that the poor performance of encoder-decoder attention for Recurrent Neural Networks (RNNs) found by Koehn and Knowles (2017) and Ghader and Monz (2017) can also be observed in the self-attentional Transformer model (Vaswani et al., 2017), further motivating an 910 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 910–920, c Hong Kong, China, Nove"
D19-1084,N13-1073,0,0.177245,", t) where a ˆij = 1 if source word i is aligned to target word j in the gold-standard data, and 0 otherwise. Note that the convolution step ensures these are not independent alignment decisions. When training our model, we begin by pretraining an MT model on unlabelled bitext; the weights of this model are used to initialize the encoder and decoder in the alignment model. To obtain an alignment at test time, the source and target word sequences are encoded and presented to the aligner. 4 4.1 Alignment Experiments Baselines We compare against two baselines: alignments obtained from FastAlign (Dyer et al., 2013), and attention-based alignments. FastAlign is a fast loglinear reparameterization of IBM Model 2 (Brown et al., 1993). Unlike in the RNN case, where a single attention layer intervenes between the encoder and decoder, Transformers make use of multi-head and multi-layer attention. Our attentional alignments are obtained by averaging across all heads of the final multi-head encoder-decoder attention layer. Both of these methods allow for fast online decoding, making them comparable to our model, and both have been used for word alignment; moreover, FastAlign is a standard choice for projection"
D19-1084,C18-1071,0,0.012975,"lign is a fast loglinear reparameterization of IBM Model 2 (Brown et al., 1993). Unlike in the RNN case, where a single attention layer intervenes between the encoder and decoder, Transformers make use of multi-head and multi-layer attention. Our attentional alignments are obtained by averaging across all heads of the final multi-head encoder-decoder attention layer. Both of these methods allow for fast online decoding, making them comparable to our model, and both have been used for word alignment; moreover, FastAlign is a standard choice for projection (Arthur et al., 2016; Fu et al., 2014; Eger et al., 2018; Agi´c et al., 2016; Sharoff, 2018). 912 Our model extends the Sockeye implementation of the Transformer model (Hieber et al., 2017).3 Following Dyer et al. (2013), FastAlign hyperparameters were tuned by training on 10K training examples, evaluating on a 100-sentence subset of the validation split. The grow-diag-final-and heuristic was used for symmetrization. Thresholds for both attention alignment and our model were tuned on the same 100-sentence validation subset. 4.2 Data Chinese character-tokenized bitext and alignments were sourced from the GALE Chinese-English Parallel Aligned Treeban"
D19-1084,J07-3002,0,0.14783,"nkel et al. (2019), to reduce BPE alignments to regular alignments, we considered a source and target word to be aligned if any of their subwords were aligned.4 This heuristic was used to evaluate all BPE models. With the task of projection for NER data in mind (Yarowsky et al., 2001), we evaluate all models on Chinese NER spans. These spans were obtained from the OntoNotes corpus, which subsumes the GALE Chinese corpus. Due to formatting differences, only a subset of the GALE sentences (287 validation, 189 test) were recoverable from OntoNotes. We evaluate our models using macro-F1 score, as Fraser and Marcu (2007) showed that alignment error rate does not match human judgments. F1 scores were obtained using the macro-F1 scorer provided by FastAlign. 3 We pre-train a 6-layer model with the Adam optimizer (Kingma and Ba, 2014) using a learning rate of 0.0002, 8headed multi-head attention, 2048-dimensional feed-forward layers, and 512-dimensional encoder/decoder output layers. We include weight-tying (Press and Wolf, 2017) between the source and target embedding layers. All other MT hyperparameters were set to the Sockeye defaults. 913 4.3 Results Our model outperforms both baselines for all languages and"
D19-1084,I17-1004,0,0.18185,"allows researchers to address a lack of annotated multilingual resources and changing ontologies with minimal annotation effort. The introduction of attention (Bahdanau et al., 2014) has allowed NMT to advance past decoding from a single sentence-level vector representation and has been crucial in supporting fluent, meaningful translations, especially of longer sentences. However, despite their intuitive analogy to word alignments, attention-based alignments typically do not correspond very closely to gold-standard human annotations and suffer from systematic errors (Koehn and Knowles, 2017; Ghader and Monz, 2017), falling short of the promise to jointly learn to align and translate. This problem has only been exacerbated by the introduction of deeper models containing multiple attention layers and heads (e.g. Vaswani et al. (2017)). There have been several attempts to mitigate this discrepancy, mostly with a view to improving the quality of output translations (Alkhouli et al., 2016, 2018; Chen et al., 2016; Liu et al., 2016) – though some work has focused specifically on alignment (Legrand et al., 2016; Zenkel et al., 2019). We confirm that the poor performance of encoder-decoder attention for Recurr"
D19-1084,E17-3017,0,0.0241198,"layer intervenes between the encoder and decoder, Transformers make use of multi-head and multi-layer attention. Our attentional alignments are obtained by averaging across all heads of the final multi-head encoder-decoder attention layer. Both of these methods allow for fast online decoding, making them comparable to our model, and both have been used for word alignment; moreover, FastAlign is a standard choice for projection (Arthur et al., 2016; Fu et al., 2014; Eger et al., 2018; Agi´c et al., 2016; Sharoff, 2018). 912 Our model extends the Sockeye implementation of the Transformer model (Hieber et al., 2017).3 Following Dyer et al. (2013), FastAlign hyperparameters were tuned by training on 10K training examples, evaluating on a 100-sentence subset of the validation split. The grow-diag-final-and heuristic was used for symmetrization. Thresholds for both attention alignment and our model were tuned on the same 100-sentence validation subset. 4.2 Data Chinese character-tokenized bitext and alignments were sourced from the GALE Chinese-English Parallel Aligned Treebank (Li et al., 2015); Arabic data was obtained from the GALE Arabic-English Parallel Aligned Treebank (Li et al., 2013). Both GALE dat"
D19-1084,D13-1176,0,0.0175993,"national Joint Conference on Natural Language Processing, pages 910–920, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics approach focusing explicitly on alignment quality. We introduce a novel alignment module that learns to produce high-quality alignments after training on 1.7K to 4.9K human-annotated alignments for Arabic and Chinese, respectively. While our module is integrated into the state-of-the-art Transformer model (Vaswani et al., 2017), the implementation is architecture-neutral, and can therefore be applied to RNN-based models (Schwenk, 2012; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) or fully convolutional models (Gehring et al., 2017). Our experiments on English-Chinese and English-Arabic bitext show that our model yields major improvements of 11 and 27 alignment F1 points (respectively) over baseline models; ablation experiments indicate with only half the data minor improvements can be observed for Chinese, and that the amount of annotated alignment data has a far greater impact on alignment score than the amount of unlabelled bitext used for pre-training. Furthermore, our NER projection trials demonstrate a major downstream improvement when us"
D19-1084,P07-2045,0,0.00724692,"Missing"
D19-1084,W17-3204,0,0.109247,"Hwa, 2005). This paradigm allows researchers to address a lack of annotated multilingual resources and changing ontologies with minimal annotation effort. The introduction of attention (Bahdanau et al., 2014) has allowed NMT to advance past decoding from a single sentence-level vector representation and has been crucial in supporting fluent, meaningful translations, especially of longer sentences. However, despite their intuitive analogy to word alignments, attention-based alignments typically do not correspond very closely to gold-standard human annotations and suffer from systematic errors (Koehn and Knowles, 2017; Ghader and Monz, 2017), falling short of the promise to jointly learn to align and translate. This problem has only been exacerbated by the introduction of deeper models containing multiple attention layers and heads (e.g. Vaswani et al. (2017)). There have been several attempts to mitigate this discrepancy, mostly with a view to improving the quality of output translations (Alkhouli et al., 2016, 2018; Chen et al., 2016; Liu et al., 2016) – though some work has focused specifically on alignment (Legrand et al., 2016; Zenkel et al., 2019). We confirm that the poor performance of encoder-deco"
D19-1084,E17-2025,0,0.0146817,"corpus. Due to formatting differences, only a subset of the GALE sentences (287 validation, 189 test) were recoverable from OntoNotes. We evaluate our models using macro-F1 score, as Fraser and Marcu (2007) showed that alignment error rate does not match human judgments. F1 scores were obtained using the macro-F1 scorer provided by FastAlign. 3 We pre-train a 6-layer model with the Adam optimizer (Kingma and Ba, 2014) using a learning rate of 0.0002, 8headed multi-head attention, 2048-dimensional feed-forward layers, and 512-dimensional encoder/decoder output layers. We include weight-tying (Press and Wolf, 2017) between the source and target embedding layers. All other MT hyperparameters were set to the Sockeye defaults. 913 4.3 Results Our model outperforms both baselines for all languages and experimental conditions. Table 2 shows that our model performs especially well in the NER setting, where we observe the largest improve4 Note that this step is necessary in order to make the produced and reference alignments comparable; to evaluate alignments, the sequences must be identical, or the evaluation is ill-posed. Figure 2: (a) Dev. F1 as function of training corpus size (log scale), contrast against"
D19-1084,C02-1070,0,0.44148,"nstream tasks, such as transferring input formatting, incorporating lexica, and humanin-the-loop translation. Crucially, they are central to cross-lingual dataset creation via projection (Yarowsky et al., 2001), where token-level annotations in a high-resource language are projected across alignments to a low-resource language; using projection, datasets have been created for a variety of natural language processing tasks, including named-entity recognition (NER), part-of-speech tagging, parsing, information extraction (IE), and semantic role labeling (Yarowsky et al., 2001; Hwa et al., 2005; Riloff et al., 2002; Pad´o and Lapata, 2009; Xi and Hwa, 2005). This paradigm allows researchers to address a lack of annotated multilingual resources and changing ontologies with minimal annotation effort. The introduction of attention (Bahdanau et al., 2014) has allowed NMT to advance past decoding from a single sentence-level vector representation and has been crucial in supporting fluent, meaningful translations, especially of longer sentences. However, despite their intuitive analogy to word alignments, attention-based alignments typically do not correspond very closely to gold-standard human annotations an"
D19-1084,C12-2104,0,0.0124335,"d the 9th International Joint Conference on Natural Language Processing, pages 910–920, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics approach focusing explicitly on alignment quality. We introduce a novel alignment module that learns to produce high-quality alignments after training on 1.7K to 4.9K human-annotated alignments for Arabic and Chinese, respectively. While our module is integrated into the state-of-the-art Transformer model (Vaswani et al., 2017), the implementation is architecture-neutral, and can therefore be applied to RNN-based models (Schwenk, 2012; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) or fully convolutional models (Gehring et al., 2017). Our experiments on English-Chinese and English-Arabic bitext show that our model yields major improvements of 11 and 27 alignment F1 points (respectively) over baseline models; ablation experiments indicate with only half the data minor improvements can be observed for Chinese, and that the amount of annotated alignment data has a far greater impact on alignment score than the amount of unlabelled bitext used for pre-training. Furthermore, our NER projection trials demonstrate a majo"
D19-1084,P16-1162,0,0.162944,"otated alignment sentence pairs. et al., 2007). Chinese data was drawn from the WMT 2017 En-Zh bitext, while the Arabic data was sourced from local resources; both Chinese and Arabic models were trained with a 60K-word vocabulary. For FastAlign, the GALE train split was concatenated to the pretraining bitext, while for both the discriminative and attentional models the GALE data was used to finetune a model pre-trained on bitext alone. Contemporary MT systems use subword units to tackle the problem of out-of-vocabulary lowfrequency words. This is often implemented by byte-pair encoding (BPE) (Sennrich et al., 2016). We applied a BPE model with 30K merge operations to all of our data for the BPE experimental condition. Training alignments were expanded by aligning all subwords of a given source word to all the subwords of the target word to which the source word aligns. At test time, following Zenkel et al. (2019), to reduce BPE alignments to regular alignments, we considered a source and target word to be aligned if any of their subwords were aligned.4 This heuristic was used to evaluate all BPE models. With the task of projection for NER data in mind (Yarowsky et al., 2001), we evaluate all models on C"
D19-1084,W16-2207,0,0.183026,"Missing"
D19-1084,L18-1135,0,0.0134943,"tion of IBM Model 2 (Brown et al., 1993). Unlike in the RNN case, where a single attention layer intervenes between the encoder and decoder, Transformers make use of multi-head and multi-layer attention. Our attentional alignments are obtained by averaging across all heads of the final multi-head encoder-decoder attention layer. Both of these methods allow for fast online decoding, making them comparable to our model, and both have been used for word alignment; moreover, FastAlign is a standard choice for projection (Arthur et al., 2016; Fu et al., 2014; Eger et al., 2018; Agi´c et al., 2016; Sharoff, 2018). 912 Our model extends the Sockeye implementation of the Transformer model (Hieber et al., 2017).3 Following Dyer et al. (2013), FastAlign hyperparameters were tuned by training on 10K training examples, evaluating on a 100-sentence subset of the validation split. The grow-diag-final-and heuristic was used for symmetrization. Thresholds for both attention alignment and our model were tuned on the same 100-sentence validation subset. 4.2 Data Chinese character-tokenized bitext and alignments were sourced from the GALE Chinese-English Parallel Aligned Treebank (Li et al., 2015); Arabic data was"
D19-1084,P14-1138,0,0.311652,"and Monz (2017), who provide a detailed analysis of where attention and alignment diverge, offering possible explanations for the modest results obtained when attempting to bias attention weights towards alignments. For example, Chen et al. (2016) introduce a loss over the attention weight that penalizes the model when the attention deviates from alignments; this yields only minor BLEU score improvements.1 Neural Alignment Models Legrand et al. (2016) develop a neural alignment model that uses a convolutional encoder for the source and target sequences and a negative-sampling-based objective. Tamura et al. (2014) introduce a supervised RNNbased aligner which conditions each alignment decision on the source and target sequences as well as previous alignment decisions. Alkhouli et al. (2018) extend the Transformer architecture for alignment by adding an additional alignment head to the multi-head encoder-decoder attention, biasing the attention weights to correspond with alignments. Similarly, Zenkel et al. (2019) introduce a single-layer attention module (akin to the standard Transformer decoder) which predicts the target word conditioned on the encoder and decoder states, following the intuition that"
D19-1084,C16-1291,0,0.0356299,"lignments, attention-based alignments typically do not correspond very closely to gold-standard human annotations and suffer from systematic errors (Koehn and Knowles, 2017; Ghader and Monz, 2017), falling short of the promise to jointly learn to align and translate. This problem has only been exacerbated by the introduction of deeper models containing multiple attention layers and heads (e.g. Vaswani et al. (2017)). There have been several attempts to mitigate this discrepancy, mostly with a view to improving the quality of output translations (Alkhouli et al., 2016, 2018; Chen et al., 2016; Liu et al., 2016) – though some work has focused specifically on alignment (Legrand et al., 2016; Zenkel et al., 2019). We confirm that the poor performance of encoder-decoder attention for Recurrent Neural Networks (RNNs) found by Koehn and Knowles (2017) and Ghader and Monz (2017) can also be observed in the self-attentional Transformer model (Vaswani et al., 2017), further motivating an 910 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 910–920, c Hong Kong, China, November 3–7, 2019. 201"
D19-1084,P19-1467,0,0.0985843,"lkhouli and Ney (2017), who bias an RNN attention module with silver-standard alignments for use as a lexical model. In the same vein, Peter et al. (2017) improve the attention module by allowing it to peek at the next target word. Note that unlike our framework, none of the models mentioned make use of existing gold-standard labeled data. Following Legrand et al. (2016) and Zenkel et al. (2019) we focus exclusively on alignment quality. A similar approach is taken 911 1 This may also be a result of noisy training data, as they use unsupervised alignments as training data for their models. by Ouyang and McKeown (2019), who introduce a pointer network-based model for monolingual phrase alignment which allows for the alignment of variable-length phrases. 3 Model NMT systems typically use an encoder-decoder architecture, where one neural network (the encoder) learns to provide another network (the decoder) with a continuous representation of the input sequence of words in the source language, s1 , . . . , sN , from which the decoder can accurately predict the target sequence t1 , . . . , tM . The intermediate representations of the encoder and decoder are known as hidden states. After training, the hidden sta"
D19-1084,H05-1107,0,0.0475175,"matting, incorporating lexica, and humanin-the-loop translation. Crucially, they are central to cross-lingual dataset creation via projection (Yarowsky et al., 2001), where token-level annotations in a high-resource language are projected across alignments to a low-resource language; using projection, datasets have been created for a variety of natural language processing tasks, including named-entity recognition (NER), part-of-speech tagging, parsing, information extraction (IE), and semantic role labeling (Yarowsky et al., 2001; Hwa et al., 2005; Riloff et al., 2002; Pad´o and Lapata, 2009; Xi and Hwa, 2005). This paradigm allows researchers to address a lack of annotated multilingual resources and changing ontologies with minimal annotation effort. The introduction of attention (Bahdanau et al., 2014) has allowed NMT to advance past decoding from a single sentence-level vector representation and has been crucial in supporting fluent, meaningful translations, especially of longer sentences. However, despite their intuitive analogy to word alignments, attention-based alignments typically do not correspond very closely to gold-standard human annotations and suffer from systematic errors (Koehn and"
D19-1084,H01-1035,0,0.863526,"slation (MT) have set new standards for performance, especially when large amounts of parallel text (bitext) are available. However, explicit word-to-word alignments, which were foundational to pre-neural statistical MT (SMT) (Brown et al., 1993), have largely been lost in neural MT (NMT) models. This is unfortunate: while alignments are not necessary for NMT systems, they have a wealth of applications in downstream tasks, such as transferring input formatting, incorporating lexica, and humanin-the-loop translation. Crucially, they are central to cross-lingual dataset creation via projection (Yarowsky et al., 2001), where token-level annotations in a high-resource language are projected across alignments to a low-resource language; using projection, datasets have been created for a variety of natural language processing tasks, including named-entity recognition (NER), part-of-speech tagging, parsing, information extraction (IE), and semantic role labeling (Yarowsky et al., 2001; Hwa et al., 2005; Riloff et al., 2002; Pad´o and Lapata, 2009; Xi and Hwa, 2005). This paradigm allows researchers to address a lack of annotated multilingual resources and changing ontologies with minimal annotation effort. The"
D19-1392,P13-1023,0,0.397933,"based neural transducer improves the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP. 1 Introduction Broad-coverage semantic parsing aims at mapping any natural language text, regardless of its domain, genre, or even the language itself, into a general-purpose meaning representation. As a long-standing topic of interest in computational linguistics, broad-coverage semantic parsing has targeted a number of meaning representation frameworks, including CCG (Steedman, 1996, 2001), DRS (Kamp and Reyle, 1993; Bos, 2008), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), SDP (Oepen et al., 2014, 2015), and UDS (White et al., 2016).1 Each of these frameworks has their specific formal and linguistic assumptions. Such framework-specific “balkanization” results in a variety of frameworkspecific parsing approaches, and the state-of-theart semantic parser for one framework is not always applicable to another. For instance, the stateof-the-art approaches to SDP parsing (Dozat and 1 Abbreviations respectively denote: Combinatory Categorical Grammar, Discourse Representation Theory, Abstract Meaning Representation, Universal Conceptual Cognitive Annotation, Semantic"
D19-1392,S15-2162,0,0.0397619,"Missing"
D19-1392,P13-1104,0,0.034945,"Missing"
D19-1392,D15-1198,0,0.158101,"Missing"
D19-1392,E17-1051,0,0.734714,"arsing (Dozat and 1 Abbreviations respectively denote: Combinatory Categorical Grammar, Discourse Representation Theory, Abstract Meaning Representation, Universal Conceptual Cognitive Annotation, Semantic Dependency Parsing, and Universal Decompositional Semantics. Manning, 2018; Peng et al., 2017a) are not directly transferable to AMR and UCCA because of the lack of explicit alignments between tokens in the sentence and nodes in the semantic graph. While transition-based approaches are adaptable to different broad-coverage semantic parsing tasks (Wang et al., 2018; Hershcovich et al., 2018; Damonte et al., 2017), when it comes to representations such as AMR whose nodes are unanchored to tokens in the sentence, a pre-trained aligner has to be used to produce the reference transition sequences (Wang et al., 2015; Damonte et al., 2017; Peng et al., 2017b). In contrast, there are attempts to develop attention-based approaches in a graph-based parsing paradigm (Dozat and Manning, 2018; Zhang et al., 2019), but they lack parsing incrementality, which is advocated in terms of computational efficiency and cognitive modeling (Nivre, 2004; Huang and Sagae, 2010). In this paper, we approach different broadcover"
D19-1392,D17-1130,0,0.140818,"opular target of data-driven semantic parsing, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). Graphbased parsers build AMRs by identifying concepts and scoring edges between them, either in a pipeline (Flanigan et al., 2014), or jointly (Zhou et al., 2016; Lyu and Titov, 2018; Zhang et al., 2019). This two-stage parsing process limits the parser incrementality. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions. Grammar-based parsers leverage external semantic resources to derive AMRs compositionally based on CCG rules (Artzi et al., 2015), or SHRG rules (Peng et al., 2015). Another line of work uses neural model translation models to convert sentences into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b), but has relied on data augmentation to produce effective parsers (van Noord and Bos, 2017; Konstas et al., 2017). Our parser differs from the previous ones in that it has incrementality without rel"
D19-1392,W13-2322,0,0.351025,"demonstrate that our attentionbased neural transducer improves the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP. 1 Introduction Broad-coverage semantic parsing aims at mapping any natural language text, regardless of its domain, genre, or even the language itself, into a general-purpose meaning representation. As a long-standing topic of interest in computational linguistics, broad-coverage semantic parsing has targeted a number of meaning representation frameworks, including CCG (Steedman, 1996, 2001), DRS (Kamp and Reyle, 1993; Bos, 2008), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), SDP (Oepen et al., 2014, 2015), and UDS (White et al., 2016).1 Each of these frameworks has their specific formal and linguistic assumptions. Such framework-specific “balkanization” results in a variety of frameworkspecific parsing approaches, and the state-of-theart semantic parser for one framework is not always applicable to another. For instance, the stateof-the-art approaches to SDP parsing (Dozat and 1 Abbreviations respectively denote: Combinatory Categorical Grammar, Discourse Representation Theory, Abstract Meaning Representation, Universal Conceptu"
D19-1392,P81-1022,0,0.682397,"Missing"
D19-1392,P18-2077,0,0.325854,"plicit alignments between tokens in the sentence and nodes in the semantic graph. While transition-based approaches are adaptable to different broad-coverage semantic parsing tasks (Wang et al., 2018; Hershcovich et al., 2018; Damonte et al., 2017), when it comes to representations such as AMR whose nodes are unanchored to tokens in the sentence, a pre-trained aligner has to be used to produce the reference transition sequences (Wang et al., 2015; Damonte et al., 2017; Peng et al., 2017b). In contrast, there are attempts to develop attention-based approaches in a graph-based parsing paradigm (Dozat and Manning, 2018; Zhang et al., 2019), but they lack parsing incrementality, which is advocated in terms of computational efficiency and cognitive modeling (Nivre, 2004; Huang and Sagae, 2010). In this paper, we approach different broadcoverage semantic parsing tasks under a unified framework of transduction. We propose an attention-based neural transducer that extends the two-stage semantic parser of Zhang et al. (2019) to directly transduce input text into a meaning representation in one stage. This transducer has properties of both transition-based approaches and graph-based approaches: on the one hand, it"
D19-1392,S16-1176,0,0.0606068,"ty. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions. Grammar-based parsers leverage external semantic resources to derive AMRs compositionally based on CCG rules (Artzi et al., 2015), or SHRG rules (Peng et al., 2015). Another line of work uses neural model translation models to convert sentences into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b), but has relied on data augmentation to produce effective parsers (van Noord and Bos, 2017; Konstas et al., 2017). Our parser differs from the previous ones in that it has incrementality without relying on pre-trained aligners, and can be effectively trained without data augmentation. Semantic Dependency Parsing (SDP) was introduced in 2014 and 2015 SemEval shared tasks (Oepen et al., 2014, 2015). It is centered around three semantic formalisms – DM (DELPH-IN MRS; Flickinger et al., 2012; Oepen and Lønning, 2006), PAS (Predicate-Argument Structures; Miyao and Tsujii, 2004"
D19-1392,S15-2154,0,0.143571,"Missing"
D19-1392,W08-2222,0,0.0287166,"R, SDP and UCCA – demonstrate that our attentionbased neural transducer improves the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP. 1 Introduction Broad-coverage semantic parsing aims at mapping any natural language text, regardless of its domain, genre, or even the language itself, into a general-purpose meaning representation. As a long-standing topic of interest in computational linguistics, broad-coverage semantic parsing has targeted a number of meaning representation frameworks, including CCG (Steedman, 1996, 2001), DRS (Kamp and Reyle, 1993; Bos, 2008), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), SDP (Oepen et al., 2014, 2015), and UDS (White et al., 2016).1 Each of these frameworks has their specific formal and linguistic assumptions. Such framework-specific “balkanization” results in a variety of frameworkspecific parsing approaches, and the state-of-theart semantic parser for one framework is not always applicable to another. For instance, the stateof-the-art approaches to SDP parsing (Dozat and 1 Abbreviations respectively denote: Combinatory Categorical Grammar, Discourse Representation Theory, Abstract Meaning Repr"
D19-1392,P15-1033,0,0.0336696,"are graph-based: Peng et al. (2017a, 2018) use a max-margin classifier on top of a BiLSTM, with the factored score for each graph over predicates, unlabeled arcs, and arc labels. Multi-task learning approaches and disjoint data have been used to improve the parser performance. Dozat and Manning (2018) extend an LSTM-based syntactic dependency parser to produce graph-structured dependencies, and carefully tune it to state of the art performance. Wang et al. (2018) extend the transition system of Choi and McCallum (2013) to produce non-projective trees, and use improved versions of stack-LSTMs (Dyer et al., 2015) to learn representation for key components. All of these are specialized for bi-lexical dependency parsing, whereas our parser can effectively produce both bi-lexical semantics graphs, and graphs that are less anchored to the surface utterance. Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) targets a level of semantic granularity that abstracts away from syntactic paraphrases in a typologicallymotivated, cross-linguistic fashion. Sentence representations in UCCA are directed acyclic graphs (DAG), where terminal nodes correspond to surface lexical tokens, and non-t"
D19-1392,D19-1393,0,0.542525,"Missing"
D19-1392,P13-2131,0,0.446911,"Missing"
D19-1392,P14-1134,0,0.372446,"es, named entities, negation and modality, into a rooted, directed, and usually acyclic graph with node and edge labels. AMR graphs abstract away from syntactic realizations, i.e., there is no explicit correspondence between elements of the graph and the surface utterance. Fig. 1(a) shows an example AMR graph. Since its first general release in 2014, AMR has been a popular target of data-driven semantic parsing, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). Graphbased parsers build AMRs by identifying concepts and scoring edges between them, either in a pipeline (Flanigan et al., 2014), or jointly (Zhou et al., 2016; Lyu and Titov, 2018; Zhang et al., 2019). This two-stage parsing process limits the parser incrementality. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions. Grammar-based parsers leverage external semantic resources to derive AMRs compositionally based on CCG rules (Artzi et al., 2015),"
D19-1392,S16-1180,0,0.0403568,"rface utterance. Fig. 1(a) shows an example AMR graph. Since its first general release in 2014, AMR has been a popular target of data-driven semantic parsing, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). Graphbased parsers build AMRs by identifying concepts and scoring edges between them, either in a pipeline (Flanigan et al., 2014), or jointly (Zhou et al., 2016; Lyu and Titov, 2018; Zhang et al., 2019). This two-stage parsing process limits the parser incrementality. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions. Grammar-based parsers leverage external semantic resources to derive AMRs compositionally based on CCG rules (Artzi et al., 2015), or SHRG rules (Peng et al., 2015). Another line of work uses neural model translation models to convert sentences into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b), but has relied on data augmentation to produce effective parsers (van Noo"
D19-1392,D18-1198,0,0.663725,"Missing"
D19-1392,P17-1104,0,0.114026,"nce. Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) targets a level of semantic granularity that abstracts away from syntactic paraphrases in a typologicallymotivated, cross-linguistic fashion. Sentence representations in UCCA are directed acyclic graphs (DAG), where terminal nodes correspond to surface lexical tokens, and non-terminal nodes to semantic units that participate in super-ordinate relations. Edges are labeled, indicating the role of a child in the relation the parent represents. Fig. 1(c) shows an example UCCA DAG. The first UCCA parser is proposed by Hershcovich et al. (2017), where they extend a transition system to produce DAGs. To leverage other semantic resources, Hershcovich et al. (2018) is one of the few attempts to present (lossy) conversion from AMR, SDP and Universal Dependencies (UD; Nivre et al., 2016) to a unified UCCA3787 express-01 ARG0 person op1 Pierre expressed ARG1 concern-01 ARG1 Pierre op2 Vinken Pierre (3) op2 Vinken (4) ARG1 concern ARG1 person (2) Term Term top (1) Pierre (3) ARG2 concern (4) Phrase his (5) (e) DM arborescence A (6) H A H (2) A A P P poss-of Pierre (3) Term Pierre Vinken expressed his concern expressed (1) compound-of P Ter"
D19-1392,P18-1035,0,0.158075,"that abstracts away from syntactic paraphrases in a typologicallymotivated, cross-linguistic fashion. Sentence representations in UCCA are directed acyclic graphs (DAG), where terminal nodes correspond to surface lexical tokens, and non-terminal nodes to semantic units that participate in super-ordinate relations. Edges are labeled, indicating the role of a child in the relation the parent represents. Fig. 1(c) shows an example UCCA DAG. The first UCCA parser is proposed by Hershcovich et al. (2017), where they extend a transition system to produce DAGs. To leverage other semantic resources, Hershcovich et al. (2018) is one of the few attempts to present (lossy) conversion from AMR, SDP and Universal Dependencies (UD; Nivre et al., 2016) to a unified UCCA3787 express-01 ARG0 person op1 Pierre expressed ARG1 concern-01 ARG1 Pierre op2 Vinken Pierre (3) op2 Vinken (4) ARG1 concern ARG1 person (2) Term Term top (1) Pierre (3) ARG2 concern (4) Phrase his (5) (e) DM arborescence A (6) H A H (2) A A P P poss-of Pierre (3) Term Pierre Vinken expressed his concern expressed (1) compound-of P Term (c) UCCA Vinken (2) (d) AMR arborescence Term (b) DM ARG1 concern-01 (5) A his poss express-01 (1) person (2) P ARG2 c"
D19-1392,P17-1014,0,0.154646,"tion systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions. Grammar-based parsers leverage external semantic resources to derive AMRs compositionally based on CCG rules (Artzi et al., 2015), or SHRG rules (Peng et al., 2015). Another line of work uses neural model translation models to convert sentences into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b), but has relied on data augmentation to produce effective parsers (van Noord and Bos, 2017; Konstas et al., 2017). Our parser differs from the previous ones in that it has incrementality without relying on pre-trained aligners, and can be effectively trained without data augmentation. Semantic Dependency Parsing (SDP) was introduced in 2014 and 2015 SemEval shared tasks (Oepen et al., 2014, 2015). It is centered around three semantic formalisms – DM (DELPH-IN MRS; Flickinger et al., 2012; Oepen and Lønning, 2006), PAS (Predicate-Argument Structures; Miyao and Tsujii, 2004), and PSD (Prague Semantic Dependencies; Hajiˇc et al., 2012) – representing predicate-argument relations between content words in a s"
D19-1392,P19-1450,0,0.10329,"Missing"
D19-1392,P18-1037,0,0.15254,"d, directed, and usually acyclic graph with node and edge labels. AMR graphs abstract away from syntactic realizations, i.e., there is no explicit correspondence between elements of the graph and the surface utterance. Fig. 1(a) shows an example AMR graph. Since its first general release in 2014, AMR has been a popular target of data-driven semantic parsing, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). Graphbased parsers build AMRs by identifying concepts and scoring edges between them, either in a pipeline (Flanigan et al., 2014), or jointly (Zhou et al., 2016; Lyu and Titov, 2018; Zhang et al., 2019). This two-stage parsing process limits the parser incrementality. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions. Grammar-based parsers leverage external semantic resources to derive AMRs compositionally based on CCG rules (Artzi et al., 2015), or SHRG rules (Peng et al., 2015). Another line of w"
D19-1392,S16-1166,0,0.0963677,"Abstract Meaning Representation (AMR; Banarescu et al., 2013) encodes sentence-level semantics, such as predicate-argument information, reentrancies, named entities, negation and modality, into a rooted, directed, and usually acyclic graph with node and edge labels. AMR graphs abstract away from syntactic realizations, i.e., there is no explicit correspondence between elements of the graph and the surface utterance. Fig. 1(a) shows an example AMR graph. Since its first general release in 2014, AMR has been a popular target of data-driven semantic parsing, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). Graphbased parsers build AMRs by identifying concepts and scoring edges between them, either in a pipeline (Flanigan et al., 2014), or jointly (Zhou et al., 2016; Lyu and Titov, 2018; Zhang et al., 2019). This two-stage parsing process limits the parser incrementality. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produc"
D19-1392,S17-2090,0,0.0512281,"eaning Representation (AMR; Banarescu et al., 2013) encodes sentence-level semantics, such as predicate-argument information, reentrancies, named entities, negation and modality, into a rooted, directed, and usually acyclic graph with node and edge labels. AMR graphs abstract away from syntactic realizations, i.e., there is no explicit correspondence between elements of the graph and the surface utterance. Fig. 1(a) shows an example AMR graph. Since its first general release in 2014, AMR has been a popular target of data-driven semantic parsing, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). Graphbased parsers build AMRs by identifying concepts and scoring edges between them, either in a pipeline (Flanigan et al., 2014), or jointly (Zhou et al., 2016; Lyu and Titov, 2018; Zhang et al., 2019). This two-stage parsing process limits the parser incrementality. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions."
D19-1392,C04-1204,0,0.158385,"rzdins and Gosko, 2016; Peng et al., 2017b), but has relied on data augmentation to produce effective parsers (van Noord and Bos, 2017; Konstas et al., 2017). Our parser differs from the previous ones in that it has incrementality without relying on pre-trained aligners, and can be effectively trained without data augmentation. Semantic Dependency Parsing (SDP) was introduced in 2014 and 2015 SemEval shared tasks (Oepen et al., 2014, 2015). It is centered around three semantic formalisms – DM (DELPH-IN MRS; Flickinger et al., 2012; Oepen and Lønning, 2006), PAS (Predicate-Argument Structures; Miyao and Tsujii, 2004), and PSD (Prague Semantic Dependencies; Hajiˇc et al., 2012) – representing predicate-argument relations between content words in a sentence. Their annotations have been converted into bi-lexical dependencies, forming directed graphs whose nodes injectively correspond to surface lexical units, and edges represent semantic relations between nodes. In this work, we focus on only the DM formalism. Fig. 1(b) shows an example DM graph. Most recent parsers for SDP are graph-based: Peng et al. (2017a, 2018) use a max-margin classifier on top of a BiLSTM, with the factored score for each graph over p"
D19-1392,P19-1451,0,0.463745,"Missing"
D19-1392,P10-1110,0,0.0867168,"Missing"
D19-1392,S15-2153,0,0.55673,"Missing"
D19-1392,S14-2008,0,0.411139,"the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP. 1 Introduction Broad-coverage semantic parsing aims at mapping any natural language text, regardless of its domain, genre, or even the language itself, into a general-purpose meaning representation. As a long-standing topic of interest in computational linguistics, broad-coverage semantic parsing has targeted a number of meaning representation frameworks, including CCG (Steedman, 1996, 2001), DRS (Kamp and Reyle, 1993; Bos, 2008), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), SDP (Oepen et al., 2014, 2015), and UDS (White et al., 2016).1 Each of these frameworks has their specific formal and linguistic assumptions. Such framework-specific “balkanization” results in a variety of frameworkspecific parsing approaches, and the state-of-theart semantic parser for one framework is not always applicable to another. For instance, the stateof-the-art approaches to SDP parsing (Dozat and 1 Abbreviations respectively denote: Combinatory Categorical Grammar, Discourse Representation Theory, Abstract Meaning Representation, Universal Conceptual Cognitive Annotation, Semantic Dependency Parsing, and U"
D19-1392,oepen-lonning-2006-discriminant,0,0.0660154,"anslation models to convert sentences into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b), but has relied on data augmentation to produce effective parsers (van Noord and Bos, 2017; Konstas et al., 2017). Our parser differs from the previous ones in that it has incrementality without relying on pre-trained aligners, and can be effectively trained without data augmentation. Semantic Dependency Parsing (SDP) was introduced in 2014 and 2015 SemEval shared tasks (Oepen et al., 2014, 2015). It is centered around three semantic formalisms – DM (DELPH-IN MRS; Flickinger et al., 2012; Oepen and Lønning, 2006), PAS (Predicate-Argument Structures; Miyao and Tsujii, 2004), and PSD (Prague Semantic Dependencies; Hajiˇc et al., 2012) – representing predicate-argument relations between content words in a sentence. Their annotations have been converted into bi-lexical dependencies, forming directed graphs whose nodes injectively correspond to surface lexical units, and edges represent semantic relations between nodes. In this work, we focus on only the DM formalism. Fig. 1(b) shows an example DM graph. Most recent parsers for SDP are graph-based: Peng et al. (2017a, 2018) use a max-margin classifier on t"
D19-1392,P17-1186,0,0.105416,"Missing"
D19-1392,N18-1135,0,0.123138,"Missing"
D19-1392,K15-1004,0,0.0393825,"hou et al., 2016; Lyu and Titov, 2018; Zhang et al., 2019). This two-stage parsing process limits the parser incrementality. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions. Grammar-based parsers leverage external semantic resources to derive AMRs compositionally based on CCG rules (Artzi et al., 2015), or SHRG rules (Peng et al., 2015). Another line of work uses neural model translation models to convert sentences into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b), but has relied on data augmentation to produce effective parsers (van Noord and Bos, 2017; Konstas et al., 2017). Our parser differs from the previous ones in that it has incrementality without relying on pre-trained aligners, and can be effectively trained without data augmentation. Semantic Dependency Parsing (SDP) was introduced in 2014 and 2015 SemEval shared tasks (Oepen et al., 2014, 2015). It is centered around three semantic formalisms –"
D19-1392,E17-1035,0,0.244366,"se frameworks has their specific formal and linguistic assumptions. Such framework-specific “balkanization” results in a variety of frameworkspecific parsing approaches, and the state-of-theart semantic parser for one framework is not always applicable to another. For instance, the stateof-the-art approaches to SDP parsing (Dozat and 1 Abbreviations respectively denote: Combinatory Categorical Grammar, Discourse Representation Theory, Abstract Meaning Representation, Universal Conceptual Cognitive Annotation, Semantic Dependency Parsing, and Universal Decompositional Semantics. Manning, 2018; Peng et al., 2017a) are not directly transferable to AMR and UCCA because of the lack of explicit alignments between tokens in the sentence and nodes in the semantic graph. While transition-based approaches are adaptable to different broad-coverage semantic parsing tasks (Wang et al., 2018; Hershcovich et al., 2018; Damonte et al., 2017), when it comes to representations such as AMR whose nodes are unanchored to tokens in the sentence, a pre-trained aligner has to be used to produce the reference transition sequences (Wang et al., 2015; Damonte et al., 2017; Peng et al., 2017b). In contrast, there are attempts"
D19-1392,D14-1162,0,0.083512,"the partial semantic graph, which helps prune the output space for both nodes and edges. Since at each decoding step, we assume the incoming edge is always from a preceding node (see Section 4.3 for the details), the predicted semantic graph is guaranteed to be a valid arborescence, and a MST algorithm is no longer needed. 4.1 Encoder At the encoding stage, we employ an encoder embedding module to convert the input text into vector representations, and a BiLSTM is used to encode vector representations into hidden states. Encoder Embedding Module concatenates word-level embeddings from GloVe (Pennington et al., 2014) and BERT2 (Devlin et al., 2018), char-level embeddings from CharCNN (Kim et al., 2016), and randomly initialized embeddings for POS tags. For AMR, it includes extra randomly initialized embeddings for anonymization indicators that tell the encoder whether a token is an anonymized token from preprocessing. For UCCA, it includes extra randomly initialized embeddings for NER tags, syntactic dependency labels, punctuation indicators, and shapes that are provided in the UCCA official dataset. Multi-layer BiLSTM (Hochreiter and Schmidhuber, 1997) is defined as:  →   −−−→ l−1 l  − s lt LSTM (st"
D19-1392,D15-1136,0,0.123967,"Missing"
D19-1392,P17-1099,0,0.198219,"source node index du , a relation type r, a target node label v, and a target node index dv . Let Y be the output space. The unified transduction problem is to seek the most-likely sequence of semantic relations Yˆ given X: Yˆ = arg max P(Y |X) Y ∈Y = arg max Y ∈Y 4 m Y P(yi |y&lt;i , X) i Transducer To tackle the unified transduction problem, we introduce an attention-based neural transducer that extends Zhang et al. (2019)’s attention-based parser. Their attention-based parser addresses semantic parsing in a two-stage process: it first employs an extended variant of pointer-generator network (See et al., 2017) to convert the input text into a list of nodes, and then uses a deep biaffine graph-based parser (Dozat and Manning, 2016) with a maximum spanning tree (MST) algorithm to create edges. In contrast, our attention-based neural transducer directly transduces the input text into a meaning representation in one stage via a sequence of semantic relations. A high-level model architecture of our transducer is depicted in Fig. 2: an encoder first encodes the input text into hidden states; and then conditioned on the hidden states, at each decoding time step, a decoder takes the previous semantic relat"
D19-1392,J01-1008,0,0.0959536,"Missing"
D19-1392,S16-1181,0,0.147679,"Missing"
D19-1392,D17-1129,0,0.400515,"Missing"
D19-1392,N15-1040,0,0.615577,"Dependency Parsing, and Universal Decompositional Semantics. Manning, 2018; Peng et al., 2017a) are not directly transferable to AMR and UCCA because of the lack of explicit alignments between tokens in the sentence and nodes in the semantic graph. While transition-based approaches are adaptable to different broad-coverage semantic parsing tasks (Wang et al., 2018; Hershcovich et al., 2018; Damonte et al., 2017), when it comes to representations such as AMR whose nodes are unanchored to tokens in the sentence, a pre-trained aligner has to be used to produce the reference transition sequences (Wang et al., 2015; Damonte et al., 2017; Peng et al., 2017b). In contrast, there are attempts to develop attention-based approaches in a graph-based parsing paradigm (Dozat and Manning, 2018; Zhang et al., 2019), but they lack parsing incrementality, which is advocated in terms of computational efficiency and cognitive modeling (Nivre, 2004; Huang and Sagae, 2010). In this paper, we approach different broadcoverage semantic parsing tasks under a unified framework of transduction. We propose an attention-based neural transducer that extends the two-stage semantic parser of Zhang et al. (2019) to directly transd"
D19-1392,D16-1177,1,0.902229,"Missing"
D19-1392,P19-1009,1,0.699796,"Missing"
D19-1392,D18-1194,1,0.890588,"Missing"
D19-1392,D16-1065,0,0.0450789,"ality, into a rooted, directed, and usually acyclic graph with node and edge labels. AMR graphs abstract away from syntactic realizations, i.e., there is no explicit correspondence between elements of the graph and the surface utterance. Fig. 1(a) shows an example AMR graph. Since its first general release in 2014, AMR has been a popular target of data-driven semantic parsing, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). Graphbased parsers build AMRs by identifying concepts and scoring edges between them, either in a pipeline (Flanigan et al., 2014), or jointly (Zhou et al., 2016; Lyu and Titov, 2018; Zhang et al., 2019). This two-stage parsing process limits the parser incrementality. Transition-based parsers either transform dependency trees into AMRs (Wang et al., 2015, 2016; Goodman et al., 2016), or employ transition systems specifically tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). Transitionbased parsers rely on pre-trained aligner produce the reference transitions. Grammar-based parsers leverage external semantic resources to derive AMRs compositionally based on CCG rules (Artzi et al., 2015), or SHRG rules (Peng et al., 201"
D19-6105,E17-1005,0,0.0522088,"Missing"
D19-6105,P15-1162,0,0.0821804,"Missing"
D19-6105,N18-1172,0,0.0181533,"lized encoders would capture sequential or positional information in our data inputs, so we do not use them. By not using contextualized encoders, each word has only one embedding, which is used regardless of its context. (t) (t) (t) (t) −[αt log qφt (yi |xi ) + βt log pθ (xi |hi )] (t) (t) for input xi and its label yi drawn from dataset (t) t. The conditioning vector for the example, hi , (t) may include information about yi . The discriminative and reconstruction task weights are αt and βt , respectively. 3 Experiments As an external baseline, we compare our approach to methods proposed by Augenstein et al. (2018), herein referred to as ARS. ARS achieve state-ofthe-art performance on topic-based sentiment analysis. We reimplement their baseline model as an additional comparison in our results (Table 3). The main contributions of ARS are additional architectural components called the label embedding layer (LEL) and the label transfer network (LTN). In the baseline model, an example’s two input sequences, x1 and x2 , are encoded using a two-stage bi-directional RNN and then passed into a taskspecific classification layer. In the LEL model, the task-specific classification layers are replaced by a label e"
D19-6105,E17-2026,0,0.0470471,"Missing"
D19-6105,N18-1111,0,0.0671651,"Missing"
D19-6105,P17-1001,0,0.045215,"Missing"
D19-6105,N18-1101,0,0.0413708,"Missing"
D19-6105,S16-1003,0,0.0712994,"Missing"
D19-6105,S16-1001,0,0.0241309,"Missing"
D19-6105,D14-1162,0,0.0816965,"s qφt (y |x) and pθ (x |h), our loss function, LGMTL , on a single example is: Pre-Trained Word Embeddings A popular way to improve performance over the use of randomly initialized word embeddings is to use pre-trained word embeddings that have been learned from large corpora. The use of pre-trained embeddings is an example of transfer learning, which unlike MTL typically involves a pipeline of tasks rather than a joint training objective. Word embeddings are usually learned by fitting a language model (or other word prediction objective) on an out-of-domain text corpus (Mikolov et al., 2013; Pennington et al., 2014). Although pre-trained word embeddings are learned in context and can thereby capture distributional syntactic information, good performance using pre-trained word embeddings would be evidence that sequence-aware models may not be necessary for MTL for the tasks we consider here. Because we restrict our models to use only bagof-words features, we seek to avoid any syntactic or sequential information that could be derived from our inputs. Any syntactic information present in pre-trained word embeddings comes from the sequences used in pre-training, not from the data in our tasks. By using pre-t"
D19-6105,N18-1202,0,0.0368748,"ch as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) that build compositional sequence representations (Iyyer et al., 2015; Wieting et al., 2016; Arora et al., 2017). Arora et al. (2017) suggest that BOW models better exploit the semantics of a sequence than RNNs do. Bag-of-Words Techniques We employ three approaches that use only bagof-words representations: pooling (aggregation) encoders, pre-trained word embeddings, and unigram generative regularization. These approaches do not model sequence-level interactions. We do not use contextualized encoders such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) because they incorporate sequence-level and positional representations. 2.1 Pooling Encoders We first consider a variant of the deep averaging network (DAN) encoder (Iyyer et al., 2015). The DAN encoder is a syntactically-oblivious encoder that consists of three steps: average (mean-pool) a sequence’s non-contextual word embeddings, pass the average through feed-forward layers, and then perform linear classification on the final layer’s ∗ Work done while at Johns Hopkins University. Our code is available at https://github.com/ felicitywang/tfmtl. 1 40 Proceeding"
D19-6105,S16-1002,0,0.0517973,"Missing"
D19-6105,P17-1194,0,0.0353764,"Missing"
D19-6105,P18-1096,0,0.0440076,"Missing"
D19-6105,P16-2038,0,0.0647292,"Missing"
drexler-etal-2014-wikipedia,W00-0507,0,\N,Missing
drexler-etal-2014-wikipedia,P02-1040,0,\N,Missing
drexler-etal-2014-wikipedia,P05-1033,0,\N,Missing
drexler-etal-2014-wikipedia,P12-2023,0,\N,Missing
drexler-etal-2014-wikipedia,2005.mtsummit-papers.11,0,\N,Missing
drexler-etal-2014-wikipedia,J07-2003,0,\N,Missing
drexler-etal-2014-wikipedia,W11-2123,0,\N,Missing
drexler-etal-2014-wikipedia,P03-1021,0,\N,Missing
drexler-etal-2014-wikipedia,W13-2226,1,\N,Missing
E09-1092,W97-0209,0,0.0865147,"ierarchy (Fellbaum, 1998) continues to be the resource of choice for many computational linguists requiring an ontology-like structure. In the work discussed here we explore the potential of WordNet as an underlying concept hierarchy on which to base generalization decisions. The use of WordNet raises the challenge of dealing with multiple semantic concepts associated with the same word, i.e., employing WordNet requires word sense disambiguation in order to associate terms observed in text with concepts (synsets) within the hierarchy. In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al. (2007).2 Others within the knowledge acquisition community have favored taking the first, most dominant sense of each word (e.g., see Suchanek et al. (2007) and Pas¸ca (2008)). As will be seen, our algorithm does not select word senses prior to generalizing them, but rather as a byproduct of the abstraction process. Moreover, it potentially selects multiple senses of a word deemed equally appropriate in a given context, and in that sen"
E09-1092,E95-1016,0,0.15787,"Missing"
E09-1092,A00-2018,0,0.00905794,"UAL) HAVE[V] (:Q DET ROOM[N])) (:I (:Q DET FEMALE-INDIVIDUAL) SLEEP[V]) (:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V] (:Q DET (:F PLUR CLOTHE[N]))) (:I (:Q DET (:F PLUR CLOTHE[N])) WASHED[A])) Here the upper-case sentences are automatically generated verbalizations of the abstracted LFs shown beneath them.1 The initial development of K NEXT was based on the hand-constructed parse trees in the Penn Treebank version of the Brown corpus, but subsequently Schubert and collaborators refined and extended the system to work with parse trees obtained with statistical parsers (e.g., that of Collins (1997) or Charniak (2000)) applied to larger corpora, such as the British National Corpus (BNC), a 100 million-word, mixed genre collection, along with Web corpora of comparable size (see work of Van Durme et al. (2008) and Van Durme and Schubert (2008) for details). The BNC yielded over 2 3.2 Propositional Templates While the procedure given here is not tied to a particular formalism in representing semantic con1 Keywords like :i, :q, and :f are used to indicate infix predication, unscoped quantification, and function application, but these details need not concern us here. 2 809 Personal communication function S COR"
E09-1092,J02-2003,0,0.126698,"Missing"
E09-1092,P97-1003,0,0.0353813,"DET FEMALE-INDIVIDUAL) HAVE[V] (:Q DET ROOM[N])) (:I (:Q DET FEMALE-INDIVIDUAL) SLEEP[V]) (:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V] (:Q DET (:F PLUR CLOTHE[N]))) (:I (:Q DET (:F PLUR CLOTHE[N])) WASHED[A])) Here the upper-case sentences are automatically generated verbalizations of the abstracted LFs shown beneath them.1 The initial development of K NEXT was based on the hand-constructed parse trees in the Penn Treebank version of the Brown corpus, but subsequently Schubert and collaborators refined and extended the system to work with parse trees obtained with statistical parsers (e.g., that of Collins (1997) or Charniak (2000)) applied to larger corpora, such as the British National Corpus (BNC), a 100 million-word, mixed genre collection, along with Web corpora of comparable size (see work of Van Durme et al. (2008) and Van Durme and Schubert (2008) for details). The BNC yielded over 2 3.2 Propositional Templates While the procedure given here is not tied to a particular formalism in representing semantic con1 Keywords like :i, :q, and :f are used to indicate infix predication, unscoped quantification, and function application, but these details need not concern us here. 2 809 Personal communica"
E09-1092,W03-0902,1,0.522822,"ntial source of such knowledge. However, current natural language understanding (NLU) methods are not general and reliable enough to enable broad assimilation, in a formalized representation, of explicitly stated knowledge in encyclopedias or similar sources. As well, such sources typically do not cover the most obvious facts of the world, such as that ice cream may be delicious and may be coated with chocolate, or that children may play in parks. Methods currently exist for extracting simple “factoids” like those about ice cream and children just mentioned (see in particular (Schubert, 2002; Schubert and Tong, 2003)), but these are quite weak as general claims, and – being unconditional Proceedings of the 12th Conference of the European Chapter of the ACL, pages 808–816, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 808 corpus statistics, concluding that future endeavors exploring knowledge extraction and WordNet should go beyond the heuristics employed in recent work. 2 factoids per sentence on average, resulting in a total collection of several million. Human judging of the factoids indicates that about 2 out of 3 factoids are perceived as reasonable claims."
E09-1092,P08-2047,0,0.0438215,"Missing"
E09-1092,J98-2002,0,0.652414,", 1998) continues to be the resource of choice for many computational linguists requiring an ontology-like structure. In the work discussed here we explore the potential of WordNet as an underlying concept hierarchy on which to base generalization decisions. The use of WordNet raises the challenge of dealing with multiple semantic concepts associated with the same word, i.e., employing WordNet requires word sense disambiguation in order to associate terms observed in text with concepts (synsets) within the hierarchy. In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al. (2007).2 Others within the knowledge acquisition community have favored taking the first, most dominant sense of each word (e.g., see Suchanek et al. (2007) and Pas¸ca (2008)). As will be seen, our algorithm does not select word senses prior to generalizing them, but rather as a byproduct of the abstraction process. Moreover, it potentially selects multiple senses of a word deemed equally appropriate in a given context, and in that sense provides coarse-gra"
E09-1092,C02-1105,0,0.250617,"Missing"
E09-1092,W08-2212,0,0.0411382,"Missing"
E09-1092,W08-2219,1,0.845166,"Missing"
E09-1092,W04-0837,0,0.011971,"ding situations in the world; they simply tell us what is generally possible and worth mentioning. 5 5.1 Experiments Tuning to WordNet Our method as described thus far is not tied to a particular word sense taxonomy. Experiments reported here relied on the following model adjustments in order to make use of WordNet (version 3.0). The function P was set to return the union of a synset’s hypernym and instance hypernym relations. Regarding the function L , WordNet is constructed such that always picking the first sense of a given nominal tends to be correct more often than not (see discussion by McCarthy et al. (2004)). To exploit this structural bias, we employed a modified version of L that results in a preference for nodes corresponding to the first sense of words to be covered, especially when the number of distinct observations were low (such as earlier, with crash helmet): Non-reliance on Frequency As can be observed, our approach makes no use of the relative or absolute frequencies of the words in W , even though such frequencies could be added as, e.g., relative weights on length in S CORE. This is a purposeful decision motivated both by practical and theoretical concerns. Practically, a large port"
E09-1092,C08-1116,1,0.894287,"Missing"
E09-1092,P06-1014,0,0.0128233,"ife or mistress or girlfriend) in the life of a particular man; ”he was faithful to his woman” 3 WOMAN : a human female employed to do housework; ”the char will clean the carpet”; ”I have a woman who comes in four hours a day while I write” *4 WOMAN : women as a class; ”it’s an insult to American womanhood”; ”woman is the glory of creation”; ”the fair sex gathered on the veranda” Figure 2: Example of a context and senses provided for 7 Allowing for multiple fine-grained senses to be judged as appropriate in a given context goes back at least to Sussna (1993); discussed more recently by, e.g., Navigli (2006). evaluation, with the fourth sense being judged as inappropriate. 813 If something is famous, it is probably a person1 , an artifact1 , or a communication2 If ? writes something, it is probably a communication2 If a person is happy with something, it is probably a communication2 , a work1 , a final result1 , or a state of affairs1 If a fish has something, it is probably a cognition1 , a torso1 , an interior2 , or a state2 If something is fast growing, it is probably a group1 or a business3 If a message undergoes something, it is probably a message2 , a transmission2 , a happening1 , or a crea"
E09-1092,N07-1071,0,\N,Missing
E09-1092,W08-2208,0,\N,Missing
E17-2011,W08-0336,0,0.0671522,"Missing"
E17-2011,P16-1004,0,0.0643804,"ve the cross-lingual IE task with a joint approach. Further, we focus on Open IE, which allows for an open set of semantic relations between a predicate and its arguments. Open IE in the monolingual setting has shown to be useful in a wide range of tasks, such as question answering (Fader et al., 2014), ontology learning (Suchanek, 2014), and summarization (ChrisInspired by the recent success of neural models in machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014), syntactic parsing (Vinyals et al., 2015; Choe and Charniak, 2016), and semantic parsing (Dong and Lapata, 2016), we propose a sequence-to-sequence model that enables end-toend cross-lingual Open IE. Essentially, we recast the problem as structured translation: the model encodes natural-language sentences and decodes predicate-argument forms (Figure 1). We show that the joint approach outperforms the pipeline on various metrics, and that the neural model is critical for the joint approach because of its capability in generating complex open IE patterns. 64 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 64–70, c V"
E17-2011,D11-1142,0,0.761024,"preferred by the user (e.g. English tuples). Conventional pipeline solutions decompose the task as machine translation followed by information extraction (or vice versa). We propose a joint solution with a neural sequence model, and show that it outperforms the pipeline in a cross-lingual open information extraction setting by 1-4 BLEU and 0.5-0.8 F1 . 1 ARG ARG Chris wants ARG Chris build ARG a boat ARG (b) Figure 1: Example of input (a) and output (b) of cross-lingual Open IE. tensen et al., 2013). A variety of work has achieved compelling results at monolingual Open IE (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015). But we are not aware of efforts that focus on both the cross-lingual and open aspects of cross-lingual Open IE, despite significant work in related areas, such as cross-lingual IE on a closed, pre-defined set of events/entities (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji et al., 2016), or bootstrapping of monolingual Open IE systems in multiple languages (Faruqui and Kumar, 2015; Kozhevnikov and Titov, 2013; van der Plas et al., 2014). Introduction Suppose an English-speaking user is faced with the daunting task of distilling facts from a"
E17-2011,P15-1034,0,0.286145,"r (e.g. English tuples). Conventional pipeline solutions decompose the task as machine translation followed by information extraction (or vice versa). We propose a joint solution with a neural sequence model, and show that it outperforms the pipeline in a cross-lingual open information extraction setting by 1-4 BLEU and 0.5-0.8 F1 . 1 ARG ARG Chris wants ARG Chris build ARG a boat ARG (b) Figure 1: Example of input (a) and output (b) of cross-lingual Open IE. tensen et al., 2013). A variety of work has achieved compelling results at monolingual Open IE (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015). But we are not aware of efforts that focus on both the cross-lingual and open aspects of cross-lingual Open IE, despite significant work in related areas, such as cross-lingual IE on a closed, pre-defined set of events/entities (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji et al., 2016), or bootstrapping of monolingual Open IE systems in multiple languages (Faruqui and Kumar, 2015; Kozhevnikov and Titov, 2013; van der Plas et al., 2014). Introduction Suppose an English-speaking user is faced with the daunting task of distilling facts from a collection of Chinese"
E17-2011,N15-1151,0,0.0368238,"f input (a) and output (b) of cross-lingual Open IE. tensen et al., 2013). A variety of work has achieved compelling results at monolingual Open IE (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015). But we are not aware of efforts that focus on both the cross-lingual and open aspects of cross-lingual Open IE, despite significant work in related areas, such as cross-lingual IE on a closed, pre-defined set of events/entities (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji et al., 2016), or bootstrapping of monolingual Open IE systems in multiple languages (Faruqui and Kumar, 2015; Kozhevnikov and Titov, 2013; van der Plas et al., 2014). Introduction Suppose an English-speaking user is faced with the daunting task of distilling facts from a collection of Chinese documents. One solution is to first translate the Chinese documents into English using a Machine Translation (MT) service, then extract the facts using an English-based Information Extraction (IE) engine. Unfortunately, imperfect translations negatively impact the IE engine, which may have been trained to expect natural English input (Sudo et al., 2004). Another approach is to first run a Chinese-based IE engin"
E17-2011,P02-1040,0,0.0980418,"03 and LDC2006G05 66 Moses (Koehn et al., 2007), directly on the same data we used to train Joint-Seq2Seq, i.e. pairs of Chinese sentences and English linearized PredPatt. We call this system Joint-Moses. We also train a Pipeline system which consists of a Moses system that translates Chinese sentence to English sentence, followed by SyntaxNet Parser (Andor et al., 2016) for Universal Dependency parsing on English, and PredPatt for predicate-argument identification. Results: We regard the generation of linearized PredPatt or linearized predicates4 as a translation problem, and use BLEU score (Papineni et al., 2002) for evaluation. As shown in Table 1, Joint Seq2Seq achieves the best BLEU scores, with an improvement 1.7 BLEU for linearized PredPatt and improvement of 4.3 BLEU for linearized predicates compared to Pipeline. PredPatt Predicates Pipeline 17.19 17.24 Joint Moses Joint Seq2Seq 18.34 18.94 16.43 21.55 k=150 k=1252 k=9535 Pipeline 32.95 28.73 27.20 Joint Moses Joint Seq2Seq 32.56 33.67 27.94 29.21 25.43 28.03 Table 2: Evaluation results (weighted F1 ) of predicates at different cluster granularities. outputs are empty. In contrast, Joint Seq2Seq generates no empty output and very few malformed"
E17-2011,P15-1001,0,0.0471709,"ized PredPatt. P (yi |y<i , A) = g(yi , sL i , ci ) minimize − Experiments Hyperparameters: Our proposed model (JointSeq2Seq) is trained using the Adam optimiser (Kingma and Ba, 2014), with mini-batch size 64 and step size 200. Both encoder and decoder have 2 layers and hidden state size 512, but different LSTM parameters sampled from U(0.05,0.05). Vocabulary size is 40K for both sides. Dropout (rate=0.5) is applied to non-recurrent connections (Srivastava et al., 2014). Gradients are clipped when their norm is bigger than 5 (Pascanu et al., 2013). We use sampled softmax to speed up training (Jean et al., 2015). Comparisons: As an alternative, we train a phrase-based machine translation system, log P (yi |y<i , A) (3) where D is the batch of training pairs, and P (yi | y<i , A) is computed by Eq.(3). Inference: We use greedy search to decode tokens one by one: yˆi = arg maxyi ∈VB P (yi |yˆ<i , A) 2 The code is available at https://github.com/ sheng-z/cross-lingual-open-ie. 3 The data comes from the GALE project; the largest bitexts are LDC2007E103 and LDC2006G05 66 Moses (Koehn et al., 2007), directly on the same data we used to train Joint-Seq2Seq, i.e. pairs of Chinese sentences and English linear"
E17-2011,P09-1048,0,0.201833,"Missing"
E17-2011,W09-1704,0,0.065801,"action setting by 1-4 BLEU and 0.5-0.8 F1 . 1 ARG ARG Chris wants ARG Chris build ARG a boat ARG (b) Figure 1: Example of input (a) and output (b) of cross-lingual Open IE. tensen et al., 2013). A variety of work has achieved compelling results at monolingual Open IE (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015). But we are not aware of efforts that focus on both the cross-lingual and open aspects of cross-lingual Open IE, despite significant work in related areas, such as cross-lingual IE on a closed, pre-defined set of events/entities (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji et al., 2016), or bootstrapping of monolingual Open IE systems in multiple languages (Faruqui and Kumar, 2015; Kozhevnikov and Titov, 2013; van der Plas et al., 2014). Introduction Suppose an English-speaking user is faced with the daunting task of distilling facts from a collection of Chinese documents. One solution is to first translate the Chinese documents into English using a Machine Translation (MT) service, then extract the facts using an English-based Information Extraction (IE) engine. Unfortunately, imperfect translations negatively impact the IE engine, whi"
E17-2011,D13-1176,0,0.0267203,"sources in the source language. Such problems with pipeline systems compound when the IE engine relies on parsers or other analytics as features. We propose to solve the cross-lingual IE task with a joint approach. Further, we focus on Open IE, which allows for an open set of semantic relations between a predicate and its arguments. Open IE in the monolingual setting has shown to be useful in a wide range of tasks, such as question answering (Fader et al., 2014), ontology learning (Suchanek, 2014), and summarization (ChrisInspired by the recent success of neural models in machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014), syntactic parsing (Vinyals et al., 2015; Choe and Charniak, 2016), and semantic parsing (Dong and Lapata, 2016), we propose a sequence-to-sequence model that enables end-toend cross-lingual Open IE. Essentially, we recast the problem as structured translation: the model encodes natural-language sentences and decodes predicate-argument forms (Figure 1). We show that the joint approach outperforms the pipeline on various metrics, and that the neural model is critical for the joint approach because of its capability in generating complex open IE pattern"
E17-2011,N15-1058,1,0.819327,"Missing"
E17-2011,P07-2045,0,0.00590556,"clipped when their norm is bigger than 5 (Pascanu et al., 2013). We use sampled softmax to speed up training (Jean et al., 2015). Comparisons: As an alternative, we train a phrase-based machine translation system, log P (yi |y<i , A) (3) where D is the batch of training pairs, and P (yi | y<i , A) is computed by Eq.(3). Inference: We use greedy search to decode tokens one by one: yˆi = arg maxyi ∈VB P (yi |yˆ<i , A) 2 The code is available at https://github.com/ sheng-z/cross-lingual-open-ie. 3 The data comes from the GALE project; the largest bitexts are LDC2007E103 and LDC2006G05 66 Moses (Koehn et al., 2007), directly on the same data we used to train Joint-Seq2Seq, i.e. pairs of Chinese sentences and English linearized PredPatt. We call this system Joint-Moses. We also train a Pipeline system which consists of a Moses system that translates Chinese sentence to English sentence, followed by SyntaxNet Parser (Andor et al., 2016) for Universal Dependency parsing on English, and PredPatt for predicate-argument identification. Results: We regard the generation of linearized PredPatt or linearized predicates4 as a translation problem, and use BLEU score (Papineni et al., 2002) for evaluation. As shown"
E17-2011,P16-1159,0,0.0444001,"Missing"
E17-2011,P13-1117,0,0.0582793,") of cross-lingual Open IE. tensen et al., 2013). A variety of work has achieved compelling results at monolingual Open IE (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015). But we are not aware of efforts that focus on both the cross-lingual and open aspects of cross-lingual Open IE, despite significant work in related areas, such as cross-lingual IE on a closed, pre-defined set of events/entities (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji et al., 2016), or bootstrapping of monolingual Open IE systems in multiple languages (Faruqui and Kumar, 2015; Kozhevnikov and Titov, 2013; van der Plas et al., 2014). Introduction Suppose an English-speaking user is faced with the daunting task of distilling facts from a collection of Chinese documents. One solution is to first translate the Chinese documents into English using a Machine Translation (MT) service, then extract the facts using an English-based Information Extraction (IE) engine. Unfortunately, imperfect translations negatively impact the IE engine, which may have been trained to expect natural English input (Sudo et al., 2004). Another approach is to first run a Chinese-based IE engine and then translate the resu"
E17-2011,W15-0807,0,0.135885,"q: collection of the specific funds) [(the special funds) related to (people ’s lives)]] Table 4: Example output. Arguments are shown in blue, and predicates shown in purple. Head tokens are underlined in bold. Token labels are omitted. 4 In linearized predicates, arguments are replaced by placeholders. For example, the linearized PredPatt in Fig 2 becomes “[ ?arg wants:ph Sth:= [ ?arg build:ph ?arg ] ]” after replacement. 5 Weighted F1 is the weighted average of individual F1 for each predicate, with weights proportional to predicate frequencies in the test data. We use token-level F1 score (Liu et al., 2015) which gives partial credits to partial matches. 6 Downloaded from: https://github.com/se4u/ mvlsa. 5 Conclusions We focus on the problem of cross-lingual open IE, and propose a joint solution based on a neu67 ral sequence-to-sequence model. Our joint approach outperforms the pipeline solution by 1-4 BLEU and 0.5-0.8 F1 . Future work includes minimum risk training (Shen et al., 2016) for directly optimizing the cross-lingual open IE metrics of interest. Furthermore, as PredPatt works on any language that has UD parsers available, we plan to evaluate cross-lingual Open IE on other target langua"
E17-2011,W11-1215,0,0.127292,"ting by 1-4 BLEU and 0.5-0.8 F1 . 1 ARG ARG Chris wants ARG Chris build ARG a boat ARG (b) Figure 1: Example of input (a) and output (b) of cross-lingual Open IE. tensen et al., 2013). A variety of work has achieved compelling results at monolingual Open IE (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015). But we are not aware of efforts that focus on both the cross-lingual and open aspects of cross-lingual Open IE, despite significant work in related areas, such as cross-lingual IE on a closed, pre-defined set of events/entities (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji et al., 2016), or bootstrapping of monolingual Open IE systems in multiple languages (Faruqui and Kumar, 2015; Kozhevnikov and Titov, 2013; van der Plas et al., 2014). Introduction Suppose an English-speaking user is faced with the daunting task of distilling facts from a collection of Chinese documents. One solution is to first translate the Chinese documents into English using a Machine Translation (MT) service, then extract the facts using an English-based Information Extraction (IE) engine. Unfortunately, imperfect translations negatively impact the IE engine, which may have been trai"
E17-2011,D15-1166,0,0.202929,"Missing"
E17-2011,C04-1127,0,0.754175,"in a cross-lingual open information extraction setting by 1-4 BLEU and 0.5-0.8 F1 . 1 ARG ARG Chris wants ARG Chris build ARG a boat ARG (b) Figure 1: Example of input (a) and output (b) of cross-lingual Open IE. tensen et al., 2013). A variety of work has achieved compelling results at monolingual Open IE (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015). But we are not aware of efforts that focus on both the cross-lingual and open aspects of cross-lingual Open IE, despite significant work in related areas, such as cross-lingual IE on a closed, pre-defined set of events/entities (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji et al., 2016), or bootstrapping of monolingual Open IE systems in multiple languages (Faruqui and Kumar, 2015; Kozhevnikov and Titov, 2013; van der Plas et al., 2014). Introduction Suppose an English-speaking user is faced with the daunting task of distilling facts from a collection of Chinese documents. One solution is to first translate the Chinese documents into English using a Machine Translation (MT) service, then extract the facts using an English-based Information Extraction (IE) engine. Unfortunately, imperfect translations negat"
E17-2011,C14-1121,0,0.0987837,"Missing"
E17-2011,D16-1177,1,0.789187,"Missing"
E17-2011,N16-1035,0,0.0149336,"es an L-layer bidirectional RNN (Schuster and Paliwal, 1997) which consists of a forward RNN reading inputs from x1 to x|A |and a backward RNN reading inputs in → − reverse from x|A |to x1 . Let hli ∈ Rn denote Our goal is to learn a model which directly maps a sentence input A in the source language into predicate-argument structures output B in the target language. Formally, we regard the input as a sequence A = x1 , · · · , x|A |, and use a linearized representation of the predicate-argument structure as the output sequence B = y1 , · · · , y|B |. While tree-based decoders are conceivable (Zhang et al., 2016), linearization of structured outputs to sequences simplifies decoding and has been shown 1 |B| Y https://github.com/hltcoe/PredPatt 65 4 the forward hidden state at time step i and layer l; it is computed by states at the previous time→ − → − −−→ −−→ step and at a lower layer: hli = f (hli−1 , hl−1 i ) → − where f is a nonlinear LSTM unit (Hochreiter − → and Schmidhuber, 1997). The lowest layer h0i is the word embedding of the token xi . The back← − ward hidden state hli is computed similarly using another LSTM, and the representation of each token xi is the concatenation of the top-layers: |"
E17-2011,D16-1257,0,\N,Missing
E17-2011,P16-1231,0,\N,Missing
E17-2015,D13-1149,0,0.0307558,"e cumulative link logit portion of the model, a categorical probability mass function with support on the property likelihood ratings l ∈ {1, . . . , 5} is determined by a latent µ and a nondecreasing real-valued cutpoint vector κ. zvcvj1 Figure 2: Linking model factor graph for token j of predicate v with three arguments. ( 1 − qj−1 P(l = j |µ, κ) = qj − qj−1 tions of Dowty’s proto-role properties by gathering answers to simple questions about how likely, on a five-point scale, it is that particular relational properties hold of arguments in PropBank (cf. Kako, 2006; Greene and Resnik, 2009; Hartshorne et al., 2013). We use these annotations, known as SPR1 (White et al., 2016), to train our semantic proto-role linking model (SPROLIM).1 3 where qj ≡ logit−1 (κj+1 − µ) and q0 ≡ 0. In Algorithm 1, we denote the parameters of this distribution as Ordκ (µ). Mapping model The mapping model has two components: (i) the canonicalizer, which maps from argument tokens to predicate-specific roles, and (ii) the linking model, which maps from predicate-specific roles to syntactic positions. We implement the canonicalizer by assuming that, for each predicate (verb) v, there is some canonical ordering of its predicate-s"
E17-2015,W04-2412,0,0.0829349,"see Fillmore 1970; Zwicky 1971; Jackendoff 1972; Carter 1976; Pinker 1989; Grimshaw 1990; Levin 1993). (1) a. b. Benjamin Van Durme Computer Science Johns Hopkins University vandurme@cs.jhu.edu [John]HITTER hit [the fence]HITTEE . [The stick]INST hit [the fence]HITTEE . A semantic role labeling (SRL) system implements the inverse of a linking theory: where a linking theory maps a predicate’s observed semantic arguments to its latent syntactic arguments, an SRL system maps a predicate’s observed syntactic arguments to its latent semantic arguments (see Gildea and Jurafsky 2002; Litkowski 2004; Carreras and Marquez 2004; Marquez et al. 2008). SRL is generally treated as a supervised task— requiring semantic role annotation, which is expensive, time-consuming, and hard to scale. This has led to the development of unsupervised systems for semantic role induction (SRI), which induce predicate-specific roles—cf. PropBank roles (Palmer et al., 2005)—from syntactic and lexical features of a predicate and its arguments. One approach to SRI that has proven fruitful is to explicitly implement linking as a compo2 Related work Prior work in SRI has tended to focus on using syntactic and lexical features to cluster argu"
E17-2015,N10-1137,0,0.124924,"AGENT or PRO TOPATIENT depending on its properties. Reisinger et al. (2015) crowd-sourced annotainto semantic roles. Swier and Stevenson (2004) introduce the first such system, which uses a bootstrapping procedure to first associate verb tokens with frames containing typed slots (drawn from VerbNet), then iteratively compute probabilities based on cooccurrence counts and fill unfilled slots based on these probabilities. Grenager and Manning (2006) introduce the idea of generating syntactic position based on a latent semantic role representation learned from syntactic and selectional features. Lang and Lapata (2010) expand on Grenager and Manning (2006) by introducing the notion of a canonicalized linking. The idea behind canonicalization is to account for the fact that the syntactic argument that a particular semantic argument is mapped to can change depending on the syntax. For instance, when hit is passivized, the HITTEE argument is mapped to subject position, where it would normally be mapped to object position. (2) [The fence]HITTEE was hit. We incorporate both ideas into our Semantic Proto-Role Linking Model (SPROLIM). SRI approaches that do not explicitly incorporate the idea of a linking theory h"
E17-2015,P11-1112,0,0.0418417,"Missing"
E17-2015,J02-3001,0,0.124316,", direct object, or prepositional object (see Fillmore 1970; Zwicky 1971; Jackendoff 1972; Carter 1976; Pinker 1989; Grimshaw 1990; Levin 1993). (1) a. b. Benjamin Van Durme Computer Science Johns Hopkins University vandurme@cs.jhu.edu [John]HITTER hit [the fence]HITTEE . [The stick]INST hit [the fence]HITTEE . A semantic role labeling (SRL) system implements the inverse of a linking theory: where a linking theory maps a predicate’s observed semantic arguments to its latent syntactic arguments, an SRL system maps a predicate’s observed syntactic arguments to its latent semantic arguments (see Gildea and Jurafsky 2002; Litkowski 2004; Carreras and Marquez 2004; Marquez et al. 2008). SRL is generally treated as a supervised task— requiring semantic role annotation, which is expensive, time-consuming, and hard to scale. This has led to the development of unsupervised systems for semantic role induction (SRI), which induce predicate-specific roles—cf. PropBank roles (Palmer et al., 2005)—from syntactic and lexical features of a predicate and its arguments. One approach to SRI that has proven fruitful is to explicitly implement linking as a compo2 Related work Prior work in SRI has tended to focus on using syn"
E17-2015,N09-1057,0,0.0267185,"l |µ, κ)a svj2 svj1 In the cumulative link logit portion of the model, a categorical probability mass function with support on the property likelihood ratings l ∈ {1, . . . , 5} is determined by a latent µ and a nondecreasing real-valued cutpoint vector κ. zvcvj1 Figure 2: Linking model factor graph for token j of predicate v with three arguments. ( 1 − qj−1 P(l = j |µ, κ) = qj − qj−1 tions of Dowty’s proto-role properties by gathering answers to simple questions about how likely, on a five-point scale, it is that particular relational properties hold of arguments in PropBank (cf. Kako, 2006; Greene and Resnik, 2009; Hartshorne et al., 2013). We use these annotations, known as SPR1 (White et al., 2016), to train our semantic proto-role linking model (SPROLIM).1 3 where qj ≡ logit−1 (κj+1 − µ) and q0 ≡ 0. In Algorithm 1, we denote the parameters of this distribution as Ordκ (µ). Mapping model The mapping model has two components: (i) the canonicalizer, which maps from argument tokens to predicate-specific roles, and (ii) the linking model, which maps from predicate-specific roles to syntactic positions. We implement the canonicalizer by assuming that, for each predicate (verb) v, there is some canonical o"
E17-2015,J14-3006,0,0.0341581,"Missing"
E17-2015,W06-1601,0,0.0259581,"O TOPATIENT are known as proto-roles is that they amount to role prototypes (Rosch and Mervis, 1975): a particular predicate-specific role can be closer or further from a PROTOAGENT or PRO TOPATIENT depending on its properties. Reisinger et al. (2015) crowd-sourced annotainto semantic roles. Swier and Stevenson (2004) introduce the first such system, which uses a bootstrapping procedure to first associate verb tokens with frames containing typed slots (drawn from VerbNet), then iteratively compute probabilities based on cooccurrence counts and fill unfilled slots based on these probabilities. Grenager and Manning (2006) introduce the idea of generating syntactic position based on a latent semantic role representation learned from syntactic and selectional features. Lang and Lapata (2010) expand on Grenager and Manning (2006) by introducing the notion of a canonicalized linking. The idea behind canonicalization is to account for the fact that the syntactic argument that a particular semantic argument is mapped to can change depending on the syntax. For instance, when hit is passivized, the HITTEE argument is mapped to subject position, where it would normally be mapped to object position. (2) [The fence]HITTE"
E17-2015,W04-0803,0,0.0172053,"itional object (see Fillmore 1970; Zwicky 1971; Jackendoff 1972; Carter 1976; Pinker 1989; Grimshaw 1990; Levin 1993). (1) a. b. Benjamin Van Durme Computer Science Johns Hopkins University vandurme@cs.jhu.edu [John]HITTER hit [the fence]HITTEE . [The stick]INST hit [the fence]HITTEE . A semantic role labeling (SRL) system implements the inverse of a linking theory: where a linking theory maps a predicate’s observed semantic arguments to its latent syntactic arguments, an SRL system maps a predicate’s observed syntactic arguments to its latent semantic arguments (see Gildea and Jurafsky 2002; Litkowski 2004; Carreras and Marquez 2004; Marquez et al. 2008). SRL is generally treated as a supervised task— requiring semantic role annotation, which is expensive, time-consuming, and hard to scale. This has led to the development of unsupervised systems for semantic role induction (SRI), which induce predicate-specific roles—cf. PropBank roles (Palmer et al., 2005)—from syntactic and lexical features of a predicate and its arguments. One approach to SRI that has proven fruitful is to explicitly implement linking as a compo2 Related work Prior work in SRI has tended to focus on using syntactic and lexic"
E17-2015,J08-2001,0,0.035448,"Missing"
E17-2015,J05-1004,0,0.23514,"linking theory: where a linking theory maps a predicate’s observed semantic arguments to its latent syntactic arguments, an SRL system maps a predicate’s observed syntactic arguments to its latent semantic arguments (see Gildea and Jurafsky 2002; Litkowski 2004; Carreras and Marquez 2004; Marquez et al. 2008). SRL is generally treated as a supervised task— requiring semantic role annotation, which is expensive, time-consuming, and hard to scale. This has led to the development of unsupervised systems for semantic role induction (SRI), which induce predicate-specific roles—cf. PropBank roles (Palmer et al., 2005)—from syntactic and lexical features of a predicate and its arguments. One approach to SRI that has proven fruitful is to explicitly implement linking as a compo2 Related work Prior work in SRI has tended to focus on using syntactic and lexical features to cluster arguments 92 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 92–98, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics µ η ψ R κ l a Algorithm 1 Semantic Proto-Role Linking Model δ 1: for verb type v ∈ V do 2: fo"
E17-2015,P11-1145,0,0.0214845,"particular semantic argument is mapped to can change depending on the syntax. For instance, when hit is passivized, the HITTEE argument is mapped to subject position, where it would normally be mapped to object position. (2) [The fence]HITTEE was hit. We incorporate both ideas into our Semantic Proto-Role Linking Model (SPROLIM). SRI approaches that do not explicitly incorporate the idea of a linking theory have also been popular. Lang and Lapata (2011a, 2014) use graph clustering methods and Lang and Lapata (2011b) use a split-merge algorithm to cluster arguments based on syntactic context. Titov and Klementiev (2011) use a non-parametric clustering method based on the Pitman-Yor Process, and Titov and Klementiev (2012) propose nonparametric cluster93 zvcvj2 zvcvj3 cedural way of thinking about this is that, first, a rater decides whether a property is applicable; if it is not, they stop; if it is, they generate a rating. The joint probability of l and a is then defined as svj3 P(l, a |µ, η, κ) ∝ P(a |η)P(l |µ, κ)a svj2 svj1 In the cumulative link logit portion of the model, a categorical probability mass function with support on the property likelihood ratings l ∈ {1, . . . , 5} is determined by a latent"
E17-2015,P12-1068,0,0.0450194,"Missing"
E17-2015,D16-1177,1,0.868738,"Missing"
E17-2028,P15-5005,0,0.0361252,"Missing"
E17-2028,D14-1165,0,0.0106915,"o a baseline that simply embeds lemmas rather than words (equivalent to fixing rk = 1). 6 Related Work Tensor factorization has already found uses in a few corners of NLP research. Van de Cruys et al. (2013) applied tensor factorization to model the compositionality of subject-verb-object triples. Similarly, Hashimoto and Tsuruoka (2015) use an implicit tensor factorization method to learn embeddings for transitive verb phrases. Tensor factorization also appears in semantic-based NLP tasks. Lei et al. (2015) explicitly factorize a tensor based on feature vectors for predicting semantic roles. Chang et al. (2014) use tensor factorization to create knowledge base embeddings optimized for relation extraction. See Bouchard et al. (2015) for a large bibliography. Other researchers have likewise attempted to escape the bag-of-words assumption in word embeddings, e.g., Yatbaz et al. (2012) incorporates morphological and orthographic features into continuous vectors; Cotterell and Sch¨utze (2015) consider a multi-task set-up to force morphological information into embeddings; Cotterell and Sch¨utze (2017) jointly morphologically segment and embed words; Levy and Goldberg (2014a) derive contexts based on depe"
E17-2028,N15-1140,1,0.893374,"Missing"
E17-2028,P16-1156,1,0.0989234,"Missing"
E17-2028,W16-2506,0,0.00454851,"33 28.54 +7.21 de 353 S IM L 44.90 28.39 50.39 68.08 40.09 53.97 +23.18 +11.7 +3.58 RG -65 Z222 29.75 31.11 +1.36 RG -65 353 70.60 64.50 71.71 63.72 +1.11 -0.78 en MEN MTURK 64.33 58.77 66.66 62.64 +2.33 +3.87 S IM L 41.62 49.70 +8.08 S IM V 30.48 29.96 +0.52 RW 40.78 42.40 +1.62 Table 2: Word similarity results comparing the compositional morphology tensor with the standard skip-gram model. Numbers indicate Spearman’s correlation coefficient ρ between human similarity judgements and the cosine distances of vectors. For each language, we compare on several sets of human judgments as listed by Faruqui et al. (2016, Table 2). tribute signal to the embedding of run. We expect these lemma embeddings to be predictive of human judgments of lemma similarity. We evaluate using standard datasets on four languages (French, Italian, German and English). Given a list of pairs of words (always lemmata), multiple native speakers judged (on a scale of 1– 10) how “similar” those words are conceptually. Our model produces a similarity judgment for each pair using the cosine similarity of their lemma embeddings wj . Table 2 shows how well this learned judgment correlates with the average human judgment. Our model does"
E17-2028,N13-1092,1,0.120323,"Missing"
E17-2028,W15-4001,0,0.0184305,"oduces a similarity judgment for each pair using the cosine similarity of their lemma embeddings wj . Table 2 shows how well this learned judgment correlates with the average human judgment. Our model does achieve higher correlation than skip-gram word embeddings. Note we did not compare to a baseline that simply embeds lemmas rather than words (equivalent to fixing rk = 1). 6 Related Work Tensor factorization has already found uses in a few corners of NLP research. Van de Cruys et al. (2013) applied tensor factorization to model the compositionality of subject-verb-object triples. Similarly, Hashimoto and Tsuruoka (2015) use an implicit tensor factorization method to learn embeddings for transitive verb phrases. Tensor factorization also appears in semantic-based NLP tasks. Lei et al. (2015) explicitly factorize a tensor based on feature vectors for predicting semantic roles. Chang et al. (2014) use tensor factorization to create knowledge base embeddings optimized for relation extraction. See Bouchard et al. (2015) for a large bibliography. Other researchers have likewise attempted to escape the bag-of-words assumption in word embeddings, e.g., Yatbaz et al. (2012) incorporates morphological and orthographic"
E17-2028,C92-2082,0,0.0789371,"onsider a multi-task set-up to force morphological information into embeddings; Cotterell and Sch¨utze (2017) jointly morphologically segment and embed words; Levy and Goldberg (2014a) derive contexts based on dependency relations; PPDB (Ganitkevitch et al., 2013) employs a mixed bag of words, parts of speech, and syntax; Rastogi et al. (2015) represent word contexts, morphology, semantic frame relations, syntactic dependency relations, and multilingual bitext counts each as separate matrices, combined via GCCA; and, finally, Schwartz et al. (2016) derived embeddings based on Hearst patterns (Hearst, 1992). Ling et al. (2015) learn position-specific word embeddings (§4.1), but do not factor them as wj rk to share parameters (we did not compare empirically to this). As demonstrated in the experiments, our tensor factorization method enables us to include other syntactic properties besides word order, e.g. morphology. Poliak et al. (2017) also create positional word embeddings. Our research direction is orthogonal to these efforts in that we provide a general purpose procedure for all sorts of higher-order coocurrence. 7 Conclusion We have presented an interpretation of the skipgram model as expo"
E17-2028,N15-1121,0,0.0411841,"Missing"
E17-2028,P14-2050,0,0.0916527,"is passed through some “inverse link” function to obtain the expected feature values under the distribution, which in turn determines j = i XX j i exp (ci · wj ) Xij log P i0 exp (ci0 · wj ) (4) This is the log-likelihood (plus a constant) if we assume that for each word j, the context vector xj was drawn from a multinomial with natural parameter vector C > wj and count parameter P Nj = i Xij . This is the same model as in Figure 1a, but with a different conditional distribution for xj , and with xj taking an additional observed parent Nj (which is the token count of word j). 2.1 Related work Levy and Goldberg (2014b) also interpreted skipgram as matrix factorization. They argued that skipgram estimation by negative sampling implicitly factorizes a shifted matrix of positive empirical pointwise mutual information values. We instead regard the skip-gram objective itself as demanding EPCA-style factorization of the count matrix X: i.e., X arose stochastically from some unknown matrix of log-linear parameters (column j of X generated from parameter column j), and we seek a rank-d estimate C > W of that matrix. pLSI (Hofmann, 1999) similarly factors an unknown matrix of multinomial probabilities, which is mu"
E17-2028,Q15-1016,0,0.0144649,"l case—it is equal to (wj ; 1) (1; rk ), which uses twice as many dimensions to embed each object. 5 Experiments We build HOSG on top of the HYPERWORDS package. All models (both skip-gram and higher-order skip-gram) are trained for 10 epochs and use 5 negative samples. All models for §5.1 are trained on the Sept. 2016 dump of the full Wikipedia. All models for §5.2 were trained on the lemmatized and POS-tagged WaCky corpora (Baroni et al., 2009) for French, Italian, German and English (Joubarne and Inkpen, 2011; Leviant and Reichart, 2015). To ensure controlled and fair experiments, we follow Levy et al. (2015) for all preprocessing. 5.1 Experiment 1: Positional Tensor We postulate that the positional tensor should encode richer notions of syntax than standard bag6 If one wanted to extend the model to decompose the context words i as well, we see at least four approaches. 7 Cotterell et al. (2016) made two further moves that could be applied to extend the present paper. First, they allowed a word to consist of any number of (unordered) morphemes— not necessarily two—whose embeddings were combined (by summation) to get the word embedding. Second, this sum also included word-specific random noise, all"
E17-2028,N15-1142,0,0.0122328,"-task set-up to force morphological information into embeddings; Cotterell and Sch¨utze (2017) jointly morphologically segment and embed words; Levy and Goldberg (2014a) derive contexts based on dependency relations; PPDB (Ganitkevitch et al., 2013) employs a mixed bag of words, parts of speech, and syntax; Rastogi et al. (2015) represent word contexts, morphology, semantic frame relations, syntactic dependency relations, and multilingual bitext counts each as separate matrices, combined via GCCA; and, finally, Schwartz et al. (2016) derived embeddings based on Hearst patterns (Hearst, 1992). Ling et al. (2015) learn position-specific word embeddings (§4.1), but do not factor them as wj rk to share parameters (we did not compare empirically to this). As demonstrated in the experiments, our tensor factorization method enables us to include other syntactic properties besides word order, e.g. morphology. Poliak et al. (2017) also create positional word embeddings. Our research direction is orthogonal to these efforts in that we provide a general purpose procedure for all sorts of higher-order coocurrence. 7 Conclusion We have presented an interpretation of the skipgram model as exponential family princ"
E17-2028,D14-1162,0,0.0855584,"syntax) or morphological information (to share parameters across related words). We experiment on 40 languages and show that our model improves upon skip-gram. 1 2 Introduction Over the past years NLP has witnessed a veritable frenzy on the topic of word embeddings: lowdimensional representations of distributional information. The embeddings, trained on extremely large text corpora such as Wikipedia and Common Crawl, are claimed to encode semantic knowledge extracted from large text corpora. Numerous methods have been proposed—the most popular being skip-gram (Mikolov et al., 2013) and GloVe (Pennington et al., 2014)—for learning these low-dimensional embeddings from a bag of contexts associated with each word type. Natural language text, however, contains richer structure than simple context-word pairs. In this work, we embed n-tuples rather than pairs, allowing us to escape the bag-of-words assumption and encode richer linguistic structures. As a first step, we offer a novel interpretation of the skip-gram model (Mikolov et al., 2013). We show how skip-gram can be viewed as an application of exponential-family principal components analysis (EPCA) (Collins et al., 2001) to an integer matrix of coocurrenc"
E17-2028,petrov-etal-2012-universal,0,0.0290821,"Missing"
E17-2028,E17-2081,1,0.885781,"Missing"
E17-2028,N15-1058,1,0.772914,"Missing"
E17-2028,N16-1060,0,0.0202674,"Missing"
E17-2028,W16-2520,0,0.0416803,"Missing"
E17-2028,N13-1134,0,0.0390591,"Missing"
E17-2028,D12-1086,0,0.0694959,"Missing"
E17-2028,L16-1262,0,\N,Missing
E17-2081,E17-2028,1,0.887017,"Missing"
E17-2081,P14-5004,0,0.0468521,"n boldface. † denotes improvement to the PPDB2.0 baseline. MAJ refers to the majority choice. as discussed in section 3. The accuracies reported in Table 1 demonstrate that ECO captures semantics on n-grams better than the baseline approach. In all of the configurations, ECO outperforms Word2Vec for phrases that are longer than one word. 5.2 Word Embedding Similarity Although ECO’s primary goal is to create n-gram embeddings, it is important for our approach to not sacrifice quality in single word embeddings. Thus, we compare our word embeddings to seven word similarity benchmarks provided by Faruqui and Dyer (2014)’s online system. To evaluate how well ECO −1 and v1 for embeds unigrams, we concatenate vw w the 5629 words provided by Faruqui and Dyer (2014) and upload our ECO word embeddings to Faruqui and Dyer (2014)’s website4. We also upload the embeddings we generate by running Word2Vec as our baseline. The scores reported in Table 2 suggest that as the number of parameters increase, ECO better retains information for word embeddings than Word2Vec. Word2Vec 100 Acronym Size WS-353-SIM 203 WS-353-REL 252 MC-30 30 Rare-Word 2034 MEN 3000 YP-130 130 SimLex-999 999 ECO 700 100 700 2 5 2 5 2 5 2 5 0.685 0"
E17-2081,N15-1184,0,0.0815331,"Missing"
E17-2081,N13-1092,1,0.898101,"Missing"
E17-2081,Q16-1002,0,0.300788,"ent word meaning via a vector of real values (Deerwester et al., 1990). The Word2Vec models introduced by Mikolov et al. (2013a) greatly popularized this semantic representation method and since then improvements to the basic Word2Vec model have been proposed (Levy and Goldberg, 2014; Ling et al., 2015). Although techniques exist to sufficiently induce representations of single tokens (Mikolov et al., 2013a; Pennington et al., 2014), current methods for creating n-gram embeddings are far from satisfactory. Recent approaches cannot embed n-grams that do not appear during training. For example, Hill et al. (2016) used a heuristic of converting phrases to tokens before learning the embeddings. Additionally, Yin and Sch¨utze (2014) queried sources to determine which phrases to embed. We propose a new method for creating phrase embeddings on-the-fly. Offline, we compute decomposed word embeddings (Figure 1a) that can be used online to Efficiently generate Compositional n-gram embeddings that are sensitive to word Order (Figure 1b). We refer to our method as ECO. ECO is ∗ -1 2 denotes equal contribution. a novel way to incorporate knowledge about phrases into machine learning tasks. We evaluate our method"
E17-2081,N15-1144,0,0.0171038,"Spearman’s ρ. When using only the features from Pavlick et al. (2015), we report a score of 0.7025. Due to run time constraints, we only include Word2Vec and ECO embeddings where d = 100. With a window size of 2, ECO’s score is 0.729 and Word2Vec’s score is 0.622. When increasing the window size to 5, ECO scores 0.7156 and Word2Vec’s ρ is 0.569. Our results suggest that these features can be useful in improving the quality of existing PPDB resources. 6 Previous work Due to the popularity of word embeddings and the boost they have provided in supervised (Le and Mikolov, 2014) and unsupervised (Lin et al., 2015) NLP tasks, recent work has focused on how to properly embed sentences and phrases. Yin and Sch¨utze (2014)’s method is similar to the method discussed in Section 3. They use Wiktionary and WordNet to determine the most common bigrams and create embeddings for those. Hill et al. (2016) use reverse dictionaries to determine which phrases define single words and use neural language models to learn a mapping between the phrases and word vectors. Both of these approaches can not generate embeddings for phrases on the fly and require an external corpus. Recent work has also focused on capturing wor"
E17-2081,N15-1142,0,0.242589,"shark 2 2 (b) ECO Figure 1: (a): Skip-embeddings for each word by generalizing Word2Vec. The numbers refer to the position, relative to the given word, that the individual skip-embedding represents. (b): ECO’s efficient heuristic for composing n-gram embeddings. 1 Introduction Semantic embeddings of words represent word meaning via a vector of real values (Deerwester et al., 1990). The Word2Vec models introduced by Mikolov et al. (2013a) greatly popularized this semantic representation method and since then improvements to the basic Word2Vec model have been proposed (Levy and Goldberg, 2014; Ling et al., 2015). Although techniques exist to sufficiently induce representations of single tokens (Mikolov et al., 2013a; Pennington et al., 2014), current methods for creating n-gram embeddings are far from satisfactory. Recent approaches cannot embed n-grams that do not appear during training. For example, Hill et al. (2016) used a heuristic of converting phrases to tokens before learning the embeddings. Additionally, Yin and Sch¨utze (2014) queried sources to determine which phrases to embed. We propose a new method for creating phrase embeddings on-the-fly. Offline, we compute decomposed word embeddings"
E17-2081,P15-2070,1,0.888684,"Missing"
E17-2081,D14-1162,0,0.0882888,"elative to the given word, that the individual skip-embedding represents. (b): ECO’s efficient heuristic for composing n-gram embeddings. 1 Introduction Semantic embeddings of words represent word meaning via a vector of real values (Deerwester et al., 1990). The Word2Vec models introduced by Mikolov et al. (2013a) greatly popularized this semantic representation method and since then improvements to the basic Word2Vec model have been proposed (Levy and Goldberg, 2014; Ling et al., 2015). Although techniques exist to sufficiently induce representations of single tokens (Mikolov et al., 2013a; Pennington et al., 2014), current methods for creating n-gram embeddings are far from satisfactory. Recent approaches cannot embed n-grams that do not appear during training. For example, Hill et al. (2016) used a heuristic of converting phrases to tokens before learning the embeddings. Additionally, Yin and Sch¨utze (2014) queried sources to determine which phrases to embed. We propose a new method for creating phrase embeddings on-the-fly. Offline, we compute decomposed word embeddings (Figure 1a) that can be used online to Efficiently generate Compositional n-gram embeddings that are sensitive to word Order (Figur"
E17-2081,P14-3006,0,0.0509847,"Missing"
E17-2081,C16-1130,0,0.0544769,"ocused on how to properly embed sentences and phrases. Yin and Sch¨utze (2014)’s method is similar to the method discussed in Section 3. They use Wiktionary and WordNet to determine the most common bigrams and create embeddings for those. Hill et al. (2016) use reverse dictionaries to determine which phrases define single words and use neural language models to learn a mapping between the phrases and word vectors. Both of these approaches can not generate embeddings for phrases on the fly and require an external corpus. Recent work has also focused on capturing word order in embeddings. While Yuan et al. (2016) are not concerned with embedding phrases, they point out issues with concatenating or averaging standard word embeddings. They train an LSTM to appropriately incorporate word vectors in the Word Sense Disambiguation task. Their model is sensitive to word order when determining the sense of a specific word. Yuan et al. (2016)’s approach is more computationally intensive than ECO. Le and Mikolov (2014)’s Paragraph Vector framework also focus on capturing word order in their embeddings. However, our method is more efficient since ECO does not require training the n-gram embeddings. 506 Ling et a"
E17-2114,N10-1145,0,0.00804739,"elect the answer string. QA systems normally employ an information retrieval (IR) system to produce the initial set of candidates, usually treated as a black box, bag-of-words process that selects candidate passages best overlapping with the content in the question. Recent efforts in corpus-based QA have been focused heavily on reranking, or answer sentence selection: filtering the candidate set as a supervised classification task to single out those that answer the given question. Extensive research has explored employing syntactic/semantic features (Yih et al., 2013; Wang and Manning, 2010; Heilman and Smith, 2010; Yao et al., 2013a) and recently using neural networks (Yu et al., 2014; Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Yin et al., 2016). The shared aspect of all these approaches is that the quality of reranking a candidate set is upper-bounded by the initial set of candidates: unless one plans on reranking the entire corpus for each question as it arrives, one is still reliant on an initial IR stage in order to obtain a computationally feasible QA system. Huang et al. (2013) used neural networks and cosine distance to rank the candidates for IR, but without providing a method to searc"
E17-2114,P14-5010,0,0.00228915,"Missing"
E17-2114,C10-1131,0,0.00886759,"xtraction (IE) step to select the answer string. QA systems normally employ an information retrieval (IR) system to produce the initial set of candidates, usually treated as a black box, bag-of-words process that selects candidate passages best overlapping with the content in the question. Recent efforts in corpus-based QA have been focused heavily on reranking, or answer sentence selection: filtering the candidate set as a supervised classification task to single out those that answer the given question. Extensive research has explored employing syntactic/semantic features (Yih et al., 2013; Wang and Manning, 2010; Heilman and Smith, 2010; Yao et al., 2013a) and recently using neural networks (Yu et al., 2014; Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Yin et al., 2016). The shared aspect of all these approaches is that the quality of reranking a candidate set is upper-bounded by the initial set of candidates: unless one plans on reranking the entire corpus for each question as it arrives, one is still reliant on an initial IR stage in order to obtain a computationally feasible QA system. Huang et al. (2013) used neural networks and cosine distance to rank the candidates for IR, but without pr"
E17-2114,P15-2116,0,0.0241291,"Missing"
E17-2114,D15-1237,0,0.012204,"Missing"
E17-2114,N13-1106,1,0.0457661,"Missing"
E17-2114,P13-2029,1,0.840322,"Missing"
E17-2114,P13-1171,0,0.00357339,"y an information extraction (IE) step to select the answer string. QA systems normally employ an information retrieval (IR) system to produce the initial set of candidates, usually treated as a black box, bag-of-words process that selects candidate passages best overlapping with the content in the question. Recent efforts in corpus-based QA have been focused heavily on reranking, or answer sentence selection: filtering the candidate set as a supervised classification task to single out those that answer the given question. Extensive research has explored employing syntactic/semantic features (Yih et al., 2013; Wang and Manning, 2010; Heilman and Smith, 2010; Yao et al., 2013a) and recently using neural networks (Yu et al., 2014; Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Yin et al., 2016). The shared aspect of all these approaches is that the quality of reranking a candidate set is upper-bounded by the initial set of candidates: unless one plans on reranking the entire corpus for each question as it arrives, one is still reliant on an initial IR stage in order to obtain a computationally feasible QA system. Huang et al. (2013) used neural networks and cosine distance to rank the candidate"
E17-2114,Q16-1019,0,0.0054567,"Missing"
I17-1084,P16-1231,0,0.136093,"bels). To make a baseline comparison, we train an OpenNMT system (Klein et al., 2017) directly on the same data ignoring the labels. Comparisons Our approach (Selective Decoding) is compared against four other approaches: (1) Joint Seq2Seq, which trains a standard encoder-decoder model on the Chinese-English dataset described in Table 1; (2) Joint Moses, which trains a phrase-based machine translation system, Moses (Koehn et al., 2007), directly on the same data; (3) Pipeline-S which consists of a Moses system that translates Chinese sentence to English sentence, followed by SyntaxNet Parser (Andor et al., 2016) for Universal Dependency parsing on English, and PredPatt (White et al., 2016) for predicate-argument identification; and (4) Pipeline-N is the same as Pipeline-S except that the Moses system is replaced by OpenNMT (Klein et al., 2017), a neural machine translation system. 5.2.3 Linearized PredPatt Approach OpenNMT 24.92 Evaluation Metrics Selective Decoding 25.16 Table 3: Evaluation results (BLEU) of sequence generation on the test set. For evaluation, we directly convert the output by our approach (e.g. Fig. 1(e)) to the form of linearized PredPatt (e.g., Fig. 1(d)), and follow the same man"
I17-1084,P15-1034,0,0.0205754,"been used in neural machine translating (Tu et al., 2016) and image caption generation (Xu et al., 2015) to explicitly control the influence from source or target contexts. The experiments in § 5 also confirms our point: our model using the selective decoding mechanism significantly improves the performance, compared to the standard encoder-decoder model. Regarding to open IE systems for generating training data, PredPatt has shown promising performance on large-scale open IE benchmarks (Zhang et al., 2017c). Compared to other existing open IE systems (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015), PredPatt uses manual language-agnostic patterns on UD, which makes it a well-founded component across languages. Additionally, the underlying structure constructed by PredPatt has been shown to be a well-formed syntax-semantics interface (Zhang et al., 2017b). The beam is used to increase the search space for the sequence Y in the target language. At each decoding time step, we first greedily select the type of decoder, and then generate candidate tokens from the selected decoder to update the beam. When the special token heosi is generated, we remove the candidate sequence from the beam. 4"
I17-1084,D14-1179,0,0.0249957,"Missing"
I17-1084,P17-4012,0,0.0167177,"embeddings), and without beam search, only using greedy search during inference (-beam search). As shown in Table 2, while the pretrained word embeddings moderately improve the BLEU score of linearized PredPatt, they have slightly negative impact on linearized predicates. Beam search helps improve the BLEU score of both. Selective Decoding explicitly models sequence generation and sequence labeling, which enables a standalone evaluation of the sequence generation process (i.e., the final output without predicate and argument labels). To make a baseline comparison, we train an OpenNMT system (Klein et al., 2017) directly on the same data ignoring the labels. Comparisons Our approach (Selective Decoding) is compared against four other approaches: (1) Joint Seq2Seq, which trains a standard encoder-decoder model on the Chinese-English dataset described in Table 1; (2) Joint Moses, which trains a phrase-based machine translation system, Moses (Koehn et al., 2007), directly on the same data; (3) Pipeline-S which consists of a Moses system that translates Chinese sentence to English sentence, followed by SyntaxNet Parser (Andor et al., 2016) for Universal Dependency parsing on English, and PredPatt (White"
I17-1084,P07-2045,0,0.00408444,"models sequence generation and sequence labeling, which enables a standalone evaluation of the sequence generation process (i.e., the final output without predicate and argument labels). To make a baseline comparison, we train an OpenNMT system (Klein et al., 2017) directly on the same data ignoring the labels. Comparisons Our approach (Selective Decoding) is compared against four other approaches: (1) Joint Seq2Seq, which trains a standard encoder-decoder model on the Chinese-English dataset described in Table 1; (2) Joint Moses, which trains a phrase-based machine translation system, Moses (Koehn et al., 2007), directly on the same data; (3) Pipeline-S which consists of a Moses system that translates Chinese sentence to English sentence, followed by SyntaxNet Parser (Andor et al., 2016) for Universal Dependency parsing on English, and PredPatt (White et al., 2016) for predicate-argument identification; and (4) Pipeline-N is the same as Pipeline-S except that the Moses system is replaced by OpenNMT (Klein et al., 2017), a neural machine translation system. 5.2.3 Linearized PredPatt Approach OpenNMT 24.92 Evaluation Metrics Selective Decoding 25.16 Table 3: Evaluation results (BLEU) of sequence gener"
I17-1084,P16-1004,0,0.022314,"the beam. When the special token heosi is generated, we remove the candidate sequence from the beam. 4 Related Work The model we propose in this paper is adapted from the RNN encoder-decoder architectures which have been successfully applied to a wide range of NLP tasks such as machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014), image description generation (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015b), syntactic parsing (Vinyals et al., 2015a), question answering (Hermann et al., 2015), summarization (Rush et al., 2015), and semantic parsing (Dong and Lapata, 2016). As a novel variation of the encoder-decoder architecture, our model provides a general solution to tasks involving translation and labeling. crosslingual open IE is an example of this kind of task. The end-to-end solution proposed by Zhang et al. (2017a) used a vanilla attention-based encoderdecoder model to achieve results which outperform the traditional pipeline solutions. Compared to the vanilla encoder-decoder model, our model splits the joint task into two concurrent tasks (i.e., labeling and translating), which are jointly learnt by a selector and multiple decoders. This eases the bur"
I17-1084,W15-0807,0,0.0258909,"ty We compute the number of the linearized PredPatt outputs from which the tree structure representation can not be recovered, including the empty outputs and the outputs which have unmatched brackets, or have zero or multiple heads for an argument or a predicate. As shown in Table 5, compared to the previous best Joint Seq2Seq approach, Selective Decoding further reduces the number of unrecoverable outputs by one order of magnitude. Joint Moses 33,178 10 Figure 4: BLEU scores of the linearized PredPatt on the test set w.r.t. the lengths of the references. We compute the token-level F1 score (Liu et al., 2015) of predicates and arguments. As shown in Table 4, Selective Decoding substantially improves the F1 score of both predicates and arguments. In the ablation test, pretrained word embeddings slightly improve F1 of predicates, but have no improvement on F1 of arguments. Beam search helps improve the score of both. PipelineN 6,014 Selective Decoding Joint Seq2Seq 5 Table 4: Evaluation results (F1 ) of predicates and arguments on the test set. PipelineS 5,965 15 Input sentence and its English translation: 我 哪怕 有 千分之一 的 希望 呢 , 我 死 活 都 要 给 他 做 最后 的 (Even if there was only a one thousandth of a hope ,"
I17-1084,D11-1142,0,0.0508307,"ilar mechanisms have been used in neural machine translating (Tu et al., 2016) and image caption generation (Xu et al., 2015) to explicitly control the influence from source or target contexts. The experiments in § 5 also confirms our point: our model using the selective decoding mechanism significantly improves the performance, compared to the standard encoder-decoder model. Regarding to open IE systems for generating training data, PredPatt has shown promising performance on large-scale open IE benchmarks (Zhang et al., 2017c). Compared to other existing open IE systems (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015), PredPatt uses manual language-agnostic patterns on UD, which makes it a well-founded component across languages. Additionally, the underlying structure constructed by PredPatt has been shown to be a well-formed syntax-semantics interface (Zhang et al., 2017b). The beam is used to increase the search space for the sequence Y in the target language. At each decoding time step, we first greedily select the type of decoder, and then generate candidate tokens from the selected decoder to update the beam. When the special token heosi is generated, we remove the candidate sequ"
I17-1084,P16-1006,0,0.0356003,"Missing"
I17-1084,D15-1166,0,0.0827738,"Missing"
I17-1084,P15-1001,0,0.0268347,"are sampled from U(−0.1, 0.1). The dropout rate is set to 0.3. The word embedding size is 300 for input tokens on both the encoder side and the decoder side. We use open-source GloVe vectors (Pennington et al., 2014) trained on Common Crawl 840B with 300 dimensions6 to initialize the word embeddings on the decoder side. The mini-batch size is set to 64 and the step size set to 50. Gradients are clipped when their norms are greater than 5 (Pascanu et al., 2013). For simplicity, we use vanilla softmax over the decoder vocabulary as opposed to more efficient alternatives such as sampled softmax (Jean et al., 2015). The vocabulary size is set to 40,000. The number of epochs 6 https://nlp.stanford.edu/projects/ glove/ 836 5.2.4 Evaluation using BLEU Table 2 shows the cased BLEU scores of linearized PredPatt and linearized predicates7 on the test set. Selective Decoding significantly improves the performance on both of them. Compared to the previous best approach (Joint Seq2Seq), Selective Decoding improves the BLEU score of linearized PredPatt to 23.88, and the score of linearized predicates to 25.42. is 20. Early stopping is used to avoid overfitting. The beam size is 5. Before feeding into the encoder,"
I17-1084,D14-1162,0,0.0805412,"-parameters setting for experiments, evaluate our approach in two kinds of scenarios, and compare the results of our approach and the other comparing approaches. 5.1 Hyper-parameters On the encoder side, both the forward RNN and the backward RNN have 2-layer stacked LSTMs with 500 hidden units. On the decoder side, all types of decoders are 2-layer stacked LSTMs with 500 hidden units. All LSTM parameters are sampled from U(−0.1, 0.1). The dropout rate is set to 0.3. The word embedding size is 300 for input tokens on both the encoder side and the decoder side. We use open-source GloVe vectors (Pennington et al., 2014) trained on Common Crawl 840B with 300 dimensions6 to initialize the word embeddings on the decoder side. The mini-batch size is set to 64 and the step size set to 50. Gradients are clipped when their norms are greater than 5 (Pascanu et al., 2013). For simplicity, we use vanilla softmax over the decoder vocabulary as opposed to more efficient alternatives such as sampled softmax (Jean et al., 2015). The vocabulary size is set to 40,000. The number of epochs 6 https://nlp.stanford.edu/projects/ glove/ 836 5.2.4 Evaluation using BLEU Table 2 shows the cased BLEU scores of linearized PredPatt an"
I17-1084,D15-1044,0,0.017442,"tokens from the selected decoder to update the beam. When the special token heosi is generated, we remove the candidate sequence from the beam. 4 Related Work The model we propose in this paper is adapted from the RNN encoder-decoder architectures which have been successfully applied to a wide range of NLP tasks such as machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014), image description generation (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015b), syntactic parsing (Vinyals et al., 2015a), question answering (Hermann et al., 2015), summarization (Rush et al., 2015), and semantic parsing (Dong and Lapata, 2016). As a novel variation of the encoder-decoder architecture, our model provides a general solution to tasks involving translation and labeling. crosslingual open IE is an example of this kind of task. The end-to-end solution proposed by Zhang et al. (2017a) used a vanilla attention-based encoderdecoder model to achieve results which outperform the traditional pipeline solutions. Compared to the vanilla encoder-decoder model, our model splits the joint task into two concurrent tasks (i.e., labeling and translating), which are jointly learnt by a sele"
I17-1084,D13-1176,0,0.014997,"e a well-formed syntax-semantics interface (Zhang et al., 2017b). The beam is used to increase the search space for the sequence Y in the target language. At each decoding time step, we first greedily select the type of decoder, and then generate candidate tokens from the selected decoder to update the beam. When the special token heosi is generated, we remove the candidate sequence from the beam. 4 Related Work The model we propose in this paper is adapted from the RNN encoder-decoder architectures which have been successfully applied to a wide range of NLP tasks such as machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014), image description generation (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015b), syntactic parsing (Vinyals et al., 2015a), question answering (Hermann et al., 2015), summarization (Rush et al., 2015), and semantic parsing (Dong and Lapata, 2016). As a novel variation of the encoder-decoder architecture, our model provides a general solution to tasks involving translation and labeling. crosslingual open IE is an example of this kind of task. The end-to-end solution proposed by Zhang et al. (2017a) used a vanilla attention-based encoderdecoder model"
I17-1084,W09-0441,0,0.0375213,"great importance to solve the cross-lingual portability issues of various NLP systems which are in the support of open information extraction (Sudo et al., 2004). Additionally, there is often a great demand for rapid access to information across languages, especially when a large-scale incident occurs (Lu et al., 2016). Conventional solutions decompose the task as a pipeline of machine translation followed by open information extraction (or vice versa), which causes a deviation since machine translation attaches equal importance to adequacy and fluency of the intermediate translation results (Snover et al., 2009), whereas the final goal focuses more on extracting correct predicates and arguments. Recently Zhang et al. (2017a) proposes an endto-end solution that outperforms the conventional pipeline solutions. They recast cross-lingual open IE as a sequence-to-sequence learning problem by converting the target facts in the tree structure (Fig. 1(c)) into a linear form called linearized PredPatt2 (Fig. 1(d)), and employ a stan1 The predicate-argument information is normally represented by relation tuples. Here, we use a richer representation (i.e., a tree structure) adopted by PredPatt (White et al., 20"
I17-1084,W17-6944,1,0.862332,"Missing"
I17-1084,L16-1521,0,0.0670588,"罪状 ” 的 良 机, (As a result, the Democratic party lost a good opportunity to list the ‘ charges ’ against Bush. Reference: (1) As (a result) , (the Democratic party) lost (a good opportunity) (2) (a good opportunity) list (the ‘ charges ’ against Bush) Selective Decoding: Datasets Task #Train #Valid #Test uzb-eng 31,581 1,373 1,373 tur-eng 20,774 903 903 amh-eng 12,140 527 527 som-eng 10,702 465 465 yor-eng 5,787 251 251 Table 8: Number of data used for cross-lingual open IE in low-resource scenarios. To prepare the experiment datasets, we first collect bitexts from DARPA LORELEI language packs (Strassel and Tracey, 2016). The source languages of the bitexts are Uzbek, Turkish, Amharic, Somali, and Yoruba.10 We then run a process similar to Zhang et al. (2017a) to generate pairs of source-language sentences and English linearized PredPatt: first, we employ SyntaxNet Parser (Andor et al., 2016) to generate Universal Dependency parses for the English sentences, and then run PredPatt (White et al., 2016) to generate English linearized PredPatt from the Universal Dependency parses. We remove empty sequences and very long sequences (length&gt;50) in the pairs, and randomly split them into training, validation and test"
I17-1084,C04-1127,0,0.672647,"English translation (b), English predicate-argument information (c), linearized PredPatt output (d) and output with separated predicate and argument labels (e). Introduction Cross-lingual open information extraction is defined as the task of extracting facts from the source language (e.g., Chinese text in Fig. 1(a)) and representing them in the target language (e.g. English predicate-argument information in Fig. 1(c))1 . It is a challenging task and of great importance to solve the cross-lingual portability issues of various NLP systems which are in the support of open information extraction (Sudo et al., 2004). Additionally, there is often a great demand for rapid access to information across languages, especially when a large-scale incident occurs (Lu et al., 2016). Conventional solutions decompose the task as a pipeline of machine translation followed by open information extraction (or vice versa), which causes a deviation since machine translation attaches equal importance to adequacy and fluency of the intermediate translation results (Snover et al., 2009), whereas the final goal focuses more on extracting correct predicates and arguments. Recently Zhang et al. (2017a) proposes an endto-end sol"
I17-1084,P16-5005,0,0.0126659,"V |Y |× T |Y |is the set of all possible (Y 0 , T 0 ) pairs. And (Yˆ , Tˆ) can be directly converted to the form of linearized PredPatt which is used for evaluation. However, it is impractical to iterate over all these (Y 0 , T 0 ) pairs during inference: here, we use beam search to generate tokens and labels as shown in Algorithm 1. ti ∈T =softmax(Uo0 si + Co0 ci + b0o )[yi ], Inference |V |is the vocabulary size of the target language. 835 whereas the GRU/LSTM gated unit itself learns to memorize long short-term dependencies. Similar mechanisms have been used in neural machine translating (Tu et al., 2016) and image caption generation (Xu et al., 2015) to explicitly control the influence from source or target contexts. The experiments in § 5 also confirms our point: our model using the selective decoding mechanism significantly improves the performance, compared to the standard encoder-decoder model. Regarding to open IE systems for generating training data, PredPatt has shown promising performance on large-scale open IE benchmarks (Zhang et al., 2017c). Compared to other existing open IE systems (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015), PredPatt uses manual language-agnost"
I17-1084,D16-1177,1,0.799451,"Missing"
I17-1084,E17-2011,1,0.850608,"Missing"
I17-1100,W07-1207,0,0.0166474,"o varying degrees. On one end of the spectrum are approaches that utilize declarative logical forms. In such approaches, semantic parsers first convert a sequence into a meaning representation that expresses the semantics needed for inference. In this case, each individual component of the logical form is clearly interpretable. Tremendous energies within computational linguistics have been spent on building declarative, componentwise-interpretable logical forms such as Hobbsian Logic (Hobbs, 1985), Discourse Representation Theory (Kamp et al., 2011), the Rochester Interactive Planning System (Allen et al., 2007), Minimal Recursion Semantics (Copestake et al., 2005), Episodic Logic (Schubert and Hwang, 2000), Combinatory Categorical Grammar (Steedman, 2000), Semantic Role Labeling (Gildea and Jurafsky, 2002), Framenet Parsing (Fillmore et al., 2003) and Abstract Meaning Representation (Banarescu et al., 2013). Opposite the above approaches are methods that utilize vector space-based logical forms. Recent work on word and string embeddings (Mikolov et al., 2013; Pennington et al., 2014) has produced vector space representations that can be induced from large corpora in an unsupervised manner that have"
I17-1100,W13-2322,0,0.0602476,"Missing"
I17-1100,N13-1092,1,0.843593,"Missing"
I17-1100,W07-1401,0,0.381524,"ral strategy to automatically generate one or more sentential hypotheses based on an input sentence and pre-existing manual semantic annotations. The resulting suite of datasets enables us to probe a statistical RTE model’s performance on different aspects of semantics. We demonstrate the value of this approach by investigating the behavior of a popular neural network RTE model. 1 Introduction The Recognizing Textual Entailment (RTE) task aims to assess a system’s ability to do textual inference—i.e. derive valid conclusions from textual clues (Dagan et al., 2006, 2013; Bar-Haim et al., 2006; Giampiccolo et al., 2007, 2009; Bentivogli et al., 2009, 2010, 2011). In this task, a system judges whether “typically, a human reading [the sentential context, or text] T would infer that [the sentential hypothesis] H is most likely true” (Dagan et al., 2006). Recent efforts in textual inference have focused on the Stanford Natural Language Inference (SNLI) dataset. SNLI is made up of hundreds of thousands of text-hypothesis pairs, wherein the texts are image captions drawn from the Flickr30k corpus (Young et al., 2014) and the hypotheses are elicited from crowdsourcing workers based on those captions (but not the c"
I17-1100,D15-1075,0,0.402868,"he FrameNet Plus (FN+) dataset, which contains likelihood judgments about the paraphrase validity of frame triggers (Pavlick et al., 2015), and 1 These recasted datasets are made publicly available at http://decomp.net. 996 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 996–1005, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (iii) the Definite Pronoun Resolution (DPR) dataset, which contains annotations relevant to complex anaphora resolution (Rahman and Ng, 2012). We use these recasted datasets to train a recent neural RTE model (Bowman et al., 2015) and measure its performance. We show that complex anaphora is the most difficult semantic phenomenon for neural RTE models to capture, followed by predicting thematic proto-role properties. Perhaps unsurprisingly, given the nature of the RTE task, paraphrasing seems to be the easiest phenomenon to model. In the next section (§2), we discuss previous work in RTE, focusing in particular on the development of RTE datasets. We then discuss our data creation process (§3) as well as the results of a small validation (§4). Finally, we report on the setup and results of our three experiments (§5) and"
I17-1100,J02-3001,0,0.0290378,"that expresses the semantics needed for inference. In this case, each individual component of the logical form is clearly interpretable. Tremendous energies within computational linguistics have been spent on building declarative, componentwise-interpretable logical forms such as Hobbsian Logic (Hobbs, 1985), Discourse Representation Theory (Kamp et al., 2011), the Rochester Interactive Planning System (Allen et al., 2007), Minimal Recursion Semantics (Copestake et al., 2005), Episodic Logic (Schubert and Hwang, 2000), Combinatory Categorical Grammar (Steedman, 2000), Semantic Role Labeling (Gildea and Jurafsky, 2002), Framenet Parsing (Fillmore et al., 2003) and Abstract Meaning Representation (Banarescu et al., 2013). Opposite the above approaches are methods that utilize vector space-based logical forms. Recent work on word and string embeddings (Mikolov et al., 2013; Pennington et al., 2014) has produced vector space representations that can be induced from large corpora in an unsupervised manner that have been used to initialize the training of neural networks for tasks as complex as English-to-French machine translation (Sutskever et al., 2014). Vector space-based intermediate forms are not commonly"
I17-1100,P16-1223,0,0.0136742,"mena. To construct such datasets we presented a general strategy of converting semantic classification examples to annotated textual inference pairs that can be used to create large datasets for free on which even neural models for RTE can be trained. Further we used these datasets to gain insights into the behavior of a popular neural RTE model and the SNLI dataset itself. The variation in the performance of that model on the three datasets showed that neural models for natural language understanding recognise lexical variations or paraphrasing much better than anaphora resolution. Recently (Chen et al., 2016) also presented a similar conclusion after manually analyzing the errors made by neural systems on a reading comprehenproperties on the SPR test set. The percentage numbers at the bottom are the contribution of the category above to the total errors. Error percentages that are close to each other are omitted for clarity. Acknowledgments This research was supported by the JHU HLTCOE, DARPA DEFT, DARPA LORELEI, and the JHU Science of Learning Institute. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this pu"
I17-1100,D16-1053,0,0.011158,"Missing"
I17-1100,P85-1008,0,0.766062,"m All approaches to natural language understanding utilize intermediate logical forms that are interpretable to varying degrees. On one end of the spectrum are approaches that utilize declarative logical forms. In such approaches, semantic parsers first convert a sequence into a meaning representation that expresses the semantics needed for inference. In this case, each individual component of the logical form is clearly interpretable. Tremendous energies within computational linguistics have been spent on building declarative, componentwise-interpretable logical forms such as Hobbsian Logic (Hobbs, 1985), Discourse Representation Theory (Kamp et al., 2011), the Rochester Interactive Planning System (Allen et al., 2007), Minimal Recursion Semantics (Copestake et al., 2005), Episodic Logic (Schubert and Hwang, 2000), Combinatory Categorical Grammar (Steedman, 2000), Semantic Role Labeling (Gildea and Jurafsky, 2002), Framenet Parsing (Fillmore et al., 2003) and Abstract Meaning Representation (Banarescu et al., 2013). Opposite the above approaches are methods that utilize vector space-based logical forms. Recent work on word and string embeddings (Mikolov et al., 2013; Pennington et al., 2014)"
I17-1100,W11-1902,0,0.0148632,"Missing"
I17-1100,marelli-etal-2014-sick,0,0.0349083,"on top of natural text. 2.2 Approaches to textual entailment Research in textual entailment, at least in its most recent form, was catalyzed by the RTE shared task (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007, 2009; Bentivogli et al., 2009, 2010, 2011). With each iteration of this shared task, manually annotated examples were created for testing competing systems. But even after multiple iterations, the amount of available data for RTE was still small. The Sentences Involving Compositional Knowledge (SICK) corpus was released with the goal of alleviating this problem (Marelli et al., 2014). A significant further contribution was made with the Stanford Natural Language Inference (SNLI) corpus, which uses crowdsourcing to gather two orders of magnitude more examples than all previous datasets (Bowman et al., 2015). SNLI enabled fully supervised training of powerful machine learning models like neural networks. A number of researchers have pursued this direction by applying completely supervised neural models for sequential data to the problem of textual entailment (Rockt¨aschel et al., 2015; Mou et al., 2015; Shuohang and Jing, 2015; Liu et al., 2016; Cheng et al., 2016; Parikh e"
I17-1100,N13-1090,0,0.00783932,"ical forms such as Hobbsian Logic (Hobbs, 1985), Discourse Representation Theory (Kamp et al., 2011), the Rochester Interactive Planning System (Allen et al., 2007), Minimal Recursion Semantics (Copestake et al., 2005), Episodic Logic (Schubert and Hwang, 2000), Combinatory Categorical Grammar (Steedman, 2000), Semantic Role Labeling (Gildea and Jurafsky, 2002), Framenet Parsing (Fillmore et al., 2003) and Abstract Meaning Representation (Banarescu et al., 2013). Opposite the above approaches are methods that utilize vector space-based logical forms. Recent work on word and string embeddings (Mikolov et al., 2013; Pennington et al., 2014) has produced vector space representations that can be induced from large corpora in an unsupervised manner that have been used to initialize the training of neural networks for tasks as complex as English-to-French machine translation (Sutskever et al., 2014). Vector space-based intermediate forms are not commonly recognized as logical forms but in light of recent work (Bouchard et al., 2015) it seems worthwhile to reconsider this view. An argument in favor of declarative, interpretable logical forms is that one can directly observe the specific mistakes made by a sy"
I17-1100,E17-1002,0,0.0126305,"Missing"
I17-1100,N16-1170,0,0.0107298,"Missing"
I17-1100,D17-3004,1,0.869421,"Missing"
I17-1100,J05-1004,0,0.05908,"textual inference examples. This strategy requires only minor effort in developing datasetspecific generation capabilities to recast annotations into a shared universal representation: natural language sentences. We demonstrate the use of this strategy in two steps. First, we construct three recasted datasets from three existing semantic resources that target three distinct semantic phenomena:1 (i) the Semantic Proto-Roles v1 (SPR) dataset (Reisinger et al., 2015), which contains likelihood judgments about the semantic proto-role properties (Dowty, 1991) of verbal arguments found in PropBank (Palmer et al., 2005), (ii) the FrameNet Plus (FN+) dataset, which contains likelihood judgments about the paraphrase validity of frame triggers (Pavlick et al., 2015), and 1 These recasted datasets are made publicly available at http://decomp.net. 996 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 996–1005, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (iii) the Definite Pronoun Resolution (DPR) dataset, which contains annotations relevant to complex anaphora resolution (Rahman and Ng, 2012). We use these recasted datasets to train a recent neural RTE"
I17-1100,D16-1244,0,0.0215936,"Missing"
I17-1100,P15-2067,1,0.597886,"Missing"
I17-1100,D14-1162,0,0.100309,"Missing"
I17-1100,D12-1071,0,0.0655808,"perties (Dowty, 1991) of verbal arguments found in PropBank (Palmer et al., 2005), (ii) the FrameNet Plus (FN+) dataset, which contains likelihood judgments about the paraphrase validity of frame triggers (Pavlick et al., 2015), and 1 These recasted datasets are made publicly available at http://decomp.net. 996 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 996–1005, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (iii) the Definite Pronoun Resolution (DPR) dataset, which contains annotations relevant to complex anaphora resolution (Rahman and Ng, 2012). We use these recasted datasets to train a recent neural RTE model (Bowman et al., 2015) and measure its performance. We show that complex anaphora is the most difficult semantic phenomenon for neural RTE models to capture, followed by predicting thematic proto-role properties. Perhaps unsurprisingly, given the nature of the RTE task, paraphrasing seems to be the easiest phenomenon to model. In the next section (§2), we discuss previous work in RTE, focusing in particular on the development of RTE datasets. We then discuss our data creation process (§3) as well as the results of a small valid"
I17-1100,W14-2901,1,0.917501,"Missing"
I17-1100,Q15-1034,1,0.836899,"Missing"
I17-1100,D16-1177,1,0.475288,"Missing"
I17-1100,Q14-1006,0,0.0927602,"erive valid conclusions from textual clues (Dagan et al., 2006, 2013; Bar-Haim et al., 2006; Giampiccolo et al., 2007, 2009; Bentivogli et al., 2009, 2010, 2011). In this task, a system judges whether “typically, a human reading [the sentential context, or text] T would infer that [the sentential hypothesis] H is most likely true” (Dagan et al., 2006). Recent efforts in textual inference have focused on the Stanford Natural Language Inference (SNLI) dataset. SNLI is made up of hundreds of thousands of text-hypothesis pairs, wherein the texts are image captions drawn from the Flickr30k corpus (Young et al., 2014) and the hypotheses are elicited from crowdsourcing workers based on those captions (but not the corresponding image). While SNLI has led to significant methodological improvements, its collection protocol does not lend itself to understanding the types of semantic knowledge necessary for properly understanding a particular example. Researchers compete on which system achieves the highest score on a test set, but this itself does not lead to an understanding of which linguistic properties are better captured by a quantitatively superior system. In contrast, datasets such as FraCaS (Cooper et a"
I17-2062,W14-3301,1,0.894246,"Missing"
I17-2062,D16-1161,0,0.256696,"C corpus (§3), demonstrating that NRL outperforms the MLE baseline both in human and automated evaluation metrics. Research in automated Grammatical Error Correction (GEC) has expanded from token-level, closed class corrections (e.g., determiners, prepositions, verb forms) to phrase-level, open class issues that consider fluency (e.g., content word choice, idiomatic collocation, word order, etc.). The expanded goals of GEC have led to new proposed models deriving from techniques in data-driven machine translation, including phrasebased MT (PBMT) (Felice et al., 2014; Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016) and neural encoder-decoder models (Yuan and Briscoe, 2016). Napoles et al. (2017) recently showed that a neural encoderdecoder can outperform PBMT on a fluencyoriented GEC data and metric. We investigate training methodologies in the neural encoder-decoder for GEC. To train the neural encoder-decoder models, maximum likelihood estimation (MLE) has been used, where the objective is to maximize the (log) likelihood of the parameters for a given training data. As Ranzato et al. (2015) indicates, however, MLE has drawbacks. The MLE objective is based 366 Proceedings of the The 8th International J"
I17-2062,E17-3017,0,0.0610603,"Missing"
I17-2062,W04-1013,0,0.0304549,"Missing"
I17-2062,P15-2097,1,0.863842,"Missing"
I17-2062,P16-1159,0,0.0419389,"ty and metric score for each sample yˆi (line 7). In the encoder-decoder model, the parameters θ are updated through back-propagation and the number of parameter updates is determined by the partial derivative of J(θ), called the policy gradient (Williams, 1992; Sutton et al., 1999) in reinforcement learning: ∂J(θ) = αE [∇ log p(ˆ y ){r(ˆ y , y) − b}] (4) ∂θ where α is a learning rate and b is an arbitrary baseline reward to reduce the variance. The sample mean reward is often used for b (Williams, 1992), and we follow it in NRL. It is reasonable to compare NRL to minimum risk training (MRT) (Shen et al., 2016). In fact, The gradient of L(θ) is as follows: T X X ∂L(θ) ∇p(yt |x, y1t−1 ; θ) = ∂θ p(yt |x, y1t−1 ; θ) hX,Y i t=1 Neural Reinforcement Learning (2) One drawback of MLE is the exposure bias (Ranzato et al., 2015). The decoder predicts a word conditioned on the correct word sequence (y1t−1 ) during training, whereas it does with the predicted word sequence (ˆ y1t−1 ) at test time. Namely, the model is not exposed to the predicted words in training time. This is problematic, because once the model fails to predict a correct word at test time, it falls off the right track and does not come back"
I17-2062,E17-2037,1,0.792118,"ation metrics. Research in automated Grammatical Error Correction (GEC) has expanded from token-level, closed class corrections (e.g., determiners, prepositions, verb forms) to phrase-level, open class issues that consider fluency (e.g., content word choice, idiomatic collocation, word order, etc.). The expanded goals of GEC have led to new proposed models deriving from techniques in data-driven machine translation, including phrasebased MT (PBMT) (Felice et al., 2014; Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016) and neural encoder-decoder models (Yuan and Briscoe, 2016). Napoles et al. (2017) recently showed that a neural encoderdecoder can outperform PBMT on a fluencyoriented GEC data and metric. We investigate training methodologies in the neural encoder-decoder for GEC. To train the neural encoder-decoder models, maximum likelihood estimation (MLE) has been used, where the objective is to maximize the (log) likelihood of the parameters for a given training data. As Ranzato et al. (2015) indicates, however, MLE has drawbacks. The MLE objective is based 366 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 366–372, c Taipei, Taiwan, N"
I17-2062,P03-1021,0,0.0568408,"Missing"
I17-2062,P12-2039,0,0.270885,"r/h) if h ≤ r where N (A, B, C, ...) is the number of overlapped n-grams among the sets, and BP brevity penalty is compute based on token length in the system hypothesis (h) and the reference (r). 3 Methods Hybrid (rule + PBMT) PBMT + GEC-feat. PBMT + Neural feat. enc-dec (MLE) + unk alignment enc-dec (MLE/NRL) Experiments Data For training the models (MLE and NRL), we use the following corpora: the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), the Cambridge Learner Corpus First Certificate English (FCE) (Yannakoudakis et al., 2011), and the Lang-8 Corpus of learner English (Tajiri et al., 2012). The basic statistics are shown in Table 1.3 We exclude some unreasonable edits (comments by editors, incomplete sentences such 4 3 https://github.com/AbiWord/enchant NRL code is available at https://github.com/ keisks/nematus/tree/nrl-gleu All the datasets are publicly available, for purposes of reproducibility. For more details about each dataset, refer to Sakaguchi et al. (2017). 5 368 Models Original AMU CAMB14 NUS CAMB16 MLE NRL Reference dev set Human GLEU -1.072 38.21 -0.405 41.74 -0.160 42.81 -0.131 46.27 -0.117 47.20 -0.052 48.24 0.169 49.82 1.769 55.26 test set Human GLEU -0.760 40."
I17-2062,P02-1040,0,0.09751,"ck of MLE is the exposure bias (Ranzato et al., 2015). The decoder predicts a word conditioned on the correct word sequence (y1t−1 ) during training, whereas it does with the predicted word sequence (ˆ y1t−1 ) at test time. Namely, the model is not exposed to the predicted words in training time. This is problematic, because once the model fails to predict a correct word at test time, it falls off the right track and does not come back to it easily. Furthermore, in most sentence generation tasks, the MLE objective does not necessarily correlate with our final evaluation metrics, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in summarization. This is because MLE optimizes word level predictions at each time step instead of evaluating sentences as a whole. GEC is no exception. It depends on sentencelevel evaluation that considers grammaticality and fluency. For this purpose, it is natural to use GLEU (Napoles et al., 2015), which has been used as a 1 The reward is given at the end of the decoder output (i.e., delayed reward). 2 We sampled sentences from softmax distribution. 367 Corpus NUCLE FCE Lang-8 # sents. 57k 34k 1M mean chars per sent. 115 74 56 # sents. edited 3"
I17-2062,P11-1019,0,0.259539,"H, R) − [N (H, S) − N (H, S, R)] p0n = N (H) ( 1 if h &gt; r BP = exp(1 − r/h) if h ≤ r where N (A, B, C, ...) is the number of overlapped n-grams among the sets, and BP brevity penalty is compute based on token length in the system hypothesis (h) and the reference (r). 3 Methods Hybrid (rule + PBMT) PBMT + GEC-feat. PBMT + Neural feat. enc-dec (MLE) + unk alignment enc-dec (MLE/NRL) Experiments Data For training the models (MLE and NRL), we use the following corpora: the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), the Cambridge Learner Corpus First Certificate English (FCE) (Yannakoudakis et al., 2011), and the Lang-8 Corpus of learner English (Tajiri et al., 2012). The basic statistics are shown in Table 1.3 We exclude some unreasonable edits (comments by editors, incomplete sentences such 4 3 https://github.com/AbiWord/enchant NRL code is available at https://github.com/ keisks/nematus/tree/nrl-gleu All the datasets are publicly available, for purposes of reproducibility. For more details about each dataset, refer to Sakaguchi et al. (2017). 5 368 Models Original AMU CAMB14 NUS CAMB16 MLE NRL Reference dev set Human GLEU -1.072 38.21 -0.405 41.74 -0.160 42.81 -0.131 46.27 -0.117 47.20 -0."
I17-3002,E17-2114,1,0.840887,"Missing"
I17-3002,P07-2045,1,0.0098071,"Missing"
K19-1005,P09-1053,0,0.0492448,"g(s), g(s0 )] + cos[g(s), g(t)])+ max(0, δ − cos[g(s), g(s0 )] + cos[g(s0 ), g(t0 )]) where g is one of (WORD, TRIGRAM, LSTM) and (t, t0 ) is a negative sample selected from a megabatch, an aggregation of m minibatches (Wieting and Gimpel, 2018).3 We evaluate the WORD model trained4 on PARA NMT, PARA BANK and PARA BANK 2 (our work). We retrieved the paraphrases from PARA - 3.8 Improving contextualized encoders with paraphrastic data Paraphrastic data can be used to fine-tune contextualized encoders such as BERT (Devlin et al., 2018). We frame the fine-tuning task as paraphrase identification (Das and Smith, 2009), where given a pair of sentences, the task is to classify them as paraphrases or non-paraphrases. To generate the training data, we extract, for each 3 We confirmed this loss with Wieting and Gimpel, that it captures their open implementation, which we employ. Wieting and Gimpel (2018) described their loss as: max(0, δ − cos(g(s), g(s0 )) + cos(g(s), g(t))), which is equivalent under their assumption the paraphrases are equivalent. 4 https://github.com/jwieting/ para-nmt-50m 49 BERT pBERT QQP MNLI STS-B MRPC 87.90 88.14 83.86 82.64 88.40 88.59 84.00 86.55 downstream tasks, in particular when"
K19-1005,C04-1051,0,0.12119,"a paraphrase pair, and (s, n) is a non-paraphrase pair. We use these to train a binary classifier with cross-entropy loss. We then use this BERT fine-tuned on paraphrases (henceforth pBERT) for fine-tuning on SQuAD 2.0 (Rajpurkar et al., 2018) and 4 NLP tasks present in the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019): Quora Question Pairs (QQP) (Chen et al., 2017), Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2018), the Semantic Textual Similarity Benchmark (STS-B) (Agirre et al., 2016), and the Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004). Following the model formulation, hyper-parameter selection and training procedure specified in Devlin et al. (2018), we add a single task-specific, randomly initialized output layer for the classifier. We present our results in Tab. 4 and Tab. 5. We observe gains for STS-B, MRPC and QQP, tasks strongly related to paraphrase identification. Fine-tuning on our paraphrase corpus also improves performance on SQuAD, a questionanswering task, while slightly degrading performance on MNLI. Overall, simple fine-tuning of BERT on our corpus leads to improvements on Sentential resources There exists mu"
K19-1005,S16-1081,0,0.166659,"PARA BANK17 /PARA BANK34 Our work1 /Our work3 Our work3 /Our work5 Our work1 /Our work5 20.58 64.16±.21 71.05±.22 69.46±.27 80.93 52.77±.48 45.00±.51 46.79±.12 2.26 5.51±.01 6.40±.19 6.25±.18 Our workmax /Our workmin 66.03±.86 49.10±.16 5.84±.33 Table 2: Collective diversity within our work compared to PARA BANK, as measured by (1-BLEU)×100, intersection/union score×100, and parse tree edit-distance. of paraphrase corpora as training data for the Semantic Textual Similarity (STS) task. STS aims to measure the degree of equivalence in meaning or semantics between a pair of sentences. Notably, Agirre et al. (2016) having been a part of the SemEval workshop (2012 -2017). The evaluation consists of human annotated English sentence pairs, scored on a scale of 0 to 5 to quantify similarity of meaning, with 0 being the least, and 5 the most similar. Wieting and Gimpel (Wieting and Gimpel, 2018) compared three encoding mechanisms: WORD, TRIGRAM and LSTM. The WORD model (Wieting et al., 2016) averages the embedding for each word in the sentence into a fixed length vector embedding for the sentence; the TRIGRAM model (Huang et al., 2013) averages over character trigrams; and the LSTM (Hochreiter and Schmidhube"
K19-1005,D17-1091,0,0.0293981,"ability of Neural Machine Translation (NMT) to recreate the translation target by conditioning on the source side of the bitext. Leveraging paraphrases in NLP In the context of semantic parsing, Berant and Liang (2014) use a paraphrase classification module to determine the match between a canonical utterance and a logical form, both using a phrase table and distributed representations. To improve question answering (QA), Duboue and ChuCarroll (2006) generate paraphrases of a given question using back-translation, and optionally replace the original question with the most relevant paraphrase. Dong et al. (2017) tackle QA by marginalizing the probability of an answer over a set of paraphrases, generated using rule-based and NMT-based methods. Fader et al. (2013) use a corpus of questions with paraphrases, to construct a corpus of semantically equivalent queries. PARA BANK took a similar approach but with the inclusion of lexical constraints from the target side of the bitext. This step allows for multiple translations from one bilingual sentence pair and promotes lexical diversity. Their work, despite being larger and shown to be less noisy than PARA NMT, relies on heuristics to produce hard constrai"
K19-1005,P98-1013,0,0.0242805,"accuracy scores are reported for MNLI. Numbers reported on Dev set F1 Exact Match Type BERT pBERT HasAns NoAns Total HasAns NoAns Total 76.81 71.44 74.12 70.34 71.44 70.89 74.21 74.95 74.58 68.00 74.95 71.48 4.1 Related works Paraphrastic resources Paraphrastic resources exist across different scopes (i.e., lexical, phrasal, sentential) and different creation strategies (i.e., manually curated, automatically generated). For a more comprehensive survey on data-driven approaches to paraphrasing, please refer to Madnani and Dorr (2010). Sub-sentential resources WordNet (Miller, 1995), FrameNet (Baker et al., 1998), and VerbNet (Schuler, 2006) can be used to extract paraphrastic expressions at lexical levels. They contain the grouping of words or phrases that share similar semantics and sometimes entailment relations. While FrameNet and VerbNet do have example sentences or frames where lexical units are put into contexts, there is no explicit paraphrastic relations among these examples. Also, these datasets tend to be small, as they were curated manually. There have been efforts to augment such resources with automatic methods (Snow et al., 2006; Pavlick et al., 2015b), but they are still confined to le"
K19-1005,N12-1017,0,0.119116,"anguage sentence which, when paired with the target-language reference, constitute a set of paraphrases. Working from the very large CzEng parallel corpus, Wieting and Gimpel (2018) produced a single paraphrase for each English sentence by translating from the Czech source. Hu et al. (2019) expanded on this by translating the Czech sentence several times, using positive or negative constraints obtained from the English reference. In terms of producing diverse paraphrases, both approaches are limited because they rely on beam search. There are potentially billions of paraphrases of a sentence (Dreyer and Marcu, 2012), yet beam search with recurrent models can only search a constant subset of them (in the beam size). There are techniques for producing more diverse paraphrases, such as the use of positive and negative constraints (Hu et al., 2019) or syntactic 1 3. Clustering. The samples are then clustered. The best item from each cluster (according to the summed score) is then returned (§2.3). 2.1 Constrained sampling Sampling is a more effective way to explore model search space than beam search, particularly in auto-regressive models that do not permit dynamic programming. We introduce two means by whic"
K19-1005,P01-1008,0,0.360989,"procedure specified in Devlin et al. (2018), we add a single task-specific, randomly initialized output layer for the classifier. We present our results in Tab. 4 and Tab. 5. We observe gains for STS-B, MRPC and QQP, tasks strongly related to paraphrase identification. Fine-tuning on our paraphrase corpus also improves performance on SQuAD, a questionanswering task, while slightly degrading performance on MNLI. Overall, simple fine-tuning of BERT on our corpus leads to improvements on Sentential resources There exists multiple human translations in the same language for some classic readings. Barzilay and McKeown (2001) sought to extract lexical paraphrastic expression from such sources. Unfortunately such resources – along with those manually constructed for text generation research (Robin, 1995; Pang et al., 2003) – are small and limited in domain. PARA NMT and PARA BANK are two much larger sentential paraphrastic resources created through back-translation. 50 Reference: Real life is sometimes thoughtless and mean. Hey, stop right there! real life is sometimes reckless and cruel . hey , stop . The real life is occasionally ruthless and cruel. The real world is occasionally ruthless and cruel. The real life"
K19-1005,P14-1133,0,0.0336483,"ANK: Our work: Table 6: Selected examples from our work, compared to paraphrastic resources with prior approaches. Our work has paraphrases that are not only different from the reference, but also diverse among themselves. 4.2 4.3 Translation-based Approaches PARA NMT is an automatically generated sentential paraphrastic resource through back-translating bilingual resources. It leveraged the imperfect ability of Neural Machine Translation (NMT) to recreate the translation target by conditioning on the source side of the bitext. Leveraging paraphrases in NLP In the context of semantic parsing, Berant and Liang (2014) use a paraphrase classification module to determine the match between a canonical utterance and a logical form, both using a phrase table and distributed representations. To improve question answering (QA), Duboue and ChuCarroll (2006) generate paraphrases of a given question using back-translation, and optionally replace the original question with the most relevant paraphrase. Dong et al. (2017) tackle QA by marginalizing the probability of an answer over a set of paraphrases, generated using rule-based and NMT-based methods. Fader et al. (2013) use a corpus of questions with paraphrases, to"
K19-1005,N06-2009,0,0.216059,"Missing"
K19-1005,D18-1045,0,0.0371487,"han beam search, particularly in auto-regressive models that do not permit dynamic programming. We introduce two means by which we can expand the hypothesis space, and produce a more diverse set of paraphrases, relative to straightforward beam search. Top-k sampling In auto-regressive neural MT, the standard sampling approach would be to choose a word wt at each decoder timestep t by sampling from the distribution P (wt |w1...t−1 ). This approach has been found effective over 1best beam search in generating source sentences in Available at http://nlp.jhu.edu/parabank2 45 2.3 back-translation (Edunov et al., 2018). However, for paraphrasing, this is not ideal, since words that are not semantically licensed by the source may be selected. Instead, we propose top-k sampling, in which we choose wt from the top k most-probable tokens at each time step. This way, we allow the model to sample flexibly, vastly opening up the hypothesis space, without creating a large risk of producing nonsensical translations. The above process produces a large set of translations of the source sentence. Many of them will be minor variants of one another, but we expect that there will be a lot of variety in the large pool. The"
K19-1005,P13-1158,0,0.0672992,"in NLP In the context of semantic parsing, Berant and Liang (2014) use a paraphrase classification module to determine the match between a canonical utterance and a logical form, both using a phrase table and distributed representations. To improve question answering (QA), Duboue and ChuCarroll (2006) generate paraphrases of a given question using back-translation, and optionally replace the original question with the most relevant paraphrase. Dong et al. (2017) tackle QA by marginalizing the probability of an answer over a set of paraphrases, generated using rule-based and NMT-based methods. Fader et al. (2013) use a corpus of questions with paraphrases, to construct a corpus of semantically equivalent queries. PARA BANK took a similar approach but with the inclusion of lexical constraints from the target side of the bitext. This step allows for multiple translations from one bilingual sentence pair and promotes lexical diversity. Their work, despite being larger and shown to be less noisy than PARA NMT, relies on heuristics to produce hard constraints on the decoder, which often causes unintended changes in semantics or grammar. Both works largely follow standard approaches in NMT, generating 1-bes"
K19-1005,P18-1007,0,0.0746948,"raints can be provided as tokens or phrases; the decoder tracks the progress of generation through each constraint and adds an infinite cost to the final word of any constraints, precluding its selection in both sampling and beam search. In order to further increase sample diversity when generating the hypotheses (§2.1), we obtain negative constraints from the source by randomly choosing a subset of tokens. We do this independently multiple times for each input sentence. This provides new sets of constraints for the inputs, independent of the decoding. Note that we use subword regularization (Kudo, 2018) during training, causing different subword segmentations to be applied to training data types each time they are encountered and helping to build more robust models. We only constrain on the Viterbi segmentation, effectively discouraging negatively constrained words from appearing in the output, instead of prohibiting them, since there are often ways for the model to produce a word by generating a different decomposition. 2.2 Edit-distance-based clustering 3 3.1 Evaluations Data All of our experiments are based on the CzEng 1.7 corpus, a subset of CzEng 1.6 (Bojar et al., 2016b) that has been"
K19-1005,J10-3003,0,0.0377699,"res are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for MNLI. Numbers reported on Dev set F1 Exact Match Type BERT pBERT HasAns NoAns Total HasAns NoAns Total 76.81 71.44 74.12 70.34 71.44 70.89 74.21 74.95 74.58 68.00 74.95 71.48 4.1 Related works Paraphrastic resources Paraphrastic resources exist across different scopes (i.e., lexical, phrasal, sentential) and different creation strategies (i.e., manually curated, automatically generated). For a more comprehensive survey on data-driven approaches to paraphrasing, please refer to Madnani and Dorr (2010). Sub-sentential resources WordNet (Miller, 1995), FrameNet (Baker et al., 1998), and VerbNet (Schuler, 2006) can be used to extract paraphrastic expressions at lexical levels. They contain the grouping of words or phrases that share similar semantics and sometimes entailment relations. While FrameNet and VerbNet do have example sentences or frames where lexical units are put into contexts, there is no explicit paraphrastic relations among these examples. Also, these datasets tend to be small, as they were curated manually. There have been efforts to augment such resources with automatic metho"
K19-1005,N12-1019,0,0.0612033,"Missing"
K19-1005,N13-1092,1,0.878282,"Missing"
K19-1005,W17-3206,0,0.0268669,"Missing"
K19-1005,P14-5010,0,0.003484,"paraphrases for each reference sentence using the approach outlined in this work. To account for randomness, we average over two independent runs in the result, shown in Tab. 1. We consider two sources of paraphrastic diversity: 1) lexical diversity, the use of different words; and 2) syntactic diversity, the change of sentence or phrasal structure. We separately measure them using bag-of-word Intersection/Union scores and parse-tree edit-distances, respectively. will be. We consider only the top 3 levels of the parse trees, excluding any terminals. Sentences are parsed with Stanford CoreNLP (Manning et al., 2014); the tree edit-distance is calculated with the APTED (Pawlik and Augsten, 2015a,b) algorithm. The average tree edit-distance for each system is shown in Tab. 1. Diversity among paraphrases Hu et al. (2019) produced multiple paraphrases for each reference. While shown to be diverse compared to the reference, the authors did not investigate whether these paraphrases are trivial rewrites of one another, as it is likely the case with beam search under a few lexical constraints. Our clustering step is specifically designed to retrieve collectively diverse paraphrases. We use the same metrics to ev"
K19-1005,E17-3017,0,0.0132251,"omposition. 2.2 Edit-distance-based clustering 3 3.1 Evaluations Data All of our experiments are based on the CzEng 1.7 corpus, a subset of CzEng 1.6 (Bojar et al., 2016b) that has been chosen for higher quality. Based on experience with data quality issues in neural MT (Ott et al., 2018; Junczys-Dowmunt, 2018), we decided to further clean the corpus. First, we normalize Unicode punctuation, and keep only bilingual pairs whose English side can be encoded with latin-1 and Czech side with latin-2. We then filter the data with dual cross-entropy filtering (Junczys-Dowmunt, 2018). We use Sockeye (Hieber et al., 2017) to train two NMT models, CS–EN and EN-CS, on a relatively clean subset of the data provided for WMT 2018 (Bojar et al., 2016a): Europarl, Wiki titles, and news commentary. We use 4 layer Transformer models (Vaswani et al., 2017) trained to convergence, with held-out likelihood evaluated on a random 500sentence subset of the WMT16 and WMT17 news test data. These models are then used to score all the remaining CzEng data after deduplication. We kept all sentences with a model score (negative log-likelihood) of less than 3.5. After applying the above two filters, we keep 19, 723, 003 out of the"
K19-1005,N16-3013,1,0.893873,"k-oriented trial-and-error. The ability to understand and produce paraphrases is a basic competency task, one that is often used as a teaching aid to validate if a student understands a statement or a concept. Current deep learning systems struggle with this task, exhibiting brittleness to both understanding and producing paraphrastic expressions (Iyyer et al., 2018). One crucial factor behind this incompetence is the dearth of sentential paraphrastic data. Many works have sought to leverage the relative abundance of sub-sentential paraphrastic resources in paraphrase detection or generation (Napoles et al., 2016). Yet, they fail to capture contextualized word choices or syntactical variations, as wordor phrase-level resources cannot incorporate information from the whole input sentence. Recent works have focused on leveraging bilingual resources to create large sentence-level paraphrastic collections using translation-based methods (Wieting and Gimpel, 2018; Hu et al., 2019). We present a novel resource with accurate and collectively diverse paraphrases, generated using stochastic decoding and clustering. By collectively diverse, we mean that the paraphrases of a given sentence cover a wide lexical an"
K19-1005,N18-1170,0,0.136454,"idates. One approach to force diverse translations is the use of hard lexical constraints at inference time (Hu et al., 2019). While effective in some cases, current approaches to automatic selection of such constraints is based on heuristics and task-oriented trial-and-error. The ability to understand and produce paraphrases is a basic competency task, one that is often used as a teaching aid to validate if a student understands a statement or a concept. Current deep learning systems struggle with this task, exhibiting brittleness to both understanding and producing paraphrastic expressions (Iyyer et al., 2018). One crucial factor behind this incompetence is the dearth of sentential paraphrastic data. Many works have sought to leverage the relative abundance of sub-sentential paraphrastic resources in paraphrase detection or generation (Napoles et al., 2016). Yet, they fail to capture contextualized word choices or syntactical variations, as wordor phrase-level resources cannot incorporate information from the whole input sentence. Recent works have focused on leveraging bilingual resources to create large sentence-level paraphrastic collections using translation-based methods (Wieting and Gimpel, 2"
K19-1005,W18-6478,0,0.0446558,"they are encountered and helping to build more robust models. We only constrain on the Viterbi segmentation, effectively discouraging negatively constrained words from appearing in the output, instead of prohibiting them, since there are often ways for the model to produce a word by generating a different decomposition. 2.2 Edit-distance-based clustering 3 3.1 Evaluations Data All of our experiments are based on the CzEng 1.7 corpus, a subset of CzEng 1.6 (Bojar et al., 2016b) that has been chosen for higher quality. Based on experience with data quality issues in neural MT (Ott et al., 2018; Junczys-Dowmunt, 2018), we decided to further clean the corpus. First, we normalize Unicode punctuation, and keep only bilingual pairs whose English side can be encoded with latin-1 and Czech side with latin-2. We then filter the data with dual cross-entropy filtering (Junczys-Dowmunt, 2018). We use Sockeye (Hieber et al., 2017) to train two NMT models, CS–EN and EN-CS, on a relatively clean subset of the data provided for WMT 2018 (Bojar et al., 2016a): Europarl, Wiki titles, and news commentary. We use 4 layer Transformer models (Vaswani et al., 2017) trained to convergence, with held-out likelihood evaluated on"
K19-1005,N03-1024,0,0.117882,"nd QQP, tasks strongly related to paraphrase identification. Fine-tuning on our paraphrase corpus also improves performance on SQuAD, a questionanswering task, while slightly degrading performance on MNLI. Overall, simple fine-tuning of BERT on our corpus leads to improvements on Sentential resources There exists multiple human translations in the same language for some classic readings. Barzilay and McKeown (2001) sought to extract lexical paraphrastic expression from such sources. Unfortunately such resources – along with those manually constructed for text generation research (Robin, 1995; Pang et al., 2003) – are small and limited in domain. PARA NMT and PARA BANK are two much larger sentential paraphrastic resources created through back-translation. 50 Reference: Real life is sometimes thoughtless and mean. Hey, stop right there! real life is sometimes reckless and cruel . hey , stop . The real life is occasionally ruthless and cruel. The real world is occasionally ruthless and cruel. The real life is sometimes reckless and cruel. Stay where you are! True life is sometimes ruthless and cruel. Actual life is sometimes ruthless and cruel. Sometimes real life is ruthless and cruel. Real life can b"
K19-1005,P15-2070,1,0.904204,"Missing"
K19-1005,P15-2067,1,0.88505,"Missing"
K19-1005,P18-1042,0,0.256353,"(Iyyer et al., 2018). One crucial factor behind this incompetence is the dearth of sentential paraphrastic data. Many works have sought to leverage the relative abundance of sub-sentential paraphrastic resources in paraphrase detection or generation (Napoles et al., 2016). Yet, they fail to capture contextualized word choices or syntactical variations, as wordor phrase-level resources cannot incorporate information from the whole input sentence. Recent works have focused on leveraging bilingual resources to create large sentence-level paraphrastic collections using translation-based methods (Wieting and Gimpel, 2018; Hu et al., 2019). We present a novel resource with accurate and collectively diverse paraphrases, generated using stochastic decoding and clustering. By collectively diverse, we mean that the paraphrases of a given sentence cover a wide lexical and syntactic spectrum. Given a bilingual input pair, our core idea is to sample a large space of outputs from a translation system, cluster the results according to a notion of token-sequence similarity, score them with two translation models (one in each direction), and then select the best item from each cluster. We believe that sampling from the w"
K19-1005,N18-1101,0,0.0412191,"tence s, we then find the (approximate) nearest neighbour n which is not s0 , among all of the sentences. We thus obtain two pairs, where (s, s0 ) is a paraphrase pair, and (s, n) is a non-paraphrase pair. We use these to train a binary classifier with cross-entropy loss. We then use this BERT fine-tuned on paraphrases (henceforth pBERT) for fine-tuning on SQuAD 2.0 (Rajpurkar et al., 2018) and 4 NLP tasks present in the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019): Quora Question Pairs (QQP) (Chen et al., 2017), Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2018), the Semantic Textual Similarity Benchmark (STS-B) (Agirre et al., 2016), and the Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004). Following the model formulation, hyper-parameter selection and training procedure specified in Devlin et al. (2018), we add a single task-specific, randomly initialized output layer for the classifier. We present our results in Tab. 4 and Tab. 5. We observe gains for STS-B, MRPC and QQP, tasks strongly related to paraphrase identification. Fine-tuning on our paraphrase corpus also improves performance on SQuAD, a questionanswering task, while slig"
K19-1005,N18-1119,1,0.671347,"skip the re-centering for that cluster. This improves the chance of the k clusters congregating candidates different from the reference in different ways. When the clustering has converged, we take the candidate with the best score from each cluster (except for the one with the reference sentence), rank them by score, and take the best n as the final output. Randomized negative constraints Negative constraints are tokens that are not permitted in the decoder output. They are not formally described in the literature, but an implementation was provided with the associated positive constraints (Post and Vilar, 2018). Negative constraints can be provided as tokens or phrases; the decoder tracks the progress of generation through each constraint and adds an infinite cost to the final word of any constraints, precluding its selection in both sampling and beam search. In order to further increase sample diversity when generating the hypotheses (§2.1), we obtain negative constraints from the source by randomly choosing a subset of tokens. We do this independently multiple times for each input sentence. This provides new sets of constraints for the inputs, independent of the decoding. Note that we use subword"
K19-1005,N13-1106,1,0.652759,"Missing"
K19-1005,P18-2124,0,0.0676573,"Missing"
K19-1005,N15-1091,0,0.0358836,"Missing"
K19-1005,P18-1020,1,0.844276,"Missing"
K19-1005,P06-1101,0,0.0617858,"b-sentential resources WordNet (Miller, 1995), FrameNet (Baker et al., 1998), and VerbNet (Schuler, 2006) can be used to extract paraphrastic expressions at lexical levels. They contain the grouping of words or phrases that share similar semantics and sometimes entailment relations. While FrameNet and VerbNet do have example sentences or frames where lexical units are put into contexts, there is no explicit paraphrastic relations among these examples. Also, these datasets tend to be small, as they were curated manually. There have been efforts to augment such resources with automatic methods (Snow et al., 2006; Pavlick et al., 2015b), but they are still confined to lexical level and sometimes require the use of other paraphrastic resources (Pavlick et al., 2015b). PPDB (Ganitkevitch et al., 2013; Pavlick et al., 2015a) automated the generation of lexical paraphrases via bilingual pivoting, taking advantage of the relative abundance of bilingual corpora. While significantly larger and more informative (e.g., ranking, entailment relations, etc.) than the above manually curated resources, PPDB suffers from ambiguity as words or phrases are removed from their sentential contexts. Table 5: SQuAD 2.0 res"
kupsc-etal-2004-pronominal,J90-4001,0,\N,Missing
kupsc-etal-2004-pronominal,1995.iwpt-1.15,0,\N,Missing
kupsc-etal-2004-pronominal,W98-1119,0,\N,Missing
kupsc-etal-2004-pronominal,C88-1021,0,\N,Missing
kupsc-etal-2004-pronominal,briscoe-carroll-2002-robust,0,\N,Missing
kupsc-etal-2004-pronominal,P95-1017,0,\N,Missing
kupsc-etal-2004-pronominal,W02-1028,0,\N,Missing
kupsc-etal-2004-pronominal,J01-4004,0,\N,Missing
kupsc-etal-2004-pronominal,2001.mtsummit-papers.43,1,\N,Missing
N09-3007,P90-1034,0,0.362427,"using as features argument positions within fragments from a syntactic dependency parser. 6 Conclusion We have presented a bootstrapping approach for creating semantically tagged lexicons. The method can effectively classify nouns with contrasting semantic properties, even when the initial training set is a very small. Further classification is possible with both manual and automatic methods by utilizing individual contextual features in the optimal model. Acknowledgments Table 7: Top-10 features that promote activation of the physical-object target in the model. glish nouns first appeared in Hindle (1990). Roark and Charniak (1998) constructed a semantic lexicon using co-occurrence statistics of nouns within noun phrases. More recently, Liakata and Pulman (2008) induced a hierarchy over nominals using as features knowledge fragments similar to the sort given by K NEXT. Our work might be viewed as aiming for the same goal (a lexico-semantic based partitioning over nominals, tied to corpus-based knowledge), but allowing for an a priori bias regarding preferred structure. The idea of bootstrapping lexical semantic properties goes back at least to Hearst (1998), where the idea is suggested of usin"
N09-3007,P06-1103,0,0.0133972,"n iterative training process, the classifier first learns from a small seed set, which contains examples of all categories (in binary classification, both positive and negative examples) manually selected to reflect human knowledge of semantic categories. The classifier then discovers new instances (and corresponding features) of each category. Based on activation values, these newly discovered instances are selectively admitted into the original training set, which increases the size of training examples for the next iteration. The iterative training algorithm described above is adopted from Klementiev and Roth (2006). The advantage of bootstrapping is the ability to automatically learn from new discoveries, which saves both time and effort required to manually examine a source lexicon. However, if implemented exactly as described above, this process has two apparent disadvantages: New examples may be wrongly classified by the model; and it is difficult to evaluate the discriminative models produced in successive iterations, as there are no standard data against which to judge them (the new examples are by definition previously unexamined). We propose two measures to alleviate these problems. First, we adm"
N09-3007,W08-2212,0,0.0527038,"r creating semantically tagged lexicons. The method can effectively classify nouns with contrasting semantic properties, even when the initial training set is a very small. Further classification is possible with both manual and automatic methods by utilizing individual contextual features in the optimal model. Acknowledgments Table 7: Top-10 features that promote activation of the physical-object target in the model. glish nouns first appeared in Hindle (1990). Roark and Charniak (1998) constructed a semantic lexicon using co-occurrence statistics of nouns within noun phrases. More recently, Liakata and Pulman (2008) induced a hierarchy over nominals using as features knowledge fragments similar to the sort given by K NEXT. Our work might be viewed as aiming for the same goal (a lexico-semantic based partitioning over nominals, tied to corpus-based knowledge), but allowing for an a priori bias regarding preferred structure. The idea of bootstrapping lexical semantic properties goes back at least to Hearst (1998), where the idea is suggested of using seed examples of a relation to discover lexico-syntactic extraction patterns and then using these to discover further examples of the desired relation. The Ba"
N09-3007,P98-2182,0,0.0513576,"es argument positions within fragments from a syntactic dependency parser. 6 Conclusion We have presented a bootstrapping approach for creating semantically tagged lexicons. The method can effectively classify nouns with contrasting semantic properties, even when the initial training set is a very small. Further classification is possible with both manual and automatic methods by utilizing individual contextual features in the optimal model. Acknowledgments Table 7: Top-10 features that promote activation of the physical-object target in the model. glish nouns first appeared in Hindle (1990). Roark and Charniak (1998) constructed a semantic lexicon using co-occurrence statistics of nouns within noun phrases. More recently, Liakata and Pulman (2008) induced a hierarchy over nominals using as features knowledge fragments similar to the sort given by K NEXT. Our work might be viewed as aiming for the same goal (a lexico-semantic based partitioning over nominals, tied to corpus-based knowledge), but allowing for an a priori bias regarding preferred structure. The idea of bootstrapping lexical semantic properties goes back at least to Hearst (1998), where the idea is suggested of using seed examples of a relati"
N09-3007,W02-1028,0,0.0762365,"Missing"
N09-3007,C08-1116,1,0.882175,"Missing"
N09-3007,C98-2177,0,\N,Missing
N12-1056,P08-1090,0,0.552481,"affinity between some word or phrase pair, (wi , wj ), as a function of co-occurance within some context boundary. Church and Hanks (1990) suggested pointwise mutual information: PMI(wi , wj ) = Pr(w ,wj ) log Pr(wi ) iPr(w , showing linguistically appealing j) results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora. Later work such as by Lin (1999) continued this tradition. Here we consider document, or discourse-level contexts, such as explored by Rosenfeld (1994) or Church (2000), and more recently by those such as Chambers and Jurafsky (2008) or Van Durme and Lall (2009b). In the spirit of recent work in randomized algorithms for large-scale HLT (such as by Ravichandran Conditional Random Sampling (CRS) Li and Church (2007) proposed CRS to approximate the contingency table between elements in a query, to be used in distributional similarity measures such as cosine similarity, correlation, and PMI. Central is the idea of the postings list, which is made up of the identifiers of each document that contains a given word or phrase. A set of such lists, one per type in the underlying vocabulary, is known as an inverted index. To reduce"
N12-1056,J90-1003,0,0.061614,"as required aggressive pruning or independence assumptions to compute scores on large collections. We show the method of Conditional Random Sampling, thus far an underutilized technique, to be a space-efficient means of representing the sufficient statistics in discourse that underly recent PMI-based work. This is demonstrated in the context of inducing Shankian script-like structures over news articles. 2 1 Background Introduction It has become common to model the distributional affinity between some word or phrase pair, (wi , wj ), as a function of co-occurance within some context boundary. Church and Hanks (1990) suggested pointwise mutual information: PMI(wi , wj ) = Pr(w ,wj ) log Pr(wi ) iPr(w , showing linguistically appealing j) results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora. Later work such as by Lin (1999) continued this tradition. Here we consider document, or discourse-level contexts, such as explored by Rosenfeld (1994) or Church (2000), and more recently by those such as Chambers and Jurafsky (2008) or Van Durme and Lall (2009b). In the spirit of recent work in randomized algorithms for large-scale HLT (such"
N12-1056,C00-1027,0,0.0190088,"n It has become common to model the distributional affinity between some word or phrase pair, (wi , wj ), as a function of co-occurance within some context boundary. Church and Hanks (1990) suggested pointwise mutual information: PMI(wi , wj ) = Pr(w ,wj ) log Pr(wi ) iPr(w , showing linguistically appealing j) results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora. Later work such as by Lin (1999) continued this tradition. Here we consider document, or discourse-level contexts, such as explored by Rosenfeld (1994) or Church (2000), and more recently by those such as Chambers and Jurafsky (2008) or Van Durme and Lall (2009b). In the spirit of recent work in randomized algorithms for large-scale HLT (such as by Ravichandran Conditional Random Sampling (CRS) Li and Church (2007) proposed CRS to approximate the contingency table between elements in a query, to be used in distributional similarity measures such as cosine similarity, correlation, and PMI. Central is the idea of the postings list, which is made up of the identifiers of each document that contains a given word or phrase. A set of such lists, one per type in th"
N12-1056,W10-2808,0,0.0271056,"Missing"
N12-1056,D08-1039,0,0.0213542,"Missing"
N12-1056,D09-1079,0,0.0604065,"Missing"
N12-1056,J07-3003,0,0.0601632,"= Pr(w ,wj ) log Pr(wi ) iPr(w , showing linguistically appealing j) results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora. Later work such as by Lin (1999) continued this tradition. Here we consider document, or discourse-level contexts, such as explored by Rosenfeld (1994) or Church (2000), and more recently by those such as Chambers and Jurafsky (2008) or Van Durme and Lall (2009b). In the spirit of recent work in randomized algorithms for large-scale HLT (such as by Ravichandran Conditional Random Sampling (CRS) Li and Church (2007) proposed CRS to approximate the contingency table between elements in a query, to be used in distributional similarity measures such as cosine similarity, correlation, and PMI. Central is the idea of the postings list, which is made up of the identifiers of each document that contains a given word or phrase. A set of such lists, one per type in the underlying vocabulary, is known as an inverted index. To reduce storage costs, a CRS truncates these lists, now called sketches, such that each sketch is no larger than some length parameter k. Formally, assume an ordered list of document identifie"
N12-1056,P99-1041,0,0.0606172,"work. This is demonstrated in the context of inducing Shankian script-like structures over news articles. 2 1 Background Introduction It has become common to model the distributional affinity between some word or phrase pair, (wi , wj ), as a function of co-occurance within some context boundary. Church and Hanks (1990) suggested pointwise mutual information: PMI(wi , wj ) = Pr(w ,wj ) log Pr(wi ) iPr(w , showing linguistically appealing j) results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora. Later work such as by Lin (1999) continued this tradition. Here we consider document, or discourse-level contexts, such as explored by Rosenfeld (1994) or Church (2000), and more recently by those such as Chambers and Jurafsky (2008) or Van Durme and Lall (2009b). In the spirit of recent work in randomized algorithms for large-scale HLT (such as by Ravichandran Conditional Random Sampling (CRS) Li and Church (2007) proposed CRS to approximate the contingency table between elements in a query, to be used in distributional similarity measures such as cosine similarity, correlation, and PMI. Central is the idea of the postings"
N12-1056,W00-1427,0,0.0118628,"sider more nuanced joint effects between more than two terms, more efficient methods would need to be considered. 3 Experiments Scripts A script, proposed by Schank (1975), is a form of Minsky-style frame that captures commonsense knowledge regarding typical events. For example, if a machine were to reason about eating at a restaurant, it should associate to this event: the exSetup Following Chambers and Jurafsky (2008), we extracted and lemmatized the verbs from the New York Times section of the Gigaword Corpus using the Stanford POS tagger (Toutanova et al., 2004) and the Morpha lemmatizer (Minnen et al., 2000). After filtering various POS tagger errors and setting a minimum document frequency (df) of 50, we went from a vocabulary of 94,803 words to 8,051.3 For various values of k we built sketches over 1,655,193 documents, for each resulting word type. 1 For example, assume some word wi that appears in documents d1 , d4 , d10 and d12 . The identifiers are then randomly permuted via π such that: d03 = d1 , d02 = d4 , d07 = d10 and d01 = d12 . Following permutation, the postings list for wi is made up of identifiers that map to the same underlying documents as before, but now in a different order. If"
N12-1056,N10-1021,0,0.0637609,"Missing"
N12-1056,P05-1077,0,0.0405007,"Missing"
N12-1056,T75-2023,0,0.241915,"rlying the co-occurrence framework of Chambers and Jurafsky was finding those verbs with high PMI. Starting with some initial element, chains were built greedily by adding the term, x, that maximized the average of the pairwise PMI between x and every term already in the chain: n Wn+1 = arg max W 1X pmi(W, Wj ) n j=1 By relying on the average pairwise PMI, they are making independence assumptions that are not always valid. In order to consider more nuanced joint effects between more than two terms, more efficient methods would need to be considered. 3 Experiments Scripts A script, proposed by Schank (1975), is a form of Minsky-style frame that captures commonsense knowledge regarding typical events. For example, if a machine were to reason about eating at a restaurant, it should associate to this event: the exSetup Following Chambers and Jurafsky (2008), we extracted and lemmatized the verbs from the New York Times section of the Gigaword Corpus using the Stanford POS tagger (Toutanova et al., 2004) and the Morpha lemmatizer (Minnen et al., 2000). After filtering various POS tagger errors and setting a minimum document frequency (df) of 50, we went from a vocabulary of 94,803 words to 8,051.3 F"
N12-1056,P08-1058,0,0.0605184,"Missing"
N12-1056,P07-1065,0,0.0810211,"Missing"
N12-1056,P10-2043,1,0.887729,"Missing"
N12-1056,N03-1033,0,\N,Missing
N12-1056,P11-1098,0,\N,Missing
N12-1078,S07-1002,0,0.0152215,"en considered in the past for constructing lexical semantic resources (e.g., (Snow et al., 2008; Akkaya et al., 2010; Parent and Eskenazi, 2010; Rumshisky, 2011)), word sense annotation is sensitive to subjectivity and usually achieves low agreement rate even among experts. Thus we first asked Turkers to re-annotate a sample of existing goldstandard data. With an eye towards costs saving, we also considered how many Turkers would be needed per item to produce results of sufficient quality. Turkers were presented sentences from the test portion of the word sense induction task of SemEval-2007 (Agirre and Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) senses. Two versions of the task were designed: 1. compare: given the same word in different sentences, tell whether their meaning is THE SAME , ALMOST THE SAME, UNLIKELY THE SAME or DIFFERENT, where the results were collapsed post-hoc into a binary same/different categorization; 2. sense map: map the meaning of a given word in a sentential context to its proper OntoNotes definition. For both tasks, 2, 599 examples were presented. We measure inter-coder agreement using Krippendorff’s Alpha (Krippendorff"
N12-1078,W10-0731,0,0.0126843,"fter first recognizing the underlying obligation sense. 621 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 621–625, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics as a reasonable process for acquiring large amounts of WSD labeled data, we go on to frame the experimental design, giving final results in Sec. 4. 2 Turker Reliability While Amazon’s Mechanical Turk (MTurk) has been been considered in the past for constructing lexical semantic resources (e.g., (Snow et al., 2008; Akkaya et al., 2010; Parent and Eskenazi, 2010; Rumshisky, 2011)), word sense annotation is sensitive to subjectivity and usually achieves low agreement rate even among experts. Thus we first asked Turkers to re-annotate a sample of existing goldstandard data. With an eye towards costs saving, we also considered how many Turkers would be needed per item to produce results of sufficient quality. Turkers were presented sentences from the test portion of the word sense induction task of SemEval-2007 (Agirre and Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) s"
N12-1078,J08-4004,0,0.0165177,"ng 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) senses. Two versions of the task were designed: 1. compare: given the same word in different sentences, tell whether their meaning is THE SAME , ALMOST THE SAME, UNLIKELY THE SAME or DIFFERENT, where the results were collapsed post-hoc into a binary same/different categorization; 2. sense map: map the meaning of a given word in a sentential context to its proper OntoNotes definition. For both tasks, 2, 599 examples were presented. We measure inter-coder agreement using Krippendorff’s Alpha (Krippendorff, 2004; Artstein and Poesio, 2008), where α ≥ 0.8 is considered to be reliable and 0.667 ≤ α < 0.8 allows for tentative conclusions. Two points emerge from Table 1: there were greater agreement rates for sense map than compare, and 3 Turkers were sufficient. 3 compare5 compare3 sense map5 sense map3 Experiment Design Data Selection We used two parallel corpora: the French-English 109 corpus (Callison-Burch et al., 2009) and the GALE Chinese-English corpus. 622 α-Turker 0.47 0.44 0.79 0.75 α-maj. 0.66 0.52 0.93 0.87 maj.-agr. 0.87 0.83 0.95 0.91 Table 1: MTurk result on testing Turker reliability. Krippendorff’s Alpha is used t"
N12-1078,P05-1074,1,0.816043,"signal its tax sense and devoir to signal its obligation sense. These French words were used as labels for different English senses. Similarly, in a cross-lingual WSD setting,1 Lefever et al. (2011) treated each English-foreign alignment as a so-called ParaSense, using it as a proxy for human labeled training data. Under the synonymy assumption, Diab and Resnik (2002) did word sense tagging by grouping together all English words that are translated into the same French word and by further enforcing that the majority sense for these English words was projected as the sense for the French word. Bannard and Callison-Burch (2005) applied the idea that French phrases aligned to the same English phrase are paraphrases in a system that induces paraphrases by pivoting through aligned foreign phrases. Based on this, and other successful prior work, it seems neither of the assumptions must hold universally. Therefore we investigate how often we might expect one or the other to dominate: we sample polysemous words from wide-domain {French,Chinese}-English corpora, and use Amazon’s Mechanical Turk (MTurk) to annotate word sense on the English side. We calculate empirical probabilities based on counting over the competing poly"
N12-1078,E09-1013,0,0.0598667,"Missing"
N12-1078,P02-1033,0,0.0342769,"ticles in which a researcher claimed success. Under the polysemy assumption, Gale et al. (1992) used French translations as English sense indicators in the task of WSD. For instance, for the English word duty, the French translation droit was taken to signal its tax sense and devoir to signal its obligation sense. These French words were used as labels for different English senses. Similarly, in a cross-lingual WSD setting,1 Lefever et al. (2011) treated each English-foreign alignment as a so-called ParaSense, using it as a proxy for human labeled training data. Under the synonymy assumption, Diab and Resnik (2002) did word sense tagging by grouping together all English words that are translated into the same French word and by further enforcing that the majority sense for these English words was projected as the sense for the French word. Bannard and Callison-Burch (2005) applied the idea that French phrases aligned to the same English phrase are paraphrases in a system that induces paraphrases by pivoting through aligned foreign phrases. Based on this, and other successful prior work, it seems neither of the assumptions must hold universally. Therefore we investigate how often we might expect one or t"
N12-1078,1992.tmi-1.9,0,0.582365,"d for both paraphrase induction and word sense disambiguation (WSD). Usually one of the following two assumptions is made for these tasks: 1. Polysemy If two different words in language A are aligned to the same word in language B, then the word in language B is polysemous. 2. Synonymy If two different words in language A are aligned to the same word in language B, then the two words in A are synonyms, and thus is not evidence of polysemy in B. Despite the alternate nature of these assumptions, both have associated articles in which a researcher claimed success. Under the polysemy assumption, Gale et al. (1992) used French translations as English sense indicators in the task of WSD. For instance, for the English word duty, the French translation droit was taken to signal its tax sense and devoir to signal its obligation sense. These French words were used as labels for different English senses. Similarly, in a cross-lingual WSD setting,1 Lefever et al. (2011) treated each English-foreign alignment as a so-called ParaSense, using it as a proxy for human labeled training data. Under the synonymy assumption, Diab and Resnik (2002) did word sense tagging by grouping together all English words that are t"
N12-1078,N06-2015,0,0.0256115,"Akkaya et al., 2010; Parent and Eskenazi, 2010; Rumshisky, 2011)), word sense annotation is sensitive to subjectivity and usually achieves low agreement rate even among experts. Thus we first asked Turkers to re-annotate a sample of existing goldstandard data. With an eye towards costs saving, we also considered how many Turkers would be needed per item to produce results of sufficient quality. Turkers were presented sentences from the test portion of the word sense induction task of SemEval-2007 (Agirre and Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) senses. Two versions of the task were designed: 1. compare: given the same word in different sentences, tell whether their meaning is THE SAME , ALMOST THE SAME, UNLIKELY THE SAME or DIFFERENT, where the results were collapsed post-hoc into a binary same/different categorization; 2. sense map: map the meaning of a given word in a sentential context to its proper OntoNotes definition. For both tasks, 2, 599 examples were presented. We measure inter-coder agreement using Krippendorff’s Alpha (Krippendorff, 2004; Artstein and Poesio, 2008), where α ≥ 0.8 is considered to be reliable and 0.667 ≤"
N12-1078,P11-2055,0,0.13284,"Missing"
N12-1078,W10-0703,0,0.017203,"g the underlying obligation sense. 621 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 621–625, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics as a reasonable process for acquiring large amounts of WSD labeled data, we go on to frame the experimental design, giving final results in Sec. 4. 2 Turker Reliability While Amazon’s Mechanical Turk (MTurk) has been been considered in the past for constructing lexical semantic resources (e.g., (Snow et al., 2008; Akkaya et al., 2010; Parent and Eskenazi, 2010; Rumshisky, 2011)), word sense annotation is sensitive to subjectivity and usually achieves low agreement rate even among experts. Thus we first asked Turkers to re-annotate a sample of existing goldstandard data. With an eye towards costs saving, we also considered how many Turkers would be needed per item to produce results of sufficient quality. Turkers were presented sentences from the test portion of the word sense induction task of SemEval-2007 (Agirre and Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) senses. Two versions of the"
N12-1078,W11-0409,0,0.016078,"sense. 621 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 621–625, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics as a reasonable process for acquiring large amounts of WSD labeled data, we go on to frame the experimental design, giving final results in Sec. 4. 2 Turker Reliability While Amazon’s Mechanical Turk (MTurk) has been been considered in the past for constructing lexical semantic resources (e.g., (Snow et al., 2008; Akkaya et al., 2010; Parent and Eskenazi, 2010; Rumshisky, 2011)), word sense annotation is sensitive to subjectivity and usually achieves low agreement rate even among experts. Thus we first asked Turkers to re-annotate a sample of existing goldstandard data. With an eye towards costs saving, we also considered how many Turkers would be needed per item to produce results of sufficient quality. Turkers were presented sentences from the test portion of the word sense induction task of SemEval-2007 (Agirre and Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) senses. Two versions of the task were designed"
N12-1078,D08-1027,0,0.0213405,"Missing"
N12-1078,W09-0401,1,\N,Missing
N13-1092,P05-1074,1,0.55111,"riety of natural language processing applications. Past paraphrase collections include automatically derived resources like DIRT (Lin and Pantel, 2001), the MSR paraphrase corpus and phrase table (Dolan et al., 2004; Quirk et al., 2004), among others. Although several groups have independently extracted paraphrases using Bannard and CallisonBurch (2005)’s bilingual pivoting technique (see Zhou et al. (2006), Riezler et al. (2007), Snover et al. (2010), among others), there has never been an official release of this resource. Extracting Paraphrases from Bitexts To extract paraphrases we follow Bannard and Callison-Burch (2005)’s bilingual pivoting method. The intuition is that two English strings e1 and e2 that translate to the same foreign string f can be assumed to have the same meaning. We can thus pivot over f and extract he1 , e2 i as a pair of paraphrases, as illustrated in Figure 1. The method extracts a diverse set of paraphrases. For thrown into jail, it extracts arrested, detained, imprisoned, incarcerated, jailed, locked up, taken into custody, and thrown into prison, along with a set of incorrect/noisy paraphrases that have different syntactic types or that are due to misalignments. For PPDB, we formula"
N13-1092,P11-1062,0,0.00591233,"and languages supported. Furthermore, we intend to improve paraphrase scoring by incorporating additional sources of information, as well as by better utilizing information present in the data, like domain or topic. We will also address points of refinement such as handling of phrase ambiguity, and effects specific to individual pivot languages. Our aim is for PPDB to be a continuously updated and improving resource. Finally, we will explore extensions to PPDB to include aspects of related large-scale resources such as lexical-semantic hierarchies (Snow et al., 2006), textual inference rules (Berant et al., 2011), relational patterns (Nakashole et al., 2012), and (lexical) conceptual networks (Navigli and Ponzetto, 2012). Acknowledgements We would like to thank Frank Ferraro for his Propbank processing tools. This material is based on research sponsored by the NSF under grant IIS-1249516 and DARPA under agreement number FA8750-13-2-0017 (the DEFT program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or end"
N13-1092,P08-1077,0,0.016869,"-sets that rely on dependency and constituency parses, part-of-speech tags, or lemmatization have been proposed in work such as by Church and Hanks (1991) and Lin and Pantel (2001). For instance, a phrase is described by the various syntactic relations such as: “what verbs have this phrase as the subject?”, or “what adjectives modify this phrase?”. Other work has used simpler n-gram features, e.g. “what words or bigrams have we seen to the left of this phrase?”. A substantial body of work has focussed on using this type of feature-set for a variety of purposes in NLP (Lapata and Keller, 2005; Bhagat and Ravichandran, 2008; Lin et al., 2010; Van Durme and Lall, 2010). For PPDB, we compute n-gram-based context signatures for the 200 million most frequent phrases in the Google n-gram corpus (Brants and Franz, 2006; Lin et al., 2010), and richer linguistic signatures for 175 million phrases in the Annotated Gigaword corpus (Napoles et al., 2012). Our features extend beyond those previously used in the work by Ganitkevitch et al. (2012). They are: 760 • Incoming and outgoing (wrt. the phrase) dependency link features, labeled with the corresponding lexical item, lemmata and POS. • Syntactic features for any constit"
N13-1092,W11-2504,1,0.357316,"Missing"
N13-1092,P05-1033,0,0.0143651,"me foreign string f can be assumed to have the same meaning. We can thus pivot over f and extract he1 , e2 i as a pair of paraphrases, as illustrated in Figure 1. The method extracts a diverse set of paraphrases. For thrown into jail, it extracts arrested, detained, imprisoned, incarcerated, jailed, locked up, taken into custody, and thrown into prison, along with a set of incorrect/noisy paraphrases that have different syntactic types or that are due to misalignments. For PPDB, we formulate our paraphrase collection as a weighted synchronous context-free grammar (SCFG) (Aho and Ullman, 1972; Chiang, 2005) 1 Freely available at http://paraphrase.org. 758 Proceedings of NAACL-HLT 2013, pages 758–764, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics ... 5 farmers were thrown into jail The paraphrase rules obtained using this method are capable of making well-formed generalizations of meaning-preserving rewrites in English. For instance, we extract the following example paraphrase, capturing the English possessive rule: in Ireland ... ... fünf Landwirte festgenommen , weil ... ... oder wurden festgenommen , gefoltert ... NP → the NP 1 of NNS 2 |the NNS2 ’s NP 1 ."
N13-1092,C08-1018,0,0.0578526,"utational Linguistics ... 5 farmers were thrown into jail The paraphrase rules obtained using this method are capable of making well-formed generalizations of meaning-preserving rewrites in English. For instance, we extract the following example paraphrase, capturing the English possessive rule: in Ireland ... ... fünf Landwirte festgenommen , weil ... ... oder wurden festgenommen , gefoltert ... NP → the NP 1 of NNS 2 |the NNS2 ’s NP 1 . ... or have been imprisoned , tortured ... Figure 1: Phrasal paraphrases are extracted via bilingual pivoting. with syntactic nonterminal labels, similar to Cohn and Lapata (2008) and Ganitkevitch et al. (2011). An SCFG rule has the form: def r = C → hf, e, ∼, ϕ ~ i, where the left-hand side of the rule, C, is a nonterminal and the right-hand sides f and e are strings of terminal and nonterminal symbols. There is a one-toone correspondence, ∼, between the nonterminals in f and e: each nonterminal symbol in f has to also appear in e. Following Zhao et al. (2008), each rule r is annotated with a vector of feature functions ϕ ~ = {ϕ1 ...ϕN } which are combined in a log-linear model (with weights ~λ) to compute the cost of applying r: cost(r) = − N X λi log ϕi . (1) i=1 To"
N13-1092,C04-1051,0,0.862531,"dition to the paraphrase collection itself, we provide tools to filter PPDB to only retain high precision paraphrases, scripts to limit the collection to phrasal or lexical paraphrases (synonyms), and software that enables users to extract paraphrases for languages other than English. 2 Introduction Paraphrases, i.e. differing textual realizations of the same meaning, have proven useful for a wide variety of natural language processing applications. Past paraphrase collections include automatically derived resources like DIRT (Lin and Pantel, 2001), the MSR paraphrase corpus and phrase table (Dolan et al., 2004; Quirk et al., 2004), among others. Although several groups have independently extracted paraphrases using Bannard and CallisonBurch (2005)’s bilingual pivoting technique (see Zhou et al. (2006), Riezler et al. (2007), Snover et al. (2010), among others), there has never been an official release of this resource. Extracting Paraphrases from Bitexts To extract paraphrases we follow Bannard and Callison-Burch (2005)’s bilingual pivoting method. The intuition is that two English strings e1 and e2 that translate to the same foreign string f can be assumed to have the same meaning. We can thus piv"
N13-1092,eisele-chen-2010-multiun,0,0.097034,"Missing"
N13-1092,D11-1108,1,0.723807,"Missing"
N13-1092,S12-1034,1,0.722511,"Missing"
N13-1092,kingsbury-palmer-2002-treebank,0,0.282654,"st S. This allows us to apply sophisticated paraphrases to the predicate while capturing its arguments in a generalized fashion. tated with distributional similarity scores based on lexical features collected from the Spanish portion of the multilingual release of the Google n-gram corpus (Brants and Franz, 2009), and the Spanish Gigaword corpus (Mendonca et al., 2009). Table 2 gives a breakdown of PPDB:Spa. 6 Analysis To estimate the usefulness of PPDB as a resource for tasks like semantic role labeling or parsing, we analyze its coverage of Propbank predicates and predicate-argument tuples (Kingsbury and Palmer, 2002). We use the Penn Treebank (Marcus et al., 1993) to map Propbank annotations to patterns which allow us to search PPDB:Eng for paraphrases that match the annotated predicate. Figure 3 illus50 Avg. Score 0 -30 Coverage 100 0.5 0.8 PP / Type Coverage 150 0 -25 -20 -15 -10 -5 0 5 140 120 100 0.6 80 0.4 60 40 0.2 20 3 1 -30 160 Relation Tokens Covered Paraphrases / Type Relation Types Covered Paraphrases / Type 1 1 0 -30 -25 -20 -15 -10 -5 0 0 -25 -20 -15 -10 -5 0 Pruning Threshold Pruning Threshold (a) PPDB:Eng coverage of Propbank predicates (top), and average human judgment score (bottom) for v"
N13-1092,W07-0733,0,0.0117205,"Missing"
N13-1092,2005.mtsummit-papers.11,0,0.265202,"Missing"
N13-1092,J10-4005,0,0.00651194,"strings of terminal and nonterminal symbols. There is a one-toone correspondence, ∼, between the nonterminals in f and e: each nonterminal symbol in f has to also appear in e. Following Zhao et al. (2008), each rule r is annotated with a vector of feature functions ϕ ~ = {ϕ1 ...ϕN } which are combined in a log-linear model (with weights ~λ) to compute the cost of applying r: cost(r) = − N X λi log ϕi . (1) i=1 To create a syntactic paraphrase grammar we first extract a foreign-to-English translation grammar from a bilingual parallel corpus, using techniques from syntactic machine translation (Koehn, 2010). Then, for each pair of translation rules where the left-hand side C and foreign string f match: def r1 = C → hf, e1 , ∼1 , ϕ ~ 1i def r2 = C → hf, e2 , ∼2 , ϕ ~ 2 i, we pivot over f to create a paraphrase rule rp : def rp = C → he1 , e2 , ∼p , ϕ ~ p i, with a combined nonterminal correspondency function ∼p . Note that the common source side f implies that e1 and e2 share the same set of nonterminal symbols. 759 The paraphrase feature vector ϕ ~ p is computed from the translation feature vectors ϕ ~ 1 and ϕ ~ 2 by following the pivoting idea. For instance, we estimate the conditional paraphra"
N13-1092,lin-etal-2010-new,0,0.0125111,"nd constituency parses, part-of-speech tags, or lemmatization have been proposed in work such as by Church and Hanks (1991) and Lin and Pantel (2001). For instance, a phrase is described by the various syntactic relations such as: “what verbs have this phrase as the subject?”, or “what adjectives modify this phrase?”. Other work has used simpler n-gram features, e.g. “what words or bigrams have we seen to the left of this phrase?”. A substantial body of work has focussed on using this type of feature-set for a variety of purposes in NLP (Lapata and Keller, 2005; Bhagat and Ravichandran, 2008; Lin et al., 2010; Van Durme and Lall, 2010). For PPDB, we compute n-gram-based context signatures for the 200 million most frequent phrases in the Google n-gram corpus (Brants and Franz, 2006; Lin et al., 2010), and richer linguistic signatures for 175 million phrases in the Annotated Gigaword corpus (Napoles et al., 2012). Our features extend beyond those previously used in the work by Ganitkevitch et al. (2012). They are: 760 • Incoming and outgoing (wrt. the phrase) dependency link features, labeled with the corresponding lexical item, lemmata and POS. • Syntactic features for any constituents governing th"
N13-1092,J93-2004,0,0.0536204,"to the predicate while capturing its arguments in a generalized fashion. tated with distributional similarity scores based on lexical features collected from the Spanish portion of the multilingual release of the Google n-gram corpus (Brants and Franz, 2009), and the Spanish Gigaword corpus (Mendonca et al., 2009). Table 2 gives a breakdown of PPDB:Spa. 6 Analysis To estimate the usefulness of PPDB as a resource for tasks like semantic role labeling or parsing, we analyze its coverage of Propbank predicates and predicate-argument tuples (Kingsbury and Palmer, 2002). We use the Penn Treebank (Marcus et al., 1993) to map Propbank annotations to patterns which allow us to search PPDB:Eng for paraphrases that match the annotated predicate. Figure 3 illus50 Avg. Score 0 -30 Coverage 100 0.5 0.8 PP / Type Coverage 150 0 -25 -20 -15 -10 -5 0 5 140 120 100 0.6 80 0.4 60 40 0.2 20 3 1 -30 160 Relation Tokens Covered Paraphrases / Type Relation Types Covered Paraphrases / Type 1 1 0 -30 -25 -20 -15 -10 -5 0 0 -25 -20 -15 -10 -5 0 Pruning Threshold Pruning Threshold (a) PPDB:Eng coverage of Propbank predicates (top), and average human judgment score (bottom) for varying pruning thresholds. (b) PPDB:Eng’s covera"
N13-1092,D12-1104,0,0.0522494,"tend to improve paraphrase scoring by incorporating additional sources of information, as well as by better utilizing information present in the data, like domain or topic. We will also address points of refinement such as handling of phrase ambiguity, and effects specific to individual pivot languages. Our aim is for PPDB to be a continuously updated and improving resource. Finally, we will explore extensions to PPDB to include aspects of related large-scale resources such as lexical-semantic hierarchies (Snow et al., 2006), textual inference rules (Berant et al., 2011), relational patterns (Nakashole et al., 2012), and (lexical) conceptual networks (Navigli and Ponzetto, 2012). Acknowledgements We would like to thank Frank Ferraro for his Propbank processing tools. This material is based on research sponsored by the NSF under grant IIS-1249516 and DARPA under agreement number FA8750-13-2-0017 (the DEFT program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA or the U.S. Government. Ref"
N13-1092,W12-3018,1,0.636441,"Missing"
N13-1092,W04-3219,0,0.206887,"rase collection itself, we provide tools to filter PPDB to only retain high precision paraphrases, scripts to limit the collection to phrasal or lexical paraphrases (synonyms), and software that enables users to extract paraphrases for languages other than English. 2 Introduction Paraphrases, i.e. differing textual realizations of the same meaning, have proven useful for a wide variety of natural language processing applications. Past paraphrase collections include automatically derived resources like DIRT (Lin and Pantel, 2001), the MSR paraphrase corpus and phrase table (Dolan et al., 2004; Quirk et al., 2004), among others. Although several groups have independently extracted paraphrases using Bannard and CallisonBurch (2005)’s bilingual pivoting technique (see Zhou et al. (2006), Riezler et al. (2007), Snover et al. (2010), among others), there has never been an official release of this resource. Extracting Paraphrases from Bitexts To extract paraphrases we follow Bannard and Callison-Burch (2005)’s bilingual pivoting method. The intuition is that two English strings e1 and e2 that translate to the same foreign string f can be assumed to have the same meaning. We can thus pivot over f and extract"
N13-1092,P07-1059,0,0.0790247,"enables users to extract paraphrases for languages other than English. 2 Introduction Paraphrases, i.e. differing textual realizations of the same meaning, have proven useful for a wide variety of natural language processing applications. Past paraphrase collections include automatically derived resources like DIRT (Lin and Pantel, 2001), the MSR paraphrase corpus and phrase table (Dolan et al., 2004; Quirk et al., 2004), among others. Although several groups have independently extracted paraphrases using Bannard and CallisonBurch (2005)’s bilingual pivoting technique (see Zhou et al. (2006), Riezler et al. (2007), Snover et al. (2010), among others), there has never been an official release of this resource. Extracting Paraphrases from Bitexts To extract paraphrases we follow Bannard and Callison-Burch (2005)’s bilingual pivoting method. The intuition is that two English strings e1 and e2 that translate to the same foreign string f can be assumed to have the same meaning. We can thus pivot over f and extract he1 , e2 i as a pair of paraphrases, as illustrated in Figure 1. The method extracts a diverse set of paraphrases. For thrown into jail, it extracts arrested, detained, imprisoned, incarcerated, j"
N13-1092,P06-1101,0,0.015993,"ion’s coverage with regard to both data size and languages supported. Furthermore, we intend to improve paraphrase scoring by incorporating additional sources of information, as well as by better utilizing information present in the data, like domain or topic. We will also address points of refinement such as handling of phrase ambiguity, and effects specific to individual pivot languages. Our aim is for PPDB to be a continuously updated and improving resource. Finally, we will explore extensions to PPDB to include aspects of related large-scale resources such as lexical-semantic hierarchies (Snow et al., 2006), textual inference rules (Berant et al., 2011), relational patterns (Nakashole et al., 2012), and (lexical) conceptual networks (Navigli and Ponzetto, 2012). Acknowledgements We would like to thank Frank Ferraro for his Propbank processing tools. This material is based on research sponsored by the NSF under grant IIS-1249516 and DARPA under agreement number FA8750-13-2-0017 (the DEFT program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be inter"
N13-1092,P10-2043,1,0.223784,"Missing"
N13-1092,P08-1116,0,0.0561042,"ert ... NP → the NP 1 of NNS 2 |the NNS2 ’s NP 1 . ... or have been imprisoned , tortured ... Figure 1: Phrasal paraphrases are extracted via bilingual pivoting. with syntactic nonterminal labels, similar to Cohn and Lapata (2008) and Ganitkevitch et al. (2011). An SCFG rule has the form: def r = C → hf, e, ∼, ϕ ~ i, where the left-hand side of the rule, C, is a nonterminal and the right-hand sides f and e are strings of terminal and nonterminal symbols. There is a one-toone correspondence, ∼, between the nonterminals in f and e: each nonterminal symbol in f has to also appear in e. Following Zhao et al. (2008), each rule r is annotated with a vector of feature functions ϕ ~ = {ϕ1 ...ϕN } which are combined in a log-linear model (with weights ~λ) to compute the cost of applying r: cost(r) = − N X λi log ϕi . (1) i=1 To create a syntactic paraphrase grammar we first extract a foreign-to-English translation grammar from a bilingual parallel corpus, using techniques from syntactic machine translation (Koehn, 2010). Then, for each pair of translation rules where the left-hand side C and foreign string f match: def r1 = C → hf, e1 , ∼1 , ϕ ~ 1i def r2 = C → hf, e2 , ∼2 , ϕ ~ 2 i, we pivot over f to creat"
N13-1092,N06-1057,0,0.00512664,", and software that enables users to extract paraphrases for languages other than English. 2 Introduction Paraphrases, i.e. differing textual realizations of the same meaning, have proven useful for a wide variety of natural language processing applications. Past paraphrase collections include automatically derived resources like DIRT (Lin and Pantel, 2001), the MSR paraphrase corpus and phrase table (Dolan et al., 2004; Quirk et al., 2004), among others. Although several groups have independently extracted paraphrases using Bannard and CallisonBurch (2005)’s bilingual pivoting technique (see Zhou et al. (2006), Riezler et al. (2007), Snover et al. (2010), among others), there has never been an official release of this resource. Extracting Paraphrases from Bitexts To extract paraphrases we follow Bannard and Callison-Burch (2005)’s bilingual pivoting method. The intuition is that two English strings e1 and e2 that translate to the same foreign string f can be assumed to have the same meaning. We can thus pivot over f and extract he1 , e2 i as a pair of paraphrases, as illustrated in Figure 1. The method extracts a diverse set of paraphrases. For thrown into jail, it extracts arrested, detained, impr"
N13-1092,J90-1003,0,\N,Missing
N13-1092,steinberger-etal-2006-jrc,0,\N,Missing
N13-1092,W09-0401,1,\N,Missing
N13-1106,P08-1081,0,0.00871982,"Missing"
N13-1106,N03-2010,0,0.00784703,"type as clearly as how much does. Some extra features are designed for what/which questions per required answer types. The question dependency tree is analyzed and the Lexical Answer Type (LAT) is extracted. The following are some examples of LAT for what questions: • color: what is Crips’ gang color? • animal: what kind of animal is an agouti? The extra LAT=? feature is also used with chunking features for what/which questions. There is significant prior work in building specialized templates or classifiers for labeling question types (Hermjakob, 2001; Li and Roth, 2002; Zhang and Lee, 2003; Hacioglu and Ward, 2003; Metzler and Croft, 2005; Blunsom et al., 2006; Moschitti et al., 2007). We designed our shallow question type features based on the intuitions of these prior work, with the goal of having a relatively compact approach that still extracts useful predictive signal. One possible drawback, however, is that if an LAT is not observed during training but shows up in testing, the sequence tagger would not know which answer type to associate with the question. In this case it falls back to the more general qword=? feature and will most likely pick the type of answers that are mostly associated with w"
N13-1106,N10-1145,0,0.87532,"ake public (together with the software) to the community.3 Related prior work is interspersed throughout the paper. Feature distance renNoun renVerb renOther insN, insV, insPunc, insDet, insOtherPos delN, delV, ... ins{N,V,P}Mod insSub, insObj insOtherRel delNMod, ... renNMod, ... XEdits alignNodes, alignNum, alignN, alignV, alignProper Tree Edit Distance Model Tree Edit Distance (§2.1) models have been shown effective in a variety of applications, including textual entailment, paraphrase identification, answer ranking and information retrieval (Reis et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Augsten et al., 2010). We chose the variant proposed by Heilman and Smith (2010), inspired by its simplicity, generality, and effectiveness. Our approach differs from those authors in their reliance on a greedy search routine to make use of a complex tree kernel. With speed a consideration, we opted for the dynamic-programming solution of Zhang and Shasha (1989) (§2.1). We added new lexicalsemantic features §(2.2) to the model and then evaluated our implementation on the QASR task, showing strong results §(2.3). 859 # edits inserting a noun, verb, punctuation mark, determiner or other POS ty"
N13-1106,W01-1203,0,0.0196754,"ow questions, even though they do not indicate an answer type as clearly as how much does. Some extra features are designed for what/which questions per required answer types. The question dependency tree is analyzed and the Lexical Answer Type (LAT) is extracted. The following are some examples of LAT for what questions: • color: what is Crips’ gang color? • animal: what kind of animal is an agouti? The extra LAT=? feature is also used with chunking features for what/which questions. There is significant prior work in building specialized templates or classifiers for labeling question types (Hermjakob, 2001; Li and Roth, 2002; Zhang and Lee, 2003; Hacioglu and Ward, 2003; Metzler and Croft, 2005; Blunsom et al., 2006; Moschitti et al., 2007). We designed our shallow question type features based on the intuitions of these prior work, with the goal of having a relatively compact approach that still extracts useful predictive signal. One possible drawback, however, is that if an LAT is not observed during training but shows up in testing, the sequence tagger would not know which answer type to associate with the question. In this case it falls back to the more general qword=? feature and will most"
N13-1106,C02-1150,0,0.249239,"n though they do not indicate an answer type as clearly as how much does. Some extra features are designed for what/which questions per required answer types. The question dependency tree is analyzed and the Lexical Answer Type (LAT) is extracted. The following are some examples of LAT for what questions: • color: what is Crips’ gang color? • animal: what kind of animal is an agouti? The extra LAT=? feature is also used with chunking features for what/which questions. There is significant prior work in building specialized templates or classifiers for labeling question types (Hermjakob, 2001; Li and Roth, 2002; Zhang and Lee, 2003; Hacioglu and Ward, 2003; Metzler and Croft, 2005; Blunsom et al., 2006; Moschitti et al., 2007). We designed our shallow question type features based on the intuitions of these prior work, with the goal of having a relatively compact approach that still extracts useful predictive signal. One possible drawback, however, is that if an LAT is not observed during training but shows up in testing, the sequence tagger would not know which answer type to associate with the question. In this case it falls back to the more general qword=? feature and will most likely pick the typ"
N13-1106,P02-1054,0,0.0315697,"Missing"
N13-1106,P05-1012,0,0.00925047,"against the TREC answer pattern (in the form of Perl regular expressions). If a sentence matched, then it was deemed a (noisy) positive example. Finally, TRAIN, DEV and TEST were manually corrected for errors. Those authors decided to limit candidate source sen861 tences to be no longer than 40 words.7 Keeping with prior work, those questions with only positive or negative examples were removed, leaving 94 of the original 100 questions for evaluation. The data was processed by Wang et al. (2007) with the following tool chain: POS tags via MXPOST (Ratnaparkhi, 1996); parse trees via MSTParser (McDonald et al., 2005) with 12 coarsegrained dependency relation labels; and named entities via Identifinder (Bikel et al., 1999). Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) are reported in Table 3. Our implementation gives state of the art performance, and is furthered improved by our inclusion of semantic features drawn from WordNet.8 3 CRF with TED for Answer Extraction In this section we move from ranking source sentences, to the next QA stage: answer extraction. Given our competitive TED-based alignment model, the most obvious solution to extraction would be to report those spans aligned from"
N13-1106,P07-1098,0,0.0123631,"hat/which questions per required answer types. The question dependency tree is analyzed and the Lexical Answer Type (LAT) is extracted. The following are some examples of LAT for what questions: • color: what is Crips’ gang color? • animal: what kind of animal is an agouti? The extra LAT=? feature is also used with chunking features for what/which questions. There is significant prior work in building specialized templates or classifiers for labeling question types (Hermjakob, 2001; Li and Roth, 2002; Zhang and Lee, 2003; Hacioglu and Ward, 2003; Metzler and Croft, 2005; Blunsom et al., 2006; Moschitti et al., 2007). We designed our shallow question type features based on the intuitions of these prior work, with the goal of having a relatively compact approach that still extracts useful predictive signal. One possible drawback, however, is that if an LAT is not observed during training but shows up in testing, the sequence tagger would not know which answer type to associate with the question. In this case it falls back to the more general qword=? feature and will most likely pick the type of answers that are mostly associated with what questions in training. Edit script Our TED module produces an edit t"
N13-1106,W96-0213,0,0.0416997,"the task collection, which were then compared against the TREC answer pattern (in the form of Perl regular expressions). If a sentence matched, then it was deemed a (noisy) positive example. Finally, TRAIN, DEV and TEST were manually corrected for errors. Those authors decided to limit candidate source sen861 tences to be no longer than 40 words.7 Keeping with prior work, those questions with only positive or negative examples were removed, leaving 94 of the original 100 questions for evaluation. The data was processed by Wang et al. (2007) with the following tool chain: POS tags via MXPOST (Ratnaparkhi, 1996); parse trees via MSTParser (McDonald et al., 2005) with 12 coarsegrained dependency relation labels; and named entities via Identifinder (Bikel et al., 1999). Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) are reported in Table 3. Our implementation gives state of the art performance, and is furthered improved by our inclusion of semantic features drawn from WordNet.8 3 CRF with TED for Answer Extraction In this section we move from ranking source sentences, to the next QA stage: answer extraction. Given our competitive TED-based alignment model, the most obvious solution to extr"
N13-1106,N03-2029,0,0.0254606,"n an aligned adjective. Alignment distance We observed that a candidate answer often appears close to an aligned word (i.e., answer tokens tend to be located “nearby” portions of text that align across the pair), especially in compound noun constructions, restrictive clauses, preposition phrases, etc. For instance, in the following pair, the answer Limp Bizkit comes from the leading compound noun: 863 • What is the name of Durst ’s group? • Limp Bizkit lead singer Fred Durst did a lot ... Past work has designed large numbers of specific templates aimed at these constructions (Soubbotin, 2001; Ravichandran et al., 2003; Clark et al., 2003; Sneiders, 2002). Here we use a single general feature that we expect to pick up much of this signal, without the significant feature engineering. Thus we incorporated a simple feature to roughly model this phenomenon. It is defined as the distance to the nearest aligned nonstop word in the original word order. In the above example, the only aligned nonstop word is Durst. Then this nearest alignment distance feature for the word Limp is: nearest dist to align(Limp):5 This is the only integer-valued feature. All other features are binary-valued. Note this feature does not s"
N13-1106,W06-3104,0,0.00950297,"ractice this gave stopwords “too much say” in guiding the overall edit sequence. The resultant system is fast in practice, processing 10,000 pre-parsed tree pairs per second on a contemporary machine.6 2.2 TED for Sentence Ranking The task of Question Answer Sentence Ranking (QASR) takes a question and a set of source sentences, returning a list sorted by the probability likelihood that each sentence contains an appropriate answer. Prior work in this includes that of: Punyakanok et al. (2004), based on mapping syntactic dependency trees; Wang et al. (2007) utilizing Quasi-Synchronous Grammar (Smith and Eisner, 2006); Heilman and Smith (2010) using TED; and Shima et al. (2008), Ding et al. (2008) and Wang and Manning (2010), who each employed a CRF in various ways. Wang et al. (2007) made their dataset public, which we use here for system validation. To date, models based on TED have shown the best performance for this task. Our implementation follows Heilman and Smith (2010), with the addition of 15 new features beyond their original 33 (see Table 1). Based on results 6 In later tasks, feature extraction and decoding will slow down the system, but the final system was still able to process 200 pairs per"
N13-1106,C10-1131,0,0.743788,"Missing"
N13-1106,D07-1003,0,0.439691,"erences, tagging or parsing errors. 860 tent terms. In practice this gave stopwords “too much say” in guiding the overall edit sequence. The resultant system is fast in practice, processing 10,000 pre-parsed tree pairs per second on a contemporary machine.6 2.2 TED for Sentence Ranking The task of Question Answer Sentence Ranking (QASR) takes a question and a set of source sentences, returning a list sorted by the probability likelihood that each sentence contains an appropriate answer. Prior work in this includes that of: Punyakanok et al. (2004), based on mapping syntactic dependency trees; Wang et al. (2007) utilizing Quasi-Synchronous Grammar (Smith and Eisner, 2006); Heilman and Smith (2010) using TED; and Shima et al. (2008), Ding et al. (2008) and Wang and Manning (2010), who each employed a CRF in various ways. Wang et al. (2007) made their dataset public, which we use here for system validation. To date, models based on TED have shown the best performance for this task. Our implementation follows Heilman and Smith (2010), with the addition of 15 new features beyond their original 33 (see Table 1). Based on results 6 In later tasks, feature extraction and decoding will slow down the system,"
N13-1121,W12-2108,1,0.801101,"Our readily-replicable approach and publiclyreleased clusters are shown to be remarkably effective and versatile, substantially outperforming state-of-the-art approaches and human accuracy on each of the tasks studied. 1 Here, we propose and evaluate classifiers that better exploit the attributes that users explicitly provide in their user profiles, such as names (e.g., first names like Mary, last names like Smith) and locations (e.g., Brasil). Such attributes have previously been used as “profile features” in supervised user classifiers (Pennacchiotti and Popescu, 2011; Burger et al., 2011; Bergsma et al., 2012). There are several motivations for exploiting these data. Often the only information available for a user is a name or location (e.g. for a new user account). Profiles also provide an orthogonal or complementary source of information to a user’s social network and textual content; gains based on profiles alone should therefore add to gains based on other data. The decisions of profile-based classifiers could also be used to bootstrap training data for other classifiers that use complementary features. Introduction There is growing interest in automatically classifying users in social media by"
N13-1121,N10-1102,0,0.062149,"ntry: 53 possible countries United States courtland dante United States tinas twin Brazil thamires gomez Denmark marte clason Lang. ID: 9 confusable languages Bulgarian valentina getova Russian borisenko yana Bulgarian NONE Ukrainian andriy kupyna Farsi kambiz barahouei Urdu musadiq sanwal Ethnicity: 13 European ethnicities German dennis hustadt Dutch bernhard hofstede French david coste Swedish mattias bjarsmyr Portuguese helder costa Race: black or white black kerry swain black darrell foskey white ty j larocca black james n jones white sean p farrell of-the-art in detecting name ethnicity (Bhargava and Kondrak, 2010). We add special begin/end characters to the attributes to mark the prefix and suffix positions. We also use a smoothed log-count; we found this to be most effective in preliminary work. Cluster Features (Clus) indicate the soft-cluster memberships of the attributes. We have features for the top-2, 5, and 20 most similar clusters in the C 50 , C 200 , and C 1000 clusterings, respectively. Like Lin and Wu (2009), we “side-step the matter of choosing the optimal value k in k-means” by using features from clusterings at different granularities. Our feature dimensions correspond to cluster IDs; fe"
N13-1121,J92-4003,0,0.280084,"Missing"
N13-1121,D11-1120,0,0.727262,"Bergsma, Mark Dredze, Benjamin Van Durme, Theresa Wilson, David Yarowsky Department of Computer Science and Human Language Technology Center of Excellence Johns Hopkins University Baltimore, MD 21218, USA shane.a.bergsma@gmail.com, mdredze@cs.jhu.edu, vandurme@cs.jhu.edu, taw@jhu.edu, yarowsky@cs.jhu.edu Abstract and Dredze, 2011), and sociolinguistic phenomena (Eisenstein et al., 2011). Classifiers for user properties often rely on information from a user’s social network (Jernigan and Mistree, 2009; Sadilek et al., 2012) or the textual content they generate (Pennacchiotti and Popescu, 2011; Burger et al., 2011). Hidden properties of social media users, such as their ethnicity, gender, and location, are often reflected in their observed attributes, such as their first and last names. Furthermore, users who communicate with each other often have similar hidden properties. We propose an algorithm that exploits these insights to cluster the observed attributes of hundreds of millions of Twitter users. Attributes such as user names are grouped together if users with those names communicate with other similar users. We separately cluster millions of unique first names, last names, and userprovided locatio"
N13-1121,J90-1003,0,0.0521202,"rm as well with 30 training examples as Ngm features do with 1000. data; thousands of training examples are needed for Ngm to rival the performance of Clus using only a handful. Since labeled data is generally expensive to obtain or in short supply, our method for exploiting unlabeled Twitter data can both save money and improve top-end performance. 7 Geolocation by Association There is a tradition in computational linguistics of grouping words both by the similarity of their context vectors (Hindle, 1990; Pereira et al., 1993; Lin, 1998) and directly by their statistical association in text (Church and Hanks, 1990; Brown et al., 1992). While the previous sections explored clusters built by vector similarity, we now explore a direct application of our attribute association data (§2). We wish to use this data to improve an existing Twitter geolocation system based on user profile locations. The system operates as follows: 1) normalize user-provided locations using a set of regular expressions (e.g. remove extra spacing, punctuation); 2) look up the normalized location in an alias list; 3) if found, map the alias to a unique string (target location), corresponding to a structured location object that incl"
N13-1121,D10-1124,0,0.208983,"Missing"
N13-1121,P11-1137,0,0.267211,"Missing"
N13-1121,P09-1080,1,0.806767,"ies, and (2) to predict user properties based on friendships. Friendship prediction systems (e.g. Facebook’s friend suggestion tool) use features such as whether both people are computer science majors (Taskar et al., 2003) or whether both are at the same location (Crandall et al., 2010; Sadilek et al., 2012). The inverse problem has been explored in the prediction of a user’s location given the location of their peers (Backstrom et al., 2010; Cho et al., 2011; Sadilek et al., 2012). Jernigan and Mistree (2009) predict a user’s sexuality based on the sexuality of their Facebook friends, while Garera and Yarowsky (2009) predict a user’s gender partly based on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al."
N13-1121,P90-1034,0,0.466102,"75 70 65 60 10 100 1000 10000 Number of training examples Figure 1: Learning curve on Race: Clus perform as well with 30 training examples as Ngm features do with 1000. data; thousands of training examples are needed for Ngm to rival the performance of Clus using only a handful. Since labeled data is generally expensive to obtain or in short supply, our method for exploiting unlabeled Twitter data can both save money and improve top-end performance. 7 Geolocation by Association There is a tradition in computational linguistics of grouping words both by the similarity of their context vectors (Hindle, 1990; Pereira et al., 1993; Lin, 1998) and directly by their statistical association in text (Church and Hanks, 1990; Brown et al., 1992). While the previous sections explored clusters built by vector similarity, we now explore a direct application of our attribute association data (§2). We wish to use this data to improve an existing Twitter geolocation system based on user profile locations. The system operates as follows: 1) normalize user-provided locations using a set of regular expressions (e.g. remove extra spacing, punctuation); 2) look up the normalized location in an alias list; 3) if fo"
N13-1121,W10-1908,0,0.0181357,"iend suggestion tool) use features such as whether both people are computer science majors (Taskar et al., 2003) or whether both are at the same location (Crandall et al., 2010; Sadilek et al., 2012). The inverse problem has been explored in the prediction of a user’s location given the location of their peers (Backstrom et al., 2010; Cho et al., 2011; Sadilek et al., 2012). Jernigan and Mistree (2009) predict a user’s sexuality based on the sexuality of their Facebook friends, while Garera and Yarowsky (2009) predict a user’s gender partly based on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are"
N13-1121,P08-1068,0,0.0236334,"ased on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting user properties in social networks. Aside from the work mentioned above that analyzes"
N13-1121,P09-1116,0,0.499477,"s). We consider each useruser link as a single event; we count it once no matter how often two specific users interact. We extract 436M user-user links in total. Attribute-Attribute Pairs We use our profile data to map each user-user link to an attribute-attribute pair; we separately count each pair of first names, last names, and locations. For example, the firstname pair (henrik, fredrik) occurs 181 times. Rather than using the raw count, we calculate the association between attributes a1 and a2 via their pointwise mutual information (PMI), following prior work in distributional clustering (Lin and Wu, 2009): PMI(a1 , a2 ) = log P(a1 , a2 ) P(a1 )P(a2 ) PMI essentially normalizes the co-occurrence by what we would expect if the attributes were independently distributed. We smooth the PMI by adding a count of 0.5 to all co-occurrence events. The most highly-associated name attributes reflect similarities in ethnicity and gender (Table 2). The most highly-ranked associates for locations are often nicknames and alternate/misspellings of those locations. For example, the locations charm city, bmore, balto, westbaltimore, b a l t i m o r e, baltimoreee, and balitmore each have the U.S. city of baltimo"
N13-1121,P98-2127,0,0.151694,"of training examples Figure 1: Learning curve on Race: Clus perform as well with 30 training examples as Ngm features do with 1000. data; thousands of training examples are needed for Ngm to rival the performance of Clus using only a handful. Since labeled data is generally expensive to obtain or in short supply, our method for exploiting unlabeled Twitter data can both save money and improve top-end performance. 7 Geolocation by Association There is a tradition in computational linguistics of grouping words both by the similarity of their context vectors (Hindle, 1990; Pereira et al., 1993; Lin, 1998) and directly by their statistical association in text (Church and Hanks, 1990; Brown et al., 1992). While the previous sections explored clusters built by vector similarity, we now explore a direct application of our attribute association data (§2). We wish to use this data to improve an existing Twitter geolocation system based on user profile locations. The system operates as follows: 1) normalize user-provided locations using a set of regular expressions (e.g. remove extra spacing, punctuation); 2) look up the normalized location in an alias list; 3) if found, map the alias to a unique str"
N13-1121,N04-1043,0,0.0365937,"er’s sexuality based on the sexuality of their Facebook friends, while Garera and Yarowsky (2009) predict a user’s gender partly based on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explos"
N13-1121,D10-1021,0,0.0278599,"or distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting user properties in social networks. Aside from the work mentioned above that analyzes a user’s social network, a large amount of work has focused on inferring user properties based on the content they generate (e.g. Burger and Henderson (2006), Schler et al. (2006), Rao et al. (2010), Mukherjee and Liu (2010), Pennacchiotti and Popescu (2011), Burger et al. (2011), Van Durme (2012)). 9 Conclusion and Future Work We presented a highly effective and readily replicable algorithm for generating language resources from Twitter communication patterns. We clustered user attributes based on both the communication of users with those attributes as well as substring similarity. Systems using our clusters significantly outperform state-of-the-art algorithms on each of the tasks investigated, and exceed human performance on each task as well. The power and versatility of our clusters is exemplified by the fac"
N13-1121,P93-1024,0,0.430574,"100 1000 10000 Number of training examples Figure 1: Learning curve on Race: Clus perform as well with 30 training examples as Ngm features do with 1000. data; thousands of training examples are needed for Ngm to rival the performance of Clus using only a handful. Since labeled data is generally expensive to obtain or in short supply, our method for exploiting unlabeled Twitter data can both save money and improve top-end performance. 7 Geolocation by Association There is a tradition in computational linguistics of grouping words both by the similarity of their context vectors (Hindle, 1990; Pereira et al., 1993; Lin, 1998) and directly by their statistical association in text (Church and Hanks, 1990; Brown et al., 1992). While the previous sections explored clusters built by vector similarity, we now explore a direct application of our attribute association data (§2). We wish to use this data to improve an existing Twitter geolocation system based on user profile locations. The system operates as follows: 1) normalize user-provided locations using a set of regular expressions (e.g. remove extra spacing, punctuation); 2) look up the normalized location in an alias list; 3) if found, map the alias to"
N13-1121,C10-2112,0,0.0212861,"luding namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting user properties in social networks. Aside from the work mentioned above that analyzes a user’s social network, a large amount of work has focused on inferring user properties based on the content they generate (e.g. Burger and Henderson (2006), Schler et al. (2006), Rao et al. (2010), Mukherjee and Liu (2010), Pennacchiotti and Popescu (2011), Burger et al. (2011), Van Durme (2012)). 9 Conclusion and Future Work We presented a highly effective and readily replicable algorithm for generating language resources from Twitter comm"
N13-1121,W09-1119,0,0.00772601,"f their Facebook friends, while Garera and Yarowsky (2009) predict a user’s gender partly based on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting u"
N13-1121,D12-1137,0,0.0205995,"ts, and we average their results to give a “Human” performance number. The two humans are experts in 1014 cali baby on the court macapá ap NONE NONE edinburgh blagoevgrad ternopil NONE jammu Table 5: Examples of class (left) and input (names, locations) for some of our evaluation tasks. this domain and have very wide knowledge of global names and locations. 5.2 Twitter Applications Country A number of recent papers have considered the task of predicting the geolocation of users, using both user content (Cheng et al., 2010; Eisenstein et al., 2010; Hecht et al., 2011; Wing and Baldridge, 2011; Roller et al., 2012) and social network (Backstrom et al., 2010; Sadilek et al., 2012). Here, we first predict user location at the level of the user’s location country. To our knowledge, we are the first to exploit user locations and names for this prediction. For this task, we obtain gold data from the portion of Twitter users who have GPS enabled (geocoded tweets). We were able to obtain a very large number of gold instances for this task, so selected only 10K for testing, 10K for development, and retained the remaining 782K for training. Language ID Identifying the language of users is an important prerequisi"
N13-1121,N12-1052,0,0.0235863,"of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting user properties in social networks. Aside from the work mentioned above that analyzes a user’s social network,"
N13-1121,P10-1040,0,0.0155684,"rowsky (2009) predict a user’s gender partly based on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting user properties in social networks. Aside f"
N13-1121,D12-1005,1,0.555299,"Missing"
N13-1121,P11-1096,0,0.0180694,"s from each of the test sets, and we average their results to give a “Human” performance number. The two humans are experts in 1014 cali baby on the court macapá ap NONE NONE edinburgh blagoevgrad ternopil NONE jammu Table 5: Examples of class (left) and input (names, locations) for some of our evaluation tasks. this domain and have very wide knowledge of global names and locations. 5.2 Twitter Applications Country A number of recent papers have considered the task of predicting the geolocation of users, using both user content (Cheng et al., 2010; Eisenstein et al., 2010; Hecht et al., 2011; Wing and Baldridge, 2011; Roller et al., 2012) and social network (Backstrom et al., 2010; Sadilek et al., 2012). Here, we first predict user location at the level of the user’s location country. To our knowledge, we are the first to exploit user locations and names for this prediction. For this task, we obtain gold data from the portion of Twitter users who have GPS enabled (geocoded tweets). We were able to obtain a very large number of gold instances for this task, so selected only 10K for testing, 10K for development, and retained the remaining 782K for training. Language ID Identifying the language of users is a"
N13-1121,C98-2122,0,\N,Missing
N15-1002,D12-1032,1,0.826461,"s we add joint factors that capture effects between alignment variables. Each joint factor φ is comprised of a constrained binary variable zφ associated with features f (φ) that indicates when the factor is active. Together with parameters w these form additional scores sφ for the objective: sφ = w · f (φ) (2) which doesn’t use global inference.2 These features are built on top of a variety of semantic resources (PPDB (Ganitkevitch et al., 2013), WordNet (Miller, 1995), FrameNet (Baker et al., 1998)) and methods for comparing mentions (tree edit distance (Yao et al., 2013), string transducer (Andrews et al., 2012)). 4 Joint Factors Our goal is to develop joint factors that improve over the feature rich local factors baseline by considering global information. Fertility A common mistake when making independent classification decisions is to align many source items to a single target item. While each link looks promising on its own, they clearly cannot all be right. Empirically, the training set reveals that many to one alignments are uncommon; thus many to one predictions are likely errors. We add a fertility factor for predicates and arguments, where fertility is defined as the number of links to an it"
N15-1002,P98-1013,0,0.166656,"ing items i and j. Using only local features, our system would greedily select alignments. To capture global aspects we add joint factors that capture effects between alignment variables. Each joint factor φ is comprised of a constrained binary variable zφ associated with features f (φ) that indicates when the factor is active. Together with parameters w these form additional scores sφ for the objective: sφ = w · f (φ) (2) which doesn’t use global inference.2 These features are built on top of a variety of semantic resources (PPDB (Ganitkevitch et al., 2013), WordNet (Miller, 1995), FrameNet (Baker et al., 1998)) and methods for comparing mentions (tree edit distance (Yao et al., 2013), string transducer (Andrews et al., 2012)). 4 Joint Factors Our goal is to develop joint factors that improve over the feature rich local factors baseline by considering global information. Fertility A common mistake when making independent classification decisions is to align many source items to a single target item. While each link looks promising on its own, they clearly cannot all be right. Empirically, the training set reveals that many to one alignments are uncommon; thus many to one predictions are likely error"
N15-1002,P10-1143,0,0.0998996,"e this extra parameter is worth allocating a portion of training data to enable tuning. Tuning τ addresses the same problem as using an asymmetric Hamming loss, but we found that doing both led to better results.4 Since we are using a global scoring function rather than a set of classifications, τ is implemented as a test-time unary factor on every alignment. 6 Experiments Data We consider two datasets for evaluation. The first is a cross-document entity and event coreference resolution dataset called the Extended Event Coref Bank (EECB) created by Lee et al. (2012) and based on a corpus from Bejan and Harabagiu (2010). The dataset contains clusters of news articles taken from Google News with annotations about coreference over entities and events. Following the procedure of Wolfe et al. (2013), we select the first document in every cluster and pair it with every other document in the cluster. The second dataset (RF) comes from Roth and Frank (2012). The dataset contains pairs of news articles that describe the same news story, and are annotated for predicate links between the document pairs. Due to the lack of annotated arguments, we can only report predicate linking performance and the psa and asp factors"
N15-1002,H93-1039,0,0.46461,"Missing"
N15-1002,Q14-1022,0,0.109653,"s(pi ) × args(pj ) or preds(ai ) × preds(aj ) respectively. Temporal Information Temporal ordering, in contrast to textual ordering, can indicate when predicates cannot align: we expect aligned predicates in both documents to share the same temporal relations. SemEval 2013 included a task on predicting temporal relations between events (UzZaman et al., 2013). Many systems produced partial relations of events in a document based on lexical aspect and tense, as well as discourse connectives like “during” or “after”. We obtain temporal relations with CAEVO, a state-of-the-art sieve-based system (Chambers et al., 2014). TimeML (Pustejovsky et al., 2003), the format for specifying temporal relations, defines relations between predicates (e.g. immediately before and simultaneous), each with an inverse (e.g. immediately after and simultaneous respectively). We will refer to a relation as R and its inverse as R−1 . Suppose we had pa and pb in the source document, px and py in the target document, and pa R1 pb , px R2 py . Given this configuration the following alignments conflict with the in-doc relations: zax zby zay zbx In-Doc Relations * * 1 1 R1 = R2 1 * * R1 = R2−1 1 where 1 means there is a link and * mea"
N15-1002,J08-4005,0,0.0321391,"Frank (2012) and Wolfe et al. (2013) we include a Lemma baseline for identifying alignments which will align any two predicates or arguments that have the same lemmatized head word.6 The Local baseline uses the same features as Wolfe et al., but none of our joint factors. In addition to running our joint model with all factors, we measure the efficacy of each individual factor by evaluating each with the local features. For evaluation we use a generous version of F1 that is defined for alignment labels composed of sure, Gs , and possible links, Gp and the system’s proposed links H (following Cohn et al. (2008), Roth and Frank (2012) and Wolfe et al. (2013)). P = |H ∩ Gp | |H| R= |H ∩ Gs | 2P R F = |Gs | P +R Note that the EECB data does not have a sure and possible distinction, so Gs = Gp , resulting in standard F1. In addition to F1, we separately measure predicate and argument F1 to demonstrate where our model makes the largest improvements. We performed a one-sided paired-bootstrap test where the null hypothesis was that the joint model was no better than the Local baseline (described in Koehn (2004)). Cases where p < 0.05 are bolded. 5 https://github.com/cnap/anno-pipeline The lemma baseline is"
N15-1002,N13-1092,1,0.866479,"Missing"
N15-1002,P13-2139,0,0.044495,"Missing"
N15-1002,W04-3250,0,0.0217182,"composed of sure, Gs , and possible links, Gp and the system’s proposed links H (following Cohn et al. (2008), Roth and Frank (2012) and Wolfe et al. (2013)). P = |H ∩ Gp | |H| R= |H ∩ Gs | 2P R F = |Gs | P +R Note that the EECB data does not have a sure and possible distinction, so Gs = Gp , resulting in standard F1. In addition to F1, we separately measure predicate and argument F1 to demonstrate where our model makes the largest improvements. We performed a one-sided paired-bootstrap test where the null hypothesis was that the joint model was no better than the Local baseline (described in Koehn (2004)). Cases where p < 0.05 are bolded. 5 https://github.com/cnap/anno-pipeline The lemma baseline is obviously sensitive to the lemmatizer used. We used the Stanford CoreNLP lemmatizer (Manning et al., 2014) and found it yielded slightly better results than previously reported as the lemma baseline (Roth and Frank, 2012), so we used it for all systems to ensure fairness and that the baseline is as strong as it could be. 6 17 7 Results Results for EECB and RF are reported in Table 7. As previously reported, using just local factors (features on pairs) improves over lemma baselines (Wolfe et al., 2"
N15-1002,N06-1015,0,0.434729,"e result may be links that conflict in their interpretation of the document. Figure 1 makes clear that jointly considering all links at once can aid individual decisions, for example, by including temporal ordering of predicates. The global nature of this task is similar to word alignment for machine translation (MT). Many systems consider alignment links between words individually, selecting the best link for each word independently of the other words in the sentence. Just as with an independent linking strategy in predicate argument alignment, this can lead to inconsistencies in the output. Lacoste-Julien et al. (2006) introduced a model that jointly resolved word alignments based on the introduction of quadratic variables, factors that depend on two alignment decisions which characterize patterns that span word-word links. Their approach achieved improved results even in the presence of little training data. 12 We present a global predicate argument alignment model based on considering quadratic interactions between alignment variables to captures patterns we expect in coherent discourse. We introduce factors which are comprised of a binary variable, multiple quadratic constraints on that variable, and fea"
N15-1002,D12-1045,0,0.297996,"e: viewing a multi-document task as one limited to individual pairs of sentences. This creates a mis-match between the goals of such work – considering entire documents – with the systems – consider individual sentences. In this work, we consider a system that takes a document level view in considering coreference for entities and predictions: the task of predicate argument linking. We treat this task as a global inference problem, leveraging multiple sources of semantic information identified at the document level. Global inference for this problem is mostly unexplored, with the exception of Lee et al. (2012) (discussed in § 8). Especially novel here is the use of document-level temporal constraints on events, representing a next step forward on the path to full understanding. Our approach avoids the pitfalls of local inference while still remaining fast and exact. We use the pairwise features of a very strong predicate argument aligner (Wolfe et al., 2013) (competitive with the state-of-the-art (Roth, 2014)), and add quadratic factors that constrain local decisions based on global document information. These global factors lead to superior performance compared to the previous state-of-the-art. We"
N15-1002,P06-1095,0,0.0397977,"eference resolution, Recasens et al. (2013) sought to overcome the problem of opaque mentions7 by finding highprecision paraphrases of entities by pivoting off verbs mentioned in similar documents. We address the issue of opaque mentions not by building a paraphrase table, but by jointly reasoning about entities that participate in coreferent events (c.f. §4); the approaches are complementary. In this work we incorporate ordering information of events. Though we consider it an upstream task, there is a line of work trying to predict temporal relations between events (Pustejovsky et al., 2003; Mani et al., 2006; Chambers et al., 2014). Our results indicate this is a useful source of information, one of the first results to show an improvement from this 7 A lexically disparate description of an entity. ˇ type of system (Glavaˇs and Snajder, 2013). We utilize an ILP to improve upon a pipelined system, similar to Roth and Yih (2004), but our work differs in that we do not use piecewise-trained classifiers. Our local similarity scores are calibrated according to a global objective by propagating the gradient back from the loss to every parameter in the model. When using piecewise training, local classif"
N15-1002,P14-5010,0,0.00383754,"F = |Gs | P +R Note that the EECB data does not have a sure and possible distinction, so Gs = Gp , resulting in standard F1. In addition to F1, we separately measure predicate and argument F1 to demonstrate where our model makes the largest improvements. We performed a one-sided paired-bootstrap test where the null hypothesis was that the joint model was no better than the Local baseline (described in Koehn (2004)). Cases where p < 0.05 are bolded. 5 https://github.com/cnap/anno-pipeline The lemma baseline is obviously sensitive to the lemmatizer used. We used the Stanford CoreNLP lemmatizer (Manning et al., 2014) and found it yielded slightly better results than previously reported as the lemma baseline (Roth and Frank, 2012), so we used it for all systems to ensure fairness and that the baseline is as strong as it could be. 6 17 7 Results Results for EECB and RF are reported in Table 7. As previously reported, using just local factors (features on pairs) improves over lemma baselines (Wolfe et al., 2013). The joint factors make statistically significant gains over local factors in almost all experiments. Fertility factors provide the largest improvements from any single constraint. A fertility penalt"
N15-1002,W12-3018,1,0.860036,"Missing"
N15-1002,N13-1110,0,0.0297615,"s are bolded. sort employed in RTE challenges. Lee et al. (2012) considered a similar problem but sought to produce clusters of entities and events rather than an alignment between two documents with the goal of improving coreference resolution. They used features which consider previous event and entity coreference decisions to make future coreference decisions in a greedy manner. This differs from our model which is built on non-greedy joint inference, but much of the signal indicating when two mentions corefer or are aligned is similar. In the context of in-document coreference resolution, Recasens et al. (2013) sought to overcome the problem of opaque mentions7 by finding highprecision paraphrases of entities by pivoting off verbs mentioned in similar documents. We address the issue of opaque mentions not by building a paraphrase table, but by jointly reasoning about entities that participate in coreferent events (c.f. §4); the approaches are complementary. In this work we incorporate ordering information of events. Though we consider it an upstream task, there is a line of work trying to predict temporal relations between events (Pustejovsky et al., 2003; Mani et al., 2006; Chambers et al., 2014)."
N15-1002,S12-1030,0,0.164929,"entences from the document pair shown in Figure 1. These sentences describe the same event, although with different details. The source sentence has four predicates and four arguments, while the target has three predicates and three arguments. In this case, one of the predicates from each sentence aligns, as do three of the arguments. We also show additional information potentially helpful to determining alignments: temporal relations between the predicates. The goal of predicate argument alignment is to assign these links indicating coreferent predicates and arguments across a document pair (Roth and Frank, 2012). Previous work by Wolfe et al. (2013) formulated 1 https://github.com/hltcoe/parma2 11 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 11–20, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics Figure 1: An example analysis and predicate argument alignment task between a source and target document. Predicates appear as hollow ovals, have blue mentions, and are aligned considering their arguments (dashed lines). Arguments, in black diamonds with green mentions, represent a document-level entity (coref"
N15-1002,W04-2401,0,0.0604425,"icipate in coreferent events (c.f. §4); the approaches are complementary. In this work we incorporate ordering information of events. Though we consider it an upstream task, there is a line of work trying to predict temporal relations between events (Pustejovsky et al., 2003; Mani et al., 2006; Chambers et al., 2014). Our results indicate this is a useful source of information, one of the first results to show an improvement from this 7 A lexically disparate description of an entity. ˇ type of system (Glavaˇs and Snajder, 2013). We utilize an ILP to improve upon a pipelined system, similar to Roth and Yih (2004), but our work differs in that we do not use piecewise-trained classifiers. Our local similarity scores are calibrated according to a global objective by propagating the gradient back from the loss to every parameter in the model. When using piecewise training, local classifiers must focus more on recall (in the spirit of Weiss and Taskar (2010)) than they would for an ordinary classification task with no global objective. Our method trains classifiers jointly with a global convex objective. While our training procedure requires decoding an integer program, the parameters we learn are globally"
N15-1002,S13-2001,0,0.0188965,"te: p a 1 − max zkl (7) zφasp ≥ zij k∈preds(ai ) l∈preds(aj ) where preds(ai ) finds the indices of all predicates that govern any mention of argument ai . The features f (φ) for both psa and asp are an intercept feature and a bucketed count of the size of args(pi ) × args(pj ) or preds(ai ) × preds(aj ) respectively. Temporal Information Temporal ordering, in contrast to textual ordering, can indicate when predicates cannot align: we expect aligned predicates in both documents to share the same temporal relations. SemEval 2013 included a task on predicting temporal relations between events (UzZaman et al., 2013). Many systems produced partial relations of events in a document based on lexical aspect and tense, as well as discourse connectives like “during” or “after”. We obtain temporal relations with CAEVO, a state-of-the-art sieve-based system (Chambers et al., 2014). TimeML (Pustejovsky et al., 2003), the format for specifying temporal relations, defines relations between predicates (e.g. immediately before and simultaneous), each with an inverse (e.g. immediately after and simultaneous respectively). We will refer to a relation as R and its inverse as R−1 . Suppose we had pa and pb in the source"
N15-1002,P13-2012,1,0.801492,"Missing"
N15-1002,N13-1106,1,0.844869,"Missing"
N15-1002,C98-1013,0,\N,Missing
N15-1058,N09-1003,0,0.267772,"Missing"
N15-1058,W13-3520,0,0.0331983,"Missing"
N15-1058,P05-1074,0,0.0490414,"nstrated that incorporat7 code.google.com/p/ word2vec,metaoptimize.com/projects/ wordreprs 8 nlp.stanford.edu/projects/glove 9 ttic.uchicago.edu/˜mbansal/data/ syntacticEmbeddings.zip,cs.cmu.edu/ ˜mfaruqui/soft.html ing bilingual data through CCA improved performance. More recently this same phenomenon was reported by Hill et al. (2014a) through their experiments over neural representations learnt from MT systems. Various other researchers have tried to improve the performance of their paraphrase systems or vector space models by using diverse sources of information such as bilingual corpora (Bannard and Callison-Burch, 2005; Huang et al., 2012; Zou et al., 2013),10 structured datasets (Yu and Dredze, 2014; Faruqui et al., 2014) or even tagged images (Bruni 10 An example of complementary views: Chan et al. (2011) observed that monolingual distributional statistics are susceptible to conflating antonyms, where bilingual data is not; on the other hand bilingual statistics are susceptible to noisy alignments, where monolingual data is not. 563 et al., 2012). However, most previous work11 did not adopt the general, simplifying view that all of these sources of data are just cooccurrence statistics coming from differe"
N15-1058,P14-2131,0,0.0141581,"91.3 96.3 79.3 86.0 81.8 91.8 87.3 92.5 69.6 85.3 83.9 94.0 91.8 92.9 91.4 95.8 0.9 Table 9: Comparison of Multiview LSA against Glove and WSG(Word2Vec Skip Gram). Using σ0.05 as the threshold † we highlighted the top performing systems in bold font. marks significant increments in performance due to use of multiple views in the Gain columns. The rs columns demonstrate that GCCA increased pearson correlation. dran, 2008; Chan et al., 2011). 7 They have been trained using either one (Pennington et al., 2014) 8 or two sources of cooccurrence statistics (Zou et al., 2013; Faruqui and Dyer, 2014; Bansal et al., 2014; Levy and Goldberg, 2014) 9 or using multi-modal data (Hill and Korhonen, 2014; Bruni et al., 2012). Dhillon et al. (2011) and Dhillon et al. (2012) were the first to use CCA as the primary method to learn vector representations and Faruqui and Dyer (2014) further demonstrated that incorporat7 code.google.com/p/ word2vec,metaoptimize.com/projects/ wordreprs 8 nlp.stanford.edu/projects/glove 9 ttic.uchicago.edu/˜mbansal/data/ syntacticEmbeddings.zip,cs.cmu.edu/ ˜mfaruqui/soft.html ing bilingual data through CCA improved performance. More recently this same phenomenon was reported by Hill et al"
N15-1058,P08-1077,0,0.0131084,"Missing"
N15-1058,P12-1015,0,0.144102,"hold of 0.05. Similar such comparisons on small datasets are performed by Hill et al. (2014a). Acronym MEN RW SCWS SIMLEX WS MTURK WS-REL WS-SEM RG MC AN-SYN AN-SEM TOEFL Size 3000 2034 2003 999 353 287 252 203 65 30 10675 8869 80 0.5 σ0.01 4.2 5.1 5.1 7.3 12.3 13.7 14.6 16.2 28.6 41.7 - 0.7 σ0.01 3.2 3.9 4.0 5.7 9.5 10.6 11.3 12.6 22.3 32.7 - 0.9 σ0.01 1.8 2.3 2.3 3.2 5.5 6.1 6.5 7.3 12.9 19.0 0.95 1.03 8.13 0.5 σ0.05 3.0 3.6 3.6 5.2 8.7 9.7 10.3 11.5 20.6 30.6 - 0.7 σ0.05 2.3 2.8 2.8 4.0 6.7 7.5 8.0 8.9 16.0 23.9 - 0.9 σ0.05 1.3 1.6 1.6 2.3 3.9 4.3 4.6 5.1 9.2 13.8 0.68 0.74 6.63 Reference (Bruni et al., 2012) (Luong et al., 2013) (Huang et al., 2012) (Hill et al., 2014b) (Finkelstein et al., 2001) (Radinsky et al., 2011) (Agirre et al., 2009) -Same-As-Above(Rubenstein and Goodenough, 1965) (Miller and Charles, 1991) (Mikolov et al., 2013a) -Same-As-Above(Landauer and Dumais, 1997) Table 2: List of test datasets used. The columns headed σpr0 contain MRDS values. The rows for accuracy based test sets contain σp0 which does not depend on r. See § 4.1 for details. items, produced respectively by algorithms A and B, and then a list of gold ratings T . Let rAT , rBT and rAB denote the Spearman correlati"
N15-1058,W11-2504,1,0.914913,"Missing"
N15-1058,W99-0613,0,0.0681907,"ike Glove or LSA would allows us to naturally perplexity or generate sequences. We also note that V´ıa et al. (2007) presented a neural network model of GCCA and adaptive/incremental GCCA. To the best of our knowledge both of these approaches have not been used for word representation learning. CCA is also an algorithm for multi-view learning (Kakade and Foster, 2007; Ganchev et al., 2008) and when we view our work as an application of multiview learning to NLP, this follows a long chain of effort started by Yarowsky (1995) and continued with Co-Training (Blum and Mitchell, 1998), CoBoosting (Collins and Singer, 1999) and 2 view perceptrons (Brefeld et al., 2006). 7 Conclusion and Future Work While previous efforts demonstrated that incorporating two views is beneficial in word-representation learning, we extended that thread of work to a logical extreme and created MVLSA to learn distributed representations using data from 46 views!13 Through evaluation of our induced representations, shown in Table 9, we demonstrated that the MVLSA algorithm is able to leverage the information present in multiple data sources to improve performance on a battery of tests against state of the art baselines. In order to per"
N15-1058,E14-1049,0,0.163755,"WSG 85.8 92.3 80.2 75.6 91.3 96.3 79.3 86.0 81.8 91.8 87.3 92.5 69.6 85.3 83.9 94.0 91.8 92.9 91.4 95.8 0.9 Table 9: Comparison of Multiview LSA against Glove and WSG(Word2Vec Skip Gram). Using σ0.05 as the threshold † we highlighted the top performing systems in bold font. marks significant increments in performance due to use of multiple views in the Gain columns. The rs columns demonstrate that GCCA increased pearson correlation. dran, 2008; Chan et al., 2011). 7 They have been trained using either one (Pennington et al., 2014) 8 or two sources of cooccurrence statistics (Zou et al., 2013; Faruqui and Dyer, 2014; Bansal et al., 2014; Levy and Goldberg, 2014) 9 or using multi-modal data (Hill and Korhonen, 2014; Bruni et al., 2012). Dhillon et al. (2011) and Dhillon et al. (2012) were the first to use CCA as the primary method to learn vector representations and Faruqui and Dyer (2014) further demonstrated that incorporat7 code.google.com/p/ word2vec,metaoptimize.com/projects/ wordreprs 8 nlp.stanford.edu/projects/glove 9 ttic.uchicago.edu/˜mbansal/data/ syntacticEmbeddings.zip,cs.cmu.edu/ ˜mfaruqui/soft.html ing bilingual data through CCA improved performance. More recently this same phenomenon was r"
N15-1058,N13-1092,1,0.720226,"Missing"
N15-1058,2003.mtsummit-systems.9,0,0.027305,"Missing"
N15-1058,D14-1032,0,0.099921,"ual views or some combination of them and measure the performance. It is clear by comparing the last column to the second column that adding in more views improves performance. Also we can see that the Dependency based views and the Bitext based views give a larger boost than the morphology and FrameNet based views, probably because the latter are so sparse. Comparison to other word representation creation methods There are a large number of methods of creating representations both multilingual and monolingual. There are many new methods such as by Yu and Dredze (2014), Faruqui et al. (2014), Hill and Korhonen (2014), and Weston et al. (2014) that are performing multiview learning and could be considered here as baselines: however it is not straightforward to use those systems to handle the variety of data that we are using. Therefore, we directly compare our method to the Glove and the SkipGram model of Word2Vec as the performance of those systems is considered state of the art. We trained these two systems on the English portion of the Polyglot 562 Wikipedia dataset.5 We also combined their outputs using MVLSA to create MV-G-WSG) embeddings. We trained our best MVLSA system with data from all views and"
N15-1058,P12-1092,0,0.535018,"small datasets are performed by Hill et al. (2014a). Acronym MEN RW SCWS SIMLEX WS MTURK WS-REL WS-SEM RG MC AN-SYN AN-SEM TOEFL Size 3000 2034 2003 999 353 287 252 203 65 30 10675 8869 80 0.5 σ0.01 4.2 5.1 5.1 7.3 12.3 13.7 14.6 16.2 28.6 41.7 - 0.7 σ0.01 3.2 3.9 4.0 5.7 9.5 10.6 11.3 12.6 22.3 32.7 - 0.9 σ0.01 1.8 2.3 2.3 3.2 5.5 6.1 6.5 7.3 12.9 19.0 0.95 1.03 8.13 0.5 σ0.05 3.0 3.6 3.6 5.2 8.7 9.7 10.3 11.5 20.6 30.6 - 0.7 σ0.05 2.3 2.8 2.8 4.0 6.7 7.5 8.0 8.9 16.0 23.9 - 0.9 σ0.05 1.3 1.6 1.6 2.3 3.9 4.3 4.6 5.1 9.2 13.8 0.68 0.74 6.63 Reference (Bruni et al., 2012) (Luong et al., 2013) (Huang et al., 2012) (Hill et al., 2014b) (Finkelstein et al., 2001) (Radinsky et al., 2011) (Agirre et al., 2009) -Same-As-Above(Rubenstein and Goodenough, 1965) (Miller and Charles, 1991) (Mikolov et al., 2013a) -Same-As-Above(Landauer and Dumais, 1997) Table 2: List of test datasets used. The columns headed σpr0 contain MRDS values. The rows for accuracy based test sets contain σp0 which does not depend on r. See § 4.1 for details. items, produced respectively by algorithms A and B, and then a list of gold ratings T . Let rAT , rBT and rAB denote the Spearman correlations between A : T , B : T and A : B respec"
N15-1058,P14-2050,0,0.00701039,"1.8 91.8 87.3 92.5 69.6 85.3 83.9 94.0 91.8 92.9 91.4 95.8 0.9 Table 9: Comparison of Multiview LSA against Glove and WSG(Word2Vec Skip Gram). Using σ0.05 as the threshold † we highlighted the top performing systems in bold font. marks significant increments in performance due to use of multiple views in the Gain columns. The rs columns demonstrate that GCCA increased pearson correlation. dran, 2008; Chan et al., 2011). 7 They have been trained using either one (Pennington et al., 2014) 8 or two sources of cooccurrence statistics (Zou et al., 2013; Faruqui and Dyer, 2014; Bansal et al., 2014; Levy and Goldberg, 2014) 9 or using multi-modal data (Hill and Korhonen, 2014; Bruni et al., 2012). Dhillon et al. (2011) and Dhillon et al. (2012) were the first to use CCA as the primary method to learn vector representations and Faruqui and Dyer (2014) further demonstrated that incorporat7 code.google.com/p/ word2vec,metaoptimize.com/projects/ wordreprs 8 nlp.stanford.edu/projects/glove 9 ttic.uchicago.edu/˜mbansal/data/ syntacticEmbeddings.zip,cs.cmu.edu/ ˜mfaruqui/soft.html ing bilingual data through CCA improved performance. More recently this same phenomenon was reported by Hill et al. (2014a) through their ex"
N15-1058,W13-3512,0,0.0328708,"Missing"
N15-1058,N13-1090,0,0.0610588,".5 σ0.01 4.2 5.1 5.1 7.3 12.3 13.7 14.6 16.2 28.6 41.7 - 0.7 σ0.01 3.2 3.9 4.0 5.7 9.5 10.6 11.3 12.6 22.3 32.7 - 0.9 σ0.01 1.8 2.3 2.3 3.2 5.5 6.1 6.5 7.3 12.9 19.0 0.95 1.03 8.13 0.5 σ0.05 3.0 3.6 3.6 5.2 8.7 9.7 10.3 11.5 20.6 30.6 - 0.7 σ0.05 2.3 2.8 2.8 4.0 6.7 7.5 8.0 8.9 16.0 23.9 - 0.9 σ0.05 1.3 1.6 1.6 2.3 3.9 4.3 4.6 5.1 9.2 13.8 0.68 0.74 6.63 Reference (Bruni et al., 2012) (Luong et al., 2013) (Huang et al., 2012) (Hill et al., 2014b) (Finkelstein et al., 2001) (Radinsky et al., 2011) (Agirre et al., 2009) -Same-As-Above(Rubenstein and Goodenough, 1965) (Miller and Charles, 1991) (Mikolov et al., 2013a) -Same-As-Above(Landauer and Dumais, 1997) Table 2: List of test datasets used. The columns headed σpr0 contain MRDS values. The rows for accuracy based test sets contain σp0 which does not depend on r. See § 4.1 for details. items, produced respectively by algorithms A and B, and then a list of gold ratings T . Let rAT , rBT and rAB denote the Spearman correlations between A : T , B : T and A : B respectively. Let rˆAT , rˆBT , rˆAB be their empirical estimates and assume that rˆBT &gt; rˆAT without loss of generality. For word similarity datasets we define σpr0 as the MRDS, such that it satis"
N15-1058,W12-3018,1,0.117539,"Missing"
N15-1058,D14-1162,0,0.104231,"Missing"
N15-1058,W14-2901,1,0.87668,"Missing"
N15-1058,P05-1077,0,0.0277843,"nally we contrast the Spearman correlations rs with Glove and Word2Vec before and after including them in the GCCA procedure. The values demonstrate that including Glove and WSG during GCCA actually increased the correlation between them and the learnt embeddings, which supports our motivation for performing GCCA in the first place. 6 Previous Work Vector space representations of words have been created using diverse frameworks including Spectral methods (Dhillon et al., 2011; Dhillon et al., 2012), 6 Neural Networks (Mikolov et al., 2013b; Collobert and Lebret, 2013), and Random Projections (Ravichandran et al., 2005; Bhagat and Ravichan5 We explicitly provided the vocabulary file to Glove and Word2Vec and set the truncation threshold for Word2Vec to 10. Glove was trained for 25 iterations. Glove was provided a window of 15 previous words and Word2Vec used a symmetric window of 10 words. 6 cis.upenn.edu/˜ungar/eigenwords Test Set MEN RW SCWS AN-SYN AN-SEM All !Morphology !Framenet !Morphology !Bitext !Wikipedia !Dependency Views !Framenet 70.1 37.2 66.4 56.4 34.3 69.8 36.4 65.8 56.3 34.3 70.1 36.1 66.3 56.2 34.3 69.9 32.2 64.2 51.2 36.2 46.4 11.6 54.5 37.6 4.1 68.4 34.9 65.5 50.5 35.3 69.5 34.1 65.2 54.4"
N15-1058,D14-1194,0,0.00892088,"of them and measure the performance. It is clear by comparing the last column to the second column that adding in more views improves performance. Also we can see that the Dependency based views and the Bitext based views give a larger boost than the morphology and FrameNet based views, probably because the latter are so sparse. Comparison to other word representation creation methods There are a large number of methods of creating representations both multilingual and monolingual. There are many new methods such as by Yu and Dredze (2014), Faruqui et al. (2014), Hill and Korhonen (2014), and Weston et al. (2014) that are performing multiview learning and could be considered here as baselines: however it is not straightforward to use those systems to handle the variety of data that we are using. Therefore, we directly compare our method to the Glove and the SkipGram model of Word2Vec as the performance of those systems is considered state of the art. We trained these two systems on the English portion of the Polyglot 562 Wikipedia dataset.5 We also combined their outputs using MVLSA to create MV-G-WSG) embeddings. We trained our best MVLSA system with data from all views and by using the individual be"
N15-1058,H89-1033,0,0.0892398,"parisons done on small sized datasets. Multiview LSA (MVLSA) is a generalization of Latent Semantic Analysis (LSA) that supports the fusion of arbitrary views of data and relies on Generalized Canonical Correlation Analysis (GCCA). We present an algorithm for fast approximate computation of GCCA, which when coupled with methods for handling missing values, is general enough to approximate some recent algorithms for inducing vector representations of words. Experiments across a comprehensive collection of test-sets show our approach to be competitive with the state of the art. 1 2 Introduction Winograd (1972) wrote that: “Two sentences are paraphrases if they produce the same representation in the internal formalism for meaning”. This intuition is made soft in vector-space models (Turney and Pantel, 2010), where we say that expressions in language are paraphrases if their representations are close under some distance measure. One of the earliest linguistic vector space models was Latent Semantic Analysis (LSA). LSA has been successfully used for Information Retrieval but it is limited in its reliance on a single matrix, or view, of term co-occurrences. Here we address the single-view limitation of"
N15-1058,P14-2089,0,0.056865,"alysis of performance where we remove individual views or some combination of them and measure the performance. It is clear by comparing the last column to the second column that adding in more views improves performance. Also we can see that the Dependency based views and the Bitext based views give a larger boost than the morphology and FrameNet based views, probably because the latter are so sparse. Comparison to other word representation creation methods There are a large number of methods of creating representations both multilingual and monolingual. There are many new methods such as by Yu and Dredze (2014), Faruqui et al. (2014), Hill and Korhonen (2014), and Weston et al. (2014) that are performing multiview learning and could be considered here as baselines: however it is not straightforward to use those systems to handle the variety of data that we are using. Therefore, we directly compare our method to the Glove and the SkipGram model of Word2Vec as the performance of those systems is considered state of the art. We trained these two systems on the English portion of the Polyglot 562 Wikipedia dataset.5 We also combined their outputs using MVLSA to create MV-G-WSG) embeddings. We trained ou"
N15-1058,D13-1141,0,0.0376334,"rs MV-G-WSG Glove WSG 85.8 92.3 80.2 75.6 91.3 96.3 79.3 86.0 81.8 91.8 87.3 92.5 69.6 85.3 83.9 94.0 91.8 92.9 91.4 95.8 0.9 Table 9: Comparison of Multiview LSA against Glove and WSG(Word2Vec Skip Gram). Using σ0.05 as the threshold † we highlighted the top performing systems in bold font. marks significant increments in performance due to use of multiple views in the Gain columns. The rs columns demonstrate that GCCA increased pearson correlation. dran, 2008; Chan et al., 2011). 7 They have been trained using either one (Pennington et al., 2014) 8 or two sources of cooccurrence statistics (Zou et al., 2013; Faruqui and Dyer, 2014; Bansal et al., 2014; Levy and Goldberg, 2014) 9 or using multi-modal data (Hill and Korhonen, 2014; Bruni et al., 2012). Dhillon et al. (2011) and Dhillon et al. (2012) were the first to use CCA as the primary method to learn vector representations and Faruqui and Dyer (2014) further demonstrated that incorporat7 code.google.com/p/ word2vec,metaoptimize.com/projects/ wordreprs 8 nlp.stanford.edu/projects/glove 9 ttic.uchicago.edu/˜mbansal/data/ syntacticEmbeddings.zip,cs.cmu.edu/ ˜mfaruqui/soft.html ing bilingual data through CCA improved performance. More recently th"
N15-1058,E14-1051,0,\N,Missing
N15-1058,N15-1184,0,\N,Missing
N15-1058,J15-4004,0,\N,Missing
N15-3018,W14-2907,1,0.881586,"Missing"
N15-3018,P14-1073,1,0.741206,"on-lexical features indicate the word’s relative positions comparing to the target entities (whether the word is the head of any target entity, in-between the two entities, or on the dependency path between entities), which improve the expressive strength of word embeddings. We store the extracted relations in C ON CRETE SituationMentions. See Figure 2 for 89 Figure 2: ACE entity relations viewed through Quicklime (Section 3.7). an example visualization. 3.6 Cross Document Coreference Resolution Cross document coreference resolution is performed via the phylogenetic entity clustering model of Andrews et al. (2014).5 Since the method is fully unsupervised we did not require a Chinese specific model. We use this system to cluster EntityMentions and store the clustering in top level C ONCRETE Clustering objects. 3.7 Creating Manual Annotations Quicklime6 is a browser-based tool for viewing and editing NLP annotations stored in a C ONCRETE document. Quicklime supports a wide array of analytics, including parse trees, token taggings, entities, mentions, and “situations” (e.g. relations.) Quicklime uses the visualization layer of BRAT (Stenetorp et al., 2012) to display some annotations but does not use the"
N15-3018,W08-0336,0,0.0139229,"tions and stored in a EntityMentionSetList. Additional annotations that are typically utilized by relation extraction systems, such as syntactic parses, are provided automatically by the pipeline. 88 3.2 Word Segmentation Chinese text processing requires the identification of word boundaries, which are not indicated in written Chinese as they are in most other languages. Our word segmentation is provided by the Stanford CoreNLP3 (Manning et al., 2014) Chinese word segmentation tool, which is a conditional random field (CRF) model with character based features and lexicon features according to Chang et al. (2008). Word segmentations decisions are represented by C ONCRETE Token objects and stored in the TokenList. We follow the Chinese Penn Treebank segmentation standard (Xue et al., 2005). Our system tracks token offsets so that segmentation is robust to unexpected spaces or line breaks within a Chinese word. 3.3 Syntax Part of speech tagging and syntactic parsing are also provided by Stanford CoreNLP. The part of speech tagger is based on Toutanova et al. (2003) adapted for Chinese, which is a log-linear model underneath. Integration with C ONCRETE was facilitated by the concrete-stanford library 4 ,"
N15-3018,P05-1045,0,0.00391797,"cy parses from the CoreNLP dependency converter. We store the constituency parses as a C ONCRETE Parse, and the dependency analyses as C ON CRETE DependencyParses. 3.4 Named Entity Recognition We support the two most common named entity annotation standards: the CoNLL standard (four types: person, organization, location and miscellaneous), and the ACE standard, which includes the additional types of geo-political entity, facility, weapon and vehicle. The ACE standard also includes support for nested entities. We used the Stanford CoreNLP NER toolkit which is a CRF model based on the method in Finkel et al. (2005), plus features based on Brown clustering. For the CoNLL standard annotations, we use one CRF model to label all the four types of entities. For the ACE standard annotations, in order to deal with the nested cases, we build one tagger for each entity type. Each entity is stored in a C ON CRETE EntityMention. 3.5 Relation Extraction Relations are extracted for every pair of entity mentions. We use a log-linear model with both traditional hand-crafted features and word embedding features. The hand-crafted features include all the baseline features of Zhou et al. (2005) (excluding the Country gaz"
N15-3018,P03-1054,0,0.0257485,"Chinese word. 3.3 Syntax Part of speech tagging and syntactic parsing are also provided by Stanford CoreNLP. The part of speech tagger is based on Toutanova et al. (2003) adapted for Chinese, which is a log-linear model underneath. Integration with C ONCRETE was facilitated by the concrete-stanford library 4 , though supporting Chinese required significant modifications to the 3 4 http://nlp.stanford.edu/software/corenlp.shtml https://github.com/hltcoe/concrete-stanford library. Resulting tags are stored in a C ONCRETE TokenTaggingList. Syntactic constituency parsing is based on the model of Klein and Manning (2003) adapted for Chinese. We obtained dependency parses from the CoreNLP dependency converter. We store the constituency parses as a C ONCRETE Parse, and the dependency analyses as C ON CRETE DependencyParses. 3.4 Named Entity Recognition We support the two most common named entity annotation standards: the CoNLL standard (four types: person, organization, location and miscellaneous), and the ACE standard, which includes the additional types of geo-political entity, facility, weapon and vehicle. The ACE standard also includes support for nested entities. We used the Stanford CoreNLP NER toolkit wh"
N15-3018,P14-5010,0,0.0125522,"ata sets include annotations for entities and a variety of relations (Aguilar et al., 2014). The labeled entities and relations are represented by C ONCRETE EntityMentions and stored in a EntityMentionSetList. Additional annotations that are typically utilized by relation extraction systems, such as syntactic parses, are provided automatically by the pipeline. 88 3.2 Word Segmentation Chinese text processing requires the identification of word boundaries, which are not indicated in written Chinese as they are in most other languages. Our word segmentation is provided by the Stanford CoreNLP3 (Manning et al., 2014) Chinese word segmentation tool, which is a conditional random field (CRF) model with character based features and lexicon features according to Chang et al. (2008). Word segmentations decisions are represented by C ONCRETE Token objects and stored in the TokenList. We follow the Chinese Penn Treebank segmentation standard (Xue et al., 2005). Our system tracks token offsets so that segmentation is robust to unexpected spaces or line breaks within a Chinese word. 3.3 Syntax Part of speech tagging and syntactic parsing are also provided by Stanford CoreNLP. The part of speech tagger is based on"
N15-3018,W12-3018,1,0.837337,"Missing"
N15-3018,P11-1053,0,0.0126349,"otations, in order to deal with the nested cases, we build one tagger for each entity type. Each entity is stored in a C ON CRETE EntityMention. 3.5 Relation Extraction Relations are extracted for every pair of entity mentions. We use a log-linear model with both traditional hand-crafted features and word embedding features. The hand-crafted features include all the baseline features of Zhou et al. (2005) (excluding the Country gazeteer and WordNet features), plus several additional carefully-chosen features that have been highly tuned for ACE-style relation extraction over years of research (Sun et al., 2011). The embedding-based features are from Yu et al. (2014), which represent each word as the outer product between its word embedding and a list of its associated non-lexical features. The non-lexical features indicate the word’s relative positions comparing to the target entities (whether the word is the head of any target entity, in-between the two entities, or on the dependency path between entities), which improve the expressive strength of word embeddings. We store the extracted relations in C ON CRETE SituationMentions. See Figure 2 for 89 Figure 2: ACE entity relations viewed through Quic"
N15-3018,N03-1033,0,0.0354521,"Chinese word segmentation tool, which is a conditional random field (CRF) model with character based features and lexicon features according to Chang et al. (2008). Word segmentations decisions are represented by C ONCRETE Token objects and stored in the TokenList. We follow the Chinese Penn Treebank segmentation standard (Xue et al., 2005). Our system tracks token offsets so that segmentation is robust to unexpected spaces or line breaks within a Chinese word. 3.3 Syntax Part of speech tagging and syntactic parsing are also provided by Stanford CoreNLP. The part of speech tagger is based on Toutanova et al. (2003) adapted for Chinese, which is a log-linear model underneath. Integration with C ONCRETE was facilitated by the concrete-stanford library 4 , though supporting Chinese required significant modifications to the 3 4 http://nlp.stanford.edu/software/corenlp.shtml https://github.com/hltcoe/concrete-stanford library. Resulting tags are stored in a C ONCRETE TokenTaggingList. Syntactic constituency parsing is based on the model of Klein and Manning (2003) adapted for Chinese. We obtained dependency parses from the CoreNLP dependency converter. We store the constituency parses as a C ONCRETE Parse, a"
N15-3018,P05-1053,0,0.027847,"l based on the method in Finkel et al. (2005), plus features based on Brown clustering. For the CoNLL standard annotations, we use one CRF model to label all the four types of entities. For the ACE standard annotations, in order to deal with the nested cases, we build one tagger for each entity type. Each entity is stored in a C ON CRETE EntityMention. 3.5 Relation Extraction Relations are extracted for every pair of entity mentions. We use a log-linear model with both traditional hand-crafted features and word embedding features. The hand-crafted features include all the baseline features of Zhou et al. (2005) (excluding the Country gazeteer and WordNet features), plus several additional carefully-chosen features that have been highly tuned for ACE-style relation extraction over years of research (Sun et al., 2011). The embedding-based features are from Yu et al. (2014), which represent each word as the outer product between its word embedding and a list of its associated non-lexical features. The non-lexical features indicate the word’s relative positions comparing to the target entities (whether the word is the head of any target entity, in-between the two entities, or on the dependency path betw"
N18-1067,S17-1027,0,0.0237433,"riments, we found that networks with three layers badly overfit the training data. Dependency parses For the T- and H-biLSTMs, we use the gold dependency parses provided in EUD1.2 when training and testing on UDS-IH2. On FactBank, MEANTIME, and UW, we follow Stanovsky et al. (2017) in using the automatic dependency parses generated by the parser in spaCy (Honnibal and Johnson, 2015).3 Lexical features Recent work on neural models in the closely related domain of genericity/habituality prediction suggests that inclusion of hand-annotated lexical features can improve classification performance (Becker et al., 2017). To assess whether similar performance gains can be obtained here, we experiment with lexical features for simple factive and implicative verbs (Kiparsky and Kiparsky, 1970; Karttunen, 1971a). When in use, these features are concatenated to the network’s input word embeddings so that, in principle, they may interact with one another and inform other hidden states in the biLSTM, akin to how verbal implicatives and factives are observed to influence the factuality of their complements. The hidden state size is increased to match the input embedding size. We consider two types: Signature feature"
N18-1067,P82-1020,0,0.770621,"Missing"
N18-1067,P16-1139,0,0.0219162,") because a model that uses dependency labels substantially increases the number of trainable paHidden state sizes We set the dimension of the (l,d) (l,d) hidden states ht and cell states ct to 300 for all layers of the stacked L- and stacked TbiLSTMs – the same size as the input word embeddings. This means that the input to the regression model is 600-dimensional, for the stacked Land T-biLSTMs, and 1200-dimensional, for the stacked H-biLSTM. For the hidden layer of the regression component, we set the dimension to half the size of the input hidden state: 300, for 2 See Miwa and Bansal 2016; Bowman et al. 2016 for alternative ways of hybridizing linear and tree LSTMs for semantic tasks. We use the current method since it allows us to make minimal changes to the architectures of each model, which in turn allows us to assess the two models’ ability to capture different aspects of factuality. 735 Verb know manage neglect hesitate attempt Signature Type Example +|+ +|− −|+ ◦|+ ◦|− fact. impl. impl. impl. impl. Jo knew that Bo ate. Jo managed to go. Jo neglected to call Bo. Jo didn’t hesitate to go. Jo didn’t attempt to go. implicative or factive behavior of a verb with respect to its complement clause,"
N18-1067,D15-1162,0,0.0174125,"ht side); ◦ indicates neither positive nor negative implication. the stacked L- and T-biLSTMs, and 600, for the stacked H-biLSTM. Bidirectional layers We consider stacked L-, T, and H-biLSTMs with either one or two layers. In preliminary experiments, we found that networks with three layers badly overfit the training data. Dependency parses For the T- and H-biLSTMs, we use the gold dependency parses provided in EUD1.2 when training and testing on UDS-IH2. On FactBank, MEANTIME, and UW, we follow Stanovsky et al. (2017) in using the automatic dependency parses generated by the parser in spaCy (Honnibal and Johnson, 2015).3 Lexical features Recent work on neural models in the closely related domain of genericity/habituality prediction suggests that inclusion of hand-annotated lexical features can improve classification performance (Becker et al., 2017). To assess whether similar performance gains can be obtained here, we experiment with lexical features for simple factive and implicative verbs (Kiparsky and Kiparsky, 1970; Karttunen, 1971a). When in use, these features are concatenated to the network’s input word embeddings so that, in principle, they may interact with one another and inform other hidden state"
N18-1067,D14-1070,0,0.110158,"Missing"
N18-1067,buck-etal-2014-n,0,0.0298369,"Missing"
N18-1067,S12-1020,0,0.293414,"Missing"
N18-1067,chang-manning-2012-sutime,0,0.0384804,"f Pavlick and Callison-Burch (2016) – henceforth, PC – and use corpus mining to automatically score verbs for implicativeness. The insight of PC lies in Karttunen’s (1971a) observation that “the main sentence containing an implicative predicate and the complement sentence necessarily agree in tense.” Accordingly, PC devise a tense agreement score – effectively, the ratio of times an embedding predicate’s tense matches the tense of the predicate it embeds – to predict implicativeness in English verbs. Their scoring method involves the use of fine-grained POS tags, the Stanford Temporal Tagger (Chang and Manning, 2012), and a number of heuristic rules, which resulted in a confirmation that tense agreement statistics are predictive of implicativeness, illustrated in part by observing a near perfect separation of a list of implicative and non-implicative verbs from Karttunen (1971a). Table 2: Implication signature features from Nairn et al. (2006). As an example, a signature of −|+ indicates negative implication under positive polarity (left side) and positive implication under negative polarity (right side); ◦ indicates neither positive nor negative implication. the stacked L- and T-biLSTMs, and 600, for the"
N18-1067,J12-2003,0,0.653305,"Missing"
N18-1067,W09-3012,0,0.534073,") and (5a), we infer that that predicate denotes a factual event, regardless of whether forget is negated. In contrast, when a predicate directly embedded by forget is untensed, as in (4b) and (5b), our inference is dependent on whether forget is negated. Thus, any model that correctly predicts factuality will need to not only be able to 732 Dataset FactBank MEANTIME UW UDS-IH2 Train Dev Test Total 6636 967 9422 22108 2462 210 3358 2642 663 218 864 2539 9761 1395 13644 27289 dency tree-based features (Lotan et al., 2013). Several systems use supervised models trained over rule-based features. Diab et al. (2009) and Prabhakaran et al. (2010) use SVMs and CRFs over lexical and dependency features for predicting author belief commitments, which they treat as a sequence tagging problem. Lee et al. (2015) train an SVM on lexical and dependency path features for their factuality dataset. Saur´ı and Pustejovsky (2012) and Stanovsky et al. (2017) train support vector models over the outputs of rule-based systems, the latter with TruthTeller. Table 1: Number of annotated predicates. ilar discrete factuality annotations. de Marneffe et al. (2012) re-annotate a portion of FactBank using crowd-sourced ordinal j"
N18-1067,D15-1189,0,0.233066,"models we propose as well as discussion of prior EFP datasets and systems (§2). We then describe our own extension of the UDS-IH1 dataset (§3), followed by our neural models (§4). Using the data we collect, along with the existing datasets, we evaluate our models (§6) in five experimental settings (§5) and analyze the results (§7). In this paper, we present two neural models of event factuality (and several variants thereof). We show that these models significantly outperform previous systems on four existing event factuality datasets – FactBank (Saur´ı and Pustejovsky, 2009), the UW dataset (Lee et al., 2015), MEANTIME (Minard et al., 2016), and Universal DeWords from effectively every syntactic category can convey information about the factuality of an event. For instance, negation (2a), modal auxiliaries (2b), determiners (2c), adverbs (2d), verbs (2e), adjectives (2f), and nouns (2g) can all con(1) Jo failed to leave no trace. 2 ⊕ Background 2.1 1 Linguistic description Data available at decomp.net. 731 Proceedings of NAACL-HLT 2018, pages 731–744 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics represent the effect of individual words in the outside co"
N18-1067,W06-3907,0,0.940443,"hether the sentence is understandable 2. PREDICATE: whether or not a particular word refers to an eventuality (event or state) 3. HAPPENED: whether or not, according to the author, the event has already happened or is currently happening 4. CONFIDENCE: how confident the annotator is about their answer to HAPPENED from 0-4 If an annotator answers no to either UNDER STANDABLE or PREDICATE , HAPPENED and CONFIDENCE do not appear. The main differences between this protocol and the others discussed above are: (i) instead of asking about annotator confidence, the other protoEvent factuality systems Nairn et al. (2006) propose a deterministic algorithm based on hand-engineered lexical features for determining event factuality. They associate certain clause-embedding verbs with implication signatures (Table 2), which are used in a recursive polarity propagation algorithm. TruthTeller is also a recursive rule-based system for factuality (“predicate truth”) prediction using implication signatures, as well as other lexical- and depen733 like the newswire-heavy corpora that the other datasets annotate, EUD1.2 contains text from genres – weblogs, newsgroups, email, reviews, and question-answers – that tend to inv"
N18-1067,J12-2002,0,\N,Missing
N18-1067,P15-1150,0,\N,Missing
N18-1067,P14-2068,0,\N,Missing
N18-1067,Q14-1017,0,\N,Missing
N18-1067,S13-2001,0,\N,Missing
N18-1067,prasad-etal-2008-penn,0,\N,Missing
N18-1067,D14-1162,0,\N,Missing
N18-1067,N13-1091,0,\N,Missing
N18-1067,D16-1046,0,\N,Missing
N18-1067,D16-1240,0,\N,Missing
N18-1067,D16-1177,1,\N,Missing
N18-1067,C10-2117,0,\N,Missing
N18-1067,P16-1105,0,\N,Missing
N18-2002,W17-1602,0,0.0639067,"Missing"
N18-2002,P06-1005,0,0.196152,"Missing"
N18-2002,W17-1601,0,0.0318041,"e as a lower bound for equitability in automated systems. Prior work also analyzes social and gender stereotyping in existing NLP and vision datasets (van Miltenburg, 2016; Rudinger et al., 2017). Tatman (2017) investigates the impact of gender and dialect on deployed speech recognition systems, while Zhao et al. (2017) introduce a method to reduce amplification effects on models trained with gender-biased datasets. Koolen and van Cranenburgh (2017) examine the relationship between author gender and text attributes, noting the potential for researcher interpretation bias in such studies. Both Larson (2017) and Koolen and van Cranenburgh (2017) offer guidelines to NLP researchers and computational social scientists who wish to predict gender as a variable. Hovy and Spruit (2016) introduce a helpful set of terminology for identifying and categorizing types of bias that manifest in AI systems, including overgeneralization, which we observe in our work here. Finally, we note independent but closely related work by Zhao et al. (2018), published concurrently with this paper. In their work, Zhao et al. (2018) also propose a Winograd schema-like test for gender bias in coreference resolution systems (c"
N18-2002,P14-1005,0,0.0287752,"Missing"
N18-2002,W11-1902,0,0.14151,"Missing"
N18-2002,W12-4501,0,0.119796,"Missing"
N18-2002,W11-1901,0,0.0970937,"Missing"
N18-2002,D16-1245,0,0.027164,"cupation+pronoun feature can be highly informative, and the overly confident model can exhibit strong bias when applied to a new domain. 1. OCCUPATION , a person referred to by their occupation and a definite article, e.g., “the paramedic.” 2. PARTICIPANT , a secondary (human) participant, e.g., “the passenger.” 3. PRONOUN , a pronoun that is coreferent with either OCCUPATION or PARTICIPANT. Neural The move to deep neural models led to more powerful antecedent scoring functions, and the subsequent learned feature combinations resulted in new state-of-the-art performance (Wiseman et al., 2015; Clark and Manning, 2016b). Global inference over these models further improved performance (Wiseman et al., 2016; Clark and Manning, 2016a), but from the perspective of potential bias, the information available to the model is largely the same as in the statistical models. A notable exception is in the case of systems which make use of pre-trained word embeddings (Clark and Manning, 2016b), which have been shown to contain bias and have the potential to introduce bias into the system. We use a list of 60 one-word occupations obtained from Caliskan et al. (2017) (see supplement), with corresponding gender percentages"
N18-2002,P16-1061,0,0.0188781,"cupation+pronoun feature can be highly informative, and the overly confident model can exhibit strong bias when applied to a new domain. 1. OCCUPATION , a person referred to by their occupation and a definite article, e.g., “the paramedic.” 2. PARTICIPANT , a secondary (human) participant, e.g., “the passenger.” 3. PRONOUN , a pronoun that is coreferent with either OCCUPATION or PARTICIPANT. Neural The move to deep neural models led to more powerful antecedent scoring functions, and the subsequent learned feature combinations resulted in new state-of-the-art performance (Wiseman et al., 2015; Clark and Manning, 2016b). Global inference over these models further improved performance (Wiseman et al., 2016; Clark and Manning, 2016a), but from the perspective of potential bias, the information available to the model is largely the same as in the statistical models. A notable exception is in the case of systems which make use of pre-trained word embeddings (Clark and Manning, 2016b), which have been shown to contain bias and have the potential to introduce bias into the system. We use a list of 60 one-word occupations obtained from Caliskan et al. (2017) (see supplement), with corresponding gender percentages"
N18-2002,W17-1609,1,0.821069,"Missing"
N18-2002,D13-1203,0,0.0347183,"m. 3 Winogender Schemas Our intent is to reveal cases where coreference systems may be more or less likely to recognize a pronoun as coreferent with a particular occupation based on pronoun gender, as observed in Figure 1. To this end, we create a specialized evaluation set consisting of 120 hand-written sentence templates, in the style of the Winograd Schemas (Levesque et al., 2011). Each sentence contains three referring expressions of interest: Statistical Statistical methods, often with millions of parameters, ultimately surpassed the performance of rule-based systems on shared task data (Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). The system of Durrett and Klein (2013) replaced hand-written rules with simple feature templates. Combinations of these features implicitly capture linguistic phenomena useful for resolving antecedents, but they may also unintentionally capture bias in the data. For instance, for occupations which are not frequently found in the data, an occupation+pronoun feature can be highly informative, and the overly confident model can exhibit strong bias when applied to a new domain. 1. OCCUPATION , a person referred to by their occupation and a definite article, e.g., “th"
N18-2002,W17-1606,0,0.0487341,"er schemas may be extended broadly to probe for other manifestations of gender bias. Though we have used human-validated schemas to demonstrate that existing NLP systems are comparatively more prone to gender-based overgeneralization, we do not presume that matching human judgment is the ultimate objective of this line of research. Rather, human judgements, which carry their own implicit biases, serve as a lower bound for equitability in automated systems. Prior work also analyzes social and gender stereotyping in existing NLP and vision datasets (van Miltenburg, 2016; Rudinger et al., 2017). Tatman (2017) investigates the impact of gender and dialect on deployed speech recognition systems, while Zhao et al. (2017) introduce a method to reduce amplification effects on models trained with gender-biased datasets. Koolen and van Cranenburgh (2017) examine the relationship between author gender and text attributes, noting the potential for researcher interpretation bias in such studies. Both Larson (2017) and Koolen and van Cranenburgh (2017) offer guidelines to NLP researchers and computational social scientists who wish to predict gender as a variable. Hovy and Spruit (2016) introduce a helpful s"
N18-2002,P16-2096,0,0.284703,"this be? That a majority of people are reportedly unable to solve this riddle1 is taken as evidence of underlying implicit gender bias (Wapman and Belle, 2014): many first-time listeners have difficulty assigning both the role of “mother” and “surgeon” to the same entity. As the riddle reveals, the task of coreference resolution in English is tightly bound with questions of gender, for humans and automated systems alike (see Figure 1). As awareness grows of the ways in which data-driven AI technologies may acquire and amplify human-like biases (Caliskan et al., 2017; Barocas and Selbst, 2016; Hovy and Spruit, 2016), this work investigates how gender biases manifest in coreference resolution systems. There are many ways one could approach this question; here we focus on gender bias with respect to occupations, for which we have corresponding U.S. employment statistics. Our approach is to construct a challenge dataset in 1 2 Coreference Systems In this work, we evaluate three publiclyavailable off-the-shelf coreference resolution systems, representing three different machine learning paradigms: rule-based systems, feature-driven 2 https://github.com/rudinger/ winogender-schemas The surgeon is the boy’s mo"
N18-2002,P15-1137,0,0.0126939,"und in the data, an occupation+pronoun feature can be highly informative, and the overly confident model can exhibit strong bias when applied to a new domain. 1. OCCUPATION , a person referred to by their occupation and a definite article, e.g., “the paramedic.” 2. PARTICIPANT , a secondary (human) participant, e.g., “the passenger.” 3. PRONOUN , a pronoun that is coreferent with either OCCUPATION or PARTICIPANT. Neural The move to deep neural models led to more powerful antecedent scoring functions, and the subsequent learned feature combinations resulted in new state-of-the-art performance (Wiseman et al., 2015; Clark and Manning, 2016b). Global inference over these models further improved performance (Wiseman et al., 2016; Clark and Manning, 2016a), but from the perspective of potential bias, the information available to the model is largely the same as in the statistical models. A notable exception is in the case of systems which make use of pre-trained word embeddings (Clark and Manning, 2016b), which have been shown to contain bias and have the potential to introduce bias into the system. We use a list of 60 one-word occupations obtained from Caliskan et al. (2017) (see supplement), with corresp"
N18-2002,N16-1114,0,0.0477683,"Missing"
N18-2002,D17-1323,0,0.132641,"uman-validated schemas to demonstrate that existing NLP systems are comparatively more prone to gender-based overgeneralization, we do not presume that matching human judgment is the ultimate objective of this line of research. Rather, human judgements, which carry their own implicit biases, serve as a lower bound for equitability in automated systems. Prior work also analyzes social and gender stereotyping in existing NLP and vision datasets (van Miltenburg, 2016; Rudinger et al., 2017). Tatman (2017) investigates the impact of gender and dialect on deployed speech recognition systems, while Zhao et al. (2017) introduce a method to reduce amplification effects on models trained with gender-biased datasets. Koolen and van Cranenburgh (2017) examine the relationship between author gender and text attributes, noting the potential for researcher interpretation bias in such studies. Both Larson (2017) and Koolen and van Cranenburgh (2017) offer guidelines to NLP researchers and computational social scientists who wish to predict gender as a variable. Hovy and Spruit (2016) introduce a helpful set of terminology for identifying and categorizing types of bias that manifest in AI systems, including overgen"
N18-2002,N18-2003,0,0.16657,"and van Cranenburgh (2017) examine the relationship between author gender and text attributes, noting the potential for researcher interpretation bias in such studies. Both Larson (2017) and Koolen and van Cranenburgh (2017) offer guidelines to NLP researchers and computational social scientists who wish to predict gender as a variable. Hovy and Spruit (2016) introduce a helpful set of terminology for identifying and categorizing types of bias that manifest in AI systems, including overgeneralization, which we observe in our work here. Finally, we note independent but closely related work by Zhao et al. (2018), published concurrently with this paper. In their work, Zhao et al. (2018) also propose a Winograd schema-like test for gender bias in coreference resolution systems (called “WinoBias”). Though similar in appearance, these two efforts have notable differences in substance and emphasis. The contribution of this work is focused primarily on schema construction and validation, with extensive analysis of observed system bias, revealing its correlation with biases present in real-world and textual statistics; by contrast, Zhao et al. (2018) present methods of debiasing existing systems, showing th"
N18-2082,W17-3209,0,0.0195564,"as used to train the NMT encoder, in capturing semantic proto-roles and paraphrastic inference. In Table 1, we notice a large improvement using sentence representations from an NMT encoder that was trained on en-es parallel text. The improvements are most profound when a classifier trained on DPR data predicts entailment focused on seAppendix D includes some illustrative examples. This is seen in the last columns of the top row in Table 1. 516 MNLI-1 MNLI-2 ar es zh de MAJ 45.9 46.6 45.7 46.7 46.6 48.2 48.0 48.9 35.6 36.5 Gao and Vogel (2011) add semantic-roles to improve phrase-based MT, and Carpuat et al. (2017) demonstrate how filtering parallel sentences that are not parallel in meaning improves translation. Recent work explores how representations learned by NMT systems can improve semantic tasks. McCann et al. (2017) show improvements in many tasks by using contextualized word vectors extracted from a LSTM encoder trained for MT. Their goal is to use NMT to improve other tasks while we focus on using NLI to determine what NMT models learn about different semantic phenomena. Researchers have explored what NMT models learn about other linguistic phenomena, such as morphology (Dalvi et al., 2017; Be"
N18-2082,D17-1311,0,0.0251135,"Berry Rejoins WPP Group Berry was sentient 7 3 3 Figure 1: Example sentence pairs for the different semantic phenomena. DPR deals with complex anaphora resolution, FN+ is concerned with paraphrastic inference, and SPR covers Reisinger et al. (2015)’s semantic proto-roles. 3 / 7 indicates that the first sentence entails / does not entail the second. What do neural machine translation (NMT) models learn about semantics? Many researchers suggest that state-of-the-art NMT models learn representations that capture the meaning of sentences (Gu et al., 2016; Johnson et al., 2017; Zhou et al., 2017; Andreas and Klein, 2017; Neubig, 2017; Koehn, 2017). However, there is limited understanding of how specific semantic phenomena are captured in NMT representations beyond this broad notion. For instance, how well do these representations capture Dowty (1991)’s thematic proto-roles? Are these representations sufficient for understanding paraphrastic inference? Do the sentence representations encompass complex anaphora resolution? We argue that existing semantic annotations recast as Natural Language Inference (NLI) can be leveraged to investigate whether sentence representations encoded by NMT models capture these se"
N18-2082,P07-1005,0,0.0673881,"Missing"
N18-2082,D16-1053,0,0.0226068,"Missing"
N18-2082,D17-1070,0,0.121882,"Missing"
N18-2082,D15-1075,0,0.48305,"e NLI sentence pairs with their respective labels and semantic phenomena. We evaluate NMT sentence representations of 4 NMT models from 2 domains on 4 different NLI datasets to investigate how well they capture different semantic phenomena. We use White et al. (2017)’s Unified Semantic Evaluation Framework (USEF) that recasts three semantic phenomena NLI: 1) semantic proto-roles, 2) paraphrastic inference, 3) and complex anaphora resolution. Additionally, we evaluate the NMT sentence representations on 4) Multi-NLI, a recent extension of the Stanford Natural Language Inference dataset (SNLI) (Bowman et al., 2015) that includes multiple genres and domains (Williams et al., 1 Code developed and data used are available at https: //github.com/boknilev/nmt-repr-analysis. 2 Sometimes referred to as recognizing textual entailment (Dagan et al., 2006, 2013). 1. Introduction 513 Proceedings of NAACL-HLT 2018, pages 513–523 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics so correctly identifying them can be important for translation. For example, English does not usually explicitly mark volition, a proto-role, except by using adverbs like intentionally or accidentally."
N18-2082,I17-1015,1,0.829229,"Carpuat et al. (2017) demonstrate how filtering parallel sentences that are not parallel in meaning improves translation. Recent work explores how representations learned by NMT systems can improve semantic tasks. McCann et al. (2017) show improvements in many tasks by using contextualized word vectors extracted from a LSTM encoder trained for MT. Their goal is to use NMT to improve other tasks while we focus on using NLI to determine what NMT models learn about different semantic phenomena. Researchers have explored what NMT models learn about other linguistic phenomena, such as morphology (Dalvi et al., 2017; Belinkov et al., 2017a), syntax (Shi et al., 2016), and lexical semantics (Belinkov et al., 2017b), including word senses (Marvin and Koehn, 2018; Liu et al., 2018) Table 5: Accuracies for MNLI test sets. MNLI-1 refers to the matched case and MNLI-2 is the mismatched. mantic proto-roles or paraphrastic inference. We also note that using the NMT encoder trained on en-es parallel text results in the highest results in 5 of the 6 proto-roles in the top portion of Table 4. When using other sentence representations (Appendix A), we notice that using representations from English-German encoders co"
N18-2082,P14-2124,0,0.0670495,"Missing"
N18-2082,W11-1012,0,0.0246855,"nguage Our experiments show differences based on which target language was used to train the NMT encoder, in capturing semantic proto-roles and paraphrastic inference. In Table 1, we notice a large improvement using sentence representations from an NMT encoder that was trained on en-es parallel text. The improvements are most profound when a classifier trained on DPR data predicts entailment focused on seAppendix D includes some illustrative examples. This is seen in the last columns of the top row in Table 1. 516 MNLI-1 MNLI-2 ar es zh de MAJ 45.9 46.6 45.7 46.7 46.6 48.2 48.0 48.9 35.6 36.5 Gao and Vogel (2011) add semantic-roles to improve phrase-based MT, and Carpuat et al. (2017) demonstrate how filtering parallel sentences that are not parallel in meaning improves translation. Recent work explores how representations learned by NMT systems can improve semantic tasks. McCann et al. (2017) show improvements in many tasks by using contextualized word vectors extracted from a LSTM encoder trained for MT. Their goal is to use NMT to improve other tasks while we focus on using NLI to determine what NMT models learn about different semantic phenomena. Researchers have explored what NMT models learn abo"
N18-2082,P11-1023,0,0.0503801,"Missing"
N18-2082,P14-5010,0,0.00252649,"hat our classifiers using the representations from the NMT encoder perform poorly. Although the sentences in FN+ are much longer than in the other datasets, sentence length does not seem to be responsible for the poor FN+ results. The classifiers do not noticeably perform better on shorter sentences than longer ones, as noted in Appendix C. Upon manual inspection, we noticed that in many not-entailed examples, swapped paraphrases had different part-of-speech (POS) tags. This begs the question of whether different POS tags for swapped paraphrases affects the accuracies. Using Stanford CoreNLP (Manning et al., 2014), we partition our validation set based on whether the paraphrases share the same POS tag. Table 3 reports dev set accuracies using classifiers trained on FN+. Classifiers using features from NMT encoders trained on the three languages from the UN corpus noticeably perform better on cases where paraphrases have different POS tags compared to paraphrases with the same POS tags. These difNatural Language Inference data We use four distinct datasets to train classifiers: MultiNLI (Williams et al., 2017), a recent expansion of SNLI containing a broad array of domains that was used in the 2017 RepE"
N18-2082,P02-1031,0,0.0462015,"rforms the baseline for a proto-role, all the other classifiers do as well. The classifiers outperform the majority baseline for 6 of the reported 16 proto-roles. We observe these 6 properties are more associated with proto-agents than proto-patients. The larger improvements over the majority baseline for SPR compared to FN+ and DPR is not surprising. Dowty (1991) posited that proto-agent, and -patient should correlate with English syntactic subject, and object, respectively, and empirically the necessity of [syntactic] parsing for predicate argument recognition has been observed in practice (Gildea and Palmer, 2002; Punyakanok et al., 2008). Further, recent work is suggestive that LSTM-based frameworks implicitly may encode syntax based on certain learning objectives (Linzen et al., 2016; Shi et al., 2016; Belinkov et al., 2017b). It is unclear whether NMT encoders capture semantic proto-roles specifically or just underlying syntax that affects the proto-roles. Proto-role entailment (SPR) When predicting SPR entailments using a classifier trained on SPR data, we noticeably outperform the majority baseline but are below USEF. Both ours and USEF’s accuracies are lower than Teichert et al. (2017)’s best re"
N18-2082,W18-1812,0,0.0223432,"how representations learned by NMT systems can improve semantic tasks. McCann et al. (2017) show improvements in many tasks by using contextualized word vectors extracted from a LSTM encoder trained for MT. Their goal is to use NMT to improve other tasks while we focus on using NLI to determine what NMT models learn about different semantic phenomena. Researchers have explored what NMT models learn about other linguistic phenomena, such as morphology (Dalvi et al., 2017; Belinkov et al., 2017a), syntax (Shi et al., 2016), and lexical semantics (Belinkov et al., 2017b), including word senses (Marvin and Koehn, 2018; Liu et al., 2018) Table 5: Accuracies for MNLI test sets. MNLI-1 refers to the matched case and MNLI-2 is the mismatched. mantic proto-roles or paraphrastic inference. We also note that using the NMT encoder trained on en-es parallel text results in the highest results in 5 of the 6 proto-roles in the top portion of Table 4. When using other sentence representations (Appendix A), we notice that using representations from English-German encoders consistently outperforms using the other encoders (Tables 6 and 7). This prevents us from making generalizations regarding specific target side langu"
N18-2082,P16-1154,0,0.016256,"ses five research reactors Iran has five research reactors Berry Rejoins WPP Group Berry was sentient 7 3 3 Figure 1: Example sentence pairs for the different semantic phenomena. DPR deals with complex anaphora resolution, FN+ is concerned with paraphrastic inference, and SPR covers Reisinger et al. (2015)’s semantic proto-roles. 3 / 7 indicates that the first sentence entails / does not entail the second. What do neural machine translation (NMT) models learn about semantics? Many researchers suggest that state-of-the-art NMT models learn representations that capture the meaning of sentences (Gu et al., 2016; Johnson et al., 2017; Zhou et al., 2017; Andreas and Klein, 2017; Neubig, 2017; Koehn, 2017). However, there is limited understanding of how specific semantic phenomena are captured in NMT representations beyond this broad notion. For instance, how well do these representations capture Dowty (1991)’s thematic proto-roles? Are these representations sufficient for understanding paraphrastic inference? Do the sentence representations encompass complex anaphora resolution? We argue that existing semantic annotations recast as Natural Language Inference (NLI) can be leveraged to investigate wheth"
N18-2082,P15-2067,1,0.667806,"Missing"
N18-2082,D17-3004,1,0.884949,"Missing"
N18-2082,Q15-1034,1,\N,Missing
N18-2082,P17-1080,1,\N,Missing
N18-2082,W17-5301,0,\N,Missing
N18-2082,I17-1100,1,\N,Missing
N18-2082,W17-5703,0,\N,Missing
N18-2082,N18-1121,0,\N,Missing
N18-2082,I17-1001,1,\N,Missing
N18-2082,J13-3001,0,\N,Missing
N18-2082,2020.wmt-1.63,0,\N,Missing
N18-2082,2020.coling-tutorials.3,0,\N,Missing
N19-1090,I17-2075,0,0.0203472,"ntation has been used to improve performance and robustness in deep neural models. In NMT, the most common approach is back-translation, where monolingual text in the target language is translated to create synthetic source sentences (Sennrich et al., 2016). Variants of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, though they only consider word-level rewrites and encourage label-preservation, while we paraphrase whole sentences with lexical constraints, independent of a gold label. Most similar to our experiments, Iyyer et al. (2018) explored syntactic paraphrasing for augmentat"
N19-1090,D18-1316,0,0.0513884,"Missing"
N19-1090,D17-1070,0,0.0136465,"ns. Given both the numerical boost seen by aggregation and the above examples, we hypothesize that the rewriter does not frequently change entailment semantics. Because the semantics remain similar, and because the paraphrases were gener5.2 Question Answering We apply our paraphrastic rewriter to the task of question answer sentence selection to see if augmenting with paraphrases leads to improvements. The task is defined as follows: Given a question q and a set of candidate sentences {ci }, select the candidates which answer q. Model We adapt a popular neural architecture for NLI, InferSent (Conneau et al., 2017), to our QA sentence selection task. In InferSent, the questions and answers (originally the premises and hypotheses) are embedded using an uncontextualized word embedding (e.g. GloVe), which we also experiment with ELMo (Peters et al., 2018) to incorporate recent advancements in large-scale contextualized pre-training. Bidirectional LSTMs (Graves and Schmidhuber, 2005) are run atop of these contextualized embeddings and a maxpooling layer is used to generate a feature vector for both the question and the answer. Following various matching methods (Mou et al., 2016) and a multi-layer feed-forw"
N19-1090,D17-1098,0,0.276218,"ster exploration of constraint strategies: we illustrate this via data augmentation experiments with a monolingual rewriter applied to the tasks of natural language inference, question answering and machine translation, showing improvements in all three. 1 Introduction For many natural language generation tasks, we often know word(s) that should (or should not) be in the output sentence. Examples include terminology databases in Machine Translation (MT) (Hokamp and Liu, 2017), names (and generic responses) in dialogue generation (Li et al., 2016; Gu et al., 2016), objects in image captioning (Anderson et al., 2017), and facts in abstractive summarization (See et al., 2017). One approach to enforce hard lexical constraints in the output is to modify the inference procedure to enforce their presence directly (Hokamp and Liu, 2017). These constraints could be either positive (a word must appear in the output) or negative (a word must be avoided). While negative constraints could be easily enforced by preventing hypotheses with prohibited tokens from entering the beam, placing positive constraints in natural and meaningful ways is less straightforward. We improve upon previous work by vectorizing the dynami"
N19-1090,D18-1045,0,0.0242484,"iter; nor demonstrate its utility on NLP tasks. • Monolingual rewriting constraint heuristics for automatic data augmentation leading to improvements on NLI / QA / MT. 2 Data augmentation Data augmentation has been used to improve performance and robustness in deep neural models. In NMT, the most common approach is back-translation, where monolingual text in the target language is translated to create synthetic source sentences (Sennrich et al., 2016). Variants of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, though they only consider word-level rewrites and encourage label-pr"
N19-1090,P17-2090,0,0.0246626,"asks. • Monolingual rewriting constraint heuristics for automatic data augmentation leading to improvements on NLI / QA / MT. 2 Data augmentation Data augmentation has been used to improve performance and robustness in deep neural models. In NMT, the most common approach is back-translation, where monolingual text in the target language is translated to create synthetic source sentences (Sennrich et al., 2016). Variants of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, though they only consider word-level rewrites and encourage label-preservation, while we paraphrase whole sent"
N19-1090,D18-1040,0,0.0196406,"e-wild sentences; explore more sophisticated versions of the rewriter; nor demonstrate its utility on NLP tasks. • Monolingual rewriting constraint heuristics for automatic data augmentation leading to improvements on NLI / QA / MT. 2 Data augmentation Data augmentation has been used to improve performance and robustness in deep neural models. In NMT, the most common approach is back-translation, where monolingual text in the target language is translated to create synthetic source sentences (Sennrich et al., 2016). Variants of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, thoug"
N19-1090,N13-1092,1,0.879148,"Missing"
N19-1090,P16-1154,0,0.0178036,"ositive constraints. Faster decoding enables faster exploration of constraint strategies: we illustrate this via data augmentation experiments with a monolingual rewriter applied to the tasks of natural language inference, question answering and machine translation, showing improvements in all three. 1 Introduction For many natural language generation tasks, we often know word(s) that should (or should not) be in the output sentence. Examples include terminology databases in Machine Translation (MT) (Hokamp and Liu, 2017), names (and generic responses) in dialogue generation (Li et al., 2016; Gu et al., 2016), objects in image captioning (Anderson et al., 2017), and facts in abstractive summarization (See et al., 2017). One approach to enforce hard lexical constraints in the output is to modify the inference procedure to enforce their presence directly (Hokamp and Liu, 2017). These constraints could be either positive (a word must appear in the output) or negative (a word must be avoided). While negative constraints could be easily enforced by preventing hypotheses with prohibited tokens from entering the beam, placing positive constraints in natural and meaningful ways is less straightforward. We"
N19-1090,E17-3017,0,0.025193,"tic pairs in PARA BANK originated from CzEng5 that: (1) have a regression score over 0.50; (2) only consist of ASCII characters after punctuation normalization; and (3) have a reference/paraphrase token Jaccard index between 0.25 and 0.65. We retain 141,381,887 paraphrastic pairs, out of over 220 million, as training data after applying these filters. To ensure output quality, we only use back-translated paraphrases as source. PARA BANK is a real-cased resource. We mark all words that have first-character capitalization and convert them to lowercase. The marking is Evaluation We use S OCKEYE (Hieber et al., 2017)4 for our evaluations. We trained a 6-layer German–English Transformer using the default settings on the WMT’18 training data and the newstest2018 test set for evaluation (Bojar et al., 2018). Following Post and Vilar (2018), we compare decoding results in an unconstrained setting and with two sets of positive constraints: “rand3”, which selects 3 random words from the reference, and “phr4”, which selects a single 4-word phrase. We report decoding speed (in sentences per second) and BLEU score (Papineni et al., 2002), as measured by SacreBLEU (Post, 2018). The results are 4 Improved Monolingua"
N19-1090,P17-1141,0,0.345145,"ive translation and domain adaptation. Translation applications handling large amounts of data will clearly benefit from improvements in speed: the same is true for large-scale data augmentation via rewriting. In this case, a practitioner will ideally explore various task-specific rewriting strategies that may lead to improvements as observed during development, and then incorporate the best strategy into a test-final model. Recently, sentential paraphrasing gained the ability to enforce lexical constraints (Hu et al., 2019), but constrained decoding was still too inefficient to be practical (Hokamp and Liu, 2017) at a large scale. Even with the approach described by Post and Vilar, exploring the space of possible rewriting strategies on a taskspecific basis may be overly time consuming: our performance improvements to their algorithm lowers the barrier of entry, where one may more practically experiment with various strategies during development. To illustrate our point, we build an improved monolingual sentential rewriter that can be conditioned on arbitrary positive and negative lexical constraints and use this to augment data for three external NLP tasks with different strategies: Natural Language"
N19-1090,C18-1105,0,0.0135787,"e most common approach is back-translation, where monolingual text in the target language is translated to create synthetic source sentences (Sennrich et al., 2016). Variants of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, though they only consider word-level rewrites and encourage label-preservation, while we paraphrase whole sentences with lexical constraints, independent of a gold label. Most similar to our experiments, Iyyer et al. (2018) explored syntactic paraphrasing for augmentation in sentiment and NLI tasks, extending prior work on PARA NMT. Background Constraine"
N19-1090,2015.iwslt-evaluation.11,0,0.0433639,"Missing"
N19-1090,E17-1083,0,0.0375255,"hs in dependency trees. Weisman et al. (2012) explored learning inference relations between verbs in broader scopes (document or corpus level). PPDB (Ganitkevitch et al., 2013) constructs paraphrase pairs by linking words or phrases that share the same translation in another language. PARA NMT (Wieting and Gimpel, 2018) and PARA BANK (Hu et al., 2019) used back-translation to build a large paraphrase collection from bilingual corpora. For arbitrary sentence rewriting, Napoles et al. (2016) used statistical machine translation in tandem with PPDB as a black box monolingual sentential rewriter. Mallinson et al. (2017) used a series of NMT model pairs to perform backtranslations for monolingual paraphrasing. A similar approach was adopted by PARA NMT to create a large paraphrase collection, which is used to train a monolingual sentence rewriter for canonicalization. PARA BANK (Hu et al., 2019) extends 1 3 Improved Constrained Decoding Lexically-constrained decoding is a modification to beam search that yields decoder outputs honoring user-supplied constraints. These constraints can be provided in the form of: positive constraints, which specify that certain tokens or token sequences must be present in the o"
N19-1090,N18-1170,0,0.0440982,"ly used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, though they only consider word-level rewrites and encourage label-preservation, while we paraphrase whole sentences with lexical constraints, independent of a gold label. Most similar to our experiments, Iyyer et al. (2018) explored syntactic paraphrasing for augmentation in sentiment and NLI tasks, extending prior work on PARA NMT. Background Constrained decoding Prior work explored methods to apply lexical constraints to a Neural Machine Translation (NMT) decoder (Hokamp and Liu, 2017; Anderson et al., 2017). However, most of these methods are slow and impractical as they change beam sizes at different time steps, which breaks the optimized computation graph. Post and Vilar (2018) proposed a means of dynamically allocating the slots in a fixed-size beam to ensure that even progress was made in meeting an arbit"
N19-1090,P16-2022,0,0.0214771,"ure for NLI, InferSent (Conneau et al., 2017), to our QA sentence selection task. In InferSent, the questions and answers (originally the premises and hypotheses) are embedded using an uncontextualized word embedding (e.g. GloVe), which we also experiment with ELMo (Peters et al., 2018) to incorporate recent advancements in large-scale contextualized pre-training. Bidirectional LSTMs (Graves and Schmidhuber, 2005) are run atop of these contextualized embeddings and a maxpooling layer is used to generate a feature vector for both the question and the answer. Following various matching methods (Mou et al., 2016) and a multi-layer feed-forward neural network, the model produces a final score. We train the system following the method proposed by Rao et al. (2016), utilizing a ranking loss (Weston and Watkins, 1999) that contrasts positive answers against negative ones. 846 Paraphrase Generation We augment each answer candidate sentence with exactly 1 paraphrase in the dataset using the following heuristics: (1) named entities shared between a specific answer and its corresponding question are retained as positive constraints; (2) correct answer spans are retained as positive constraints; (3) words with"
N19-1090,P16-1002,0,0.0135507,"e and robustness in deep neural models. In NMT, the most common approach is back-translation, where monolingual text in the target language is translated to create synthetic source sentences (Sennrich et al., 2016). Variants of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, though they only consider word-level rewrites and encourage label-preservation, while we paraphrase whole sentences with lexical constraints, independent of a gold label. Most similar to our experiments, Iyyer et al. (2018) explored syntactic paraphrasing for augmentation in sentiment and NLI tasks, extendin"
N19-1090,N16-3013,1,0.86673,"astic expressions through existing corpora. For example, DIRT (Lin and Pantel, 2001) extracts paraphrastic expressions from paths in dependency trees. Weisman et al. (2012) explored learning inference relations between verbs in broader scopes (document or corpus level). PPDB (Ganitkevitch et al., 2013) constructs paraphrase pairs by linking words or phrases that share the same translation in another language. PARA NMT (Wieting and Gimpel, 2018) and PARA BANK (Hu et al., 2019) used back-translation to build a large paraphrase collection from bilingual corpora. For arbitrary sentence rewriting, Napoles et al. (2016) used statistical machine translation in tandem with PPDB as a black box monolingual sentential rewriter. Mallinson et al. (2017) used a series of NMT model pairs to perform backtranslations for monolingual paraphrasing. A similar approach was adopted by PARA NMT to create a large paraphrase collection, which is used to train a monolingual sentence rewriter for canonicalization. PARA BANK (Hu et al., 2019) extends 1 3 Improved Constrained Decoding Lexically-constrained decoding is a modification to beam search that yields decoder outputs honoring user-supplied constraints. These constraints ca"
N19-1090,P02-1040,0,0.104681,"ation and convert them to lowercase. The marking is Evaluation We use S OCKEYE (Hieber et al., 2017)4 for our evaluations. We trained a 6-layer German–English Transformer using the default settings on the WMT’18 training data and the newstest2018 test set for evaluation (Bojar et al., 2018). Following Post and Vilar (2018), we compare decoding results in an unconstrained setting and with two sets of positive constraints: “rand3”, which selects 3 random words from the reference, and “phr4”, which selects a single 4-word phrase. We report decoding speed (in sentences per second) and BLEU score (Papineni et al., 2002), as measured by SacreBLEU (Post, 2018). The results are 4 Improved Monolingual Rewriter 5 PARA BANK generated paraphrases from two large bilingual corpora, CzEng (Bojar et al., 2016a) and GigaFrEn (Callison-Burch et al., 2009). We picked paraphrases from only CzEng, the larger one of the two. https://github.com/awslabs/sockeye/ 843 used as a source factor (Sennrich and Haddow, 2016) to the encoder. This helps us to decrease the vocabulary size of the the training data. We learn a shared byte-pair encoding (BPE) over the entire training data with 30,000 BPE operations (Sennrich et al., 2016),"
N19-1090,N18-2072,0,0.0130767,"of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, though they only consider word-level rewrites and encourage label-preservation, while we paraphrase whole sentences with lexical constraints, independent of a gold label. Most similar to our experiments, Iyyer et al. (2018) explored syntactic paraphrasing for augmentation in sentiment and NLI tasks, extending prior work on PARA NMT. Background Constrained decoding Prior work explored methods to apply lexical constraints to a Neural Machine Translation (NMT) decoder (Hokamp and Liu, 2017; Anderson et al., 2017). However, mos"
N19-1090,N18-1202,0,0.0123599,"estion Answering We apply our paraphrastic rewriter to the task of question answer sentence selection to see if augmenting with paraphrases leads to improvements. The task is defined as follows: Given a question q and a set of candidate sentences {ci }, select the candidates which answer q. Model We adapt a popular neural architecture for NLI, InferSent (Conneau et al., 2017), to our QA sentence selection task. In InferSent, the questions and answers (originally the premises and hypotheses) are embedded using an uncontextualized word embedding (e.g. GloVe), which we also experiment with ELMo (Peters et al., 2018) to incorporate recent advancements in large-scale contextualized pre-training. Bidirectional LSTMs (Graves and Schmidhuber, 2005) are run atop of these contextualized embeddings and a maxpooling layer is used to generate a feature vector for both the question and the answer. Following various matching methods (Mou et al., 2016) and a multi-layer feed-forward neural network, the model produces a final score. We train the system following the method proposed by Rao et al. (2016), utilizing a ranking loss (Weston and Watkins, 1999) that contrasts positive answers against negative ones. 846 Parap"
N19-1090,P07-2045,0,0.00818714,"Missing"
N19-1090,W18-6319,1,0.814891,"s Evaluation We use S OCKEYE (Hieber et al., 2017)4 for our evaluations. We trained a 6-layer German–English Transformer using the default settings on the WMT’18 training data and the newstest2018 test set for evaluation (Bojar et al., 2018). Following Post and Vilar (2018), we compare decoding results in an unconstrained setting and with two sets of positive constraints: “rand3”, which selects 3 random words from the reference, and “phr4”, which selects a single 4-word phrase. We report decoding speed (in sentences per second) and BLEU score (Papineni et al., 2002), as measured by SacreBLEU (Post, 2018). The results are 4 Improved Monolingual Rewriter 5 PARA BANK generated paraphrases from two large bilingual corpora, CzEng (Bojar et al., 2016a) and GigaFrEn (Callison-Burch et al., 2009). We picked paraphrases from only CzEng, the larger one of the two. https://github.com/awslabs/sockeye/ 843 used as a source factor (Sennrich and Haddow, 2016) to the encoder. This helps us to decrease the vocabulary size of the the training data. We learn a shared byte-pair encoding (BPE) over the entire training data with 30,000 BPE operations (Sennrich et al., 2016), keeping all vocabulary items with a fre"
N19-1090,N18-1119,1,0.356301,"n (See et al., 2017). One approach to enforce hard lexical constraints in the output is to modify the inference procedure to enforce their presence directly (Hokamp and Liu, 2017). These constraints could be either positive (a word must appear in the output) or negative (a word must be avoided). While negative constraints could be easily enforced by preventing hypotheses with prohibited tokens from entering the beam, placing positive constraints in natural and meaningful ways is less straightforward. We improve upon previous work by vectorizing the dynamic beam allocation (DBA) algorithm from Post and Vilar (2018) and by incorporating multi-state tries, • A more efficient and robust approach to lexically-constrained decoding with vectorized DBA and trie representations; • A trained and freely available lexically839 Proceedings of NAACL-HLT 2019, pages 839–850 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics constrained monolingual rewriter1 with improvements in both human-judged semantic similarity and fluency over the initial PARA BANK rewriter (Hu et al., 2019); PARA NMT’s approach and produced a NMTbased rewriter with the ability to apply lexical constr"
N19-1090,N16-1014,0,0.0613793,"en working with positive constraints. Faster decoding enables faster exploration of constraint strategies: we illustrate this via data augmentation experiments with a monolingual rewriter applied to the tasks of natural language inference, question answering and machine translation, showing improvements in all three. 1 Introduction For many natural language generation tasks, we often know word(s) that should (or should not) be in the output sentence. Examples include terminology databases in Machine Translation (MT) (Hokamp and Liu, 2017), names (and generic responses) in dialogue generation (Li et al., 2016; Gu et al., 2016), objects in image captioning (Anderson et al., 2017), and facts in abstractive summarization (See et al., 2017). One approach to enforce hard lexical constraints in the output is to modify the inference procedure to enforce their presence directly (Hokamp and Liu, 2017). These constraints could be either positive (a word must appear in the output) or negative (a word must be avoided). While negative constraints could be easily enforced by preventing hypotheses with prohibited tokens from entering the beam, placing positive constraints in natural and meaningful ways is less s"
N19-1090,P18-1020,1,0.800323,"Missing"
N19-1090,P17-1099,0,0.0185148,"a data augmentation experiments with a monolingual rewriter applied to the tasks of natural language inference, question answering and machine translation, showing improvements in all three. 1 Introduction For many natural language generation tasks, we often know word(s) that should (or should not) be in the output sentence. Examples include terminology databases in Machine Translation (MT) (Hokamp and Liu, 2017), names (and generic responses) in dialogue generation (Li et al., 2016; Gu et al., 2016), objects in image captioning (Anderson et al., 2017), and facts in abstractive summarization (See et al., 2017). One approach to enforce hard lexical constraints in the output is to modify the inference procedure to enforce their presence directly (Hokamp and Liu, 2017). These constraints could be either positive (a word must appear in the output) or negative (a word must be avoided). While negative constraints could be easily enforced by preventing hypotheses with prohibited tokens from entering the beam, placing positive constraints in natural and meaningful ways is less straightforward. We improve upon previous work by vectorizing the dynamic beam allocation (DBA) algorithm from Post and Vilar (2018"
N19-1090,P18-1042,0,0.0752775,"t scale to batching, instead they sequentially processed constraints for sentences within the batch. Paraphrases and Rewriting Many works sought to create paraphrases or paraphrastic expressions through existing corpora. For example, DIRT (Lin and Pantel, 2001) extracts paraphrastic expressions from paths in dependency trees. Weisman et al. (2012) explored learning inference relations between verbs in broader scopes (document or corpus level). PPDB (Ganitkevitch et al., 2013) constructs paraphrase pairs by linking words or phrases that share the same translation in another language. PARA NMT (Wieting and Gimpel, 2018) and PARA BANK (Hu et al., 2019) used back-translation to build a large paraphrase collection from bilingual corpora. For arbitrary sentence rewriting, Napoles et al. (2016) used statistical machine translation in tandem with PPDB as a black box monolingual sentential rewriter. Mallinson et al. (2017) used a series of NMT model pairs to perform backtranslations for monolingual paraphrasing. A similar approach was adopted by PARA NMT to create a large paraphrase collection, which is used to train a monolingual sentence rewriter for canonicalization. PARA BANK (Hu et al., 2019) extends 1 3 Impro"
N19-1090,N18-1101,0,0.0129018,"merely indicative of the potential in data augmentation via constrained paraphrasing, and are by no means a thorough investigation of strategies that yield the best improvements. Such an investigation, however, could be enabled by our algorithmic improvements and practitioners’ domain expertise. 5.1 Natural Language Inference Natural language inference is the task of determining entailment. Two sentences, a premise p and a hypothesis h, are labelled with ENTAIL MENT, CONTRADICTION , or NEUTRAL depending on whether p logically entails, contradicts, or does not interact with h. MultiNLI (MNLI) (Williams et al., 2018) is a large, multi-genre dataset for natural language inference. The dataset is also divided into matched and mismatched portions based on whether the source of the evaluation data matches the source of the training set. Recent models rely on contextual sentence encoders pretrained on vast amounts of English monolingual text (Peters et al., 2018; Devlin et al., 2018). Paraphrastic Data Augmentation We demonstrate the utility of our improved lexically-constrained decoding via data augmentation with some simple rewriting heuristics and two augmentation strategies. First, the model could be We tr"
N19-1090,W16-2209,0,0.0270133,"and with two sets of positive constraints: “rand3”, which selects 3 random words from the reference, and “phr4”, which selects a single 4-word phrase. We report decoding speed (in sentences per second) and BLEU score (Papineni et al., 2002), as measured by SacreBLEU (Post, 2018). The results are 4 Improved Monolingual Rewriter 5 PARA BANK generated paraphrases from two large bilingual corpora, CzEng (Bojar et al., 2016a) and GigaFrEn (Callison-Burch et al., 2009). We picked paraphrases from only CzEng, the larger one of the two. https://github.com/awslabs/sockeye/ 843 used as a source factor (Sennrich and Haddow, 2016) to the encoder. This helps us to decrease the vocabulary size of the the training data. We learn a shared byte-pair encoding (BPE) over the entire training data with 30,000 BPE operations (Sennrich et al., 2016), keeping all vocabulary items with a frequency over 50 in the post-BPE data. We follow Sennrich and Haddow (2016) and use “BIOE&quot; tagging to annotate BPE segmentation and broadcast the casing factor accordingly. The encoder uses both source factors. The model is trained on 2 NVIDIA GTX 1080Ti’s until convergence (5 days). LSTM alpha Transf. alpha Transf. Full STD Fluency 74.5 78.3 81.7"
N19-1090,P16-1009,0,0.0391384,"multiple paraphrases. However, Hu et al. (2019) did not: evaluate the rewriter’s performance on in-the-wild sentences; explore more sophisticated versions of the rewriter; nor demonstrate its utility on NLP tasks. • Monolingual rewriting constraint heuristics for automatic data augmentation leading to improvements on NLI / QA / MT. 2 Data augmentation Data augmentation has been used to improve performance and robustness in deep neural models. In NMT, the most common approach is back-translation, where monolingual text in the target language is translated to create synthetic source sentences (Sennrich et al., 2016). Variants of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of"
N19-1090,D07-1003,0,0.0643621,"the training data. 5.3 We apply our paraphrastic rewriter to the WMT 2016 Turkish-English translation task (Bojar et al., 2016b). We see no improvement in English to Turkish translation, but see a 1.1 BLEU improvement when training an initial NMT system on half paraphrased and half original data, and continued training on the original data. Full details of the experiments are in Appendix C. This was the highest concentration of standard data we experimented with, and future work will explore additional ways of data augmentation using paraphrases. Data Setup We augment the raw TREC-QA dataset (Wang et al., 2007) under the following orthogonal strategies: (1) augmenting the training set with the paraphrases generated via the approach described above; (2) augmenting the answer candidates at evaluation time, and choosing the max score among the paraphrases as the score (aggregation by voting). 6 Experimental Results We evaluate our models using average precision (MAP) and mean reciprocal rank (MRR). Model selection is done with early stopping to choose the epoch with the maximum MAP score. Note that the “Baseline (+ELMo)” settings below falls back to the standard QA selection task, and our score under E"
N19-1090,D18-1100,0,0.0224374,"ristics for automatic data augmentation leading to improvements on NLI / QA / MT. 2 Data augmentation Data augmentation has been used to improve performance and robustness in deep neural models. In NMT, the most common approach is back-translation, where monolingual text in the target language is translated to create synthetic source sentences (Sennrich et al., 2016). Variants of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, though they only consider word-level rewrites and encourage label-preservation, while we paraphrase whole sentences with lexical constraints, independen"
N19-1090,D12-1018,0,0.0122348,"d Vilar (2018) proposed a means of dynamically allocating the slots in a fixed-size beam to ensure that even progress was made in meeting an arbitrary number of constraints provided with the input sentence. However, despite it being their motivation, their approach did not scale to batching, instead they sequentially processed constraints for sentences within the batch. Paraphrases and Rewriting Many works sought to create paraphrases or paraphrastic expressions through existing corpora. For example, DIRT (Lin and Pantel, 2001) extracts paraphrastic expressions from paths in dependency trees. Weisman et al. (2012) explored learning inference relations between verbs in broader scopes (document or corpus level). PPDB (Ganitkevitch et al., 2013) constructs paraphrase pairs by linking words or phrases that share the same translation in another language. PARA NMT (Wieting and Gimpel, 2018) and PARA BANK (Hu et al., 2019) used back-translation to build a large paraphrase collection from bilingual corpora. For arbitrary sentence rewriting, Napoles et al. (2016) used statistical machine translation in tandem with PPDB as a black box monolingual sentential rewriter. Mallinson et al. (2017) used a series of NMT"
P08-1003,A00-1031,0,0.0100952,"area hospitals={carolinas medical center, nyack hospital,...} multiple languages={chuukese, ladino, mandarin, us english,...} Table 1: Class labels found in WordNet in original form, or found in WordNet after removal of leading words, or not found in WordNet at all accompanied by its frequency of occurrence in the logs. The document collection consists of approximately 100 million Web documents in English, as available in a Web repository snapshot from 2006. The textual portion of the documents is cleaned of HTML, tokenized, split into sentences and part-ofspeech tagged using the TnT tagger (Brants, 2000). 3.2 Evaluation of Labeled Classes of Instances Extraction Parameters: The set of instances that can be potentially acquired by the extraction algorithm described in Section 2.1 is heuristically limited to the top five million queries with the highest frequency within the input query logs. In the extracted data, a class label (e.g., search engines) is associated with one or more instances (e.g., google). Similarly, an instance (e.g., google) is associated with one or more class labels (e.g., search engines and internet search engines). The values chosen for the weighting parameters J and K fr"
P08-1003,H05-1071,0,0.0511656,"Missing"
P08-1003,W99-0613,0,0.0321868,"Missing"
P08-1003,P07-1030,0,0.206806,"Missing"
P08-1003,W06-1656,0,0.0439863,"Missing"
P08-1003,C92-2082,0,0.164125,"s lowed by the generation of unlabeled clusters {S} of distributionally similar queries, by clustering vectors of contextual features collected around the occurrences of queries {Q} within documents {D} (Steps 2 and 3). Finally, the intermediate data {M E } and {S} is merged and filtered into smaller, more accurate labeled sets of instances (Steps 4 through 15). Step 1 in Figure 2 applies lexico-syntactic patterns {E} that aim at extracting Is-A pairs of an instance (e.g., Google) and an associated class label (e.g., Internet search engines) from text. The two patterns, which are inspired by (Hearst, 1992) and have been the de-facto extraction technique in previous work on extracting conceptual hierarchies from text (cf. (Ponzetto and Strube, 2007; Snow et al., 2006)), can be summarized as: h[..] C [such as|including] I [and|,|.]i, where I is a potential instance (e.g., Venezuelan equine encephalitis) and C is a potential class label for the instance (e.g., zoonotic diseases), for example in the sentence: “The expansion of the farms increased the spread of zoonotic diseases such as Venezuelan equine encephalitis [..]”. During matching, all string comparisons are caseinsensitive. In order for a"
P08-1003,P06-1103,0,0.0104555,"Missing"
P08-1003,P99-1004,0,0.261117,"Missing"
P08-1003,P98-2127,0,0.0174179,"Missing"
P08-1003,N04-1041,0,0.0536796,"Missing"
P08-1003,C04-1122,0,0.0167233,"Missing"
P08-1003,P06-1101,0,0.134703,"er Rochester, New York 14627 vandurme@cs.rochester.edu Marius Pas¸ca Google Inc. Mountain View, California 94043 mars@google.com Abstract A new approach to large-scale information extraction exploits both Web documents and query logs to acquire thousands of opendomain classes of instances, along with relevant sets of open-domain class attributes at precision levels previously obtained only on small-scale, manually-assembled classes. 1 Introduction Current methods for large-scale information extraction take advantage of unstructured text available from either Web documents (Banko et al., 2007; Snow et al., 2006) or, more recently, logs of Web search queries (Pas¸ca, 2007) to acquire useful knowledge with minimal supervision. Given a manually-specified target attribute (e.g., birth years for people) and starting from as few as 10 seed facts such as (e.g., John Lennon, 1941), as many as a million facts of the same type can be derived from unstructured text within Web documents (Pas¸ca et al., 2006). Similarly, given a manually-specified target class (e.g., Drug) with its instances (e.g., Vicodin and Xanax) and starting from as few as 5 seed attributes (e.g., side effects and maximum dose for Drug), oth"
P08-1003,C98-2122,0,\N,Missing
P08-1113,D07-1090,0,0.0134055,"Missing"
P08-1113,J93-2003,0,0.0521848,"trimmed the pre-parenthesis text with a length-based constraint. The cut-off point is the first (counting from right to left) potential boundary position (see Sec. 3.2) such that C ≥ 2 E + K, where C is the length of the Chinese text, E is the length of the English text in the parentheses and K is a constant (we used K=6 in our experiments). The lengths C and E are measured in bytes, except when the English text is an abbreviation (in that case, E is multiplied by 5). 4 Word Alignment Word alignment is a well-studied topic in Machine Translation with many algorithms having been 997 proposed (Brown et al, 1993; Och and Ney 2003). We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000). The algorithm assumes that there is a score associated with each pair of words in a bi-text. It sorts the word pairs in descending order of their scores, selecting pairs based on the resultant order. A pair of words is linked if none of the two words were previously linked to any other words. The algorithm terminates when there are no more links to make. Tiedemann (2004) compared a variety of alignment algorithms and found Competitive Linking to have one"
P08-1113,2007.mtsummit-papers.9,0,0.411576,"is a method for mining parenthetical translations by treating text snippets containing candidate pairs as a partially parallel corpus and using a word alignment algorithm to establish the correspondences between in-parenthesis and pre-parenthesis words. This technique allows us to identify translation pairs even if they only appeared once on the entire web. As a result, we were able to obtain 26.7 million Chinese-English translation pairs from web documents in Chinese. This is over two orders of magnitude more than the number of extracted translation pairs in the previously reported results (Cao, et al. 2007). The next section presents an overview of our algorithm, which is then detailed in Sections 3 and 4. We evaluate our results in Section 5 by comparison with bilingually linked Wikipedia titles and by using the extracted pairs as additional training data in a statistical machine translation system. 2 Mining Parenthetical Translations A parenthetical translation matches the pattern: (4) precede the English term Lower Egypt. Owing to the frequency with which 下埃及 appears as a candidate, and in varying contexts, one has a good reason to believe下埃及is the correct translation of Lower Egypt. … 下游 地区"
P08-1113,J93-1003,0,0.0344834,"w consecutive sequence of words on one side to be linked to the same word on the other side. Specifically, instead of requiring both ei and fj to have no previous linkages, we only require that at least one of them be unlinked and that (suppose ei is unlinked and fj is linked to ek) none of the words between ei and ek be linked to any word other than fj. 4.2 Link scoring We used φ2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as χ2 (Zhang, S. Vogel. 2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005). The φ2 statistics for a pair of words ei and fj is computed as (ad ! bc )2 &quot;2 = (a + b )(a + c )(b + d )(c + d ) where a is the number of sentence pairs containing both ei and fj; a+b is the number of sentence pairs containing ei; a+c is the number of sentence pairs containing fj; d is the number of sentence pairs containing neither ei nor fj. The φ2 score ranges from 0 to 1. We set a threshold at 0.001, below which the φ2 scores are treated as 0. 4.3 Bias in the partially parallel corpus Since only the last few Chinese words in a can"
P08-1113,N03-1017,0,0.00662772,"Missing"
P08-1113,J00-2004,0,0.063267,"ary position (see Sec. 3.2) such that C ≥ 2 E + K, where C is the length of the Chinese text, E is the length of the English text in the parentheses and K is a constant (we used K=6 in our experiments). The lengths C and E are measured in bytes, except when the English text is an abbreviation (in that case, E is multiplied by 5). 4 Word Alignment Word alignment is a well-studied topic in Machine Translation with many algorithms having been 997 proposed (Brown et al, 1993; Och and Ney 2003). We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000). The algorithm assumes that there is a score associated with each pair of words in a bi-text. It sorts the word pairs in descending order of their scores, selecting pairs based on the resultant order. A pair of words is linked if none of the two words were previously linked to any other words. The algorithm terminates when there are no more links to make. Tiedemann (2004) compared a variety of alignment algorithms and found Competitive Linking to have one of the highest precision scores. A disadvantage of Competitive Linking, however, is that the alignments are restricted word-to-word alignme"
P08-1113,W01-1413,0,0.182686,"a candidate, and in varying contexts, one has a good reason to believe下埃及is the correct translation of Lower Egypt. … 下游 地区 … 中心 which is a sequence of m non-English words followed by a sequence of n English words in parentheses. In the remainder of the paper, we assume the non-English text is Chinese, but our technique works for other languages as well. There have been two approaches to finding such parenthetical translations. One is to assume that the English term e1e2…en is given and use a search engine to retrieve text snippets containing e1e2…en from predominately non-English web pages (Nagata et al, 2001, Kwok et al, 2005). Another method (Cao et al, 2007) is to go through a nonEnglish corpus and collect all instances that match the parenthetical pattern in (4). We followed the second approach since it does not require a predefined list of English terms and is amendable for extraction at large scale. In both cases, one can obtain a list of candidate pairs, where the translation of the in-parenthesis terms is a suffix of the pre-parenthesis text. The lengths and frequency counts of the suffixes have been used to determine what is the translation of the in-parenthesis term (Kwok et al, 2005). F"
P08-1113,J03-1002,0,0.00319521,"renthesis text with a length-based constraint. The cut-off point is the first (counting from right to left) potential boundary position (see Sec. 3.2) such that C ≥ 2 E + K, where C is the length of the Chinese text, E is the length of the English text in the parentheses and K is a constant (we used K=6 in our experiments). The lengths C and E are measured in bytes, except when the English text is an abbreviation (in that case, E is multiplied by 5). 4 Word Alignment Word alignment is a well-studied topic in Machine Translation with many algorithms having been 997 proposed (Brown et al, 1993; Och and Ney 2003). We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000). The algorithm assumes that there is a score associated with each pair of words in a bi-text. It sorts the word pairs in descending order of their scores, selecting pairs based on the resultant order. A pair of words is linked if none of the two words were previously linked to any other words. The algorithm terminates when there are no more links to make. Tiedemann (2004) compared a variety of alignment algorithms and found Competitive Linking to have one of the highest prec"
P08-1113,H05-1010,0,0.0238294,"ked to the same word on the other side. Specifically, instead of requiring both ei and fj to have no previous linkages, we only require that at least one of them be unlinked and that (suppose ei is unlinked and fj is linked to ek) none of the words between ei and ek be linked to any word other than fj. 4.2 Link scoring We used φ2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as χ2 (Zhang, S. Vogel. 2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005). The φ2 statistics for a pair of words ei and fj is computed as (ad ! bc )2 &quot;2 = (a + b )(a + c )(b + d )(c + d ) where a is the number of sentence pairs containing both ei and fj; a+b is the number of sentence pairs containing ei; a+c is the number of sentence pairs containing fj; d is the number of sentence pairs containing neither ei nor fj. The φ2 score ranges from 0 to 1. We set a threshold at 0.001, below which the φ2 scores are treated as 0. 4.3 Bias in the partially parallel corpus Since only the last few Chinese words in a candidate pair are expected to be translated, there should be"
P08-1113,C04-1031,0,0.0126114,"-studied topic in Machine Translation with many algorithms having been 997 proposed (Brown et al, 1993; Och and Ney 2003). We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000). The algorithm assumes that there is a score associated with each pair of words in a bi-text. It sorts the word pairs in descending order of their scores, selecting pairs based on the resultant order. A pair of words is linked if none of the two words were previously linked to any other words. The algorithm terminates when there are no more links to make. Tiedemann (2004) compared a variety of alignment algorithms and found Competitive Linking to have one of the highest precision scores. A disadvantage of Competitive Linking, however, is that the alignments are restricted word-to-word alignments, which implies that multi-word expressions can only be partially linked at best. 4.1 Dealing with multi-word alignment We made a small change to Competitive Linking to allow consecutive sequence of words on one side to be linked to the same word on the other side. Specifically, instead of requiring both ei and fj to have no previous linkages, we only require that at le"
P08-1113,D07-1106,0,0.0415382,"lable-level regularities in transliterated terms. Consider again the Shapiro example in the introduction section. There are numerous correct transliterations for the same English word, some of which are not very frequent. For example, the word 夏布洛happens to have a similar φ2 score with Shapiro as the word 流利 (fluency), which is totally unrelated to Shapiro but happened to have the same co-occurrence statistics in the (partially) parallel corpus. Previous approaches to parenthetical translations relied on specialized algorithms to deal with transliterations (Cao et al, 2007; Jiang et al, 2007; Wu and Chang, 2007). They convert Chinese words into their phonetic representations (Pinyin) and use the known transliterations in a bilingual dictionary to train a transliteration model. We adopted a simpler approach that does not require any additional resources such as pronunciation dictionaries and bilingual dictionaries. In addition to computing the φ2 scores between words, we also compute the φ2 scores of prefixes and suffixes of Chinese and English words. For both languages, the prefix of a word is defined as the first three bytes of the word and the suffix is defined as the last three bytes. Since we use"
P08-1113,W05-0829,0,0.0279943,"Missing"
P08-1113,H91-1026,0,\N,Missing
P10-2043,D09-1079,0,0.0622793,"ts from the streaming algorithms community to problems in processing large text collections. The term streaming refers to a model where data is made available sequentially, and it is assumed that resource limitations preclude storing the entirety of the data for offline (batch) processing. Statistics of interest are approximated via online, randomized algorithms. Examples of text applications include: collecting approximate counts (Talbot, 2009; Van Durme and Lall, 2009a), finding top-n elements (Goyal et al., 2009), estimating term co-occurrence (Li et al., 2008), adaptive language modeling (Levenberg and Osborne, 2009), and building top-k ranklists based on pointwise mutual information (Van Durme and Lall, 2009b). Here we revisit the work of Ravichandran et al. (2005) on building word similarity measures from large text collections by using the Locality Sensitive Hash (LSH) method of Charikar (2002). For the common case of feature updates being additive over a data stream (such as when tracking lexical co-occurrence), we show that LSH signatures can be maintained online, without additional cosine−similarity(~u, ~v ) = ~u · ~v . |~u||~v | This similarity is the cosine of the angle between these high-dimensio"
P10-2043,N10-1021,0,0.38923,"Missing"
P10-2043,P05-1077,0,0.212435,"y requirements than when using the standard offline technique. We envision this method being used in conjunction with dynamic clustering algorithms, for a variety of applications. For example, Petrovic et al. (2010) made use of LSH signatures generated over individual tweets, for the purpose of first story detection. Streaming LSH should allow for the clustering of Twitter authors, based on the tweets they generate, with signatures continually updated over the Twitter stream. Motivated by the recent interest in streaming algorithms for processing large text collections, we revisit the work of Ravichandran et al. (2005) on using the Locality Sensitive Hash (LSH) method of Charikar (2002) to enable fast, approximate comparisons of vector cosine similarity. For the common case of feature updates being additive over a data stream, we show that LSH signatures can be maintained online, without additional approximation error, and with lower memory requirements than when using the standard offline technique. 1 2 Locality Sensitive Hashing We are concerned with computing the cosine similarity of feature vectors, defined for a pair of vectors ~u and ~v as the dot product normalized by their lengths: Introduction Ther"
P10-2043,N09-1058,0,0.283631,"Missing"
P11-2004,P08-1077,0,0.0647923,"8–23, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 0.5 Method 0.4 Mean.Absolute.Error Thus, we propose to approximate the online hash function, using a novel technique we call Reservoir Counting, in order to create a space trade-off between the number of projections and the amount of memory each projection requires. We show experimentally that this leads to greater accuracy approximations at the same memory cost, or similar accuracy approximations at a significantly reduced cost. This result is relevant to work in large-scale distributional semantics (Bhagat and Ravichandran, 2008; Van Durme and Lall, 2009; Pantel et al., 2009; Lin et al., 2010; Goyal et al., 2010; Bergsma and Van Durme, 2011), as well as large-scale processing of social media (Petrovic et al., 2010). Discrete Normal 0.3 0.2 0.1 0.2 0.4 0.6 0.8 1.0 Percent.Zeros Figure 1: With b = 256, mean absolute error in cosine approximation when using a projection based on N (0, 1), compared to {−1, 0, 1}. Approach While not strictly required, we assume here to be dealing exclusively with integer-valued features. We then employ an integer-valued projection matrix in order to work with an integer-valued stream of o"
P11-2004,W10-2808,0,0.0246041,"Missing"
P11-2004,lin-etal-2010-new,0,0.0214929,"Linguistics 2 0.5 Method 0.4 Mean.Absolute.Error Thus, we propose to approximate the online hash function, using a novel technique we call Reservoir Counting, in order to create a space trade-off between the number of projections and the amount of memory each projection requires. We show experimentally that this leads to greater accuracy approximations at the same memory cost, or similar accuracy approximations at a significantly reduced cost. This result is relevant to work in large-scale distributional semantics (Bhagat and Ravichandran, 2008; Van Durme and Lall, 2009; Pantel et al., 2009; Lin et al., 2010; Goyal et al., 2010; Bergsma and Van Durme, 2011), as well as large-scale processing of social media (Petrovic et al., 2010). Discrete Normal 0.3 0.2 0.1 0.2 0.4 0.6 0.8 1.0 Percent.Zeros Figure 1: With b = 256, mean absolute error in cosine approximation when using a projection based on N (0, 1), compared to {−1, 0, 1}. Approach While not strictly required, we assume here to be dealing exclusively with integer-valued features. We then employ an integer-valued projection matrix in order to work with an integer-valued stream of online updates, which is reduced (implicitly) to a stream of posit"
P11-2004,N10-1021,0,0.0619809,"el technique we call Reservoir Counting, in order to create a space trade-off between the number of projections and the amount of memory each projection requires. We show experimentally that this leads to greater accuracy approximations at the same memory cost, or similar accuracy approximations at a significantly reduced cost. This result is relevant to work in large-scale distributional semantics (Bhagat and Ravichandran, 2008; Van Durme and Lall, 2009; Pantel et al., 2009; Lin et al., 2010; Goyal et al., 2010; Bergsma and Van Durme, 2011), as well as large-scale processing of social media (Petrovic et al., 2010). Discrete Normal 0.3 0.2 0.1 0.2 0.4 0.6 0.8 1.0 Percent.Zeros Figure 1: With b = 256, mean absolute error in cosine approximation when using a projection based on N (0, 1), compared to {−1, 0, 1}. Approach While not strictly required, we assume here to be dealing exclusively with integer-valued features. We then employ an integer-valued projection matrix in order to work with an integer-valued stream of online updates, which is reduced (implicitly) to a stream of positive and negative unit updates. The sign of the sum of these updates is approximated through a novel twist on Reservoir Sampli"
P11-2004,P05-1077,0,0.193524,"s in the streaming setting, allowing for maintaining a larger number of signatures, or an increased level of approximation accuracy at a similar memory footprint. 1 Introduction Feature vectors based on lexical co-occurrence are often of a high dimension, d. This leads to O(d) operations to calculate cosine similarity, a fundamental tool in distributional semantics. This is improved in practice through the use of data structures that exploit feature sparsity, leading to an expected O(f ) operations, where f is the number of unique features we expect to have non-zero entries in a given vector. Ravichandran et al. (2005) showed that the Locality Sensitive Hash (LSH) procedure of Charikar (2002), following from Indyk and Motwani (1998) and Goemans and Williamson (1995), could be successfully used to compress textually derived feature vectors in order to achieve speed efficiencies in large-scale noun clustering. Such LSH bit signatures are constructed using the following hash function, where ~v ∈ Rd is a vector in the original feature space, and  ~r is randomly drawn from N (0, 1)d : 1 if ~v · ~r ≥ 0, h(~v ) = 0 otherwise. If hb (~v ) is the b-bit signature resulting from b such hash functions, then the cosine"
P11-2004,P10-2043,1,0.900733,"Missing"
P11-2004,D09-1098,0,\N,Missing
P13-1070,J90-1003,0,0.216445,"relevant and correct part of both a male and a female (and many other living and inanimate objects), but it does not help us distinguish males from females in social media. We therefore rank our attributes for each class by their strength of association with instances of that specific class.1 To calculate the association, we first disregard the count of each (I,A) pair and consider each unique pair to be a single probabilistic event. We then convert the (I,A) pairs to corresponding (C,A) pairs by replacing I with the corresponding class, C. We then calculate the pointwise mutual information (Church and Hanks, 1990) between each C and A over the set of events: PMI(C, A) = log p(C, A) p(C)p(A) (1) If the PMI&gt;0, the observed probability of a class and attribute co-occurring is greater than the probability of co-occurrence that we would expect if C and A were independently distributed. For each class, we rank the attributes by their PMI scores. 2 One can also view the work of manually filtering attributes as a kind of “feature labeling.” There is evidence from Zaidan et al. (2007) that a few hours of feature labeling can be more productive than annotating new training examples. In fact, since Zaidan et al."
P13-1070,P11-1137,0,0.0301319,"curve for Bootstrapped logistic-regression classifier, with automaticallylabeled data, for different feature classes. Table 4: Examples of highly-weighted BoW (content) and Usr (username) features (in descending order of weight) in the Bootstrapped system for predicting user gender in Twitter. features capture reasonable associations between gender classes and particular names (such as mike, tony, omar, etc.) and also between gender classes and common nouns (such as guy, dad, sir, etc.). 7 Many recent papers have analyzed the language of social media users, along dimensions such as ethnicity (Eisenstein et al., 2011; Rao et al., 2011; Pennacchiotti and Popescu, 2011; Fink et al., 2012) time zone (Kiciman, 2010), political orientation (Rao et al., 2010; Pennacchiotti and Popescu, 2011) and gender (Rao et al., 2010; Burger et al., 2011; Van Durme, 2012). Related Work User Characterization The field of sociolinguistics has long been concerned with how various morphological, phonological and stylistic aspects of language can vary with a person’s age, gender, social class, etc. (Fischer, 1968; Labov, 1972). This early work therefore had an emphasis on analyzing the form of language, as opposed to its content."
P13-1070,W04-3221,0,0.170316,"using supervised learning (Section 2). The features of the classifier are indicators of specific words in the user-generated text. While a human would assume that someone with boyhood heroes is male, a standard classifier has no way of exploiting such knowledge unless the phrase occurs in training data. We present an algorithm that improves user characterization by collecting and exploiting such common-sense knowledge. Our work is inspired by algorithms that processes large text corpora in order to discover the attributes of semantic classes, e.g. (Berland and Charniak, 1999; Schubert, 2002; Almuhareb and Poesio, 2004; Tokunaga et al., 2005; Girju et al., 2006; Pas¸ca and Van Durme, 2008; Alfonseca et al., 2010). We learn the distinguishing attributes of different demographic groups (Section 3), and then automatically assign users to these groups whenever they refer to a distinguishing attribute in their writings (Section 4). Our approach obviates the need for expensive annotation efforts, and allows us to rapidly bootstrap training data for new classification tasks. We validate our approach by advancing the state-of-the-art on the most well-studied user classification task: predicting user gender (Section"
P13-1070,P06-1005,1,0.358928,"Missing"
P13-1070,P09-1080,0,0.210198,", 1968; Labov, 1972). This early work therefore had an emphasis on analyzing the form of language, as opposed to its content. This emphasis continued into early machine learning approaches, which predicted author properties based on the usage of function words, partsof-speech, punctuation (Koppel et al., 2002) and spelling/grammatical errors (Koppel et al., 2005). Recently, researchers have focused less on the sociolinguistic implications and more on the tasks themselves, naturally leading to classifiers with feature representations capturing content in addition to style (Schler et al., 2006; Garera and Yarowsky, 2009; Mukherjee and Liu, 2010). Our work represents a logical next step for contentbased classification, a step partly suggested by Schler et al. (2006) who noted that “those who are interested in automatically profiling bloggers for commercial purposes would be well served by considering additional features - which we deliberately ignore in this study - such as author selfidentification.” Class-Attribute Extraction The idea of using simple patterns to extract useful semantic relations goes back to Hearst (1992) who focused on hyponyms. Hearst reports that she “tried applying this technique to mer"
P13-1070,W12-2108,1,0.477,"Missing"
P13-1070,J02-3001,0,0.0535782,"sification; for example, the attributes peak fertility and loveliness are highly 716 defines common sense as “human consensus reality knowledge: the facts and concepts that you and I know and which we each assume the other knows.” While we are the first to exploit commonsense knowledge in user characterization, common sense has been applied to a range of other problems in natural language processing. In many ways WordNet can be regarded as a collection of common-sense relationships. WordNet has been applied in a myriad of NLP applications, including in seminal works on semantic-role labeling (Gildea and Jurafsky, 2002), coreference resolution (Soon et al., 2001) and spelling correction (Budanitsky and Hirst, 2006). Also, many approaches to the task of sentiment analysis “begin with a large lexicon of words marked with their prior polarity” (Wilson et al., 2009). Like our class-attribute associations, the common-sense knowledge that the word cool is positive while unethical is negative can be learned from associations in web-scale data (Turney, 2002). We might also view information about synonyms or conceptually-similar words as a kind of commonsense knowledge. In this perspective, our work is related to rec"
P13-1070,J06-1005,0,0.249193,"s of the classifier are indicators of specific words in the user-generated text. While a human would assume that someone with boyhood heroes is male, a standard classifier has no way of exploiting such knowledge unless the phrase occurs in training data. We present an algorithm that improves user characterization by collecting and exploiting such common-sense knowledge. Our work is inspired by algorithms that processes large text corpora in order to discover the attributes of semantic classes, e.g. (Berland and Charniak, 1999; Schubert, 2002; Almuhareb and Poesio, 2004; Tokunaga et al., 2005; Girju et al., 2006; Pas¸ca and Van Durme, 2008; Alfonseca et al., 2010). We learn the distinguishing attributes of different demographic groups (Section 3), and then automatically assign users to these groups whenever they refer to a distinguishing attribute in their writings (Section 4). Our approach obviates the need for expensive annotation efforts, and allows us to rapidly bootstrap training data for new classification tasks. We validate our approach by advancing the state-of-the-art on the most well-studied user classification task: predicting user gender (Section 5). Our bootstrapped system, trained purel"
P13-1070,N13-1121,1,0.802672,"Missing"
P13-1070,C92-2082,0,0.0305604,"presentations capturing content in addition to style (Schler et al., 2006; Garera and Yarowsky, 2009; Mukherjee and Liu, 2010). Our work represents a logical next step for contentbased classification, a step partly suggested by Schler et al. (2006) who noted that “those who are interested in automatically profiling bloggers for commercial purposes would be well served by considering additional features - which we deliberately ignore in this study - such as author selfidentification.” Class-Attribute Extraction The idea of using simple patterns to extract useful semantic relations goes back to Hearst (1992) who focused on hyponyms. Hearst reports that she “tried applying this technique to meronymy (i.e., the part/whole relation), but without great success.” Berland and Charniak (1999) did have success using Hearststyle patterns for part-whole detection, which they attribute to their “very large corpus and the use of more refined statistical measures for ranking the output.” Girju et al. (2006) devised a supervised classification scheme for part/whole relation discovery that integrates the evidence from multiple patterns. These efforts focused exclusively on the meronymy relation as used in WordN"
P13-1070,P99-1008,0,0.107458,"a classification task and train classifiers using supervised learning (Section 2). The features of the classifier are indicators of specific words in the user-generated text. While a human would assume that someone with boyhood heroes is male, a standard classifier has no way of exploiting such knowledge unless the phrase occurs in training data. We present an algorithm that improves user characterization by collecting and exploiting such common-sense knowledge. Our work is inspired by algorithms that processes large text corpora in order to discover the attributes of semantic classes, e.g. (Berland and Charniak, 1999; Schubert, 2002; Almuhareb and Poesio, 2004; Tokunaga et al., 2005; Girju et al., 2006; Pas¸ca and Van Durme, 2008; Alfonseca et al., 2010). We learn the distinguishing attributes of different demographic groups (Section 3), and then automatically assign users to these groups whenever they refer to a distinguishing attribute in their writings (Section 4). Our approach obviates the need for expensive annotation efforts, and allows us to rapidly bootstrap training data for new classification tasks. We validate our approach by advancing the state-of-the-art on the most well-studied user classifi"
P13-1070,J06-1003,0,0.0161912,"mon sense as “human consensus reality knowledge: the facts and concepts that you and I know and which we each assume the other knows.” While we are the first to exploit commonsense knowledge in user characterization, common sense has been applied to a range of other problems in natural language processing. In many ways WordNet can be regarded as a collection of common-sense relationships. WordNet has been applied in a myriad of NLP applications, including in seminal works on semantic-role labeling (Gildea and Jurafsky, 2002), coreference resolution (Soon et al., 2001) and spelling correction (Budanitsky and Hirst, 2006). Also, many approaches to the task of sentiment analysis “begin with a large lexicon of words marked with their prior polarity” (Wilson et al., 2009). Like our class-attribute associations, the common-sense knowledge that the word cool is positive while unethical is negative can be learned from associations in web-scale data (Turney, 2002). We might also view information about synonyms or conceptually-similar words as a kind of commonsense knowledge. In this perspective, our work is related to recent work that has extracted distributionally-similar words from web-scale data and applied this k"
P13-1070,D11-1120,0,0.540517,"(Rao et al., 2010), deDirac was one of my boyhood heroes. I’m glad I met him once. RT Paul Dirac image by artist Eric Handy: http:... 710 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 710–720, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Van Durme et al., 2008), acquired automatically from corpora (Pas¸ca and Van Durme, 2008; Alfonseca et al., 2010), or simply specified by hand (Schubert, 2002). cision trees (Pennacchiotti and Popescu, 2011), logistic regression (Van Durme, 2012), and the Winnow algorithm (Burger et al., 2011). Content Features: BoW Prior classifiers use a set of features encoding the presence of specific words in the user-generated text. We call these features BoW features as they encode the standard Bag-of-Words representation which has been highly effective in text categorization and information retrieval (Sebastiani, 2002). Creation of Instance Lists We use an instancebased approach; our instances are derived from collections of common nouns that are associated with roles and occupations of people. For the gender task that we study in our experiments, we acquire class instances by filtering the"
P13-1070,P09-1116,0,0.0106674,"lysis “begin with a large lexicon of words marked with their prior polarity” (Wilson et al., 2009). Like our class-attribute associations, the common-sense knowledge that the word cool is positive while unethical is negative can be learned from associations in web-scale data (Turney, 2002). We might also view information about synonyms or conceptually-similar words as a kind of commonsense knowledge. In this perspective, our work is related to recent work that has extracted distributionally-similar words from web-scale data and applied this knowledge in tasks such as named-entity recognition (Lin and Wu, 2009) and dependency parsing (T¨ackstr¨om et al., 2012). associated with females. As subsequent research became more focused on applications, looser definitions of class attributes were adopted. Almuhareb and Poesio (2004) automatically mined class attributes that include parts, qualities, and those with an “agentive” or “telic” role with the class. Their extended set of attributes was shown to enable an improved representation of nouns for the purpose of clustering these nouns into semantic concepts. Tokunaga et al. (2005) define attributes as properties that can serve as focus words in questions"
P13-1070,lin-etal-2010-new,1,0.584501,"ivalent pattern when I is multi-token. The output of this process is a set of (I,A) pairs. In attribute extraction, typically one must choose between the precise results of rich patterns (involving punctuation and parts-of-speech) applied to small corpora (Berland and Charniak, 1999) and the high-coverage results of superficial patterns applied to web-scale data, e.g. via the Google API (Almuhareb and Poesio, 2004). We obtain the best of both worlds by matching our precise pattern against a version of the Google Ngram Corpus that includes the part-of-speech tag distributions for every N-gram (Lin et al., 2010). We found that applying this pattern to web-scale data is effective in extracting useful attributes. We acquired around 20,000 attributes in total. Finding Distinguishing Attributes Unlike prior work, we aim to find distinguishing properties of each class; that is, the kinds of properties that uniquely distinguish a particular category. Prior work has mostly focused on finding “relevant” attributes (Alfonseca et al., 2010) or “correct” parts (Berland and Charniak, 1999). A leg is a relevant and correct part of both a male and a female (and many other living and inanimate objects), but it does"
P13-1070,P09-1070,0,0.0384471,"Missing"
P13-1070,P98-2180,0,0.154138,"l, we did not invest in developing annotation guidelines or measuring inter-annotator agreement. We make these filter attributes available online as an attachment to this article, available through the ACL Anthology. Ultimately, we discovered that manual filtering was necessary to avoid certain pathological cases in our Twitter data. For example, our PMI scoring finds homepage to be strongly associated with males. In our gold-standard gender data (Section 5), however, every user has a homepage [by dataset construction]; we might therefore incorrectly classify every user as Male. We agree with Richardson et al. (1998) that “automatic procedures ... provide the only credible prospect for acquiring world knowledge on the scale needed to support common-sense reasoning” but “hand vetting” might be needed to ensure “accuracy and consistency in production level systems.” Since our approach requires manual involvement in the filtering of the attribute list, one might argue that one should simply manually enumerate the most relevant attributes directly. However, the manual generation of conceptual features by a single researcher results in substantial variability both across and within participants (McRae et al.,"
P13-1070,D10-1021,0,0.100743,"early work therefore had an emphasis on analyzing the form of language, as opposed to its content. This emphasis continued into early machine learning approaches, which predicted author properties based on the usage of function words, partsof-speech, punctuation (Koppel et al., 2002) and spelling/grammatical errors (Koppel et al., 2005). Recently, researchers have focused less on the sociolinguistic implications and more on the tasks themselves, naturally leading to classifiers with feature representations capturing content in addition to style (Schler et al., 2006; Garera and Yarowsky, 2009; Mukherjee and Liu, 2010). Our work represents a logical next step for contentbased classification, a step partly suggested by Schler et al. (2006) who noted that “those who are interested in automatically profiling bloggers for commercial purposes would be well served by considering additional features - which we deliberately ignore in this study - such as author selfidentification.” Class-Attribute Extraction The idea of using simple patterns to extract useful semantic relations goes back to Hearst (1992) who focused on hyponyms. Hearst reports that she “tried applying this technique to meronymy (i.e., the part/whol"
P13-1070,N13-1039,0,0.00532433,"Missing"
P13-1070,P06-1015,0,0.0147669,"is not the only way to extract classattribute information. Experts can manually specify the attributes of entities, as in the WordNet project (Miller et al., 1990). Others have automatically extracted attribute relations from dictionary definitions (Richardson et al., 1998), structured online sources such as Wikipedia infoboxes, (Wu and Weld, 2007) and large-scale collections of high-quality tabular web data (Cafarella et al., 2008). Attribute extraction has also been viewed as a sub-component or special case of the information obtained by general-purpose knowledge extractors (Schubert, 2002; Pantel and Pennacchiotti, 2006). 8 Conclusion We have proposed, developed and successfully evaluated a novel approach to user characterization based on exploiting knowledge of user class attributes. The knowledge is obtained using a new algorithm that discovers distinguishing attributes of particular classes. Our approach to discovering distinguishing attributes represents a significant new direction for research in class-attribute extraction, and provides a valuable bridge between the fields of user characterization and lexical knowledge extraction. We presented three effective techniques for leveraging this knowledge with"
P13-1070,J01-4004,0,0.0649289,"ty and loveliness are highly 716 defines common sense as “human consensus reality knowledge: the facts and concepts that you and I know and which we each assume the other knows.” While we are the first to exploit commonsense knowledge in user characterization, common sense has been applied to a range of other problems in natural language processing. In many ways WordNet can be regarded as a collection of common-sense relationships. WordNet has been applied in a myriad of NLP applications, including in seminal works on semantic-role labeling (Gildea and Jurafsky, 2002), coreference resolution (Soon et al., 2001) and spelling correction (Budanitsky and Hirst, 2006). Also, many approaches to the task of sentiment analysis “begin with a large lexicon of words marked with their prior polarity” (Wilson et al., 2009). Like our class-attribute associations, the common-sense knowledge that the word cool is positive while unethical is negative can be learned from associations in web-scale data (Turney, 2002). We might also view information about synonyms or conceptually-similar words as a kind of commonsense knowledge. In this perspective, our work is related to recent work that has extracted distributionally"
P13-1070,N12-1052,0,0.00978192,"Missing"
P13-1070,P08-1003,1,0.694405,"Missing"
P13-1070,I05-1010,0,0.0260643,"scale data and applied this knowledge in tasks such as named-entity recognition (Lin and Wu, 2009) and dependency parsing (T¨ackstr¨om et al., 2012). associated with females. As subsequent research became more focused on applications, looser definitions of class attributes were adopted. Almuhareb and Poesio (2004) automatically mined class attributes that include parts, qualities, and those with an “agentive” or “telic” role with the class. Their extended set of attributes was shown to enable an improved representation of nouns for the purpose of clustering these nouns into semantic concepts. Tokunaga et al. (2005) define attributes as properties that can serve as focus words in questions about a target class; e.g. director is an attribute of a movie since one might ask, “Who is the director of this movie?” Another line of research has been motivated by the observation that much of Internet search consists of people looking for values of various class attributes (Bellare et al., 2007; Pas¸ca and Van Durme, 2007; Pas¸ca and Van Durme, 2008; Alfonseca et al., 2010). By knowing the attributes of different classes, search engines can better recognize that queries such as “altitude guadalajara” or “populatio"
P13-1070,P02-1053,0,0.00364681,"tion of common-sense relationships. WordNet has been applied in a myriad of NLP applications, including in seminal works on semantic-role labeling (Gildea and Jurafsky, 2002), coreference resolution (Soon et al., 2001) and spelling correction (Budanitsky and Hirst, 2006). Also, many approaches to the task of sentiment analysis “begin with a large lexicon of words marked with their prior polarity” (Wilson et al., 2009). Like our class-attribute associations, the common-sense knowledge that the word cool is positive while unethical is negative can be learned from associations in web-scale data (Turney, 2002). We might also view information about synonyms or conceptually-similar words as a kind of commonsense knowledge. In this perspective, our work is related to recent work that has extracted distributionally-similar words from web-scale data and applied this knowledge in tasks such as named-entity recognition (Lin and Wu, 2009) and dependency parsing (T¨ackstr¨om et al., 2012). associated with females. As subsequent research became more focused on applications, looser definitions of class attributes were adopted. Almuhareb and Poesio (2004) automatically mined class attributes that include parts"
P13-1070,C08-1116,1,0.8973,"Missing"
P13-1070,D12-1005,1,0.362312,"Missing"
P13-1070,J09-3003,0,0.0246109,"to exploit commonsense knowledge in user characterization, common sense has been applied to a range of other problems in natural language processing. In many ways WordNet can be regarded as a collection of common-sense relationships. WordNet has been applied in a myriad of NLP applications, including in seminal works on semantic-role labeling (Gildea and Jurafsky, 2002), coreference resolution (Soon et al., 2001) and spelling correction (Budanitsky and Hirst, 2006). Also, many approaches to the task of sentiment analysis “begin with a large lexicon of words marked with their prior polarity” (Wilson et al., 2009). Like our class-attribute associations, the common-sense knowledge that the word cool is positive while unethical is negative can be learned from associations in web-scale data (Turney, 2002). We might also view information about synonyms or conceptually-similar words as a kind of commonsense knowledge. In this perspective, our work is related to recent work that has extracted distributionally-similar words from web-scale data and applied this knowledge in tasks such as named-entity recognition (Lin and Wu, 2009) and dependency parsing (T¨ackstr¨om et al., 2012). associated with females. As s"
P13-1070,N07-1033,0,0.00900098,"corresponding (C,A) pairs by replacing I with the corresponding class, C. We then calculate the pointwise mutual information (Church and Hanks, 1990) between each C and A over the set of events: PMI(C, A) = log p(C, A) p(C)p(A) (1) If the PMI&gt;0, the observed probability of a class and attribute co-occurring is greater than the probability of co-occurrence that we would expect if C and A were independently distributed. For each class, we rank the attributes by their PMI scores. 2 One can also view the work of manually filtering attributes as a kind of “feature labeling.” There is evidence from Zaidan et al. (2007) that a few hours of feature labeling can be more productive than annotating new training examples. In fact, since Zaidan et al. (2007) label features at the token level (e.g., in our case one would highlight “handbag” in a given tweet), while we label features at the type level (e.g., deciding whether to mark the word “handbag” as feminine in general), our process is likely even more efficient. Future work may also wish to consider this connection to socalled ”annotator rationales” more deeply. 1 Reisinger and Pas¸ca (2009) considered the related problem of finding the most appropriate class"
P13-1070,C98-2175,0,\N,Missing
P13-2012,D12-1032,1,0.765713,"g synonym, hypernym, hyponym, meronym, and holonym edges and bucket the length. String Transducer To represent similarity between arguments that are names, we use a stochastic edit distance model. This stochastic string-tostring transducer has latent “edit” and “no edit” regions where the latent regions allow the model to assign high probability to contiguous regions of edits (or no edits), which are typical between variations of person names. In an edit region, parameters govern the relative probability of insertion, deletion, substitution, and copy operations. We use the transducer model of Andrews et al. (2012). Since in-domain name pairs were not available, we picked 10,000 entities at random from Wikipedia to estimate the transducer parameters. The entity labels were used as weak supervision during EM, as in Andrews et al. (2012). For a pair of mention spans, we compute the conditional log-likelihood of the two mentions going both ways, take the max, and then bucket to get binary features. We duplicate these features with copies that only fire if both mentions are tagged as PER, ORG or LOC. treat as independent experts. For each of these rule probabilities (experts), we find all rules that match t"
P13-2012,W99-0201,0,0.415464,"ry of entity disambiguation both within and across documents. While most information extraction work focuses on entities and noun phrases, there have been a few attempts at predicate, or event, disambiguation. Commonly a situational predicate is taken to correspond to either an event or a state, lexically realized in verbs such as “elect” or nominalizations such as “election”. Similar to entity coreference resolution, almost all of this work assumes unanchored mentions: predicate argument tuples are grouped together based on coreferent events. The first work on event coreference dates back to Bagga and Baldwin (1999). More recently, this task has been considered by Bejan and Harabagiu (2010) and Lee et al. (2012). As with unanchored entity disambiguation, these methods rely on clustering methods and evaluation metrics. Another view of predicate disambiguation seeks 2 PARMA (Predicate ARguMent Aligner) is a pipelined system with a wide variety of features used to align predicates and arguments in two documents. Predicates are represented as mention spans and arguments are represented as coreference chains (sets of mention spans) provided by in-document coreference resolution systems such as included in the"
P13-2012,P98-1013,0,0.0851903,"of mention spans, we compute the conditional log-likelihood of the two mentions going both ways, take the max, and then bucket to get binary features. We duplicate these features with copies that only fire if both mentions are tagged as PER, ORG or LOC. treat as independent experts. For each of these rule probabilities (experts), we find all rules that match the head tokens of a given alignment and have a feature for the max and harmonic mean of the log probabilities of the resulting rule set. FrameNet FrameNet is a lexical database based on Charles Fillmore’s Frame Semantics (Fillmore, 1976; Baker et al., 1998). The database (and the theory) is organized around semantic frames that can be thought of as descriptions of events. Frames crucially include specification of the participants, or Frame Elements, in the event. The Destroying frame, for instance, includes frame elements Destroyer or Cause Undergoer. Frames are related to other frames through inheritance and perspectivization. For instance the frames Commerce buy and Commerce sell (with respective lexical realizations “buy” and “sell”) are both perspectives of Commerce goods-transfer (no lexical realizations) which inherits from Transfer (with"
P13-2012,P10-1143,0,0.142587,"Missing"
P13-2012,N13-1106,1,0.858341,"Missing"
P13-2012,J08-4005,1,0.795859,"ions we perform can vary the “relatedness” of the two documents in terms of the predicates and arguments that they talk about. This reflects our expectation of real world data, where we do not expect perfect overlap in predicates and arguments between a source and target document, as you would in translation data. Lastly, we prune any document pairs that have more than 80 predicates or arguments or have a Jaccard index on bags of lemmas greater than 0.5, to give us a dataset of 328 document pairs. Metric We use precision, recall, and F1. For the RF dataset, we follow Roth and Frank (2012) and Cohn et al. (2008) and evaluate on a version of F1 that considers SURE and POSSIBLE links, which are available in the RF data. Given an alignment to be scored A and a reference alignment B which contains SURE and POSSIBLE links, Bs and Bp respectively, precision and recall are: 8 LDC2010T10, LDC2010T11, LDC2010T12, LDC2010T14, LDC2010T17, LDC2010T23, LDC2002T01, LDC2003T18, and LDC2005T05 P = 66 |A ∩ Bp | |A| R= |A ∩ Bs | |Bs | (1) EECB lemma RF lemma Roth and Frank MTC lemma PARMA PARMA PARMA F1 63.5 74.3 48.3 54.8 57.6 42.1 59.2 P 84.8 80.5 40.3 59.7 52.4 51.3 73.4 R 50.8 69.0 60.3 50.7 64.0 35.7 49.6 task di"
P13-2012,P07-1033,0,0.164696,"Missing"
P13-2012,C10-1032,1,0.882206,"Missing"
P13-2012,N13-1092,1,0.853348,"Missing"
P13-2012,P11-1095,0,0.0328258,"Missing"
P13-2012,D12-1045,0,0.367258,"ses on entities and noun phrases, there have been a few attempts at predicate, or event, disambiguation. Commonly a situational predicate is taken to correspond to either an event or a state, lexically realized in verbs such as “elect” or nominalizations such as “election”. Similar to entity coreference resolution, almost all of this work assumes unanchored mentions: predicate argument tuples are grouped together based on coreferent events. The first work on event coreference dates back to Bagga and Baldwin (1999). More recently, this task has been considered by Bejan and Harabagiu (2010) and Lee et al. (2012). As with unanchored entity disambiguation, these methods rely on clustering methods and evaluation metrics. Another view of predicate disambiguation seeks 2 PARMA (Predicate ARguMent Aligner) is a pipelined system with a wide variety of features used to align predicates and arguments in two documents. Predicates are represented as mention spans and arguments are represented as coreference chains (sets of mention spans) provided by in-document coreference resolution systems such as included in the Stanford NLP toolkit. Results indicated that the chains are of sufficient quality so as not to li"
P13-2012,W12-3018,1,0.876778,"Missing"
P13-2012,S12-1030,0,0.165538,"jamin Van Durme, Mark Dredze, Nicholas Andrews, Charley Beller, Chris Callison-Burch, Jay DeYoung, Justin Snyder, Jonathan Weese, Tan Xu† , and Xuchen Yao Human Language Technology Center of Excellence Johns Hopkins University, Baltimore, Maryland USA †University of Maryland, College Park, Maryland USA Abstract to link or align predicate argument tuples to an existing anchored resource containing references to events or actions, similar to anchored entity disambiguation (entity linking) (Dredze et al., 2010; Han and Sun, 2011). The most relevant, and perhaps only, work in this area is that of Roth and Frank (2012) who linked predicates across document pairs, measuring the F1 of aligned pairs. Here we present PARMA, a new system for predicate argument alignment. As opposed to Roth and Frank, PARMA is designed as a a trainable platform for the incorporation of the sort of lexical semantic resources used in the related areas of Recognizing Textual Entailment (RTE) and Question Answering (QA). We demonstrate the effectiveness of this approach by achieving state of the art performance on the data of Roth and Frank despite having little relevant training data. We then show that while the “lemma match” heuris"
P13-2012,C98-1013,0,\N,Missing
P13-2029,J08-4004,0,0.0141711,"by Lin and Katz (2006) contains 109 questions from TREC 2002 and provides a nearexhaustive judgment of relevant documents for each question. We removed 10 questions that do not have an answer by matching the TREC answer patterns. Then we call this test set MIT99. Training Set for QA We used Amazon Mechanical Turk to collect training data for the QA system by issuing answer-bearing queries for TREC19992003 questions. For the top 10 retrieved sentences for each question, three Turkers judged whether each sentence contained the answer. The inter-coder agreement rate was 0.81 (Krippendorff, 2004; Artstein and Poesio, 2008). The 99 questions of MIT99 were extracted from the Turk collection as our TESTgold with the remaining as TRAIN, with statistics shown in Table 2. Note that only 88 questions out of MIT99 have an answer from the top 10 query results. Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (Bird and Loper, 2004), dependencyparsed by the Stanford Parser (Klein and Manning, 2003), and NER-tagged by the Illinois Named Entity Tagger (Ratinov and Roth, 2009) with an 18-label type set. Corpus Preprocessing for IR The AQUAINT (LDC2002T31) corpus, on which the MIT99 q"
P13-2029,W08-1801,0,0.0248203,"distinction between our method and the technique of learning to rank applied in 3 4 160 Rarely are all three aspects presented in concert (see §2). http://code.google.com/p/jacana/ Callan (2008) extended this work with approximate matching and smoothing. Most research uses parsing to assign deep structures. Compared to shallow (POS, NER) structured retrieval, deep structures need more processing power and smoothing, but might also be more precise. 5 Most of the above (except Kaisser (2012)) only reported on IR or QA, but not both, assuming that improvement in one naturally improves the other. Bilotti and Nyberg (2008) challenged this assumption and called for tighter coupling between IR and QA. This paper is aimed at that challenge. 3 (non-GPE ). Both of them are learned to be important to where questions. Error tolerance along the NLP pipeline. IR and QA share the same processing pipeline. Systematic errors made by the processing tools are tolerated, in the sense that if the same preprocessing error is made on both the question and sentence, an answer may still be found. Take the previous where question, besides NER [0]= GPE and NER [0]= LOC, we also found oddly NER [0]= PERSON an important feature, due t"
P13-2029,P04-3031,0,0.0626902,"ystem by issuing answer-bearing queries for TREC19992003 questions. For the top 10 retrieved sentences for each question, three Turkers judged whether each sentence contained the answer. The inter-coder agreement rate was 0.81 (Krippendorff, 2004; Artstein and Poesio, 2008). The 99 questions of MIT99 were extracted from the Turk collection as our TESTgold with the remaining as TRAIN, with statistics shown in Table 2. Note that only 88 questions out of MIT99 have an answer from the top 10 query results. Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (Bird and Loper, 2004), dependencyparsed by the Stanford Parser (Klein and Manning, 2003), and NER-tagged by the Illinois Named Entity Tagger (Ratinov and Roth, 2009) with an 18-label type set. Corpus Preprocessing for IR The AQUAINT (LDC2002T31) corpus, on which the MIT99 questions are based, was processed in exactly the same manner as was the QA training set. But only sentence boundaries, POS tags and NER labels were kept as the annotation of the corpus. Figure 1: Coupled retrieval with queries directly constructed from highest weighted features of downstream QA. The retrieved and ranked list of sentences is POS"
P13-2029,W09-1119,0,0.0435715,"whether each sentence contained the answer. The inter-coder agreement rate was 0.81 (Krippendorff, 2004; Artstein and Poesio, 2008). The 99 questions of MIT99 were extracted from the Turk collection as our TESTgold with the remaining as TRAIN, with statistics shown in Table 2. Note that only 88 questions out of MIT99 have an answer from the top 10 query results. Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (Bird and Loper, 2004), dependencyparsed by the Stanford Parser (Klein and Manning, 2003), and NER-tagged by the Illinois Named Entity Tagger (Ratinov and Roth, 2009) with an 18-label type set. Corpus Preprocessing for IR The AQUAINT (LDC2002T31) corpus, on which the MIT99 questions are based, was processed in exactly the same manner as was the QA training set. But only sentence boundaries, POS tags and NER labels were kept as the annotation of the corpus. Figure 1: Coupled retrieval with queries directly constructed from highest weighted features of downstream QA. The retrieved and ranked list of sentences is POS and NER tagged, but only query-relevant tags are shown due to space limit. A bag-of-words retrieval approach would have the sentence shown above"
P13-2029,E12-1010,0,0.021652,"d entities: we are already using POS tags as a demonstration. We can potentially use any helpful answer features in retrieval. For instance, if the QA system learns that in order to is highly correlated with why question through lexicalized features, or some certain dependency relations are helpful in answering questions with specific structures, then it is natural and easy for the IR component to incorporate them. 2 Background Besides Predictive Annotation, our work is closest to structured retrieval, which covers techniques of dependency path mapping (Lin and Pantel, 2001; Cui et al., 2005; Kaisser, 2012), graph matching with Semantic Role Labeling (Shen and Lapata, 2007) and answer type checking (Pinchak et al., 2009), etc. Specifically, Bilotti et al. (2007) proposed indexing text with their semantic roles and named entities. Queries then include constraints of semantic roles and named entities for the predicate and its arguments in the question. Improvements in recall of answer-bearing sentences were shown over the bag-of-words baseline. Zhao and There is also a distinction between our method and the technique of learning to rank applied in 3 4 160 Rarely are all three aspects presented in"
P13-2029,D07-1002,0,0.0684965,". We can potentially use any helpful answer features in retrieval. For instance, if the QA system learns that in order to is highly correlated with why question through lexicalized features, or some certain dependency relations are helpful in answering questions with specific structures, then it is natural and easy for the IR component to incorporate them. 2 Background Besides Predictive Annotation, our work is closest to structured retrieval, which covers techniques of dependency path mapping (Lin and Pantel, 2001; Cui et al., 2005; Kaisser, 2012), graph matching with Semantic Role Labeling (Shen and Lapata, 2007) and answer type checking (Pinchak et al., 2009), etc. Specifically, Bilotti et al. (2007) proposed indexing text with their semantic roles and named entities. Queries then include constraints of semantic roles and named entities for the predicate and its arguments in the question. Improvements in recall of answer-bearing sentences were shown over the bag-of-words baseline. Zhao and There is also a distinction between our method and the technique of learning to rank applied in 3 4 160 Rarely are all three aspects presented in concert (see §2). http://code.google.com/p/jacana/ Callan (2008) ext"
P13-2029,P03-1054,0,0.00622919,"ns. For the top 10 retrieved sentences for each question, three Turkers judged whether each sentence contained the answer. The inter-coder agreement rate was 0.81 (Krippendorff, 2004; Artstein and Poesio, 2008). The 99 questions of MIT99 were extracted from the Turk collection as our TESTgold with the remaining as TRAIN, with statistics shown in Table 2. Note that only 88 questions out of MIT99 have an answer from the top 10 query results. Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (Bird and Loper, 2004), dependencyparsed by the Stanford Parser (Klein and Manning, 2003), and NER-tagged by the Illinois Named Entity Tagger (Ratinov and Roth, 2009) with an 18-label type set. Corpus Preprocessing for IR The AQUAINT (LDC2002T31) corpus, on which the MIT99 questions are based, was processed in exactly the same manner as was the QA training set. But only sentence boundaries, POS tags and NER labels were kept as the annotation of the corpus. Figure 1: Coupled retrieval with queries directly constructed from highest weighted features of downstream QA. The retrieved and ranked list of sentences is POS and NER tagged, but only query-relevant tags are shown due to space"
P13-2029,N13-1106,1,0.819007,"Missing"
P13-2029,P06-4018,0,\N,Missing
P13-2029,W02-0109,0,\N,Missing
P13-2123,W07-1427,0,0.175196,"Missing"
P13-2123,N10-1044,0,0.0384138,"Missing"
P13-2123,P09-2073,0,0.0414792,"in the task of tagging mail addresses, a feature of “5 consecutive digits” is highly indicative of a POSTCODE. However, in the alignment model, it does not make sense to design features based on a hard-coded state, say, a feature of “source word lemma matching target word lemma” fires for state index 6. To avoid this data sparsity problem, all features are defined implicitly with respect to the state. For instance: yields the lowest tree edit distance. Other tree or graph matching work for alignment includes that of (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Chambers et al., 2007; Mehdad, 2009; Roth and Frank, 2012). Finally, feature and model design in monolingual alignment is often inspired by bilingual work, including distortion modeling, phrasal alignment, syntactic constraints, etc (Och and Ney, 2003; DeNero and Klein, 2007; Bansal et al., 2011). 3 The Alignment Model 3.1 Model Design Our work is heavily influenced by the bilingual alignment literature, especially the discriminative model proposed by Blunsom and Cohn (2006). Given a source sentence s of length M , and a target sentence t of length N , the alignment from s to t is a sequence of target word indices a, where am∈["
P13-2123,J03-1002,0,0.0065229,"te, say, a feature of “source word lemma matching target word lemma” fires for state index 6. To avoid this data sparsity problem, all features are defined implicitly with respect to the state. For instance: yields the lowest tree edit distance. Other tree or graph matching work for alignment includes that of (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Chambers et al., 2007; Mehdad, 2009; Roth and Frank, 2012). Finally, feature and model design in monolingual alignment is often inspired by bilingual work, including distortion modeling, phrasal alignment, syntactic constraints, etc (Och and Ney, 2003; DeNero and Klein, 2007; Bansal et al., 2011). 3 The Alignment Model 3.1 Model Design Our work is heavily influenced by the bilingual alignment literature, especially the discriminative model proposed by Blunsom and Cohn (2006). Given a source sentence s of length M , and a target sentence t of length N , the alignment from s to t is a sequence of target word indices a, where am∈[1,M ] ∈ [0, N ]. We specify that when am = 0, source word st is aligned to a NULL state, i.e., deleted. This models a many-to-one alignment from source to target. Multiple source words can be aligned to the same targ"
P13-2123,D12-1016,0,0.0743357,"tagging mail addresses, a feature of “5 consecutive digits” is highly indicative of a POSTCODE. However, in the alignment model, it does not make sense to design features based on a hard-coded state, say, a feature of “source word lemma matching target word lemma” fires for state index 6. To avoid this data sparsity problem, all features are defined implicitly with respect to the state. For instance: yields the lowest tree edit distance. Other tree or graph matching work for alignment includes that of (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Chambers et al., 2007; Mehdad, 2009; Roth and Frank, 2012). Finally, feature and model design in monolingual alignment is often inspired by bilingual work, including distortion modeling, phrasal alignment, syntactic constraints, etc (Och and Ney, 2003; DeNero and Klein, 2007; Bansal et al., 2011). 3 The Alignment Model 3.1 Model Design Our work is heavily influenced by the bilingual alignment literature, especially the discriminative model proposed by Blunsom and Cohn (2006). Given a source sentence s of length M , and a target sentence t of length N , the alignment from s to t is a sequence of target word indices a, where am∈[1,M ] ∈ [0, N ]. We spe"
P13-2123,P11-2044,0,0.496604,"ilizes arbitrary features (to make use of word similarity measure and lexical resources) and exploits deeper sentence structures (especially in the case of major languages where robust parsers are available). In this setting the balance between precision and speed becomes an issue: while we might leverage an extensive NLP pipeline for a ∗ 2 Related Work The MANLI aligner (MacCartney et al., 2008) was first proposed to align premise and hypothesis sentences for the task of natural language inference. It applies perceptron learning and handles phrase-based alignment of arbitrary phrase lengths. Thadani and McKeown (2011) optimized this model by decoding via Integer Linear Programming (ILP). Benefiting from modern ILP solvers, this led to an order-of-magnitude speedup. With extra syntactic constraints added, the exact alignment match rate for whole sentence pairs was also significantly improved. Besides the above supervised methods, indirect supervision has also been explored. Among them, Wang and Manning (2010) extended the work of McCallum et al. (2005) and modeled alignment as latent variables. Heilman and Smith (2010) used tree kernels to search for the alignment that 1 Performed while faculty at Johns Hop"
P13-2123,C10-1131,0,0.0630786,"Missing"
P13-2123,N10-1145,0,\N,Missing
P13-2123,W08-1301,0,\N,Missing
P13-2123,D08-1084,0,\N,Missing
P13-2123,P11-1131,0,\N,Missing
P13-2123,P07-1003,0,\N,Missing
P13-2123,J08-4005,1,\N,Missing
P13-2123,N10-1112,0,\N,Missing
P13-2123,P06-1009,0,\N,Missing
P14-1018,P09-1080,0,0.00750402,"user name morphology for predicting user gender and ethnicity. Golbeck et al. (2010) incorporate Twitter data in a spatial model of political ideology. Streaming Approaches Van Durme (2012b) proposed streaming models to predict user gender in Twitter. Other works suggested to process Related Work Supervised Batch Approaches The vast majority of work on predicting latent user attributes in social media apply supervised static SVM models for discrete categorical e.g., gender and regression models for continuous attributes e.g., age with lexical bag-of-word features for classifying user gender (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b), age (Rao et al., 2010; Nguyen et al., 2011; Nguyen et al., 2013) or political orientation. We present an overview of the existing models for political preference prediction in Table 1. Bergsma et al. (2012) following up on Rao’s work (2010) on adding socio-linguistic features to improve gender, ethnicity and political preference prediction show that incorporating stylistic and syntactic information to the bag-of-word features improves gender classification. Other methods characterize Twitter users by applying limited amounts of networ"
P14-1018,N12-1033,0,0.00958345,"sted to process Related Work Supervised Batch Approaches The vast majority of work on predicting latent user attributes in social media apply supervised static SVM models for discrete categorical e.g., gender and regression models for continuous attributes e.g., age with lexical bag-of-word features for classifying user gender (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b), age (Rao et al., 2010; Nguyen et al., 2011; Nguyen et al., 2013) or political orientation. We present an overview of the existing models for political preference prediction in Table 1. Bergsma et al. (2012) following up on Rao’s work (2010) on adding socio-linguistic features to improve gender, ethnicity and political preference prediction show that incorporating stylistic and syntactic information to the bag-of-word features improves gender classification. Other methods characterize Twitter users by applying limited amounts of network structure information in addition to lexical features. ConApproach Users Tweets Rao et al. (2010) 1K 2M Pennacchiotti and Popescu 10.3K (2011a) Conover et al. (2011) 1,000 Zamal et al. (2012) 400 Cohen and Ruths (2013) This paper (batch classification) This paper"
P14-1018,N13-1121,1,0.827115,"Missing"
P14-1018,D11-1023,0,0.0493339,"Missing"
P14-1018,P13-1098,0,0.0356244,"et al. (2013) investigates tweeting and retweeting behavior for political learning during 2012 US Presidential election. The most similar work to ours is by Zamal et al. (2012), where the authors apply features from the tweets authored by a user’s friend to infer attributes of that user. In this paper, we study different types of user social circles in addition to a friend network. Additionally, using social media for mining political opinions (O’Connor et al., 2010a; Maynard and Funk, 2012) or understanding sociopolitical trends and voting outcomes (Tumasjan et al., 2010; Gayo-Avello, 2012; Lampos et al., 2013) is becoming a common practice. For instance, Lampos et al. (2013) propose a bilinear user-centric model for predicting voting intentions in the UK and Australia from social media data. Other works explore political blogs to predict what content will get the most comments (Yano et al., 2013) or analyze communications from Capitol Hill12 to predict campaign contributors based on this content (Yano and Smith, 2013). Unsupervised Batch Approaches Bergsma et al. (2013) show that large-scale clustering of user names improves gender, ethnicity and location classification on Twitter. O’Connor et al."
P14-1018,W11-2122,0,0.0541939,"Missing"
P14-1018,D11-1120,0,0.815073,"d Follower Hashtag Usermention ● ● ● Retweet Reply User 10 20 50 100 log(Tweets Per Neighbor) 200 (b) Ggeo : 10 neighbors 200 ● ● ● ● ● 5 200 50 Accuracy 0.50 0.55 0.60 0.65 0.70 0.75 50 ● ● ● ● 0.50 Accuracy 0.55 0.60 ● ● ● ● ● ● ● ● (c) Gcand : 2 neighbors 100 250 500 ● ● ● ● 1000 2000 ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● Friend Follower Hashtag Usermention 5 Retweet Reply User 10 20 50 100 log(Tweets Per Neighbor) 200 (d) Gcand : 10 neighbors Figure 4: Modeling the influence of the number of tweets per neighbor t=[5, .., 200] for Gcand and Ggeo graphs. analytics (Burger et al., 2011; Van Durme, 2012b) have performed this straight-forward comparison. For that purpose, we take a random partition containing 100 users of Gcand graph and perform four independent classification experiments – two runs using 5 and two runs using 100 tweets per user. Figure 3 demonstrates that more tweets during prediction time lead to higher accuracy by showing that more users with 100 tweets are correctly classified e.g., filled green markers in the right upper quadrant are true Republicans and in the left lower quadrant are true Democrats. Moreover, a lot of users with 100 tweets are close to"
P14-1018,P05-1057,0,0.0589818,"Missing"
P14-1018,D10-1124,0,0.200776,"Missing"
P14-1018,D12-1135,0,0.244453,"tent attribute a(vi ), in our case it is binary a(vi ) ∈ {D, R}, where D stands for Democratic and R for Republican users. Each edge eij ∈ E represents a connection between vi and vj , eij = (vi , vj ) and defines different social circles between Twitter users e.g., follower (f ), friend (b), user mention (m), hashtag (h), reply (y) and retweet (w). Thus, E ∈ V (2) ×{f, b, h, m, w, y}. We denote a set of edges of a given type as φr (E) for r ∈ {f, b, h, m, w, y}. We denote a set of vertices adjacent to vi by social circle type r as Nr (vi ) which is equivalent to {vj |eij ∈ φr (E)}. Following Filippova (2012) we refer to Nr (vi ) as vi ’s social circle, otherwise known as a neighborhood. In most cases, we only work with a sample of a social circle, denoted by Nr0 (vi ) where |Nr0 (vi ) |= k is its size for vi . Figure 1 presents an example of a social graph derived from Twitter. Notably, users from different social circles can be shared across the users of the same or different classes e.g., a user vj can be 2.3 Geo-Centric Graph We construct a geo-centric graph Ggeo by collecting n = 135 Democratic and m = 135 Republican users from the Maryland, Virginia and Delaware region of the US with self-re"
P14-1018,W11-1515,0,0.556686,"Missing"
P14-1018,W06-1639,0,0.0415027,"ent. This may be an effect of ‘sparseness’ of relevant user data, in that users talk about politics very sporadically compared to a random sample of their neighbors. Substantial signal for political preference prediction is distributed in the neighborhood. Querying for more neighbors per user is more beneficial than querying for extra content from the existing neighbors e.g., 5 tweets from 10 neighbors leads to higher accuracy than 25 tweets from 2 neighbors or 50 tweets from 1 neighbor. This may be also the effect of data heterogeneity in social media compared to e.g., political debate text (Thomas et al., 2006). These findings demonstrate that a substantial signal is distributed over the neighborhood content. Neighborhoods constructed from friend, user mention and retweet relationships are most effective. Friend, user mention and retweet neighborhoods show the best accuracy for predicting political preferences of Twitter users. We think that friend relationships are more effective than e.g., follower relationships because it is very likely Acknowledgments The authors would like to thank the anonymous reviewers for their helpful comments. 13 http://money.cnn.com/2013/08/26/technology/social/ facebook"
P14-1018,D12-1005,1,0.848574,"Missing"
P14-1018,D13-1007,0,0.0266248,"Missing"
P14-1018,N10-1021,0,0.0934689,"Missing"
P14-1018,N09-1024,0,0.0168472,"Missing"
P14-1090,D13-1160,0,0.841902,"type is likely currency. We formalize this approach in §4. One challenge for natural language querying against a KB is the relative informality of queries as compared to the grammar of a KB. For example, for the question: who cheated on celebrity A, answers can be retrieved via the Freebase relation celebrity.infidelity.participant, but the connection between the phrase cheated on and the formal KB relation is not explicit. To alleviate this problem, the best attempt so far is to map from ReVerb (Fader et al., 2011) predicateargument triples to Freebase relation triples (Cai and Yates, 2013; Berant et al., 2013). Note that to boost precision, ReVerb has already pruned down less frequent or credible triples, yielding not as much coverage as its text source, ClueWeb. Here we instead directly mine relation mappings from ClueWeb and show that both direct relation mapping precision and indirect QA F1 improve by a large margin. Details in §5. Finally, we tested our system, jacanafreebase,2 on a realistic dataset generously contributed by Berant et al. (2013), who collected thousands of commonly asked questions by crawling the Google Suggest service. Our method achieves state-of-the-art performance with F1"
P14-1090,P04-3031,0,0.168943,"Missing"
P14-1090,P85-1008,0,0.207359,"Missing"
P14-1090,P12-1051,0,0.00711323,"wo prominent challenges: model and data. The model challenge involves finding the best meaning representation for the question, converting it into a query and executing the query on the KB. Most work approaches this via the bridge of various intermediate representations, including combinatory categorial grammar (Zettlemoyer and Collins, 2005, 2007, 2009; Kwiatkowski et al., 2010, 2011, 2013), synchronous context-free grammars (Wong and Mooney, 2007), dependency trees (Liang et al., 2011; Berant et al., 2013), string kernels (Kate and Mooney, 2006; Chen and Mooney, 2011), and tree transducers (Jones et al., 2012). These works successfully showed their effectiveness in QA, despite the fact that most of them require hand-labeled logic annotations. More recent research started to minimize this direct supervision by using latent meaning representations (Berant et 2 4 Graph Features Our model is inspired by an intuition on how everyday people search for answers. If you asked someone: what is the name of justin bieber brother,3 and gave them access to Freebase, that person might first determine that the question 3 All examples used in this paper come from the training data crawled from Google Suggest. They"
P14-1090,J93-2003,0,0.0661759,"Missing"
P14-1090,P06-1115,0,0.029292,"e patterns of QA pairs automatically. Background QA from a KB faces two prominent challenges: model and data. The model challenge involves finding the best meaning representation for the question, converting it into a query and executing the query on the KB. Most work approaches this via the bridge of various intermediate representations, including combinatory categorial grammar (Zettlemoyer and Collins, 2005, 2007, 2009; Kwiatkowski et al., 2010, 2011, 2013), synchronous context-free grammars (Wong and Mooney, 2007), dependency trees (Liang et al., 2011; Berant et al., 2013), string kernels (Kate and Mooney, 2006; Chen and Mooney, 2011), and tree transducers (Jones et al., 2012). These works successfully showed their effectiveness in QA, despite the fact that most of them require hand-labeled logic annotations. More recent research started to minimize this direct supervision by using latent meaning representations (Berant et 2 4 Graph Features Our model is inspired by an intuition on how everyday people search for answers. If you asked someone: what is the name of justin bieber brother,3 and gave them access to Freebase, that person might first determine that the question 3 All examples used in this p"
P14-1090,P13-1042,0,0.702259,"shnamurthy and Mitchell, 2012). We instead attack the problem of QA from a KB from an IE perspective: we learn directly the pattern of QA pairs, represented by the dependency parse of questions and the Freebase structure of answer candidates, without the use of intermediate, general purpose meaning representations. The data challenge is more formally framed as ontology or (textual) schema matching (Hobbs, 1985; Rahm and Bernstein, 2001; Euzenat and Shvaiko, 2007): matching structure of two ontologies/databases or (in extension) mapping between KB relations and NL text. In terms of the latter, Cai and Yates (2013) and Berant et al. (2013) applied pattern matching and relation intersection between Freebase relations and predicateargument triples from the ReVerb OpenIE system (Fader et al., 2011). Kwiatkowski et al. (2013) expanded their CCG lexicon with Wiktionary word tags towards more domain independence. Fader et al. (2013) learned question paraphrases from aligning multiple questions with the same answers generated by WikiAnswers. The key factor to their success is to have a huge text source. Our work pushes the data challenge to the limit by mining directly from ClueWeb, a 5TB collection of web dat"
P14-1090,J06-4003,0,0.0163596,"Missing"
P14-1090,D12-1069,0,0.667014,"Missing"
P14-1090,D10-1119,0,0.0435793,"commonly used (Chu-Carroll et al., 2012). We propose instead to learn discriminative features from the data with shallow question analysis. The final system captures intuitive patterns of QA pairs automatically. Background QA from a KB faces two prominent challenges: model and data. The model challenge involves finding the best meaning representation for the question, converting it into a query and executing the query on the KB. Most work approaches this via the bridge of various intermediate representations, including combinatory categorial grammar (Zettlemoyer and Collins, 2005, 2007, 2009; Kwiatkowski et al., 2010, 2011, 2013), synchronous context-free grammars (Wong and Mooney, 2007), dependency trees (Liang et al., 2011; Berant et al., 2013), string kernels (Kate and Mooney, 2006; Chen and Mooney, 2011), and tree transducers (Jones et al., 2012). These works successfully showed their effectiveness in QA, despite the fact that most of them require hand-labeled logic annotations. More recent research started to minimize this direct supervision by using latent meaning representations (Berant et 2 4 Graph Features Our model is inspired by an intuition on how everyday people search for answers. If you ask"
P14-1090,D11-1142,0,0.155933,"USA, June 23-25 2014. 2014 Association for Computational Linguistics ukraine, the expected answer type is likely currency. We formalize this approach in §4. One challenge for natural language querying against a KB is the relative informality of queries as compared to the grammar of a KB. For example, for the question: who cheated on celebrity A, answers can be retrieved via the Freebase relation celebrity.infidelity.participant, but the connection between the phrase cheated on and the formal KB relation is not explicit. To alleviate this problem, the best attempt so far is to map from ReVerb (Fader et al., 2011) predicateargument triples to Freebase relation triples (Cai and Yates, 2013; Berant et al., 2013). Note that to boost precision, ReVerb has already pruned down less frequent or credible triples, yielding not as much coverage as its text source, ClueWeb. Here we instead directly mine relation mappings from ClueWeb and show that both direct relation mapping precision and indirect QA F1 improve by a large margin. Details in §5. Finally, we tested our system, jacanafreebase,2 on a realistic dataset generously contributed by Berant et al. (2013), who collected thousands of commonly asked questions"
P14-1090,D11-1140,0,0.0737426,"Missing"
P14-1090,P13-1158,0,0.432793,"e data challenge is more formally framed as ontology or (textual) schema matching (Hobbs, 1985; Rahm and Bernstein, 2001; Euzenat and Shvaiko, 2007): matching structure of two ontologies/databases or (in extension) mapping between KB relations and NL text. In terms of the latter, Cai and Yates (2013) and Berant et al. (2013) applied pattern matching and relation intersection between Freebase relations and predicateargument triples from the ReVerb OpenIE system (Fader et al., 2011). Kwiatkowski et al. (2013) expanded their CCG lexicon with Wiktionary word tags towards more domain independence. Fader et al. (2013) learned question paraphrases from aligning multiple questions with the same answers generated by WikiAnswers. The key factor to their success is to have a huge text source. Our work pushes the data challenge to the limit by mining directly from ClueWeb, a 5TB collection of web data. Finally, the KB community has developed other means for QA without semantic parsing (Lopez et al., 2005; Frank et al., 2007; Unger et al., 2012; Yahya et al., 2012; Shekarpour et al., 2013). Most of these work executed SPARQL queries on interlinked data represented by RDF (Resource Description Framework) triples,"
P14-1090,D13-1161,0,0.654983,"Missing"
P14-1090,P11-1060,0,0.126239,"llow question analysis. The final system captures intuitive patterns of QA pairs automatically. Background QA from a KB faces two prominent challenges: model and data. The model challenge involves finding the best meaning representation for the question, converting it into a query and executing the query on the KB. Most work approaches this via the bridge of various intermediate representations, including combinatory categorial grammar (Zettlemoyer and Collins, 2005, 2007, 2009; Kwiatkowski et al., 2010, 2011, 2013), synchronous context-free grammars (Wong and Mooney, 2007), dependency trees (Liang et al., 2011; Berant et al., 2013), string kernels (Kate and Mooney, 2006; Chen and Mooney, 2011), and tree transducers (Jones et al., 2012). These works successfully showed their effectiveness in QA, despite the fact that most of them require hand-labeled logic annotations. More recent research started to minimize this direct supervision by using latent meaning representations (Berant et 2 4 Graph Features Our model is inspired by an intuition on how everyday people search for answers. If you asked someone: what is the name of justin bieber brother,3 and gave them access to Freebase, that person might fi"
P14-1090,W12-3016,0,0.0106805,"Missing"
P14-1090,J03-1002,0,0.0182046,"Missing"
P14-1090,P07-1121,0,0.0587495,"riminative features from the data with shallow question analysis. The final system captures intuitive patterns of QA pairs automatically. Background QA from a KB faces two prominent challenges: model and data. The model challenge involves finding the best meaning representation for the question, converting it into a query and executing the query on the KB. Most work approaches this via the bridge of various intermediate representations, including combinatory categorial grammar (Zettlemoyer and Collins, 2005, 2007, 2009; Kwiatkowski et al., 2010, 2011, 2013), synchronous context-free grammars (Wong and Mooney, 2007), dependency trees (Liang et al., 2011; Berant et al., 2013), string kernels (Kate and Mooney, 2006; Chen and Mooney, 2011), and tree transducers (Jones et al., 2012). These works successfully showed their effectiveness in QA, despite the fact that most of them require hand-labeled logic annotations. More recent research started to minimize this direct supervision by using latent meaning representations (Berant et 2 4 Graph Features Our model is inspired by an intuition on how everyday people search for answers. If you asked someone: what is the name of justin bieber brother,3 and gave them ac"
P14-1090,D12-1035,0,0.304114,"b OpenIE system (Fader et al., 2011). Kwiatkowski et al. (2013) expanded their CCG lexicon with Wiktionary word tags towards more domain independence. Fader et al. (2013) learned question paraphrases from aligning multiple questions with the same answers generated by WikiAnswers. The key factor to their success is to have a huge text source. Our work pushes the data challenge to the limit by mining directly from ClueWeb, a 5TB collection of web data. Finally, the KB community has developed other means for QA without semantic parsing (Lopez et al., 2005; Frank et al., 2007; Unger et al., 2012; Yahya et al., 2012; Shekarpour et al., 2013). Most of these work executed SPARQL queries on interlinked data represented by RDF (Resource Description Framework) triples, or simply performed triple matching. Heuristics and manual templates were also commonly used (Chu-Carroll et al., 2012). We propose instead to learn discriminative features from the data with shallow question analysis. The final system captures intuitive patterns of QA pairs automatically. Background QA from a KB faces two prominent challenges: model and data. The model challenge involves finding the best meaning representation for the question"
P14-1090,D07-1071,0,0.117744,"Missing"
P14-1090,P09-1110,0,0.0387714,"Missing"
P14-1090,P06-4018,0,\N,Missing
P14-1090,W02-0109,0,\N,Missing
P14-1111,W13-3520,0,0.0433198,"Missing"
P14-1111,W09-1206,0,0.049848,"Missing"
P14-1111,D08-1008,0,0.237198,"Missing"
P14-1111,P04-1061,0,0.547449,"e-of-the-art performance in the lowresource setting (§ 4.4). When the models have access to observed syntactic trees, they achieve near state-of-the-art accuracy in the high-resource setting on some languages (§ 4.3). Examining the learning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicat"
P14-1111,P08-1068,0,0.553211,"sed data, and each subsequent component is trained using the 1-best output of the previous components. A typical pipeline consists of a POS tagger, dependency parser, and semantic role labeler. In this section, we introduce pipelines that remove the need for a supervised tagger and parser by training in an unsupervised and distantly supervised fashion. Brown Clusters We use fully unsupervised Brown clusters (Brown et al., 1992) in place of POS tags. Brown clusters have been used to good effect for various NLP tasks such as named entity recognition (Miller et al., 2004) and dependency parsing (Koo et al., 2008; Spitkovsky et al., 2011). The clusters are formed by a greedy hierachical clustering algorithm that finds an assignment of words to classes by maximizing the likelihood of the training data under a latent-class bigram model. Each word type is assigned to a finegrained cluster at a leaf of the hierarchy of clusters. Each cluster can be uniquely identified by the path from the root cluster to that leaf. Representing this path as a bit-string (with 1 indicating a left and 0 indicating a right child) allows a simple coarsening of the clusters by truncating the bit-strings. We train 1000 Brown cl"
P14-1111,boxwell-white-2008-projecting,0,0.0592361,"Missing"
P14-1111,I11-1022,0,0.307934,"ning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. Johansson and Nugues (2008) and Llu´ıs et al. (2013) extend this idea by coupling predictions of a dependency parser with predictions from a semantic role labeler. In the model from Johansson and Nugues (2008), t"
P14-1111,Q13-1018,0,0.0474088,"Missing"
P14-1111,J92-4003,0,0.152391,"g: 1. Pipeline vs. joint training (Figures 1 and 2) Unsupervised Syntax in the Pipeline Typical SRL systems are trained following a pipeline where the first component is trained on supervised data, and each subsequent component is trained using the 1-best output of the previous components. A typical pipeline consists of a POS tagger, dependency parser, and semantic role labeler. In this section, we introduce pipelines that remove the need for a supervised tagger and parser by training in an unsupervised and distantly supervised fashion. Brown Clusters We use fully unsupervised Brown clusters (Brown et al., 1992) in place of POS tags. Brown clusters have been used to good effect for various NLP tasks such as named entity recognition (Miller et al., 2004) and dependency parsing (Koo et al., 2008; Spitkovsky et al., 2011). The clusters are formed by a greedy hierachical clustering algorithm that finds an assignment of words to classes by maximizing the likelihood of the training data under a latent-class bigram model. Each word type is assigned to a finegrained cluster at a leaf of the hierarchy of clusters. Each cluster can be uniquely identified by the path from the root cluster to that leaf. Represen"
P14-1111,J07-3004,0,0.0451889,"Missing"
P14-1111,J93-2004,0,0.0462684,"useful. 4.1 Data The CoNLL-2009 Shared Task (Hajiˇc et al., 2009) dataset contains POS tags, lemmas, morphological features, syntactic dependencies, predicate senses, and semantic roles annotations for 7 languages: Catalan, Chinese, Czech, English, German, Japanese,4 Spanish. The CoNLL-2005 and -2008 Shared Task datasets provide English SRL annotation, and for cross dataset comparability we consider only verbal predicates (more details in § 4.4). To compare with prior approaches that use semantic supervision for grammar induction, we utilize Section 23 of the WSJ portion of the Penn Treebank (Marcus et al., 1993). 4.2 Feature Template Sets Our primary feature set IGC consists of 127 template unigrams that emphasize coarse properties (i.e., properties 7, 9, and 11 in Table 1). We also explore the 31 template unigrams5 IGB described 3 To reduce hash collisions, We use MurmurHash v3 https://code.google.com/p/smhasher. 4 We do not report results on Japanese as that data was only made freely available to researchers that competed in CoNLL 2009. 5 Because we do not include a binary factor between predicate sense and semantic role, we do not include sense as a by Bj¨orkelund et al. (2009). Each of IGC and IG"
P14-1111,D11-1139,0,0.0160017,"e the relative position (Bj¨orkelund et al., 2009), geneological relationship, distance (Zhao et al., 2009), and binned distance (Koo et al., 2008) between two words in the path. From Llu´ıs et al. (2013), we use 1, 2, 3gram path features of words/POS tags (path-grams), and the number of non-consecutive token pairs in a predicate-argument path (continuity). 3.4 Feature Selection Constructing all feature template unigrams and bigrams would yield an unwieldy number of features. We therefore determine the top N template bigrams for a dataset and factor a according to an information gain measure (Martins et al., 2011): IGa,m = X X f ∈Tm xa p(f, xa ) log2 p(f, xa ) p(f )p(xa ) where Tm is the mth feature template, f is a particular instantiation of that template, and xa is an assignment to the variables in factor a. The probabilities are empirical estimates computed from the training data. This is simply the mutual information of the feature template instantiation with the variable assignment. This filtering approach was treated as a simple baseline in Martins et al. (2011) to contrast with increasingly popular gradient based regularization approaches. Unlike the gradient based ap1181 proaches, this filteri"
P14-1111,P05-1012,0,0.0593277,"h as a predicate’s POS tag and an argument’s word) are important for state-of-the-art (Zhao et al., 2009; Bj¨orkelund et al., 2009). Second, for syntactic dependency parsing, combining Brown cluster features with word forms or POS tags yields high accuracy even with little training data (Koo et al., 2008). We create binary indicator features for each model using feature templates. Our feature template definitions build from those used by the top performing systems in the CoNLL-2009 Shared Task, Zhao et al. (2009) and Bj¨orkelund et al. (2009) and from features in syntactic dependency parsing (McDonald et al., 2005; Koo et al., 2008). Template Creation Feature templates are defined over triples of hproperty, positions, orderi. Properties, listed in Table 1, are extracted from word positions within the sentence, shown in Table 2. Single positions for a word wi include its syntactic parent, its leftmost farthest child (leftFarChild), its rightmost nearest sibling (rightNearSib), etc. Following Zhao et al. (2009), we include the notion of verb and noun supports and sections of the dependency path. Also following Zhao et al. (2009), properties from a set of positions can be put together in three possible or"
P14-1111,N04-1043,0,0.0325195,"ere the first component is trained on supervised data, and each subsequent component is trained using the 1-best output of the previous components. A typical pipeline consists of a POS tagger, dependency parser, and semantic role labeler. In this section, we introduce pipelines that remove the need for a supervised tagger and parser by training in an unsupervised and distantly supervised fashion. Brown Clusters We use fully unsupervised Brown clusters (Brown et al., 1992) in place of POS tags. Brown clusters have been used to good effect for various NLP tasks such as named entity recognition (Miller et al., 2004) and dependency parsing (Koo et al., 2008; Spitkovsky et al., 2011). The clusters are formed by a greedy hierachical clustering algorithm that finds an assignment of words to classes by maximizing the likelihood of the training data under a latent-class bigram model. Each word type is assigned to a finegrained cluster at a leaf of the hierarchy of clusters. Each cluster can be uniquely identified by the path from the root cluster to that leaf. Representing this path as a bit-string (with 1 indicating a left and 0 indicating a right child) allows a simple coarsening of the clusters by truncatin"
P14-1111,D12-1074,0,0.0885432,"ciation for Computational Linguistics, pages 1177–1187, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Modeling contributions: • Simpler joint CRF for syntactic and semantic dependency parsing than previously reported. • New application of unsupervised grammar induction: low-resource SRL. • Constrained grammar induction using SRL for distant-supervision. • Use of Brown clusters in place of POS tags for low-resource SRL. The pipeline models are introduced in § 3.1 and jointly-trained models for syntactic and semantic dependencies (similar in form to Naradowsky et al. (2012)) are introduced in § 3.2. In the pipeline models, we develop a novel approach to unsupervised grammar induction and explore performance using SRL as distant supervision. The joint models use a non-loopy conditional random field (CRF) with a global factor constraining latent syntactic edge variables to form a tree. Efficient exact marginal inference is possible by embedding a dynamic programming algorithm within belief propagation as in Smith and Eisner (2008). Even at the expense of no dependency path features, the joint models best pipeline-trained models for state-of-the-art performance in"
P14-1111,P92-1017,0,0.483183,"e permitted. The constrained syntactic DMV parser treats the semantic graph as observed, and constrains the syntactic parent to be chosen from one of the semantic parents, if there are any. In some cases, imposing this constraint would not permit any projective dependency parses—in this case, we ignore the semantic constraint for that sentence. We parse with the CKY algorithm (Younger, 1967; Aho and Ullman, 1972) by utilizing a PCFG corresponding to the DMV (Cohn et al., 2010). Each chart cell allows only non-terminals compatible with the constrained sets. This can be viewed as a variation of Pereira and Schabes (1992). Semantic Dependency Model As described above, semantic role labeling can be cast as a structured prediction problem where the structure is a labeled semantic dependency graph. We define a conditional random field (CRF) (Lafferty et al., 2001) for this task. Because each word in a sentence may be in a semantic relationship with any other word (including itself), a sentence of length n has n2 possible edges. We define a single L+1-ary variable for each edge, whose value can be any of L semantic labels or a special label indicating there is no predicate-argument relationship between the two wor"
P14-1111,W07-2416,0,0.0277147,"better model for grammar induction would result in better performance for SRL. We therefore turn to an analysis of other approaches to grammar induction in Table 8, evaluated on the Penn Treebank. We contrast with methods using distant supervision (Naseem and Barzilay, 2011; Spitkovsky et al., 2010b) and fully unsupervised dependency parsing (Spitkovsky et al., 2013). Following prior work, we exclude punctuation from evaluation and convert the constituency trees to dependencies.12 The approach from Spitkovsky et al. (2013) 12 Naseem and Barzilay (2011) and our results use the Penn converter (Pierre and Heiki-Jaan, 2007). Spitkovsky et al. (2010b; 2013) use Collins (1999) head percolation rules. (SAJ’13) outperforms all other approaches, including our marginalized settings. We therefore may be able to achieve further gains in the pipeline model by considering better models of latent syntax, or better search techniques that break out of local optima. Similarly, improving the nonconvex optimization of our latent-variable CRF (Marginalized) may offer further gains. 5 Discussion and Future Work We have compared various approaches for lowresource semantic role labeling at the state-of-theart level. We find that we"
P14-1111,J08-2005,0,0.0617942,"ut semantic argument heads, not spans. Table 5 presents our results. Boxwell et al. (2011) (B’11) uses additional supervision in the form of a CCG tag dictionary derived from supervised data with (tdc) and without (tc) a cutoff. Our model does very poorly on the ’05 spanbased evaluation because the constituent bracketing of the marginalized trees are inaccurate. This is elucidated by instead evaluating on the oracle spans, where our F1 scores are higher than Boxwell et al. (2011). We also contrast with relavant high-resource methods with span/head conversions from Johansson and Nugues (2008): Punyakanok et al. (2008) (PRY’08) and Johansson and Nugues (2008) (JN’08). 50 40 Language / Dependency Parser Catalan / Marginalized Catalan / DMV+C German / Marginalized German / DMV+C 30 20 0 20000 40000 Number of Training Sentences 60000 Figure 3: Learning curve for semantic dependency supervision in Catalan and German. F1 of SRL only (without sense disambiguation) shown as the number of training sentences is increased. but a drop in performance for German. This may reflect a difference between the languages, or may reflect the difference between the annotation of the languages: both the Catalan and Spanish data o"
P14-1111,W09-1208,0,0.0850188,"arbitrary graphical structure.1 1 Introduction The goal of semantic role labeling (SRL) is to identify predicates and arguments and label their semantic contribution in a sentence. Such labeling defines who did what to whom, when, where and how. For example, in the sentence “The kids ran the marathon”, ran assigns a role to kids to denote that they are the runners; and a role to marathon to denote that it is the race course. Models for SRL have increasingly come to rely on an array of NLP tools (e.g., parsers, lemmatizers) in order to obtain state-of-the-art results (Bj¨orkelund et al., 2009; Zhao et al., 2009). Each tool is typically trained on hand-annotated data, thus placing SRL at the end of a very highresource NLP pipeline. However, richly annotated data such as that provided in parsing treebanks is expensive to produce, and may be tied to specific domains (e.g., newswire). Many languages do 1 not have such supervised resources (low-resource languages), which makes exploring SRL crosslinguistically difficult. The problem of SRL for low-resource languages is an important one to solve, as solutions pave the way for a wide range of applications: Accurate identification of the semantic roles of en"
P14-1111,D08-1016,0,0.170869,"RL. The pipeline models are introduced in § 3.1 and jointly-trained models for syntactic and semantic dependencies (similar in form to Naradowsky et al. (2012)) are introduced in § 3.2. In the pipeline models, we develop a novel approach to unsupervised grammar induction and explore performance using SRL as distant supervision. The joint models use a non-loopy conditional random field (CRF) with a global factor constraining latent syntactic edge variables to form a tree. Efficient exact marginal inference is possible by embedding a dynamic programming algorithm within belief propagation as in Smith and Eisner (2008). Even at the expense of no dependency path features, the joint models best pipeline-trained models for state-of-the-art performance in the lowresource setting (§ 4.4). When the models have access to observed syntactic trees, they achieve near state-of-the-art accuracy in the high-resource setting on some languages (§ 4.3). Examining the learning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Re"
P14-1111,W10-2902,0,0.32565,"in the lowresource setting (§ 4.4). When the models have access to observed syntactic trees, they achieve near state-of-the-art accuracy in the high-resource setting on some languages (§ 4.3). Examining the learning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. Joha"
P14-1111,P10-1130,0,0.267378,"in the lowresource setting (§ 4.4). When the models have access to observed syntactic trees, they achieve near state-of-the-art accuracy in the high-resource setting on some languages (§ 4.3). Examining the learning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. Joha"
P14-1111,D11-1118,0,0.0375614,"Missing"
P14-1111,D13-1204,0,0.0703612,"raining. In Viterbi EM, the E-step finds the maximum likelihood corpus parse given the current model parameters. The M-step then finds the maximum likelihood parameters given the corpus parse. We utilize this approach to produce unsupervised syntactic features for the SRL task. Grammar induction work has further demonstrated that distant supervision in the form of ACE-style relations (Naseem and Barzilay, 2011) or HTML markup (Spitkovsky et al., 2010b) can lead to considerable gains. Recent work in fully unsupervised dependency parsing has supplanted these methods with even higher accuracies (Spitkovsky et al., 2013) by arranging optimizers into networks that suggest informed restarts based on previously identified local optima. We do not reimplement these approaches within the SRL pipeline here, but provide comparison of these methods against our grammar induction approach in isolation in § 4.5. In both pipeline and joint models, we use features adapted from state-of-the-art approaches to SRL. This includes Zhao et al. (2009) features, who use feature templates from combinations of word properties, syntactic positions including head and children, and semantic properties; and features from Bj¨orkelund et"
P14-1111,P05-1073,0,0.0765562,"acy in the high-resource setting on some languages (§ 4.3). Examining the learning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. Johansson and Nugues (2008) and Llu´ıs et al. (2013) extend this idea by coupling predictions of a dependency parser with predictions fr"
P14-1111,J03-4003,0,\N,Missing
P14-1111,W09-1201,0,\N,Missing
P14-1111,P08-1000,0,\N,Missing
P14-2002,D11-1033,0,0.0207744,".g., the relative frequencies of fife, lord, and duke, in the BNC are far greater than in Google). used to predict the appropriateness of a document for inclusion in an “automatically balanced” corpus. languages). This furthers the arguments of the “the web as corpus” community (Kilgarriff and Grefenstette, 2003) with respect to psycholinguistics. Finally, combining multiple sources of frequency estimates is one way researchers may be able to reduce the prediction bias from any single corpus. This relates to work in automatically building domain specific corpora (e.g., Moore and Lewis (2010), Axelrod et al. (2011), Daum´e III and Jagarlamudi (2011), Wang et al. (2014), Gao et al. (2002), and Lin et al. (1997)). Those efforts focus on building representative document collections for a target domain, usually based on a seed set of initial documents. Our results prompt the question: can one use human behavior as the target in the construction of such a corpus? Concretely, can we build corpora by optimizing an objective measure that minimizes error in predicting human reaction times? Prior work in building balanced corpora used either rough estimates of the ratio of genre styles a normal human is exposed t"
P14-2002,J03-3001,0,0.0148967,", mow, guess, pact, strife, tract, hank, howl, foe, nap stuff, whiz, tech, lot, kind, creek, darn, dike, bet, kid wow, sauce, mall, deck, full, spray, flute, rib, guy, bunch heck, guess, right, full, stuff, lot, last, well, guy, fair Table 2: Examples of words with largest difference in z-transformed log frequencies (e.g., the relative frequencies of fife, lord, and duke, in the BNC are far greater than in Google). used to predict the appropriateness of a document for inclusion in an “automatically balanced” corpus. languages). This furthers the arguments of the “the web as corpus” community (Kilgarriff and Grefenstette, 2003) with respect to psycholinguistics. Finally, combining multiple sources of frequency estimates is one way researchers may be able to reduce the prediction bias from any single corpus. This relates to work in automatically building domain specific corpora (e.g., Moore and Lewis (2010), Axelrod et al. (2011), Daum´e III and Jagarlamudi (2011), Wang et al. (2014), Gao et al. (2002), and Lin et al. (1997)). Those efforts focus on building representative document collections for a target domain, usually based on a seed set of initial documents. Our results prompt the question: can one use human beh"
P14-2002,P10-2041,0,0.0410908,"ormed log frequencies (e.g., the relative frequencies of fife, lord, and duke, in the BNC are far greater than in Google). used to predict the appropriateness of a document for inclusion in an “automatically balanced” corpus. languages). This furthers the arguments of the “the web as corpus” community (Kilgarriff and Grefenstette, 2003) with respect to psycholinguistics. Finally, combining multiple sources of frequency estimates is one way researchers may be able to reduce the prediction bias from any single corpus. This relates to work in automatically building domain specific corpora (e.g., Moore and Lewis (2010), Axelrod et al. (2011), Daum´e III and Jagarlamudi (2011), Wang et al. (2014), Gao et al. (2002), and Lin et al. (1997)). Those efforts focus on building representative document collections for a target domain, usually based on a seed set of initial documents. Our results prompt the question: can one use human behavior as the target in the construction of such a corpus? Concretely, can we build corpora by optimizing an objective measure that minimizes error in predicting human reaction times? Prior work in building balanced corpora used either rough estimates of the ratio of genre styles a no"
P14-2002,N04-1025,0,0.0442216,"uestion: can one use human behavior as the target in the construction of such a corpus? Concretely, can we build corpora by optimizing an objective measure that minimizes error in predicting human reaction times? Prior work in building balanced corpora used either rough estimates of the ratio of genre styles a normal human is exposed to daily (e.g., the Brown corpus (Kucera and Francis, 1967)), or simply sampled text evenly across genres (e.g., COCA: the Corpus of Contemporary American English (Davies, 2009)). Just as language models have been used to predict reading grade-level of documents (Collins-Thompson and Callan, 2004), human language models could be 4 Conclusion We have shown intuitive, domain-specific biases in the prediction of human behavioral measures via corpora of various genres. While some psycholinguists have previously acknowledged that different corpora carry different predictive power, this is the first work to our knowledge to systematically document these biases across a range of corpora, and to relate these predictive errors to domain bias, a pressing issue in the NLP community. With these results in hand, future work may now consider the automatic construction of a “properly” balanced text c"
P14-2002,P11-2071,0,0.0404986,"Missing"
P14-2030,W04-3221,0,0.0261226,"might explore given access to social role prediction, we see in Figure 1 a correlation between self-reported role and the chance of an account still being publicly visible, with roles such as belieber and directioner on the one hand, and doctor and teacher on the other. Table 2: Number of self-identifying users per “role”. While rich in interesting labels, cases such as very highlight the purposeful simplicity of the current approach. Study 2 In the second study we exploit a complementary signal based on characteristic conceptual attributes of a social role, or concept class (Schubert, 2002; Almuhareb and Poesio, 2004; Pas¸ca and Van Durme, 2008). We identify typical attributes of a given social role by collecting terms in the Google n-gram corpus that occur frequently in a possessive construction with that role. For example, with the role doctor we extract terms matching the simple pattern “doctor’s ”. 2 ● 0.65 ● ● direc belitioner optiember s ieist sopohld r more pessoim rando ist m dan .0 hip cer randoster sin m.2 freshger mothman er ra chenedrom.1 le rappeader chris r ar tian smotikst a torer vegecta womrian athlean g te engeinek waitr eer nurseess m studaen doctont po r wr eet r athiteis grand t lawym"
P14-2030,W11-1515,0,0.0491815,"Missing"
P14-2030,P13-1070,1,0.902162,"Missing"
P14-2030,W12-2108,1,0.907833,"Missing"
P14-2030,P08-1003,1,0.878199,"Missing"
P14-2030,D11-1120,0,0.189613,"previously explored, successfully retrieving a variety of fine-grained roles. For a given role (e.g., writer), we can further identify characteristic attributes using a simple possessive construction (e.g., writer’s ). Tweets that incorporate the attribute terms in first person possessives (my ) are confirmed to be an indicator that the author holds the associated social role. 1 Introduction With the rise of social media, researchers have sought to induce models for predicting latent author attributes such as gender, age, and political preferences (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b; Zamal et al., 2012). Such models are clearly in line with the goals of both computational advertising (Wortman, 2008) and the growing area of computational social science (Conover et al., 2011; Nguyen et al., 2011; Paul and Dredze, 2011; Pennacchiotti and Popescu, 2011; Mohammad et al., 2013) where big data and computation supplement methods based on, e.g., direct human surveys. For example, Eisenstein et al. (2010) demonstrated a model that predicted where an author was located in order to analyze regional distinctions in communication. While some users explicitly share th"
P14-2030,J90-1003,0,0.269137,"n may have a wife, then a tweet saying: ...my wife... can be taken as potential evidence of membership in the male conceptual class. In our second study, we test whether this idea extends to our wider set of fine-grained roles. For example, we aimed to discover that a doctor may have a patient, while a hairdresser may have a salon; these properties can be expressed in firstperson content as possessives like my patient or my salon. We approached this task by selecting target roles from the first experiment and ranking characteristic attributes for each using pointwise mutual information (PMI) (Church and Hanks, 1990). First, we counted all terms matching a target social role’s possessive pattern (e.g., doctor’s ) in the web-scale n-gram corpus Google V2 (Lin et al., 2010)5 . We ranked the collected terms by computing PMI between classes and attribute terms. Probabilities were estimated from counts of the class-attribute pairs along with counts matching the generic possessive patterns his and her which serve as general background categories. Following suggestions by Bergsma and Van Durme, we manually filtered the ranked list.6 We removed attributes that were either (a) not nominal, or (b) not indicative of"
P14-2030,D10-1124,0,0.113793,"Missing"
P14-2030,P09-1080,0,0.10973,"rts significantly richer classification than previously explored, successfully retrieving a variety of fine-grained roles. For a given role (e.g., writer), we can further identify characteristic attributes using a simple possessive construction (e.g., writer’s ). Tweets that incorporate the attribute terms in first person possessives (my ) are confirmed to be an indicator that the author holds the associated social role. 1 Introduction With the rise of social media, researchers have sought to induce models for predicting latent author attributes such as gender, age, and political preferences (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b; Zamal et al., 2012). Such models are clearly in line with the goals of both computational advertising (Wortman, 2008) and the growing area of computational social science (Conover et al., 2011; Nguyen et al., 2011; Paul and Dredze, 2011; Pennacchiotti and Popescu, 2011; Mohammad et al., 2013) where big data and computation supplement methods based on, e.g., direct human surveys. For example, Eisenstein et al. (2010) demonstrated a model that predicted where an author was located in order to analyze regional distinctions in communicatio"
P14-2030,D12-1005,1,0.883427,"Missing"
P14-2030,N13-1097,0,0.0256949,"me@cs.jhu.edu Abstract Twitter clients, having a larger collection of automatically identified users within a region was preferable even though the predictions for any given user were uncertain. We show that media such as Twitter can support classification that is more fine-grained than gender or general location. Predicting social roles such as doctor, teacher, vegetarian, christian, may open the door to large-scale passive surveys of public discourse that dwarf what has been previously available to social scientists. For example, work on tracking the spread of flu infections across Twitter (Lamb et al., 2013) might be enhanced with a factor based on aggregate predictions of author occupation. We present two studies showing that firstperson social content (tweets) contains intuitive signals for such fine-grained roles. We argue that non-trivial classifiers may be constructed based purely on leveraging simple linguistic patterns. These baselines suggest a wide range of author categories to be explored further in future work. Motivated by work predicting coarsegrained author categories in social media, such as gender or political preference, we explore whether Twitter contains information to support"
P14-2030,lin-etal-2010-new,1,0.715053,"ether this idea extends to our wider set of fine-grained roles. For example, we aimed to discover that a doctor may have a patient, while a hairdresser may have a salon; these properties can be expressed in firstperson content as possessives like my patient or my salon. We approached this task by selecting target roles from the first experiment and ranking characteristic attributes for each using pointwise mutual information (PMI) (Church and Hanks, 1990). First, we counted all terms matching a target social role’s possessive pattern (e.g., doctor’s ) in the web-scale n-gram corpus Google V2 (Lin et al., 2010)5 . We ranked the collected terms by computing PMI between classes and attribute terms. Probabilities were estimated from counts of the class-attribute pairs along with counts matching the generic possessive patterns his and her which serve as general background categories. Following suggestions by Bergsma and Van Durme, we manually filtered the ranked list.6 We removed attributes that were either (a) not nominal, or (b) not indicative of the social role. This left fewer than 30 attribute terms per role, with many roles having fewer than 10. We next performed a precision test to identify poten"
P14-2073,D11-1024,0,0.0483701,"Missing"
P14-2073,U11-1004,0,0.061529,"Missing"
P14-2073,P12-2017,0,0.0341496,"Missing"
P14-2073,N10-1012,0,0.0210805,"Missing"
P14-2112,P11-2004,1,0.885442,"Missing"
P14-2112,P97-1048,0,0.172164,"stream length) for each item in the stream. We address two problems: language changes over time, and the observation that space is a problem, even for compact sketches. Statistical language models often assume either a local Markov property (when working with utterances, or sentences), or that content is generated fully i.i.d. (such as in document-level topic models). However, language shows observable priming effects, sometimes called triggers, where the occurrence of a given term decreases the surprisal of some other term later in the same discourse (Lau et al., 1993; Church and Gale, 1995; Beeferman et al., 1997; Church, 2000). Conventional cache and trigger models typically do not deal with new terms and can be seen as adjusting the parameters of a fixed model. Accounting for previously unseen entries in a language model can be naively simple: as they appear in new training data, add them to the model! However in practice we are constrained by available space: how many unique phrases can we store, given the target application environment? Our work is concerned with modeling language that might change over time, in accordance with current trending discourse topics, but under a strict space constraint"
P14-2112,D12-1005,1,0.87926,"Missing"
P14-2112,C00-1027,0,0.0293647,"item in the stream. We address two problems: language changes over time, and the observation that space is a problem, even for compact sketches. Statistical language models often assume either a local Markov property (when working with utterances, or sentences), or that content is generated fully i.i.d. (such as in document-level topic models). However, language shows observable priming effects, sometimes called triggers, where the occurrence of a given term decreases the surprisal of some other term later in the same discourse (Lau et al., 1993; Church and Gale, 1995; Beeferman et al., 1997; Church, 2000). Conventional cache and trigger models typically do not deal with new terms and can be seen as adjusting the parameters of a fixed model. Accounting for previously unseen entries in a language model can be naively simple: as they appear in new training data, add them to the model! However in practice we are constrained by available space: how many unique phrases can we store, given the target application environment? Our work is concerned with modeling language that might change over time, in accordance with current trending discourse topics, but under a strict space constraint. With a fixed"
P14-2112,N09-1058,0,0.054755,"Missing"
P14-2112,P11-1038,0,0.0229182,".5 304.4 Table 5: Perplexities for differently selected samples over Gigaword (sample size = 10 blocks, β = 1.1). Lower is better. Table 3: Perplexities for different sample sizes over Twitter. Lower is better. 4.4 Static 416.5 436.7 461.8 315.6 319.1 462.5 5 Conclusion We have introduced exponential reservoir sampling as an elegant way to model a stream of unbounded size, yet using fixed space. It naturally allows one to take account of recency effects present in many natural streams. We expect that our language model could improve other Social Media tasks, for example lexical normalisation (Han and Baldwin, 2011) or even event detection (Lin et al., 2011). The approach is fully general and not just limited to language modelling. Future work should look at other distributions for sampling and consider tasks such as machine translation over Social Media. GigaWord Twitter is a fast moving, rapidly changing multilingual stream and it is not surprising that our exponential reservoir sampling proves beneficial. Is it still useful for a more conventional stream that is drawn from a much smaller population of reporters? We repeated our experiments, using the same rolling training and testing evaluation as bef"
P14-2112,W11-2123,0,0.0724046,".9 614.3 615.0 620.0 647.2 650.1 633.0 636.5 630.4 634.5 608.4 610.8 603.3 604.4 2.0 619.4 611.1 612.1 621.6 628.1 658.0 644.6 641.6 618.4 610.0 Table 2: Perplexities for different β values over Twitter (sample size = five days). Lower is better. We test the model on unseen data from all of the next day (or block). Afterwards, we advance to the next day (block) and repeat, potentially incorporating the previously seen test data into the current training data. Evaluation is in terms of perplexity (which is standard for language modelling). We used KenLM for building models and evaluating them (Heafield, 2011). Each model was an unpruned trigram, with Kneser-Ney smoothing. Increasing the language model order would not change the results. Here the focus is upon which data is used in a model (that is, which data is added and which data is removed) and not upon making it compact or making retraining efficient. • Static. This model was trained using data from the start of the duration and never varied. It is a baseline. • Exact. This model was trained using all available data from the start of the stream and acts as an upper bound on performance. • Moving Window. This model used all data in a fixed-siz"
P14-2112,D09-1079,1,0.679706,"uable, but there is also signal in the previous stream. Introduction Work by Talbot and Osborne (2007), Van Durme and Lall (2009) and Goyal et al. (2009) considered the problem of building very large language models via the use of randomized data structures known as sketches.1 While efficient, these structures still scale linearly in the number of items stored, and do not handle deletions well: if processing an unbounded stream of text, with new words and phrases being regularly added to the model, then with a fixed amount of space, errors will only increase over time. This was pointed out by Levenberg and Osborne (2009), who investigated an alternate approach employing perfecthashing to allow for deletions over time. Their deletion criterion was task-specific and based on how a machine translation system queried a language model. Our sampling methods are based on reservoir sampling (Vitter, 1985), a popularly known method in some areas of computer science, but which has seen little use within computational linguistics.2 Standard reservoir sampling is a method for maintaining a uniform sample over a dynamic stream of elements, using constant space. Novel to this community, we consider a variant owing to Aggar"
P14-2112,P07-1065,1,0.786246,"red by this community until now. Using language models over Twitter and Newswire as a testbed, our experimental results based on perplexity support the intuition that recently observed data generally outweighs that seen in the past, but that at times, the past can have valuable signals enabling better modelling of the present. 1 We show experimentally that a moving window is better than uniform sampling, and further that exponential (biased) sampling is best of all. For streaming data, recently encountered data is valuable, but there is also signal in the previous stream. Introduction Work by Talbot and Osborne (2007), Van Durme and Lall (2009) and Goyal et al. (2009) considered the problem of building very large language models via the use of randomized data structures known as sketches.1 While efficient, these structures still scale linearly in the number of items stored, and do not handle deletions well: if processing an unbounded stream of text, with new words and phrases being regularly added to the model, then with a fixed amount of space, errors will only increase over time. This was pointed out by Levenberg and Osborne (2009), who investigated an alternate approach employing perfecthashing to allow"
P15-1146,P05-1074,1,0.5637,"DB, annotated on MTurk as described in Section 5 1514 Lexical Distributional Paraphrase Translation Path WordNet We use the lemmas, POS tags, and phrase lengths of p1 and p2 , the substrings shared by p1 and p2 , and the Levenstein, Jaccard, and Hamming distances between p1 and p2 . Given a dependency context vectors for p1 and p2 , we compute the number of shared contexts, and the Jaccard, Cosine, Lin1998, Weeds2004, Clarke2009, and Szpektor2008 similarities between the vectors. We include 33 paraphrase features distributed with PPDB, which include the paraphrase probabilities as computed in Bannard and Callison-Burch (2005). We refer the reader to Ganitkevitch and CallisonBurch (2014) for a complete description of all of the features included with PPDB. We include the number of foreign language “pivots” (translations) shared by p1 and p2 for each of 24 languages used in the construction of PPDB, as a fraction of the total number of translations observed for each of p1 and p2 . We include a sparse vector of all lexico-syntactic patterns (paths through a dependency parse) which are observed between p1 and p2 in the Annotated Gigaword corpus (Napoles et al., 2012). We include binary features indicating whether Word"
P15-1146,D07-1017,0,0.0891599,"country/patriotic drive/vehicle family/home basketball/court playing/toy islamic/jihad delay/time Unrelated girl/play found/party profit/year man/talk car/family holiday/series green/tennis sunday/tour city/south back/view Table 1: Examples of different types of entailment relations appearing in PPDB. Burch, 2007), and unrelated pairs, e.g. due to misalignments or polysemy in the foreign language. The unclear semantics severely limits the applicability of paraphrase resources to natural language understanding (NLU) tasks. Some efforts have been made to identify directionality of paraphrases (Bhagat et al., 2007; Kotlerman et al., 2010), but tasks like RTE require even richer semantic information. For example, in the T/H pair shown in Figure 1, a system needs information not only about equivalent words (12/twelve) and asymmetric entailments (riots/unrest), but also semantic exclusion (Denmark/Jordan). Such lexical entailment relations are captured by natural logic, a formalism which views natural language itself as a meaning representation, eschewing external representations such as First Order Logic (FOL). This is a great fit for automatically extracted paraphrases, since the phrase pairs themselves"
P15-1146,S14-2114,1,0.90105,"Missing"
P15-1146,W08-2222,1,0.588578,"Missing"
P15-1146,S14-2141,0,0.0747714,"dependent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphr"
P15-1146,W04-3205,0,0.0861134,"tempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, ma"
P15-1146,P11-1062,0,0.0709412,"Missing"
P15-1146,S13-2045,0,0.00626621,"ringboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference usin"
P15-1146,W09-0215,0,0.0129149,"ve of the ¬ relation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as described in Bannard and Callison-Burch (2005). W"
P15-1146,C04-1051,0,0.466255,"sk of recognizing textual entailment (RTE). In RTE, a system is given two pieces of text, often called the text (T) and the hypothesis (H), and asked to determine whether T entails H, T contradicts H, or T and H are unrelatable (Figure 1). In contrast, data-driving paraphrasing typically sidesteps developing a clear definition of “meaning the same thing” and instead “assume[s] paraphrasing is a coherent notion and concentrate[s] on devices that can produce paraphrases” (Barzilay, 2003). Recent work on paraphrase extraction has resulted in enormous paraphrase collections (Lin and Pantel, 2001; Dolan et al., 2004; Ganitkevitch et al., 2013), but the usefulness of these collections is limited by the fast-and-loose treatment of the meaning of paraphrases. One concrete definition that is sometimes used for paraphrases requires that they be bidirectionally entailing (Androutsopoulos and Malakasiotis, 2010). That is, in terms of RTE, it is assumed that if P is a paraphrase of Q, then P entails Q and Q entails P. In reality, paraphrases are often more nuanced (Bhagat and Hovy, 2013), and the entries in most paraphrase resources certainly do not match this definition. For instance, Lin and Pantel (2001) extr"
P15-1146,ganitkevitch-callison-burch-2014-multilingual,1,0.922495,"Missing"
P15-1146,S14-2055,0,0.0262925,"trieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphrases. Compared to other paraphrase resources such as the DIRT database (12 million rules) (Lin and Pantel, 2001) and the MSR paraphrase phrase tabl"
P15-1146,P98-2127,0,0.382648,"⇠ Cosine Similarity shades/the shade yard/backyard each other/man picture/drawing practice/target Monolingual (symmetric) ¬ large/small ⌘ few/several ¬ different/same ¬ other/same ¬ put/take Monolingual (asymmetric) A boy/little boy A man/two men A child/three children ⌘ is playing/play A side/both sides ⌘ A ⌘ ⌘ ⌘ Bilingual dad/father some kid/child a lot of/many female/woman male/man Table 3: Top scoring pairs (x/y) according to various similarity measures, along with their manually classified entailment labels. Column 1 is cosine similarity based on dependency contexts. Column 2 is based on Lin (1998), column 3 on Weeds (2004), and column 4 is a novel feature. Precise definitions of each metric are given in the supplementary material. in X and in Y separate X from Y to X and/or to Y from X to Y more/less X than Y ate levels of agreement (Fleiss’s  = 0.56) (Landis and Koch, 1977). For a fuller discussion of the annotation, refer to the supplementary material. 6 Automatic Classification Table 4: Top paths associated with the ¬ class. We aim to build a classifier to automatically assign entailment types to entries in the PPDB, and to demonstrate that it performs well both intrinsically and e"
P15-1146,W07-1431,0,0.153459,"wing external representations such as First Order Logic (FOL). This is a great fit for automatically extracted paraphrases, since the phrase pairs themselves can be used as the semantic representation with minimal additional annotation. But as is, paraphrase resources lack such annotation. As a result, NLU systems rely on manually built resources like WordNet, which are limited in coverage and often lead to incorrect inferences (Kaplan and Schubert, 2001). In fact, in the most recent RTE challenge, over half of the submitted systems used WordNet (Pontiki et al., 2014). Even the NatLog system (MacCartney and Manning, 2007), which popularized natural logic for RTE, relied on WordNet and did not solve the problem of assigning natural logic relations at scale. The main contributions of this paper are: • We add a concrete, interpretable semantics to the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013), the largest paraphrase resource currently available. We give each entry in the database a label describing the entailment relationship between the phrases. • We develop a statistical model to predict these relations. The enormous size of PPDB– over 77 million phrase pairs!– makes it impossible to perform this t"
P15-1146,J10-3003,0,0.048805,"Missing"
P15-1146,N13-1092,1,0.191264,"Missing"
P15-1146,S14-2001,0,0.0556365,"Missing"
P15-1146,W07-1401,0,0.0157195,"RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a represent"
P15-1146,W12-3018,1,0.892473,"Missing"
P15-1146,D09-1122,0,0.082205,"Missing"
P15-1146,C92-2082,0,0.478737,"ources Approaches to paraphrase identification have exploited signal from distributional contexts (Lin and Pantel, 2001; Szpektor et al., 2004), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 15"
P15-1146,P06-1101,0,0.0656433,"4), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly"
P15-1146,C08-1107,0,0.0134612,"of the paths most indicative of the ¬ relation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as described in Bannard and Callison-"
P15-1146,W04-3206,0,0.0162431,"Missing"
P15-1146,C04-1146,0,0.0100585,"es examples of some of the paths most indicative of the ¬ relation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as describe"
P15-1146,Q14-1034,1,0.68632,"Missing"
P15-1146,S14-2044,0,0.0221701,"tion, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphrases. Compared to other paraphrase resources such as the DIRT database (12 million rules) (Lin and Pantel, 2001) and the MSR paraphrase phrase table (13 million) (Dola"
P15-1146,W07-1409,0,\N,Missing
P15-1146,C98-2122,0,\N,Missing
P15-1146,S14-2004,0,\N,Missing
P15-1146,J13-3001,0,\N,Missing
P15-2010,D08-1021,1,0.774015,"phrases. We address the problem of customizing paraphrase models to specific target domains. We explore the following ideas: Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which a paraphrase rule might apply, these scores are not differentiated by domain, and instead correspond to the general domain represented by the model’s training data. As illustrated by Table 1, paraphrases that are highly probable in the general domain (e.g. hot = sexy) can be extremely improbable in more specialized domains like bi"
P15-2010,P13-2121,0,0.0208555,"m the entire bitext. 4 Experimental Conditions Domain data We evaluate our domain-specific paraphrasing model in the target domain of biology. Our monolingual in-domain data is a combination of text from the GENIA database (Kim et al., 2003) and text from an introductory biology textbook. Our bilingual general-domain data is the 109 word parallel corpus (Callison-Burch et al., 58 2009), a collection of French-English parallel data covering a mix of genres from legal text (Steinberger et al., 2006) to movie subtitles (Tiedemann, 2012). We use 5-gram language models with Kneser-Ney discounting (Heafield et al., 2013). Evaluation We measure the precision and recall of paraphrase pairs produced by each of our models by collecting human judgments of what paraphrases are acceptable in sentences drawn from the target domain and in sentences drawn from the general domain. We sample 15K sentences from our biology data, and 10K general-domain sentences from Wikipedia. We select a phrase from each sentence, and show the list of candidate paraphrases1 to 5 human judges. Judges make a binary decision about whether each paraphrase is appropriate given the domain-specific context. We consider a paraphrase rule to be g"
P15-2010,J10-3003,0,0.0554926,"paraphrase extraction techniques learn paraphrases for a mix of senses that work well in general. But in specific domains, paraphrasing should be sensitive to specialized language use. domain: the verb treat is used in expressions like treat you to dinner in conversational domains versus treat an infection in biology. This domain shift changes the acceptability of its paraphrases. We address the problem of customizing paraphrase models to specific target domains. We explore the following ideas: Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms at"
P15-2010,W07-0716,0,0.0611515,"Missing"
P15-2010,D09-1074,0,0.0542275,"Missing"
P15-2010,apidianaki-etal-2014-semantic,0,0.075967,"customizing paraphrase models to specific target domains. We explore the following ideas: Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which a paraphrase rule might apply, these scores are not differentiated by domain, and instead correspond to the general domain represented by the model’s training data. As illustrated by Table 1, paraphrases that are highly probable in the general domain (e.g. hot = sexy) can be extremely improbable in more specialized domains like biology. Dominant word senses change dep"
P15-2010,P10-2041,0,0.527199,"data. 2. We improve our domain-specific paraphrases by weighting each training example based on its domain score, instead of treating each example equally. 3. We dramatically improve recall while maintaining precision by combining the subsampled in-domain paraphrase scores with the general-domain paraphrase scores. 2 Background The paraphrase extraction algorithm that we customize is the bilingual pivoting method (Bannard and Callison-Burch, 2005) that was used to create PPDB, the paraphrase database (Ganitkevitch et al., 2013). To perform the subsampling, we adapt and improve the method that Moore and Lewis (2010) originally developed for domain-specific language models in machine translation. ∗ Incubated by the Allen Institute for Artificial Intelligence. 57 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 57–62, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2.1 according to each language model (LM). That is, for a sentence si , we compute Paraphrase extraction Paraphrases can be extracted via bilingual pivoting. Intuitively, if two"
P15-2010,D11-1033,0,0.0380065,"2 provides examples of paraphrases extracted using our domain-specific AUC 39.5 40.8 40.8 41.2 41.9 42.3 43.7 ∆absolute – +1.3 +1.3 +1.7 +2.4 +2.8 +4.2 ∆relative – +3.3 +3.3 +4.3 +6.1 +7.1 +10.6 Table 3: AUC (× 100) for each model in the biology domain from Figure 2(a). model for biology versus the baseline model. 6 Related Work Domain-specific paraphrasing has not received previous attention, but there is relevant prior work on domain-specific machine translation (MT). We build on the Moore-Lewis method, which has been used for language models (Moore and Lewis, 2010) and translation models (Axelrod et al., 2011). Similar methods use LM perplexity to rank sentences (Gao et al., 2002; Yasuda et al., 2008), rather than the difference in cross-entropy. Within MT, Foster and Kuhn (2007) used loglinear weightings of translation probabilities to combine models trained in different domains, as we do here. Relevant to our proposed method of 60 fractional counting, (Madnani et al., 2007) used introduced a count-centric approach to paraphrase probability estimation. Matsoukas et al. (2009) and Foster et al. (2010) explored weighted training sentences for MT, but set weights discriminatively based on sentence-le"
P15-2010,N15-1023,1,0.825856,"ns. We explore the following ideas: Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which a paraphrase rule might apply, these scores are not differentiated by domain, and instead correspond to the general domain represented by the model’s training data. As illustrated by Table 1, paraphrases that are highly probable in the general domain (e.g. hot = sexy) can be extremely improbable in more specialized domains like biology. Dominant word senses change depending on 1. We sort sentences in the training corpus ba"
P15-2010,P05-1074,1,0.900281,"ge depending on 1. We sort sentences in the training corpus based on how well they represent the target domain, and then extract paraphrases from a subsample of the most domain-like data. 2. We improve our domain-specific paraphrases by weighting each training example based on its domain score, instead of treating each example equally. 3. We dramatically improve recall while maintaining precision by combining the subsampled in-domain paraphrase scores with the general-domain paraphrase scores. 2 Background The paraphrase extraction algorithm that we customize is the bilingual pivoting method (Bannard and Callison-Burch, 2005) that was used to create PPDB, the paraphrase database (Ganitkevitch et al., 2013). To perform the subsampling, we adapt and improve the method that Moore and Lewis (2010) originally developed for domain-specific language models in machine translation. ∗ Incubated by the Allen Institute for Artificial Intelligence. 57 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 57–62, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2.1 ac"
P15-2010,tiedemann-2012-parallel,0,0.0169721,"advantage of producing the full set of paraphrases that can be extracted from the entire bitext. 4 Experimental Conditions Domain data We evaluate our domain-specific paraphrasing model in the target domain of biology. Our monolingual in-domain data is a combination of text from the GENIA database (Kim et al., 2003) and text from an introductory biology textbook. Our bilingual general-domain data is the 109 word parallel corpus (Callison-Burch et al., 58 2009), a collection of French-English parallel data covering a mix of genres from legal text (Steinberger et al., 2006) to movie subtitles (Tiedemann, 2012). We use 5-gram language models with Kneser-Ney discounting (Heafield et al., 2013). Evaluation We measure the precision and recall of paraphrase pairs produced by each of our models by collecting human judgments of what paraphrases are acceptable in sentences drawn from the target domain and in sentences drawn from the general domain. We sample 15K sentences from our biology data, and 10K general-domain sentences from Wikipedia. We select a phrase from each sentence, and show the list of candidate paraphrases1 to 5 human judges. Judges make a binary decision about whether each paraphrase is a"
P15-2010,C12-1177,0,0.0191268,"ific target domains. We explore the following ideas: Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which a paraphrase rule might apply, these scores are not differentiated by domain, and instead correspond to the general domain represented by the model’s training data. As illustrated by Table 1, paraphrases that are highly probable in the general domain (e.g. hot = sexy) can be extremely improbable in more specialized domains like biology. Dominant word senses change depending on 1. We sort sentenc"
P15-2010,I08-2088,0,0.0804447,"Missing"
P15-2010,steinberger-etal-2006-jrc,0,\N,Missing
P15-2010,D10-1044,0,\N,Missing
P15-2010,W09-0401,1,\N,Missing
P15-2010,N13-1092,1,\N,Missing
P15-2010,J13-3001,0,\N,Missing
P15-2067,W07-1424,0,0.157149,"Missing"
P15-2067,ferrandez-etal-2010-aligning,0,0.118792,"Missing"
P15-2067,P13-2130,0,0.15027,"Missing"
P15-2067,N13-1092,1,0.691494,"Missing"
P15-2067,P10-2045,0,0.033468,"Missing"
P15-2067,J02-3001,0,0.237857,"Missing"
P15-2067,P98-1013,0,0.13149,"Missing"
P15-2067,W10-0735,0,0.029492,"Missing"
P15-2067,P13-2121,0,0.0542521,"Missing"
P15-2067,P14-1136,0,0.035095,"Missing"
P15-2067,D08-1021,1,0.826454,"Missing"
P15-2067,W10-0907,0,0.0542799,"Missing"
P15-2067,N03-2022,0,0.0541534,"Missing"
P15-2067,P11-1144,0,0.0227914,"Missing"
P15-2067,C10-2107,0,0.0853446,"Missing"
P15-2067,N12-1086,0,0.098033,"Missing"
P15-2067,D08-1048,0,0.205318,"Missing"
P15-2067,W14-2901,1,0.897335,"Missing"
P15-2067,D07-1002,0,0.049493,"Missing"
P15-2067,C98-1013,0,\N,Missing
P15-2070,D11-1108,1,0.522536,"Missing"
P15-2070,N13-1092,1,0.806581,"Missing"
P15-2070,S13-1005,0,0.0116965,"udes finegrained entailment relations, word embedding similarities, and style annotations. 1 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Introduction The Paraphrase Database (PPDB) is a collection of over 100 million paraphrases that was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between correct and incorrect paraphrases? Within the paraphrase sets, are all of the paraphr"
P15-2070,D13-1090,0,0.0129039,"ntailment relations, word embedding similarities, and style annotations. 1 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Introduction The Paraphrase Database (PPDB) is a collection of over 100 million paraphrases that was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between correct and incorrect paraphrases? Within the paraphrase sets, are all of the paraphrases truly substitutable"
P15-2070,P05-1074,1,0.656316,"zontal axis) for four ways of automatically ranking the paraphrases: p(e2 |e1 ) (far left), PPDB 1.0’s heuristic ranking method (middle left), word2vec similarity (middle right), and our supervised model for PPDB 2.0 (far right). Our rankings achieve the highest correlation with human judgements with a Spearman’s ρ of 0.71. Upon publication of this paper, we will release PPDB 2.0 along with a set of 26K phrase pairs annotated with human similarity judgments. 2 + + + + + + Improved rankings of paraphrases The notion of ranking paraphrases goes back to the original method that PPDB is based on. Bannard and Callison-Burch (2005) introduced the bilingual pivoting method, which extracts incarcerated as a potential paraphrase of put in prison since they are both aligned to festgenommen in different sentence pairs in an English-German bitext. Since incarcerated aligns to many foreign words (in many languages) the list of potential paraphrases is long. Paraphrases vary in quality since the alignments are automatically produced and noisy. In order to rank the paraphrases, Bannard and CallisonBurch (2005) define a paraphrase probability in terms of the translation model probabilities p(f |e) and p(e|f ): p(e2 |e1 ) ≈ X p(e2"
P15-2070,D11-1009,0,0.0153625,"straining to types which appear in PPDB), and collected human judgments for their full list of paraphrases. 2 https://code.google.com/p/word2vec/ For phrases, we use the vector of the rarest word as an approximation of the vector for the phrase. 3 427 1. 2. 3. 4. 5. achieves a 9-12 point improvement in MRR over the PPDB 1.0 rankings. Similarly, it improves AP by 7-9 points. 3 Other Additions Entailment relations 4 the final analysis the last the finish the final part the last part Related Work The most closely related work to our supervised re-ranking of PPDB is work by Zhao et al. (2008) and Malakasiotis and Androutsopoulos (2011). Zhao et al. (2008) improved Bannard and Callison-Burch (2005)’s paraphrase probability by converting it into log-linear model inspired by machine translation, allowing them to incorporate a variety of features. Malakasiotis and Androutsopoulos (2011) developed a similar model trained on human judgements. Both efforts apply their model to natural language generation by paraphrasing full sentences. We apply our model to the sub-sentential paraphrases directly, in order to improve the quality of the Paraphrase Database. Also related is work by Chan et al. (2011) which reranked bilingually-extra"
P15-2070,S14-2141,0,0.0183407,"s than PPDB 1.0’s heuristic rankings. Each paraphrase pair in the database now also includes finegrained entailment relations, word embedding similarities, and style annotations. 1 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Introduction The Paraphrase Database (PPDB) is a collection of over 100 million paraphrases that was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between c"
P15-2070,N15-1023,1,0.665525,"2015). However, the Rastogi et al. (2015) embeddings included here were shown to be state-of-the art in Style scores Some of the variation within paraphrase sets can be attributed to stylistic variations of language. We automatically induce style information on each rule in PPDB for two dimensions– complexity and formality. Table 2 shows some paraphrases of the end, sorted from most complex to most simple using these scores. These classifications could be useful for natural language generation tasks like text simplification (Xu et al., 2015). A complete evaluation of these scores is given in Pavlick and Nenkova (2015). 3.3 11. 12. 13. 14. 15. used to measure word and phrase similarity, possibly to improve paraphrasing. Multiview Latent Semantic Analysis (MVLSA) is a state-of-the-art method for modeling word similarities. MVLSA can incorporate an arbitrary number of data views, such as monolingual signals, bilingual signals, and even signals from other embeddings. PPDB 2.0 contains new similarity features based on MVLSA embeddings for all phrases. A complete discussion is given in Rastogi et al. (2015). Although we typically think of paraphrases as equivalent or as bidirectionally entailing, a substantial f"
P15-2070,P11-1062,0,0.0101222,"milar model trained on human judgements. Both efforts apply their model to natural language generation by paraphrasing full sentences. We apply our model to the sub-sentential paraphrases directly, in order to improve the quality of the Paraphrase Database. Also related is work by Chan et al. (2011) which reranked bilingually-extracted paraphrases using monolingual distributional similarities, but did not use a supervised model. Work that is relevant to our classification of semantic entailment types to each paraphrase, includes learning directionality of inference rules (Bhagat et al., 2007; Berant et al., 2011) and learning hypernyms rather than paraphrases (Snow et al., 2004). Our style annotations are related to Xu et al. (2012)’s efforts at learning stylistic paraphrases. Our word embeddings additions to the paraphrase database are related to many current projects on that topic, including projects that attempt to customize embeddings to lexical resources (Faruqui et al., 2015). However, the Rastogi et al. (2015) embeddings included here were shown to be state-of-the art in Style scores Some of the variation within paraphrase sets can be attributed to stylistic variations of language. We automatic"
P15-2070,P15-1146,1,0.863748,"Missing"
P15-2070,D07-1017,0,0.0315597,"(2011) developed a similar model trained on human judgements. Both efforts apply their model to natural language generation by paraphrasing full sentences. We apply our model to the sub-sentential paraphrases directly, in order to improve the quality of the Paraphrase Database. Also related is work by Chan et al. (2011) which reranked bilingually-extracted paraphrases using monolingual distributional similarities, but did not use a supervised model. Work that is relevant to our classification of semantic entailment types to each paraphrase, includes learning directionality of inference rules (Bhagat et al., 2007; Berant et al., 2011) and learning hypernyms rather than paraphrases (Snow et al., 2004). Our style annotations are related to Xu et al. (2012)’s efforts at learning stylistic paraphrases. Our word embeddings additions to the paraphrase database are related to many current projects on that topic, including projects that attempt to customize embeddings to lexical resources (Faruqui et al., 2015). However, the Rastogi et al. (2015) embeddings included here were shown to be state-of-the art in Style scores Some of the variation within paraphrase sets can be attributed to stylistic variations of"
P15-2070,D14-1162,0,0.112672,"Missing"
P15-2070,S14-2114,0,0.0184637,"Missing"
P15-2070,N15-1058,1,0.0846646,"Missing"
P15-2070,Q14-1018,0,0.0256743,"embedding similarities, and style annotations. 1 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Introduction The Paraphrase Database (PPDB) is a collection of over 100 million paraphrases that was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between correct and incorrect paraphrases? Within the paraphrase sets, are all of the paraphrases truly substitutable or do they sometimes"
P15-2070,S14-2039,0,0.013621,"embedding similarities, and style annotations. 1 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Introduction The Paraphrase Database (PPDB) is a collection of over 100 million paraphrases that was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between correct and incorrect paraphrases? Within the paraphrase sets, are all of the paraphrases truly substitutable or do they sometimes"
P15-2070,C12-1177,0,0.00936657,"ntences. We apply our model to the sub-sentential paraphrases directly, in order to improve the quality of the Paraphrase Database. Also related is work by Chan et al. (2011) which reranked bilingually-extracted paraphrases using monolingual distributional similarities, but did not use a supervised model. Work that is relevant to our classification of semantic entailment types to each paraphrase, includes learning directionality of inference rules (Bhagat et al., 2007; Berant et al., 2011) and learning hypernyms rather than paraphrases (Snow et al., 2004). Our style annotations are related to Xu et al. (2012)’s efforts at learning stylistic paraphrases. Our word embeddings additions to the paraphrase database are related to many current projects on that topic, including projects that attempt to customize embeddings to lexical resources (Faruqui et al., 2015). However, the Rastogi et al. (2015) embeddings included here were shown to be state-of-the art in Style scores Some of the variation within paraphrase sets can be attributed to stylistic variations of language. We automatically induce style information on each rule in PPDB for two dimensions– complexity and formality. Table 2 shows some paraph"
P15-2070,Q15-1021,1,0.0693675,"ttempt to customize embeddings to lexical resources (Faruqui et al., 2015). However, the Rastogi et al. (2015) embeddings included here were shown to be state-of-the art in Style scores Some of the variation within paraphrase sets can be attributed to stylistic variations of language. We automatically induce style information on each rule in PPDB for two dimensions– complexity and formality. Table 2 shows some paraphrases of the end, sorted from most complex to most simple using these scores. These classifications could be useful for natural language generation tasks like text simplification (Xu et al., 2015). A complete evaluation of these scores is given in Pavlick and Nenkova (2015). 3.3 11. 12. 13. 14. 15. used to measure word and phrase similarity, possibly to improve paraphrasing. Multiview Latent Semantic Analysis (MVLSA) is a state-of-the-art method for modeling word similarities. MVLSA can incorporate an arbitrary number of data views, such as monolingual signals, bilingual signals, and even signals from other embeddings. PPDB 2.0 contains new similarity features based on MVLSA embeddings for all phrases. A complete discussion is given in Rastogi et al. (2015). Although we typically think"
P15-2070,D13-1056,1,0.547488,"Missing"
P15-2070,P14-2089,0,0.046308,"was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between correct and incorrect paraphrases? Within the paraphrase sets, are all of the paraphrases truly substitutable or do they sometimes exhibit other types of relationships (like directional entailment)? When the paraphrases share the same meaning, are there stylistic reasons why we should choose one versus anot"
P15-2070,P08-1116,0,0.0365443,"pes from Wikipedia (constraining to types which appear in PPDB), and collected human judgments for their full list of paraphrases. 2 https://code.google.com/p/word2vec/ For phrases, we use the vector of the rarest word as an approximation of the vector for the phrase. 3 427 1. 2. 3. 4. 5. achieves a 9-12 point improvement in MRR over the PPDB 1.0 rankings. Similarly, it improves AP by 7-9 points. 3 Other Additions Entailment relations 4 the final analysis the last the finish the final part the last part Related Work The most closely related work to our supervised re-ranking of PPDB is work by Zhao et al. (2008) and Malakasiotis and Androutsopoulos (2011). Zhao et al. (2008) improved Bannard and Callison-Burch (2005)’s paraphrase probability by converting it into log-linear model inspired by machine translation, allowing them to incorporate a variety of features. Malakasiotis and Androutsopoulos (2011) developed a similar model trained on human judgements. Both efforts apply their model to natural language generation by paraphrasing full sentences. We apply our model to the sub-sentential paraphrases directly, in order to improve the quality of the Paraphrase Database. Also related is work by Chan et"
P15-2070,N15-1184,0,\N,Missing
P15-2070,W11-2504,1,\N,Missing
P15-2070,D08-1021,1,\N,Missing
P17-1095,D12-1075,0,0.0258911,"abilities in Hy are easy to compute because Hy has the form of a language model, and prefix probabilities in Py are therefore also easy to compute (using a prefix tree for efficiency). This concludes the description of the segmental sampler. Note that the particle Gibbs procedure is unchanged. 4 Inducing parts-of-speech with type-level supervision Automatically inducing parts-of-speech from raw text is a challenging problem (Goldwater et al., 2005). Our focus here is on the easier problem of type-supervised part-of-speech induction, in which (partial) dictionaries are used to guide inference (Garrette and Baldridge, 2012; Li et al., 2012). Conditioned on the unlabeled corpus and dictionary, we use the MCMC procedure described in §3.1 to impute the latent parts-of-speech. Since dictionaries are freely available for hundreds of languages,6 we see this as a mild additional requirement in practice over the purely unsupervised setting. In prior work, dictionaries have been used as constraints on possible parts-of-speech: words appearing in the dictionary take one of their known parts1034 6 https://www.wiktionary.org/ of-speech. In our setting, however, the dictionaries are not constraints but evidence. If monthly"
P17-1095,P07-1094,0,0.0518943,"d with this but did not observe any consistent advantage to doing so in our setting. 4 The label sequence is terminated by a distinguished endof-sequence label, again written as $. 1031 x1:T are then generated conditioned on the label sequence via the corresponding Py distribution (defined in §2.3). All observations with the same label y are drawn from the same Py , and thus this subsequence of observations is distributed according to the Chinese restaurant process (1). We model y using another sequence memoizer model. This is similar to other hierarchical Bayesian models of latent sequences (Goldwater and Griffiths, 2007; Blunsom and Cohn, 2010), but again, it does not limit the Markov order (the number of preceding labels that are conditioned on). Thus, the probability of a sequence of latent types is computed in the same way as the base distribution in §2.4, that is, p(y1:T ) := T Y t=1  Gy1:t−1 (yt ) Gy1:T ($) (4) where Gv (y) denotes the conditional probability of latent label y ∈ Y given the left context v ∈ Y ∗ . Each Gv is a distribution over Y, defined recursively as G ∼ PYP(d , α , UY ) (5) Gv ∼ PYP(d|v |, α|v |, Gσ(v) ) The probability of transitioning to label yt depends on the assignments of"
P17-1095,U11-1004,0,0.0151687,"r (4).  The emission distribution p(xt |Yt = y) depends on the emissions observed from any earlier tokens of y, because of the Chinese restaurant process (1). When  is the only complication, block Metropolis-Hastings samplers have proven effective (Johnson et al., 2007). However, this approach uses dynamic programming to sample from a proposal distribution efficiently, which ¬ precludes in our case. Instead, we use sequential Monte Carlo (SMC)—sometimes called particle filtering—as a proposal distribution. Particle filtering is typically used in online settings, including word segmentation (Borschinger and Johnson, 2011), to make decisions before all of x has been observed. However, we are interested in the inference (or smoothing) problem that conditions on all of x (Dubbin and Blunsom, 2012; Tripuraneni et al., 2015). SMC employs a proposal distribution q(y |x) 1032 whose definition decomposes as follows: q(y1 |x1 ) T Y t=2 q(yt |y1:t−1 , x1:t ) (6) for T = |x|. To sample a sequence of latent labels, first sample an initial label y1 from q1 , then proceed incrementally by sampling yt from qt (· |y1:t−1 , x1:t ) for t = 2, . . . , T . The final sampled sequence y is called a particle, and is given an unnorma"
P17-1095,P11-1061,0,0.0282037,"section, we are interested in evaluating our proposed Bayesian model in the context of low-resource NER. Experiments We follow the experimental procedure described in Li et al. (2012), and use their released code and data to compare to their best model: a second-order maximum entropy Markov model parametrized with log-linear features (SHMM - ME). This model uses hand-crafted features designed to distinguish between different parts-of-speech, and it has special handling for rare words. This approach is surprisingly effective and outperforms alternate approaches such as cross-lingual transfer (Das and Petrov, 2011). However, it also has limitations, since words that do not appear in the dictionary will be unconstrained, and spurious or incorrect lexical entries may lead to propagation of errors. The lexicons are taken from the Wiktionary project; their size and coverage are documented by (Li et al., 2012). We evaluate our model on multi-lingual data released as part of the CoNLL 2007 and CoNLL-X shared tasks. In particular, we use the same set of languages as Li et al. (2012).7 For our method, we impute the parts-of-speech by running particle Gibbs for 100 epochs, where one epoch consists of resampling"
P17-1095,D11-1057,1,0.836785,"pes to appear ≥ c times in an unbounded sequence of IID draws from Py . When c = 1, this is equivalent to modeling the lexicon as my draws without replacement from Py .2 Unfortunately, draws without replacement are no longer IID or exchangeable: order matters. It would therefore become difficult to condition inference and learning on an observed lexicon, because we would need to explicitly sum or sample over the possibilities for the latent sequence of tokens (or stick segments). We therefore adopt the simpler deficient model. A version of our lexicon model (with c = 1) was previously used by Dreyer and Eisner (2011, Appendix C), who observed a list of verb paradigm types rather than word or entity-name types. 2.3 Prior distribution over Py We assume a priori that Py was drawn from a Pitman-Yor process (PYP) (Pitman and Yor, 1997). Both the lexicon and the ordinary corpus are observations that provide information about Py . The PYP is defined by three parameters: a concentration parameter α, a discount parameter d, and a base distribution Hy . In our case, Hy is a distribution over X = Σ∗ , the set of possible strings over a finite character alphabet Σ. For example, HLOC is used to choose new place names"
P17-1095,W12-1907,0,0.0155307,"mplication, block Metropolis-Hastings samplers have proven effective (Johnson et al., 2007). However, this approach uses dynamic programming to sample from a proposal distribution efficiently, which ¬ precludes in our case. Instead, we use sequential Monte Carlo (SMC)—sometimes called particle filtering—as a proposal distribution. Particle filtering is typically used in online settings, including word segmentation (Borschinger and Johnson, 2011), to make decisions before all of x has been observed. However, we are interested in the inference (or smoothing) problem that conditions on all of x (Dubbin and Blunsom, 2012; Tripuraneni et al., 2015). SMC employs a proposal distribution q(y |x) 1032 whose definition decomposes as follows: q(y1 |x1 ) T Y t=2 q(yt |y1:t−1 , x1:t ) (6) for T = |x|. To sample a sequence of latent labels, first sample an initial label y1 from q1 , then proceed incrementally by sampling yt from qt (· |y1:t−1 , x1:t ) for t = 2, . . . , T . The final sampled sequence y is called a particle, and is given an unnormalized importance weight of w ˜=w ˜T · p($ |y1:T ) where w ˜T was built up via w ˜t := w ˜t−1 · p(y1:t , x1:t ) p(y1:t−1 , x1:t−1 ) q(yt |y1:t−1 , x1:t ) (7) The SMC procedure"
P17-1095,P05-1045,0,0.0620648,"Missing"
P17-1095,D12-1127,0,0.0333437,"Missing"
P17-1095,P09-1012,0,0.0338895,"that generative models are typically less feature-rich than their globally normalized discriminative counterparts (e.g. conditional random fields). In designing our approach—the hierarchical sequence memoizer (HSM)—we aim to be reasonably expressive while retaining practically useful inference algorithms. We propose a Bayesian nonparametric model to serve as a generative distribution responsible for both lexicon and corpus data. The proposed model memoizes previously used lexical entries (words or phrases) but backs off to a character-level distribution when generating novel types (Teh, 2006; Mochihashi et al., 2009). We propose an efficient inference algorithm for the proposed model using particle Gibbs sampling (§3). Our code is available at https://github.com/noa/bayesner. 1029 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1029–1039 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1095 2 Model Our goal is to fit a model that can automatically annotate text. We observe a supervised or unsupervised training corpus. For each label y in the annotation scheme, we also observe a lexic"
P17-1095,P05-1003,0,0.0357877,"we use lexical resources to guide part-of-speech induction (§4) and to bootstrap named-entity recognizers in low-resource languages (§5). Given their success, it is perhaps surprising that incorporating gazetteers or dictionaries into discriminative models (e.g. conditional random fields) may sometimes hurt performance. This phenomena is called weight under-training, in which lexical features—which detect whether a name is listed in the dictionary or gazetteer—are given excessive weight at the expense of other useful features such as spelling features that would generalize to unlisted names (Smith et al., 2005; Sutton et al., 2006; Smith and Osborne, 2006). Furthermore, discriminative training with lexical features requires sufficient annotated training data, which poses challenges for the unsupervised and low-resource settings we consider here. Our observation is that Bayesian modeling provides a principled solution. The lexicon is itself a dataset that was generated by some process. Practically, this means that lexicon entries (words or phrases) may be treated as additional observations. As a result, these entries provide information about how names are spelled. The presence of the lexicon theref"
P17-1095,W06-2918,0,0.0369595,"f-speech induction (§4) and to bootstrap named-entity recognizers in low-resource languages (§5). Given their success, it is perhaps surprising that incorporating gazetteers or dictionaries into discriminative models (e.g. conditional random fields) may sometimes hurt performance. This phenomena is called weight under-training, in which lexical features—which detect whether a name is listed in the dictionary or gazetteer—are given excessive weight at the expense of other useful features such as spelling features that would generalize to unlisted names (Smith et al., 2005; Sutton et al., 2006; Smith and Osborne, 2006). Furthermore, discriminative training with lexical features requires sufficient annotated training data, which poses challenges for the unsupervised and low-resource settings we consider here. Our observation is that Bayesian modeling provides a principled solution. The lexicon is itself a dataset that was generated by some process. Practically, this means that lexicon entries (words or phrases) may be treated as additional observations. As a result, these entries provide information about how names are spelled. The presence of the lexicon therefore now improves training of the spelling featu"
P17-1095,L16-1521,0,0.0289298,"not fully documented. 5 5.1 Boostrapping NER with type-level supervision Data Most languages do not have corpora annotated for parts-of-speech, named-entities, syntactic parses, or other linguistic annotations. Therefore, rapidly deploying natural language technologies in a new language may be challenging. In the context of facilitating relief responses in emergencies such as natural disasters, the DARPA LORELEI (Low Resource Languages for Emergent Incidents) program has sponsored the development and release of representative “language packs” for Turkish and Uzbek with more languages planned (Strassel and Tracey, 2016). We use the named-entity annotations as part of these language packs which include persons, locations, organizations, and geo-political entities, in order to explore bootstrapping named-entity recognition from small amounts of data. We consider two types of data: ¬ in-context annotations, where sentences are fully annotated for named-entities, and  lexical resources. The LORELEI language packs lack adequate indomain lexical resources for our purposes. Therefore, we simulate in-domain lexical resources by holding out portions of the annotated development data and deriving dictionaries and nam"
P17-1095,N06-1012,0,0.0356646,"urces to guide part-of-speech induction (§4) and to bootstrap named-entity recognizers in low-resource languages (§5). Given their success, it is perhaps surprising that incorporating gazetteers or dictionaries into discriminative models (e.g. conditional random fields) may sometimes hurt performance. This phenomena is called weight under-training, in which lexical features—which detect whether a name is listed in the dictionary or gazetteer—are given excessive weight at the expense of other useful features such as spelling features that would generalize to unlisted names (Smith et al., 2005; Sutton et al., 2006; Smith and Osborne, 2006). Furthermore, discriminative training with lexical features requires sufficient annotated training data, which poses challenges for the unsupervised and low-resource settings we consider here. Our observation is that Bayesian modeling provides a principled solution. The lexicon is itself a dataset that was generated by some process. Practically, this means that lexicon entries (words or phrases) may be treated as additional observations. As a result, these entries provide information about how names are spelled. The presence of the lexicon therefore now improves trai"
P17-1095,P06-1124,0,0.0429109,"ownside is that generative models are typically less feature-rich than their globally normalized discriminative counterparts (e.g. conditional random fields). In designing our approach—the hierarchical sequence memoizer (HSM)—we aim to be reasonably expressive while retaining practically useful inference algorithms. We propose a Bayesian nonparametric model to serve as a generative distribution responsible for both lexicon and corpus data. The proposed model memoizes previously used lexical entries (words or phrases) but backs off to a character-level distribution when generating novel types (Teh, 2006; Mochihashi et al., 2009). We propose an efficient inference algorithm for the proposed model using particle Gibbs sampling (§3). Our code is available at https://github.com/noa/bayesner. 1029 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1029–1039 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1095 2 Model Our goal is to fit a model that can automatically annotate text. We observe a supervised or unsupervised training corpus. For each label y in the annotation schem"
P17-1095,P11-1087,0,\N,Missing
P17-1095,N07-1018,0,\N,Missing
P17-2030,P16-1070,0,0.0764261,"sy-first strategy. The decoder iteratively corrects the most probable ungrammatical token by applying different classifiers for each error type. The EREF parser also depends on the easy-first strategy to find ungrammatical index to be deleted, inserted, or substituted, but it parses and corrects errors jointly whereas the decoder is designed as a grammatical error correction framework rather than a parser. There is a line of work for parsing ungrammatical sentences (e.g., web forum) by adapting an existing parsing scheme on domain specific annotations (Petrov and McDonald, 2012; Cahill, 2015; Berzak et al., 2016; Nagata and Sakaguchi, 2016). Although we share an interest with respect to dealing with ungrammatical sentences, EREF focuses on the parsing scheme for repairing grammatical errors instead of adapting a parser with a domain specific annotation scheme. More broadly, our work can also be regarded as one of the joint parsing and text normalization tasks such as joint spelling correction and POS tagging (Sakaguchi et al., 2012), word segmentation and POS tagging (Kaji and Kitsuregawa, 2014; Qian et al., 2015). 5 Acknowledgments This work was supported in part by the JHU Human Language Technology"
P17-2030,W15-1616,0,0.0125666,"er with the easy-first strategy. The decoder iteratively corrects the most probable ungrammatical token by applying different classifiers for each error type. The EREF parser also depends on the easy-first strategy to find ungrammatical index to be deleted, inserted, or substituted, but it parses and corrects errors jointly whereas the decoder is designed as a grammatical error correction framework rather than a parser. There is a line of work for parsing ungrammatical sentences (e.g., web forum) by adapting an existing parsing scheme on domain specific annotations (Petrov and McDonald, 2012; Cahill, 2015; Berzak et al., 2016; Nagata and Sakaguchi, 2016). Although we share an interest with respect to dealing with ungrammatical sentences, EREF focuses on the parsing scheme for repairing grammatical errors instead of adapting a parser with a domain specific annotation scheme. More broadly, our work can also be regarded as one of the joint parsing and text normalization tasks such as joint spelling correction and POS tagging (Sakaguchi et al., 2012), word segmentation and POS tagging (Kaji and Kitsuregawa, 2014; Qian et al., 2015). 5 Acknowledgments This work was supported in part by the JHU Huma"
P17-2030,W02-1001,0,0.0923888,"tokens w1 , ..., wn , and it keeps updating pending through derivations. Unlike left-to-right (e.g., shift-reduce) parsing algorithms (Yamada and Matsumoto, 2003; Nivre, 2004), EF iteratively selects the best pair of adjoining tokens and chooses the direction of attachment: ATTACHLEFT or ATTACHRIGHT. Once the action is committed, the corresponding dependency arc is added and the child token is removed from pending. The first two derivations in Figure 1 depict ATTACHRIGHT and ATTACHLEFT. Pseudocode is shown in Algorithm 1 (lines 1, 3-12). The parser is trained using the structured perceptron (Collins, 2002) to choose actions to take given a set of features expanded from templates. The cost of actions is computed at every step by checking the validity: whether a new arc is included in the gold parse and whether the child already has all its children. See GE for further description of feature templates and structured perceptron training. Since it is possible that there are multiple valid sequence of actions and it is important to examine a large search space, the parser is allowed to explore (possibly incorrect) actions with a certain probability, termed learning with exploration by Goldberg and N"
P17-2030,D12-1052,0,0.0308823,"d interregnum) for a given speech utterance. Our work could be considered an extension via adding SUBSTITUTE and INSERT actions, although we depend on easy-first non-directional parsing framework instead of a left-to-right strategy. Importantly, the DELETE action is easier to handle than the SUBSTITUTE and INSERT actions, because they bring us challenging issues about a process of candidate word generation and avoiding an infinite loop in derivation. We have addressed these issues as explained in §2. In terms of the literature from grammatical error correction, this work is closely related to Dahlmeier and Ng (2012), where they show an error correction decoder with the easy-first strategy. The decoder iteratively corrects the most probable ungrammatical token by applying different classifiers for each error type. The EREF parser also depends on the easy-first strategy to find ungrammatical index to be deleted, inserted, or substituted, but it parses and corrects errors jointly whereas the decoder is designed as a grammatical error correction framework rather than a parser. There is a line of work for parsing ungrammatical sentences (e.g., web forum) by adapting an existing parsing scheme on domain specif"
P17-2030,J93-2004,0,0.0613868,". We do not deal with a swapping action (Nivre, 2009) to deal with word reordering errors, since the errors are even less frequent than other error types (Leacock et al., 2014). SUBSTITUTE replaces a token to a grammatically more probable token, DELETE removes an unnecessary token, and INSERT inserts a new token at a designated index. These actions are shown in Figure 1 and Algorithm 1 (lines 13-25). Because the length of pending decreases as an attachment occurs, the parser 190 In the first experiment, as in GE, we train and evaluate our parser on the English dataset from the Penn Treebank (Marcus et al., 1993) with the Penn2Malt conversion program (Sections 2-21 for training, 22 for tuning, and 23 for test). We use the PTB for the dependency experiment, since there are no ungrammatical text corpora that has dependency annotation on the corrected texts by human. We choose the following most frequent error types that are used in CoNLL 2013 shared task (Ng et al., 2013): Algorithm 2: Check validity during training 1 2 3 4 5 6 7 Function isValid(act, repaired, Gold) d before = editDistance(repaired, Gold) repaired + = repaired.apply(act) d after = editDistance(repaired + , Gold) if d before &gt; d after t"
P17-2030,P16-1173,1,0.797184,"e decoder iteratively corrects the most probable ungrammatical token by applying different classifiers for each error type. The EREF parser also depends on the easy-first strategy to find ungrammatical index to be deleted, inserted, or substituted, but it parses and corrects errors jointly whereas the decoder is designed as a grammatical error correction framework rather than a parser. There is a line of work for parsing ungrammatical sentences (e.g., web forum) by adapting an existing parsing scheme on domain specific annotations (Petrov and McDonald, 2012; Cahill, 2015; Berzak et al., 2016; Nagata and Sakaguchi, 2016). Although we share an interest with respect to dealing with ungrammatical sentences, EREF focuses on the parsing scheme for repairing grammatical errors instead of adapting a parser with a domain specific annotation scheme. More broadly, our work can also be regarded as one of the joint parsing and text normalization tasks such as joint spelling correction and POS tagging (Sakaguchi et al., 2012), word segmentation and POS tagging (Kaji and Kitsuregawa, 2014; Qian et al., 2015). 5 Acknowledgments This work was supported in part by the JHU Human Language Technology Center of Excellence (HLTCOE"
P17-2030,W09-2112,0,0.0856712,"Missing"
P17-2030,W12-3018,1,0.794323,"Missing"
P17-2030,N10-1115,0,0.0306583,"riety of noisy outputs, such as ungrammatical webpages, speech disfluencies, and the text in language learner’s essays. Such non-canonical text contains grammatical errors such as substitutions, insertions, and deletions. For example, a nonnative speaker of English might write “*I look in forward hear from you”, where in is inserted, to is deleted, and hearing is substituted incorrectly. We propose a novel dependency parsing scheme that jointly parses and repairs ungrammatical sentences with these sorts of errors. The parser is based on the non-directional easy-first (EF) parser introduced by Goldberg and Elhadad (2010) (GE herein), which iteratively adds the most probable arc until the parse tree is completed. These actions are called ATTACHLEFT and ATTACHRIGHT depending on the direction of the arc. We extend the EF parsing scheme to be robust for ungrammatical inputs by correcting grammatical erforward to hearing from you Figure 1: Illustrative example of partial derivation under error-repair easy-first non-directional dependency parsing. Solid arrows represent ATTACHRIGHT and ATTACHLEFT in Goldberg and Elhadad (2010). Dotted arcs correspond to actions for each step. Following the notation by GE: arcs are"
P17-2030,W13-3601,0,0.0221208,"ure 1 and Algorithm 1 (lines 13-25). Because the length of pending decreases as an attachment occurs, the parser 190 In the first experiment, as in GE, we train and evaluate our parser on the English dataset from the Penn Treebank (Marcus et al., 1993) with the Penn2Malt conversion program (Sections 2-21 for training, 22 for tuning, and 23 for test). We use the PTB for the dependency experiment, since there are no ungrammatical text corpora that has dependency annotation on the corrected texts by human. We choose the following most frequent error types that are used in CoNLL 2013 shared task (Ng et al., 2013): Algorithm 2: Check validity during training 1 2 3 4 5 6 7 Function isValid(act, repaired, Gold) d before = editDistance(repaired, Gold) repaired + = repaired.apply(act) d after = editDistance(repaired + , Gold) if d before &gt; d after then return true; else return false; end for ActsER is based on validity. The validity of the new actions is computed by taking the edit distance (d) between the Gold tokens (w1∗ ... wr∗ ) and the sentence state that the parser stores in repaired (w ˆ1 ... w ˆm ). When the edit distance after taking an action (d after ) is smaller than before (d before ), we rega"
P17-2030,Q13-1033,0,0.0179366,"Collins, 2002) to choose actions to take given a set of features expanded from templates. The cost of actions is computed at every step by checking the validity: whether a new arc is included in the gold parse and whether the child already has all its children. See GE for further description of feature templates and structured perceptron training. Since it is possible that there are multiple valid sequence of actions and it is important to examine a large search space, the parser is allowed to explore (possibly incorrect) actions with a certain probability, termed learning with exploration by Goldberg and Nivre (2013). act∈Acts∪Acts ER 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 s.t. 1 ≤ i ≤ len(pending) ∩ isLegal(act, pending) if best ∈ Acts then (parent, child) ← edgeFor(best) Arcs.add((parent, child)) pending.remove(child) else if best = SUBSTITUTE then c = bestCandidate(best, repaired) pending.replace(pi , c) repaired.replace(w ˆpi .idx , c) else if best = DELETE then pending.remove(pi ) repaired.remove(w ˆpi .idx ) Arcs.updateIndex() else if best = INSERT then c = bestCandidate(best, repaired) pending.insert(i, c) repaired.insert(pi .idx, c) Arcs.updateIndex() end return Arcs, repaired a"
P17-2030,W04-0308,0,0.0803614,"← w1 ...wn 5 repaired = w ˆ1 ...w ˆn ← w1 ...wn 6 while len (pending) &gt; 1 do 7 best ← argmax score (act (i)) Model Non-directional Easy-first Parsing Let us begin with a brief review of a non-directional easyfirst (EF) parsing scheme proposed by GE, which is the foundation of our proposed scheme described in the following sections. The EF parser has a list of partial structures p1 , ..., pk (called pending) initialized with sentence tokens w1 , ..., wn , and it keeps updating pending through derivations. Unlike left-to-right (e.g., shift-reduce) parsing algorithms (Yamada and Matsumoto, 2003; Nivre, 2004), EF iteratively selects the best pair of adjoining tokens and chooses the direction of attachment: ATTACHLEFT or ATTACHRIGHT. Once the action is committed, the corresponding dependency arc is added and the child token is removed from pending. The first two derivations in Figure 1 depict ATTACHRIGHT and ATTACHLEFT. Pseudocode is shown in Algorithm 1 (lines 1, 3-12). The parser is trained using the structured perceptron (Collins, 2002) to choose actions to take given a set of features expanded from templates. The cost of actions is computed at every step by checking the validity: whether a new"
P17-2030,W11-2123,0,0.0477824,"Missing"
P17-2030,P09-1040,0,0.028119,"ding, repaired, and parents and children in a (partial) dependency tree (Arcs). To find the best candidate for SUBSTITUTE and INSERT efficiently, we restrict candidates to the same part-of-speech or pre-defined candidate list. We select the best candidate by comparing each n-gram language model score with the same surrounding context. Similar to EF, while training the parser, the cost Error-repair variant of EF Error-repair nondirectional easy-first parsing scheme (EREF) is a variant of EF. We add three new actions: SUBSTITUTE, DELETE, INSERT as ActsER . We do not deal with a swapping action (Nivre, 2009) to deal with word reordering errors, since the errors are even less frequent than other error types (Leacock et al., 2014). SUBSTITUTE replaces a token to a grammatically more probable token, DELETE removes an unnecessary token, and INSERT inserts a new token at a designated index. These actions are shown in Figure 1 and Algorithm 1 (lines 13-25). Because the length of pending decreases as an attachment occurs, the parser 190 In the first experiment, as in GE, we train and evaluate our parser on the English dataset from the Penn Treebank (Marcus et al., 1993) with the Penn2Malt conversion pro"
P17-2030,P14-2029,0,0.0728258,"Missing"
P17-2030,D15-1211,0,0.0393197,"Missing"
P17-2030,Q14-1011,0,0.0174173,"d the total number of edits during derivation. The experimental result has demonstrated robustness of EREF parsers against EF and grammaticality improvement. Our work is positioned at the intersection of noisy text parsing and grammatical error correction. The EREF is a flexible formalism not only for grammatical error correction but other tasks with jointly editing and parsing a given sentence. Our work lies at the intersection of parsing noncanonical texts and grammatical error correction. Joint dependency parsing and disfluency detection has been pursued (Rasooli and Tetreault, 2013, 2014; Honnibal and Johnson, 2014; Wu et al., 2015; Yoshikawa et al., 2016), where a parser jointly parses and detects disfluency (e.g., reparandum and interregnum) for a given speech utterance. Our work could be considered an extension via adding SUBSTITUTE and INSERT actions, although we depend on easy-first non-directional parsing framework instead of a left-to-right strategy. Importantly, the DELETE action is easier to handle than the SUBSTITUTE and INSERT actions, because they bring us challenging issues about a process of candidate word generation and avoiding an infinite loop in derivation. We have addressed these issu"
P17-2030,D14-1011,0,0.027595,"Missing"
P17-2030,D13-1013,0,0.0197434,"f editing history for each token and the total number of edits during derivation. The experimental result has demonstrated robustness of EREF parsers against EF and grammaticality improvement. Our work is positioned at the intersection of noisy text parsing and grammatical error correction. The EREF is a flexible formalism not only for grammatical error correction but other tasks with jointly editing and parsing a given sentence. Our work lies at the intersection of parsing noncanonical texts and grammatical error correction. Joint dependency parsing and disfluency detection has been pursued (Rasooli and Tetreault, 2013, 2014; Honnibal and Johnson, 2014; Wu et al., 2015; Yoshikawa et al., 2016), where a parser jointly parses and detects disfluency (e.g., reparandum and interregnum) for a given speech utterance. Our work could be considered an extension via adding SUBSTITUTE and INSERT actions, although we depend on easy-first non-directional parsing framework instead of a left-to-right strategy. Importantly, the DELETE action is easier to handle than the SUBSTITUTE and INSERT actions, because they bring us challenging issues about a process of candidate word generation and avoiding an infinite loop in deriva"
P17-2030,E14-4010,0,0.0333871,"Missing"
P17-2030,roark-etal-2006-sparseval,0,0.0914786,"Missing"
P17-2030,Q14-1033,0,0.023988,"Missing"
P17-2030,C12-1144,1,0.836526,"Missing"
P17-2030,P15-1048,0,0.0129816,"during derivation. The experimental result has demonstrated robustness of EREF parsers against EF and grammaticality improvement. Our work is positioned at the intersection of noisy text parsing and grammatical error correction. The EREF is a flexible formalism not only for grammatical error correction but other tasks with jointly editing and parsing a given sentence. Our work lies at the intersection of parsing noncanonical texts and grammatical error correction. Joint dependency parsing and disfluency detection has been pursued (Rasooli and Tetreault, 2013, 2014; Honnibal and Johnson, 2014; Wu et al., 2015; Yoshikawa et al., 2016), where a parser jointly parses and detects disfluency (e.g., reparandum and interregnum) for a given speech utterance. Our work could be considered an extension via adding SUBSTITUTE and INSERT actions, although we depend on easy-first non-directional parsing framework instead of a left-to-right strategy. Importantly, the DELETE action is easier to handle than the SUBSTITUTE and INSERT actions, because they bring us challenging issues about a process of candidate word generation and avoiding an infinite loop in derivation. We have addressed these issues as explained i"
P17-2030,W03-3023,0,0.162372,"= { } 4 pending = p1 ...pn ← w1 ...wn 5 repaired = w ˆ1 ...w ˆn ← w1 ...wn 6 while len (pending) &gt; 1 do 7 best ← argmax score (act (i)) Model Non-directional Easy-first Parsing Let us begin with a brief review of a non-directional easyfirst (EF) parsing scheme proposed by GE, which is the foundation of our proposed scheme described in the following sections. The EF parser has a list of partial structures p1 , ..., pk (called pending) initialized with sentence tokens w1 , ..., wn , and it keeps updating pending through derivations. Unlike left-to-right (e.g., shift-reduce) parsing algorithms (Yamada and Matsumoto, 2003; Nivre, 2004), EF iteratively selects the best pair of adjoining tokens and chooses the direction of attachment: ATTACHLEFT or ATTACHRIGHT. Once the action is committed, the corresponding dependency arc is added and the child token is removed from pending. The first two derivations in Figure 1 depict ATTACHRIGHT and ATTACHLEFT. Pseudocode is shown in Algorithm 1 (lines 1, 3-12). The parser is trained using the structured perceptron (Collins, 2002) to choose actions to take given a set of features expanded from templates. The cost of actions is computed at every step by checking the validity:"
P17-2030,D16-1109,0,0.0201967,". The experimental result has demonstrated robustness of EREF parsers against EF and grammaticality improvement. Our work is positioned at the intersection of noisy text parsing and grammatical error correction. The EREF is a flexible formalism not only for grammatical error correction but other tasks with jointly editing and parsing a given sentence. Our work lies at the intersection of parsing noncanonical texts and grammatical error correction. Joint dependency parsing and disfluency detection has been pursued (Rasooli and Tetreault, 2013, 2014; Honnibal and Johnson, 2014; Wu et al., 2015; Yoshikawa et al., 2016), where a parser jointly parses and detects disfluency (e.g., reparandum and interregnum) for a given speech utterance. Our work could be considered an extension via adding SUBSTITUTE and INSERT actions, although we depend on easy-first non-directional parsing framework instead of a left-to-right strategy. Importantly, the DELETE action is easier to handle than the SUBSTITUTE and INSERT actions, because they bring us challenging issues about a process of candidate word generation and avoiding an infinite loop in derivation. We have addressed these issues as explained in §2. In terms of the lit"
P17-2048,D11-1142,0,0.0600668,"his because they create entities on the fly. We use annotated versions of Gigaword 5 (Parker et al., 2011; Ferraro et al., 2014) and English Wikipedia (February 24, 2016 dump) to construct our PKBs.4 We use Amazon Mechanical Turk workers as annotators. We generated our PKBs with τ = 15, ρ = 0.5, αt = 40, αc = 20, and αa = 10. These constants were tuned by hand Trigger Word Analysis At this stage we have found on the order of 2 to 20 sentences which mention the query and a related entity which will be used to determine the relation between them. There is work on rule-based (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015), supervised (Mausam et al., 2012), 2 Mentions with a score near τ may be coreferent, so we prefer low scoring mentions to avoid over-splitting entities. 3 These values depend on the query (which are more or less rare in a corpus) and pruning thresholds (for our experiments we stop at 100 query mentions) 4 We do not use the coreference annotations provided by Annotated Gigaword, only the features described in §3.1. 307 and are not sensitive to small changes. We take a subset of the PKB which covers the 15 most related entities and the one-best trigger for each. We call th"
P17-2048,P15-1034,0,0.0852611,"ate entities on the fly. We use annotated versions of Gigaword 5 (Parker et al., 2011; Ferraro et al., 2014) and English Wikipedia (February 24, 2016 dump) to construct our PKBs.4 We use Amazon Mechanical Turk workers as annotators. We generated our PKBs with τ = 15, ρ = 0.5, αt = 40, αc = 20, and αa = 10. These constants were tuned by hand Trigger Word Analysis At this stage we have found on the order of 2 to 20 sentences which mention the query and a related entity which will be used to determine the relation between them. There is work on rule-based (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015), supervised (Mausam et al., 2012), 2 Mentions with a score near τ may be coreferent, so we prefer low scoring mentions to avoid over-splitting entities. 3 These values depend on the query (which are more or less rare in a corpus) and pruning thresholds (for our experiments we stop at 100 query mentions) 4 We do not use the coreference annotations provided by Annotated Gigaword, only the features described in §3.1. 307 and are not sensitive to small changes. We take a subset of the PKB which covers the 15 most related entities and the one-best trigger for each. We call these “explanations” whe"
P17-2048,P98-1012,0,0.0619664,"cker.com/r/hltcoe/pocket-knowledge-basepopulation 305 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 305–310 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2048 Figure 1: High level steps of the PKB construction process. Example PKBs can be found in Table 3. The first two steps of initial search and related entities are described in §3.1, the third step of joint searching in §3.2, and finally extracting triggers in §3.3. we adopt the vector space model (Bagga and Baldwin, 1998). First, triage features are used to locate sentences in an inverted index, including the mention headword and all word unigrams and bigrams (case sensitive and insensitive). Next, we use context features: a word unigram tf-idf vector. We implement a compromise between Cucerzan (2007) (used sentences before, after, and containing a mention) and Bagga and Baldwin (1998) (used any sentence in a coreference chain). Instead of running a coreference resolver, we use the high-precision heuristic of linking mentions with the same headword and NER 2 type. Terms are weighted as 1+d where d is the dista"
P17-2048,W03-0405,0,0.103607,"e mention headword and all word unigrams and bigrams (case sensitive and insensitive). Next, we use context features: a word unigram tf-idf vector. We implement a compromise between Cucerzan (2007) (used sentences before, after, and containing a mention) and Bagga and Baldwin (1998) (used any sentence in a coreference chain). Instead of running a coreference resolver, we use the high-precision heuristic of linking mentions with the same headword and NER 2 type. Terms are weighted as 1+d where d is the distance in sentences to the nearest mention. Our attribute features are a generalization of Mann and Yarowsky (2003). They train a few ad-hoc relation extractors like birth year and occupation from seed facts. Their extractions provide high-precision signal for merging entity mentions. We found extracting all NNP* or capitalized JJ* words within 4 edges in a dependency tree was less sparse, requires no seeds, and produced similar quality attributes. We union these attributes across mentions found by the headword and NER type coreference heuristic to build a 2 fine-grain tf-idf vector. We use the same 1+d re-weighting for attributes, except where d is the distance in dependency edges to the entity mention he"
P17-2048,D12-1048,0,0.230246,"notated versions of Gigaword 5 (Parker et al., 2011; Ferraro et al., 2014) and English Wikipedia (February 24, 2016 dump) to construct our PKBs.4 We use Amazon Mechanical Turk workers as annotators. We generated our PKBs with τ = 15, ρ = 0.5, αt = 40, αc = 20, and αa = 10. These constants were tuned by hand Trigger Word Analysis At this stage we have found on the order of 2 to 20 sentences which mention the query and a related entity which will be used to determine the relation between them. There is work on rule-based (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015), supervised (Mausam et al., 2012), 2 Mentions with a score near τ may be coreferent, so we prefer low scoring mentions to avoid over-splitting entities. 3 These values depend on the query (which are more or less rare in a corpus) and pruning thresholds (for our experiments we stop at 100 query mentions) 4 We do not use the coreference annotations provided by Annotated Gigaword, only the features described in §3.1. 307 and are not sensitive to small changes. We take a subset of the PKB which covers the 15 most related entities and the one-best trigger for each. We call these “explanations” where each is a sentence with three l"
P17-2048,P09-1113,0,0.07802,"ly consider the subset of mentions that have cos θt &gt; 0, which can be efficiently retrieved via an inverted index. Any mention with a score higher than τ is considered coreferent with the query. We extract mentions in the same sentences as the query as candidate related entities if they have an NER type of PER, ORG, or LOC. We link candidate mentions against entities in the PKB using the same coreference score used to retrieve query mentions. If a candidate’s best link has a score s < τ , we promote it to an entity and add it to the PKB with probability 1 − τs .2 3.2 and distantly-supervised (Mintz et al., 2009) methods for characterizing relations in text. Our method is similar to distant supervision, where a KB of known facts is used to infer how relations are expressed, but we use supervision from the KB being constructed. We cast the problem of characterizing a relation as a search for trigger words. We state our priors on trigger words and condition on the data to find likely triggers. Predicate (triggers) and arguments are syntactically close together. Assuming the related entity mention heads are arguments, we compute the probability that these two random walks in a dependency tree end up at t"
P17-2048,D07-1074,0,0.0250731,"P17-2048 Figure 1: High level steps of the PKB construction process. Example PKBs can be found in Table 3. The first two steps of initial search and related entities are described in §3.1, the third step of joint searching in §3.2, and finally extracting triggers in §3.3. we adopt the vector space model (Bagga and Baldwin, 1998). First, triage features are used to locate sentences in an inverted index, including the mention headword and all word unigrams and bigrams (case sensitive and insensitive). Next, we use context features: a word unigram tf-idf vector. We implement a compromise between Cucerzan (2007) (used sentences before, after, and containing a mention) and Bagga and Baldwin (1998) (used any sentence in a coreference chain). Instead of running a coreference resolver, we use the high-precision heuristic of linking mentions with the same headword and NER 2 type. Terms are weighted as 1+d where d is the distance in sentences to the nearest mention. Our attribute features are a generalization of Mann and Yarowsky (2003). They train a few ad-hoc relation extractors like birth year and occupation from seed facts. Their extractions provide high-precision signal for merging entity mentions. We"
P18-1020,D15-1008,0,0.0880989,"humans providing responses that are used to derive graded values on natural language contexts, or in the ordering of systems corresponding to their perceived performance on some task. Many NLP datasets involve eliciting from annotators some graded response. The most popular annotation scheme is the n-ary ordinal approach as illustrated in Figure 1(a). For example, text may be labeled for sentiment as positive, neutral or negative (Wiebe et al., 1999; Pang et al., 2002; Turney, 2002, inter alia); or under political spectrum analysis as liberal, neutral, or conservative (O’Connor et al., 2010; Bamman and Smith, 2015). A response may correspond to a likelihood judgment, e.g., how likely a predicate is factive (Lee et al., 2015), or that some natural language inference may hold (Zhang et al., 2017). Responses may correspond to a notion of semantic similarity, e.g., whether one word can be substituted for another in context (Pavlick et al., 2015), or whether an entire sentence is more or less similar than another (Marelli et al., 2014), and so on. Less common in NLP are system comparisons based on direct human ratings, but an exception includes the annual shared task evaluations of the Conference on Machine"
P18-1020,W12-3102,0,0.0321624,"Missing"
P18-1020,P14-2002,1,0.808814,"Missing"
P18-1020,W13-2305,0,0.0404393,"ions (i.e., ratio scale in Stevens’s terminology), which directly correspond to continuous quantities (Figure 1(b)). In scalar DA,3 each instance in the collection (Si ∈ S1N ) is annotated with values (e.g., on the range 0 to 100) often by several annotators. The notion of quantitative difference is enabled by the property of absolute zero: the scale is bounded. For example, distance, length, mass, size etc. are represented by this scale. In the annual shared task evaluation of the WMT, DA has been used for scoring adequacy and fluency of machine learning system outputs with human evaluation (Graham et al., 2013, 2014; Bojar et al., 2016, 2017), and has separately been used in creating datasets such as for factuality (Lee et al., 2015). Why perhaps obviated? Because of two concerns: (1) annotators may not have a pre-existing, well-calibrated scale for performing DA on a particular collection according to a particular task;4 and (2) it is known that people may be biased in their scalar estimates (Tversky and Kahneman, 1974). Regarding (1), this motivates us to consider RA on the intuition that annotators may give more calibrated responses when performed in the context of other elements. Regarding (2),"
P18-1020,E14-1047,0,0.0891845,"Missing"
P18-1020,N13-1132,0,0.0514393,"each label 1 2 3 3.1 Online Pairwise Ranking Aggregation Unbounded Model Pairwise ranking aggregation (Thurstone, 1927) is a method to obtain a total ranking on instances, 3 In the rest of the paper, we take DA to mean scalar annotation rather than ordinals. 4 E.g., try to imagine your level of calibration to a hypothetical task described as ”On a scale of 1 to 100, label this tweet according to a conservative / liberal political spectrum.” 5 There has been a line of work on relative weighting of annotators, based on their agreement with others (Whitehill et al., 2009; Welinder et al., 2010; Hovy et al., 2013). In this paper, however, we do not perform such annotator weighting. Pronounced as “easel”. We release the code at http://decomp.net/. 209 assuming that scalar value for each sample point follows a Gaussian distribution, N (µi , σ 2 ). The parameters {µi } are interpreted as mean scalar annotation.6 Given the parameters, the probability that Si is preferred () over Sj is defined as   µi − µ j √ p(Si  Sj ) = Φ , (1) 2σ vi≡j (t, ) vij (t, ) 1 3 2 0 −1 1 0 −4 −2 0 2 tij = µi − µj −2 4 (a) vij 1.00 1.00 0.75 0.75 0.50 0.25 Si ,Sj ∈{S1N } −4 −2 0 2 4 tij = µi − µj (Herbrich et al., 2006)"
P18-1020,D15-1189,0,0.253399,"systems corresponding to their perceived performance on some task. Many NLP datasets involve eliciting from annotators some graded response. The most popular annotation scheme is the n-ary ordinal approach as illustrated in Figure 1(a). For example, text may be labeled for sentiment as positive, neutral or negative (Wiebe et al., 1999; Pang et al., 2002; Turney, 2002, inter alia); or under political spectrum analysis as liberal, neutral, or conservative (O’Connor et al., 2010; Bamman and Smith, 2015). A response may correspond to a likelihood judgment, e.g., how likely a predicate is factive (Lee et al., 2015), or that some natural language inference may hold (Zhang et al., 2017). Responses may correspond to a notion of semantic similarity, e.g., whether one word can be substituted for another in context (Pavlick et al., 2015), or whether an entire sentence is more or less similar than another (Marelli et al., 2014), and so on. Less common in NLP are system comparisons based on direct human ratings, but an exception includes the annual shared task evaluations of the Conference on Machine Translation (WMT). There, MT practitioners submit system outputs based on a shared set of source sentences, whic"
P18-1020,W15-3001,0,0.0235088,"Missing"
P18-1020,D13-1009,0,0.020712,"e interpreted as mean scalar annotation.6 Given the parameters, the probability that Si is preferred () over Sj is defined as   µi − µ j √ p(Si  Sj ) = Φ , (1) 2σ vi≡j (t, ) vij (t, ) 1 3 2 0 −1 1 0 −4 −2 0 2 tij = µi − µj −2 4 (a) vij 1.00 1.00 0.75 0.75 0.50 0.25 Si ,Sj ∈{S1N } −4 −2 0 2 4 tij = µi − µj (Herbrich et al., 2006) extends the Thurstone model by applying a Bayesian online and active learning framework, allowing for ties. TrueSkill has been used in the Xbox Live online gaming community,7 and has been applied for various NLP tasks, such as question difficulty estimation (Liu et al., 2013), ranking speech quality (Baumann, 2017), and ranking machine translation and grammatical error correction systems with human evaluation (Bojar et al., 2014, 2015; Sakaguchi et al., 2014, 2016) In the same way as the Thurstone model, TrueSkill assumes that scalar values for each instance Si (i.e., skill level for each player in the context of TrueSkill) follow a Gaussian distribution N (µi , σi2 ), where σi is also parameterized as the uncertainty of the scalar value for each instance. Importantly, TrueSkill uses a Bayesian online learning scheme, and the parameters are iteratively updated aft"
P18-1020,marelli-etal-2014-sick,0,0.0278489,"negative (Wiebe et al., 1999; Pang et al., 2002; Turney, 2002, inter alia); or under political spectrum analysis as liberal, neutral, or conservative (O’Connor et al., 2010; Bamman and Smith, 2015). A response may correspond to a likelihood judgment, e.g., how likely a predicate is factive (Lee et al., 2015), or that some natural language inference may hold (Zhang et al., 2017). Responses may correspond to a notion of semantic similarity, e.g., whether one word can be substituted for another in context (Pavlick et al., 2015), or whether an entire sentence is more or less similar than another (Marelli et al., 2014), and so on. Less common in NLP are system comparisons based on direct human ratings, but an exception includes the annual shared task evaluations of the Conference on Machine Translation (WMT). There, MT practitioners submit system outputs based on a shared set of source sentences, which are then judged relative to other system outputs. Various aggregation strategies have been employed over the years to take these relative comparisons and derive competitive rankings between shared task entrants (Callison-Burch et al., 2012; 208 Proceedings of the 56th Annual Meeting of the Association for Com"
P18-1020,P02-1053,0,0.0117754,"rned here with the construction of datasets and evaluation of systems within natural language processing (NLP). Specifically, humans providing responses that are used to derive graded values on natural language contexts, or in the ordering of systems corresponding to their perceived performance on some task. Many NLP datasets involve eliciting from annotators some graded response. The most popular annotation scheme is the n-ary ordinal approach as illustrated in Figure 1(a). For example, text may be labeled for sentiment as positive, neutral or negative (Wiebe et al., 1999; Pang et al., 2002; Turney, 2002, inter alia); or under political spectrum analysis as liberal, neutral, or conservative (O’Connor et al., 2010; Bamman and Smith, 2015). A response may correspond to a likelihood judgment, e.g., how likely a predicate is factive (Lee et al., 2015), or that some natural language inference may hold (Zhang et al., 2017). Responses may correspond to a notion of semantic similarity, e.g., whether one word can be substituted for another in context (Pavlick et al., 2015), or whether an entire sentence is more or less similar than another (Marelli et al., 2014), and so on. Less common in NLP are syst"
P18-1020,N18-2012,0,0.158396,"Missing"
P18-1020,W02-1011,0,0.0240283,"uction We are concerned here with the construction of datasets and evaluation of systems within natural language processing (NLP). Specifically, humans providing responses that are used to derive graded values on natural language contexts, or in the ordering of systems corresponding to their perceived performance on some task. Many NLP datasets involve eliciting from annotators some graded response. The most popular annotation scheme is the n-ary ordinal approach as illustrated in Figure 1(a). For example, text may be labeled for sentiment as positive, neutral or negative (Wiebe et al., 1999; Pang et al., 2002; Turney, 2002, inter alia); or under political spectrum analysis as liberal, neutral, or conservative (O’Connor et al., 2010; Bamman and Smith, 2015). A response may correspond to a likelihood judgment, e.g., how likely a predicate is factive (Lee et al., 2015), or that some natural language inference may hold (Zhang et al., 2017). Responses may correspond to a notion of semantic similarity, e.g., whether one word can be substituted for another in context (Pavlick et al., 2015), or whether an entire sentence is more or less similar than another (Marelli et al., 2014), and so on. Less common i"
P18-1020,P99-1032,0,0.123019,"Missing"
P18-1020,P15-2067,1,0.910202,"Missing"
P18-1020,Q16-1013,1,0.858234,"Missing"
P18-1020,W14-3301,1,0.926319,"Missing"
P19-1009,P13-1023,0,0.223892,"ent 70.2 61.9 64.3 76.3 68.3 71.3 Average Pooling Max Pooling AMR 1.0 AMR 2.0 70.2±0.1 70.0±0.1 76.3±0.1 76.2±0.1 Table 6: S MATCH scores based different pooling functions. Standard deviation is over 3 runs on the test data. 8 Conclusion We proposed an attention-based model for AMR parsing where we introduced a series of novel components into a transductive setting that extend beyond what a typical NMT system would do on this task. Our model achieves the best performance on two AMR corpora. For future work, we would like to extend our model to other semantic parsing tasks (Oepen et al., 2014; Abend and Rappoport, 2013). We are also interested in semantic parsing in cross-lingual settings (Zhang et al., 2018; Damonte and Cohen, 2018). Acknowledgments We thank the anonymous reviewers for their valuable feedback. This work was supported in part by the JHU Human Language Technology Center of Excellence (HLTCOE), and DARPA LORELEI and AIDA. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA or the U"
P19-1009,P13-2131,0,0.587459,"Missing"
P19-1009,S16-1186,0,0.080467,"Missing"
P19-1009,P14-1134,0,0.603011,"ioning their incoming edges. Introduction Abstract Meaning Representation (AMR, Banarescu et al., 2013) parsing is the task of transducing natural language text into AMR, a graphbased formalism used for capturing sentence-level semantics. Challenges in AMR parsing include: (1) its property of reentrancy – the same concept can participate in multiple relations – which leads to graphs in contrast to trees (Wang et al., 2015); (2) the lack of gold alignments between nodes (concepts) in the graph and words in the text which limits attempts to rely on explicit alignments to generate training data (Flanigan et al., 2014; Wang et al., 2015; Damonte et al., 2017; Foland and Martin, 2017; Peng et al., 2017b; Groschwitz et al., 2018; Guo and Lu, 2018); and (3) relatively limited amounts of labeled data (Konstas et al., 2017). Recent attempts to overcome these challenges include: modeling alignments as latent variables (Lyu and Titov, 2018); leveraging external semantic resources (Artzi et al., 2015; Bjerva et al., 2016); data augmentation (Konstas et al., 2017; van Noord and Bos, 2017b); and employing attention-based sequence-to-sequence models (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos,"
P19-1009,Q16-1023,0,0.0320924,"ning time, we obtain the reference list of nodes and their indices using a pre-order traversal over the reference AMR tree. We also evaluate other traversal strategies, and will discuss their difference in Section 7.2. Edge Prediction Given a input sentence w, a node list u, and indices d, we look for the highest scoring parse tree y in the space Y(u) of valid trees over u with the constraint of d. A parse tree y is a set of directed head-modifier edges y = {(ui , uj ) |1 ≤ i, j ≤ m}. In order to make the search tractable, we follow the arcfactored graph-based approach (McDonald et al., 2005; Kiperwasser and Goldberg, 2016), decomposing the score of a tree to the sum of the score of its head-modifier edges: If we consider the AMR tree with indexed nodes as the prediction target, then our approach to parsing is formalized as a two-stage process: node prediction and edge prediction.1 An example of the parsing process is shown in Figure 2. Node Prediction Given a input sentence w = hw1 , ..., wn i, each wi a word in the sentence, our approach sequentially decodes a list of nodes u = hu1 , ..., um i and deterministically assigns their indices d = hd1 , ..., dm i. P (u) = help himself. Node Prediction Task Formalizat"
P19-1009,P17-1014,0,0.423126,"xternal semantic resources,3 and is aligner-free. Grammar-based approaches are represented by Artzi et al. (2015); Peng et al. (2015) who leveraged external semantic resources, and employed CCG-based or SHRG-based grammar induction approaches converting logical forms into AMRs. Pust et al. (2015) recast AMR parsing as a machine translation problem, while also drawing features from external semantic resources. Attention-based parsing with Seq2Seq-style models have been considered (Barzdins and Gosko, 2016; Peng et al., 2017b), but are limited by the relatively small amount of labeled AMR data. Konstas et al. (2017) overcame this by making use of millions of unlabeled data through self-training, while van Noord and Bos (2017b) showed significant gains via a characterlevel Seq2Seq model and a large amount of silverstandard AMR training data. In contrast, our approach supported by extended pointer generator can be effectively trained on the limited amount of labeled AMR data, with no data augmentation. Prediction For node prediction, based on the final probability distribution P (node) (ut ) at each decoding time step, we implement both greedy search and beam search to sequentially decode a node list u and"
P19-1009,P17-1043,0,0.0960249,"Missing"
P19-1009,S16-1180,0,0.0359894,"a coverage P loss to penalize repetitive nodes: covlosst = i min(atsrc [i], covt [i]), where covt is the sum of source attention distributions over all previous decoding time steps: covt = P t−1 t0 t0 =0 asrc . See See et al. (2017) for full details. 4.4 quires no explicit alignments, but implicitly learns a source-side copy mechanism using attention. Transition-based approaches began with Wang et al. (2015, 2016), who incrementally transform dependency parses into AMRs using transitonbased models, which was followed by a line of research, such as Puzikov et al. (2016); Brandt et al. (2016); Goodman et al. (2016); Damonte et al. (2017); Ballesteros and Al-Onaizan (2017); Groschwitz et al. (2018). A pre-trained aligner, e.g. Pourdamghani et al. (2014); Liu et al. (2018), is needed for most parsers to generate training data (e.g., oracles for a transition-based parser). Our approach makes no significant use of external semantic resources,3 and is aligner-free. Grammar-based approaches are represented by Artzi et al. (2015); Peng et al. (2015) who leveraged external semantic resources, and employed CCG-based or SHRG-based grammar induction approaches converting logical forms into AMRs. Pust et al. (2015)"
P19-1009,Q16-1037,0,0.0286727,"Missing"
P19-1009,P18-1170,0,0.112378,"Missing"
P19-1009,D18-1264,0,0.269007,"ecoding time steps: covt = P t−1 t0 t0 =0 asrc . See See et al. (2017) for full details. 4.4 quires no explicit alignments, but implicitly learns a source-side copy mechanism using attention. Transition-based approaches began with Wang et al. (2015, 2016), who incrementally transform dependency parses into AMRs using transitonbased models, which was followed by a line of research, such as Puzikov et al. (2016); Brandt et al. (2016); Goodman et al. (2016); Damonte et al. (2017); Ballesteros and Al-Onaizan (2017); Groschwitz et al. (2018). A pre-trained aligner, e.g. Pourdamghani et al. (2014); Liu et al. (2018), is needed for most parsers to generate training data (e.g., oracles for a transition-based parser). Our approach makes no significant use of external semantic resources,3 and is aligner-free. Grammar-based approaches are represented by Artzi et al. (2015); Peng et al. (2015) who leveraged external semantic resources, and employed CCG-based or SHRG-based grammar induction approaches converting logical forms into AMRs. Pust et al. (2015) recast AMR parsing as a machine translation problem, while also drawing features from external semantic resources. Attention-based parsing with Seq2Seq-style"
P19-1009,P16-1154,0,0.0549117,"work (See et al., 2017) for node prediction. The pointer-generator network was proposed for text summarization, which can copy words from the source text via pointing, while retaining the ability to produce novel words through the generator. The major difference of our extension is that it can copy nodes, not only from the source text, but also from the previously generated nodes on the target side. This target-side pointing is well-suited to our task as nodes we will predict can be copies of other nodes. While there are other pointer/copy networks (Gulcehre et al., 2016; Merity et al., 2016; Gu et al., 2016; Miao and Blunsom, 2016; Nallapati et al., 2016), we found the pointer-generator network very effective at reducing data sparsity in 82 previous node ut−1 (while training, ut−1 is the previous node of the reference node list; at test time it is the previous node emitted by the decoder), and the attentional vector set−1 from the previous step (explained later in this section). sl0 is the concatenation of last encoder hidden states → − ← − from f l and f l respectively. Source attention distribution atsrc is calculated by additive attention (Bahdanau et al., 2014): of generating word-level embe"
P19-1009,D15-1166,0,0.158373,"Missing"
P19-1009,P16-1014,0,0.0153774,"(2018), we extend the pointer-generator network (See et al., 2017) for node prediction. The pointer-generator network was proposed for text summarization, which can copy words from the source text via pointing, while retaining the ability to produce novel words through the generator. The major difference of our extension is that it can copy nodes, not only from the source text, but also from the previously generated nodes on the target side. This target-side pointing is well-suited to our task as nodes we will predict can be copies of other nodes. While there are other pointer/copy networks (Gulcehre et al., 2016; Merity et al., 2016; Gu et al., 2016; Miao and Blunsom, 2016; Nallapati et al., 2016), we found the pointer-generator network very effective at reducing data sparsity in 82 previous node ut−1 (while training, ut−1 is the previous node of the reference node list; at test time it is the previous node emitted by the decoder), and the attentional vector set−1 from the previous step (explained later in this section). sl0 is the concatenation of last encoder hidden states → − ← − from f l and f l respectively. Source attention distribution atsrc is calculated by additive attention (Bahdanau et al."
P19-1009,P18-1037,0,0.555127,"can participate in multiple relations – which leads to graphs in contrast to trees (Wang et al., 2015); (2) the lack of gold alignments between nodes (concepts) in the graph and words in the text which limits attempts to rely on explicit alignments to generate training data (Flanigan et al., 2014; Wang et al., 2015; Damonte et al., 2017; Foland and Martin, 2017; Peng et al., 2017b; Groschwitz et al., 2018; Guo and Lu, 2018); and (3) relatively limited amounts of labeled data (Konstas et al., 2017). Recent attempts to overcome these challenges include: modeling alignments as latent variables (Lyu and Titov, 2018); leveraging external semantic resources (Artzi et al., 2015; Bjerva et al., 2016); data augmentation (Konstas et al., 2017; van Noord and Bos, 2017b); and employing attention-based sequence-to-sequence models (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017b). In this paper, we introduce a different way to handle reentrancy, and propose an attention-based model that treats AMR parsing as sequence-tograph transduction. The proposed model, supported by an extended pointer-generator network, is aligner-free and can be effectively trained with limited amount of labeled AMR"
P19-1009,P14-5010,0,0.00933403,"Missing"
P19-1009,K15-1004,0,0.0416047,"ransform dependency parses into AMRs using transitonbased models, which was followed by a line of research, such as Puzikov et al. (2016); Brandt et al. (2016); Goodman et al. (2016); Damonte et al. (2017); Ballesteros and Al-Onaizan (2017); Groschwitz et al. (2018). A pre-trained aligner, e.g. Pourdamghani et al. (2014); Liu et al. (2018), is needed for most parsers to generate training data (e.g., oracles for a transition-based parser). Our approach makes no significant use of external semantic resources,3 and is aligner-free. Grammar-based approaches are represented by Artzi et al. (2015); Peng et al. (2015) who leveraged external semantic resources, and employed CCG-based or SHRG-based grammar induction approaches converting logical forms into AMRs. Pust et al. (2015) recast AMR parsing as a machine translation problem, while also drawing features from external semantic resources. Attention-based parsing with Seq2Seq-style models have been considered (Barzdins and Gosko, 2016; Peng et al., 2017b), but are limited by the relatively small amount of labeled AMR data. Konstas et al. (2017) overcame this by making use of millions of unlabeled data through self-training, while van Noord and Bos (2017b"
P19-1009,P05-1012,0,0.650777,"f the 57th Annual Meeting of the Association for Computational Linguistics, pages 80–94 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics The ARG1 of “help-01”. While efforts have gone into developing graph-based algorithms for AMR parsing (Chiang et al., 2013; Flanigan et al., 2014), it is more challenging to parse a sentence into an AMR graph rather than a tree as there are efficient off-the-shelf tree-based algorithms, e.g., Chu and Liu (1965); Edmonds (1968). To leverage these tree-based algorithms as well as other structured prediction paradigms (McDonald et al., 2005), we introduce another view of reentrancy. AMR reentrancy is employed when a node participates in multiple semantic relations. We convert an AMR graph into a tree by duplicating nodes that have reentrant relations; that is, whenever a node has a reentrant relation, we make a copy of that node and use the copy to participate in the relation, thereby resulting in a tree. Next, in order to preserve the reentrancy information, we add an extra layer of annotation by assigning an index to each node. Duplicated nodes are assigned the same index as the original node. Figure 1(b) shows a resultant AMR"
P19-1009,E17-1035,0,0.658672,"et al., 2013) parsing is the task of transducing natural language text into AMR, a graphbased formalism used for capturing sentence-level semantics. Challenges in AMR parsing include: (1) its property of reentrancy – the same concept can participate in multiple relations – which leads to graphs in contrast to trees (Wang et al., 2015); (2) the lack of gold alignments between nodes (concepts) in the graph and words in the text which limits attempts to rely on explicit alignments to generate training data (Flanigan et al., 2014; Wang et al., 2015; Damonte et al., 2017; Foland and Martin, 2017; Peng et al., 2017b; Groschwitz et al., 2018; Guo and Lu, 2018); and (3) relatively limited amounts of labeled data (Konstas et al., 2017). Recent attempts to overcome these challenges include: modeling alignments as latent variables (Lyu and Titov, 2018); leveraging external semantic resources (Artzi et al., 2015; Bjerva et al., 2016); data augmentation (Konstas et al., 2017; van Noord and Bos, 2017b); and employing attention-based sequence-to-sequence models (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017b). In this paper, we introduce a different way to handle reentrancy, and propose"
P19-1009,D14-1162,0,0.0825124,"stributions as well as the vocabulary distribution are weighted by these probabilities respectively, and then summed to obtain the final distribution, from which we make our prediction. Best viewed in color. 4 Model AMR parsing, which will be shown in Section 7.2. As depicted in Figure 3, the extended pointergenerator network consists of four major components: an encoder embedding layer, an encoder, a decoder embedding layer, and a decoder. Encoder Embedding Layer This layer converts words in input sentences into vector representations. Each vector is the concatenation of embeddings of GloVe (Pennington et al., 2014), BERT (Devlin et al., 2018), POS (part-of-speech) tags and anonymization indicators, and features learned by a character-level convolutional neural network (CharCNN, Kim et al., 2016). Anonymization indicators are binary indicators that tell the encoder whether the word is an anonymized word. In preprocessing, text spans of named entities in input sentences will be replaced by anonymized tokens (e.g. person, country) to reduce sparsity (see the Appendix for details). Except BERT, all other embeddings are fetched from their corresponding learned embedding lookup tables. BERT takes subword unit"
P19-1009,D16-1031,0,0.0267743,"2017) for node prediction. The pointer-generator network was proposed for text summarization, which can copy words from the source text via pointing, while retaining the ability to produce novel words through the generator. The major difference of our extension is that it can copy nodes, not only from the source text, but also from the previously generated nodes on the target side. This target-side pointing is well-suited to our task as nodes we will predict can be copies of other nodes. While there are other pointer/copy networks (Gulcehre et al., 2016; Merity et al., 2016; Gu et al., 2016; Miao and Blunsom, 2016; Nallapati et al., 2016), we found the pointer-generator network very effective at reducing data sparsity in 82 previous node ut−1 (while training, ut−1 is the previous node of the reference node list; at test time it is the previous node emitted by the decoder), and the attentional vector set−1 from the previous step (explained later in this section). sl0 is the concatenation of last encoder hidden states → − ← − from f l and f l respectively. Source attention distribution atsrc is calculated by additive attention (Bahdanau et al., 2014): of generating word-level embeddings from BERT. BERT E"
P19-1009,D14-1048,0,0.351871,"ibutions over all previous decoding time steps: covt = P t−1 t0 t0 =0 asrc . See See et al. (2017) for full details. 4.4 quires no explicit alignments, but implicitly learns a source-side copy mechanism using attention. Transition-based approaches began with Wang et al. (2015, 2016), who incrementally transform dependency parses into AMRs using transitonbased models, which was followed by a line of research, such as Puzikov et al. (2016); Brandt et al. (2016); Goodman et al. (2016); Damonte et al. (2017); Ballesteros and Al-Onaizan (2017); Groschwitz et al. (2018). A pre-trained aligner, e.g. Pourdamghani et al. (2014); Liu et al. (2018), is needed for most parsers to generate training data (e.g., oracles for a transition-based parser). Our approach makes no significant use of external semantic resources,3 and is aligner-free. Grammar-based approaches are represented by Artzi et al. (2015); Peng et al. (2015) who leveraged external semantic resources, and employed CCG-based or SHRG-based grammar induction approaches converting logical forms into AMRs. Pust et al. (2015) recast AMR parsing as a machine translation problem, while also drawing features from external semantic resources. Attention-based parsing"
P19-1009,K16-1028,0,0.0170043,"on. The pointer-generator network was proposed for text summarization, which can copy words from the source text via pointing, while retaining the ability to produce novel words through the generator. The major difference of our extension is that it can copy nodes, not only from the source text, but also from the previously generated nodes on the target side. This target-side pointing is well-suited to our task as nodes we will predict can be copies of other nodes. While there are other pointer/copy networks (Gulcehre et al., 2016; Merity et al., 2016; Gu et al., 2016; Miao and Blunsom, 2016; Nallapati et al., 2016), we found the pointer-generator network very effective at reducing data sparsity in 82 previous node ut−1 (while training, ut−1 is the previous node of the reference node list; at test time it is the previous node emitted by the decoder), and the attentional vector set−1 from the previous step (explained later in this section). sl0 is the concatenation of last encoder hidden states → − ← − from f l and f l respectively. Source attention distribution atsrc is calculated by additive attention (Bahdanau et al., 2014): of generating word-level embeddings from BERT. BERT Embeddings Average Pooling"
P19-1009,D15-1136,0,0.151725,"dman et al. (2016); Damonte et al. (2017); Ballesteros and Al-Onaizan (2017); Groschwitz et al. (2018). A pre-trained aligner, e.g. Pourdamghani et al. (2014); Liu et al. (2018), is needed for most parsers to generate training data (e.g., oracles for a transition-based parser). Our approach makes no significant use of external semantic resources,3 and is aligner-free. Grammar-based approaches are represented by Artzi et al. (2015); Peng et al. (2015) who leveraged external semantic resources, and employed CCG-based or SHRG-based grammar induction approaches converting logical forms into AMRs. Pust et al. (2015) recast AMR parsing as a machine translation problem, while also drawing features from external semantic resources. Attention-based parsing with Seq2Seq-style models have been considered (Barzdins and Gosko, 2016; Peng et al., 2017b), but are limited by the relatively small amount of labeled AMR data. Konstas et al. (2017) overcame this by making use of millions of unlabeled data through self-training, while van Noord and Bos (2017b) showed significant gains via a characterlevel Seq2Seq model and a large amount of silverstandard AMR training data. In contrast, our approach supported by extende"
P19-1009,P19-1451,0,0.33754,"Missing"
P19-1009,W17-7306,0,0.304179,"Missing"
P19-1009,S16-1178,0,0.0176029,") + log Pk,t (l) + λcovlosst ] 84 covlosst is a coverage P loss to penalize repetitive nodes: covlosst = i min(atsrc [i], covt [i]), where covt is the sum of source attention distributions over all previous decoding time steps: covt = P t−1 t0 t0 =0 asrc . See See et al. (2017) for full details. 4.4 quires no explicit alignments, but implicitly learns a source-side copy mechanism using attention. Transition-based approaches began with Wang et al. (2015, 2016), who incrementally transform dependency parses into AMRs using transitonbased models, which was followed by a line of research, such as Puzikov et al. (2016); Brandt et al. (2016); Goodman et al. (2016); Damonte et al. (2017); Ballesteros and Al-Onaizan (2017); Groschwitz et al. (2018). A pre-trained aligner, e.g. Pourdamghani et al. (2014); Liu et al. (2018), is needed for most parsers to generate training data (e.g., oracles for a transition-based parser). Our approach makes no significant use of external semantic resources,3 and is aligner-free. Grammar-based approaches are represented by Artzi et al. (2015); Peng et al. (2015) who leveraged external semantic resources, and employed CCG-based or SHRG-based grammar induction approaches convertin"
P19-1009,S14-2008,0,0.106925,"lignment Pure Alignment 70.2 61.9 64.3 76.3 68.3 71.3 Average Pooling Max Pooling AMR 1.0 AMR 2.0 70.2±0.1 70.0±0.1 76.3±0.1 76.2±0.1 Table 6: S MATCH scores based different pooling functions. Standard deviation is over 3 runs on the test data. 8 Conclusion We proposed an attention-based model for AMR parsing where we introduced a series of novel components into a transductive setting that extend beyond what a typical NMT system would do on this task. Our model achieves the best performance on two AMR corpora. For future work, we would like to extend our model to other semantic parsing tasks (Oepen et al., 2014; Abend and Rappoport, 2013). We are also interested in semantic parsing in cross-lingual settings (Zhang et al., 2018; Damonte and Cohen, 2018). Acknowledgments We thank the anonymous reviewers for their valuable feedback. This work was supported in part by the JHU Human Language Technology Center of Excellence (HLTCOE), and DARPA LORELEI and AIDA. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or en"
P19-1009,P17-1099,0,0.0359169,"tiple hidden states of BERT. In order to accurately use these hidden states to represent each word, we apply an average pooling function to the outputs of BERT. Figure 4 illustrates the process Our model has two main modules: (1) an extended pointer-generator network for node prediction; and (2) a deep biaffine classifier for edge prediction. The two modules correspond to the two-stage process for AMR parsing, and they are jointly learned during training. 4.1 Extended Pointer-Generator Network Inspired by the self-copy mechanism in Zhang et al. (2018), we extend the pointer-generator network (See et al., 2017) for node prediction. The pointer-generator network was proposed for text summarization, which can copy words from the source text via pointing, while retaining the ability to produce novel words through the generator. The major difference of our extension is that it can copy nodes, not only from the source text, but also from the previously generated nodes on the target side. This target-side pointing is well-suited to our task as nodes we will predict can be copies of other nodes. While there are other pointer/copy networks (Gulcehre et al., 2016; Merity et al., 2016; Gu et al., 2016; Miao a"
P19-1009,P17-1186,0,0.144991,"Missing"
P19-1009,S16-1181,0,0.202215,"Missing"
P19-1009,D17-1129,0,0.450804,"Missing"
P19-1009,N15-1040,0,0.385945,"himself.” (a) A standard AMR graph. (b) An AMR tree with node indices as an extra layer of annotation, where the corresponding graph can be recovered by merging nodes of the same index and unioning their incoming edges. Introduction Abstract Meaning Representation (AMR, Banarescu et al., 2013) parsing is the task of transducing natural language text into AMR, a graphbased formalism used for capturing sentence-level semantics. Challenges in AMR parsing include: (1) its property of reentrancy – the same concept can participate in multiple relations – which leads to graphs in contrast to trees (Wang et al., 2015); (2) the lack of gold alignments between nodes (concepts) in the graph and words in the text which limits attempts to rely on explicit alignments to generate training data (Flanigan et al., 2014; Wang et al., 2015; Damonte et al., 2017; Foland and Martin, 2017; Peng et al., 2017b; Groschwitz et al., 2018; Guo and Lu, 2018); and (3) relatively limited amounts of labeled data (Konstas et al., 2017). Recent attempts to overcome these challenges include: modeling alignments as latent variables (Lyu and Titov, 2018); leveraging external semantic resources (Artzi et al., 2015; Bjerva et al., 2016);"
P19-1009,P15-1095,0,0.110026,"Missing"
P19-1009,D18-1194,1,0.889516,"Missing"
P19-1009,D16-1065,0,0.401352,"X score(ui , uj ) y∈Y(u) (u ,u )∈y i j Based on the scores of the edges, the highest scoring parse tree (i.e., maximum spanning arborescence) can be efficiently found using the Chu-Liu-Edmonnds algorithm. We further incorporate indices as constraints in the algorithm, which is described in Section 4.4. After obtaining the parse tree, we merge identically indexed nodes to recover the standard AMR graph. P (ui |u&lt;i , d&lt;i , w) i=1 Note that we allow the same node to occur multi1 The two-stage process is similar to “concept identification” and “relation identification” in Flanigan et al. (2014); Zhou et al. (2016); Lyu and Titov (2018); inter alia. 81 Final Distribution &lt;latexit sha1_base64=""LBJIGmiRa8y7fNwyIgzxQ26kqoY="">AAAB9XicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi2GPBi8cK9gPaWDbbTbt0dxN2J2oJ/R9ePCji1f/izX/jts1BWx8MPN6bYWZemAhu0PO+ncLa+sbmVnG7tLO7t39QPjxqmTjVlDVpLGLdCYlhgivWRI6CdRLNiAwFa4fj65nffmDa8Fjd4SRhgSRDxSNOCVrpPun3kD2hlpnRdNovV7yqN4e7SvycVCBHo1/+6g1imkqmkApiTNf3EgwyopFTwaalXmpYQuiYDFnXUkUkM0E2v3rqnlll4EaxtqXQnau/JzIijZnI0HZKgiOz7M3E/7xuilEtyLhKUmSKLhZFqXAxdmcRuAOuGUUxsYRQze2tLh0RTSjaoEo2BH/55VXSuqj6XtW/vazUa3kcRTiBUzgHH66gDjfQgCZQ0PAMr/DmPDovzrvzsWgtOPnMMfyB8/kDY1qTCw==&lt;/latexit> &lt;latexit sha1_base64=""R"
P19-1009,P13-1091,0,\N,Missing
P19-1009,D15-1198,0,\N,Missing
P19-1009,W13-2322,0,\N,Missing
P19-1009,S16-1179,0,\N,Missing
P19-1009,S16-1182,0,\N,Missing
P19-1009,N18-1104,0,\N,Missing
P19-1009,P18-2077,0,\N,Missing
P19-1009,D18-1198,0,\N,Missing
P19-1009,E17-1051,0,\N,Missing
P19-1084,D17-1070,0,0.0437902,"the baseline model learned to rely on the presence/absence of the bias term c, always predicting T RUE/FALSE respectively. Table 1 shows the results of our two proposed methods. As we increase the hyper-parameters α and β, our methods initially behave like the baseline, learning the training set but failing on the test set. However, with strong enough hyperparameters (moving towards the bottom in the tables), they perform perfectly on both the biased training set and the unbiased test set. For Method 1, stronger hyper-parameters work better. Baseline & Implementation Details We use InferSent (Conneau et al., 2017) as our baseline model because it has been shown to work well on popular NLI datasets and is representative of many NLI models. We use separate BiLSTM encoders to learn vector representations of P and H.11 The vector representations are combined following Mou et al. (2016),12 and passed to an MLP classifier with one hidden layer. Our proposed 9 Detailed descriptions of these datasets can be found in Poliak et al. (2018b). 10 We leave additional NLI datasets, such as the Diverse NLI Collection (Poliak et al., 2018a), for future work. 11 Many NLI models encode P and H separately (Rockt¨aschel et"
P19-1084,S19-1028,1,0.7647,"Missing"
P19-1084,D15-1075,0,0.462661,"taset B), a letter c is appended to the hypothesis side in the T RUE examples, but not in the FALSE examples. In order to transfer well to the test set, a model that is trained on this training set needs to learn the underlying relationship—that P entails H if and only if their first letter is identical—rather than relying on the presence of c in the hypothesis side. max L1 (θ) = (1 − α) log pθ (y |P, H) θ − α log pθ,φ (y |P 0 , H) max L2 (φ) = β log pθ,φ (y |P 0 , H) Common NLI datasets Moving to existing NLI datasets, we train models on the Stanford Natural Language Inference dataset (SNLI; Bowman et al., 2015), since it is known to contain significant annotation artifacts. We evaluate the robustness of our methods on other, target datasets. As target datasets, we use the 10 datasets investigated by Poliak et al. (2018b) in their hypothesisonly study, plus two test sets: GLUE’s diagnostic test set, which was carefully constructed to not contain hypothesis-biases (Wang et al., 2018), and SNLI-hard, a subset of the SNLI test set that is thought to have fewer biases (Gururangan et al., 2018). The target datasets include humanjudged datasets that used automatic methods to pair premises and hypotheses, a"
P19-1084,C18-1055,0,0.0314895,"ur methods with known biases in NLI datasets, the effects of stronger bias removal, and the possibility of fine-tuning on the target datasets. Our methodology can be extended to handle biases in other tasks where one is concerned with finding relationships between two objects, such as visual question answering, story cloze completion, and reading comprehension. We hope to encourage such investigation in the broader community. Improving model robustness Neural networks are sensitive to adversarial examples, primarily in machine vision, but also in NLP (Jia & Liang, 2017; Belinkov & Bisk, 2018; Ebrahimi et al., 2018; Heigold et al., 2018; Mudrakarta et al., 2018; Ribeiro et al., 2018; Belinkov & Glass, 2019). A common approach to improving robustness is to include adversarial examples in training (Szegedy et al., 2014; Goodfellow et al., 2015). However, this may not generalize well to new types of examples (Xiaoyong Yuan, 2017; Tramr et al., 2018). Domain-adversarial neural networks aim to increase robustness to domain change, by learning to be oblivious to the domain using gradient reversals (Ganin et al., 2016). Our methods rely similarly on gradient reversals when encouraging models to ignore dataset-"
P19-1084,P17-2097,0,0.232016,"ctic clues alone (Snow et al., 2006; Vanderwende & Dolan, 2006). Recent work also found artifacts in new NLI datasets (Tsuchiya, 2018; Gururangan et al., 2018; Poliak et al., 2018b). Other NLU datasets also exhibit biases. In ROC Stories (Mostafazadeh et al., 2016), a story cloze dataset, Schwartz et al. (2017b) obtained a high performance by only considering the candidate endings, without even looking at the story context. In this case, stylistic features of the candidate endings alone, such as the length or certain words, were strong indicators of the correct ending (Schwartz et al., 2017a; Cai et al., 2017). A similar phenomenon was observed in reading comprehension, where systems performed non-trivially well by using only the final sentence in the passage or ignoring the passage altogether (Kaushik & Lipton, 2018). Finally, multiple studies found non-trivial performance in visual question answering (VQA) by using only the question, without access to the image, due to question biases (Zhang et al., 2016; Kafle & Kanan, 2016, 2017; Goyal et al., 2017; Agrawal et al., 2017). Our main goal is to determine whether our methods help a model perform well across multiple datasets by ignoring dataset-spe"
P19-1084,P18-1225,0,0.0192681,"niques developed for textual entailment“ datasets, e.g., RTE-3, do not transfer well to other domains, specifically conversational entailment (Zhang & Chai, 2009, 2010). Bowman et al. (2015) and Williams et al. (2018) demonstrated (specifically in their respective Tables 7 and 4) how models trained on SNLI and MNLI may not transfer well across other NLI datasets like SICK. Talman & Chatzikyriakidis (2018) recently reported similar findings using many advanced deep-learning models. versarial examples that do not conform to logical rules and regularize models based on those examples. Similarly, Kang et al. (2018) incorporate external linguistic resources and use a GAN-style framework to adversarially train robust NLI models. In contrast, we do not use external resources and we are interested in mitigating hypothesisonly biases. Finally, a similar approach has recently been used to mitigate biases in VQA (Ramakrishnan et al., 2018; Grand & Belinkov, 2019). 8 Conclusion Biases in annotations are a major source of concern for the quality of NLI datasets and systems. We presented a solution for combating annotation biases by proposing two training methods to predict the probability of a premise given an e"
P19-1084,D18-1546,0,0.0793546,"exhibit biases. In ROC Stories (Mostafazadeh et al., 2016), a story cloze dataset, Schwartz et al. (2017b) obtained a high performance by only considering the candidate endings, without even looking at the story context. In this case, stylistic features of the candidate endings alone, such as the length or certain words, were strong indicators of the correct ending (Schwartz et al., 2017a; Cai et al., 2017). A similar phenomenon was observed in reading comprehension, where systems performed non-trivially well by using only the final sentence in the passage or ignoring the passage altogether (Kaushik & Lipton, 2018). Finally, multiple studies found non-trivial performance in visual question answering (VQA) by using only the question, without access to the image, due to question biases (Zhang et al., 2016; Kafle & Kanan, 2016, 2017; Goyal et al., 2017; Agrawal et al., 2017). Our main goal is to determine whether our methods help a model perform well across multiple datasets by ignoring dataset-specific artifacts. In turn, we did not update the models’ parameters on other datasets. But, what if we are given different amounts of training data for a new NLI dataset? To determine if our approach is still help"
P19-1084,W19-1801,1,0.837619,"ss other NLI datasets like SICK. Talman & Chatzikyriakidis (2018) recently reported similar findings using many advanced deep-learning models. versarial examples that do not conform to logical rules and regularize models based on those examples. Similarly, Kang et al. (2018) incorporate external linguistic resources and use a GAN-style framework to adversarially train robust NLI models. In contrast, we do not use external resources and we are interested in mitigating hypothesisonly biases. Finally, a similar approach has recently been used to mitigate biases in VQA (Ramakrishnan et al., 2018; Grand & Belinkov, 2019). 8 Conclusion Biases in annotations are a major source of concern for the quality of NLI datasets and systems. We presented a solution for combating annotation biases by proposing two training methods to predict the probability of a premise given an entailment label and a hypothesis. We demonstrated that this discourages the hypothesis encoder from learning the biases to instead obtain a less biased representation. When empirically evaluating our approaches, we found that in a synthetic setting, as well as on a wide-range of existing NLI datasets, our methods perform better than the tradition"
P19-1084,N18-2017,0,0.0783303,"Missing"
P19-1084,I17-1011,0,0.118985,"Missing"
P19-1084,P18-2005,0,0.0390164,"vious to the domain using gradient reversals (Ganin et al., 2016). Our methods rely similarly on gradient reversals when encouraging models to ignore dataset-specific artifacts. One distinction is that domain-adversarial networks require knowledge of the domain at training time, while our methods learn to ignore latent artifacts and do not require direct supervision in the form of a domain label. Others have attempted to remove biases from learned representations, e.g., gender biases in word embeddings (Bolukbasi et al., 2016) or sensitive information like sex and age in text representations (Li et al., 2018). However, removing such attributes from text representations may be difficult (Elazar & Goldberg, 2018). In contrast to this line of work, our final goal is not the removal of such attributes per se; instead, we strive for more robust representations that better transfer to other datasets, similar to Li et al. (2018). Recent work has applied adversarial learning to NLI. Minervini & Riedel (2018) generate adAcknowledgements We would like to thank Aviad Rubinstein and Cynthia Dwork for discussing an earlier version of this work and the anonymous reviewers for their useful comments. Y.B. was sup"
P19-1084,P16-1204,0,0.135588,"Missing"
P19-1084,marelli-etal-2014-sick,0,0.149192,"Missing"
P19-1084,P15-2067,1,0.883981,"Missing"
P19-1084,K18-1007,0,0.0984265,"in label. Others have attempted to remove biases from learned representations, e.g., gender biases in word embeddings (Bolukbasi et al., 2016) or sensitive information like sex and age in text representations (Li et al., 2018). However, removing such attributes from text representations may be difficult (Elazar & Goldberg, 2018). In contrast to this line of work, our final goal is not the removal of such attributes per se; instead, we strive for more robust representations that better transfer to other datasets, similar to Li et al. (2018). Recent work has applied adversarial learning to NLI. Minervini & Riedel (2018) generate adAcknowledgements We would like to thank Aviad Rubinstein and Cynthia Dwork for discussing an earlier version of this work and the anonymous reviewers for their useful comments. Y.B. was supported by the Harvard Mind, Brain, and Behavior Initiative. A.P. and B.V.D were supported by JHU-HLTCOE and DARPA LORELEI. A.M.R gratefully acknowledges the support of NSF 1845664. Views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA or the U.S. Government. 885 References URL http://ww"
P19-1084,D14-1162,0,0.0849444,"Missing"
P19-1084,N16-1098,0,0.0286455,"ises will lead to a model’s degradation. 6.3 7 Fine-tuning on target datasets Related Work Biases and artifacts in NLU datasets Many natural language undersrtanding (NLU) datasets contain annotation artifacts. Early work on NLI, also known as recognizing textual entailment (RTE), found biases that allowed models to perform relatively well by focusing on syntactic clues alone (Snow et al., 2006; Vanderwende & Dolan, 2006). Recent work also found artifacts in new NLI datasets (Tsuchiya, 2018; Gururangan et al., 2018; Poliak et al., 2018b). Other NLU datasets also exhibit biases. In ROC Stories (Mostafazadeh et al., 2016), a story cloze dataset, Schwartz et al. (2017b) obtained a high performance by only considering the candidate endings, without even looking at the story context. In this case, stylistic features of the candidate endings alone, such as the length or certain words, were strong indicators of the correct ending (Schwartz et al., 2017a; Cai et al., 2017). A similar phenomenon was observed in reading comprehension, where systems performed non-trivially well by using only the final sentence in the passage or ignoring the passage altogether (Kaushik & Lipton, 2018). Finally, multiple studies found no"
P19-1084,W18-5441,1,0.895818,"Missing"
P19-1084,P16-2022,0,0.0707933,"ng the training set but failing on the test set. However, with strong enough hyperparameters (moving towards the bottom in the tables), they perform perfectly on both the biased training set and the unbiased test set. For Method 1, stronger hyper-parameters work better. Baseline & Implementation Details We use InferSent (Conneau et al., 2017) as our baseline model because it has been shown to work well on popular NLI datasets and is representative of many NLI models. We use separate BiLSTM encoders to learn vector representations of P and H.11 The vector representations are combined following Mou et al. (2016),12 and passed to an MLP classifier with one hidden layer. Our proposed 9 Detailed descriptions of these datasets can be found in Poliak et al. (2018b). 10 We leave additional NLI datasets, such as the Diverse NLI Collection (Poliak et al., 2018a), for future work. 11 Many NLI models encode P and H separately (Rockt¨aschel et al., 2016; Mou et al., 2016; Liu et al., 2016; Cheng et al., 2016; Chen et al., 2017), although some share information between the encoders via attention (Parikh et al., 2016; Duan et al., 2018). 12 Specifically, representations are concatenated, subtracted, and multiplie"
P19-1084,P18-1176,0,0.0561621,"Missing"
P19-1084,D12-1071,0,0.092754,"are provided in Appendix A.2. For both methods, we sweep hyper-parameters α, β over {0.05, 0.1, 0.2, 0.4, 0.8, 1.0}. For each target dataset, we choose the best-performing model on its development set and report results on the test set.13 sense Inference (JOCI; Zhang et al., 2017), Multiple Premise Entailment (MPE; Lai et al., 2017),and Sentences Involving Compositional Knowledge (SICK; Marelli et al., 2014). The target datasets also include datasets recast by White et al. (2017) to evaluate different semantic phenomena: FrameNet+ (FN+; Pavlick et al., 2015), Definite Pronoun Resolution (DPR; Rahman & Ng, 2012), and Semantic Proto-Roles (SPR; Reisinger et al., 2015).9 As many of these datasets have different label spaces than SNLI, we define a mapping (Appendix A.1) from our models’ predictions to each target dataset’s labels. Finally, we also test on the Multi-genre NLI dataset (MNLI; Williams et al., 2018), a successor to SNLI.10 5 Results 5.1 Synthetic Experiments To examine how well our methods work in a controlled setup, we train on the biased dataset (B), but evaluate on the unbiased test set (A). As expected, without a method to remove hypothesisonly biases, the baseline fails to generalize t"
P19-1084,D16-1244,0,0.170648,"Missing"
P19-1084,Q15-1034,1,0.916901,"Missing"
P19-1084,P18-1079,0,0.45999,"sentations of P and H, and a classification layer, gθ , which learns a distribution over y. Typically, this is done by maximizing this discriminative likelihood directly, which will act as our baseline (Figure 1a). However, many NLI datasets contain biases that allow models to perform non-trivially well when accessing just the hypotheses (Tsuchiya, 2018; Gururangan et al., 2018; Poliak et al., 2018b). This allows models to leverage hypothesis-only biases that may be present in a dataset. A model may perform well on a specific dataset, without identifying whether P entails H. Gururangan et al. (2018) argue that “the bulk” of many models’ “success [is] attribute[d] to the easy examples”. Consequently, this may limit how well a model trained on one dataset would perform on other datasets that may have different artifacts. Consider an example where P and H are strings from {a, b, c}, and an environment where P enIn summary, in this paper we make the follow878 tails H if and only if the first letters are the same, as in synthetic dataset A. In such a setting, a model should be able to learn the correct condition for P to entail H.4 3.1 Our first approach is to estimate the term p(y |H) direct"
P19-1084,L18-1239,0,0.442811,"se for Granted: Mitigating Artifacts in Natural Language Inference Yonatan Belinkov13∗ Adam Poliak2∗ Stuart M. Shieber1 Benjamin Van Durme2 Alexander M. Rush1 1 2 3 Harvard University Johns Hopkins University Massachusetts Institute of Technology {belinkov,shieber,srush}@seas.harvard.edu {azpoliak,vandurme}@cs.jhu.edu Abstract many NLI datasets contain biases, or annotation artifacts, i.e., features present in hypotheses that enable models to perform surprisingly well using only the hypothesis, without learning the relationship between two texts (Gururangan et al., 2018; Poliak et al., 2018b; Tsuchiya, 2018).3 For instance, in some datasets, negation words like “not” and “nobody” are often associated with a relationship of contradiction. As a ramification of such biases, models may not generalize well to other datasets that contain different or no such biases. Recent studies have tried to create new NLI datasets that do not contain such artifacts, but many approaches to dealing with this issue remain unsatisfactory: constructing new datasets (Sharma et al., 2018) is costly and may still result in other artifacts; filtering “easy” examples and defining a harder subset is useful for evaluation purp"
P19-1084,W18-5446,0,0.0389124,"= (1 − α) log pθ (y |P, H) θ − α log pθ,φ (y |P 0 , H) max L2 (φ) = β log pθ,φ (y |P 0 , H) Common NLI datasets Moving to existing NLI datasets, we train models on the Stanford Natural Language Inference dataset (SNLI; Bowman et al., 2015), since it is known to contain significant annotation artifacts. We evaluate the robustness of our methods on other, target datasets. As target datasets, we use the 10 datasets investigated by Poliak et al. (2018b) in their hypothesisonly study, plus two test sets: GLUE’s diagnostic test set, which was carefully constructed to not contain hypothesis-biases (Wang et al., 2018), and SNLI-hard, a subset of the SNLI test set that is thought to have fewer biases (Gururangan et al., 2018). The target datasets include humanjudged datasets that used automatic methods to pair premises and hypotheses, and then relied on humans to label the pairs: SCITAIL (Khot et al., 2018), ADD-ONE-RTE (Pavlick & CallisonBurch, 2016), Johns Hopkins Ordinal Commonφ Finally, we share the classifier weights between pθ (y |P, H) and pφ,θ (y |P 0 , H). In a sense this is counter-intuitive, since pθ is being trained to unlearn bias, while pφ,θ is being trained to learn it. However, if the models"
P19-1084,K17-1004,0,0.222659,"-tuning on target datasets Related Work Biases and artifacts in NLU datasets Many natural language undersrtanding (NLU) datasets contain annotation artifacts. Early work on NLI, also known as recognizing textual entailment (RTE), found biases that allowed models to perform relatively well by focusing on syntactic clues alone (Snow et al., 2006; Vanderwende & Dolan, 2006). Recent work also found artifacts in new NLI datasets (Tsuchiya, 2018; Gururangan et al., 2018; Poliak et al., 2018b). Other NLU datasets also exhibit biases. In ROC Stories (Mostafazadeh et al., 2016), a story cloze dataset, Schwartz et al. (2017b) obtained a high performance by only considering the candidate endings, without even looking at the story context. In this case, stylistic features of the candidate endings alone, such as the length or certain words, were strong indicators of the correct ending (Schwartz et al., 2017a; Cai et al., 2017). A similar phenomenon was observed in reading comprehension, where systems performed non-trivially well by using only the final sentence in the passage or ignoring the passage altogether (Kaushik & Lipton, 2018). Finally, multiple studies found non-trivial performance in visual question answe"
P19-1084,I17-1100,1,0.922317,"Missing"
P19-1084,W17-0907,0,0.0297645,"Missing"
P19-1084,N18-1101,0,0.316511,"Entailment (MPE; Lai et al., 2017),and Sentences Involving Compositional Knowledge (SICK; Marelli et al., 2014). The target datasets also include datasets recast by White et al. (2017) to evaluate different semantic phenomena: FrameNet+ (FN+; Pavlick et al., 2015), Definite Pronoun Resolution (DPR; Rahman & Ng, 2012), and Semantic Proto-Roles (SPR; Reisinger et al., 2015).9 As many of these datasets have different label spaces than SNLI, we define a mapping (Appendix A.1) from our models’ predictions to each target dataset’s labels. Finally, we also test on the Multi-genre NLI dataset (MNLI; Williams et al., 2018), a successor to SNLI.10 5 Results 5.1 Synthetic Experiments To examine how well our methods work in a controlled setup, we train on the biased dataset (B), but evaluate on the unbiased test set (A). As expected, without a method to remove hypothesisonly biases, the baseline fails to generalize to the test set. Examining its predictions, we found that the baseline model learned to rely on the presence/absence of the bias term c, always predicting T RUE/FALSE respectively. Table 1 shows the results of our two proposed methods. As we increase the hyper-parameters α and β, our methods initially b"
P19-1084,N06-1005,0,0.725739,"Missing"
P19-1084,D10-1074,0,0.0834512,"Missing"
P19-1084,W09-3930,0,0.0870273,"Missing"
P19-1084,Q17-1027,1,0.9051,"Missing"
P19-1084,D16-1053,0,\N,Missing
P19-1084,P17-1152,0,\N,Missing
P19-1084,D18-1007,1,\N,Missing
P19-1084,Q19-1004,1,\N,Missing
P19-1280,W13-1202,0,0.0604725,"Missing"
P19-1280,Q19-1035,1,0.88046,"Missing"
P19-1280,W11-0116,0,0.295134,"convolutional architectures (Dligach et al., 2017). Such models have furthermore been used to construct document timelines from a set of predicted temporal relations (Leeuwenberg and Moens, 2018). Such use of pairwise annotations can result in inconsistent temporal graphs, and efforts have been made to avert this issue by employing temporal reasoning (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; Laokulrat et al., 2016; Ning et al., 2017; Leeuwenberg and Moens, 2017). Other work has aimed at modeling event durations from text (Pan et al., 2007; Gusev et al., 2011; Williams and Katz, 2012), though this work does not tie duration to temporal relations (see also Filatova and Hovy, 2001). Our approach combines duration and temporal relation information within a unified framework, discussed below. 3 Data Collection We collect the Universal Decompositional Semantics Time (UDS-T) dataset, which is annotated on top of the Universal Dependencies (Silveira et al., 2014; De Marneffe et al., 2014; Nivre et al., 2015) 2907 Dataset TimeBank TempEval 2010 TempEval 2013 TimeBank-Dense Hong et al. (2016) UDS-T #Events #Event-Event Relations 7,935 5,688 11,145 1,729 86"
P19-1280,J87-3004,0,0.454801,"le difficulty drawing a timeline for these events, likely producing something like that in Figure 1. But how do we know that the breaking, the running away, the confrontation, and the calling were short, while the parents being at work was not? And why should the first four be in sequence, with the last containing the others? The answers to these questions likely involve a complex interplay between linguistic information, on the one hand, and common sense knowledge about events and their relationships, on the other (Minsky, 1975; Schank and Abelson, 1975; Lamport, 1978; Allen and Hayes, 1985; Hobbs et al., 1987; Hwang and Schubert, 1994). But it remains a question how best to capture this interaction. A promising line of attack lies in the task of temporal relation extraction. Prior work in this domain has approached this task as a classification problem, labeling pairs of eventreferring expressions—e.g. broke or be at work in (1)—and time-referring expressions—e.g. 3pm or two hours—with categorical temporal relations (Pustejovsky et al., 2003; Styler IV et al., 2014; Minard et al., 2016). The downside of this approach is that time-referring expressions must be relied upon to express duration inform"
P19-1280,W16-1701,0,0.085996,"Missing"
P19-1280,D15-1189,0,0.0339518,".96 Table 2: Results on test data based on different model representations; ⇢ denotes the Spearman-correlation coefficient; rank-diff is the duration rank difference. The model highlighted in blue performs best on durations and is also close to the top performing model for relations on the development set. The numbers highlighted in bold are the best-performing numbers on the test data in the respective columns. kernel to train on the training sets of these datasets using the feature vector obtained from our model.3 Following recent work using continuous labels in event factuality prediction (Lee et al., 2015; Stanovsky et al., 2017; Rudinger et al., 2018; White et al., 2018) and genericity prediction (Govindarajan et al., 2019), we report three metrics for the duration prediction: Spearman correlation (⇢), mean rank difference (rank diff ), and proportion rank difference explained (R1). We report three metrics for the relation prediction: Spearman correlation between the normalized values of actual beginning and end points and the predicted ones (absolute ⇢), the Spearman correlation between the actual and predicted values in Lrel (relative ⇢), and the proportion of MAE explained (R1). R1 = 1 MAE"
P19-1280,D18-1155,0,0.249741,"sed architectures— e.g. CAEVO (Chambers et al., 2014) and CATENA (Mirza and Tonelli, 2016). Recently, Ning et al. (2017) use a structured learning approach and show significant improvements on both TempEval-3 (UzZaman et al., 2013) and TimeBank-Dense (Cassidy et al., 2014). Ning et al. (2018) show further improvements on TimeBank-Dense by jointly modeling causal and temporal relations using Constrained Conditional Models and formulating the problem as an Interger Linear Programming problem. Neural network-based approaches have used both recurrent (Tourille et al., 2017; Cheng and Miyao, 2017; Leeuwenberg and Moens, 2018) and convolutional architectures (Dligach et al., 2017). Such models have furthermore been used to construct document timelines from a set of predicted temporal relations (Leeuwenberg and Moens, 2018). Such use of pairwise annotations can result in inconsistent temporal graphs, and efforts have been made to avert this issue by employing temporal reasoning (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; Laokulrat et al., 2016; Ning et al., 2017; Leeuwenberg and Moens, 2017). Other work has aimed at modeling event durations from text (Pan et al., 20"
P19-1280,E17-1108,0,0.137319,"Missing"
P19-1280,D15-1166,0,0.00738719,"the k th predicate is defined similarly to the event representation, but instead of stacking the predicate’s span, we stack the hidden representations of the entire sentence H. SENT SENT adur = tanh ASENT DUR gpredk + b DUR k SENT ↵durk = softmax(Hadur ) k What to feed my Tuner…. dog been sick for …. What to feed my ELMo…. dog been sick for …. What to feed my dog …. been sick for …. gdurk = [gpredk ; ↵durk H] Figure 6: Network diagram for model. Dashed arrows are only included in some models. duration model, and a relation model. These components use multiple layers of dot product attention (Luong et al., 2015) on top of an embedding H 2 RN ⇥D for a sentence s = [w1 , . . . , wN ] tuned on the three M -dimensional contextual embeddings produced by ELMo (Peters et al., 2018) for that sentence, concatenated together. H = tanh (ELMo(s)W TUNE + bTUNE ) where D is the dimension for the tuned embeddings, W TUNE 2 R3M ⇥D , and bTUNE 2 RN ⇥D . Event model We define the model’s representation for the event referred to by predicate k as gpredk 2 RD , where D is the embedding size. We build this representation using a variant of dotproduct attention, based on the predicate root. SPAN SPAN apred = tanh ASPAN PR"
P19-1280,P06-1095,0,0.125067,"a multi-stage annotation pipeline where various event-event phenomena, including temporal relations and sub1 Data and code are available at http://decomp.io/. event relations are annotated together in the same datasets (O’Gorman et al., 2016). Similarly, Hong et al. (2016) build a cross-document event corpus which covers fine-grained event-event relations and roles with more number of event types and sub-types (see also Fokkens et al., 2013). Models Early systems for temporal relation extraction use hand-tagged features modeled with multinomial logistic regression and support vector machines (Mani et al., 2006; Bethard, 2013; Lin et al., 2015). Other approaches use combined rulebased and learning-based approaches (D’Souza and Ng, 2013) and sieve-based architectures— e.g. CAEVO (Chambers et al., 2014) and CATENA (Mirza and Tonelli, 2016). Recently, Ning et al. (2017) use a structured learning approach and show significant improvements on both TempEval-3 (UzZaman et al., 2013) and TimeBank-Dense (Cassidy et al., 2014). Ning et al. (2018) show further improvements on TimeBank-Dense by jointly modeling causal and temporal relations using Constrained Conditional Models and formulating the problem as an"
P19-1280,P14-5010,0,0.00530329,"Missing"
P19-1280,L16-1699,0,0.073726,"Missing"
P19-1280,C16-1007,0,0.0289809,"rman et al., 2016). Similarly, Hong et al. (2016) build a cross-document event corpus which covers fine-grained event-event relations and roles with more number of event types and sub-types (see also Fokkens et al., 2013). Models Early systems for temporal relation extraction use hand-tagged features modeled with multinomial logistic regression and support vector machines (Mani et al., 2006; Bethard, 2013; Lin et al., 2015). Other approaches use combined rulebased and learning-based approaches (D’Souza and Ng, 2013) and sieve-based architectures— e.g. CAEVO (Chambers et al., 2014) and CATENA (Mirza and Tonelli, 2016). Recently, Ning et al. (2017) use a structured learning approach and show significant improvements on both TempEval-3 (UzZaman et al., 2013) and TimeBank-Dense (Cassidy et al., 2014). Ning et al. (2018) show further improvements on TimeBank-Dense by jointly modeling causal and temporal relations using Constrained Conditional Models and formulating the problem as an Interger Linear Programming problem. Neural network-based approaches have used both recurrent (Tourille et al., 2017; Cheng and Miyao, 2017; Leeuwenberg and Moens, 2018) and convolutional architectures (Dligach et al., 2017). Such"
P19-1280,D17-1108,0,0.560671,"et al. (2016) build a cross-document event corpus which covers fine-grained event-event relations and roles with more number of event types and sub-types (see also Fokkens et al., 2013). Models Early systems for temporal relation extraction use hand-tagged features modeled with multinomial logistic regression and support vector machines (Mani et al., 2006; Bethard, 2013; Lin et al., 2015). Other approaches use combined rulebased and learning-based approaches (D’Souza and Ng, 2013) and sieve-based architectures— e.g. CAEVO (Chambers et al., 2014) and CATENA (Mirza and Tonelli, 2016). Recently, Ning et al. (2017) use a structured learning approach and show significant improvements on both TempEval-3 (UzZaman et al., 2013) and TimeBank-Dense (Cassidy et al., 2014). Ning et al. (2018) show further improvements on TimeBank-Dense by jointly modeling causal and temporal relations using Constrained Conditional Models and formulating the problem as an Interger Linear Programming problem. Neural network-based approaches have used both recurrent (Tourille et al., 2017; Cheng and Miyao, 2017; Leeuwenberg and Moens, 2018) and convolutional architectures (Dligach et al., 2017). Such models have furthermore been u"
P19-1280,P18-1212,0,0.10038,"Missing"
P19-1280,W16-5706,0,0.244604,"Missing"
P19-1280,N18-1202,0,0.06222,"sentence H. SENT SENT adur = tanh ASENT DUR gpredk + b DUR k SENT ↵durk = softmax(Hadur ) k What to feed my Tuner…. dog been sick for …. What to feed my ELMo…. dog been sick for …. What to feed my dog …. been sick for …. gdurk = [gpredk ; ↵durk H] Figure 6: Network diagram for model. Dashed arrows are only included in some models. duration model, and a relation model. These components use multiple layers of dot product attention (Luong et al., 2015) on top of an embedding H 2 RN ⇥D for a sentence s = [w1 , . . . , wN ] tuned on the three M -dimensional contextual embeddings produced by ELMo (Peters et al., 2018) for that sentence, concatenated together. H = tanh (ELMo(s)W TUNE + bTUNE ) where D is the dimension for the tuned embeddings, W TUNE 2 R3M ⇥D , and bTUNE 2 RN ⇥D . Event model We define the model’s representation for the event referred to by predicate k as gpredk 2 RD , where D is the embedding size. We build this representation using a variant of dotproduct attention, based on the predicate root. SPAN SPAN apred = tanh ASPAN PRED h ROOT (predk ) + b PRED k SPAN ↵predk = softmax HSPAN(predk ) apred k gpredk = [hROOT(predk ) ; ↵predk HSPAN(predk ) ] D⇥D , bSPAN 2 RD ; h where ASPAN ROOT (pred"
P19-1280,N18-1067,1,0.887358,"Missing"
P19-1280,silveira-etal-2014-gold,0,0.100932,"Missing"
P19-1280,P17-2056,0,0.0391647,"ts on test data based on different model representations; ⇢ denotes the Spearman-correlation coefficient; rank-diff is the duration rank difference. The model highlighted in blue performs best on durations and is also close to the top performing model for relations on the development set. The numbers highlighted in bold are the best-performing numbers on the test data in the respective columns. kernel to train on the training sets of these datasets using the feature vector obtained from our model.3 Following recent work using continuous labels in event factuality prediction (Lee et al., 2015; Stanovsky et al., 2017; Rudinger et al., 2018; White et al., 2018) and genericity prediction (Govindarajan et al., 2019), we report three metrics for the duration prediction: Spearman correlation (⇢), mean rank difference (rank diff ), and proportion rank difference explained (R1). We report three metrics for the relation prediction: Spearman correlation between the normalized values of actual beginning and end points and the predicted ones (absolute ⇢), the Spearman correlation between the actual and predicted values in Lrel (relative ⇢), and the proportion of MAE explained (R1). R1 = 1 MAEmodel MAEbaseline where"
P19-1280,S13-2001,0,0.684853,"mporal relation datasets use the TimeML standard (Pustejovsky et al., 2003; Styler IV et al., 2014; Minard et al., 2016). TimeBank is one of the earliest large corpora built using this standard, aimed at capturing ‘salient’ temporal relations between events (Pustejovsky et al., 2003). The TempEval competitions build on TimeBank by covering relations between all the events and times in a sentence. Inter-sentential relations, which are necessary for document-level reasoning, have not been a focus of the TempEval tasks, though at least one sub-task does address them (Verhagen et al., 2007, 2010; UzZaman et al., 2013, and see Chambers et al. 2014). Part of this likely has to do with the sparsity inherent in the TempEval event-graphs. This sparsity has been addressed with corpora such as the TimeBank-Dense, where annotators label all local-edges irrespective of ambiguity (Cassidy et al., 2014). TimeBank-Dense does not capture the complete graph over event and time relations, instead attempting to achieve completeness by capturing all relations both within a sentence and between neighboring sentences. We take inspiration from this work for our own framework. This line of work has been further improved on by"
P19-1280,S07-1014,0,0.146124,"ystems. Corpora Most large temporal relation datasets use the TimeML standard (Pustejovsky et al., 2003; Styler IV et al., 2014; Minard et al., 2016). TimeBank is one of the earliest large corpora built using this standard, aimed at capturing ‘salient’ temporal relations between events (Pustejovsky et al., 2003). The TempEval competitions build on TimeBank by covering relations between all the events and times in a sentence. Inter-sentential relations, which are necessary for document-level reasoning, have not been a focus of the TempEval tasks, though at least one sub-task does address them (Verhagen et al., 2007, 2010; UzZaman et al., 2013, and see Chambers et al. 2014). Part of this likely has to do with the sparsity inherent in the TempEval event-graphs. This sparsity has been addressed with corpora such as the TimeBank-Dense, where annotators label all local-edges irrespective of ambiguity (Cassidy et al., 2014). TimeBank-Dense does not capture the complete graph over event and time relations, instead attempting to achieve completeness by capturing all relations both within a sentence and between neighboring sentences. We take inspiration from this work for our own framework. This line of work has"
P19-1280,D16-1177,1,0.918076,"Missing"
P19-1280,D18-1501,1,0.893994,"Missing"
P19-1280,P12-2044,0,0.0920866,"tectures (Dligach et al., 2017). Such models have furthermore been used to construct document timelines from a set of predicted temporal relations (Leeuwenberg and Moens, 2018). Such use of pairwise annotations can result in inconsistent temporal graphs, and efforts have been made to avert this issue by employing temporal reasoning (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; Laokulrat et al., 2016; Ning et al., 2017; Leeuwenberg and Moens, 2017). Other work has aimed at modeling event durations from text (Pan et al., 2007; Gusev et al., 2011; Williams and Katz, 2012), though this work does not tie duration to temporal relations (see also Filatova and Hovy, 2001). Our approach combines duration and temporal relation information within a unified framework, discussed below. 3 Data Collection We collect the Universal Decompositional Semantics Time (UDS-T) dataset, which is annotated on top of the Universal Dependencies (Silveira et al., 2014; De Marneffe et al., 2014; Nivre et al., 2015) 2907 Dataset TimeBank TempEval 2010 TempEval 2013 TimeBank-Dense Hong et al. (2016) UDS-T #Events #Event-Event Relations 7,935 5,688 11,145 1,729 863 32,302 3,481 3,308 5,272"
P19-1280,P09-1046,0,0.0372159,"l Models and formulating the problem as an Interger Linear Programming problem. Neural network-based approaches have used both recurrent (Tourille et al., 2017; Cheng and Miyao, 2017; Leeuwenberg and Moens, 2018) and convolutional architectures (Dligach et al., 2017). Such models have furthermore been used to construct document timelines from a set of predicted temporal relations (Leeuwenberg and Moens, 2018). Such use of pairwise annotations can result in inconsistent temporal graphs, and efforts have been made to avert this issue by employing temporal reasoning (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; Laokulrat et al., 2016; Ning et al., 2017; Leeuwenberg and Moens, 2017). Other work has aimed at modeling event durations from text (Pan et al., 2007; Gusev et al., 2011; Williams and Katz, 2012), though this work does not tie duration to temporal relations (see also Filatova and Hovy, 2001). Our approach combines duration and temporal relation information within a unified framework, discussed below. 3 Data Collection We collect the Universal Decompositional Semantics Time (UDS-T) dataset, which is annotated on top of the Universal Dependencies (Silve"
P19-1280,P17-2035,0,0.127753,"approaches (D’Souza and Ng, 2013) and sieve-based architectures— e.g. CAEVO (Chambers et al., 2014) and CATENA (Mirza and Tonelli, 2016). Recently, Ning et al. (2017) use a structured learning approach and show significant improvements on both TempEval-3 (UzZaman et al., 2013) and TimeBank-Dense (Cassidy et al., 2014). Ning et al. (2018) show further improvements on TimeBank-Dense by jointly modeling causal and temporal relations using Constrained Conditional Models and formulating the problem as an Interger Linear Programming problem. Neural network-based approaches have used both recurrent (Tourille et al., 2017; Cheng and Miyao, 2017; Leeuwenberg and Moens, 2018) and convolutional architectures (Dligach et al., 2017). Such models have furthermore been used to construct document timelines from a set of predicted temporal relations (Leeuwenberg and Moens, 2018). Such use of pairwise annotations can result in inconsistent temporal graphs, and efforts have been made to avert this issue by employing temporal reasoning (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; Laokulrat et al., 2016; Ning et al., 2017; Leeuwenberg and Moens, 2017). Other work has aimed"
P19-1280,W17-6944,1,0.838829,"Missing"
P19-1280,de-marneffe-etal-2014-universal,0,\N,Missing
P19-1280,D12-1062,0,\N,Missing
P19-1280,W01-1313,0,\N,Missing
P19-1280,N13-1112,0,\N,Missing
P19-1280,S10-1010,0,\N,Missing
P19-1280,Q14-1022,0,\N,Missing
P19-1280,P14-2082,0,\N,Missing
P19-1280,S13-2002,0,\N,Missing
P19-1280,E17-2118,0,\N,Missing
P19-1280,P17-2001,0,\N,Missing
P19-1280,D08-1073,0,\N,Missing
P19-1439,E17-2026,0,0.0800118,"ng and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically investigate transfer performance with limited amounts of data and find similar evidence of catastrophic forgetting. 4466 Multitask representation learning in NLP is well studied, and again can be traced back at least as far as Collobert et al. (2011). Luong et al. (2016) show promising results combining translation and parsing; Subramanian et al. (2018) benefit from multitask learning in sentence-to-vector encoding; and Bingel and Søgaard (2017) and Changpinyo et al. (2018) offer studies of when multitask learning is helpful for lower-level NLP tasks. 3 Task |Train| Task Type GLUE Tasks CoLA SST MRPC QQP STS MNLI QNLI RTE WNLI 8.5K 67K 3.7K 364K 7K 393K 105K 2.5K 634 acceptability sentiment paraphrase detection paraphrase detection sentence similarity NLI QA (NLI) NLI coreference resolution (NLI) Outside Tasks Transfer Paradigms DisSent WT LM WT LM BWB MT En-De MT En-Ru Reddit SkipThought We consider two recent paradigms for transfer learning: pretraining and intermediate training. See Figure 1 for a graphical depiction. Pretraining"
P19-1439,S17-2001,0,0.0228104,"line is especially strong because our ELMo-style models use a skip connection from the input of the encoder to the output, allowing the task-specific component to see the input representations, yielding a model similar to Iyyer et al. (2015). GLUE Tasks We use the nine tasks included with GLUE as pretraining and intermediate tasks: acceptability classification with CoLA (Warstadt et al., 2018); binary sentiment classification with SST (Socher et al., 2013); semantic similarity with the MSR Paraphrase Corpus (MRPC; Dolan and Brockett, 2005), Quora Question Pairs1 (QQP), and STS-Benchmark (STS; Cer et al., 2017); and textual entailment with the Multi-Genre NLI Corpus (MNLI Williams et al., 2018), RTE 1, 2, 3, and 5 (RTE; Dagan et al., 2006, et seq.), and data from SQuAD (QNLI;2 Rajpurkar et al., 2016) and the Winograd Schema Challenge (WNLI; Levesque et al., 2011) recast as entailment in the style of White et al. (2017). MNLI and QQP have previously been shown to be effective for pretraining in other settings (Conneau et al., 2017; Subramanian et al., 2018; Phang et al., 2018). Other tasks are included to represent a broad sample of labeling schemes commonly used in NLP. Outside Tasks We train langua"
P19-1439,C18-1251,0,0.0459627,"Missing"
P19-1439,D14-1082,0,0.0210975,"Missing"
P19-1439,D17-1070,0,0.20309,"bservations suggest that while scaling up LM pretraining (as in Radford et al., 2019) is likely the most straightforward path to further gains, our current methods for multitask and transfer learning may be substantially limiting our results. 2 Related Work Work on reusable sentence encoders can be traced back at least as far as the multitask model of Collobert et al. (2011). Several works focused on learning reusable sentence-to-vector encodings, where the pretrained encoder produces a fixed-size representation for each input sentence (Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017). More recent reusable sentence encoders such as CoVe (McCann et al., 2017) and GPT (Radford et al., 2018) instead represent sentences as sequences of vectors. These methods work well, but most use distinct pretraining objectives, and none offers a substantial investigation of the choice of objective like we conduct here. We build on two methods for pretraining sentence encoders on language modeling: ELMo and BERT. ELMo consists of a forward and backward LSTM (Hochreiter and Schmidhuber, 1997), the hidden states of which are used to produce a contextual vector representation for each token in"
P19-1439,N19-1423,0,0.616052,"ond Language Modeling Alex Wang,∗1 Jan Hula,2 Patrick Xia,2 Raghavendra Pappagari,2 R. Thomas McCoy,2 , Roma Patel,3 Najoung Kim,2 Ian Tenney,4 Yinghui Huang,6 Katherin Yu,5 Shuning Jin,7 Berlin Chen8 Benjamin Van Durme,2 Edouard Grave,5 Ellie Pavlick,3,4 and Samuel R. Bowman1 1 New York University, 2 Johns Hopkins University, 3 Brown University, 4 Google AI Language, 5 Facebook, 6 IBM, 7 University of Minnesota Duluth, 8 Swarthmore College Abstract Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo’s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target tasks. In addition, fine-tuning"
P19-1439,I05-5002,0,0.0294281,"a baseline of a randomly initialized BiLSTM with no further training. This baseline is especially strong because our ELMo-style models use a skip connection from the input of the encoder to the output, allowing the task-specific component to see the input representations, yielding a model similar to Iyyer et al. (2015). GLUE Tasks We use the nine tasks included with GLUE as pretraining and intermediate tasks: acceptability classification with CoLA (Warstadt et al., 2018); binary sentiment classification with SST (Socher et al., 2013); semantic similarity with the MSR Paraphrase Corpus (MRPC; Dolan and Brockett, 2005), Quora Question Pairs1 (QQP), and STS-Benchmark (STS; Cer et al., 2017); and textual entailment with the Multi-Genre NLI Corpus (MNLI Williams et al., 2018), RTE 1, 2, 3, and 5 (RTE; Dagan et al., 2006, et seq.), and data from SQuAD (QNLI;2 Rajpurkar et al., 2016) and the Winograd Schema Challenge (WNLI; Levesque et al., 2011) recast as entailment in the style of White et al. (2017). MNLI and QQP have previously been shown to be effective for pretraining in other settings (Conneau et al., 2017; Subramanian et al., 2018; Phang et al., 2018). Other tasks are included to represent a broad sample"
P19-1439,N16-1162,0,0.0469202,"Missing"
P19-1439,J07-3004,0,0.0375747,"Missing"
P19-1439,P18-1031,0,0.0247575,"(left), and learn a target task model on top of the representations it produces (right). Middle (intermediate ELMo training): We train a BiLSTM on top of ELMo for an intermediate task (left). We then train a target task model on top of the intermediate task BiLSTM and ELMo (right). Bottom (intermediate BERT training): We fine-tune BERT on an intermediate task (left), and then fine-tune the resulting model again on a target task (right). limitation has prompted interest in pretraining for these encoders: The encoders are first trained on outside data, and then plugged into a target task model. Howard and Ruder (2018), Peters et al. (2018a), Radford et al. (2018), and Devlin et al. (2019) establish that encoders pretrained on variants of the language modeling task can be reused to yield strong performance on downstream NLP tasks. Subsequent work has homed in on language modeling (LM) pretraining, finding that such mod4465 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4465–4476 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics els can be productively fine-tuned on intermediate tasks like natural language inference bef"
P19-1439,P15-1162,0,0.0816432,"Missing"
P19-1439,N18-1038,0,0.0431129,"Missing"
P19-1439,P19-1441,0,0.176928,"sed to produce a contextual vector representation for each token in the inputted sequence. ELMo is adapted to target tasks by freezing the model weights and only learning a set of task-specific scalar weights that are used to compute a linear combination of the LSTM layers. BERT consists of a pretrained Transformer (Vaswani et al., 2017), and is adapted to downstream tasks by fine-tuning the entire model. Follow-up work has explored parameterefficient fine-tuning (Stickland and Murray, 2019; Houlsby et al., 2019) and better target task adaptation via multitask fine-tuning (Phang et al., 2018; Liu et al., 2019), but work in this area is nascent. The successes of sentence encoder pretraining have sparked a line of work analyzing these models (Zhang and Bowman, 2018; Peters et al., 2018b; Tenney et al., 2019b; Peters et al., 2019; Tenney et al., 2019a; Liu et al., 2019, i.a.). Our work also attempts to better understand what is learned by pretrained encoders, but we study this question entirely through the lens of pretraining and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically inves"
P19-1439,J93-2004,0,0.0711626,"Missing"
P19-1439,P19-1442,0,0.0239473,"n WMT14 English-German (Bojar et al., 2014) and WMT17 English-Russian (Bojar et al., 2017). We train SkipThought-style sequence-to-sequence (seq2seq) models to read a 1 data.quora.com/First-Quora-DatasetRelease-Question-Pairs 2 QNLI has been re-released with updated splits since the original release. We use the original splits. sentence from WT and predict the following sentence (Kiros et al., 2015; Tang et al., 2017). We train DisSent models to read two clauses from WT that are connected by a discourse marker such as and, but, or so and predict the the discourse marker (Jernite et al., 2017; Nie et al., 2019). Finally, we train seq2seq models to predict the response to a given comment from Reddit, using a previously existing dataset obtained by a third party (available on pushshift.io), comprised of 18M comment– response pairs from 2008-2011. This dataset was used by Yang et al. (2018) to train sentence encoders. Multitask Learning We consider three sets of these tasks for multitask pretraining and intermediate training: all GLUE tasks, all non-GLUE (outside) tasks, and all tasks. 5 Models and Experimental Details We implement our models using the jiant toolkit,3 which is in turn built on AllenNLP"
P19-1439,N18-1202,0,0.787227,"? Sentence-Level Pretraining Beyond Language Modeling Alex Wang,∗1 Jan Hula,2 Patrick Xia,2 Raghavendra Pappagari,2 R. Thomas McCoy,2 , Roma Patel,3 Najoung Kim,2 Ian Tenney,4 Yinghui Huang,6 Katherin Yu,5 Shuning Jin,7 Berlin Chen8 Benjamin Van Durme,2 Edouard Grave,5 Ellie Pavlick,3,4 and Samuel R. Bowman1 1 New York University, 2 Johns Hopkins University, 3 Brown University, 4 Google AI Language, 5 Facebook, 6 IBM, 7 University of Minnesota Duluth, 8 Swarthmore College Abstract Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo’s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target"
P19-1439,W19-4302,0,0.0321696,"sed to compute a linear combination of the LSTM layers. BERT consists of a pretrained Transformer (Vaswani et al., 2017), and is adapted to downstream tasks by fine-tuning the entire model. Follow-up work has explored parameterefficient fine-tuning (Stickland and Murray, 2019; Houlsby et al., 2019) and better target task adaptation via multitask fine-tuning (Phang et al., 2018; Liu et al., 2019), but work in this area is nascent. The successes of sentence encoder pretraining have sparked a line of work analyzing these models (Zhang and Bowman, 2018; Peters et al., 2018b; Tenney et al., 2019b; Peters et al., 2019; Tenney et al., 2019a; Liu et al., 2019, i.a.). Our work also attempts to better understand what is learned by pretrained encoders, but we study this question entirely through the lens of pretraining and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically investigate transfer performance with limited amounts of data and find similar evidence of catastrophic forgetting. 4466 Multitask representation learning in NLP is well studied, and again can be traced back at least as far as"
P19-1439,D18-1179,0,0.247572,"? Sentence-Level Pretraining Beyond Language Modeling Alex Wang,∗1 Jan Hula,2 Patrick Xia,2 Raghavendra Pappagari,2 R. Thomas McCoy,2 , Roma Patel,3 Najoung Kim,2 Ian Tenney,4 Yinghui Huang,6 Katherin Yu,5 Shuning Jin,7 Berlin Chen8 Benjamin Van Durme,2 Edouard Grave,5 Ellie Pavlick,3,4 and Samuel R. Bowman1 1 New York University, 2 Johns Hopkins University, 3 Brown University, 4 Google AI Language, 5 Facebook, 6 IBM, 7 University of Minnesota Duluth, 8 Swarthmore College Abstract Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo’s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target"
P19-1439,P18-2124,0,0.0384363,"Task Model BiLSTM Pretrained BiLSTM Input Text Input Text Intermediate Task Output Target Task Output Intermediate Task Model Target Task Model BiLSTM Intermediate Task-Trained BiLSTM ELMo Introduction State-of-the-art models in natural language processing (NLP) often incorporate encoder functions which generate a sequence of vectors intended to represent the in-context meaning of each word in an input text. These encoders have typically been trained directly on the target task at hand, which can be effective for data-rich tasks and yields human performance on some narrowlydefined benchmarks (Rajpurkar et al., 2018; Hassan et al., 2018), but is tenable only for the few tasks with millions of training data examples. This ∗ This paper supercedes “Looking for ELMo’s Friends: Sentence-Level Pretraining Beyond Language Modeling”, an earlier version of this work by the same authors. Correspondence to: alexwang@nyu.edu ❄ ❄ ❄ ELMo ❄ Input Text Input Text Intermediate Task Output Target Task Output Intermediate Task Model Target Task Model BERT Intermediate Task-Trained BERT Input Text Input Text Figure 1: Learning settings that we consider. Model components with frozen parameters are shown in gray and decorated"
P19-1439,P16-2022,0,0.0140103,"sion classifier. For seq2seq tasks (MT, SkipThought, pushshift.io Reddit dataset) we replace the classifier with a single-layer LSTM word-level decoder and initialize the hidden state with the [CLS] representation. For ELMo-style models, we use several model types: • Single-sentence classification tasks: We train a linear projection over the output states of the encoder, max-pool those projected states, and feed the result to an MLP. 4469 • Sentence-pair tasks: We perform the same steps on both sentences and use the heuristic feature vector [h1 ; h2 ; h1 · h2 ; h1 − h2 ] in the MLP, following Mou et al. (2016). When training target-task models on QQP, STS, MNLI, and QNLI, we use a cross-sentence attention mechanism similar to BiDAF (Seo et al., 2017). We do not use this mechanism in other cases as early results indicated it hurt transfer performance. • Seq2seq tasks (MT, SkipThought, pushshift.io Reddit dataset): We use a single-layer LSTM decoder where the hidden state is initialized with the pooled input representation. • Language modeling: We follow ELMo by concatenating forward and backward models and learning layer mixing weights. To use GLUE tasks for pretraining or intermediate training in a"
P19-1439,D13-1170,0,0.00874927,", 2019b). Accordingly, our pretraining and intermediate ELMo experiments include a baseline of a randomly initialized BiLSTM with no further training. This baseline is especially strong because our ELMo-style models use a skip connection from the input of the encoder to the output, allowing the task-specific component to see the input representations, yielding a model similar to Iyyer et al. (2015). GLUE Tasks We use the nine tasks included with GLUE as pretraining and intermediate tasks: acceptability classification with CoLA (Warstadt et al., 2018); binary sentiment classification with SST (Socher et al., 2013); semantic similarity with the MSR Paraphrase Corpus (MRPC; Dolan and Brockett, 2005), Quora Question Pairs1 (QQP), and STS-Benchmark (STS; Cer et al., 2017); and textual entailment with the Multi-Genre NLI Corpus (MNLI Williams et al., 2018), RTE 1, 2, 3, and 5 (RTE; Dagan et al., 2006, et seq.), and data from SQuAD (QNLI;2 Rajpurkar et al., 2016) and the Winograd Schema Challenge (WNLI; Levesque et al., 2011) recast as entailment in the style of White et al. (2017). MNLI and QQP have previously been shown to be effective for pretraining in other settings (Conneau et al., 2017; Subramanian et"
P19-1439,W17-2625,0,0.0590097,"Missing"
P19-1439,P19-1452,1,0.863686,"lar weights that are used to compute a linear combination of the LSTM layers. BERT consists of a pretrained Transformer (Vaswani et al., 2017), and is adapted to downstream tasks by fine-tuning the entire model. Follow-up work has explored parameterefficient fine-tuning (Stickland and Murray, 2019; Houlsby et al., 2019) and better target task adaptation via multitask fine-tuning (Phang et al., 2018; Liu et al., 2019), but work in this area is nascent. The successes of sentence encoder pretraining have sparked a line of work analyzing these models (Zhang and Bowman, 2018; Peters et al., 2018b; Tenney et al., 2019b; Peters et al., 2019; Tenney et al., 2019a; Liu et al., 2019, i.a.). Our work also attempts to better understand what is learned by pretrained encoders, but we study this question entirely through the lens of pretraining and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically investigate transfer performance with limited amounts of data and find similar evidence of catastrophic forgetting. 4466 Multitask representation learning in NLP is well studied, and again can be traced b"
P19-1439,I17-1100,1,0.867491,"Missing"
P19-1439,N18-1101,1,0.826868,"om the input of the encoder to the output, allowing the task-specific component to see the input representations, yielding a model similar to Iyyer et al. (2015). GLUE Tasks We use the nine tasks included with GLUE as pretraining and intermediate tasks: acceptability classification with CoLA (Warstadt et al., 2018); binary sentiment classification with SST (Socher et al., 2013); semantic similarity with the MSR Paraphrase Corpus (MRPC; Dolan and Brockett, 2005), Quora Question Pairs1 (QQP), and STS-Benchmark (STS; Cer et al., 2017); and textual entailment with the Multi-Genre NLI Corpus (MNLI Williams et al., 2018), RTE 1, 2, 3, and 5 (RTE; Dagan et al., 2006, et seq.), and data from SQuAD (QNLI;2 Rajpurkar et al., 2016) and the Winograd Schema Challenge (WNLI; Levesque et al., 2011) recast as entailment in the style of White et al. (2017). MNLI and QQP have previously been shown to be effective for pretraining in other settings (Conneau et al., 2017; Subramanian et al., 2018; Phang et al., 2018). Other tasks are included to represent a broad sample of labeling schemes commonly used in NLP. Outside Tasks We train language models on two datasets: WikiText-103 (WT; Merity et al., 2017) and Billion Word La"
P19-1439,1983.tc-1.13,0,0.144827,"Missing"
P19-1439,W18-3022,0,0.0192405,"he original release. We use the original splits. sentence from WT and predict the following sentence (Kiros et al., 2015; Tang et al., 2017). We train DisSent models to read two clauses from WT that are connected by a discourse marker such as and, but, or so and predict the the discourse marker (Jernite et al., 2017; Nie et al., 2019). Finally, we train seq2seq models to predict the response to a given comment from Reddit, using a previously existing dataset obtained by a third party (available on pushshift.io), comprised of 18M comment– response pairs from 2008-2011. This dataset was used by Yang et al. (2018) to train sentence encoders. Multitask Learning We consider three sets of these tasks for multitask pretraining and intermediate training: all GLUE tasks, all non-GLUE (outside) tasks, and all tasks. 5 Models and Experimental Details We implement our models using the jiant toolkit,3 which is in turn built on AllenNLP (Gardner et al., 2017) and on a public PyTorch implementation of BERT.4 Appendix A presents additional details. Encoder Architecture For both the pretraining and intermediate ELMo experiments, we process words using a pretrained character-level convolutional neural network (CNN) f"
P19-1439,W18-5448,1,0.914222,"s and only learning a set of task-specific scalar weights that are used to compute a linear combination of the LSTM layers. BERT consists of a pretrained Transformer (Vaswani et al., 2017), and is adapted to downstream tasks by fine-tuning the entire model. Follow-up work has explored parameterefficient fine-tuning (Stickland and Murray, 2019; Houlsby et al., 2019) and better target task adaptation via multitask fine-tuning (Phang et al., 2018; Liu et al., 2019), but work in this area is nascent. The successes of sentence encoder pretraining have sparked a line of work analyzing these models (Zhang and Bowman, 2018; Peters et al., 2018b; Tenney et al., 2019b; Peters et al., 2019; Tenney et al., 2019a; Liu et al., 2019, i.a.). Our work also attempts to better understand what is learned by pretrained encoders, but we study this question entirely through the lens of pretraining and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically investigate transfer performance with limited amounts of data and find similar evidence of catastrophic forgetting. 4466 Multitask representation learning in NLP"
P19-1439,Q17-1027,1,0.847544,"Missing"
P19-1475,D15-1075,0,0.0291488,"A: by causality estimation through pointwise mutual information (Gordon et al., 2011) or data-driven methods (Luo et al., 2016; Sasaki et al., 2017), or through a pre-trained language model (Radford et al., 2018, GPT).1 Under the Johns Hopkins Ordinal Commonsense Inference (JOCI) dataset (Zhang et al., 2017), instead of selecting which hypothesis is the most plausible, a model is expected to directly assign ordinal 5-level Likert scale judgments (from impossible to very likely). If taking an ordinal interpretation of NLI, this can be viewed as a 5-way variant of the 3-way labels used in SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018). In this paper, we recast MNLI and JOCI as COPA-style plausibility tasks by sampling and constructing (p, h, h 0) triples from these two datasets. Each premise-hypothesis pair (p, h) is labeled with different levels of plausibility y p,h .2 1 [CLS ; p ; SEP ; h ; SEP] As reported in https://blog.openai.com/ language-unsupervised/. 2 For MNLI, entailment > neutral > contradiction; for JOCI, very likely > likely > plausible > technically possible > impossible. exp F(p, hi ) N X . (1) exp F(p, h j ) j=1 Margin-based loss As we have argued before, the cross entrop"
P19-1475,N19-1423,0,0.0182234,"X X Figure 1: COPA-like pairs may be constructed from datasets such as MultiNLI, where a premise and two hypotheses are presented, where the correct – most plausible – item depends on the competing hypothesis. Cross entropy log-loss Margin-loss CON NEU ENT Distribution Distribution CON NEU ENT Score Score Figure 2: Dev set score distribution on COPA-pairs derived from MNLI, after training with cross entropy logloss and margin-loss. Margin-loss leads to a more intuitively plausible encoding of Neutral statements. Introduction Contextualized encoders such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) have led to improvements on various structurally similar Natural Language Understanding (NLU) tasks such as variants of Natural Language Inference (NLI). Such tasks model the conditional interpretation of a sentence (e.g., an NLI hypothesis) based on some other context (usually some other sentence, e.g., an NLI premise). The structural similarity of these tasks points to a structurally similar modeling approach: (1) concatenate the conditioning context (premise) to a sentence to be interpreted, (2) ∗ This work was done while the first author was visiting Johns Hopkins University. read this pa"
P19-1475,N18-1101,0,0.206402,"ext: a procedure that maximizes the probability of the correct item at training time, thereby minimizing the probability of the other alternative(s), will seemingly learn to misread future examples. We argue that COPA-style tasks should intuitively be approached as learning to rank problems (Burges et al., 2005; Cao et al., 2007), where an encoder on competing items is trained to assign relatively higher or lower scores to candidates, rather than maximizing or minimizing probabilities. In the following we investigate three datasets, beginning with a constructed COPA-style variant of MultiNLI (Williams et al., 2018, later MNLI), designed to be adversarial (see Figure 1). Results on this dataset support our intuition (see Figure 2). We then construct a second synthetic dataset based on JOCI (Zhang et al., 2017), which employed a finer label set than NLI, and a margin-based approach strictly outperforms log-loss in this case. Finally, we demonstrate state-of-the-art on COPA, showing that a BERT-based model trained with margin-loss significantly outperforms a log-loss alternative. 2 3 Models In models based on GPT and BERT for plausibility or NLI, similar neural architectures have been employed. The premis"
P19-1475,Q17-1027,1,0.877185,"Missing"
P19-1475,W17-6937,0,\N,Missing
Q15-1034,P98-1013,0,0.100153,"Missing"
Q15-1034,S07-1018,0,0.0170478,"Missing"
Q15-1034,W05-0620,0,0.0202156,"Missing"
Q15-1034,N10-1138,0,0.0505699,"Missing"
Q15-1034,J02-3001,0,0.0491556,"Missing"
Q15-1034,P14-2065,0,0.049778,"Missing"
Q15-1034,D13-1149,0,0.154239,"Missing"
Q15-1034,W04-0803,0,0.0309818,"Missing"
Q15-1034,J93-2004,0,0.0611536,"Missing"
Q15-1034,J05-1004,0,0.0784135,"n. 475 Transactions of the Association for Computational Linguistics, vol. 3, pp. 475–488, 2015. Action Editor: Diana McCarthy. Submission batch: 3/2015; Revision batch 6/2015; Published 8/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. replicate these previous experiments, and demonstrate that what can be done in this domain in a controlled lab experiment can be done via crowdsourcing. We extend this to a large-scale MTurk annotation task using corpus data. This task presents an annotator with a particular (‘token-level’) sentence from PropBank (Palmer et al., 2005) and a highlighted argument, and asks them for a likelihood judgment about a property; for example, “How likely or unlikely is it that A RG is sentient?”. By looking across many token-level instances of a verb, we can then infer type-level information about the verb. We discuss results from this task over 11 role properties annotated by a single (trusted) annotator on approximately 5000 verb tokens. Our results represent the first large-scale corpus study explicitly aimed at confirming Dowty’s proto-role hypothesis: Proto-Agent properties predict the mapping of semantic arguments to subject an"
Q15-1034,W04-1908,0,0.0586686,"Missing"
Q15-1034,N15-1058,1,0.74265,"Missing"
Q15-1034,C04-1133,0,\N,Missing
Q15-1034,C98-1013,0,\N,Missing
Q17-1027,S12-1051,0,0.016845,"coin. H: That flip comes up heads. No human reading T should infer that H is true. A model trained to make ordinal predictions should say: “plausible, with probability 1.0”, whereas a model trained to make binary entailed/not-entailed predictions should say: “not entailed, with probability 1.0”. The following example exhibits the same property: T: An animal eats food. H: A person eats food. Again, with high confidence, H is plausible; and, with high confidence, it is also not entailed. Non-entailing Inference Of the various non“entailment” textual inference tasks, a few are most salient here. Agirre et al. (2012) piloted a Textual Similarity evaluation which has been refined in subsequent years. Systems produce scalar values corresponding to predictions of how similar the meaning is between two provided sentences, e.g., the following pair from SICK was judged very similar (4.2 out of 5), while also being a contradiction: There is no biker jumping in the air and A lone biker is jumping in the air. The ordinal approach we advocate for relies on a graded notion, like textual similarity. The Choice of Plausible Alternative (COPA) task (Roemmele et al., 2011) was a reaction to RTE, similarly motivated to p"
Q17-1027,P98-1013,0,0.0627839,"se inference examples, judged to hold with varying levels of subjective likelihood (§5). We provide baseline results (§6) for prediction on the JOCI corpus.5 4 For further background see discussions by Van Durme (2010), Gordon and Van Durme (2013), Rudinger et al. (2015) and Misra et al. (2016). 5 The JOCI corpus is released freely at: http://decomp. net/. 380 2 Background Mining Common Sense Building large collections of common-sense knowledge can be done manually via professionals (Hobbs and Navarretta, 1993), but at considerable cost in terms of time and expense (Miller, 1995; Lenat, 1995; Baker et al., 1998; Friedland et al., 2004). Efforts have pursued volunteers (Singh, 2002; Havasi et al., 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor. Many have pursued automating the process, such as in expanding lexical hierarchies (Hearst, 1992; Snow et al., 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al., 2011), reading reference materials (Richardson et al., 1998; Suchanek et al., 2007), mining search engine query logs (Pas¸ca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovere"
Q17-1027,P11-1062,0,0.00914887,"ning Common Sense Building large collections of common-sense knowledge can be done manually via professionals (Hobbs and Navarretta, 1993), but at considerable cost in terms of time and expense (Miller, 1995; Lenat, 1995; Baker et al., 1998; Friedland et al., 2004). Efforts have pursued volunteers (Singh, 2002; Havasi et al., 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor. Many have pursued automating the process, such as in expanding lexical hierarchies (Hearst, 1992; Snow et al., 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al., 2011), reading reference materials (Richardson et al., 1998; Suchanek et al., 2007), mining search engine query logs (Pas¸ca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al., 2003; Banko and Etzioni, 2007). In this article we are concerned with knowledge mining for purposes of seeding a text generation process (constructing common-sense inference examples). Common-sense Tasks Many textual inference tasks have been designed to require some degree of common-sense knowledge"
Q17-1027,D15-1075,0,0.196309,"rk et al., 2003; Banko and Etzioni, 2007). In this article we are concerned with knowledge mining for purposes of seeding a text generation process (constructing common-sense inference examples). Common-sense Tasks Many textual inference tasks have been designed to require some degree of common-sense knowledge, e.g., the Winograd Schema Challenge discussed by Levesque et al. (2011). The data for these tasks are either smaller, carefully constructed evaluation sets by professionals, following efforts like the F RAC A S test suite (Cooper et al., 1996), or they rely on crowdsourced elicitation (Bowman et al., 2015). Crowdsourcing is scalable, but elicitation protocols can lead to biased responses unlikely to contain a wide range of possible common-sense inferences. Humans can generally agree on the plausibility of a wide range of possible inference pairs, but they are not likely to generate them from an initial prompt.6 The construction of SICK (Sentences Involving Compositional Knowledge) made use of existing paraphrastic sentence pairs (descriptions by differ6 McRae et al. (2005): Features such as &lt;is larger than a tulip&gt; or &lt;moves faster than an infant&gt;, for example; although logically possible, do n"
Q17-1027,P08-1090,0,0.0281911,"ere is no biker jumping in the air and A lone biker is jumping in the air. The ordinal approach we advocate for relies on a graded notion, like textual similarity. The Choice of Plausible Alternative (COPA) task (Roemmele et al., 2011) was a reaction to RTE, similarly motivated to probe a system’s ability to understand inferences that are not strictly entailed. A single context was provided, with two alternative inferences, and a system had to judge which was more plausible. The COPA dataset was manually elicited, and is not large; we discuss this data further in §5. The Narrative Cloze task (Chambers and Jurafsky, 2008) requires a system to score candidate inferences as to how likely they are to appear in a document that also included the provided context. Many such inferences are then not strictly entailed by the context. Further, the Cloze task gives the benefit of being able to generate very large numbers of examples automatically by simply occluding parts of existing documents and asking a system to predict what is missing. The LAMBADA dataset (Paperno et al., 2016) is akin to our strategy for automatic generation followed by human filtering, but for Cloze examples. As our concern is with inferences that"
Q17-1027,W99-0631,0,0.0911746,"ugh the measurement of human reading times when using prompts derived from our ordinal common-sense inference examples. Computational modeling of (unconditional) semantic plausibility has been explored by those such as Pad´o et al. (2009), Erk et al. (2010) and Sayeed et al. (2015). Textual Entailment A multi-year source of textual inference examples were generated under the Recognizing Textual Entailment (RTE) Challenges, introduced by Dagan et al. (2006): 7 This notion of thematic plausibility is then related to the notion of verb-argument selectional preference (Zernik, 1992; Resnik, 1993; Clark and Weir, 1999), and sortal (in)correctness (Thomason, 1972). 8 Thanks to the anonymous reviewer for this connection. 381 We say that T entails H if, typically, a human reading T would infer that H is most likely true. This somewhat informal definition is based on (and assumes) common human understanding of language as well as common background knowledge. This definition strayed from the more strict notion of entailment as used by linguistic semanticists, such as those involved with F RAC A S. While Giampiccolo et al. (2008) extended binary RTE with an “unknown” category, the entailment community has primari"
Q17-1027,W03-0901,0,0.154374,"2002; Havasi et al., 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor. Many have pursued automating the process, such as in expanding lexical hierarchies (Hearst, 1992; Snow et al., 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al., 2011), reading reference materials (Richardson et al., 1998; Suchanek et al., 2007), mining search engine query logs (Pas¸ca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al., 2003; Banko and Etzioni, 2007). In this article we are concerned with knowledge mining for purposes of seeding a text generation process (constructing common-sense inference examples). Common-sense Tasks Many textual inference tasks have been designed to require some degree of common-sense knowledge, e.g., the Winograd Schema Challenge discussed by Levesque et al. (2011). The data for these tasks are either smaller, carefully constructed evaluation sets by professionals, following efforts like the F RAC A S test suite (Cooper et al., 1996), or they rely on crowdsourced elicitation (Bowman et al.,"
Q17-1027,de-marneffe-etal-2014-universal,0,0.0235901,"Missing"
Q17-1027,J10-4007,0,0.103965,"Missing"
Q17-1027,W11-0112,0,0.0204984,"ce, i.e. the new posterior on the world after reading the sentence. A new sentence in a discourse is almost never entailed by another sentence in the discourse, because such a sentence would add no new information. In order to successfully process a discourse, there needs to be some understanding of what new information can be, possibly or plausibly, added to the discourse. Collecting sentence pairs with ordinal entailment connections is potentially useful for improving and testing these language understanding capabilities that would be needed by algorithms for applications like storytelling. Garrette et al. (2011) and Beltagy et al. (2017) treated textual entailment as probabilistic logical inference in Markov Logic Networks (Richardson and Domingos, 2006). However, the notion of probability in their entailment task has a subtle distinction from our problem of common-sense inference. The probability of being an entailment given by a probabilistic model trained for a binary classification (being an entailment or not) is not necessarily the same as the likelihood of an inference being true. For example: T: A person flips a coin. H: That flip comes up heads. No human reading T should infer that H is true."
Q17-1027,S13-1036,0,0.0204765,"for ordinal regression model on A-test and B-test. (*p-value&lt;.01 for ρ) We also run a feature ablation test. Table 6 shows that the most useful features differ for Atest and B-test. On A-test, where the inferences are elicited from humans, removal of similarity- and bow-based features together results in the largest performance drop. On B-test, by contrast, removing similarity and bow features results in a comtions in ordinal prediction tasks (Baccianella et al., 2009; Bennett and Lanning, 2007; Gaudette and Japkowicz, 2009; Agresti and Kateri, 2011; Popescu and Dinu, 2009; Liu et al., 2015; Gella et al., 2013). parable performance drop to removing seq2seq features. These observations point to statistical differences between human-elicited and auto-generated hypotheses, a motivating point of the JOCI corpus. 7 Conclusions and Future Work In motivating the need for automatically building collections of common-sense knowledge, Clark et al. (2003) wrote: Acknowledgments Thank you to action editor Mark Steedman and the anonymous reviewers for their feedback, as well as colleagues including Lenhart Schubert, Kyle Rawlins, Aaron White, and Keisuke Sakaguchi. This work was supported in part by DARPA LORELE"
Q17-1027,C92-2082,0,0.222733,"(2016). 5 The JOCI corpus is released freely at: http://decomp. net/. 380 2 Background Mining Common Sense Building large collections of common-sense knowledge can be done manually via professionals (Hobbs and Navarretta, 1993), but at considerable cost in terms of time and expense (Miller, 1995; Lenat, 1995; Baker et al., 1998; Friedland et al., 2004). Efforts have pursued volunteers (Singh, 2002; Havasi et al., 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor. Many have pursued automating the process, such as in expanding lexical hierarchies (Hearst, 1992; Snow et al., 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al., 2011), reading reference materials (Richardson et al., 1998; Suchanek et al., 2007), mining search engine query logs (Pas¸ca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al., 2003; Banko and Etzioni, 2007). In this article we are concerned with knowledge mining for purposes of seeding a text generation process (constructing common-sense inference examples). Common-sense Tasks"
Q17-1027,T87-1006,0,0.349843,".2 Common-sense inference – inferences based on common-sense knowledge – is possibilistic: things everyone more or less would expect to hold in a given context, but without the necessary strength of logical entailment.3 Because natural language corpora exhibits human reporting bias (Gordon and Van Durme, 2013), systems that derive knowledge exclusively from such corpora may be more accurately considered models of language, rather than of the Introduction We use words to talk about the world. Therefore, to understand what words mean, we must have a prior explication of how we view the world. – Hobbs (1987) Researchers in Artificial Intelligence and (Computational) Linguistics have long-cited the requirement of common-sense knowledge in language understanding.1 This knowledge is viewed as a key 1 Schank (1975): It has been apparent ... within ... natural language understanding ... that the eventual limit to our solution ... would be our ability to characterize world knowledge. 2 McCarthy (1959): a program has common sense if it automatically deduces for itself a sufficiently wide class of immediate consequences of anything it is told and what it already knows. 3 Many of the bridging inferences o"
Q17-1027,C02-1105,0,0.00837765,"pursued volunteers (Singh, 2002; Havasi et al., 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor. Many have pursued automating the process, such as in expanding lexical hierarchies (Hearst, 1992; Snow et al., 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al., 2011), reading reference materials (Richardson et al., 1998; Suchanek et al., 2007), mining search engine query logs (Pas¸ca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al., 2003; Banko and Etzioni, 2007). In this article we are concerned with knowledge mining for purposes of seeding a text generation process (constructing common-sense inference examples). Common-sense Tasks Many textual inference tasks have been designed to require some degree of common-sense knowledge, e.g., the Winograd Schema Challenge discussed by Levesque et al. (2011). The data for these tasks are either smaller, carefully constructed evaluation sets by professionals, following efforts like the F RAC A S test suite (Cooper et al., 1996), or they rely on crowdsourced elicitat"
Q17-1027,P15-1145,0,0.0153587,": Ablation results for ordinal regression model on A-test and B-test. (*p-value&lt;.01 for ρ) We also run a feature ablation test. Table 6 shows that the most useful features differ for Atest and B-test. On A-test, where the inferences are elicited from humans, removal of similarity- and bow-based features together results in the largest performance drop. On B-test, by contrast, removing similarity and bow features results in a comtions in ordinal prediction tasks (Baccianella et al., 2009; Bennett and Lanning, 2007; Gaudette and Japkowicz, 2009; Agresti and Kateri, 2011; Popescu and Dinu, 2009; Liu et al., 2015; Gella et al., 2013). parable performance drop to removing seq2seq features. These observations point to statistical differences between human-elicited and auto-generated hypotheses, a motivating point of the JOCI corpus. 7 Conclusions and Future Work In motivating the need for automatically building collections of common-sense knowledge, Clark et al. (2003) wrote: Acknowledgments Thank you to action editor Mark Steedman and the anonymous reviewers for their feedback, as well as colleagues including Lenhart Schubert, Kyle Rawlins, Aaron White, and Keisuke Sakaguchi. This work was supported in"
Q17-1027,W07-1431,0,0.0403768,"entails H if, typically, a human reading T would infer that H is most likely true. This somewhat informal definition is based on (and assumes) common human understanding of language as well as common background knowledge. This definition strayed from the more strict notion of entailment as used by linguistic semanticists, such as those involved with F RAC A S. While Giampiccolo et al. (2008) extended binary RTE with an “unknown” category, the entailment community has primarily focused on issues such as “paraphrase” and “monotonicity”. An example of this is the Natural Logic implementation of MacCartney and Manning (2007). Language understanding in context is not only understanding the entailments of a sentence, but also the plausible inferences of the sentence, i.e. the new posterior on the world after reading the sentence. A new sentence in a discourse is almost never entailed by another sentence in the discourse, because such a sentence would add no new information. In order to successfully process a discourse, there needs to be some understanding of what new information can be, possibly or plausibly, added to the discourse. Collecting sentence pairs with ordinal entailment connections is potentially useful"
Q17-1027,marelli-etal-2014-sick,0,0.0295091,"ce pairs, but they are not likely to generate them from an initial prompt.6 The construction of SICK (Sentences Involving Compositional Knowledge) made use of existing paraphrastic sentence pairs (descriptions by differ6 McRae et al. (2005): Features such as &lt;is larger than a tulip&gt; or &lt;moves faster than an infant&gt;, for example; although logically possible, do not occur in [human responses] [...] Although people are capable of verifying that a &lt;dog is larger than a pencil&gt;. ent people of the same image), which were modified through a series of rule-based transformations then judged by humans (Marelli et al., 2014). As with SICK, we rely on humans only for judging provided examples, rather than elicitation of text. Unlike SICK, our generation is based on a process targeted specifically at common sense (see §4.1.1). Plausibility Researchers in psycholinguistics have explored a notion of plausibility in human sentence processing, where, for instance, arguments to predicates are intuitively more or less “plausible” as fillers to different thematic roles, as reflected in human reading times. For example, McRae et al. (1998) looked at manipulations such as: (a) The boss hired by the corporation was perfect f"
Q17-1027,H92-1116,0,0.627619,"Missing"
Q17-1027,N16-1098,0,0.0825906,"lso included the provided context. Many such inferences are then not strictly entailed by the context. Further, the Cloze task gives the benefit of being able to generate very large numbers of examples automatically by simply occluding parts of existing documents and asking a system to predict what is missing. The LAMBADA dataset (Paperno et al., 2016) is akin to our strategy for automatic generation followed by human filtering, but for Cloze examples. As our concern is with inferences that are often true but never stated in a document, this approach is not viable here. The ROCStories corpus (Mostafazadeh et al., 2016) elicited a more “plausible” collection of documents in order to retain the narrative Cloze in the context of common-sense inference. The ROCStories corpus can be viewed as an extension of the idea behind 382 the COPA corpus, done at a larger scale with crowdsourcing, and with multi-sentence contexts; we consider this dataset in §5. Alongside the narrative Cloze, Pichotta and Mooney (2016) made use of a 5-point Likert scale (very likely to very unlikely) as a secondary evaluation of various script induction techniques. While they were concerned with measuring their ability to generate very lik"
Q17-1027,N07-1071,0,0.0310536,"sponding argument with a placeholder, similar to Van Durme et al. (2009) (see Fig 3 (b)). We remove any template associated with a sense if it occurs less than two times for that sense, 12 https://pypi.python.org/pypi/ PyStanfordDependencies 13 Using English glosses of the logical representations, abstraction of “a long, dark corridor” would yield “corridor” for example; “a small office at the end of a long dark corridor” would yield “office”; and “Mrs. MacReady” would yield “person”. See Schubert (2002) for detail. 14 In order to avoid too general senses, we set cut points at the depth of 4 (Pantel et al., 2007) to truncate the hierarchy and consider all 81,861 senses below these points. 384 leaving 38 million unique templates. (c) Deriving properties via WordNet: At this step, we want to associate with each WordNet sense a set of possible properties. We employ three strategies. The first strategy is to use a decision tree to pick out highly discriminative properties for each WordNet sense. Specifically, for each set of cohyponyms,15 we train a decision tree using the associated templates as features. For example, in Fig 3 (c), we train a decision tree over the cohyponyms of publication.n.01. Then th"
Q17-1027,P16-1144,0,0.0232549,"re plausible. The COPA dataset was manually elicited, and is not large; we discuss this data further in §5. The Narrative Cloze task (Chambers and Jurafsky, 2008) requires a system to score candidate inferences as to how likely they are to appear in a document that also included the provided context. Many such inferences are then not strictly entailed by the context. Further, the Cloze task gives the benefit of being able to generate very large numbers of examples automatically by simply occluding parts of existing documents and asking a system to predict what is missing. The LAMBADA dataset (Paperno et al., 2016) is akin to our strategy for automatic generation followed by human filtering, but for Cloze examples. As our concern is with inferences that are often true but never stated in a document, this approach is not viable here. The ROCStories corpus (Mostafazadeh et al., 2016) elicited a more “plausible” collection of documents in order to retain the narrative Cloze in the context of common-sense inference. The ROCStories corpus can be viewed as an extension of the idea behind 382 the COPA corpus, done at a larger scale with crowdsourcing, and with multi-sentence contexts; we consider this dataset"
Q17-1027,R09-1063,0,0.0142342,".00 .00 .00 .05 Table 6: Ablation results for ordinal regression model on A-test and B-test. (*p-value&lt;.01 for ρ) We also run a feature ablation test. Table 6 shows that the most useful features differ for Atest and B-test. On A-test, where the inferences are elicited from humans, removal of similarity- and bow-based features together results in the largest performance drop. On B-test, by contrast, removing similarity and bow features results in a comtions in ordinal prediction tasks (Baccianella et al., 2009; Bennett and Lanning, 2007; Gaudette and Japkowicz, 2009; Agresti and Kateri, 2011; Popescu and Dinu, 2009; Liu et al., 2015; Gella et al., 2013). parable performance drop to removing seq2seq features. These observations point to statistical differences between human-elicited and auto-generated hypotheses, a motivating point of the JOCI corpus. 7 Conclusions and Future Work In motivating the need for automatically building collections of common-sense knowledge, Clark et al. (2003) wrote: Acknowledgments Thank you to action editor Mark Steedman and the anonymous reviewers for their feedback, as well as colleagues including Lenhart Schubert, Kyle Rawlins, Aaron White, and Keisuke Sakaguchi. This wor"
Q17-1027,H93-1054,0,0.106892,", perhaps through the measurement of human reading times when using prompts derived from our ordinal common-sense inference examples. Computational modeling of (unconditional) semantic plausibility has been explored by those such as Pad´o et al. (2009), Erk et al. (2010) and Sayeed et al. (2015). Textual Entailment A multi-year source of textual inference examples were generated under the Recognizing Textual Entailment (RTE) Challenges, introduced by Dagan et al. (2006): 7 This notion of thematic plausibility is then related to the notion of verb-argument selectional preference (Zernik, 1992; Resnik, 1993; Clark and Weir, 1999), and sortal (in)correctness (Thomason, 1972). 8 Thanks to the anonymous reviewer for this connection. 381 We say that T entails H if, typically, a human reading T would infer that H is most likely true. This somewhat informal definition is based on (and assumes) common human understanding of language as well as common background knowledge. This definition strayed from the more strict notion of entailment as used by linguistic semanticists, such as those involved with F RAC A S. While Giampiccolo et al. (2008) extended binary RTE with an “unknown” category, the entailmen"
Q17-1027,P98-2180,0,0.106775,"mon-sense knowledge can be done manually via professionals (Hobbs and Navarretta, 1993), but at considerable cost in terms of time and expense (Miller, 1995; Lenat, 1995; Baker et al., 1998; Friedland et al., 2004). Efforts have pursued volunteers (Singh, 2002; Havasi et al., 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor. Many have pursued automating the process, such as in expanding lexical hierarchies (Hearst, 1992; Snow et al., 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al., 2011), reading reference materials (Richardson et al., 1998; Suchanek et al., 2007), mining search engine query logs (Pas¸ca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al., 2003; Banko and Etzioni, 2007). In this article we are concerned with knowledge mining for purposes of seeding a text generation process (constructing common-sense inference examples). Common-sense Tasks Many textual inference tasks have been designed to require some degree of common-sense knowledge, e.g., the Winograd Schema Challenge discussed by Lev"
Q17-1027,D15-1195,1,0.837784,"Missing"
Q17-1027,T75-2023,0,0.585373,"entailment.3 Because natural language corpora exhibits human reporting bias (Gordon and Van Durme, 2013), systems that derive knowledge exclusively from such corpora may be more accurately considered models of language, rather than of the Introduction We use words to talk about the world. Therefore, to understand what words mean, we must have a prior explication of how we view the world. – Hobbs (1987) Researchers in Artificial Intelligence and (Computational) Linguistics have long-cited the requirement of common-sense knowledge in language understanding.1 This knowledge is viewed as a key 1 Schank (1975): It has been apparent ... within ... natural language understanding ... that the eventual limit to our solution ... would be our ability to characterize world knowledge. 2 McCarthy (1959): a program has common sense if it automatically deduces for itself a sufficiently wide class of immediate consequences of anything it is told and what it already knows. 3 Many of the bridging inferences of Clark (1975) make use of common-sense knowledge, such as the following example of “Probable part”: I walked into the room. The windows looked out to the bay. To resolve the definite reference the windows,"
Q17-1027,P06-1101,0,0.0142468,"JOCI corpus is released freely at: http://decomp. net/. 380 2 Background Mining Common Sense Building large collections of common-sense knowledge can be done manually via professionals (Hobbs and Navarretta, 1993), but at considerable cost in terms of time and expense (Miller, 1995; Lenat, 1995; Baker et al., 1998; Friedland et al., 2004). Efforts have pursued volunteers (Singh, 2002; Havasi et al., 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor. Many have pursued automating the process, such as in expanding lexical hierarchies (Hearst, 1992; Snow et al., 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al., 2011), reading reference materials (Richardson et al., 1998; Suchanek et al., 2007), mining search engine query logs (Pas¸ca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al., 2003; Banko and Etzioni, 2007). In this article we are concerned with knowledge mining for purposes of seeding a text generation process (constructing common-sense inference examples). Common-sense Tasks Many textual infere"
Q17-1027,W08-2219,1,0.802414,"Missing"
Q17-1027,E09-1092,1,0.874567,"Missing"
Q17-1027,D16-1177,1,0.0444107,"Missing"
Q17-1027,Q14-1006,0,0.0321308,") and ROCStories (Mostafazadeh et al., 2016), paired with hypotheses generated via methods described in §4.1. These pairs are then annotated with ordinal labels using crowdsourcing (§4.2). We also include context-hypothesis pairs directly taken from SNLI and other corpora (e.g., as premise-hypothesis pairs), and re-annotate them with ordinal labels. 5.1 Data sources for Context-Hypothesis Pairs In order to compare with existing inference corpora, we choose contexts from two resources: (1) the first sentence in the sentence pairs of the SNLI corpus which are captions from the Flickr30k corpus (Young et al., 2014), and (2) the first sentence in the stories of the ROCStories corpus. We then collect candidates of automatically generated common-sense inferences (AGCI) against these contexts. Specifically, in the SNLI train set, there are over 150K different first sentences, involving 7,414 different arguments according to predicate-argument extraction. We randomly choose 4,600 arguments. For each argument, we sample one first sentence that has the argument and collect candidates of AGCI against this as context. We also do the same generation for the SNLI development set and test set. We also collect candi"
Q17-1027,C92-4212,0,0.0392934,"al future work, perhaps through the measurement of human reading times when using prompts derived from our ordinal common-sense inference examples. Computational modeling of (unconditional) semantic plausibility has been explored by those such as Pad´o et al. (2009), Erk et al. (2010) and Sayeed et al. (2015). Textual Entailment A multi-year source of textual inference examples were generated under the Recognizing Textual Entailment (RTE) Challenges, introduced by Dagan et al. (2006): 7 This notion of thematic plausibility is then related to the notion of verb-argument selectional preference (Zernik, 1992; Resnik, 1993; Clark and Weir, 1999), and sortal (in)correctness (Thomason, 1972). 8 Thanks to the anonymous reviewer for this connection. 381 We say that T entails H if, typically, a human reading T would infer that H is most likely true. This somewhat informal definition is based on (and assumes) common human understanding of language as well as common background knowledge. This definition strayed from the more strict notion of entailment as used by linguistic semanticists, such as those involved with F RAC A S. While Giampiccolo et al. (2008) extended binary RTE with an “unknown” category,"
Q17-1027,E17-2011,1,0.766207,"Missing"
Q19-1035,W17-6901,0,0.0376694,"Missing"
Q19-1035,P98-1013,0,0.105543,"ntparnasse storage still available? But we do see that there is a slight tendency (ρ = −0.25) for predicates that are hypotheticalreferring not to be particular-referring—for example, knows in (18a) and do in (18b) are hypotheticals in the lower left. 3. VerbNet Verb classes from VerbNet (Schuler, 2005) for root predicate lemmas. 4. FrameNet Frames evoked by root predicate lemmas in the predicate protocol and for both 509   0, 10−5 , 10−4 , 10−3 , and the dropout probability d ∈ {0.1, 0.2, 0.3, 0.4, 0.5}. the root argument lemma and its predicate head in the argument protocol from FrameNet (Baker et al., 1998). Development For all models, we train for at most 20 epochs with early stopping. At the end of each epoch, the L1 loss is calculated on the development set, and if it is higher than the previous epoch, we stop training, saving the parameter values from the previous epoch. 5. WordNet The union of WordNet (Fellbaum, 1998) supersenses (Ciaramita and Johnson, 2003) for all WordNet senses the root argument or predicate lemmas can have. And we consider two sets of token-level handengineered features. Evaluation Consonant with work in event factuality prediction, we report Pearson correlation (ρ) an"
Q19-1035,D18-1454,0,0.0277573,"c Statements Venkata Govindarajan Benjamin Van Durme Aaron Steven White University of Rochester Johns Hopkins University University of Rochester Abstract have a long history in both the linguistics and artificial intelligence literatures (see Carlson, 2011; Maienborn et al., 2011; Leslie and Lerner, 2016). Nevertheless, few modern semantic parsers make a systematic distinction (cf. Abzianidze and Bos, 2017). This is problematic, because the ability to accurately capture different modes of generalization is likely key to building systems with robust common sense reasoning (Zhang et al., 2017a; Bauer et al., 2018): Such systems need some source for general knowledge about the world (McCarthy, 1960, 1980, 1986; Minsky, 1974; Schank and Abelson, 1975; Hobbs et al., 1987; Reiter, 1987) and natural language text seems like a prime candidate. It is also surprising, because there is no dearth of data relevant to linguistic expressions of generalization (Doddington et al., 2004; Cybulska and Vossen, 2014b; Friedrich et al., 2015). One obstacle to further progress on generalization is that current frameworks tend to take standard descriptive categories as sharp classes— for example, EPISODIC, GENERIC, HABITUAL"
Q19-1035,de-marneffe-etal-2014-universal,0,0.115584,"Missing"
Q19-1035,P10-1143,0,0.0758239,"Missing"
Q19-1035,N19-1423,0,0.0256321,"t properties While type-level handengineered and learned features perform relatively poorly for properties such as IS.PARTICULAR and IS.KIND for arguments, they are able to predict IS.ABSTRACT relatively well compared to the models with all features. The converse of this also holds: Token-level hand-engineered features are better able to predict IS.PARTICULAR and IS.KIND, but perform relatively poorly on their own for IS.ABSTRACT. relevant to generalization that the contextual learned features are missing. This performance boost may be diminished by improved contextual encoders, such as BERT (Devlin et al., 2019). Predicate properties We see a pattern similar to the one observed for the argument properties mirrored in the predicate properties: Whereas type-level hand-engineered and learned features perform relatively poorly for properties such as IS.PARTICULAR and IS.HYPOTHETICAL, they are able to predict IS.DYNAMIC relatively well compared with the models with all features. The converse of this also holds: Token-level hand-engineered features are better able to predict IS.PARTICULAR and IS.HYPOTHETICAL, but perform relatively poorly on their own for IS.DYNAMIC. One caveat here is that, unlike for IS."
Q19-1035,doddington-etal-2004-automatic,0,0.190944,"Missing"
Q19-1035,W03-1022,0,0.25733,"Missing"
Q19-1035,P14-2085,0,0.176693,"of more basic properties, such as volitionality, causation, change-of-state, and so forth, and is similarly inspired by classic theoretical work (Dowty, 1991). In our framework, prototypical episodics, habituals, and generics correspond to sets of properties that the referents of a clause’s head predicate and arguments have—namely, clausal categories are built up from properties of the predicates that head (8) Those firemen are available. (9) Those firemen are strong. This situation is improved upon in the Richer Event Descriptions (RED; O’Gorman et al., 2016) and Situation Entities (SitEnt; Friedrich and Palmer, 2014a,b; Friedrich et al., 2015; Friedrich and Pinkal, 2015b,a; Friedrich et al., 2016) frameworks, which annotate both NPs and entire clauses for genericity. In particular, SitEnt, which is used to annotate MASC (Ide et al., 2010) and Wikipedia, has the nice property that it recognizes the existence of abstract entities and lexical aspectual class of clauses’ main verbs, along with habituality and genericity. This is useful because, in addition to decomposing statements using the genericity of the main referent and event, this framework recognizes that lexical aspect is an independent phenomenon."
Q19-1035,W14-4921,0,0.128249,"of more basic properties, such as volitionality, causation, change-of-state, and so forth, and is similarly inspired by classic theoretical work (Dowty, 1991). In our framework, prototypical episodics, habituals, and generics correspond to sets of properties that the referents of a clause’s head predicate and arguments have—namely, clausal categories are built up from properties of the predicates that head (8) Those firemen are available. (9) Those firemen are strong. This situation is improved upon in the Richer Event Descriptions (RED; O’Gorman et al., 2016) and Situation Entities (SitEnt; Friedrich and Palmer, 2014a,b; Friedrich et al., 2015; Friedrich and Pinkal, 2015b,a; Friedrich et al., 2016) frameworks, which annotate both NPs and entire clauses for genericity. In particular, SitEnt, which is used to annotate MASC (Ide et al., 2010) and Wikipedia, has the nice property that it recognizes the existence of abstract entities and lexical aspectual class of clauses’ main verbs, along with habituality and genericity. This is useful because, in addition to decomposing statements using the genericity of the main referent and event, this framework recognizes that lexical aspect is an independent phenomenon."
Q19-1035,cybulska-vossen-2014-using,0,0.0459897,"Abzianidze and Bos, 2017). This is problematic, because the ability to accurately capture different modes of generalization is likely key to building systems with robust common sense reasoning (Zhang et al., 2017a; Bauer et al., 2018): Such systems need some source for general knowledge about the world (McCarthy, 1960, 1980, 1986; Minsky, 1974; Schank and Abelson, 1975; Hobbs et al., 1987; Reiter, 1987) and natural language text seems like a prime candidate. It is also surprising, because there is no dearth of data relevant to linguistic expressions of generalization (Doddington et al., 2004; Cybulska and Vossen, 2014b; Friedrich et al., 2015). One obstacle to further progress on generalization is that current frameworks tend to take standard descriptive categories as sharp classes— for example, EPISODIC, GENERIC, HABITUAL for statements and KIND, INDIVIDUAL for noun phrases. This may seem reasonable for sentences like (1a), where Mary clearly refers to a particular individual, or (3a), where Bishops clearly refers to a kind; but natural text is less forgiving (Grimm, 2014, 2016, 2018). Consider the underlined arguments in (4): Do they refer to kinds or individuals? We present a novel semantic framework fo"
Q19-1035,W15-1603,0,0.175509,"his is problematic, because the ability to accurately capture different modes of generalization is likely key to building systems with robust common sense reasoning (Zhang et al., 2017a; Bauer et al., 2018): Such systems need some source for general knowledge about the world (McCarthy, 1960, 1980, 1986; Minsky, 1974; Schank and Abelson, 1975; Hobbs et al., 1987; Reiter, 1987) and natural language text seems like a prime candidate. It is also surprising, because there is no dearth of data relevant to linguistic expressions of generalization (Doddington et al., 2004; Cybulska and Vossen, 2014b; Friedrich et al., 2015). One obstacle to further progress on generalization is that current frameworks tend to take standard descriptive categories as sharp classes— for example, EPISODIC, GENERIC, HABITUAL for statements and KIND, INDIVIDUAL for noun phrases. This may seem reasonable for sentences like (1a), where Mary clearly refers to a particular individual, or (3a), where Bishops clearly refers to a kind; but natural text is less forgiving (Grimm, 2014, 2016, 2018). Consider the underlined arguments in (4): Do they refer to kinds or individuals? We present a novel semantic framework for modeling linguistic expr"
Q19-1035,P16-1166,0,0.174439,"orth, and is similarly inspired by classic theoretical work (Dowty, 1991). In our framework, prototypical episodics, habituals, and generics correspond to sets of properties that the referents of a clause’s head predicate and arguments have—namely, clausal categories are built up from properties of the predicates that head (8) Those firemen are available. (9) Those firemen are strong. This situation is improved upon in the Richer Event Descriptions (RED; O’Gorman et al., 2016) and Situation Entities (SitEnt; Friedrich and Palmer, 2014a,b; Friedrich et al., 2015; Friedrich and Pinkal, 2015b,a; Friedrich et al., 2016) frameworks, which annotate both NPs and entire clauses for genericity. In particular, SitEnt, which is used to annotate MASC (Ide et al., 2010) and Wikipedia, has the nice property that it recognizes the existence of abstract entities and lexical aspectual class of clauses’ main verbs, along with habituality and genericity. This is useful because, in addition to decomposing statements using the genericity of the main referent and event, this framework recognizes that lexical aspect is an independent phenomenon. In practice, however, the annotations produced by this framework are mapped into a"
Q19-1035,D12-1045,0,0.0212284,"., 2018, and see Mathew, 2009; Louis and Nenkova, 2011) and ECB+ (Cybulska and Vossen, 2014a,b) corpora. The ARRAU corpus is mainly intended to capture anaphora resolution, but following the GNOME guidelines (Poesio, 2004), it also annotates entity mentions for a GENERIC attribute, sensitive to whether the mention is in the scope of a relevant semantic operator (e.g., a conditional or quantifier) and whether the nominal refers to a type of object whose genericity is left underspecified, such as a substance. The ECB+ corpus is an extension of the EventCorefBank (ECB; Bejan and Harabagiu, 2010; Lee et al., 2012), which annotates Google News texts for event coreference in accordance with the TimeML specification (Pustejovsky et al., 2003), and is an improvement in the sense that, in addition to entity mentions, event mentions may be labeled with a GENERIC class. The ECB+ approach is useful, since episodic, habitual, and generic statements can straightforwardly be described using combinations of event and entity mention labels. For example, in ECB+, episodic statements involve only non-generic entity and event mentions; habitual statements involve a generic event mention and at least one Background Mos"
Q19-1035,D15-1294,0,0.060483,"Missing"
Q19-1035,P15-1123,0,0.0461654,"Missing"
Q19-1035,D15-1189,0,0.126844,"ories. 506 KIND, and for arguments; PARTICULAR, and DYNAMIC for predicates) token, and their interaction; and (iii) by-annotator random intercepts and random slopes for property with diagonal covariance matrices. The rationale behind (i) is that true should be associated with positive values; false should be associated with negative values; and the confidence rating should control how far from zero the normalized rating is, adjusting for the biases of annotators that responded to a particular item. The resulting response scale is analogous to current approaches to event factuality annotation (Lee et al., 2015; Stanovsky et al., 2017; Rudinger et al., 2018). We obtain a normalized score from these models by setting the Best Linear Unbiased Predictors for the by-annotator random effects to zero and using the Best Linear Unbiased Estimators for the fixed effects to obtain a real-valued label for each token on each property. This procedure amounts to estimating a label for each property and each token based on the ‘‘average annotator.’’ ABSTRACT HYPOTHETICAL, Figure 2: Mean property value for each clause type. interannotator agreement. This pattern is surprising, because our model is based on annotati"
Q19-1035,I11-1068,0,0.03113,") refers to an abstract entity/event that could be construed as both particular-referring, in that it is the service at a specific restaurant, and kind-referring, in that it encompasses all service events at that restaurant; and bus in (7) refers to potentially multiple distinct buses that are grouped into a kind by the fact that they drive a particular line. (5) That vintner makes three different wines. (6) The service at that restaurant is excellent. (7) That bureaucrat takes the 90 bus to work. This deficit is remedied to some extent in the ARRAU (Poesio et al., 2018, and see Mathew, 2009; Louis and Nenkova, 2011) and ECB+ (Cybulska and Vossen, 2014a,b) corpora. The ARRAU corpus is mainly intended to capture anaphora resolution, but following the GNOME guidelines (Poesio, 2004), it also annotates entity mentions for a GENERIC attribute, sensitive to whether the mention is in the scope of a relevant semantic operator (e.g., a conditional or quantifier) and whether the nominal refers to a type of object whose genericity is left underspecified, such as a substance. The ECB+ corpus is an extension of the EventCorefBank (ECB; Bejan and Harabagiu, 2010; Lee et al., 2012), which annotates Google News texts fo"
Q19-1035,J87-3004,0,0.691688,"a long history in both the linguistics and artificial intelligence literatures (see Carlson, 2011; Maienborn et al., 2011; Leslie and Lerner, 2016). Nevertheless, few modern semantic parsers make a systematic distinction (cf. Abzianidze and Bos, 2017). This is problematic, because the ability to accurately capture different modes of generalization is likely key to building systems with robust common sense reasoning (Zhang et al., 2017a; Bauer et al., 2018): Such systems need some source for general knowledge about the world (McCarthy, 1960, 1980, 1986; Minsky, 1974; Schank and Abelson, 1975; Hobbs et al., 1987; Reiter, 1987) and natural language text seems like a prime candidate. It is also surprising, because there is no dearth of data relevant to linguistic expressions of generalization (Doddington et al., 2004; Cybulska and Vossen, 2014b; Friedrich et al., 2015). One obstacle to further progress on generalization is that current frameworks tend to take standard descriptive categories as sharp classes— for example, EPISODIC, GENERIC, HABITUAL for statements and KIND, INDIVIDUAL for noun phrases. This may seem reasonable for sentences like (1a), where Mary clearly refers to a particular individual"
Q19-1035,P10-2013,0,0.0167584,"d to sets of properties that the referents of a clause’s head predicate and arguments have—namely, clausal categories are built up from properties of the predicates that head (8) Those firemen are available. (9) Those firemen are strong. This situation is improved upon in the Richer Event Descriptions (RED; O’Gorman et al., 2016) and Situation Entities (SitEnt; Friedrich and Palmer, 2014a,b; Friedrich et al., 2015; Friedrich and Pinkal, 2015b,a; Friedrich et al., 2016) frameworks, which annotate both NPs and entire clauses for genericity. In particular, SitEnt, which is used to annotate MASC (Ide et al., 2010) and Wikipedia, has the nice property that it recognizes the existence of abstract entities and lexical aspectual class of clauses’ main verbs, along with habituality and genericity. This is useful because, in addition to decomposing statements using the genericity of the main referent and event, this framework recognizes that lexical aspect is an independent phenomenon. In practice, however, the annotations produced by this framework are mapped into a multi-class scheme containing only the high-level GENERIC-HABITUAL-EPISODIC distinction—alongside a conceptually independent distinction among"
Q19-1035,D14-1162,0,0.0844278,"ρ tells us how similar the predictions are to the true values, ignoring scale, and R1 tells us how close the predictions are to the true values, after accounting for variability in the data. We focus mainly on differences in relative performance among our models, but for comparison, stateof-the-art event factuality prediction systems obtain ρ ≈ 0.77 and R1 ≈ 0.57 for predicting event factuality on the predicates we annotate (Rudinger et al., 2018). Learned features For our type-level learned features, we use the 42B uncased GloVe embeddings for the root of the annotated predicate or argument (Pennington et al., 2014). For our tokenlevel learned features, we use 1,024-dimensional ELMo embeddings (Peters et al., 2018). To obtain the latter, the UD-EWT sentences are passed as input to the ELMo three-layered biLM, and we extract the output of all three hidden layers for the root of the annotated predicates and arguments, giving us 3,072-dimensional vectors for each. Labeling models For each protocol, we predict the three normalized properties corresponding to the annotated token(s) using different subsets of the above features. The feature representation is used as the input to a multilayer perceptron with Re"
Q19-1035,N18-1202,0,0.061285,"the predictions are to the true values, after accounting for variability in the data. We focus mainly on differences in relative performance among our models, but for comparison, stateof-the-art event factuality prediction systems obtain ρ ≈ 0.77 and R1 ≈ 0.57 for predicting event factuality on the predicates we annotate (Rudinger et al., 2018). Learned features For our type-level learned features, we use the 42B uncased GloVe embeddings for the root of the annotated predicate or argument (Pennington et al., 2014). For our tokenlevel learned features, we use 1,024-dimensional ELMo embeddings (Peters et al., 2018). To obtain the latter, the UD-EWT sentences are passed as input to the ELMo three-layered biLM, and we extract the output of all three hidden layers for the root of the annotated predicates and arguments, giving us 3,072-dimensional vectors for each. Labeling models For each protocol, we predict the three normalized properties corresponding to the annotated token(s) using different subsets of the above features. The feature representation is used as the input to a multilayer perceptron with ReLU nonlinearity and L1 loss. The number of hidden layers and their sizes are hyperparameters that we"
Q19-1035,W16-5706,0,0.100312,"Missing"
Q19-1035,W04-0210,0,0.0369068,"mpasses all service events at that restaurant; and bus in (7) refers to potentially multiple distinct buses that are grouped into a kind by the fact that they drive a particular line. (5) That vintner makes three different wines. (6) The service at that restaurant is excellent. (7) That bureaucrat takes the 90 bus to work. This deficit is remedied to some extent in the ARRAU (Poesio et al., 2018, and see Mathew, 2009; Louis and Nenkova, 2011) and ECB+ (Cybulska and Vossen, 2014a,b) corpora. The ARRAU corpus is mainly intended to capture anaphora resolution, but following the GNOME guidelines (Poesio, 2004), it also annotates entity mentions for a GENERIC attribute, sensitive to whether the mention is in the scope of a relevant semantic operator (e.g., a conditional or quantifier) and whether the nominal refers to a type of object whose genericity is left underspecified, such as a substance. The ECB+ corpus is an extension of the EventCorefBank (ECB; Bejan and Harabagiu, 2010; Lee et al., 2012), which annotates Google News texts for event coreference in accordance with the TimeML specification (Pustejovsky et al., 2003), and is an improvement in the sense that, in addition to entity mentions, ev"
Q19-1035,Q15-1034,1,0.907783,"Missing"
Q19-1035,P19-1280,1,0.88046,"Missing"
Q19-1035,N18-1067,1,0.879008,"Missing"
Q19-1035,E17-2015,1,0.903307,"Missing"
Q19-1035,D16-1177,1,0.916027,"Missing"
Q19-1035,silveira-etal-2014-gold,0,0.175848,"Missing"
Q19-1035,P17-2056,0,0.136276,"nd for arguments; PARTICULAR, and DYNAMIC for predicates) token, and their interaction; and (iii) by-annotator random intercepts and random slopes for property with diagonal covariance matrices. The rationale behind (i) is that true should be associated with positive values; false should be associated with negative values; and the confidence rating should control how far from zero the normalized rating is, adjusting for the biases of annotators that responded to a particular item. The resulting response scale is analogous to current approaches to event factuality annotation (Lee et al., 2015; Stanovsky et al., 2017; Rudinger et al., 2018). We obtain a normalized score from these models by setting the Best Linear Unbiased Predictors for the by-annotator random effects to zero and using the Best Linear Unbiased Estimators for the fixed effects to obtain a real-valued label for each token on each property. This procedure amounts to estimating a label for each property and each token based on the ‘‘average annotator.’’ ABSTRACT HYPOTHETICAL, Figure 2: Mean property value for each clause type. interannotator agreement. This pattern is surprising, because our model is based on annotations by very lightly trai"
Q19-1035,W17-6944,1,0.837665,"Missing"
Q19-1035,C98-1013,0,\N,Missing
Q19-1035,P10-1005,0,\N,Missing
Q19-1035,Q17-1027,1,\N,Missing
S12-1034,P05-1074,1,0.922837,"Missing"
S12-1034,P99-1071,0,0.149569,"ompression, achieving stateof-the-art quality. 1 Introduction A wide variety of applications in natural language processing can be cast in terms of text-to-text generation. Given input in the form of natural language, a text-to-text generation system produces natural language output that is subject to a set of constraints. Compression systems, for instance, produce shorter sentences. Paraphrases, i.e. differing textual realizations of the same meaning, are a crucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (Barzilay et al., 1999; Barzilay, 2003), query expansion (Anick and Tipirneni, 1999; Riezler et al., 2007), question answering (McKeown, 1979; Ravichandran and Hovy, 2002), sentence compression (Cohn and Lapata, 2008; Zhao et al., 2009), and simplification (Wubben et al., 2012). Paraphrase collections for text-to-text generation have been extracted from a variety of different corpora. Several approaches rely on bilingual paral• We show that using monolingual distributional similarity features improves paraphrase quality beyond what we can achieve with features estimated from bilingual data. • We define distribution"
S12-1034,P08-1077,0,0.666827,"lexical items in its context, such as: “for what verbs do we see with the phrase as the subject?”, or “what adjectives modify the phrase?”. However, when moving to vast text collections or collapsed representations of large text corpora, linguistic annotations can become impractically expensive to produce. A straightforward and widely used solution is to fall back onto lexical n-gram features, e.g. “what words or bigrams have we seen to the left of this phrase?” A substantial body of work has focussed on using this type of feature-set for a variety of purposes in NLP (Lapata and Keller, 2005; Bhagat and Ravichandran, 2008; Lin et al., 2010; Van Durme and Lall, 2010). 2.3 Other Related Work Recently, Chan et al. (2011) presented an initial investigation into combining phrasal paraphrases obtained through bilingual pivoting with monolingual distributional information. Their work investigated a reranking approach and evaluated their method via a substitution task, showing that the two sources of information are complementary and can yield improvements in paraphrase quality when combined. 3 Incorporating Distributional Similarity In order to incorporate distributional similarity information into the paraphrasing s"
S12-1034,D08-1021,1,0.944103,"Missing"
S12-1034,W11-2504,1,0.872905,"Missing"
S12-1034,P05-1033,0,0.0829457,"realizations, matching the lexicalized portions of the rule and generalizing over the nonterminals. Background Approaches to paraphrase extraction differ based on their underlying data source. In Section 2.1 we outline pivot-based paraphrase extraction from bilingual data, while the contextual features used to determine closeness in meaning in monolingual approaches is described in Section 2.2. 2.1 ⌃ h Paraphrase Extraction via Pivoting Following Ganitkevitch et al. (2011), we formulate our paraphrases as a syntactically annotated synchronous context-free grammar (SCFG) (Aho and Ullman, 1972; Chiang, 2005). An SCFG rule has the form: r = C → hf, e, ∼, ϕ ~ i, where the left-hand side of the rule, C, is a nonterminal and the right-hand sides f and e are strings of terminal and nonterminal symbols. There is a one-to-one correspondency between the nonterminals in f and e: each nonterminal symbol in f has to also appear in e. The function ∼ captures this bijective mapping between the nonterminals. Drawing on machine translation terminology, we refer to f as the source and e as the target side of the rule. Each rule is annotated with a feature vector of feature functions ϕ ~ = {ϕ1 ...ϕN } that, using"
S12-1034,C08-1018,0,0.0748158,"rm of natural language, a text-to-text generation system produces natural language output that is subject to a set of constraints. Compression systems, for instance, produce shorter sentences. Paraphrases, i.e. differing textual realizations of the same meaning, are a crucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (Barzilay et al., 1999; Barzilay, 2003), query expansion (Anick and Tipirneni, 1999; Riezler et al., 2007), question answering (McKeown, 1979; Ravichandran and Hovy, 2002), sentence compression (Cohn and Lapata, 2008; Zhao et al., 2009), and simplification (Wubben et al., 2012). Paraphrase collections for text-to-text generation have been extracted from a variety of different corpora. Several approaches rely on bilingual paral• We show that using monolingual distributional similarity features improves paraphrase quality beyond what we can achieve with features estimated from bilingual data. • We define distributional similarity for paraphrase patterns that contain constituent-level gaps, e.g. sim(one JJ instance of NP , a JJ case of NP ). This generalizes over distributional similarity for contiguous phra"
S12-1034,D11-1108,1,0.892318,"Missing"
S12-1034,W12-3134,1,0.928703,"VP NP NP DT+NNP CD NNS JJ twelve cartoons insulting NNP DT the prophet mohammad 12 of the cartoons that are offensive to the prophet mohammad CD NNS DT JJ NNP DT+NNP NP NP VP NP Figure 3: An example of a synchronous paraphrastic derivation, here a sentence compression. Shaded words are deleted in the indicated rule applications. Figure 2 illustrates syntax-constrained pivoting and feature aggregation over multiple foreign language translations for a paraphrase pattern. After the SCFG has been extracted, it can be used within standard machine translation machinery, such as the Joshua decoder (Ganitkevitch et al., 2012). Figure 3 shows an example for a synchronous paraphrastic derivation produced as a result of applying our paraphrase grammar in the decoding process. The approach outlined relies on aligned bilingual texts to identify phrases and patterns that are equivalent in meaning. When extracting paraphrases from monolingual text, we have to rely on an entirely different set of semantic cues and features. 2.2 Monolingual Distributional Similarity Methods based on monolingual text corpora measure the similarity of phrases based on contextual features. To describe a phrase e, we define a set of features t"
S12-1034,D11-1125,0,0.0390613,"s that aim to better describe a rule’s compressive power: on top of the word count features wcount src and wcount tgt and the word count difference feature wcount diff , we add character based count and difference features ccount src , ccount tgt , and ccount diff , as well as logwcount tgt compression ratio features word cr = log wcount src ccount tgt and the analogously defined char cr = log ccount src . For model tuning and decoding we used the Joshua machine translation system (Ganitkevitch et al., 2012). The model weights are estimated using an implementation of the PRO tuning algorithm (Hopkins and May, 2011), with P R E´ CIS as our objective function (Ganitkevitch et al., 2011). The language model used in our paraphraser and the Clarke and Lapata (2008) baseline system is a Kneser-Ney discounted 5-gram model estimated on the Gigaword corpus using the SRILM toolkit (Stolcke, 2002). 260 VBG amod the long-term IN TO DT ⇣ ⌘ ~ syntax the long-term = sig ⇣ investment NN JJ lex-R-investment pos-L-IN-TO lex-L-on-to pos-L-TO lex-L-to dep-det-R-investment pos-R-NN dep-amod-R-investment dep-det-R-NN dep-amod-R-NN syn-gov-NP syn-miss-L-NN ⇣ Figure 6: An example of the syntactic featureset. The phrase “the lo"
S12-1034,2005.mtsummit-papers.11,0,0.0609411,"r falls in the range 0.5 &lt; cr ≤ 0.8. From these, we select 936 sentences for the development set, as well as 560 sentences for a test set that we use to gauge the performance of our system. We contrast our distributional similarity-informed paraphrase system with a pivoting-only baseline, as well as an implementation of Clarke and Lapata (2008)’s state-of-the-art compression model which uses a series of constraints in an integer linear programming (ILP) solver. 4.2 Baseline Paraphrase Grammar We extract our paraphrase grammar from the French–English portion of the Europarl corpus (version 5) (Koehn, 2005). The Berkeley aligner (Liang et al., 2006) and the Berkeley parser (Petrov and Klein, 2007) are used to align the bitext and parse the English side, respectively. The paraphrase grammar is produced using the Hadoop-based Thrax Left Right .. VP PP 25 achieve the long-term the long-term goals 23 det the long-term plans 97 43 revise the long-term holding on to 64 confirmed the long-term the long-term investment 10 .. ⇣ ⌘ ~ ngram the long-term = sig ⇣ NP L-achieve = 25 R-plans = 97 L-revise = 43 R-goals = 23 L-confirmed = 64 R-investment = 10 ⇣ Figure 5: An example of the n-gram feature extractio"
S12-1034,J10-4005,0,0.0317159,"e and e as the target side of the rule. Each rule is annotated with a feature vector of feature functions ϕ ~ = {ϕ1 ...ϕN } that, using a corresponding weight vector ~λ, are combined in a loglinear model to compute the cost of applying r: cost(r) = − N X λi log ϕi . To extract paraphrases we follow the intuition that two English strings e1 and e2 that translate to the same foreign string f can be assumed to have the same meaning, as illustrated in Figure 1.1 First, we use standard machine translation methods to extract a foreign-to-English translation grammar from a bilingual parallel corpus (Koehn, 2010). Then, for each pair of translation rules where the left-hand side C and foreign string f match: r1 = C → hf, e1 , ∼1 , ϕ ~ 1i r2 = C → hf, e2 , ∼2 , ϕ ~ 2 i, we pivot over f to create a paraphrase rule rp : rp = C → he1 , e2 , ∼p , ϕ ~ p i, with a combined nonterminal correspondency function ∼p . Note that the common source side f implies that e1 and e2 share the same set of nonterminal symbols. The paraphrase feature vector ϕ ~ p is computed from the translation feature vectors ϕ ~ 1 and ϕ ~ 2 by following the pivoting idea. For instance, we estimate the conditional paraphrase probability p"
S12-1034,N06-1014,0,0.0135792,"From these, we select 936 sentences for the development set, as well as 560 sentences for a test set that we use to gauge the performance of our system. We contrast our distributional similarity-informed paraphrase system with a pivoting-only baseline, as well as an implementation of Clarke and Lapata (2008)’s state-of-the-art compression model which uses a series of constraints in an integer linear programming (ILP) solver. 4.2 Baseline Paraphrase Grammar We extract our paraphrase grammar from the French–English portion of the Europarl corpus (version 5) (Koehn, 2005). The Berkeley aligner (Liang et al., 2006) and the Berkeley parser (Petrov and Klein, 2007) are used to align the bitext and parse the English side, respectively. The paraphrase grammar is produced using the Hadoop-based Thrax Left Right .. VP PP 25 achieve the long-term the long-term goals 23 det the long-term plans 97 43 revise the long-term holding on to 64 confirmed the long-term the long-term investment 10 .. ⇣ ⌘ ~ ngram the long-term = sig ⇣ NP L-achieve = 25 R-plans = 97 L-revise = 43 R-goals = 23 L-confirmed = 64 R-investment = 10 ⇣ Figure 5: An example of the n-gram feature extraction on an n-gram corpus. Here, “the long-term"
S12-1034,lin-etal-2010-new,0,0.169001,"uch as: “for what verbs do we see with the phrase as the subject?”, or “what adjectives modify the phrase?”. However, when moving to vast text collections or collapsed representations of large text corpora, linguistic annotations can become impractically expensive to produce. A straightforward and widely used solution is to fall back onto lexical n-gram features, e.g. “what words or bigrams have we seen to the left of this phrase?” A substantial body of work has focussed on using this type of feature-set for a variety of purposes in NLP (Lapata and Keller, 2005; Bhagat and Ravichandran, 2008; Lin et al., 2010; Van Durme and Lall, 2010). 2.3 Other Related Work Recently, Chan et al. (2011) presented an initial investigation into combining phrasal paraphrases obtained through bilingual pivoting with monolingual distributional information. Their work investigated a reranking approach and evaluated their method via a substitution task, showing that the two sources of information are complementary and can yield improvements in paraphrase quality when combined. 3 Incorporating Distributional Similarity In order to incorporate distributional similarity information into the paraphrasing system, we need to"
S12-1034,P79-1016,0,0.663091,"be cast in terms of text-to-text generation. Given input in the form of natural language, a text-to-text generation system produces natural language output that is subject to a set of constraints. Compression systems, for instance, produce shorter sentences. Paraphrases, i.e. differing textual realizations of the same meaning, are a crucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (Barzilay et al., 1999; Barzilay, 2003), query expansion (Anick and Tipirneni, 1999; Riezler et al., 2007), question answering (McKeown, 1979; Ravichandran and Hovy, 2002), sentence compression (Cohn and Lapata, 2008; Zhao et al., 2009), and simplification (Wubben et al., 2012). Paraphrase collections for text-to-text generation have been extracted from a variety of different corpora. Several approaches rely on bilingual paral• We show that using monolingual distributional similarity features improves paraphrase quality beyond what we can achieve with features estimated from bilingual data. • We define distributional similarity for paraphrase patterns that contain constituent-level gaps, e.g. sim(one JJ instance of NP , a JJ case o"
S12-1034,W11-1610,1,0.91607,"Missing"
S12-1034,W12-3018,1,0.806725,"Missing"
S12-1034,N07-1051,0,0.0131708,"development set, as well as 560 sentences for a test set that we use to gauge the performance of our system. We contrast our distributional similarity-informed paraphrase system with a pivoting-only baseline, as well as an implementation of Clarke and Lapata (2008)’s state-of-the-art compression model which uses a series of constraints in an integer linear programming (ILP) solver. 4.2 Baseline Paraphrase Grammar We extract our paraphrase grammar from the French–English portion of the Europarl corpus (version 5) (Koehn, 2005). The Berkeley aligner (Liang et al., 2006) and the Berkeley parser (Petrov and Klein, 2007) are used to align the bitext and parse the English side, respectively. The paraphrase grammar is produced using the Hadoop-based Thrax Left Right .. VP PP 25 achieve the long-term the long-term goals 23 det the long-term plans 97 43 revise the long-term holding on to 64 confirmed the long-term the long-term investment 10 .. ⇣ ⌘ ~ ngram the long-term = sig ⇣ NP L-achieve = 25 R-plans = 97 L-revise = 43 R-goals = 23 L-confirmed = 64 R-investment = 10 ⇣ Figure 5: An example of the n-gram feature extraction on an n-gram corpus. Here, “the long-term” is seen preceded by “revise” (43 times) and fol"
S12-1034,P02-1006,0,0.0114452,"ms of text-to-text generation. Given input in the form of natural language, a text-to-text generation system produces natural language output that is subject to a set of constraints. Compression systems, for instance, produce shorter sentences. Paraphrases, i.e. differing textual realizations of the same meaning, are a crucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (Barzilay et al., 1999; Barzilay, 2003), query expansion (Anick and Tipirneni, 1999; Riezler et al., 2007), question answering (McKeown, 1979; Ravichandran and Hovy, 2002), sentence compression (Cohn and Lapata, 2008; Zhao et al., 2009), and simplification (Wubben et al., 2012). Paraphrase collections for text-to-text generation have been extracted from a variety of different corpora. Several approaches rely on bilingual paral• We show that using monolingual distributional similarity features improves paraphrase quality beyond what we can achieve with features estimated from bilingual data. • We define distributional similarity for paraphrase patterns that contain constituent-level gaps, e.g. sim(one JJ instance of NP , a JJ case of NP ). This generalizes over"
S12-1034,P05-1077,0,0.0223875,"d constituent labels for the phrase. The latter are split in governing constituent and missing constituent (with directionality). Figure 6 illustrates the syntax model’s feature extraction for an example phrase occurrence. Using this method we extract distributional signatures for over 12 million 1-to-4-gram phrases. 4.3.3 Locality Sensitive Hashing Collecting distributional signatures for a large number of phrases quickly leads to unmanageably large datasets. Storing the syntax model’s 12 million signatures in a compressed readable format, for instance, requires over 20GB of disk space. Like Ravichandran et al. (2005) and Bhagat and Ravichandran (2008), we rely on locality sensitive hashing (LSH) to make the use of these large collections practical. In order to avoid explicitly computing the feature vectors, which can be memory intensive for frequent phrases, we chose the online LSH variant described by Van Durme and Lall (2010), as implemented in the Jerboa toolkit (Van Durme, 2012). This method, based on the earlier work of Indyk and 261 Motwani (1998) and Charikar (2002), approximates the cosine similarity between two feature vectors based on the Hamming distance in a dimensionalityreduced bitwise repre"
S12-1034,P07-1059,0,0.0799536,"ications in natural language processing can be cast in terms of text-to-text generation. Given input in the form of natural language, a text-to-text generation system produces natural language output that is subject to a set of constraints. Compression systems, for instance, produce shorter sentences. Paraphrases, i.e. differing textual realizations of the same meaning, are a crucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (Barzilay et al., 1999; Barzilay, 2003), query expansion (Anick and Tipirneni, 1999; Riezler et al., 2007), question answering (McKeown, 1979; Ravichandran and Hovy, 2002), sentence compression (Cohn and Lapata, 2008; Zhao et al., 2009), and simplification (Wubben et al., 2012). Paraphrase collections for text-to-text generation have been extracted from a variety of different corpora. Several approaches rely on bilingual paral• We show that using monolingual distributional similarity features improves paraphrase quality beyond what we can achieve with features estimated from bilingual data. • We define distributional similarity for paraphrase patterns that contain constituent-level gaps, e.g. sim("
S12-1034,P10-2043,1,0.863591,"Missing"
S12-1034,P12-1107,0,0.0554392,"Missing"
S12-1034,N12-1078,1,0.848712,"Missing"
S12-1034,P08-1089,0,0.0575905,"Missing"
S12-1034,P09-1094,0,0.0225826,"a text-to-text generation system produces natural language output that is subject to a set of constraints. Compression systems, for instance, produce shorter sentences. Paraphrases, i.e. differing textual realizations of the same meaning, are a crucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (Barzilay et al., 1999; Barzilay, 2003), query expansion (Anick and Tipirneni, 1999; Riezler et al., 2007), question answering (McKeown, 1979; Ravichandran and Hovy, 2002), sentence compression (Cohn and Lapata, 2008; Zhao et al., 2009), and simplification (Wubben et al., 2012). Paraphrase collections for text-to-text generation have been extracted from a variety of different corpora. Several approaches rely on bilingual paral• We show that using monolingual distributional similarity features improves paraphrase quality beyond what we can achieve with features estimated from bilingual data. • We define distributional similarity for paraphrase patterns that contain constituent-level gaps, e.g. sim(one JJ instance of NP , a JJ case of NP ). This generalizes over distributional similarity for contiguous phrases. • We compare di"
S12-1034,J90-1003,0,\N,Missing
S15-1024,P08-1090,0,0.577726,"m the website “Dinners from Hell.” Our results suggest that applying these techniques to a domain-specific dataset may be reasonable way to learn domain-specific scripts. The automatic induction of scripts (Schank and Abelson, 1977) has been the focus of many recent works. In this paper, we employ a variety of these methods to learn Schank and Abelson’s canonical restaurant script, using a novel dataset of restaurant narratives we have compiled from a website called “Dinners from Hell.” Our models learn narrative chains, script-like structures that we evaluate with the “narrative cloze” task (Chambers and Jurafsky, 2008). 1 2 Introduction A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977). Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et"
S15-1024,P09-1068,0,0.325579,"” task (Chambers and Jurafsky, 2008). 1 2 Introduction A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977). Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed. For many specialized applications, however, knowledge of a few relevant scripts may be more useful than knowledge of many irrelevant scripts. With this scenario in mind, we attempt to learn Background Previous work in the automatic induction of scripts or script-like structures has taken a number of different approaches. Regneri et al. (2010) attemp"
S15-1024,P11-1098,0,0.0291101,"lar structures in a probabilistic framework with Hidden Markov Models.) Although Regneri et al. (2010), like us, are concerned with learning pre-specified scripts, our approach is different in that we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learning may appear limited to domains for which a corpus conveniently already exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for"
S15-1024,J90-1003,0,0.34642,"y exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). It is from this body of work that we borrow techniques to apply to the Dinners from Hell dataset. As defined by Chambers and Jurafsky (2008), a narrative chain is “a partially ord"
S15-1024,de-marneffe-etal-2006-generating,0,0.0309946,"Missing"
S15-1024,E12-1034,0,0.444975,"Missing"
S15-1024,P14-5010,0,0.00819738,"arrative cloze test, any event e that was observed during training in fewer than D distinct documents will receive a worse score (i.e. be ranked behind) any event e0 whose count meets the document threshold. 4 Dataset: Dinners From Hell The source of our data for this experiment is a blog called “Dinners From Hell”2 where readers submit stories about their terrible restaurant experiences. For an example story, see Figure 1. To process the raw data, we stripped all HTML and other non-story content from each file and processed the remaining text with the Stanford CoreNLP pipeline version 3.3.1 (Manning et al., 2014). Of the 237 stories obtained, we manually filtered out 94 stories that were “off-topic” (e.g., letters to the webmaster, dinners not at restaurants), leaving a total of 143 stories. The average story length is 352 words. 4.1 Annotation For the purposes of evaluation only, we hired four undergraduates to annotate every non-copular verb in each story as either corresponding to an event “related to the experience of eating in a restaurant” (e.g., ordered a steak), “unrelated to the experience of eating in a restaurant” (e.g., answered the phone), or uncertain. We used the WebAnno platform for an"
S15-1024,N04-1041,0,0.0726893,"al models are introduced by Jans et al. (2012) and we use them here, as well. First, the ordered pmi model, eˆ = arg max e∈V k X pmi(ei , e) + i=1 n X pmi(e, ei ) i=k+1 (4) where C(e1 , e2 ) is asymmetric, i.e., C(e1 , e2 ) counts only cases in which e1 occurs before e2 . Second, the bigram probability model: eˆ = arg max e∈V where p(e2 |e1 ) = metric. k Y p(e|ei ) i=1 C(e1 ,e2 ) C(e1 ,∗) n Y p(ei |e) (5) i=k+1 and C(e1 , e2 ) is asymDiscounting For each model, we add an option for discounting the computed scores. In the case of the two PMI-based models, we use the discount score described in Pantel and Ravichandran (2004) and used by Chambers and Jurafsky (2008). For the bigram probability model, this PMI discount score would be inappropriate, so we instead use absolute discounting. 207 Document Threshold We include a document threshold parameter, D, that ensures that, in any narrative cloze test, any event e that was observed during training in fewer than D distinct documents will receive a worse score (i.e. be ranked behind) any event e0 whose count meets the document threshold. 4 Dataset: Dinners From Hell The source of our data for this experiment is a blog called “Dinners From Hell”2 where readers submit"
S15-1024,E14-1024,0,0.534804,"uction A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977). Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed. For many specialized applications, however, knowledge of a few relevant scripts may be more useful than knowledge of many irrelevant scripts. With this scenario in mind, we attempt to learn Background Previous work in the automatic induction of scripts or script-like structures has taken a number of different approaches. Regneri et al. (2010) attempt to learn the structure of specific scripts by"
S15-1024,P10-1100,1,0.740772,"Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed. For many specialized applications, however, knowledge of a few relevant scripts may be more useful than knowledge of many irrelevant scripts. With this scenario in mind, we attempt to learn Background Previous work in the automatic induction of scripts or script-like structures has taken a number of different approaches. Regneri et al. (2010) attempt to learn the structure of specific scripts by eliciting event sequence descriptions (ESDs) from humans to which they apply multiple sequence alignment (MSA) to yield one global structure per script. (Orr et al. (2014) learn similar structures in a probabilistic framework with Hidden Markov Models.) Although Regneri et al. (2010), like us, are concerned with learning pre-specified scripts, our approach is different in that we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learnin"
S15-1024,H91-1059,0,0.0679119,"at we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learning may appear limited to domains for which a corpus conveniently already exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers"
S15-1024,P13-4001,0,0.0436435,"Missing"
S17-1011,W13-5500,0,0.0785013,"Missing"
S17-1011,N06-2015,0,0.041987,"j and are trained with negative sampling (Goldberg and Levy, 2014). Frame semantics currently used in NLP have a rich history in linguistic literature. Fillmore (1976)’s frames are based on a word’s context and prototypical concepts that an individual word evokes; they intend to represent the meaning of lexical items by mapping words to real world concepts and shared experiences. Frame-based semantics have inspired many semantic annotation schemata and datasets, such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and Verbnet (Schuler, 2005), as well as composite resources (Hovy et al., 2006; Palmer, 2009; Banarescu et al., 2012).1 Thematic Roles and Proto Roles These resources map words to their meanings through discrete/categorically labeled frames and roles; sometimes, as in FrameNet, the roles can be very descriptive (e.g., the D EGREE role for the A F FIRM OR DENY frame), while in other cases, as in PropBank, the roles can be quite general (e.g., A RG 0). Regardless of the actual schema, the roles are based on thematic roles, which map a predicate’s arguments to a semantic representation that makes various semantic distinctions among the arguments (Dowty, 1989).2 Dowty (1991"
S17-1011,E17-2028,1,0.89107,"Missing"
S17-1011,D14-1181,0,0.00257494,"oduction Consider “Bill” in Fig. 1: what is his involvement with the words “would try,” and what does this involvement mean? Word embeddings represent such meaning as points in a real-valued vector space (Deerwester et al., 1990; Mikolov et al., 2013). These representations are often learned by exploiting the frequency that the word cooccurs with contexts, often within a user-defined window (Harris, 1954; Turney and Pantel, 2010). When built from large-scale sources, like Wikipedia or web crawls, embeddings capture general characteristics of words and allow for robust downstream applications (Kim, 2014; Das et al., 2015). Frame semantics generalize word meanings to that of analyzing structured and interconnected labeled “concepts” and abstractions (Minsky, 1974; Fillmore, 1976, 1982). These concepts, or roles, implicitly encode expected properties of that word. In a frame semantic analysis of Fig. 1, the segment “would try” triggers the ATTEMPT frame, filling the expected roles AGENT and G OAL with “Bill” and “the same tactic,” respectively. While frame semantics provide a structured form for analyzing words with crisp, categorically-labeled concepts, the encoded properties and expectations"
S17-1011,P15-1077,0,0.0143566,"nsider “Bill” in Fig. 1: what is his involvement with the words “would try,” and what does this involvement mean? Word embeddings represent such meaning as points in a real-valued vector space (Deerwester et al., 1990; Mikolov et al., 2013). These representations are often learned by exploiting the frequency that the word cooccurs with contexts, often within a user-defined window (Harris, 1954; Turney and Pantel, 2010). When built from large-scale sources, like Wikipedia or web crawls, embeddings capture general characteristics of words and allow for robust downstream applications (Kim, 2014; Das et al., 2015). Frame semantics generalize word meanings to that of analyzing structured and interconnected labeled “concepts” and abstractions (Minsky, 1974; Fillmore, 1976, 1982). These concepts, or roles, implicitly encode expected properties of that word. In a frame semantic analysis of Fig. 1, the segment “would try” triggers the ATTEMPT frame, filling the expected roles AGENT and G OAL with “Bill” and “the same tactic,” respectively. While frame semantics provide a structured form for analyzing words with crisp, categorically-labeled concepts, the encoded properties and expectations are implicit. What"
S17-1011,P14-2050,0,0.208239,"91) argues for proto-thematic roles, e.g. P ROTO -AGENT rather than AGENT, where distinctions in proto-roles are based on clusterings of logical entailments. That is, P ROTO -AGENTs often have certain properties in common, e.g., manipulating other objects or willingly participating in an action; P ROTO -PATIENTs are often changed or affected by some action. By decomposing the meaning of roles into properties or expectations that can be reasoned about, proto-roles can be seen as including a form of vector representation within structured frame semantics. 3 3.1 Skip-Gram as Matrix Factorization Levy and Goldberg (2014b), and subsequently Keerthi et al. (2015), showed how vectors learned under SG with the negative sampling are, under certain conditions, the factorization of (shifted) positive pointwise mutual information. Cotterell et al. (2017) showed that SG is a form of exponential family PCA that factorizes the matrix of word/context cooccurrence counts (rather than shifted positive PMI values). With this interpretation, they generalize SG from matrix to tensor factorization, and provide a theoretical basis for modeling higher-order SG (or additional context, such as morphological features of words) wit"
S17-1011,J05-1004,0,0.0291525,"roduct. Traditionally, the context words i are those words within a small window of j and are trained with negative sampling (Goldberg and Levy, 2014). Frame semantics currently used in NLP have a rich history in linguistic literature. Fillmore (1976)’s frames are based on a word’s context and prototypical concepts that an individual word evokes; they intend to represent the meaning of lexical items by mapping words to real world concepts and shared experiences. Frame-based semantics have inspired many semantic annotation schemata and datasets, such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and Verbnet (Schuler, 2005), as well as composite resources (Hovy et al., 2006; Palmer, 2009; Banarescu et al., 2012).1 Thematic Roles and Proto Roles These resources map words to their meanings through discrete/categorically labeled frames and roles; sometimes, as in FrameNet, the roles can be very descriptive (e.g., the D EGREE role for the A F FIRM OR DENY frame), while in other cases, as in PropBank, the roles can be quite general (e.g., A RG 0). Regardless of the actual schema, the roles are based on thematic roles, which map a predicate’s arguments to a semantic representation that mak"
S17-1011,D16-1177,1,0.900121,"Missing"
S17-1011,N15-1058,1,0.896754,"Missing"
S17-1011,W16-5905,1,0.893286,"Missing"
S17-1011,P15-1173,0,0.0341317,"Missing"
S17-1011,D17-3004,1,0.878664,"Missing"
S17-1011,D15-1243,0,0.0282225,"ta with both the standard sliding context window approach (§3) and the frame-based approach (§4). Upper numbers (Roman) are for newswire; lower numbers (italics) are Wikipedia. For both corpora, 800 total FrameNet frame types and 5100 PropBank frame types are extracted. to enable any arbitrary dimensional tensor factorization, as described in §3.2. We learn 100dimensional embeddings for words that appear at least 100 times from 15 negative samples.4 The implementation is available at https://github. com/fmof/tensor-factorization. Metric We evaluate our learned (trigger) embeddings w via QVEC (Tsvetkov et al., 2015). QVEC uses canonical correlation analysis to measure the Pearson correlation between w and a collection of oracle lexical vectors o. These oracle vectors are derived from a human-annotated resource. For QVEC , higher is better: a higher score indicates w more closely correlates (positively) with o. Evaluating Semantic Content with SPR Motivated by Dowty (1991)’s proto-role theory, Reisinger et al. (2015), with a subsequent expansion by White et al. (2016), annotated thousands of predicate-argument pairs (v, a) with (boolean) applicability and (ordinal) likelihoods of wellmotivated semantic pr"
S17-1011,D15-1306,0,0.0607359,"Missing"
S17-1011,Q15-1034,1,\N,Missing
S17-1011,W13-5503,0,\N,Missing
S18-2017,W09-1704,0,0.0193066,"tanh(ct ) ∈ (−1, 1)D . Therefore, extra work is needed to ensure h0t ∈ (−1, 1)D . For this purpose, we follow the recipe3 : • Sample h00t ∈ (−1, 1)D by independently sampling each entry from an uniform distribution over (−1, 1); • Sample a scalar λt ∈ (0, 1) from a Beta distribution B(α, β) where α and β are hyperparameters to be tuned; • Compute h0t = ht + λt (h00t − ht ) such that h0t ∈ (−1, 1)D lies on the line segment between ht and h00t . 4 Related Work Cross-lingual information extraction has drawn a great deal of attention from researchers. Some (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji and Nothman, 2016) worked in closed domains, i.e. on a predefined set of events and/or entities, Zhang et al. (2017b) explored this problem in open domain and their attentional encoder-decoder model significantly outperformed a baseline system that does translation and parsing in a pipeline. Zhang et al. (2017c) further improved the results by inventing a hierarchical architecture that learns to first predict the next semantic structure tag and then select a tagdependent decoder for token generation. Orthogonal to these efforts, Halo aims to help all neural models on"
S18-2017,P07-2045,0,0.00961903,"Missing"
S18-2017,D15-1166,0,0.0628326,"016). Halo differs from these methods because 1) it makes use of the task-specific information— vocabulary is partitioned by semantic structure tags; and 2) it makes use of the human belief that the hidden representations of tokens with the same semantic structure tag should stay close to each other. Some 5 ers are low resourced, meaning they have much fewer samples and lower token/type ratios. 5.2 Before applying our Halo technique, we first improved the current state-of-the-art neural model of Zhang et al. (2017c) by using residual connections (He et al., 2016) and multiplicative attention (Luong et al., 2015), which effectively improved the model performance. We refer to the model of Zhang et al. (2017c) and our improved version as ModelZ and ModelP respectively5 . Experiments We evaluate our method on several real-world CLIE datasets measured by BLEU (Papineni et al., 2002) and F1, as proposed by Zhang et al. (2017b). For the generated linearized PredPatt outputs and their references, the former metric4 measures their n-gram similarity, and the latter measures their token-level overlap. In fact, F1 is computed separately for predicate and argument, as F1 P RED and F1 A RG respectively. 5.1 Model"
S18-2017,P02-1040,0,0.102246,"should stay close to each other. Some 5 ers are low resourced, meaning they have much fewer samples and lower token/type ratios. 5.2 Before applying our Halo technique, we first improved the current state-of-the-art neural model of Zhang et al. (2017c) by using residual connections (He et al., 2016) and multiplicative attention (Luong et al., 2015), which effectively improved the model performance. We refer to the model of Zhang et al. (2017c) and our improved version as ModelZ and ModelP respectively5 . Experiments We evaluate our method on several real-world CLIE datasets measured by BLEU (Papineni et al., 2002) and F1, as proposed by Zhang et al. (2017b). For the generated linearized PredPatt outputs and their references, the former metric4 measures their n-gram similarity, and the latter measures their token-level overlap. In fact, F1 is computed separately for predicate and argument, as F1 P RED and F1 A RG respectively. 5.1 Model Implementation 5.3 Experimental Details In experiments, instead of using the full vocabularies shown in table 1, we set a minimum count threshold for each dataset, to replace the rare words by a special out-of-vocabulary symbol. These thresholds were tuned on dev sets. T"
S18-2017,P09-1048,0,0.01758,"Missing"
S18-2017,D16-1177,1,0.847053,"Missing"
S18-2017,E17-2011,1,0.885386,"Missing"
S18-2017,W11-1215,0,0.0126728,"∈ (−1, 1)D . Therefore, extra work is needed to ensure h0t ∈ (−1, 1)D . For this purpose, we follow the recipe3 : • Sample h00t ∈ (−1, 1)D by independently sampling each entry from an uniform distribution over (−1, 1); • Sample a scalar λt ∈ (0, 1) from a Beta distribution B(α, β) where α and β are hyperparameters to be tuned; • Compute h0t = ht + λt (h00t − ht ) such that h0t ∈ (−1, 1)D lies on the line segment between ht and h00t . 4 Related Work Cross-lingual information extraction has drawn a great deal of attention from researchers. Some (Sudo et al., 2004; Parton et al., 2009; Ji, 2009; Snover et al., 2011; Ji and Nothman, 2016) worked in closed domains, i.e. on a predefined set of events and/or entities, Zhang et al. (2017b) explored this problem in open domain and their attentional encoder-decoder model significantly outperformed a baseline system that does translation and parsing in a pipeline. Zhang et al. (2017c) further improved the results by inventing a hierarchical architecture that learns to first predict the next semantic structure tag and then select a tagdependent decoder for token generation. Orthogonal to these efforts, Halo aims to help all neural models on this task, rather tha"
S18-2017,I17-1084,1,0.828392,"Missing"
S18-2017,L16-1521,0,0.0664641,"thresholds were tuned on dev sets. The Beta distribution is very flexible. In general, its variance is a decreasing function of α + β, and when α + β is fixed, the mean is an increasing function of α. In our experiments, we fixed α + β = 20 and only lightly tuned α on dev sets. Optimal values of α stay close to 1. Datasets Multiple datasets were used to demonstrate the effectiveness of our proposed method, where one sample in each dataset is a source language sentence paired with its linearized English PredPatt output. These datasets were first introduced as the DARPA LORELEI Language Packs (Strassel and Tracey, 2016), and then used for this task by Zhang et al. (2017b,c). As shown in table 1, the C HINESE dataset has almost one million training samples and a high token/type ratio, while the oth5.4 Results As shown in Table 2, ModelP outperforms ModelZ on all the datasets measured by all the metrics, except for F1 P RED on C HINESE dataset. Our Halo technique consistently boosts the model performance of M ODEL P except for F1 P RED on T URKISH. 4 The MOSES implementation (Koehn et al., 2007) was used as in all the previous work on this task. 5 145 Z stands for Zhang and P for Plus. Additionally, experiment"
S18-2017,C04-1127,0,0.815218,"te tokens with the same semantic structure tag (either predicate or argument) as the centroid. We call this technique Halo, because the process of each hidden state taking up its surroundings is analogous to how the halo is formed around the sun. The method is believed to help the model generalize better, by learning more semantics-aware and noise-insensitive hidden states without introducing extra parameters. Introduction Cross-lingual information extraction (CLIE) is the task of distilling and representing factual information in a target language from the textual input in a source language (Sudo et al., 2004; Zhang et al., 2017b). For example, Fig. 1 illustrates a pair of input Chinese sentence and its English predicateargument information1 , where predicate and argument are well used semantic structure tags. It is of great importance to solve the task, as to provide viable solutions to extracting information from the text of languages that suffer from no or little existing information extraction tools. Neural models have empirically proven successful in this task (Zhang et al., 2017b,c), but still remain unsatisfactory in low resource (i.e. small number of training samples) settings. These neura"
S18-2022,E17-1075,0,0.0994966,"Missing"
S18-2022,D15-1103,0,0.284133,"Missing"
S18-2022,D17-1284,0,0.0693452,"ncode entity e and its context x into feature vectors, and we consider both sentence-level context xs and document-level context xd in contrast to prior work which only takes sentence-level context (Gillick et al., 2014; Shimaoka et al., 2017). 1 gd (xd ) = relu(Wd1 tanh(Wd2 DM(xd ))), (7) where DM is a pretrained distributed memory model (Le and Mikolov, 2014) which converts the document-level context into a distributed representation. Wd1 and Wd2 are weight matrices. 1 Document-level context has also been exploited in Yaghoobzadeh and Sch¨utze (2015); Yang et al. (2016); Karn et al. (2017); Gupta et al. (2017). 174 2.3 Adaptive Thresholds Loose Macro: In prior work, a fixed threshold (rt = 0.5) is used for classification of all types (Ling and Weld, 2012; Shimaoka et al., 2017). We instead assign a different threshold to each type that is optimized to maximize the overall strict F1 on the dev set. We show the definition of strict F1 in Section 3.1. 3 N 1 X |Tˆi ∩ Ti | P = N |Tˆi | i=1 R= i=1 Loose Micro: Experiments Train Dev Test Types 251,039 84,078 2,000,000 2,202 2,000 10,000 8,963 13,766 563 89 93 113 3.2 Hyperparameters We use open-source GloVe vectors (Pennington et al., 2014) trained on Com"
S18-2022,E17-2119,0,0.0245922,"Missing"
S18-2022,D16-1144,0,0.651474,"ˆ i=1 |Ti ∩ Ti | P N ˆ i=1 |Ti | PN ˆ i=1 |Ti ∩ Ti | R= P N i=1 |Ti | P = We conduct experiments on three publicly available datasets.2 Table 1 shows the statistics of these datasets. OntoNotes: Gillick et al. (2014) sampled sentences from OntoNotes (Weischedel et al., 2011) and annotated entities in these sentences using 89 types. We use the same train/dev/test splits in Shimaoka et al. (2017). Document-level contexts are retrieved from the original OntoNotes corpus. BBN: Weischedel and Brunstein (2005) annotated entities in Wall Street Journal using 93 types. We use the train/test splits in Ren et al. (2016b) and randomly hold out 2,000 pairs for dev. Document contexts are retrieved from the original corpus. FIGER: Ling and Weld (2012) sampled sentences from 780k Wikipedia articles and 434 news reports to form the train and test data respectively, and annotated entities using 113 types. The splits we use are the same in Shimaoka et al. (2017). OntoNotes BBN FIGER N 1 X |Tˆi ∩ Ti | N |Ti | N 1 X ˆ δ(Ti = Ti ) N i=1 3 For PLE (Ren et al., 2016b), we were unable to replicate the performance benefits reported in their work, so we report the results after running their codebase. 2 We made the source"
S18-2022,W16-1313,0,0.332997,"Missing"
S18-2022,D15-1166,0,0.172852,"Missing"
S18-2022,E17-1119,0,0.104033,"application to downstream tasks, recent work on entity typing has moved beyond standard coarse types towards finer-grained semantic types with richer ontologies (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015). Rather than assuming an entity can be uniquely categorized into a single type, the task has been approached as a multi-label classification problem: e.g., in “... became a top seller ... Monopoly is played in 114 countries. ...” (Figure 1), “Monopoly” is considered both a game as well as a product. The state-of-the-art approach (Shimaoka et al., 2017) for fine-grained entity typing employs an attentive neural architecture to learn representations of the entity mention as well as its context. These representations are then combined with hand-crafted features (e.g., lexical and syntactic features), and fed into a linear classifier with a fixed threshold. While this approach outperforms previous approaches which only use sparse binary features (Ling and Weld, 2012; Gillick et al., 2014) or distributed representations (Yogatama et al., 2015), it has a few drawbacks: (1) the representations of left and right contexts are learnt independently, i"
S18-2022,C16-1017,0,0.503215,"... /other /other /other/health /other/health/treatment /organization /organization/company /organization /organization/company B Bozell joins Backer Spielvogel Bates and Ogilvy Group as U.S. agencies with interests in Korean agencies. Table 2: Examples showing the improvement brought by document-level contexts and dot-product attention. Entities are shown in the green box. The gray boxes visualize attention weights (darkness) on context tokens. Approach Approach Strict Macro Micro B INARY(Gillick et al., 2014) K WSABIE(Yogatama et al., 2015) N/A N/A N/A N/A 70.01 72.98 PLE(Ren et al., 2016b) Ma et al. (2016) AFET(Ren et al., 2016a) F NET(Abhishek et al., 2017) N EURAL(Shimaoka et al., 2017) w/o Hand-crafted features 51.61 49.30 55.10 52.20 51.74 47.15 67.39 68.23 71.10 68.50 70.98 65.53 62.38 61.27 64.70 63.30 64.91 58.25 O UR A PPROACH w/o Adaptive thresholds w/o Document-level contexts w/ Hand-crafted features 55.52 53.49 53.17 54.40 73.33 73.11 72.14 73.13 67.61 66.78 66.51 66.89 Strict Macro Micro K WSABIE(Yogatama et al., 2015) N/A Attentive(Shimaoka et al., 2016) 58.97 F NET(Abhishek et al., 2017) 65.80 N/A 77.96 81.20 72.25 74.94 77.40 Ling and Weld (2012) PLE(Ren et al., 2016b) Ma et al."
S18-2022,D15-1083,0,0.107698,"Missing"
S18-2022,N16-1174,0,0.0372348,"ur model contains three encoders which encode entity e and its context x into feature vectors, and we consider both sentence-level context xs and document-level context xd in contrast to prior work which only takes sentence-level context (Gillick et al., 2014; Shimaoka et al., 2017). 1 gd (xd ) = relu(Wd1 tanh(Wd2 DM(xd ))), (7) where DM is a pretrained distributed memory model (Le and Mikolov, 2014) which converts the document-level context into a distributed representation. Wd1 and Wd2 are weight matrices. 1 Document-level context has also been exploited in Yaghoobzadeh and Sch¨utze (2015); Yang et al. (2016); Karn et al. (2017); Gupta et al. (2017). 174 2.3 Adaptive Thresholds Loose Macro: In prior work, a fixed threshold (rt = 0.5) is used for classification of all types (Ling and Weld, 2012; Shimaoka et al., 2017). We instead assign a different threshold to each type that is optimized to maximize the overall strict F1 on the dev set. We show the definition of strict F1 in Section 3.1. 3 N 1 X |Tˆi ∩ Ti | P = N |Tˆi | i=1 R= i=1 Loose Micro: Experiments Train Dev Test Types 251,039 84,078 2,000,000 2,202 2,000 10,000 8,963 13,766 563 89 93 113 3.2 Hyperparameters We use open-source GloVe vectors"
S18-2022,D14-1162,0,0.0810725,"Karn et al. (2017); Gupta et al. (2017). 174 2.3 Adaptive Thresholds Loose Macro: In prior work, a fixed threshold (rt = 0.5) is used for classification of all types (Ling and Weld, 2012; Shimaoka et al., 2017). We instead assign a different threshold to each type that is optimized to maximize the overall strict F1 on the dev set. We show the definition of strict F1 in Section 3.1. 3 N 1 X |Tˆi ∩ Ti | P = N |Tˆi | i=1 R= i=1 Loose Micro: Experiments Train Dev Test Types 251,039 84,078 2,000,000 2,202 2,000 10,000 8,963 13,766 563 89 93 113 3.2 Hyperparameters We use open-source GloVe vectors (Pennington et al., 2014) trained on Common Crawl 840B with 300 dimensions to initialize word embeddings used in all encoders. All weight parameters are sampled from U(−0.01, 0.01). The encoder for sentence-level context is a 2-layer bidirectional RNN with 200 hidden units. The DM output size is 50. Sizes of Wa , Wd1 and Wd2 are 200×300, 70×50, and 50×70 respectively. Adam optimizer (Kingma and Ba, 2014) and mini-batch gradient is used for optimization. Batch size is 200. Dropout (rate=0.5) is applied to three feature functions. To avoid overfitting, we choose models which yield the best strict F1 on dev sets. 3.3 Res"
S18-2022,N13-1071,0,0.0648779,"Missing"
S18-2022,P15-2048,0,0.649898,"” (Figure 1), “Monopoly” is considered both a game as well as a product. The state-of-the-art approach (Shimaoka et al., 2017) for fine-grained entity typing employs an attentive neural architecture to learn representations of the entity mention as well as its context. These representations are then combined with hand-crafted features (e.g., lexical and syntactic features), and fed into a linear classifier with a fixed threshold. While this approach outperforms previous approaches which only use sparse binary features (Ling and Weld, 2012; Gillick et al., 2014) or distributed representations (Yogatama et al., 2015), it has a few drawbacks: (1) the representations of left and right contexts are learnt independently, ignoring their mutual connection; (2) the attention on context is computed solely upon the context, considering no alignment to the entity; (3) document-level contexts which could be useful in classification are not exploited; and (4) hand-crafted features heavily rely on system or human annotations. To overcome these drawbacks, we propose a neural architecture (Figure 1) which learns more context-aware representations by using a better attention mechanism and taking advantage of semantic dis"
S18-2022,C12-2133,0,0.207282,"Missing"
S18-2023,W07-1417,0,0.0862764,"2.91 85.67 91.21 86.92 94.46 93.20 96.34 +47.99 +46.63 +34.62 +34.56 +30.18 +23.73 +21.75 +11.29 +10.65 +8.75 +5.13 +2.87 +2.77 +0.44 +0.34 +0.11 Table 4: NLI accuracies on the SPR development data; each property appears in 956 hypotheses. the word level (MacCartney et al., 2008; MacCartney, 2009). Beyond bag-of-words, are there multi-word expressions or syntactic phenomena that might encode label biases? 7 Related Work Lexical Semantics Non-semantic information to help NLI In NLI datasets, non-semantic linguistic features have been used to improve NLI models. Vanderwende and Dolan (2006) and Blake (2007) demonstrate how sentence structure alone can provide a high signal for NLI. Instead of using external sources of knowledge, which was a common trend at the time, Blake (2007) improved results on RTE by combining syntactic features. More recently, BarHaim et al. (2015) introduce an inference formalism based on syntactic-parse trees. A survey of gains (Table 4) in the SPR dataset suggest a number of its property-driven hypotheses, such as X was sentient in [the event], can be accurately guessed based on lexical semantics (background knowledge learned from training) of the argument. For example,"
S18-2023,D15-1075,0,0.784348,"y-class prior across most of the ten examined datasets. We examine whether: (1) hypotheses contain statistical irregularities within each 180 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 180–191 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics entailment class that are “giveaways” to a welltrained hypothesis-only model, (2) the way in which an NLI dataset is constructed is related to how prone it is to this particular weakness, and (3) the majority baselines might not be as indicative of “the difficulty of the task” (Bowman et al., 2015) as previously thought. We are not the first to consider the inherent difficulty of NLI datasets. For example, MacCartney (2009) used a simple bag-of-words model to evaluate early iterations of Recognizing Textual Entailment (RTE) challenge sets.1 Concerns have been raised previously about the hypotheses in the Stanford Natural Language Inference (SNLI) dataset specifically, such as by Rudinger et al. (2017) and in unpublished work.2 Here, we survey of large number of existing NLI datasets under the lens of a hypothesis-only model.3 Concurrently, Tsuchiya (2018) and Gururangan et al. (2018) si"
S18-2023,W17-0908,0,0.0416078,"Missing"
S18-2023,W03-0906,0,0.293774,"Missing"
S18-2023,N18-2017,0,0.1477,"Missing"
S18-2023,D17-1070,0,0.282344,"easoning. Goldberg (2017) suggests that “solving [NLI] perfectly entails human level understanding of language”, and Nangia et al. (2017) argue that “in order for a system to perform well at natural language inference, it needs to handle nearly the full complexity of natural language understanding.” However, if biases in NLI datasets, especially those that do not reflect commonsense knowledge, allow models to achieve high levels of performance without needing to reason about hypotheses based on corresponding contexts, our current datasets may fall short of these goals. 3 Methodology We modify Conneau et al. (2017)’s InferSent method to train a neural model to classify just the hypotheses. We choose InferSent because it performed competitively with the best-scoring systems on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), while being representative of the types of neural architectures commonly used for NLI tasks. InferSent uses a BiLSTM encoder, and constructs a sentence representation by max-pooling over its hidden states. This sentence representation of a hypothesis is used as input to a MLP classifier to predict the NLI tag. Motivation Our approach is inspired by recent"
S18-2023,I17-1011,0,0.115062,"Missing"
S18-2023,W02-0109,0,0.0925754,"he-art performance by ignoring the narrative and training a linear classifier with features related to the writing style of the two potential endings, rather than their content. It has also been shown that features focusing on sentence length, sentiment, and negation are sufficient for achieving high accuracy on this dataset (Schwartz et al., 2017a; Cai et al., 2017; Bugert et al., 2017). NLI is often viewed as an integral part of NLU. Condoravdi et al. (2003) argue that it is a necessary metric for evaluating an NLU system, since it We preprocess each recast dataset using the NLTK tokenizer (Loper and Bird, 2002). Following Conneau et al. (2017), we map the resulting tokens to 300-dimensional GloVe vectors (Pennington et al., 2014) trained on 840 billion tokens from the Common Crawl, using the GloVe OOV vector for unknown words. We optimize via SGD, with an initial learning rate of 0.1, and decay rate of 0.99. We allow at most 20 epochs of training with optional early stopping according to the following policy: when the accuracy on the development set decreases, we divide the learning rate by 5 and stop training when learning rate is &lt; 10−5 . 4 1 MacCartney (2009), Ch. 2.2: “the RTE1 test suite is the"
S18-2023,U15-1007,0,0.0152438,"gender, race, and ethnic stereotypes (Rudinger et al., 2017). Furthermore, Zhang et al. (2017) commented that this “elicitation protocols can lead to biased responses unlikely to contain a wide range of possible common-sense inferences.” Add-one RTE This mixed-genre dataset tests whether NLI systems can understand adjectivenoun compounds (Pavlick and Callison-Burch, 2016). Premise sentences were extracted from Annotated Gigaword (Napoles et al., 2012), image captions (Young et al., 2014), the Internet Argument Corpus (Walker et al., 2012), and fictional stories from the GutenTag dataset (Mac Kim and Cassidy, 2015). To create hypotheses, adjectives were removed or inserted before nouns in a premise, and crowd-sourced workers were asked to provide reliable labels (ENTAILED, NOTENTAILED ). Multi-NLI Multi-NLI is a recent expansion of SNLI aimed to add greater diversity to the existing dataset (Williams et al., 2017). Premises in MultiNLI can originate from fictional stories, personal letters, telephone speech, and a 9/11 report. 4.2 SciTail Recently released, SciTail is an NLI dataset created from 4th grade science questions and multiple-choice answers (Khot et al., 2018). Hypotheses are assertions conver"
S18-2023,S13-2045,0,0.0174857,"e, are there correlation between sentence length and label class in these data sets? Is there a particular construction method that minimizes the amount of “give-away” words present in the dataset? And lastly, our study is another in a line of research which looks for irregularities at 187 Acknowledgements NLI’s resurgence Starting in the mid-2000’s, multiple community-wide shared tasks focused on NLI, then commonly referred to as RTE, i.e, recognizing textual entailment. Starting with Dagan et al. (2006), there have been eight iterations of the PASCAL RTE challenge with the most recent being Dzikovska et al. (2013).9 NLI datasets were relatively small, ranging from thousands to tens of thousands of labeled sentence pairs. In turn, NLI models often used alignmentbased techniques (MacCartney et al., 2008) or manually engineered features (Androutsopoulos and Malakasiotis, 2010). Bowman et al. (2015) sparked a renewed interested in NLI, particularly among deep-learning researchers. By developing and releasing a large NLI dataset containing over 550K examples, Bowman et al. (2015) enabled the community to successfully apply deep learning models to the NLI problem. 8 This work was supported by Johns Hopkins U"
S18-2023,D08-1084,0,0.348037,"aware used in volitional physically existed caused sentient existed before changed chang. state existed after existed during location physical contact chang. possession moved stationary during 88.70 77.30 87.45 87.97 82.11 94.35 80.23 72.18 71.76 79.29 90.06 93.83 89.33 94.87 93.51 96.44 59.94 52.72 64.96 65.38 63.08 76.26 65.90 64.85 64.85 72.91 85.67 91.21 86.92 94.46 93.20 96.34 +47.99 +46.63 +34.62 +34.56 +30.18 +23.73 +21.75 +11.29 +10.65 +8.75 +5.13 +2.87 +2.77 +0.44 +0.34 +0.11 Table 4: NLI accuracies on the SPR development data; each property appears in 956 hypotheses. the word level (MacCartney et al., 2008; MacCartney, 2009). Beyond bag-of-words, are there multi-word expressions or syntactic phenomena that might encode label biases? 7 Related Work Lexical Semantics Non-semantic information to help NLI In NLI datasets, non-semantic linguistic features have been used to improve NLI models. Vanderwende and Dolan (2006) and Blake (2007) demonstrate how sentence structure alone can provide a high signal for NLI. Instead of using external sources of knowledge, which was a common trend at the time, Blake (2007) improved results on RTE by combining syntactic features. More recently, BarHaim et al. (201"
S18-2023,N13-1092,1,0.873168,"Missing"
S18-2023,marelli-etal-2014-sick,0,0.113015,"troupe and so does Osaka Table 1: Basic statistics about the NLI datasets we consider. ‘Size’ refers to the total number of labeled premisehypothesis pairs in each dataset (for datasets with &gt; 100K examples, numbers are rounded down to the nearest 25K). The ‘Creation Protocol’ column indicates how the dataset was created. The ‘Class’ column reports the number of class labels/tags. The last column shows an example hypothesis from each dataset. 4.1 Sentences Involving Compositional Knowledge (SICK) To evaluate how well compositional distributional semantic models handle “challenging phenomena”, Marelli et al. (2014) introduced SICK, which used rules to expand or normalize existing premises to create more difficult examples. Workers were asked to label the relatedness of these resulting pairs, and these labels were then converted into the same three-way label space as SNLI and Multi-NLI. Human Elicited In cases where humans were given a context and asked to generate a corresponding hypothesis and label, we consider these datasets to be elicited. Although we consider only two such datasets, they are the largest datasets included in our study and are currently popular amongst researchers. The elicited NLI d"
S18-2023,W05-1208,0,0.058354,"erspective of NLU, one should be aware of what the hypothesis-only baseline is capable of, to recognize those cases where access to the context is required and therefore more interesting under NLI. 6.5 Proto-Role World Knowledge and NLI As mentioned earlier, hypothesis-only models that perform without exploiting statistical irregularities may be performing NLI only in the sense that it is understanding language based on prior background knowledge. Here, we take the approach that interesting NLI should depend on both premise and hypotheses. Prior work in NLI reflect this approach. For example, Glickman and Dagan (2005) argue that “the notion of textual entailment is relevant only” for hypothesis that are not world facts, e.g. “Paris is the capital of France.” Glickman et al. (2005a,b), introduce a probabilistic framework for NLI where the premise entails a hypothesis if, and only if, the probability of the hypothesis being true increases as a result of the premise. Open Questions There may remain statistical irregularities, which we leave for future work to explore. For example, are there correlation between sentence length and label class in these data sets? Is there a particular construction method that m"
S18-2023,N16-1098,0,0.171878,"on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), while being representative of the types of neural architectures commonly used for NLI tasks. InferSent uses a BiLSTM encoder, and constructs a sentence representation by max-pooling over its hidden states. This sentence representation of a hypothesis is used as input to a MLP classifier to predict the NLI tag. Motivation Our approach is inspired by recent studies that show how biases in an NLU dataset allow models to perform well on the task without understanding the meaning of the text. In the Story Cloze task (Mostafazadeh et al., 2016, 2017), a model is presented with a short four-sentence narrative and asked to complete it by choosing one of two suggested concluding sentences. While the task is presented as a new common-sense reasoning framework, Schwartz et al. (2017b) achieved state-of-the-art performance by ignoring the narrative and training a linear classifier with features related to the writing style of the two potential endings, rather than their content. It has also been shown that features focusing on sentence length, sentiment, and negation are sufficient for achieving high accuracy on this dataset (Schwartz et"
S18-2023,N18-2082,1,0.898475,"Missing"
S18-2023,D12-1071,0,0.129724,"Missing"
S18-2023,W14-2901,1,0.907612,"Missing"
S18-2023,W17-5301,0,0.114222,"ists of two fragments of natural language text (a context, also known as a premise, and a hypothesis), and a label indicating the entailment relation between the two fragments (e.g., ENTAILMENT, NEUTRAL , CONTRADICTION ). Computationally, the task of NLI is to predict an entailment relation label (output) given a premise-hypothesis pair (input), i.e., to determine whether the truth of the hypothesis follows from the truth of the premise (Dagan et al., 2006, 2013). When these NLI datasets are constructed to facilitate the training and evaluation of natural language understanding (NLU) systems (Nangia et al., 2017), it is tempting to claim that systems achieving high accuracy on such datasets have successfully “understood” natural language or at least a logical relationship between a premise and hypothesis. While this paper does not attempt to We present the results of a hypothesis-only baseline across ten NLI-style datasets and advocate for its inclusion in future dataset reports. We find that this baseline can perform above the majority-class prior across most of the ten examined datasets. We examine whether: (1) hypotheses contain statistical irregularities within each 180 Proceedings of the 7th Join"
S18-2023,Q15-1034,1,0.901976,"Missing"
S18-2023,W12-3018,1,0.847547,"Missing"
S18-2023,W17-1609,1,0.884341,"Missing"
S18-2023,P16-1204,0,0.0675802,"ourced workers a premise sentence (sourced from Flickr image captions), and asked them to generate a corresponding hypothesis sentence for each of the three labels (ENTAILMENT, NEUTRAL , CONTRADICTION ). SNLI is known to contain stereotypical biases based on gender, race, and ethnic stereotypes (Rudinger et al., 2017). Furthermore, Zhang et al. (2017) commented that this “elicitation protocols can lead to biased responses unlikely to contain a wide range of possible common-sense inferences.” Add-one RTE This mixed-genre dataset tests whether NLI systems can understand adjectivenoun compounds (Pavlick and Callison-Burch, 2016). Premise sentences were extracted from Annotated Gigaword (Napoles et al., 2012), image captions (Young et al., 2014), the Internet Argument Corpus (Walker et al., 2012), and fictional stories from the GutenTag dataset (Mac Kim and Cassidy, 2015). To create hypotheses, adjectives were removed or inserted before nouns in a premise, and crowd-sourced workers were asked to provide reliable labels (ENTAILED, NOTENTAILED ). Multi-NLI Multi-NLI is a recent expansion of SNLI aimed to add greater diversity to the existing dataset (Williams et al., 2017). Premises in MultiNLI can originate from fictio"
S18-2023,K17-1004,0,0.160438,"ation by max-pooling over its hidden states. This sentence representation of a hypothesis is used as input to a MLP classifier to predict the NLI tag. Motivation Our approach is inspired by recent studies that show how biases in an NLU dataset allow models to perform well on the task without understanding the meaning of the text. In the Story Cloze task (Mostafazadeh et al., 2016, 2017), a model is presented with a short four-sentence narrative and asked to complete it by choosing one of two suggested concluding sentences. While the task is presented as a new common-sense reasoning framework, Schwartz et al. (2017b) achieved state-of-the-art performance by ignoring the narrative and training a linear classifier with features related to the writing style of the two potential endings, rather than their content. It has also been shown that features focusing on sentence length, sentiment, and negation are sufficient for achieving high accuracy on this dataset (Schwartz et al., 2017a; Cai et al., 2017; Bugert et al., 2017). NLI is often viewed as an integral part of NLU. Condoravdi et al. (2003) argue that it is a necessary metric for evaluating an NLU system, since it We preprocess each recast dataset usin"
S18-2023,P15-2067,1,0.87975,"Missing"
S18-2023,W17-0907,0,0.0218053,"ation by max-pooling over its hidden states. This sentence representation of a hypothesis is used as input to a MLP classifier to predict the NLI tag. Motivation Our approach is inspired by recent studies that show how biases in an NLU dataset allow models to perform well on the task without understanding the meaning of the text. In the Story Cloze task (Mostafazadeh et al., 2016, 2017), a model is presented with a short four-sentence narrative and asked to complete it by choosing one of two suggested concluding sentences. While the task is presented as a new common-sense reasoning framework, Schwartz et al. (2017b) achieved state-of-the-art performance by ignoring the narrative and training a linear classifier with features related to the writing style of the two potential endings, rather than their content. It has also been shown that features focusing on sentence length, sentiment, and negation are sufficient for achieving high accuracy on this dataset (Schwartz et al., 2017a; Cai et al., 2017; Bugert et al., 2017). NLI is often viewed as an integral part of NLU. Condoravdi et al. (2003) argue that it is a necessary metric for evaluating an NLU system, since it We preprocess each recast dataset usin"
S18-2023,D14-1162,0,0.0798297,"Missing"
S18-2023,L18-1239,0,0.195669,"e difficulty of the task” (Bowman et al., 2015) as previously thought. We are not the first to consider the inherent difficulty of NLI datasets. For example, MacCartney (2009) used a simple bag-of-words model to evaluate early iterations of Recognizing Textual Entailment (RTE) challenge sets.1 Concerns have been raised previously about the hypotheses in the Stanford Natural Language Inference (SNLI) dataset specifically, such as by Rudinger et al. (2017) and in unpublished work.2 Here, we survey of large number of existing NLI datasets under the lens of a hypothesis-only model.3 Concurrently, Tsuchiya (2018) and Gururangan et al. (2018) similarly trained an NLI classifier with access limited to hypotheses and discovered similar results on three of the ten datasets that we study. 2 forces a model to perform many distinct types of reasoning. Goldberg (2017) suggests that “solving [NLI] perfectly entails human level understanding of language”, and Nangia et al. (2017) argue that “in order for a system to perform well at natural language inference, it needs to handle nearly the full complexity of natural language understanding.” However, if biases in NLI datasets, especially those that do not reflect"
S18-2023,N12-1072,0,0.033297,"L , CONTRADICTION ). SNLI is known to contain stereotypical biases based on gender, race, and ethnic stereotypes (Rudinger et al., 2017). Furthermore, Zhang et al. (2017) commented that this “elicitation protocols can lead to biased responses unlikely to contain a wide range of possible common-sense inferences.” Add-one RTE This mixed-genre dataset tests whether NLI systems can understand adjectivenoun compounds (Pavlick and Callison-Burch, 2016). Premise sentences were extracted from Annotated Gigaword (Napoles et al., 2012), image captions (Young et al., 2014), the Internet Argument Corpus (Walker et al., 2012), and fictional stories from the GutenTag dataset (Mac Kim and Cassidy, 2015). To create hypotheses, adjectives were removed or inserted before nouns in a premise, and crowd-sourced workers were asked to provide reliable labels (ENTAILED, NOTENTAILED ). Multi-NLI Multi-NLI is a recent expansion of SNLI aimed to add greater diversity to the existing dataset (Williams et al., 2017). Premises in MultiNLI can originate from fictional stories, personal letters, telephone speech, and a 9/11 report. 4.2 SciTail Recently released, SciTail is an NLI dataset created from 4th grade science questions and"
S18-2023,W17-4413,0,0.0282009,"d or inserted before nouns in a premise, and crowd-sourced workers were asked to provide reliable labels (ENTAILED, NOTENTAILED ). Multi-NLI Multi-NLI is a recent expansion of SNLI aimed to add greater diversity to the existing dataset (Williams et al., 2017). Premises in MultiNLI can originate from fictional stories, personal letters, telephone speech, and a 9/11 report. 4.2 SciTail Recently released, SciTail is an NLI dataset created from 4th grade science questions and multiple-choice answers (Khot et al., 2018). Hypotheses are assertions converted from question-answer pairs found in SciQ (Welbl et al., 2017). Hypotheses are automatically paired with premise sentences from domain specific texts (Clark et al., 2016), and labeled (ENTAILMENT, NEUTRAL) by crowdsourced workers. Notably, the construction Human Judged Alternatively, if hypotheses and premises were automatically paired but labeled by a human, we consider the dataset to be judged. Our humanjudged data sets are: 182 method allows for the same sentence to appear as a hypothesis for more than one premise. labeled NLI sentence pairs were generated from SPR annotations by creating general templates. Multiple Premise Entailment (MPE) Unlike the"
S18-2023,I17-1100,1,0.913538,"Missing"
S18-2023,Q14-1006,0,0.0499871,"ce for each of the three labels (ENTAILMENT, NEUTRAL , CONTRADICTION ). SNLI is known to contain stereotypical biases based on gender, race, and ethnic stereotypes (Rudinger et al., 2017). Furthermore, Zhang et al. (2017) commented that this “elicitation protocols can lead to biased responses unlikely to contain a wide range of possible common-sense inferences.” Add-one RTE This mixed-genre dataset tests whether NLI systems can understand adjectivenoun compounds (Pavlick and Callison-Burch, 2016). Premise sentences were extracted from Annotated Gigaword (Napoles et al., 2012), image captions (Young et al., 2014), the Internet Argument Corpus (Walker et al., 2012), and fictional stories from the GutenTag dataset (Mac Kim and Cassidy, 2015). To create hypotheses, adjectives were removed or inserted before nouns in a premise, and crowd-sourced workers were asked to provide reliable labels (ENTAILED, NOTENTAILED ). Multi-NLI Multi-NLI is a recent expansion of SNLI aimed to add greater diversity to the existing dataset (Williams et al., 2017). Premises in MultiNLI can originate from fictional stories, personal letters, telephone speech, and a 9/11 report. 4.2 SciTail Recently released, SciTail is an NLI d"
S18-2023,W17-0906,0,\N,Missing
S18-2023,P17-2097,0,\N,Missing
S19-1026,J99-2004,0,0.0608644,"Missing"
S19-1026,W15-2712,0,0.0230209,"cture constant. We ask whether the linguistic properties implicitly captured by pretraining objectives measurably affect the types of linguistic information encoded in the learned representations. To this end, we explore whether qualitatively different objectives lead to demonstrably different sentence representations. We focus our analysis on function words because they play a key role in compositional meaning—e.g., introducing and identifying discourse referents or representing relationships between entities or ideas—and are not yet considered to be well-modeled by distributional semantics (Bernardi et al., 2015). Our results suggest that different pretraining objectives give rise to differences in function word comprehension; for instance, we see that natural language inference helps understanding negation, and CCG supertagging helps recognizing meaningful sentence boundaries. However, overall, we find that the observed differences are not always straightforwardly interpretable, and further investigation is needed to determine which specific aspects of pretraining tasks yield good representations of function words. The analyses we present contribute new results in an ongoing line of research aimed at"
S19-1026,N19-1423,0,0.201655,"n of negation. 1 Introduction Many recent advances in NLP have been driven by new approaches to representation learning— i.e., the design of models whose primary aim is to yield representations of words or sentences that are useful for a range of downstream applications (Bowman et al., 2017). Approaches to representation learning typically differ in the architecture of the model used to learn the representations, the objective used to train that network, or both. Varying these factors can significantly impact performance on a broad range of NLP tasks (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). This paper investigates the role of pretraining objectives of sentence encoders, with respect to ∗ Corresponding authors: Najoung Kim (n.kim@jhu.edu), Ellie Pavlick (ellie pavlick@brown.edu) their capacity to understand function words (e.g., prepositions, conjunctions). Although the importance of finding an effective pretraining objective for learning better (or more generalizable) representations is well acknowledged, relatively few studies offer a controlled comparison of diverse pretraining objectives, holding model architecture constant. We ask whether the linguistic properties implicitl"
S19-1026,W16-2524,0,0.0505528,"aining and probing datasets. A regression analysis shows that vocabulary overlap overall is not a significant predictor of performance on the probing set (p = 0.39). No single probing set performance was significantly affected by vocabulary overlap either (all p > .05 after Bonferroni correction for multiple comparisons). Figure 2: Prediction overlap on the probing tasks for models trained on different pretraining tasks (i.e., how often models make identical predictions on a particular probing set). 5 Related Work An active line of work focuses on “probing” neural representations of language. Ettinger et al. (2016, 2017); Zhu et al. (2018), i.a., use a task-based approach similar to ours, where tasks that require a specific subset of linguistic knowledge are used to perform qualitative evaluation. Gulordava et al. (2018), Giulianelli et al. (2018), Rønning et al. (2018), and Jumelet and Hupkes (2018) make a focused contribution towards a particular linguistic phenomenon (agreement, ellipsis, negative polarity). Using recast NLI, Poliak et al. (2018a) probe for semantic phenomena in neural machine translation encoders. Stali¯unaite and Bonfil (2017); Mahler et al. (2017); Ribeiro et al. (2018) use simil"
S19-1026,W17-5401,0,0.0333637,"Missing"
S19-1026,W18-5426,0,0.050884,"bulary overlap either (all p > .05 after Bonferroni correction for multiple comparisons). Figure 2: Prediction overlap on the probing tasks for models trained on different pretraining tasks (i.e., how often models make identical predictions on a particular probing set). 5 Related Work An active line of work focuses on “probing” neural representations of language. Ettinger et al. (2016, 2017); Zhu et al. (2018), i.a., use a task-based approach similar to ours, where tasks that require a specific subset of linguistic knowledge are used to perform qualitative evaluation. Gulordava et al. (2018), Giulianelli et al. (2018), Rønning et al. (2018), and Jumelet and Hupkes (2018) make a focused contribution towards a particular linguistic phenomenon (agreement, ellipsis, negative polarity). Using recast NLI, Poliak et al. (2018a) probe for semantic phenomena in neural machine translation encoders. Stali¯unaite and Bonfil (2017); Mahler et al. (2017); Ribeiro et al. (2018) use similar strategies to our structural mutation method, although their primary goal was to break existing systems by adversarial modifications rather than to compare different models. Ribeiro et al. (2018) and our work both test for proper compr"
S19-1026,N18-1038,0,0.0609676,"Missing"
S19-1026,W17-5405,0,0.026707,"Missing"
S19-1026,C18-1198,0,0.038925,"probe for semantic phenomena in neural machine translation encoders. Stali¯unaite and Bonfil (2017); Mahler et al. (2017); Ribeiro et al. (2018) use similar strategies to our structural mutation method, although their primary goal was to break existing systems by adversarial modifications rather than to compare different models. Ribeiro et al. (2018) and our work both test for proper comprehension of the modified expressions, but our modifications are designed to induce semantic changes whereas their modifications are intended to preserve the original meaning. Our strategy is close to that of Naik et al. (2018), but our modifications are more constrained and lexically targeted. The design of our NLI-style probing tasks follows the recent line of work which advocates for NLI as a general-purpose format for diagnostic tasks (White et al., 2017; Poliak et al., 2018b). This idea is similar in spirit to McCann et al. (2018), which advocates for question answering as a general-purpose format, to edge probing (Tenney et al., 2019) which probes for syntactic and semantic structures via a common labeling format, and to GLUE (Wang et al., 2018) which aggregates a variety of tasks that share a common sentencec"
S19-1026,N18-1202,0,0.406307,"elps the comprehension of negation. 1 Introduction Many recent advances in NLP have been driven by new approaches to representation learning— i.e., the design of models whose primary aim is to yield representations of words or sentences that are useful for a range of downstream applications (Bowman et al., 2017). Approaches to representation learning typically differ in the architecture of the model used to learn the representations, the objective used to train that network, or both. Varying these factors can significantly impact performance on a broad range of NLP tasks (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). This paper investigates the role of pretraining objectives of sentence encoders, with respect to ∗ Corresponding authors: Najoung Kim (n.kim@jhu.edu), Ellie Pavlick (ellie pavlick@brown.edu) their capacity to understand function words (e.g., prepositions, conjunctions). Although the importance of finding an effective pretraining objective for learning better (or more generalizable) representations is well acknowledged, relatively few studies offer a controlled comparison of diverse pretraining objectives, holding model architecture constant. We ask whether the linguisti"
S19-1026,N18-2082,1,0.879934,"Missing"
S19-1026,W18-5441,1,0.897134,"Missing"
S19-1026,P18-1079,0,0.0486921,"language. Ettinger et al. (2016, 2017); Zhu et al. (2018), i.a., use a task-based approach similar to ours, where tasks that require a specific subset of linguistic knowledge are used to perform qualitative evaluation. Gulordava et al. (2018), Giulianelli et al. (2018), Rønning et al. (2018), and Jumelet and Hupkes (2018) make a focused contribution towards a particular linguistic phenomenon (agreement, ellipsis, negative polarity). Using recast NLI, Poliak et al. (2018a) probe for semantic phenomena in neural machine translation encoders. Stali¯unaite and Bonfil (2017); Mahler et al. (2017); Ribeiro et al. (2018) use similar strategies to our structural mutation method, although their primary goal was to break existing systems by adversarial modifications rather than to compare different models. Ribeiro et al. (2018) and our work both test for proper comprehension of the modified expressions, but our modifications are designed to induce semantic changes whereas their modifications are intended to preserve the original meaning. Our strategy is close to that of Naik et al. (2018), but our modifications are more constrained and lexically targeted. The design of our NLI-style probing tasks follows the rec"
S19-1026,W18-5409,0,0.0502653,"Missing"
S19-1026,P18-1018,0,0.0235763,"s none of them correctly (0/6). Here is an example of a numeric usage of below that the NLI model answered correctly but the image model answered incorrectly: P: Only those whose incomes do not exceed 125 percent of the federal poverty level qualify . . . H: Those whose incomes are below 125 percent qualify . . . (P→H) The image model’s bias towards the spatial usage is intuitive, since the numeric usage of below (i.e., as a counterpart to exceed) is difficult to learn from visual clues only. This concrete-abstract duality, which is not specific to below but common to most other prepositions (Schneider et al., 2018), may partially explain why the image-caption model behaves so differently from all other models, which are not trained on a multimodal objective. 4.3 Data Size and Genre Effects As can be seen from the varying sizes of the pretraining dataset reported in Figure 1, seeing more data during pretraining does not imply better performance on probing tasks. Also, as noted before, the fact that pretraining can hurt performance suggests that if the task is not the “right” task, adding more datapoints during pretraining can lead models to learn counterproductive representations. Another potential confo"
S19-1026,W17-5410,0,0.0296726,"Missing"
S19-1026,W17-2625,0,0.063243,"Missing"
S19-1026,W18-5446,1,0.80362,"o preserve the original meaning. Our strategy is close to that of Naik et al. (2018), but our modifications are more constrained and lexically targeted. The design of our NLI-style probing tasks follows the recent line of work which advocates for NLI as a general-purpose format for diagnostic tasks (White et al., 2017; Poliak et al., 2018b). This idea is similar in spirit to McCann et al. (2018), which advocates for question answering as a general-purpose format, to edge probing (Tenney et al., 2019) which probes for syntactic and semantic structures via a common labeling format, and to GLUE (Wang et al., 2018) which aggregates a variety of tasks that share a common sentenceclassification format. The primary difference in our work is that we focus specifically on the understanding of function words in context. We also present a suite of several tasks, but each one focuses on a particular structure, whereas tasks proposed in the works above generally aggregate multiple phenomena. Each of our tasks isolates each function word type and employ a targeted modification strategy that gives us a more narrowlyfocused, informative scope of analysis. 6 Conclusion We propose a new challenge set of nine tasks th"
S19-1026,I17-1100,1,0.878632,"Missing"
S19-1026,N18-1101,1,0.788304,"dex (σ = 2) and rounding to the nearest integer. 2.3 NLI-Based Tasks Our NLI-based probing tasks ask whether the choice of function word affects the inferences licensed by a sentence. These tasks consist of a pair of sentences—a premise p and a hypothesis h— and ask whether or not p entails h. We exploit the label changes induced by a targeted mutation of 3 We use WikiText instead of BWB because adjacent sentences in BWB are not logically contiguous and therefore may not be from the same discourse context. the sentence pairs taken from the Multi-genre Natural Language Inference dataset (MNLI, Williams et al., 2018). The rationale is that, if a change to a single function word in the premise changes the entailment label, that function word must play a significant role in the semantics of the sentence. Prepositions We manually curate a list of prepositions (see Appendix D) that are likely to be swapped with each other without affecting the grammaticality of the sentence. We generate mutated NLI pairs by finding occurrences of the prepositions in our list and randomly replacing them with other prepositions in the list. Our list consists of a set of locatives4 and several other manually-selected preposition"
S19-1026,Q17-1027,1,0.86489,"Missing"
S19-1026,P18-2100,0,0.0613273,"Missing"
S19-1028,N18-2017,0,0.0824448,"Missing"
S19-1028,D15-1075,0,0.21254,"Missing"
S19-1028,N18-1111,0,0.0169731,"ils another (hypothesis) - contain hypothesis-only biases that allow models to perform the task surprisingly well by only considering hypotheses while ignoring the corresponding premises. For instance, such a method correctly predicted the examples in Table 1 as contradictions. As datasets may always contain biases, it is important to analyze whether, and to what extent, models are immune to or rely on known biases. Furthermore, it is important to build models that can overcome these biases. Recent work in NLP aims to build more robust systems using adversarial methods (Alzantot et al., 2018; Chen & Cardie, 2018; Belinkov & Bisk, 2018, i.a.). In particular, Elazar & Goldberg (2018) attempted to use adversarial training to remove demographic attributes from text data, with limited success. Inspired by this line of work, we use adversarial learning to add small components to an existing and popular NLI system that has been used to learn general sentence representations (Conneau et al., 2017). The adversarial ∗ techniques include (1) using an external adversarial classifier conditioned on hypotheses alone, and (2) creating noisy, perturbed training examples. In our analyses we ask whether hidden, hypoth"
S19-1028,P18-1225,0,0.0559285,"ude (1) using an external adversarial classifier conditioned on hypotheses alone, and (2) creating noisy, perturbed training examples. In our analyses we ask whether hidden, hypothesisonly biases are no longer present in the resulting sentence representations after adversarial learning. The goal is to build models with less bias, ideally while limiting the inevitable degradation in task performance. Our results suggest that progress on this goal may depend on which adversarial learning techniques are used. Although recent work has applied adversarial learning to NLI (Minervini & Riedel, 2018; Kang et al., 2018), this is the first work to our knowledge that explicitly studies NLI models designed to ignore hypothesis-only biases. 2 Methods We consider two types of adversarial methods. In the first method, we incorporate an external classifier to force the hypothesis-encoder to ignore hypothesis-only biases. In the second method, we randomly swap premises in the training set to create noisy examples. Equal contribution 256 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 256–262 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics"
S19-1028,D17-1070,0,0.235347,"e immune to or rely on known biases. Furthermore, it is important to build models that can overcome these biases. Recent work in NLP aims to build more robust systems using adversarial methods (Alzantot et al., 2018; Chen & Cardie, 2018; Belinkov & Bisk, 2018, i.a.). In particular, Elazar & Goldberg (2018) attempted to use adversarial training to remove demographic attributes from text data, with limited success. Inspired by this line of work, we use adversarial learning to add small components to an existing and popular NLI system that has been used to learn general sentence representations (Conneau et al., 2017). The adversarial ∗ techniques include (1) using an external adversarial classifier conditioned on hypotheses alone, and (2) creating noisy, perturbed training examples. In our analyses we ask whether hidden, hypothesisonly biases are no longer present in the resulting sentence representations after adversarial learning. The goal is to build models with less bias, ideally while limiting the inevitable degradation in task performance. Our results suggest that progress on this goal may depend on which adversarial learning techniques are used. Although recent work has applied adversarial learning"
S19-1028,K18-1007,0,0.120137,"ersarial ∗ techniques include (1) using an external adversarial classifier conditioned on hypotheses alone, and (2) creating noisy, perturbed training examples. In our analyses we ask whether hidden, hypothesisonly biases are no longer present in the resulting sentence representations after adversarial learning. The goal is to build models with less bias, ideally while limiting the inevitable degradation in task performance. Our results suggest that progress on this goal may depend on which adversarial learning techniques are used. Although recent work has applied adversarial learning to NLI (Minervini & Riedel, 2018; Kang et al., 2018), this is the first work to our knowledge that explicitly studies NLI models designed to ignore hypothesis-only biases. 2 Methods We consider two types of adversarial methods. In the first method, we incorporate an external classifier to force the hypothesis-encoder to ignore hypothesis-only biases. In the second method, we randomly swap premises in the training set to create noisy examples. Equal contribution 256 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 256–262 c Minneapolis, June 6–7, 2019. 2019 Association for Comput"
S19-1028,P16-2022,0,0.0833388,"filtered such that it may not contain unwanted artifacts. We apply both adversarial techniques to InferSent (Conneau et al., 2017), which serves as our general NLI architecture.2 Following the standard training details used in InferSent, we encode premises and hypotheses separately using bi-directional long short-term memory (BiLSTM) networks (Hochreiter & Schmidhuber, 1997). Premises and hypotheses are initially mapped (token-by-token) to Glove (Pennington et al., 2014) representations. We use max-pooling over the BiLSTM states to extract premise and hypothesis representations and, following Mou et al. (2016), combine the representations by concatenating their vectors, their difference, and their multiplication (element-wise). We use the default training hyper-parameters in the released InferSent codebase.3 These include setting the initial learning rate to 0.1 and the decay rate to 0.99, using SGD optimization and dividing the learning rate by 5 at every epoch when the accuracy deceases on the validation set. The default settings also include stopping training either when the learning rate drops below 10−5 or after 20 epochs. In both adversarial settings, the hyper-parameters are swept through {0"
S19-1028,D14-1162,0,0.0955863,"LI. We use the standard SNLI split and report validation and test results. We also test on SNLI-hard, a subset of SNLI that Gururangan et al. (2018) filtered such that it may not contain unwanted artifacts. We apply both adversarial techniques to InferSent (Conneau et al., 2017), which serves as our general NLI architecture.2 Following the standard training details used in InferSent, we encode premises and hypotheses separately using bi-directional long short-term memory (BiLSTM) networks (Hochreiter & Schmidhuber, 1997). Premises and hypotheses are initially mapped (token-by-token) to Glove (Pennington et al., 2014) representations. We use max-pooling over the BiLSTM states to extract premise and hypothesis representations and, following Mou et al. (2016), combine the representations by concatenating their vectors, their difference, and their multiplication (element-wise). We use the default training hyper-parameters in the released InferSent codebase.3 These include setting the initial learning rate to 0.1 and the decay rate to 0.99, using SGD optimization and dividing the learning rate by 5 at every epoch when the accuracy deceases on the validation set. The default settings also include stopping train"
S19-1028,W17-0907,0,0.145469,"Missing"
S19-1028,L18-1239,0,0.442817,"othesis-only biases. Adversarial learning may help models ignore sensitive biases and spurious correlations in data. We evaluate whether adversarial learning can be used in NLI to encourage models to learn representations free of hypothesis-only biases. Our analyses indicate that the representations learned via adversarial learning may be less biased, with only small drops in NLI accuracy. 1 A person writing something on a newspaper I A person is driving a fire truck A man is doing tricks on a skateboard I Nobody is doing tricks Table 1: Examples from SNLI’s development set that Poliak et al. (2018)’s hypothesis-only model correctly predicted as contradictions. The first line in each section is a premise and lines with I are corresponding hypotheses. The italicized words are correlated with the “contradiction” label in SNLI Introduction Popular datasets for Natural Language Inference (NLI) - the task of determining whether one sentence (premise) likely entails another (hypothesis) - contain hypothesis-only biases that allow models to perform the task surprisingly well by only considering hypotheses while ignoring the corresponding premises. For instance, such a method correctly predicted"
S19-1028,W18-5448,0,0.0525636,"Missing"
S19-1028,D18-1316,0,\N,Missing
S19-1028,D18-1002,0,\N,Missing
S19-1028,W19-1801,1,\N,Missing
W03-0908,J95-4004,0,0.00615965,"punct) (ortho ?) (root ?) (tokens 6))) (qa ( (gap ( (atype temporal) (path (*MULT* adjunct object)))) (qtype entity))) (root found) (subject ( (BBN-name person) (Brill-pos NNP) (cat n) (definite +) (gen-pn +) (human +) (number sg) (ortho &quot;Wendy’s&quot;) (person third) (proper-noun +) (root wendy) (tokens 3))) (tense past) (tokens 5)) Figure 1: When was Wendy’s founded: KANTOO fstructure 3.1 Questions The question analysis consists of two steps: lexical processing and syntactic parsing. For the lexical processing step, we have integrated several external resources: the Brill part-of-speech tagger (Brill, 1995), BBN IdentiFinder (BBN, 2000) (to tag named entities such as proper names, time expressions, numbers, etc.), WordNet (Fellbaum, 1998) (for semantic categorization), and the KANTOO Lexifier (Nyberg and Mitamura, 2000) (to access a syntactic lexicon for verb valence information). The hand-written grammars employed in the project are based on the Lexical Functional Grammar (LFG) formalism (Bresnan, 1982), and are used with the KANTOO parser (Nyberg and Mitamura, 2000). The parser outputs a functional structure (f-structure) which specifies the grammatical functions of question components, e.g.,"
W03-0908,P03-1003,0,0.0321005,"th information extraction (IE) techniques, modified to be applicable to unrestricted texts. Although semantics-poor techniques, such as surface pattern matching (Soubbotin, 2002; Ravichandran and Hovy, 2002) or statistical methods (Ittycheriah et al., 2002), have been successful in answering factoid questions, more complex tasks require a consideration of text meaning. This requirement has motivated work on QA systems to incorporate knowledge processing components such as semantic representation, ontologies, reasoning and inference engines, e.g., (Moldovan et al., 2003), (Hovy et al., 2002), (Chu-Carroll et al., 2003). Since world knowledge databases for open-domain tasks are unavailable, alternative approaches for meaning representation must be adopted. In this paper, we present our preliminary approach to semantics-based answer detection in the JAVELIN QA system (Nyberg et al., 2003). In contrast to other QA systems, we are trying to realize a formal model for a lightweight semantics-based opendomain question answering. We propose a constrained semantic representation as well as an explicit unification 1 The authors appear in alphabetical order. System Components The JAVELIN system consists of four basic"
W03-0908,1995.iwpt-1.15,0,0.0160458,"exicon for verb valence information). The hand-written grammars employed in the project are based on the Lexical Functional Grammar (LFG) formalism (Bresnan, 1982), and are used with the KANTOO parser (Nyberg and Mitamura, 2000). The parser outputs a functional structure (f-structure) which specifies the grammatical functions of question components, e.g., subject, object, adjunct, etc. As illustrated in Fig. 1, the resulting f-structure provides a deep, detailed syntactic analysis of the question. 3.2 Passages Passages selected by the retrieval engine are processed by the Link Grammar parser (Grinberg et al., 1995). The parser uses a lexicalized grammar which specifies links, i.e., grammatical functions, and provides a constituent structure as output. The parser covers a wide range of syntactic constructions and is robust: it can skip over unrecognized fragments of text, and is able to handle unknown words. An example of the passage analysis produced by the Link Parser is presented in Fig. 2. Links are treated as predicates which relate various arguments. For example, O in Fig. 2 indicates that Wendy’s is an object of the verb founded. In parallel to the Link parser, passages are tagged with the BBN Ide"
W03-0908,W01-1203,0,0.030826,"linguistic analysis: the question analysis and passage understanding modules (Question Analyzer and Information Extractor, respectively). The relevant aspects of syntactic processing in both modules are presented in Section 3, whereas the semantic representation is introduced in Section 4. 3 Parsing The system employs two different parsing techniques: a chart parser with hand-written grammars for question analysis, and a lexicalized, broad coverage skipping parser for passage analysis. For question analysis, parsing serves two goals: to identify the finest answer focus (Moldovan et al., 2000; Hermjakob, 2001), and to produce a grammatical analysis (f-structure) for questions. Due to the lack of publicly available parsers which have suitable coverage of question forms, we have manually developed a set of grammars to achieve these goals. On the other hand, the limited coverage and ambiguity in these grammars made adopting the same approach for passage analysis inefficient. In effect, we use two distinct parsers which provide two syntactic representations, including grammatical functions. These syntactic structures are then transformed into a common semantic representation discussed in Section 4. ( ("
W03-0908,P00-1071,0,0.0261933,"mponents which support linguistic analysis: the question analysis and passage understanding modules (Question Analyzer and Information Extractor, respectively). The relevant aspects of syntactic processing in both modules are presented in Section 3, whereas the semantic representation is introduced in Section 4. 3 Parsing The system employs two different parsing techniques: a chart parser with hand-written grammars for question analysis, and a lexicalized, broad coverage skipping parser for passage analysis. For question analysis, parsing serves two goals: to identify the finest answer focus (Moldovan et al., 2000; Hermjakob, 2001), and to produce a grammatical analysis (f-structure) for questions. Due to the lack of publicly available parsers which have suitable coverage of question forms, we have manually developed a set of grammars to achieve these goals. On the other hand, the limited coverage and ambiguity in these grammars made adopting the same approach for passage analysis inefficient. In effect, we use two distinct parsers which provide two syntactic representations, including grammatical functions. These syntactic structures are then transformed into a common semantic representation discussed"
W03-0908,nyberg-mitamura-2000-kantoo,1,0.819708,"-pn +) (human +) (number sg) (ortho &quot;Wendy’s&quot;) (person third) (proper-noun +) (root wendy) (tokens 3))) (tense past) (tokens 5)) Figure 1: When was Wendy’s founded: KANTOO fstructure 3.1 Questions The question analysis consists of two steps: lexical processing and syntactic parsing. For the lexical processing step, we have integrated several external resources: the Brill part-of-speech tagger (Brill, 1995), BBN IdentiFinder (BBN, 2000) (to tag named entities such as proper names, time expressions, numbers, etc.), WordNet (Fellbaum, 1998) (for semantic categorization), and the KANTOO Lexifier (Nyberg and Mitamura, 2000) (to access a syntactic lexicon for verb valence information). The hand-written grammars employed in the project are based on the Lexical Functional Grammar (LFG) formalism (Bresnan, 1982), and are used with the KANTOO parser (Nyberg and Mitamura, 2000). The parser outputs a functional structure (f-structure) which specifies the grammatical functions of question components, e.g., subject, object, adjunct, etc. As illustrated in Fig. 1, the resulting f-structure provides a deep, detailed syntactic analysis of the question. 3.2 Passages Passages selected by the retrieval engine are processed by"
W03-0908,P02-1006,0,0.0160179,"egies; Sections 4 and 5 describe our preliminary semantic representation and the unification framework which assigns confidence values to answer candidates. The final section contains a summary and future plans. 2 1 Introduction Modern Question Answering (QA) systems aim at providing answers to natural language questions in an opendomain context. This task is usually achieved by combining information retrieval (IR) with information extraction (IE) techniques, modified to be applicable to unrestricted texts. Although semantics-poor techniques, such as surface pattern matching (Soubbotin, 2002; Ravichandran and Hovy, 2002) or statistical methods (Ittycheriah et al., 2002), have been successful in answering factoid questions, more complex tasks require a consideration of text meaning. This requirement has motivated work on QA systems to incorporate knowledge processing components such as semantic representation, ontologies, reasoning and inference engines, e.g., (Moldovan et al., 2003), (Hovy et al., 2002), (Chu-Carroll et al., 2003). Since world knowledge databases for open-domain tasks are unavailable, alternative approaches for meaning representation must be adopted. In this paper, we present our preliminary"
W08-2219,W03-0901,0,0.2588,"Missing"
W08-2219,W99-0631,0,0.0305971,"ed for Open Knowledge Extraction: the conversion of arbitrary input sentences into general world knowledge represented in a logical form possibly usable for inference. Results show the feasibility of extraction via the use of sophisticated natural language processing as applied to web texts. 2 Previous Work Given that the concern here is with open knowledge extraction, the myriad projects that target a few prespecified types of relations occurring in a large corpus are set aside. Among early efforts, one might count work on deriving selectional preferences (e.g., Zernik (1992); Resnik (1993); Clark and Weir (1999)) or partial predicateargument structure (e.g., Abney (1996)) as steps in the direction of open knowledge extraction, though typically few of the tuples obtained (often a type of subject plus a verb, or a verb plus a type of object) can be interpreted as complete items of world knowledge. Another somewhat relevant line of research was initiated by Zelle and Mooney (1996), concerned with learning to map NL database queries into formal DB queries (a kind of semantic interpretation). This was pursued further, for instance, by Zettlemoyer and Collins (2005) and Wong and Mooney (2007), aimed at lea"
W08-2219,P97-1003,0,0.121537,"ms (e.g., leaving just a head noun phrase). 2.1 K NEXT K NEXT (Schubert, 2002) was originally designed for application to collections of manually annotated parse trees, such as the Brown corpus. In order to extract knowledge from larger text collections, the system has been extended for processing arbitrary text through the use of third-party parsers. In addition, numerous improvements have been made to the semantic interpretation rules, the filtering techniques, and other components of the system. The extraction procedure is as follows: 1. Parse each sentence using a Treebank-trained parser (Collins, 1997; Charniak, 1999). 2. Preprocess the parse tree, for better interpretability (e.g., distinguish different types of SBAR phrases and different types of PPs, identify temporal phrases, etc.). 3. Apply a set of 80 interpretive rules for computing unscoped logical forms (ULFs) of the sentence and all lower-level constituents in a bottom-up sweep; at the same time, abstract and collect phrasal logical forms that promise to yield stand-alone propositions (e.g., ULFs of clauses and of pre- or post-modified nominals are prime candidates). The ULFs are rendered in Episodic Logic (e.g., (Schubert and Hw"
W08-2219,J02-3001,0,0.00751587,", this approach requires annotation of texts with logical forms, and extending this approach to general texts would seemingly require a massive corpus of hand-annotated text — and the logical forms would have to cover far more phenomena than are found in DB queries (e.g., attitudes, generalized quantifiers, etc.). Another line of relevant work is that on semantic role labelling. One early example was MindNet (Richardson et al., 1998), which was based on collecting 24 semantic role relations from MRDs such as the American Heritage Dictionary. More recent representative efforts includes that of Gildea and Jurafsky (2002), Gildea and Palmer (2002), and Punyakanok et al. (2008). The relevance of this work comes from the fact that identifying the arguments of the verbs in a sentence is a first step towards forming predications, and these may in many cases correspond to items of world knowledge. Open Knowledge Extraction through Compositional Language Processing 241 Liakata and Pulman (2002) built a system for recovering Davidsonian predicateargument structures from the Penn Treebank through the application of a small set of syntactic templates targeting head nodes of verb arguments. The authors illustrate their"
W08-2219,P02-1031,0,0.109263,"ing general world knowledge, given in a logical form potentially usable for inference, may be extracted in high volume from arbitrary input sentences. We compare these results with those obtained in recent work on Open Information Extraction, indicating with some examples the quite different kinds of output obtained by the two approaches. Finally, we observe that portions of the extracted knowledge are comparable to results of recent work on class attribute extraction. 239 240 Van Durme and Schubert 1 Introduction Several early studies in large-scale text processing (Liakata and Pulman, 2002; Gildea and Palmer, 2002; Schubert, 2002) showed that having access to a sentence’s syntax enabled credible, automated semantic analysis. These studies suggest that the use of increasingly sophisticated linguistic analysis tools could enable an explosion in available symbolic knowledge. Nonetheless, much of the subsequent work in extraction has remained averse to the use of the linguistic deep structure of text; this decision is typically justified by a desire to keep the extraction system as computationally lightweight as possible. The acquisition of background knowledge is not an activity that needs to occur online"
W08-2219,C02-1105,0,0.282128,"sonable quality, representing general world knowledge, given in a logical form potentially usable for inference, may be extracted in high volume from arbitrary input sentences. We compare these results with those obtained in recent work on Open Information Extraction, indicating with some examples the quite different kinds of output obtained by the two approaches. Finally, we observe that portions of the extracted knowledge are comparable to results of recent work on class attribute extraction. 239 240 Van Durme and Schubert 1 Introduction Several early studies in large-scale text processing (Liakata and Pulman, 2002; Gildea and Palmer, 2002; Schubert, 2002) showed that having access to a sentence’s syntax enabled credible, automated semantic analysis. These studies suggest that the use of increasingly sophisticated linguistic analysis tools could enable an explosion in available symbolic knowledge. Nonetheless, much of the subsequent work in extraction has remained averse to the use of the linguistic deep structure of text; this decision is typically justified by a desire to keep the extraction system as computationally lightweight as possible. The acquisition of background knowledge is not an activity t"
W08-2219,H93-1054,0,0.198084,"paper is designed for Open Knowledge Extraction: the conversion of arbitrary input sentences into general world knowledge represented in a logical form possibly usable for inference. Results show the feasibility of extraction via the use of sophisticated natural language processing as applied to web texts. 2 Previous Work Given that the concern here is with open knowledge extraction, the myriad projects that target a few prespecified types of relations occurring in a large corpus are set aside. Among early efforts, one might count work on deriving selectional preferences (e.g., Zernik (1992); Resnik (1993); Clark and Weir (1999)) or partial predicateargument structure (e.g., Abney (1996)) as steps in the direction of open knowledge extraction, though typically few of the tuples obtained (often a type of subject plus a verb, or a verb plus a type of object) can be interpreted as complete items of world knowledge. Another somewhat relevant line of research was initiated by Zelle and Mooney (1996), concerned with learning to map NL database queries into formal DB queries (a kind of semantic interpretation). This was pursued further, for instance, by Zettlemoyer and Collins (2005) and Wong and Moon"
W08-2219,P98-2180,0,0.0358415,"and Mooney (2007), aimed at learning log-linear models, or (in the latter case) synchronous CF grammars augmented with lambda operators, for mapping English queries to DB queries. However, this approach requires annotation of texts with logical forms, and extending this approach to general texts would seemingly require a massive corpus of hand-annotated text — and the logical forms would have to cover far more phenomena than are found in DB queries (e.g., attitudes, generalized quantifiers, etc.). Another line of relevant work is that on semantic role labelling. One early example was MindNet (Richardson et al., 1998), which was based on collecting 24 semantic role relations from MRDs such as the American Heritage Dictionary. More recent representative efforts includes that of Gildea and Jurafsky (2002), Gildea and Palmer (2002), and Punyakanok et al. (2008). The relevance of this work comes from the fact that identifying the arguments of the verbs in a sentence is a first step towards forming predications, and these may in many cases correspond to items of world knowledge. Open Knowledge Extraction through Compositional Language Processing 241 Liakata and Pulman (2002) built a system for recovering Davids"
W08-2219,W03-0902,1,0.416496,"UR person.n)) want.v (Ka (rid.a (of.p (DET dictator.n))))] which is verbalized as PERSONS ported by the text fragment: MAY WANT TO BE RID OF A DICTATOR and is sup... and that if the Spanish people wanted to be rid of Franco, they must achieve this by ... Later examples will be translated into a more conventional logical form. One larger collection we have processed since the 2002-3 work on Treebank corpora is the British National Corpus (BNC), consisting of 100 million words of mixedgenre text passages. The quality of resulting propositions has been assessed by the hand-judging methodology of Schubert and Tong (2003), yielding positive judgements almost as frequently as for the Brown Treebank corpus. The next section, concerned with the web corpus collected and used by Banko et al. (2007), contains a fuller description of the judging method. The BNC-based KB, containing 6,205,877 extracted propositions, is publicly searchable via a recently developed online knowledge browser.3 2 Where Ka is an action/attribute reification operator. 3 http://www.cs.rochester.edu/u/vandurme/epik Van Durme and Schubert 244 3 Experiments The experiments reported here were aimed at a comparative assessment of linguistically ba"
W08-2219,P07-1121,0,0.0130814,"Resnik (1993); Clark and Weir (1999)) or partial predicateargument structure (e.g., Abney (1996)) as steps in the direction of open knowledge extraction, though typically few of the tuples obtained (often a type of subject plus a verb, or a verb plus a type of object) can be interpreted as complete items of world knowledge. Another somewhat relevant line of research was initiated by Zelle and Mooney (1996), concerned with learning to map NL database queries into formal DB queries (a kind of semantic interpretation). This was pursued further, for instance, by Zettlemoyer and Collins (2005) and Wong and Mooney (2007), aimed at learning log-linear models, or (in the latter case) synchronous CF grammars augmented with lambda operators, for mapping English queries to DB queries. However, this approach requires annotation of texts with logical forms, and extending this approach to general texts would seemingly require a massive corpus of hand-annotated text — and the logical forms would have to cover far more phenomena than are found in DB queries (e.g., attitudes, generalized quantifiers, etc.). Another line of relevant work is that on semantic role labelling. One early example was MindNet (Richardson et al."
W08-2219,C92-4212,0,0.392104,"plored in this paper is designed for Open Knowledge Extraction: the conversion of arbitrary input sentences into general world knowledge represented in a logical form possibly usable for inference. Results show the feasibility of extraction via the use of sophisticated natural language processing as applied to web texts. 2 Previous Work Given that the concern here is with open knowledge extraction, the myriad projects that target a few prespecified types of relations occurring in a large corpus are set aside. Among early efforts, one might count work on deriving selectional preferences (e.g., Zernik (1992); Resnik (1993); Clark and Weir (1999)) or partial predicateargument structure (e.g., Abney (1996)) as steps in the direction of open knowledge extraction, though typically few of the tuples obtained (often a type of subject plus a verb, or a verb plus a type of object) can be interpreted as complete items of world knowledge. Another somewhat relevant line of research was initiated by Zelle and Mooney (1996), concerned with learning to map NL database queries into formal DB queries (a kind of semantic interpretation). This was pursued further, for instance, by Zettlemoyer and Collins (2005) an"
W08-2219,A00-2018,0,\N,Missing
W08-2219,J08-2005,0,\N,Missing
W08-2219,C98-2175,0,\N,Missing
W10-0724,C02-1144,0,0.0346537,"S CAN BE LIKELY TO GO ON FOR SOME HOURS ’. While it is expected that eventually sufficiently clean knowledge bases will be produced for inferences to be made about everyday things and events, currently the average quality of automatically acquired knowledge is not good enough to be used in traditional reasoning systems. An obstacle for knowledge extraction is the lack of an easy method for evaluating – and thus improving – the quality of results. Evaluation in acquisition systems is typically done by human judging of random samples of output, usually by the reporting authors themselves (e.g., Lin and Pantel, 2002; Schubert and Tong, 2003; Banko et al., 2007). This is time-consuming, and it has the potential for bias: it would be preferable to have people other than AI researchers label whether an output is commonsense knowledge or not. We explore the use of Amazon’s Mechanical Turk service, an online labor market, as a means of acquiring many non-expert judgements for little cost. 2 Related Work While Open Mind Commons (Speer, 2007) asks users to vote for or against commonsense statements contributed by others users in order to come to a consensus, we seek to evaluate an automatic system. Snow et al."
W10-0724,W03-0902,1,0.726268,"ON FOR SOME HOURS ’. While it is expected that eventually sufficiently clean knowledge bases will be produced for inferences to be made about everyday things and events, currently the average quality of automatically acquired knowledge is not good enough to be used in traditional reasoning systems. An obstacle for knowledge extraction is the lack of an easy method for evaluating – and thus improving – the quality of results. Evaluation in acquisition systems is typically done by human judging of random samples of output, usually by the reporting authors themselves (e.g., Lin and Pantel, 2002; Schubert and Tong, 2003; Banko et al., 2007). This is time-consuming, and it has the potential for bias: it would be preferable to have people other than AI researchers label whether an output is commonsense knowledge or not. We explore the use of Amazon’s Mechanical Turk service, an online labor market, as a means of acquiring many non-expert judgements for little cost. 2 Related Work While Open Mind Commons (Speer, 2007) asks users to vote for or against commonsense statements contributed by others users in order to come to a consensus, we seek to evaluate an automatic system. Snow et al. (2008) compared the quali"
W10-0724,W08-2219,1,0.817803,"; Schubert and Tong, 2003; Banko et al., 2007). This is time-consuming, and it has the potential for bias: it would be preferable to have people other than AI researchers label whether an output is commonsense knowledge or not. We explore the use of Amazon’s Mechanical Turk service, an online labor market, as a means of acquiring many non-expert judgements for little cost. 2 Related Work While Open Mind Commons (Speer, 2007) asks users to vote for or against commonsense statements contributed by others users in order to come to a consensus, we seek to evaluate an automatic system. Snow et al. (2008) compared the quality of labels produced by non-expert Turkers against those made by experts for a variety of NLP tasks and found that they required only four responses per item to emulate expert annotations. Kittur et al. (2008) describe the use and 1 Public release of the basic K NEXT engine is forthcoming. 159 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 159–162, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics The statement above is a reasonably clear, entirely plausible, generic cla"
W10-0724,D08-1027,0,\N,Missing
W11-0505,W02-0109,0,0.0289966,"hierarchical clustering as future work, but allow multiple memberships. In addition to clustering using Wikipedia’s inter-page hyperlink structure, we experimented with two families of clustering algorithms pertaining to topic models: the K-means clustering vector space model and the latent Dirichlet allocation (LDA) probabilistic topic model. We used the Mallet software (McCallum, 2002) to run these topic models. We retrieve the latest revision of each article on the day that WikiTopics selected it. We strip unnecessary HTML tags and Wiki templates with mwlib5 and split sentences with NLTK (Loper and Bird, 2002). Normalization, tokenization, and stop words removal were performed, but no stemming was performed. The unigram (bag-of-words) model was used and the number 5 http://code.pediapress.com/wiki/wiki/mwlib Test set Human-1 Human-2 Human-3 ConComp OneHop K-means tf K-means tf-idf LDA # Clusters 48.6 50.0 53.8 31.8 45.2 50 50 44.8 B3 F-score 0.70 ± 0.08 0.71 ± 0.11 0.74 ± 0.10 0.42 ± 0.18 0.58 ± 0.17 0.52 ± 0.04 0.58 ± 0.09 0.43 ± 0.08 Airbus A320 family Super Bowl XLIII Air Force One Arizona Cardinals Chesley Sullenberger Super Bowl US Airways Flight 1549 Kurt Warner 2009 flu pandemic by country S"
W11-0505,N10-1021,0,0.0837913,"Missing"
W11-0505,D07-1047,0,0.0246484,"llow new events in newswire, and to detect the first story about a new event (Allan et al., 1998). Allan et al. (2000) evaluated a variety of vector space clustering schemes, where the best settings from those experiments were then used in our work. This was followed recently by Petrovi´c et al. (2010), who took an approximate approach to first story detection, as applied to Twitter in an on-line streaming setting. Such a system might provide additional information to WikiTopics by helping to identify and describe current events that have yet to be explicitly described in a Wikipedia article. Svore et al. (2007) explored enhancing single-document summariation using news query logs, which may also be applicable to WikiTopics. Wikipedia’s inter-article links have been utilized to construct a topic ontology (Syed et al., 2008), word segmentation corpora (Gabay et al., 2008), or to compute semantic relatedness (Milne and Witten, 2008). In our work, we found the link structure to be as useful to cluster topically related articles as well as the article text. In future work, the text and the link structure will be combined as Chaudhuri et al. (2009) explored multi-view hierarchical clustering for Wikipedia"
W11-0505,P10-1058,0,0.0196557,"rming web crawls, article text extraction, clustering, classification, summarization, and web page generation. The system processes a constant stream of newswire documents. In contrast, WikiTopics analyzes a static set of articles. Hierarchical clustering like three-level clustering of Newsblaster (Hatzivassiloglou et al., 2000) could be applied to WikiTopics to organize current events hierarchically. Summarizing multiple sentences that are extracted from the articles in the same cluster would provide a comprehensive description about the current event. Integer linear programmingbased models (Woodsend and Lapata, 2010) may prove to be useful to generate summaries while global constraints like length, grammar, and coverage are met. The problem of Topic Detection and Tracking (TDT) is to identify and follow new events in newswire, and to detect the first story about a new event (Allan et al., 1998). Allan et al. (2000) evaluated a variety of vector space clustering schemes, where the best settings from those experiments were then used in our work. This was followed recently by Petrovi´c et al. (2010), who took an approximate approach to first story detection, as applied to Twitter in an on-line streaming sett"
W11-0505,P06-4018,0,\N,Missing
W11-1102,S07-1002,0,0.382535,"ioned into the same bin based on whether a system deems them to have the same underlying meaning. A large body of related work can be found in (Sch¨utze, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Purandare and Pedersen, 2004; Bordag, 2006; Niu et al., 2007; Pedersen, 2007; Brody and Lapata, 2009; Li et al., 2010; Klapaftis and Manandhar, 2010). Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval20071 (Agirre and Soroa, 2007). A deficiency of the LDA model for WSI is that the number of senses needs to be manually specified a priori, either separately for each word type, or (as done by B&L) some fixed value that is shared globally across all types. Nonparametric methods instead have the flexibility of automatically deciding the number of sense cluters (Vlachos et al., 2009; Reisinger and Mooney, 2010). In this work we first independently verify the results of B&L, and then tackle the limitation on fixing the number of senses through the use of the Hierarchical Dirichlet Process (HDP) (Teh et al., 2006), a nonparame"
W11-1102,E06-1018,0,0.0238963,"ction Word Sense Induction (WSI) is the task of automatically discovering latent senses for each word type, across a collection of that word’s tokens situated in context. WSI differs from Word Sense Disambiguation (WSD) in that the task does not assume access to some prespecified sense inventory. This amounts to a clustering task: instances of a word are partitioned into the same bin based on whether a system deems them to have the same underlying meaning. A large body of related work can be found in (Sch¨utze, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Purandare and Pedersen, 2004; Bordag, 2006; Niu et al., 2007; Pedersen, 2007; Brody and Lapata, 2009; Li et al., 2010; Klapaftis and Manandhar, 2010). Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval20071 (Agirre and Soroa, 2007). A deficiency of the LDA model for WSI is that the number of senses needs to be manually specified a priori, either separately for each word type, or (as done by B&L) some fixed value that is shared globally across all types."
W11-1102,E09-1013,0,0.779197,"automatically discovering latent senses for each word type, across a collection of that word’s tokens situated in context. WSI differs from Word Sense Disambiguation (WSD) in that the task does not assume access to some prespecified sense inventory. This amounts to a clustering task: instances of a word are partitioned into the same bin based on whether a system deems them to have the same underlying meaning. A large body of related work can be found in (Sch¨utze, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Purandare and Pedersen, 2004; Bordag, 2006; Niu et al., 2007; Pedersen, 2007; Brody and Lapata, 2009; Li et al., 2010; Klapaftis and Manandhar, 2010). Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval20071 (Agirre and Soroa, 2007). A deficiency of the LDA model for WSI is that the number of senses needs to be manually specified a priori, either separately for each word type, or (as done by B&L) some fixed value that is shared globally across all types. Nonparametric methods instead have the flexibility of aut"
W11-1102,E03-1020,0,0.455702,"es, when then applied in a restricted domain. 1 Introduction Word Sense Induction (WSI) is the task of automatically discovering latent senses for each word type, across a collection of that word’s tokens situated in context. WSI differs from Word Sense Disambiguation (WSD) in that the task does not assume access to some prespecified sense inventory. This amounts to a clustering task: instances of a word are partitioned into the same bin based on whether a system deems them to have the same underlying meaning. A large body of related work can be found in (Sch¨utze, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Purandare and Pedersen, 2004; Bordag, 2006; Niu et al., 2007; Pedersen, 2007; Brody and Lapata, 2009; Li et al., 2010; Klapaftis and Manandhar, 2010). Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval20071 (Agirre and Soroa, 2007). A deficiency of the LDA model for WSI is that the number of senses needs to be manually specified a priori, either separately for each word type, or (as done by B&L) some fixed val"
W11-1102,N06-2015,0,0.0612524,"Missing"
W11-1102,D10-1073,0,0.0680993,"or each word type, across a collection of that word’s tokens situated in context. WSI differs from Word Sense Disambiguation (WSD) in that the task does not assume access to some prespecified sense inventory. This amounts to a clustering task: instances of a word are partitioned into the same bin based on whether a system deems them to have the same underlying meaning. A large body of related work can be found in (Sch¨utze, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Purandare and Pedersen, 2004; Bordag, 2006; Niu et al., 2007; Pedersen, 2007; Brody and Lapata, 2009; Li et al., 2010; Klapaftis and Manandhar, 2010). Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval20071 (Agirre and Soroa, 2007). A deficiency of the LDA model for WSI is that the number of senses needs to be manually specified a priori, either separately for each word type, or (as done by B&L) some fixed value that is shared globally across all types. Nonparametric methods instead have the flexibility of automatically deciding the number of sense cluters ("
W11-1102,P10-1116,0,0.0356243,"g latent senses for each word type, across a collection of that word’s tokens situated in context. WSI differs from Word Sense Disambiguation (WSD) in that the task does not assume access to some prespecified sense inventory. This amounts to a clustering task: instances of a word are partitioned into the same bin based on whether a system deems them to have the same underlying meaning. A large body of related work can be found in (Sch¨utze, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Purandare and Pedersen, 2004; Bordag, 2006; Niu et al., 2007; Pedersen, 2007; Brody and Lapata, 2009; Li et al., 2010; Klapaftis and Manandhar, 2010). Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval20071 (Agirre and Soroa, 2007). A deficiency of the LDA model for WSI is that the number of senses needs to be manually specified a priori, either separately for each word type, or (as done by B&L) some fixed value that is shared globally across all types. Nonparametric methods instead have the flexibility of automatically decidi"
W11-1102,S07-1037,0,0.148053,"se Induction (WSI) is the task of automatically discovering latent senses for each word type, across a collection of that word’s tokens situated in context. WSI differs from Word Sense Disambiguation (WSD) in that the task does not assume access to some prespecified sense inventory. This amounts to a clustering task: instances of a word are partitioned into the same bin based on whether a system deems them to have the same underlying meaning. A large body of related work can be found in (Sch¨utze, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Purandare and Pedersen, 2004; Bordag, 2006; Niu et al., 2007; Pedersen, 2007; Brody and Lapata, 2009; Li et al., 2010; Klapaftis and Manandhar, 2010). Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval20071 (Agirre and Soroa, 2007). A deficiency of the LDA model for WSI is that the number of senses needs to be manually specified a priori, either separately for each word type, or (as done by B&L) some fixed value that is shared globally across all types. Nonparametric met"
W11-1102,S07-1087,0,0.0172468,"is the task of automatically discovering latent senses for each word type, across a collection of that word’s tokens situated in context. WSI differs from Word Sense Disambiguation (WSD) in that the task does not assume access to some prespecified sense inventory. This amounts to a clustering task: instances of a word are partitioned into the same bin based on whether a system deems them to have the same underlying meaning. A large body of related work can be found in (Sch¨utze, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Purandare and Pedersen, 2004; Bordag, 2006; Niu et al., 2007; Pedersen, 2007; Brody and Lapata, 2009; Li et al., 2010; Klapaftis and Manandhar, 2010). Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval20071 (Agirre and Soroa, 2007). A deficiency of the LDA model for WSI is that the number of senses needs to be manually specified a priori, either separately for each word type, or (as done by B&L) some fixed value that is shared globally across all types. Nonparametric methods instead hav"
W11-1102,W04-2406,0,0.0714145,"a restricted domain. 1 Introduction Word Sense Induction (WSI) is the task of automatically discovering latent senses for each word type, across a collection of that word’s tokens situated in context. WSI differs from Word Sense Disambiguation (WSD) in that the task does not assume access to some prespecified sense inventory. This amounts to a clustering task: instances of a word are partitioned into the same bin based on whether a system deems them to have the same underlying meaning. A large body of related work can be found in (Sch¨utze, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Purandare and Pedersen, 2004; Bordag, 2006; Niu et al., 2007; Pedersen, 2007; Brody and Lapata, 2009; Li et al., 2010; Klapaftis and Manandhar, 2010). Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval20071 (Agirre and Soroa, 2007). A deficiency of the LDA model for WSI is that the number of senses needs to be manually specified a priori, either separately for each word type, or (as done by B&L) some fixed value that is shared globally acr"
W11-1102,D10-1114,0,0.0195373,"09) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval20071 (Agirre and Soroa, 2007). A deficiency of the LDA model for WSI is that the number of senses needs to be manually specified a priori, either separately for each word type, or (as done by B&L) some fixed value that is shared globally across all types. Nonparametric methods instead have the flexibility of automatically deciding the number of sense cluters (Vlachos et al., 2009; Reisinger and Mooney, 2010). In this work we first independently verify the results of B&L, and then tackle the limitation on fixing the number of senses through the use of the Hierarchical Dirichlet Process (HDP) (Teh et al., 2006), a nonparametric Bayesian model. We show this approach leads to results of similar quality as LDA, when using a bag-of-words context model, in addition to allowing for variability in the number of senses across different words and domains. When trained on a restricted domain corpus for which manually labeled sense data was present, we verify that the model may be tuned to posit a similar num"
W11-1102,J98-1004,0,0.910932,"Missing"
W11-1102,W09-0210,0,0.0359004,". Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval20071 (Agirre and Soroa, 2007). A deficiency of the LDA model for WSI is that the number of senses needs to be manually specified a priori, either separately for each word type, or (as done by B&L) some fixed value that is shared globally across all types. Nonparametric methods instead have the flexibility of automatically deciding the number of sense cluters (Vlachos et al., 2009; Reisinger and Mooney, 2010). In this work we first independently verify the results of B&L, and then tackle the limitation on fixing the number of senses through the use of the Hierarchical Dirichlet Process (HDP) (Teh et al., 2006), a nonparametric Bayesian model. We show this approach leads to results of similar quality as LDA, when using a bag-of-words context model, in addition to allowing for variability in the number of senses across different words and domains. When trained on a restricted domain corpus for which manually labeled sense data was present, we verify that the model may be"
W11-1102,C92-2070,0,0.0622082,"ss senses for job helps to prevent assigning too fined-grained senses in case the same words in two similar contexts are mistakenly regarded as carrying different senses. 2 H  G0 0 Gm Bayesian Word Sense Induction   sm , n m wm,n sm , n n ∈[1, N m ] m∈[1, M ]  k  k ∈[1, K ] wm,n Figure 2: Hierarchical Dirichlet Process (HDP) for WSI. n ∈[1, N m ] m∈[1, M ] p(wm,n ) = Figure 1: Latent Dirichlet Allocation (LDA) for WSI. K X p(wm,n |sm,n = k)p(sm,n = k). k=1 As in prior work including B&L, we rely on the intuition that the senses of words are hinted at by their contextual information (Yarowsky, 1992). From the perspective of a generative process, neighboring words of a target are generated by the target’s underlying sense.2 Both LDA and HDP define graphical models that generate collections of discrete data. The sense of a target word is first drawn from a distribution and then the context of this word is generated according to that distribution. But while LDA assumes a fixed, finite set of distributions, the HDP draws from an infinite set of distributions generated by a Dirichlet Process. This section details the distinction. Figure 1 shows the LDA model for word sense induction. The conv"
W11-1102,J93-2004,0,\N,Missing
W11-1610,P05-1074,1,0.828271,"e, the NIST05 Arabic reference set has a mean compression rate of 0.92 with 4 references per set. sions having the same character-based compression rate may have different word-based compression rates. The advantage of a character-based substitution model is in choosing shorter words when possible, freeing space for more content words. Going by word length alone would exclude the many paraphrases with fewer characters than the original phrase and the same number of words (or more). 3.1 Paraphrase Acquisition To generate paraphrases for use in our experiments, we took the approach described by Bannard and Callison-Burch (2005), which extracts paraphrases from bilingual parallel corpora. Figure 1 illustrates the process. A phrase to be paraphrased, like thrown into jail, is found in a German-English parallel corpus. The corresponding foreign phrase (festgenommen) is identified using word alignment and phrase extraction techniques from phrase-based statistical machine translation (Koehn et al., 2003). Other occurrences of the foreign phrase in the parallel corpus may align to another English phrase like jailed. Following Bannard and Callison-Burch, we treated any English phrases that share a common foreign phrase as"
W11-1610,D08-1021,1,0.862367,"paraphrases. Thus, thrown into jail not only paraphrases as jailed, but also as arrested, detained, imprisoned, incarcerated, locked up, taken into custody, and thrown into prison . Moreover, because the method relies on noisy and potentially inaccurate word alignments, it is prone to generate many bad paraphrases, such as maltreated, thrown, cases, custody, arrest, owners, and protection. To rank candidates, Bannard and Callison-Burch defined the paraphrase probability p(e2 |e1 ) based on the translation model probabilities p(e|f ) and p(f |e) from statistical machine translation. Following Callison-Burch (2008), we refine selection by requiring both the original phrase and paraphrase to be of the same syntactic type, which leads to more grammatical paraphrases. Although many excellent paraphrases are extracted from parallel corpora, many others are unsuitable and the translation score does not always accurately distinguish the two. Therefore, we re86 Paraphrase study in detail scrutinise consider keep learn study studied studying it in detail undertook Monlingual 1.00 0.94 0.90 0.83 0.57 0.42 0.28 0.16 0.06 Bilingual 0.70 0.08 0.20 0.03 0.10 0.07 0.01 0.05 0.06 Table 1: Candidate paraphrases for stu"
W11-1610,N10-1084,0,0.022851,"night and Marcu (2002), Nomoto (2009), Galanis and Androutsopoulos (2010), Filippova and Strube (2008), McDonald (2006), Yamangil and Shieber (2010), Cohn and Lapata (2008), Cohn and Lapata (2009), Turner and Charniak (2005)). Woodsend et al. (2010) incorporate paraphrase rules into a deletion model. Previous work in subtitling has made oneword substitutions to decrease character length at high compression rates (Glickman et al., 2006). More recent approaches in steganography have used paraphrase substitution to encode information in text but focus on grammaticality, not meaning preservation (Chang and Clark, 2010). Zhao et al. (2009) applied an adaptable paraphrasing pipeline to sentence 2 Taken from the main page of http://wsj.com, April 9, 2011. 85 compression, optimizing for F-measure over a manually annotated set of gold standard paraphrases. Sentence compression has been considered before in contexts outside of summarization, such as headline, title, and subtitle generation (Dorr et al., 2003; Vandeghinste and Pan, 2004; Marsi et al., 2009). Corston-Oliver (2001) deleted characters from words to shorten the character length of sentences. To our knowledge character-based compression has not been ex"
W11-1610,C08-1018,0,0.507127,"Missing"
W11-1610,W03-0501,0,0.304101,"h compression rates (Glickman et al., 2006). More recent approaches in steganography have used paraphrase substitution to encode information in text but focus on grammaticality, not meaning preservation (Chang and Clark, 2010). Zhao et al. (2009) applied an adaptable paraphrasing pipeline to sentence 2 Taken from the main page of http://wsj.com, April 9, 2011. 85 compression, optimizing for F-measure over a manually annotated set of gold standard paraphrases. Sentence compression has been considered before in contexts outside of summarization, such as headline, title, and subtitle generation (Dorr et al., 2003; Vandeghinste and Pan, 2004; Marsi et al., 2009). Corston-Oliver (2001) deleted characters from words to shorten the character length of sentences. To our knowledge character-based compression has not been examined before with the surging popularity and utility of Twitter. 3 Sentence Tightening The distinction between tightening and compression can be illustrated by considering how much space needs to be preserved. In the case of microblogging, often a sentence has just a few too many characters and needs to be “tightened”. On the other hand, if a sentence is much longer than a desired length"
W11-1610,W08-1105,0,0.29131,"Missing"
W11-1610,N10-1131,0,0.266767,"Missing"
W11-1610,N07-1023,0,0.640033,"Missing"
W11-1610,W06-2907,0,0.177064,"r than extractive (Marsi et al., 2010). This is one sense in which paraphrastic compression can improve existing compression methodologies. 1 Compression rate is defined as the compression length over original length, so lower values indicate shorter sentences. While not currently the standard, character-based lengths have been considered before in compression, and we believe that it is relevant for current and future applications. Character lengths have been used for document summarization (DUC 2004, Over and Yen (2004)), summarizing for mobile devices (Corston-Oliver, 2001), and subtitling (Glickman et al., 2006). Although in the past strict word limits have been imposed for various documents, information transmitted electronically is often limited by the number of bytes, which directly relates to the number of characters. Mobile devices, SMS messages, and microblogging sites such as Twitter are increasingly important for quickly spreading information. In this context, it is important to consider characterbased constraints. We examine whether paraphrastic compression allows more information to be conveyed in the same number of characters as deletion-only compressions. For example, the length constrain"
W11-1610,N03-1017,0,0.00431456,"hrases with fewer characters than the original phrase and the same number of words (or more). 3.1 Paraphrase Acquisition To generate paraphrases for use in our experiments, we took the approach described by Bannard and Callison-Burch (2005), which extracts paraphrases from bilingual parallel corpora. Figure 1 illustrates the process. A phrase to be paraphrased, like thrown into jail, is found in a German-English parallel corpus. The corresponding foreign phrase (festgenommen) is identified using word alignment and phrase extraction techniques from phrase-based statistical machine translation (Koehn et al., 2003). Other occurrences of the foreign phrase in the parallel corpus may align to another English phrase like jailed. Following Bannard and Callison-Burch, we treated any English phrases that share a common foreign phrase as potential paraphrases of each other. As the original phrase occurs several times and aligns with many different foreign phrases, each of these may align to a variety of other English paraphrases. Thus, thrown into jail not only paraphrases as jailed, but also as arrested, detained, imprisoned, incarcerated, locked up, taken into custody, and thrown into prison . Moreover, beca"
W11-1610,lin-etal-2010-new,0,0.016322,"etail undertook Monlingual 1.00 0.94 0.90 0.83 0.57 0.42 0.28 0.16 0.06 Bilingual 0.70 0.08 0.20 0.03 0.10 0.07 0.01 0.05 0.06 Table 1: Candidate paraphrases for study in detail with corresponding approximate cosine similarity (Monolingual) and translation model (Bilingual) scores. ranked our candidates based on monolingual distributional similarity, employing the method described by Van Durme and Lall (2010) to derive approximate cosine similarity scores over feature counts using single token, independent left and right contexts. Features were computed from the web-scale n-gram collection of Lin et al. (2010). As 5-grams are the highest order of n-gram in this collection, the allowable set of paraphrases have at most four words (which allows at least one word of context). To our knowledge this is the first time such techniques have been used in combination in order to derive higher quality paraphrase candidates. See Table 1 for an example. The monolingual-filtering technique we describe is by no means limited to paraphrases extracted from bilingual corpora. It could be applied to other datadriven paraphrasing techniques (see Madnani and Dorr (2010) for a survey). Although it is particularly well s"
W11-1610,J10-3003,0,0.0202358,"s were computed from the web-scale n-gram collection of Lin et al. (2010). As 5-grams are the highest order of n-gram in this collection, the allowable set of paraphrases have at most four words (which allows at least one word of context). To our knowledge this is the first time such techniques have been used in combination in order to derive higher quality paraphrase candidates. See Table 1 for an example. The monolingual-filtering technique we describe is by no means limited to paraphrases extracted from bilingual corpora. It could be applied to other datadriven paraphrasing techniques (see Madnani and Dorr (2010) for a survey). Although it is particularly well suited to the bilingual extracted corpora, since the information that it adds is orthogonal to that model, it would presumably add less to paraphrasing techniques that already take advantage of monolingual distributional similarity (Pereira et al., 1993; Lin and Pantel, 2001; Barzilay and Lee, 2003). In order to evaluate the paraphrase candidates and scoring techniques, we randomly selected 1,000 paraphrase sets where the source phrase was present in the corpus described in Clarke and Lapata (2008). For each phrase and set of candidate paraphras"
W11-1610,W09-0604,0,0.0180963,"re recent approaches in steganography have used paraphrase substitution to encode information in text but focus on grammaticality, not meaning preservation (Chang and Clark, 2010). Zhao et al. (2009) applied an adaptable paraphrasing pipeline to sentence 2 Taken from the main page of http://wsj.com, April 9, 2011. 85 compression, optimizing for F-measure over a manually annotated set of gold standard paraphrases. Sentence compression has been considered before in contexts outside of summarization, such as headline, title, and subtitle generation (Dorr et al., 2003; Vandeghinste and Pan, 2004; Marsi et al., 2009). Corston-Oliver (2001) deleted characters from words to shorten the character length of sentences. To our knowledge character-based compression has not been examined before with the surging popularity and utility of Twitter. 3 Sentence Tightening The distinction between tightening and compression can be illustrated by considering how much space needs to be preserved. In the case of microblogging, often a sentence has just a few too many characters and needs to be “tightened”. On the other hand, if a sentence is much longer than a desired length, more drastic compression is necessary. The firs"
W11-1610,E06-1038,0,0.613846,"Missing"
W11-1610,W11-1611,1,0.897263,"Missing"
W11-1610,D09-1041,0,0.227018,"Missing"
W11-1610,P93-1024,0,0.0224638,"used in combination in order to derive higher quality paraphrase candidates. See Table 1 for an example. The monolingual-filtering technique we describe is by no means limited to paraphrases extracted from bilingual corpora. It could be applied to other datadriven paraphrasing techniques (see Madnani and Dorr (2010) for a survey). Although it is particularly well suited to the bilingual extracted corpora, since the information that it adds is orthogonal to that model, it would presumably add less to paraphrasing techniques that already take advantage of monolingual distributional similarity (Pereira et al., 1993; Lin and Pantel, 2001; Barzilay and Lee, 2003). In order to evaluate the paraphrase candidates and scoring techniques, we randomly selected 1,000 paraphrase sets where the source phrase was present in the corpus described in Clarke and Lapata (2008). For each phrase and set of candidate paraphrases, we extracted all of the contexts from the corpus in which the source phrase appeared. Human judges were presented each sentence with the original phrase and the same sentences with each paraphrase candidate ... last week five farmers were thrown into jail in Ireland because they resisted ... ... l"
W11-1610,P05-1036,0,0.372896,"he original sentence length and compared these to compressions generated using just deletions. Manual evaluation found that the oracle-then-deletion compressions to preserve more meaning than deletion-only compressions at uniform compression rates. 2 Related work Most of the previous research on sentence compression focuses on deletion using syntactic information, (e.g., Galley and McKeown (2007), Knight and Marcu (2002), Nomoto (2009), Galanis and Androutsopoulos (2010), Filippova and Strube (2008), McDonald (2006), Yamangil and Shieber (2010), Cohn and Lapata (2008), Cohn and Lapata (2009), Turner and Charniak (2005)). Woodsend et al. (2010) incorporate paraphrase rules into a deletion model. Previous work in subtitling has made oneword substitutions to decrease character length at high compression rates (Glickman et al., 2006). More recent approaches in steganography have used paraphrase substitution to encode information in text but focus on grammaticality, not meaning preservation (Chang and Clark, 2010). Zhao et al. (2009) applied an adaptable paraphrasing pipeline to sentence 2 Taken from the main page of http://wsj.com, April 9, 2011. 85 compression, optimizing for F-measure over a manually annotate"
W11-1610,P10-2043,1,0.883009,"Missing"
W11-1610,W04-1015,0,0.0846973,"(Glickman et al., 2006). More recent approaches in steganography have used paraphrase substitution to encode information in text but focus on grammaticality, not meaning preservation (Chang and Clark, 2010). Zhao et al. (2009) applied an adaptable paraphrasing pipeline to sentence 2 Taken from the main page of http://wsj.com, April 9, 2011. 85 compression, optimizing for F-measure over a manually annotated set of gold standard paraphrases. Sentence compression has been considered before in contexts outside of summarization, such as headline, title, and subtitle generation (Dorr et al., 2003; Vandeghinste and Pan, 2004; Marsi et al., 2009). Corston-Oliver (2001) deleted characters from words to shorten the character length of sentences. To our knowledge character-based compression has not been examined before with the surging popularity and utility of Twitter. 3 Sentence Tightening The distinction between tightening and compression can be illustrated by considering how much space needs to be preserved. In the case of microblogging, often a sentence has just a few too many characters and needs to be “tightened”. On the other hand, if a sentence is much longer than a desired length, more drastic compression i"
W11-1610,D10-1050,0,0.0844201,"nd compared these to compressions generated using just deletions. Manual evaluation found that the oracle-then-deletion compressions to preserve more meaning than deletion-only compressions at uniform compression rates. 2 Related work Most of the previous research on sentence compression focuses on deletion using syntactic information, (e.g., Galley and McKeown (2007), Knight and Marcu (2002), Nomoto (2009), Galanis and Androutsopoulos (2010), Filippova and Strube (2008), McDonald (2006), Yamangil and Shieber (2010), Cohn and Lapata (2008), Cohn and Lapata (2009), Turner and Charniak (2005)). Woodsend et al. (2010) incorporate paraphrase rules into a deletion model. Previous work in subtitling has made oneword substitutions to decrease character length at high compression rates (Glickman et al., 2006). More recent approaches in steganography have used paraphrase substitution to encode information in text but focus on grammaticality, not meaning preservation (Chang and Clark, 2010). Zhao et al. (2009) applied an adaptable paraphrasing pipeline to sentence 2 Taken from the main page of http://wsj.com, April 9, 2011. 85 compression, optimizing for F-measure over a manually annotated set of gold standard pa"
W11-1610,P10-1096,0,0.036457,"Missing"
W11-1610,P09-1094,0,0.0208317,"Nomoto (2009), Galanis and Androutsopoulos (2010), Filippova and Strube (2008), McDonald (2006), Yamangil and Shieber (2010), Cohn and Lapata (2008), Cohn and Lapata (2009), Turner and Charniak (2005)). Woodsend et al. (2010) incorporate paraphrase rules into a deletion model. Previous work in subtitling has made oneword substitutions to decrease character length at high compression rates (Glickman et al., 2006). More recent approaches in steganography have used paraphrase substitution to encode information in text but focus on grammaticality, not meaning preservation (Chang and Clark, 2010). Zhao et al. (2009) applied an adaptable paraphrasing pipeline to sentence 2 Taken from the main page of http://wsj.com, April 9, 2011. 85 compression, optimizing for F-measure over a manually annotated set of gold standard paraphrases. Sentence compression has been considered before in contexts outside of summarization, such as headline, title, and subtitle generation (Dorr et al., 2003; Vandeghinste and Pan, 2004; Marsi et al., 2009). Corston-Oliver (2001) deleted characters from words to shorten the character length of sentences. To our knowledge character-based compression has not been examined before with t"
W11-1610,N03-1003,0,\N,Missing
W11-1611,W00-1401,0,0.0115426,"is corpus (Knight and Marcu, 2000), which contains a small set of 1067 extracted sentences from article/abstract pairs, and the manually annotated Clarke and Lapata (2008) corpus, consisting of nearly 3000 sentences from news articles and broadcast news transcripts. These corpora contain one gold standard for each sentence. 2.1 Automatic Techniques One of the most widely used automatic metrics is the F1 measure over grammatical relations of the goldstandard compressions (Riezler et al., 2003). This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al., 2000) for judging compression quality (Clarke and Lapata, 2006). F1 has also been used over unigrams (Martins and Smith, 2009) and bigrams (Unno et al., 2006). Unno et al. (2006) compared the F1 measures to BLEU scores (using the gold standard as a single reference) over varying compression rates, and found that BLEU behaves similarly to both F1 measures. A syntactic approach considers the alignment over parse trees (Jing, 2000), and a similar technique has been used with dependency trees to evaluate the quality of sentence fusions (Marsi and Krahmer, 2005). The only metric that has been shown to c"
W11-1611,W06-1421,0,0.0219611,"ressions should be grammatical and retain important meaning, they must be evaluated along these two dimensions. Evaluation is a difficult problem for NLG, and many of the problems identified in this work are relevant for other generation tasks. Shared tasks are popular in many areas as a way to compare system performance in an unbiased manner. Unlike other tasks, such as machine translation, there is no shared-task evaluation for compression, even though some compression systems are indirectly evaluated as a part of DUC. The benefits of shared-task evaluation have been discussed before (e.g., Belz and Kilgarriff (2006) and Reiter and Belz (2006)), and they include comparing systems fairly under the same conditions. One difficulty in evaluating compression systems fairly is that an unbiased automatic metric is hard to define. Automatic evaluation relies on a comparison to a single gold standard at a predetermined length, which greatly limits the types of compressions that can be fairly judged. As we will discuss in Section 2.1.1, automatic evaluation assumes that deletions are independent, considers only a single gold standard, and cannot handle compressions with paraphrasing. Like for most areas in NLG, hum"
W11-1611,E06-1032,1,0.734381,"eriving such corpora from existing corpora of multi-reference translations. The longest reference translation can be paired with the shortest reference to represent a long sentence and corresponding paraphrased goldstandard compression. Similar to machine translation or summarization, automatic translation of paraphrastic compressions would require multiple references to capture allowable variation, since there are often many equally valid ways of compressing an input. ROUGE or BLEU could be applied to a set of multiplereference compressions, although BLEU is not without its own shortcomings (Callison-Burch et al., 2006). One benefit of both ROUGE and BLEU is that they are based on n-gram recall and precision (respectively) instead of word-error rate, so reordering and word substitutions can be evaluated. Dorr et al. (2003) used BLEU for evaluation in the context of headline generation, which uses rewording and is related to sentence compression. Alternatively, manual evalation can be adapted from other NLG domains, such as the techniques described in the following section. 2.2 Manual Evaluation In order to determine semantic and syntactic suitability, manual evaluation is preferable over automatic techniques"
W11-1611,P06-1048,0,0.175001,"l set of 1067 extracted sentences from article/abstract pairs, and the manually annotated Clarke and Lapata (2008) corpus, consisting of nearly 3000 sentences from news articles and broadcast news transcripts. These corpora contain one gold standard for each sentence. 2.1 Automatic Techniques One of the most widely used automatic metrics is the F1 measure over grammatical relations of the goldstandard compressions (Riezler et al., 2003). This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al., 2000) for judging compression quality (Clarke and Lapata, 2006). F1 has also been used over unigrams (Martins and Smith, 2009) and bigrams (Unno et al., 2006). Unno et al. (2006) compared the F1 measures to BLEU scores (using the gold standard as a single reference) over varying compression rates, and found that BLEU behaves similarly to both F1 measures. A syntactic approach considers the alignment over parse trees (Jing, 2000), and a similar technique has been used with dependency trees to evaluate the quality of sentence fusions (Marsi and Krahmer, 2005). The only metric that has been shown to correlate with human judgments is F1 (Clarke and Lapata, 20"
W11-1611,D07-1001,0,0.461746,"inal sentence is preserved. Decisions are rated along a 5-point scale (LDC, 2005). Most compression systems consider sentences out of context (a few exceptions exist, e.g., Daum´e III and Marcu (2002), Martins and Smith (2009), and Lin (2003)). Contextual cues and discourse structure may not be a factor to consider if the sentences are generated for use out of context. An example of a context-aware approach considered the summaries formed by shortened sentences and evaluated the compression systems based on how well people could answer questions about the original document from the summaries (Clarke and Lapata, 2007). This technique has been used before for evaluating summarization and text comprehension (Mani et al., 2002; Morris et al., 1992). 2.2.1 Pitfalls of Manual Evaluation Grammar judgments decrease when the compression is presented alongside the original sentence. Figure 1 shows that the mean grammar rating for the same compressions is on average about 0.3 points higher when the compression is judged in isolation. Researchers should be careful to state when grammar is judged on compressions lacking reference sentences. Another factor is the group of judges. Obviously different studies will rely o"
W11-1611,C08-1018,0,0.383827,"tems could instead report the quality of compressions at several different compression rates, as Nomoto (2008) did. Alternatively, systems could evaluate compressions that are of a similar length as the gold standard compression, to fix a length for the purpose of evaluation. Output length is controlled for evaluation in some other areas, notably DUC. Systems compress by deletion and not substitution. More recent approaches to compression introduce reordering and paraphrase operations (e.g., dencies (Briscoe, 2006) while there are over 50 Stanford Dependencies (de Marneffe and Manning, 2008). Cohn and Lapata (2008), Woodsend et al. (2010), and Napoles et al. (2011)). For paraphrastic compressions, manual evaluation alone reliably determines the compression quality. Because automatic evaluation metrics compare shortened sentences to extractive gold standards, they cannot be applied to paraphrastic compression. To apply automatic techniques to substitutionbased compression, one would need a gold-standard set of paraphrastic compressions. These are rare. Cohn and Lapata (2008) created an abstractive corpus, which contains word reordering and paraphrasing in addition to deletion. Unfortunately, this corpus"
W11-1611,P02-1057,0,0.212377,"Missing"
W11-1611,W03-0501,0,0.0521744,"andard compression. Similar to machine translation or summarization, automatic translation of paraphrastic compressions would require multiple references to capture allowable variation, since there are often many equally valid ways of compressing an input. ROUGE or BLEU could be applied to a set of multiplereference compressions, although BLEU is not without its own shortcomings (Callison-Burch et al., 2006). One benefit of both ROUGE and BLEU is that they are based on n-gram recall and precision (respectively) instead of word-error rate, so reordering and word substitutions can be evaluated. Dorr et al. (2003) used BLEU for evaluation in the context of headline generation, which uses rewording and is related to sentence compression. Alternatively, manual evalation can be adapted from other NLG domains, such as the techniques described in the following section. 2.2 Manual Evaluation In order to determine semantic and syntactic suitability, manual evaluation is preferable over automatic techniques whenever possible. The most widely practiced manual evaluation methodology was first used by Knight and Marcu (2002). Judges grade each compressed sentence against the original and make two separate decisio"
W11-1611,N10-1131,0,0.0550021,": • highlight the importance of comparing systems with similar compression rates, • argue that comparisons in many previous publications are invalid, • provide suggestions for unbiased evaluation. While many may find this discussion intuitive, these points are not addressed in much of the existing research, and therefore it is crucial to enumerate them in order to improve the scientific validity of the task. 2 Current Practices Because it was developed in support of extractive summarization (Knight and Marcu, 2000), compression has mostly been framed as a deletion task (e.g., McDonald (2006), Galanis and Androutsopoulos (2010), Clarke and Lapata (2008), and Galley 91 Workshop on Monolingual Text-To-Text Generation, pages 91–97, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 91–97, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics Words Sentence 31 Kaczynski faces charges contained in a 10-count federal indictment naming him as the person responsible for transporting bombs and bomb parts from Montana to California and mailing them to victims . 17 Kaczynski faces charges naming him responsible for transporting bombs to California and maili"
W11-1611,N07-1023,0,0.466241,"Missing"
W11-1611,A00-1043,0,0.75034,"ns of the goldstandard compressions (Riezler et al., 2003). This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al., 2000) for judging compression quality (Clarke and Lapata, 2006). F1 has also been used over unigrams (Martins and Smith, 2009) and bigrams (Unno et al., 2006). Unno et al. (2006) compared the F1 measures to BLEU scores (using the gold standard as a single reference) over varying compression rates, and found that BLEU behaves similarly to both F1 measures. A syntactic approach considers the alignment over parse trees (Jing, 2000), and a similar technique has been used with dependency trees to evaluate the quality of sentence fusions (Marsi and Krahmer, 2005). The only metric that has been shown to correlate with human judgments is F1 (Clarke and Lapata, 2006), but even this is not entirely reliable. F1 over grammatical relations also depends on parser accuracy and the type of dependency relations used.1 1 For example, the RASP parser uses 16 grammatical depen92 2.1.1 Pitfalls of Automatic Evaluation Automatic evaluation operates under three often incorrect assumptions: Deletions are independent. The dependency structu"
W11-1611,W03-1101,0,0.030491,"ntactic suitability, manual evaluation is preferable over automatic techniques whenever possible. The most widely practiced manual evaluation methodology was first used by Knight and Marcu (2002). Judges grade each compressed sentence against the original and make two separate decisions: how grammatical 93 is the compression and how much of the meaning from the original sentence is preserved. Decisions are rated along a 5-point scale (LDC, 2005). Most compression systems consider sentences out of context (a few exceptions exist, e.g., Daum´e III and Marcu (2002), Martins and Smith (2009), and Lin (2003)). Contextual cues and discourse structure may not be a factor to consider if the sentences are generated for use out of context. An example of a context-aware approach considered the summaries formed by shortened sentences and evaluated the compression systems based on how well people could answer questions about the original document from the summaries (Clarke and Lapata, 2007). This technique has been used before for evaluating summarization and text comprehension (Mani et al., 2002; Morris et al., 1992). 2.2.1 Pitfalls of Manual Evaluation Grammar judgments decrease when the compression is"
W11-1611,W05-1612,0,0.0131887,"and is better than Simple String Accuracy (Bangalore et al., 2000) for judging compression quality (Clarke and Lapata, 2006). F1 has also been used over unigrams (Martins and Smith, 2009) and bigrams (Unno et al., 2006). Unno et al. (2006) compared the F1 measures to BLEU scores (using the gold standard as a single reference) over varying compression rates, and found that BLEU behaves similarly to both F1 measures. A syntactic approach considers the alignment over parse trees (Jing, 2000), and a similar technique has been used with dependency trees to evaluate the quality of sentence fusions (Marsi and Krahmer, 2005). The only metric that has been shown to correlate with human judgments is F1 (Clarke and Lapata, 2006), but even this is not entirely reliable. F1 over grammatical relations also depends on parser accuracy and the type of dependency relations used.1 1 For example, the RASP parser uses 16 grammatical depen92 2.1.1 Pitfalls of Automatic Evaluation Automatic evaluation operates under three often incorrect assumptions: Deletions are independent. The dependency structure of a sentence may be unaltered when dependent words are not deleted as a unit. Examples of words that should be treated as a sin"
W11-1611,W09-1801,0,0.0717532,"and the manually annotated Clarke and Lapata (2008) corpus, consisting of nearly 3000 sentences from news articles and broadcast news transcripts. These corpora contain one gold standard for each sentence. 2.1 Automatic Techniques One of the most widely used automatic metrics is the F1 measure over grammatical relations of the goldstandard compressions (Riezler et al., 2003). This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al., 2000) for judging compression quality (Clarke and Lapata, 2006). F1 has also been used over unigrams (Martins and Smith, 2009) and bigrams (Unno et al., 2006). Unno et al. (2006) compared the F1 measures to BLEU scores (using the gold standard as a single reference) over varying compression rates, and found that BLEU behaves similarly to both F1 measures. A syntactic approach considers the alignment over parse trees (Jing, 2000), and a similar technique has been used with dependency trees to evaluate the quality of sentence fusions (Marsi and Krahmer, 2005). The only metric that has been shown to correlate with human judgments is F1 (Clarke and Lapata, 2006), but even this is not entirely reliable. F1 over grammatica"
W11-1611,E06-1038,0,0.716021,". In this work we: • highlight the importance of comparing systems with similar compression rates, • argue that comparisons in many previous publications are invalid, • provide suggestions for unbiased evaluation. While many may find this discussion intuitive, these points are not addressed in much of the existing research, and therefore it is crucial to enumerate them in order to improve the scientific validity of the task. 2 Current Practices Because it was developed in support of extractive summarization (Knight and Marcu, 2000), compression has mostly been framed as a deletion task (e.g., McDonald (2006), Galanis and Androutsopoulos (2010), Clarke and Lapata (2008), and Galley 91 Workshop on Monolingual Text-To-Text Generation, pages 91–97, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 91–97, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics Words Sentence 31 Kaczynski faces charges contained in a 10-count federal indictment naming him as the person responsible for transporting bombs and bomb parts from Montana to California and mailing them to victims . 17 Kaczynski faces charges naming him responsible for transp"
W11-1611,W11-1610,1,0.896071,"Missing"
W11-1611,P08-1035,0,0.0177534,"compression. Automatic evaluation considers a single gold-standard compression. This ignores the possibility of different length compressions and equally good compressions of the same length, where multiple non-overlapping deletions are acceptable. For an example, see Table 1. Having multiple gold standards would provide references at different compression lengths and reflect different deletion choices (see Section 3). Since no large corpus with multiple gold standards exists to our knowledge, systems could instead report the quality of compressions at several different compression rates, as Nomoto (2008) did. Alternatively, systems could evaluate compressions that are of a similar length as the gold standard compression, to fix a length for the purpose of evaluation. Output length is controlled for evaluation in some other areas, notably DUC. Systems compress by deletion and not substitution. More recent approaches to compression introduce reordering and paraphrase operations (e.g., dencies (Briscoe, 2006) while there are over 50 Stanford Dependencies (de Marneffe and Manning, 2008). Cohn and Lapata (2008), Woodsend et al. (2010), and Napoles et al. (2011)). For paraphrastic compressions, man"
W11-1611,D09-1041,0,0.0712496,"ammatically loss. 4 Mismatched Comparisons We have observed that a difference in compression rates as small as 5 percentage points can influence the quality ratings by as much as 0.1 points and conclude: systems must be compared using similar levels of compression. In particular, if system A’s output is higher quality, but longer than system B’s, then it is not necessarily the case that A is better than B. Conversely, if B has results at least as good as system A, one can claim that B is better, since B’s output is shorter. Here are some examples in the literature of mismatched comparisons: • Nomoto (2009) concluded their system significantly outperformed that of Cohn and Lapata (2008). However, the compression rate of their system ranged from 45 to 74, while the compression rate of Cohn and Lapata (2008) was 35. This claim is unverifiable without further comparison. • Clarke and Lapata (2007), when comparing against McDonald (2006), reported significantly better results at a 5-point higher compression rate. At first glance, this does not seem like a remarkable difference. However, Model C&L McD C&L McD Meaning 3.83 3.94 3.76∗ 3.50∗ Grammar 3.66 3.87 3.53∗ 3.17∗ 5 CompR 64.1 64.2 78.4∗ 68.5∗ Ta"
W11-1611,W06-1422,0,0.018026,"and retain important meaning, they must be evaluated along these two dimensions. Evaluation is a difficult problem for NLG, and many of the problems identified in this work are relevant for other generation tasks. Shared tasks are popular in many areas as a way to compare system performance in an unbiased manner. Unlike other tasks, such as machine translation, there is no shared-task evaluation for compression, even though some compression systems are indirectly evaluated as a part of DUC. The benefits of shared-task evaluation have been discussed before (e.g., Belz and Kilgarriff (2006) and Reiter and Belz (2006)), and they include comparing systems fairly under the same conditions. One difficulty in evaluating compression systems fairly is that an unbiased automatic metric is hard to define. Automatic evaluation relies on a comparison to a single gold standard at a predetermined length, which greatly limits the types of compressions that can be fairly judged. As we will discuss in Section 2.1.1, automatic evaluation assumes that deletions are independent, considers only a single gold standard, and cannot handle compressions with paraphrasing. Like for most areas in NLG, human evaluation is preferable"
W11-1611,N03-1026,0,0.467005,"s will be few and therefore gold-standard compressions must be manually annotated. The most popular corpora are the Ziff-Davis corpus (Knight and Marcu, 2000), which contains a small set of 1067 extracted sentences from article/abstract pairs, and the manually annotated Clarke and Lapata (2008) corpus, consisting of nearly 3000 sentences from news articles and broadcast news transcripts. These corpora contain one gold standard for each sentence. 2.1 Automatic Techniques One of the most widely used automatic metrics is the F1 measure over grammatical relations of the goldstandard compressions (Riezler et al., 2003). This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al., 2000) for judging compression quality (Clarke and Lapata, 2006). F1 has also been used over unigrams (Martins and Smith, 2009) and bigrams (Unno et al., 2006). Unno et al. (2006) compared the F1 measures to BLEU scores (using the gold standard as a single reference) over varying compression rates, and found that BLEU behaves similarly to both F1 measures. A syntactic approach considers the alignment over parse trees (Jing, 2000), and a similar technique has been used with de"
W11-1611,P06-2109,0,0.734019,"Lapata (2008) corpus, consisting of nearly 3000 sentences from news articles and broadcast news transcripts. These corpora contain one gold standard for each sentence. 2.1 Automatic Techniques One of the most widely used automatic metrics is the F1 measure over grammatical relations of the goldstandard compressions (Riezler et al., 2003). This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al., 2000) for judging compression quality (Clarke and Lapata, 2006). F1 has also been used over unigrams (Martins and Smith, 2009) and bigrams (Unno et al., 2006). Unno et al. (2006) compared the F1 measures to BLEU scores (using the gold standard as a single reference) over varying compression rates, and found that BLEU behaves similarly to both F1 measures. A syntactic approach considers the alignment over parse trees (Jing, 2000), and a similar technique has been used with dependency trees to evaluate the quality of sentence fusions (Marsi and Krahmer, 2005). The only metric that has been shown to correlate with human judgments is F1 (Clarke and Lapata, 2006), but even this is not entirely reliable. F1 over grammatical relations also depends on pars"
W11-1611,D10-1050,0,0.0222828,"t the quality of compressions at several different compression rates, as Nomoto (2008) did. Alternatively, systems could evaluate compressions that are of a similar length as the gold standard compression, to fix a length for the purpose of evaluation. Output length is controlled for evaluation in some other areas, notably DUC. Systems compress by deletion and not substitution. More recent approaches to compression introduce reordering and paraphrase operations (e.g., dencies (Briscoe, 2006) while there are over 50 Stanford Dependencies (de Marneffe and Manning, 2008). Cohn and Lapata (2008), Woodsend et al. (2010), and Napoles et al. (2011)). For paraphrastic compressions, manual evaluation alone reliably determines the compression quality. Because automatic evaluation metrics compare shortened sentences to extractive gold standards, they cannot be applied to paraphrastic compression. To apply automatic techniques to substitutionbased compression, one would need a gold-standard set of paraphrastic compressions. These are rare. Cohn and Lapata (2008) created an abstractive corpus, which contains word reordering and paraphrasing in addition to deletion. Unfortunately, this corpus is small (575 sentences)"
W11-2504,P05-1074,1,0.911847,"ng Monolingual Distributional Similarity Tsz Ping Chan, Chris Callison-Burch and Benjamin Van Durme Center for Language and Speech Processing, and HLTCOE Johns Hopkins University Abstract put of a pivot-based bilingual paraphrase model. In this paper we investigate the strengths and weaknesses of scoring paraphrases using monolingual distributional similarity versus the bilingually calculated paraphrase probability. We show that monolingual cosine similarity calculated on large volumes of text ranks bilingually-extracted paraphrases better than the paraphrase probability originally defined by Bannard and Callison-Burch (2005). While our current implementation shows improvement mainly in grammaticality, other contextual features are expected to enhance the meaning preservation of paraphrases. We also show that monolingual scores can provide a reasonable threshold for picking out high precision paraphrases. This paper improves an existing bilingual paraphrase extraction technique using monolingual distributional similarity to rerank candidate paraphrases. Raw monolingual data provides a complementary and orthogonal source of information that lessens the commonly observed errors in bilingual pivotbased methods. Our e"
W11-2504,P08-1077,0,0.577352,"Missing"
W11-2504,D08-1021,1,0.924614,"translation probabilities from grams are no longer than 5 tokens by design), it was not feasible to parse, which led to the use of n-gram a statistical translation model. Anecdotally, this paraphrase probability some- contexts. Here we use adjacent unigrams. For each times seems unable to discriminate between good phrase x we wished to paraphrase, we extracted the and bad paraphrases, so some researchers disregard context vector of x from the n-gram collection as it and treat the extracted paraphrases as an unsorted such: every (n-gram, frequency) pair of the form: set (Snover et al., 2010). Callison-Burch (2008) (ax, f ), or (xb, f ), gave rise to the (feature, value) attempts to improve the ranking by limiting para- pair: (wi−1 =a, f ), or (wi+1 =b, f ), respectively. In order to scale to this size of a collection, we relied phrases to be the same syntactic type. We attempt to rerank the paraphrases using other on Locality Sensitive Hashing (LSH), as was done information. This is similar to the efforts of Zhao previously by Ravichandran et al. (2005) and Bhaet al. (2008), who made use of multiple resources to gat and Ravichandran (2008). To avoid computing derive feature functions and extract paraph"
W11-2504,N03-1017,0,0.0030838,"irk et al., 2004). We exploit both methodologies, applying a monolingually-derived similarity metric to the outRelated Work Paraphrase Extraction from Bitexts Bannard and Callison-Burch (2005) proposed identifying paraphrases by pivoting through phrases in a bilingual parallel corpora. Figure 1 illustrates their paraphrase extraction process. The target phrase, e.g. thrown into jail, is found in a German-English parallel corpus. The corresponding foreign phrase (festgenommen) is identified using word alignment and phrase extraction techniques from phrase-based statistical machine translation (Koehn et al., 2003). Other occurrences of the foreign phrase in the parallel corpus may align to a distinct English phrase, such as jailed. As the original phrase occurs several times and aligns with many different foreign phrases, each of these may align to a variety of other English paraphrases. Thus, thrown into jail not only paraphrases as jailed, but also as arrested, detained, imprisoned, incarcerated, locked up, and so on. Bad paraphrases, such as maltreated, thrown, cases, custody, arrest, and protection, may also arise due to poor word alignment quality and other factors. 33 Proceedings of the GEMS 2011"
W11-2504,2005.mtsummit-papers.11,0,0.0206692,"paraphrases since, by construction, they do not share any foreign translation and hence their paraphrase scores are not defined. As expected from the drawbacks of monolingual-based statistics, willing and eager are assigned top scores by MonoDS, although good paraphrases such as somewhat reluctant and disinclined are also ranked highly. This illustrates how BiP complements the monolingual reranking technique by providing orthogonal information to address the issue of antonyms for MonoDS. 3.3 Implementation Details For BiP and SyntBiP, the French-English parallel text from the Europarl corpus (Koehn, 2005) was used to train the paraphrase model. The parallel corpus was extracted from proceedings of the European parliament with a total of about 1.3 million sentences and close to 97 million words in the English text. Word alignments were generated with the Berkeley aligner. For SyntBiP, the English side of the parallel corpus was parsed using the Stanford parser (Klein and Manning, 2003). The translation models were trained with Thrax, a grammar extractor for machine translation (Weese et al., 2011). Thrax extracts phrase pairs that are labeled with complex syntactic labels following Zollmann and"
W11-2504,lin-etal-2010-new,0,0.0231586,"ilingual parallel corpus to extract textually similar partly because they both often apparaphrases. pear as the object of the verb eat. While syntacBannard and Callison-Burch (2005) defined a tic contexts provide strong evidence of distributional paraphrase probability to rank these paraphrase can- preferences, it is computationally expensive to parse very large corpora, so it is also common to represent didates, as follows: context vectors with simpler representations like adeˆ2 = arg max p(e2 |e1 ) (1) jacent words and n-grams (Lapata and Keller, 2005; e2 6=e1 Bhagat and Ravichandran, 2008; Lin et al., 2010; X p(e2 |e1 ) = p(e2 , f |e1 ) (2) Van Durme and Lall, 2010). In these models, apf ple and orange might be judged similar because both X tend to be one word to the right of some, and one to = p(e2 |f, e1 )p(f |e1 ) (3) the left of juice. f X Here we calculate distributional similarity using a ≈ p(e2 |f )p(f |e1 ) (4) web-scale n-gram corpus (Brants and Franz, 2006; f Lin et al., 2010). Given both the size of the collecwhere p(e2 |e1 ) is the paraphrase probability, and tion, and that the n-grams are sub-sentential (the np(e|f ) and p(f |e) are translation probabilities from grams are no longe"
W11-2504,P97-1009,0,0.0814776,"assessment of paraphrase quality in terms of grammaticality, yet have minimal effects on meaning preservation of paraphrases. While we speculated that MonoDS would improve both meaning and grammar scoring for paraphrases, we found in the results that only grammaticality was improved from the monolingual approach. This is likely due to the choice of how context is represented, which in this case is only single neighboring words. A consideration for future work to enhance paraphrasal meaning preservation would be to explore other contextual representations, such as syntactic dependency parsing (Lin, 1997), mutual information between co-occurences of phrases Church and Hanks (1991), or increasing the number of neighboring words used in n-gram based representations. In future work we will make use of other complementary bilingual and monolingual knowledge sources by combining other features such as n-gram length, language model scores, etc. One approach would be to perform minimum error rate training similar to Zhao et al. (2008) in which linear weights of a feature function for a set of paraphrases candidate are trained iteratively to minimize the phrasalsubstitution-based error rate. Instead o"
W11-2504,J10-3003,0,0.0520831,"errors in bilingual pivotbased methods. Our experiments reveal that monolingual scoring of bilingually extracted paraphrases has a significantly stronger correlation with human judgment for grammaticality than the probabilities assigned by the bilingual pivoting method does. The results also show that monolingual distribution similarity can serve as a threshold for high precision paraphrase selection. 2 1 Introduction 2.1 Paraphrasing is the rewording of a phrase such that meaning is preserved. Data-driven paraphrase acquisition techniques can be categorized by the type of data that they use (Madnani and Dorr, 2010). Monolingual paraphrasing techniques cluster phrases through statistical characteristics such as dependency path similarities or distributional cooccurrence information (Lin and Pantel, 2001; Pasca and Dienes, 2005). Bilingual paraphrasing techniques use parallel corpora to extract potential paraphrases by grouping English phrases that share the same foreign translations (Bannard and CallisonBurch, 2005). Other efforts blur the lines between the two, applying techniques from statistical machine translation to monolingual data or extracting paraphrases from multiple English translations of the"
W11-2504,P08-1118,0,0.0278629,"Missing"
W11-2504,D08-1103,0,0.00578013,"he paraphrase was assigned a low score of 0.098 as compared to other paraphrase candidates with the correct syntactic type. Note that the SyntBiP produced significantly fewer paraphrase candidates, since its paraphrase candidates must be the same syntactic type as the original phrase. Identity paraphrases are excluded for the rest of the discussion in this paper. 3.2 Susceptibility to Antonyms Monolingual distributional similarity is widely known to conflate words with opposite meaning and has motivated a large body of prior work on antonym detection (Lin and Zhao, 2003; Lin and Pantel, 2001; Mohammad et al., 2008a; Mohammad et al., 2008b; Marneffe et al., 2008; Voorhees, 2008). In contrast, the antonyms of a phrase are rarely produced during pivoting of the BiP methods because they tend not to share the same foreign translations. Since the reranking framework proposed here begins with paraphrases acquired by the BiP methodology, MonoDS can considerably enhance the quality of ranking while sidestepping the antonym problem that arises from using MonoDS alone. To support this intuition, an example of a paraphrase list with inserted hand-selected phrases ranked by each reranking methods is shown in Table"
W11-2504,N03-1024,0,0.0723111,"rases through statistical characteristics such as dependency path similarities or distributional cooccurrence information (Lin and Pantel, 2001; Pasca and Dienes, 2005). Bilingual paraphrasing techniques use parallel corpora to extract potential paraphrases by grouping English phrases that share the same foreign translations (Bannard and CallisonBurch, 2005). Other efforts blur the lines between the two, applying techniques from statistical machine translation to monolingual data or extracting paraphrases from multiple English translations of the same foreign text (Barzilay and McKeown, 2001; Pang et al., 2003; Quirk et al., 2004). We exploit both methodologies, applying a monolingually-derived similarity metric to the outRelated Work Paraphrase Extraction from Bitexts Bannard and Callison-Burch (2005) proposed identifying paraphrases by pivoting through phrases in a bilingual parallel corpora. Figure 1 illustrates their paraphrase extraction process. The target phrase, e.g. thrown into jail, is found in a German-English parallel corpus. The corresponding foreign phrase (festgenommen) is identified using word alignment and phrase extraction techniques from phrase-based statistical machine translati"
W11-2504,I05-1011,0,0.178451,"obabilities assigned by the bilingual pivoting method does. The results also show that monolingual distribution similarity can serve as a threshold for high precision paraphrase selection. 2 1 Introduction 2.1 Paraphrasing is the rewording of a phrase such that meaning is preserved. Data-driven paraphrase acquisition techniques can be categorized by the type of data that they use (Madnani and Dorr, 2010). Monolingual paraphrasing techniques cluster phrases through statistical characteristics such as dependency path similarities or distributional cooccurrence information (Lin and Pantel, 2001; Pasca and Dienes, 2005). Bilingual paraphrasing techniques use parallel corpora to extract potential paraphrases by grouping English phrases that share the same foreign translations (Bannard and CallisonBurch, 2005). Other efforts blur the lines between the two, applying techniques from statistical machine translation to monolingual data or extracting paraphrases from multiple English translations of the same foreign text (Barzilay and McKeown, 2001; Pang et al., 2003; Quirk et al., 2004). We exploit both methodologies, applying a monolingually-derived similarity metric to the outRelated Work Paraphrase Extraction f"
W11-2504,W04-3219,0,0.0233104,"stical characteristics such as dependency path similarities or distributional cooccurrence information (Lin and Pantel, 2001; Pasca and Dienes, 2005). Bilingual paraphrasing techniques use parallel corpora to extract potential paraphrases by grouping English phrases that share the same foreign translations (Bannard and CallisonBurch, 2005). Other efforts blur the lines between the two, applying techniques from statistical machine translation to monolingual data or extracting paraphrases from multiple English translations of the same foreign text (Barzilay and McKeown, 2001; Pang et al., 2003; Quirk et al., 2004). We exploit both methodologies, applying a monolingually-derived similarity metric to the outRelated Work Paraphrase Extraction from Bitexts Bannard and Callison-Burch (2005) proposed identifying paraphrases by pivoting through phrases in a bilingual parallel corpora. Figure 1 illustrates their paraphrase extraction process. The target phrase, e.g. thrown into jail, is found in a German-English parallel corpus. The corresponding foreign phrase (festgenommen) is identified using word alignment and phrase extraction techniques from phrase-based statistical machine translation (Koehn et al., 200"
W11-2504,P05-1077,0,0.0246964,"the n-gram collection as it and treat the extracted paraphrases as an unsorted such: every (n-gram, frequency) pair of the form: set (Snover et al., 2010). Callison-Burch (2008) (ax, f ), or (xb, f ), gave rise to the (feature, value) attempts to improve the ranking by limiting para- pair: (wi−1 =a, f ), or (wi+1 =b, f ), respectively. In order to scale to this size of a collection, we relied phrases to be the same syntactic type. We attempt to rerank the paraphrases using other on Locality Sensitive Hashing (LSH), as was done information. This is similar to the efforts of Zhao previously by Ravichandran et al. (2005) and Bhaet al. (2008), who made use of multiple resources to gat and Ravichandran (2008). To avoid computing derive feature functions and extract paraphrase ta- feature vectors explicitly, which can be a memory bles. The paraphrase that maximizes a log-linear intensive bottleneck, we employed the online LSH combination of various feature functions is then se- variant described by Van Durme and Lall (2010). This variant, based on the earlier work of Indyk lected as the optimal paraphrase. Feature weights in the model are optimized by minimizing a phrase and Motwani (1998) and Charikar (2002), a"
W11-2504,P07-1058,0,0.0236409,"us consists of at most 5-gram and each distributional similarity feature requires a single neighboring token, the LSH signatures are generated only for phrases that are 4-gram or less. Phrases that didn’t appear in the n-grams with at least one feature were discarded. 4 Human Evaluation The different paraphrase scoring methods were compared through a manual evaluation conducted on Amazon Mechanical Turk. A set of 100 test phrases were selected and for each test phrase, five distinct sentences were randomly sampled to capture the fact that paraphrases are valid in some contexts but not others (Szpektor et al., 2007). Judges evaluated the paraphrase quality through a substitution test: For each sampled sentence, the test phrase is substituted with automatically-generated paraphrases. The sentences and the phrases are drawn from the English side of the Europarl corpus. Judges indicated the amount of the original meaning preserved by the paraphrases and the grammaticality of the resulting sentences. They assigned two values to each sentence using the 5-point scales defined in CallisonBurch (2008). The 100 test phrases consisted of 25 unigrams, 25 bigrams, 25 trigrams and 25 4-grams. These 25 phrases were ra"
W11-2504,P10-2043,1,0.82626,"Missing"
W11-2504,P08-1008,0,0.0210868,"raphrase candidates with the correct syntactic type. Note that the SyntBiP produced significantly fewer paraphrase candidates, since its paraphrase candidates must be the same syntactic type as the original phrase. Identity paraphrases are excluded for the rest of the discussion in this paper. 3.2 Susceptibility to Antonyms Monolingual distributional similarity is widely known to conflate words with opposite meaning and has motivated a large body of prior work on antonym detection (Lin and Zhao, 2003; Lin and Pantel, 2001; Mohammad et al., 2008a; Mohammad et al., 2008b; Marneffe et al., 2008; Voorhees, 2008). In contrast, the antonyms of a phrase are rarely produced during pivoting of the BiP methods because they tend not to share the same foreign translations. Since the reranking framework proposed here begins with paraphrases acquired by the BiP methodology, MonoDS can considerably enhance the quality of ranking while sidestepping the antonym problem that arises from using MonoDS alone. To support this intuition, an example of a paraphrase list with inserted hand-selected phrases ranked by each reranking methods is shown in Table 21 . Hand-selected antonyms of reluctant are inserted into the pa"
W11-2504,W11-2160,1,0.0373664,".3 Implementation Details For BiP and SyntBiP, the French-English parallel text from the Europarl corpus (Koehn, 2005) was used to train the paraphrase model. The parallel corpus was extracted from proceedings of the European parliament with a total of about 1.3 million sentences and close to 97 million words in the English text. Word alignments were generated with the Berkeley aligner. For SyntBiP, the English side of the parallel corpus was parsed using the Stanford parser (Klein and Manning, 2003). The translation models were trained with Thrax, a grammar extractor for machine translation (Weese et al., 2011). Thrax extracts phrase pairs that are labeled with complex syntactic labels following Zollmann and Venugopal (2006). For MonoDS, the web-scale n-gram collection of Lin et al. (2010) was used to compute the monolingual distributional similarity features, using 512 bits per signature in the resultant LSH projection. Following Van Durme and Lall (2010), we implic1 Generating a paraphrase list by MonoDS alone requires building features for all phrases in the corpus, which is computationally impractical and hence, was not considered here. 36 itly represented the projection matrix with a pool of si"
W11-2504,P08-1116,0,0.0696498,"g words. A consideration for future work to enhance paraphrasal meaning preservation would be to explore other contextual representations, such as syntactic dependency parsing (Lin, 1997), mutual information between co-occurences of phrases Church and Hanks (1991), or increasing the number of neighboring words used in n-gram based representations. In future work we will make use of other complementary bilingual and monolingual knowledge sources by combining other features such as n-gram length, language model scores, etc. One approach would be to perform minimum error rate training similar to Zhao et al. (2008) in which linear weights of a feature function for a set of paraphrases candidate are trained iteratively to minimize the phrasalsubstitution-based error rate. Instead of phrasal substitution in Zhao’s method, quantitative measure of correlation with human judgment can be used as the objective function to be optimized during training. Other techniques such as SVM-rank (Joachims, 2002) may also be investigated for aggregating results from multiple ranked lists. 8 Acknowledgements Thanks to Courtney Napoles for advice regarding a pilot version of this work. Thanks to Jonathan Weese, Matt Post an"
W11-2504,W06-3119,0,0.0599526,"Koehn, 2005) was used to train the paraphrase model. The parallel corpus was extracted from proceedings of the European parliament with a total of about 1.3 million sentences and close to 97 million words in the English text. Word alignments were generated with the Berkeley aligner. For SyntBiP, the English side of the parallel corpus was parsed using the Stanford parser (Klein and Manning, 2003). The translation models were trained with Thrax, a grammar extractor for machine translation (Weese et al., 2011). Thrax extracts phrase pairs that are labeled with complex syntactic labels following Zollmann and Venugopal (2006). For MonoDS, the web-scale n-gram collection of Lin et al. (2010) was used to compute the monolingual distributional similarity features, using 512 bits per signature in the resultant LSH projection. Following Van Durme and Lall (2010), we implic1 Generating a paraphrase list by MonoDS alone requires building features for all phrases in the corpus, which is computationally impractical and hence, was not considered here. 36 itly represented the projection matrix with a pool of size 10,000. In order to expand the coverage of the candidates scored by the monolingual method, the LSH signatures ar"
W11-2504,J90-1003,0,\N,Missing
W11-2504,P01-1008,0,\N,Missing
W12-1904,P10-1112,0,0.0148274,", and though Monte Carlo techniques can provide an approximation, the samplers can be complex, difficult to code, and slow to converge. This history suggests two approaches to state-split TSGs: (1) a Bayesian non-parametric sampling approach (incorporate state-splitting into existing TSG work), or (2) EM (incorporate TSG induction into existing state-splitting work). We choose the latter path, and in the next section will describe our approach which combines the simplicity of DOP, the intuitions motivating the Bayesian approach, and the efficiency of EM-based state-splitting. In related work, Bansal and Klein (2010) combine (1996)’s implicit DOP representation with a number of the manual refinements described in Klein and Manning (2003). They achieve some of the best reported parsing scores for TSG work and demonstrate the complementarity of the tasks, but their approach is not able to learn arbitrary distributions over fragments, and the state splits are determined in a fixed pre-processing step. Our approach addresses both of these limitations. 3 State-Split TSG Induction In this section we describe how we combine the ideas of dop, Bayesian-induced TSGs and Petrov et al. (2006)’s state-splitting framew"
W12-1904,E93-1006,0,0.272999,"contextfree grammars by generalizing the atomic units of the grammar from depth-one productions to fragments of arbitrary size. An example TSG fragment along with equivalent CFG rules are depicted in Figure 1. The two formalisms are weakly equivalent, and computing the most probable derivation of a sentence with a TSG can be done in cubic time. Unfortunately, learning TSGs is not straightforward, in large part because TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. One class of existing approaches, known as Data-Oriented Parsing, simply uses all the fragments (Bod, 1993, DOP). This does not scale well to large treebanks, forcing the use of implicit representations (Goodman, 1996) or heuristic subsets (Bod, 2001). It has also been generally observed that the use of all fragments results in poor, overfit grammars, though this can be addressed with held-out data (Zollmann and Sima’an, 2005) or statistical estimators to rule out fragments that are unlikely to generalize (Zuidema, 2007). More recently, a number of groups have found success employing Bayesian non-parametric priors (Post and Gildea, 2009; Cohn et al., 2010), which put a downward pressure on fragmen"
W12-1904,P01-1010,0,0.0368287,"ragment along with equivalent CFG rules are depicted in Figure 1. The two formalisms are weakly equivalent, and computing the most probable derivation of a sentence with a TSG can be done in cubic time. Unfortunately, learning TSGs is not straightforward, in large part because TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. One class of existing approaches, known as Data-Oriented Parsing, simply uses all the fragments (Bod, 1993, DOP). This does not scale well to large treebanks, forcing the use of implicit representations (Goodman, 1996) or heuristic subsets (Bod, 2001). It has also been generally observed that the use of all fragments results in poor, overfit grammars, though this can be addressed with held-out data (Zollmann and Sima’an, 2005) or statistical estimators to rule out fragments that are unlikely to generalize (Zuidema, 2007). More recently, a number of groups have found success employing Bayesian non-parametric priors (Post and Gildea, 2009; Cohn et al., 2010), which put a downward pressure on fragment size except where the data warrant the inclusion of larger fragments. Unfortunately, proper inference under these models is intractable, and th"
W12-1904,W10-1406,1,0.815854,"y by thresholding at every iteration. (a) Modal construction. (b) Modifiable NP. NP2 S2 NP0 MD VP0 NN VP0 president will (c) Nominal-modification. 4 Datasets (d) PP construction. NP0 We perform a qualitative analysis of fragments learned on datasets for two languages: the Korean Treebank v2.0 (Han and Ryu, 2005) and a comparably-sized portion of the WSJ portion of the Penn Treebank (Marcus et al., 1993). The Korean Treebank (KTB) has predefined splits; to be comparable for our analysis, from the PTB we used §2-3 for training and §22 for validation (we refer to this as wsj2-3). As described in Chung et al. (2010), although Korean presents its own challenges to grammar induction, the KTB yields additional difficulties by including a high occurrence of very flat rules (in 5K sentences, there are 13 NP rules with at least four righthand side NPs) and a coarser nonterminal set than that of the Penn Treebank. On both sets, we run for two iterations. Recall that our algorithm is designed to induce a state-split TSG on a binarized tree; as neither dataset is binarized in native form we apply a left-branching binarization across all trees in both collections as a preprocessing step. Petrov et al. (2006) found"
W12-1904,N09-1062,0,0.0423612,"Missing"
W12-1904,W06-1638,0,0.0185372,"aging the complementary natures of these two approaches. 1 Introduction Context-free grammars (CFGs) are a useful tool for describing the structure of language, modeling a variety of linguistic phenomena while still permitting efficient inference. However, it is widely acknowledged that CFGs employed in practice make unrealistic independence and structural assumptions, resulting in grammars that are overly permissive. One successful approach has been to refine the nonterminals of grammars, first manually (Johnson, 1998; Klein and Manning, 2003) and later automatically (Matsuzaki et al., 2005; Dreyer and Eisner, 2006; Petrov et al., 2006). In addition to improving parsing accuracy, the automatically learned latent annotations of these latter approaches yield results that accord well with human intuitions, especially at the lexical or preterminal level (for example, separating demonstrative adjectives from definite articles under the DT tag). It is more difficult, though, to extend this analysis to higher-level nonterminals, where the long-distance interactions among latent annotations of internal nodes are subtle and difficult to trace. In another line of work, many researchers have examined the use of fo"
W12-1904,W12-2013,1,0.826852,"Missing"
W12-1904,W96-0214,0,0.0867703,"s of arbitrary size. An example TSG fragment along with equivalent CFG rules are depicted in Figure 1. The two formalisms are weakly equivalent, and computing the most probable derivation of a sentence with a TSG can be done in cubic time. Unfortunately, learning TSGs is not straightforward, in large part because TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. One class of existing approaches, known as Data-Oriented Parsing, simply uses all the fragments (Bod, 1993, DOP). This does not scale well to large treebanks, forcing the use of implicit representations (Goodman, 1996) or heuristic subsets (Bod, 2001). It has also been generally observed that the use of all fragments results in poor, overfit grammars, though this can be addressed with held-out data (Zollmann and Sima’an, 2005) or statistical estimators to rule out fragments that are unlikely to generalize (Zuidema, 2007). More recently, a number of groups have found success employing Bayesian non-parametric priors (Post and Gildea, 2009; Cohn et al., 2010), which put a downward pressure on fragment size except where the data warrant the inclusion of larger fragments. Unfortunately, proper inference under th"
W12-1904,J98-4004,0,0.322991,"ility in grammar induction within the structured guidance provided by the treebank, leveraging the complementary natures of these two approaches. 1 Introduction Context-free grammars (CFGs) are a useful tool for describing the structure of language, modeling a variety of linguistic phenomena while still permitting efficient inference. However, it is widely acknowledged that CFGs employed in practice make unrealistic independence and structural assumptions, resulting in grammars that are overly permissive. One successful approach has been to refine the nonterminals of grammars, first manually (Johnson, 1998; Klein and Manning, 2003) and later automatically (Matsuzaki et al., 2005; Dreyer and Eisner, 2006; Petrov et al., 2006). In addition to improving parsing accuracy, the automatically learned latent annotations of these latter approaches yield results that accord well with human intuitions, especially at the lexical or preterminal level (for example, separating demonstrative adjectives from definite articles under the DT tag). It is more difficult, though, to extend this analysis to higher-level nonterminals, where the long-distance interactions among latent annotations of internal nodes are s"
W12-1904,P03-1054,0,0.588832,"r induction within the structured guidance provided by the treebank, leveraging the complementary natures of these two approaches. 1 Introduction Context-free grammars (CFGs) are a useful tool for describing the structure of language, modeling a variety of linguistic phenomena while still permitting efficient inference. However, it is widely acknowledged that CFGs employed in practice make unrealistic independence and structural assumptions, resulting in grammars that are overly permissive. One successful approach has been to refine the nonterminals of grammars, first manually (Johnson, 1998; Klein and Manning, 2003) and later automatically (Matsuzaki et al., 2005; Dreyer and Eisner, 2006; Petrov et al., 2006). In addition to improving parsing accuracy, the automatically learned latent annotations of these latter approaches yield results that accord well with human intuitions, especially at the lexical or preterminal level (for example, separating demonstrative adjectives from definite articles under the DT tag). It is more difficult, though, to extend this analysis to higher-level nonterminals, where the long-distance interactions among latent annotations of internal nodes are subtle and difficult to tra"
W12-1904,J93-2004,0,0.0451158,"inference) while at the same time allowing for the modeling of long distance dependencies. Fragments from such grammars are intuitive, capturing exactly the sorts of phrasal-level properties (such as predicate-argument structure) that are not present in Treebank CFGs and which are difficult to model with latent annotations. This paper is motivated by the complementarity of these approaches. We present our progress in learning latent-variable TSGs in a joint approach that extends the split-merge framework of Petrov et al. (2006). We present our current results on the Penn and Korean treebanks (Marcus et al., 1993; Han et al., 2001), demonstrating that we are able to learn fragments that draw on the strengths of both approaches. Table 1 situates this work among other contributions. In addition to experimenting directly with the Penn and Korean Treebanks, we also conducted two experiments in this framework with the Universal 23 NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 23–30, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics CFG none Charniak ’97 manual Klein & Manning ’03 automatic Matsuzaki et al. ’05 Petrov et al. ’06 Dreyer & Eisner ’06 TSG"
W12-1904,P05-1010,0,0.608017,"d by the treebank, leveraging the complementary natures of these two approaches. 1 Introduction Context-free grammars (CFGs) are a useful tool for describing the structure of language, modeling a variety of linguistic phenomena while still permitting efficient inference. However, it is widely acknowledged that CFGs employed in practice make unrealistic independence and structural assumptions, resulting in grammars that are overly permissive. One successful approach has been to refine the nonterminals of grammars, first manually (Johnson, 1998; Klein and Manning, 2003) and later automatically (Matsuzaki et al., 2005; Dreyer and Eisner, 2006; Petrov et al., 2006). In addition to improving parsing accuracy, the automatically learned latent annotations of these latter approaches yield results that accord well with human intuitions, especially at the lexical or preterminal level (for example, separating demonstrative adjectives from definite articles under the DT tag). It is more difficult, though, to extend this analysis to higher-level nonterminals, where the long-distance interactions among latent annotations of internal nodes are subtle and difficult to trace. In another line of work, many researchers ha"
W12-1904,P06-1055,0,0.825777,"atures of these two approaches. 1 Introduction Context-free grammars (CFGs) are a useful tool for describing the structure of language, modeling a variety of linguistic phenomena while still permitting efficient inference. However, it is widely acknowledged that CFGs employed in practice make unrealistic independence and structural assumptions, resulting in grammars that are overly permissive. One successful approach has been to refine the nonterminals of grammars, first manually (Johnson, 1998; Klein and Manning, 2003) and later automatically (Matsuzaki et al., 2005; Dreyer and Eisner, 2006; Petrov et al., 2006). In addition to improving parsing accuracy, the automatically learned latent annotations of these latter approaches yield results that accord well with human intuitions, especially at the lexical or preterminal level (for example, separating demonstrative adjectives from definite articles under the DT tag). It is more difficult, though, to extend this analysis to higher-level nonterminals, where the long-distance interactions among latent annotations of internal nodes are subtle and difficult to trace. In another line of work, many researchers have examined the use of formalisms with an exten"
W12-1904,P09-2012,1,0.927289,"pproaches, known as Data-Oriented Parsing, simply uses all the fragments (Bod, 1993, DOP). This does not scale well to large treebanks, forcing the use of implicit representations (Goodman, 1996) or heuristic subsets (Bod, 2001). It has also been generally observed that the use of all fragments results in poor, overfit grammars, though this can be addressed with held-out data (Zollmann and Sima’an, 2005) or statistical estimators to rule out fragments that are unlikely to generalize (Zuidema, 2007). More recently, a number of groups have found success employing Bayesian non-parametric priors (Post and Gildea, 2009; Cohn et al., 2010), which put a downward pressure on fragment size except where the data warrant the inclusion of larger fragments. Unfortunately, proper inference under these models is intractable, and though Monte Carlo techniques can provide an approximation, the samplers can be complex, difficult to code, and slow to converge. This history suggests two approaches to state-split TSGs: (1) a Bayesian non-parametric sampling approach (incorporate state-splitting into existing TSG work), or (2) EM (incorporate TSG induction into existing state-splitting work). We choose the latter path, and"
W12-1904,D07-1058,0,0.0186452,"ecause TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. One class of existing approaches, known as Data-Oriented Parsing, simply uses all the fragments (Bod, 1993, DOP). This does not scale well to large treebanks, forcing the use of implicit representations (Goodman, 1996) or heuristic subsets (Bod, 2001). It has also been generally observed that the use of all fragments results in poor, overfit grammars, though this can be addressed with held-out data (Zollmann and Sima’an, 2005) or statistical estimators to rule out fragments that are unlikely to generalize (Zuidema, 2007). More recently, a number of groups have found success employing Bayesian non-parametric priors (Post and Gildea, 2009; Cohn et al., 2010), which put a downward pressure on fragment size except where the data warrant the inclusion of larger fragments. Unfortunately, proper inference under these models is intractable, and though Monte Carlo techniques can provide an approximation, the samplers can be complex, difficult to code, and slow to converge. This history suggests two approaches to state-split TSGs: (1) a Bayesian non-parametric sampling approach (incorporate state-splitting into existin"
W12-1904,petrov-etal-2012-universal,0,\N,Missing
W12-2013,E93-1006,0,0.813376,"titution Grammars (TSGs) Though CFGs and TSGs are weakly equivalent, TSGs permit nonterminals to rewrite as tree fragments of arbitrary size, whereas CFG rewrites are limited to depth-one productions. Figure 1 depicts an example TSG fragment and equivalent CFG rules; note that the entire internal structure of 1a is described within a single rewrite. Unfortunately, learning probabilistic TSGs is not straight-forward, in large part because TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. Approaches to this problem began by taking all fragments Fall in a treebank (Bod, 1993; Goodman, 1996), which resulted in very large grammars composed mostly of fragments very unlikely to generalize.1 A range of heuristic solutions reduced these grammar sizes to a much smaller, more compact subset of all fragments (Zollmann and Sima’an, 2005; Zuidema, 2007). More recently, more principled models have been proposed, taking the form of inference in Bayesian non-parametric models (Post and Gildea, 2009; Cohn et al., 2009). In addition to providing a formal model for TSGs, these techniques address the overfitting problem of 1 The n-gram analog would be something like storing all 30"
W12-2013,2008.amta-papers.4,0,0.0977812,"f languages, since they do not model language structure or correspondences beyond the narrow Markov context. Context-free grammars (CFGs) address many of the problems inherent in n-grams, and are therefore intuitively much better suited for grammaticality judgments. Unfortunately, CFGs used in practice are permissive (Och et al., 2004) and make unrealistic independence and structural assumptions, resulting in “leaky” grammars that overgenerate and thus serve poorly as models of language. However, approaches that make use of the CFG productions as discriminative features have performed better. Cherry and Quirk (2008) improved upon an ngram baseline in grammatical classification by adjusting CFG production weights with a latent SVM, while others have found it useful to use comparisons between scores of different parsers (Wagner et al., 2009) or the use of CFG productions in linear classification settings (Wong and Dras, 2010) in classifying sentences in different grammaticality settings. Another successful approach in grammaticality tasks has been the use of grammars with an extended domain of locality. Post (2011) demonstrated that larger syntactic patterns obtained from Tree Substitution Grammars (Joshi,"
W12-2013,P10-2042,0,0.0187858,"dea, 2009; Cohn et al., 2009). In addition to providing a formal model for TSGs, these techniques address the overfitting problem of 1 The n-gram analog would be something like storing all 30grams seen in a corpus. 117 all fragments grammars with priors that discourage large fragments unless there is enough evidence to warrant their inclusion in the grammar. The problem with such approaches, however, is that the sampling procedures used to infer them can be complex, difficult to code, and slow to converge. Although more general techniques have been proposed to better explore the search space (Cohn and Blunsom, 2010; Cohn et al., 2010; Liang et al., 2010), the complexity and non-determinism of these samplers remain, and there are no publicly available implementations. The underlying premise behind these grammar learning approaches was the need for a probabilistic grammar for parsing. Post (2011) showed that the fragments extracted from derivations obtained by parsing with probabilistic TSGs were useful as features in two coarse-grained grammaticality tasks. In such a setting, fragments are needed for classification, but it is not clear that they need to be obtained from derivations produced by parsing wi"
W12-2013,N09-1062,0,0.0776293,"part because TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. Approaches to this problem began by taking all fragments Fall in a treebank (Bod, 1993; Goodman, 1996), which resulted in very large grammars composed mostly of fragments very unlikely to generalize.1 A range of heuristic solutions reduced these grammar sizes to a much smaller, more compact subset of all fragments (Zollmann and Sima’an, 2005; Zuidema, 2007). More recently, more principled models have been proposed, taking the form of inference in Bayesian non-parametric models (Post and Gildea, 2009; Cohn et al., 2009). In addition to providing a formal model for TSGs, these techniques address the overfitting problem of 1 The n-gram analog would be something like storing all 30grams seen in a corpus. 117 all fragments grammars with priors that discourage large fragments unless there is enough evidence to warrant their inclusion in the grammar. The problem with such approaches, however, is that the sampling procedures used to infer them can be complex, difficult to code, and slow to converge. Although more general techniques have been proposed to better explore the search space (Cohn and Blunsom, 2010; Cohn"
W12-2013,W09-2112,0,0.0950589,"he growth of intermediate rankings Fhr,Ki . Secondly, we have two tunable parameters R and K, which can be thought of as weakly being related to the base measure and concentration parameter of (Post and Gildea, 2009; Cohn et al., 2010). Note that by thresholding at every iteration, we enforce sparsity. 4 Experiments We view grammaticality judgment as a binary classification task: is a sequence of words grammatical or not? We evaluate on two tasks of differing granularity: the first, a coarse-grain classification, follows Cherry and Quirk (2008); the other, a fine-grain analogue, is built upon Foster and Andersen (2009). 4.1 Datasets For the coarse-grained task, we use the BLLIP5 inspired dataset, as in Post (2011), which discriminates between BLLIP sentences and KneyserNey trigram generated sentences (of equal length). Grammatical and ungrammatical examples are given in 1 and 2 below, respectively: (1) The most troublesome report may be the August merchandise trade deficit due out tomorrow . (2) To and , would come Hughey Co. may be crash victims , three billion . 5 (3) The league ’s promoters hope retirees and tourists will join die-hard fans like Mr. de Castro and pack then stands to see the seniors . Bot"
W12-2013,W96-0214,0,0.717446,"ammars (TSGs) Though CFGs and TSGs are weakly equivalent, TSGs permit nonterminals to rewrite as tree fragments of arbitrary size, whereas CFG rewrites are limited to depth-one productions. Figure 1 depicts an example TSG fragment and equivalent CFG rules; note that the entire internal structure of 1a is described within a single rewrite. Unfortunately, learning probabilistic TSGs is not straight-forward, in large part because TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. Approaches to this problem began by taking all fragments Fall in a treebank (Bod, 1993; Goodman, 1996), which resulted in very large grammars composed mostly of fragments very unlikely to generalize.1 A range of heuristic solutions reduced these grammar sizes to a much smaller, more compact subset of all fragments (Zollmann and Sima’an, 2005; Zuidema, 2007). More recently, more principled models have been proposed, taking the form of inference in Bayesian non-parametric models (Post and Gildea, 2009; Cohn et al., 2009). In addition to providing a formal model for TSGs, these techniques address the overfitting problem of 1 The n-gram analog would be something like storing all 30grams seen in a"
W12-2013,P06-1055,0,0.0770331,"r model. We optimized the models on dev data, letting the smoothing parameter be 10m , for integral m ∈ [−4, 2]: 0.1 was optimal for all models. 6 For the fine-grained task we use a version of the BNC that has been automatically modified to be 4 ungrammatical, via insertions, deletions or substitutions of grammatically important words. As has been argued in previous work, these automatically generated errors, simulate more realistic errors (Foster and Andersen, 2009). Example 3 gives an original sentence, with an italicized substitution error: We parsed all sentences with the Berkeley parser (Petrov et al., 2006). 7 We used the Berkeley grammar/parser (Petrov et al., 2006) in accurate mode; all other options were their default values. 8 csie.ntu.edu.tw/˜cjlin/liblinear/ Task coarse fine COUNT COUNT + LEX COUNT + CFG 86.3 62.9 86.8 64.3 88.3 67.0 Method COUNT + CFG , R = 3 COUNT + CFG , R = 15 bigram CFG TSG (a) Our count-based models, with R = 15, K = 50k. Task coarse fine 3 89.2 67.9 (b) Performance of 50k and varying R. 5 89.1 67.2 10 88.6 67.2 COUNT + CFG , 15 88.3 67.0 coarse 89.1 88.2 68.4 86.3 89.1 fine 67.2 66.6 61.4 64.5 67.0 Table 2: Classification accuracy on test portions for both coarse an"
W12-2013,P09-2012,1,0.954487,"ight-forward, in large part because TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. Approaches to this problem began by taking all fragments Fall in a treebank (Bod, 1993; Goodman, 1996), which resulted in very large grammars composed mostly of fragments very unlikely to generalize.1 A range of heuristic solutions reduced these grammar sizes to a much smaller, more compact subset of all fragments (Zollmann and Sima’an, 2005; Zuidema, 2007). More recently, more principled models have been proposed, taking the form of inference in Bayesian non-parametric models (Post and Gildea, 2009; Cohn et al., 2009). In addition to providing a formal model for TSGs, these techniques address the overfitting problem of 1 The n-gram analog would be something like storing all 30grams seen in a corpus. 117 all fragments grammars with priors that discourage large fragments unless there is enough evidence to warrant their inclusion in the grammar. The problem with such approaches, however, is that the sampling procedures used to infer them can be complex, difficult to code, and slow to converge. Although more general techniques have been proposed to better explore the search space (Cohn and"
W12-2013,P11-2038,1,0.30613,"rraro, Matt Post and Benjamin Van Durme Department of Computer Science, and HLTCOE Johns Hopkins University {ferraro,post,vandurme}@cs.jhu.edu Abstract Prior work has shown the utility of syntactic tree fragments as features in judging the grammaticality of text. To date such fragments have been extracted from derivations of Bayesianinduced Tree Substitution Grammars (TSGs). Evaluating on discriminative coarse and fine grammaticality classification tasks, we show that a simple, deterministic, count-based approach to fragment identification performs on par with the more complicated grammars of Post (2011). This represents a significant reduction in complexity for those interested in the use of such fragments in the development of systems for the educational domain. 1 Introduction Automatically judging grammaticality is an important component in computer-assisted education, with potential applications including large-scale essay grading and helping to interactively improve the writing of both native and L2 speakers. While ngram models have been productive throughout natural language processing (NLP), they are obviously insufficient as models of languages, since they do not model language struct"
W12-2013,U10-1011,0,0.0194587,"e (Och et al., 2004) and make unrealistic independence and structural assumptions, resulting in “leaky” grammars that overgenerate and thus serve poorly as models of language. However, approaches that make use of the CFG productions as discriminative features have performed better. Cherry and Quirk (2008) improved upon an ngram baseline in grammatical classification by adjusting CFG production weights with a latent SVM, while others have found it useful to use comparisons between scores of different parsers (Wagner et al., 2009) or the use of CFG productions in linear classification settings (Wong and Dras, 2010) in classifying sentences in different grammaticality settings. Another successful approach in grammaticality tasks has been the use of grammars with an extended domain of locality. Post (2011) demonstrated that larger syntactic patterns obtained from Tree Substitution Grammars (Joshi, 1985) outperformed the Cherry and Quirk models. The intuitions underlying their approach were that larger fragments are more natural atomic units in modeling grammatical text, and that larger fragments reduce the independence assumptions of context-free generative models since there are fewer substitution points"
W12-2013,D07-1058,0,0.386078,"note that the entire internal structure of 1a is described within a single rewrite. Unfortunately, learning probabilistic TSGs is not straight-forward, in large part because TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. Approaches to this problem began by taking all fragments Fall in a treebank (Bod, 1993; Goodman, 1996), which resulted in very large grammars composed mostly of fragments very unlikely to generalize.1 A range of heuristic solutions reduced these grammar sizes to a much smaller, more compact subset of all fragments (Zollmann and Sima’an, 2005; Zuidema, 2007). More recently, more principled models have been proposed, taking the form of inference in Bayesian non-parametric models (Post and Gildea, 2009; Cohn et al., 2009). In addition to providing a formal model for TSGs, these techniques address the overfitting problem of 1 The n-gram analog would be something like storing all 30grams seen in a corpus. 117 all fragments grammars with priors that discourage large fragments unless there is enough evidence to warrant their inclusion in the grammar. The problem with such approaches, however, is that the sampling procedures used to infer them can be co"
W12-2013,N10-1082,0,\N,Missing
W12-2013,N04-1021,0,\N,Missing
W12-2013,P07-1010,0,\N,Missing
W12-3018,P08-1090,0,0.0264646,"anchors, over syntactic dependency structures automatically derived from newswire text. While these efforts are popularly known and constitute established methodological baselines within knowledge acquisition and computational semantics, the underlying annotated corpora are not public resources. As such, direct comparison to their methods are difficult or impossible. 96 Further examples of popularly known results that are difficult to reproduce include the large-scale information extraction results surrounding TextRunner (Yates et al., 2007), or the script induction efforts first described by Chambers and Jurafsky (2008). In the latter, coreference chains were required in addition to syntactic parsing: a further computationally expensive requirement. Often researchers will provide full resultant derived resources, such as the DIRT rules or narrative chains (Chambers and Jurafsky, 2010). While this is to be encouraged (as opposed to merely allowing limited web-based access), there are likely a number of researchers that would prefer to tune, adapt, and modify large-scale extraction algorithms, if only they had ready access to the preprocessed collections that led to such resources. This is especially the case"
W12-3018,chambers-jurafsky-2010-database,0,0.0160834,"are not public resources. As such, direct comparison to their methods are difficult or impossible. 96 Further examples of popularly known results that are difficult to reproduce include the large-scale information extraction results surrounding TextRunner (Yates et al., 2007), or the script induction efforts first described by Chambers and Jurafsky (2008). In the latter, coreference chains were required in addition to syntactic parsing: a further computationally expensive requirement. Often researchers will provide full resultant derived resources, such as the DIRT rules or narrative chains (Chambers and Jurafsky, 2010). While this is to be encouraged (as opposed to merely allowing limited web-based access), there are likely a number of researchers that would prefer to tune, adapt, and modify large-scale extraction algorithms, if only they had ready access to the preprocessed collections that led to such resources. This is especially the case now, as interest in Vector Space Models (VSMs) for semantics gain increased attention within Cognitive (Mitchell and Lapata, 2010) and Computer (Turney and Pantel, 2010) Science: such models are often reliant on co-occurrence counts derived over large numbers of syntact"
W12-3018,J90-1003,0,0.116923,"provides motivation for such a resource, the tools employed, a description of the programmatic interface provided alongside the data, and examples of ongoing work already enabled by this resource. 2 Motivation Our community has long had a strong dependence on syntactically annotated corpora, going back at least as far as the Brown corpus (Francis and Ku˘cera, 1964 1971 1979). As manual annotation of syntactic structure is expensive at any large scale, researchers have regularly shifted their reliance to automatically parsed corpora when concerned with statistics of cooccurrence. For example, Church and Hanks (1990) pioneered the use of Pointwise Mutual Information (PMI) in the field, with results provided over syntactic derivations on a 44-million-word corpus of newswire, showing correlations such as the verb drink/V associating with direct objects martinis, cup water, champagne, beverage, cup coffee, and so on. This was followed by a large number of related efforts, such as that by Lin and Pantel (2001): Discovery of Inference Rules from Text (DIRT), aimed at building a collection of paths sharing distributionally similar nominal anchors, over syntactic dependency structures automatically derived from"
W12-3018,S12-1034,1,0.741345,"Missing"
W12-3018,N09-2061,0,0.0338531,"r hour. 3.1 Preprocessing Gigaword has an SGML-style markup which does not differentiate between different types of body text. For example, list items are not distinguished from complete sentences. Therefore, we coarsely identified all non-sentential lines (list items) by lines with more than one character preceding the first nonspace character, after inspection of several randomly sampled documents. The remaining lines from the <HEADLINE&gt; and <TEXT&gt; fields were segmented into sentences using the open-source tool Splitta, which reported the lowest error rate for English sentence segmentation (Gillick, 2009). Sentences were tokenized using a Penn-Treebank tokenizer (from the Stanford CoreNLP toolkit2 ). We skipped all sentences with more than 100 tokens because we observed that these sentences were often the result of sentence segmentation failure or concatenated list items. In total, we parsed 183,119,464 sentences from the collection. Our release includes information about which sentences were omitted. In an initial estimate of one file containing 548,409 sentences, we dropped 1,197 sentences due to length constraints, which is less than one percent of the total sentences. 3.2 Parsing We have P"
W12-3018,D10-1002,0,0.0396262,"Missing"
W12-3018,W11-1902,0,0.0342753,"Missing"
W12-3018,D09-1001,0,0.0398462,"NN :nsubj: VBD ←rose→ VBD :dobj: NNS NN :nsubj: VBD ←dropped→ VBD :dobj: NNS NN :nsubj: VBD ←edged→ VBD :dobj: NNS Gloss X gained Y X climbed Y X won Y X rose Y X dropped Y X edged Y Cos 1.0000 0.9883 0.9808 0.9783 0.9743 0.9700 Table 2: Relations most similar to “X gained Y” as found in Annotated Gigaword using approximate search. 5 Example Applications 5.2 The following gives two examples of work this resource and interface have already enabled.6 5.1 Shallow Semantic Parsing Ongoing work uses this resource to automatically extract relations, in the spirit of Lin and Pantel (2001) (DIRT) and Poon and Domingos (2009) (USP). First, DIRT-like dependency paths between nominal anchors are extracted and then, using these observed nominal arguments to construct feature vectors, similar paths are discovered based on an approximate nearest-neighbor scheme as employed by Ravichandran et al. (2005). For example, the most similar phrases to “X dived/gained Y” found using this method are shown in Tables 1 and 2 (e.g. the Nasdaq dived 3.5 percent). Deriving examples such as these required relatively minor amounts of effort, but only once a large annotated resource and supporting tools became available. 6 Both applicat"
W12-3018,P05-1077,0,0.0204578,"” as found in Annotated Gigaword using approximate search. 5 Example Applications 5.2 The following gives two examples of work this resource and interface have already enabled.6 5.1 Shallow Semantic Parsing Ongoing work uses this resource to automatically extract relations, in the spirit of Lin and Pantel (2001) (DIRT) and Poon and Domingos (2009) (USP). First, DIRT-like dependency paths between nominal anchors are extracted and then, using these observed nominal arguments to construct feature vectors, similar paths are discovered based on an approximate nearest-neighbor scheme as employed by Ravichandran et al. (2005). For example, the most similar phrases to “X dived/gained Y” found using this method are shown in Tables 1 and 2 (e.g. the Nasdaq dived 3.5 percent). Deriving examples such as these required relatively minor amounts of effort, but only once a large annotated resource and supporting tools became available. 6 Both applications additionally rely on the Jerboa toolkit (Van Durme, 2012), in order to handle the large scale of features and instances extractable from Annotated Gigaword. 98 Enabling Meaning-preserving Rewriting In a related project, Annotated Gigaword enabled Ganitkevitch et al. (2012"
W12-3018,N07-4013,0,0.00736133,"building a collection of paths sharing distributionally similar nominal anchors, over syntactic dependency structures automatically derived from newswire text. While these efforts are popularly known and constitute established methodological baselines within knowledge acquisition and computational semantics, the underlying annotated corpora are not public resources. As such, direct comparison to their methods are difficult or impossible. 96 Further examples of popularly known results that are difficult to reproduce include the large-scale information extraction results surrounding TextRunner (Yates et al., 2007), or the script induction efforts first described by Chambers and Jurafsky (2008). In the latter, coreference chains were required in addition to syntactic parsing: a further computationally expensive requirement. Often researchers will provide full resultant derived resources, such as the DIRT rules or narrative chains (Chambers and Jurafsky, 2010). While this is to be encouraged (as opposed to merely allowing limited web-based access), there are likely a number of researchers that would prefer to tune, adapt, and modify large-scale extraction algorithms, if only they had ready access to the"
W12-3807,P11-2049,0,0.0155415,"tion and automatic tagging of the belief modality (i.e., factivity) is described in more detail in (Diab et al., 2009b; Prabhakaran et al., 2010). There has been a considerable amount of interest in modality in the biomedical domain. Negation, uncertainty, and hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical const"
W12-3807,W09-1324,0,0.0193045,"nd hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they often render the prese"
W12-3807,baker-etal-2010-modality,1,0.685998,"Missing"
W12-3807,J12-2006,1,0.846788,"features used to train our modality tagger and presents experiments and results. Section 5 concludes and discusses future work. 58 2 Related Work Previous related work includes TimeML (Sauri et al., 2006), which involves modality annotation on events, and Factbank (Sauri and Pustejovsky, 2009), where event mentions are marked with degree of factuality. Modality is also important in the detection of uncertainty and hedging. The CoNLL shared task in 2010 (Farkas et al., 2010) deals with automatic detection of uncertainty and hedging in Wikipedia and biomedical sentences. Baker et al. (2010) and Baker et al. (2012) analyze a set of eight modalities which include belief, require and permit, in addition to the five modalities we focus on in this paper. They built a rule-based modality tagger using a semi-automatic approach to create rules. This earlier work differs from the work described in this paper in that the our emphasis is on the creation of an automatic modality tagger using machine learning techniques. Note that the annotation and automatic tagging of the belief modality (i.e., factivity) is described in more detail in (Diab et al., 2009b; Prabhakaran et al., 2010). There has been a considerable"
W12-3807,W09-3012,1,0.821042,"y of the information. Did the speaker 57 Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012), c pages 57–64, Jeju, Republic of Korea, 13 July 2012. 2012 Association for Computational Linguistics have firsthand knowledge of what he or she is reporting, or was it hearsay or inferred from indirect evidence? Sentiment deals with a speaker’s positive or negative feelings toward an event, state, or proposition. In this paper, we focus on the following five modalities; we have investigated the belief/factivity modality previously (Diab et al., 2009b; Prabhakaran et al., 2010), and we leave other modalities to future work. • Ability: can H do P? • Effort: does H try to do P? • Intention: does H intend P? • Success: does H succeed in P? • Want: does H want P? We investigate automatically training a modality tagger by using multi-class Support Vector Machines (SVMs). One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a modality tagger because modality triggers are sparse for the overwhelming majority of the sentences. Baker et al. (2010) created a modality tagg"
W12-3807,P03-1004,0,0.0266465,"removed the ones which did not in fact have a modality. In the remaining sentences (94 sentences), our expert annotated the target predicate. We refer to this as the Gold dataset in this paper. The MTurk and Gold datasets differ in terms of genres as well as annotators (Turker vs. Expert). The distribution of modalities in both MTurk and Gold annotations are given in Table 2. 4.2 Gold Ability Table 1: For each modality, the number of sentences returned by the simple tagger that we posted on MTurk. 4 MTurk Table 2: Frequency of Modalities modalities in context. For tagging, we used the Yamcha (Kudo and Matsumoto, 2003) sequence labeling system which uses the SVMlight (Joachims, 1999) package for classification. We used One versus All method for multi-class classification on a quadratic kernel with a C value of 1. We report recall and precision on word tokens in our corpus for each modality. We also report Fβ=1 (F)-measure as the harmonic mean between (P)recision and (R)ecall. 4.3 Features We used lexical features at the token level which can be extracted without any parsing with relatively high accuracy. We use the term context width to denote the window of tokens whose features are considered for predictin"
W12-3807,W09-1304,0,0.0189282,"ain. Negation, uncertainty, and hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they"
W12-3807,Y05-1014,0,0.0194818,", absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to specifically handle modal constructions, while our modal annotation approach is a part of a full translation system. The textual entailment literature includes moda"
W12-3807,W06-3907,0,0.0243548,"of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to specifically handle modal constructions, while our modal annotation approach is a part of a full translation system. The textual entailment literature includes modality annotation schemes. Identifying modalities is important to determine whether a text entails a hypothesis. Bar-Haim et al. (2007) include polarity based rules and negation and modality annotation rules. The polarity rules are based on an independent polarity lexicon (Nairn et al., 2006). The annotation rules for negation and modality of predicates are based on identifying modal verbs, as well as conditional sentences and modal adverbials. The authors read the modality off parse trees directly using simple structural rules for modifiers. 3 Constructing Modality Training Data In this section, we will discuss the procedure we followed to construct the training data for building the automatic modality tagger. In a pilot study, we obtained and ran the modality tagger described in (Baker et al., 2010) on the English side of the Urdu-English LDC language pack.2 We randomly selected"
W12-3807,C10-2117,1,0.929409,". Did the speaker 57 Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012), c pages 57–64, Jeju, Republic of Korea, 13 July 2012. 2012 Association for Computational Linguistics have firsthand knowledge of what he or she is reporting, or was it hearsay or inferred from indirect evidence? Sentiment deals with a speaker’s positive or negative feelings toward an event, state, or proposition. In this paper, we focus on the following five modalities; we have investigated the belief/factivity modality previously (Diab et al., 2009b; Prabhakaran et al., 2010), and we leave other modalities to future work. • Ability: can H do P? • Effort: does H try to do P? • Intention: does H intend P? • Success: does H succeed in P? • Want: does H want P? We investigate automatically training a modality tagger by using multi-class Support Vector Machines (SVMs). One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a modality tagger because modality triggers are sparse for the overwhelming majority of the sentences. Baker et al. (2010) created a modality tagger by using a semiautomatic"
W12-3807,C94-1018,0,0.029939,"Missing"
W12-3807,W08-0606,0,\N,Missing
W14-2416,P14-1090,1,0.581306,"Missing"
W14-2416,P14-1133,1,0.0366645,"ase, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). 1 So-called Open Information Extraction (OIE) is simply a further blurring of the distinction between IE and SP, where the schema is allowed to grow with the number of verbs, and other predicative elements of the language. 82 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82–86, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics provide richer, open-domain challenges. While the vocabulary increased, our analysis suggests that compositionality and complexity decreased. We therefore conclude that the semantic parsing community should target"
W14-2416,D13-1160,1,0.915633,"schema.1 Given the ease with which reasonably accurate, deep syntactic structure can be automatically derived over (English) text, it is not surprising that IE researchers would start including such “features” in their models. Our question is then: what is the difference between an IE system with access to syntax, as compared to a semantic parser, when both are targeting a factoid-extraction style task? While our conclusions should hold generally for similar KBs, we will focus on Freebase, such as explored by Krishnamurthy and Mitchell (2012), and then others such as Cai and Yates (2013a) and Berant et al. (2013). We compare two open-source, state-ofthe-art systems on the task of Freebase QA: the semantic parsing system SEMPRE (Berant et al., 2013), and the IE system jacana-freebase (Yao and Van Durme, 2014). We find that these two systems are on par with each other, with no significant differences in terms of accuracy between them. A major distinction between the work of Berant et al. (2013) and Yao and Van Durme (2014) is the ability of the former to represent, and compose, aggregation operators (such as argmax, or count), as well as integrate disparate pieces of information. This representational c"
W14-2416,D07-1071,0,0.0117893,"(QA) from structured data, such as DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and Yago2 (Hoffart et al., 2011), has drawn significant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). 1 So-called Open Information Extraction (OIE) is simply a further blurring of the distinction between IE and SP, where the schema is allowed to grow with the number of verbs, and other predicative elements of the language. 82 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82–86, c Baltimore, Maryland US"
W14-2416,P09-1110,0,0.0175891,"h as DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and Yago2 (Hoffart et al., 2011), has drawn significant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). 1 So-called Open Information Extraction (OIE) is simply a further blurring of the distinction between IE and SP, where the schema is allowed to grow with the number of verbs, and other predicative elements of the language. 82 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82–86, c Baltimore, Maryland USA, June 26 2014. 2014 Associati"
W14-2416,P13-1042,0,0.479728,"f a form matching a given schema.1 Given the ease with which reasonably accurate, deep syntactic structure can be automatically derived over (English) text, it is not surprising that IE researchers would start including such “features” in their models. Our question is then: what is the difference between an IE system with access to syntax, as compared to a semantic parser, when both are targeting a factoid-extraction style task? While our conclusions should hold generally for similar KBs, we will focus on Freebase, such as explored by Krishnamurthy and Mitchell (2012), and then others such as Cai and Yates (2013a) and Berant et al. (2013). We compare two open-source, state-ofthe-art systems on the task of Freebase QA: the semantic parsing system SEMPRE (Berant et al., 2013), and the IE system jacana-freebase (Yao and Van Durme, 2014). We find that these two systems are on par with each other, with no significant differences in terms of accuracy between them. A major distinction between the work of Berant et al. (2013) and Yao and Van Durme (2014) is the ability of the former to represent, and compose, aggregation operators (such as argmax, or count), as well as integrate disparate pieces of informati"
W14-2416,D12-1069,0,0.0385127,"ugh meaning in order to populate a database with factoids of a form matching a given schema.1 Given the ease with which reasonably accurate, deep syntactic structure can be automatically derived over (English) text, it is not surprising that IE researchers would start including such “features” in their models. Our question is then: what is the difference between an IE system with access to syntax, as compared to a semantic parser, when both are targeting a factoid-extraction style task? While our conclusions should hold generally for similar KBs, we will focus on Freebase, such as explored by Krishnamurthy and Mitchell (2012), and then others such as Cai and Yates (2013a) and Berant et al. (2013). We compare two open-source, state-ofthe-art systems on the task of Freebase QA: the semantic parsing system SEMPRE (Berant et al., 2013), and the IE system jacana-freebase (Yao and Van Durme, 2014). We find that these two systems are on par with each other, with no significant differences in terms of accuracy between them. A major distinction between the work of Berant et al. (2013) and Yao and Van Durme (2014) is the ability of the former to represent, and compose, aggregation operators (such as argmax, or count), as we"
W14-2416,D10-1119,0,0.0289785,"), Freebase (Bollacker et al., 2008) and Yago2 (Hoffart et al., 2011), has drawn significant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). 1 So-called Open Information Extraction (OIE) is simply a further blurring of the distinction between IE and SP, where the schema is allowed to grow with the number of verbs, and other predicative elements of the language. 82 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82–86, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Lingu"
W14-2416,D11-1140,0,0.00845976,"al., 2008) and Yago2 (Hoffart et al., 2011), has drawn significant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). 1 So-called Open Information Extraction (OIE) is simply a further blurring of the distinction between IE and SP, where the schema is allowed to grow with the number of verbs, and other predicative elements of the language. 82 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82–86, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics provide richer, ope"
W14-2416,D13-1161,0,0.0426815,"ficant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). 1 So-called Open Information Extraction (OIE) is simply a further blurring of the distinction between IE and SP, where the schema is allowed to grow with the number of verbs, and other predicative elements of the language. 82 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82–86, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics provide richer, open-domain challenges. While the vocabulary increased, our ana"
W14-2416,P11-1060,0,0.0293974,"ity of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). 1 So-called Open Information Extraction (OIE) is simply a further blurring of the distinction between IE and SP, where the schema is allowed to grow with the number of verbs, and other predicative elements of the language. 82 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82–86, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics provide richer, open-domain challenges. While the vocabulary increased, our analysis suggests that compositionality and complexity decreased. We therefore conclude tha"
W14-2515,P14-2030,1,0.859761,"Missing"
W14-2515,P11-1016,0,0.0417851,"e, can be used to inform latent author attribute prediction based on firstperson content, such as that appearing in social media services like Twitter. Future work should consider the question of priors. Our study here relied on balanced class experiments, but the more fine-grained the social role, the smaller the subset of the population we might expect will possess that role. Estimating these priors is thus an important point for future work, especially if we wish to couple such demographic predictions within a larger automatic system, such as the aggregate prediction of targeted sentiment (Jiang et al., 2011). Acknowledgements This material is partially based Table 2: Numbers of positively and negatively identified users by indicative verb. Using the same collection as the previous experiment, we trained classifiers conditioned on a given verb term. Positive instances were taken to be those with a score of 4.0 or higher, with negative instances taken to be those with scores of 1.0 or lower (strong agreement by judges that the original tweet did not provide evidence of the given role). Classification results are shown in figure 3. Note that for a number of verb terms these thresholds left very spar"
W14-2515,P13-1070,1,0.89143,"Missing"
W14-2515,J98-2002,0,0.0291078,"that select for a given social role as subject (e.g. I teach ... for teacher) are used to quickly build up binary classifiers for that role. 1 Introduction It has long been recognized that linguistic predicates preferentially select arguments that meet certain semantic criteria (Katz and Fodor, 1963; Chomsky, 1965). The verb eat for example selects for an animate subject and a comestible object. While the information encoded by selectional preferences can and has been used to support natural language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al.,"
W14-2515,D08-1007,0,0.0172838,"sambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al., 1999) like their own. 2 Inducing Selectional Preferences Consider the task of predicting social roles in more detail: For a given role, e.g. artist, we want a way to distinguish role-bearing from non-role-bearing users. We can view each social role as being a fine-grained version of a semantic class of the sort required by class-based approaches to selectional preferences (e.g. the work by Resnik (1996) and those reviewed by Light and Greiff (2002)). The goal then is to identify a set of verbs that preferen50 Proceedings of the ACL 2014 Worksh"
W14-2515,D11-1120,0,0.0348829,"ing selectional preferences and apply the resulting collections to the task of predicting fine-grained latent author attributes on Twitter. Our method for inducing selectional preferences is most similar to class-based approaches, though unlike approaches such as by Resnik (1996) we do not require a WordNet-like ontology. The vast quantity of informal, first-person text data made available by the rise of social media platforms has encouraged researchers to develop models that predict broad user categories like age, gender, and political preference (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b; Zamal et al., 2012). Such information is useful for large scale demographic research that can fuel computational social science advertising. Similarly to Beller et al. (2014), we are interested in classification that is finer-grained than gender or political affiliation, seeking instead to predict social roles like smoker, student, and artist. We make use of a light-weight, unsupervised method to identify selectional preferences and use the resulting information to rapidly bootstrap classification models. Selectional preferences, the tendencies of predicates to select for c"
W14-2515,lin-etal-2010-new,0,0.0543229,"Missing"
W14-2515,J90-1003,0,0.375007,"Missing"
W14-2515,J02-2003,0,0.0254833,"le the information encoded by selectional preferences can and has been used to support natural language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al., 1999) like their own. 2 Inducing Selectional Preferences Consider the task of predicting social roles in more detail: For a given role, e.g. artist, we want a way to distinguish role-bearing from non-role-bearing users. We can view each social role as being a fine-grained version of a semantic class of the sort required by class-based approaches to selectional preferences (e.g. the work by Resnik (1996)"
W14-2515,N07-1071,0,0.0248327,"oded by selectional preferences can and has been used to support natural language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al., 1999) like their own. 2 Inducing Selectional Preferences Consider the task of predicting social roles in more detail: For a given role, e.g. artist, we want a way to distinguish role-bearing from non-role-bearing users. We can view each social role as being a fine-grained version of a semantic class of the sort required by class-based approaches to selectional preferences (e.g. the work by Resnik (1996) and those reviewed by"
W14-2515,P07-1028,0,0.0178471,"l language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al., 1999) like their own. 2 Inducing Selectional Preferences Consider the task of predicting social roles in more detail: For a given role, e.g. artist, we want a way to distinguish role-bearing from non-role-bearing users. We can view each social role as being a fine-grained version of a semantic class of the sort required by class-based approaches to selectional preferences (e.g. the work by Resnik (1996) and those reviewed by Light and Greiff (2002)). The goal then is to identify a set"
W14-2515,W97-0209,0,0.278765,"tform Twitter. First person uses of verbs that select for a given social role as subject (e.g. I teach ... for teacher) are used to quickly build up binary classifiers for that role. 1 Introduction It has long been recognized that linguistic predicates preferentially select arguments that meet certain semantic criteria (Katz and Fodor, 1963; Chomsky, 1965). The verb eat for example selects for an animate subject and a comestible object. While the information encoded by selectional preferences can and has been used to support natural language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or ge"
W14-2515,P09-1080,0,0.0294315,"e use of a “quick and dirty” method for inducing selectional preferences and apply the resulting collections to the task of predicting fine-grained latent author attributes on Twitter. Our method for inducing selectional preferences is most similar to class-based approaches, though unlike approaches such as by Resnik (1996) we do not require a WordNet-like ontology. The vast quantity of informal, first-person text data made available by the rise of social media platforms has encouraged researchers to develop models that predict broad user categories like age, gender, and political preference (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b; Zamal et al., 2012). Such information is useful for large scale demographic research that can fuel computational social science advertising. Similarly to Beller et al. (2014), we are interested in classification that is finer-grained than gender or political affiliation, seeking instead to predict social roles like smoker, student, and artist. We make use of a light-weight, unsupervised method to identify selectional preferences and use the resulting information to rapidly bootstrap classification models. Selectional preferences, the t"
W14-2515,P10-1044,0,0.0319547,"tic criteria (Katz and Fodor, 1963; Chomsky, 1965). The verb eat for example selects for an animate subject and a comestible object. While the information encoded by selectional preferences can and has been used to support natural language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al., 1999) like their own. 2 Inducing Selectional Preferences Consider the task of predicting social roles in more detail: For a given role, e.g. artist, we want a way to distinguish role-bearing from non-role-bearing users. We can view each social role as being a fine-grain"
W14-2515,J02-3001,0,0.0432865,"t (e.g. I teach ... for teacher) are used to quickly build up binary classifiers for that role. 1 Introduction It has long been recognized that linguistic predicates preferentially select arguments that meet certain semantic criteria (Katz and Fodor, 1963; Chomsky, 1965). The verb eat for example selects for an animate subject and a comestible object. While the information encoded by selectional preferences can and has been used to support natural language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al., 1999) like their own. 2 Inducing Selectional Preference"
W14-2515,P99-1014,0,0.0543061,"and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al., 1999) like their own. 2 Inducing Selectional Preferences Consider the task of predicting social roles in more detail: For a given role, e.g. artist, we want a way to distinguish role-bearing from non-role-bearing users. We can view each social role as being a fine-grained version of a semantic class of the sort required by class-based approaches to selectional preferences (e.g. the work by Resnik (1996) and those reviewed by Light and Greiff (2002)). The goal then is to identify a set of verbs that preferen50 Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Sci"
W14-2515,D12-1005,1,0.89171,"Missing"
W14-2901,P10-2045,0,0.131826,"opic model to learn clusters of words (O’ Connor, 2012; Materna, 2012), or attempting to learn symbolic concepts and attributes from dictionary definitions of words (Orfan and Allen, 2013). relation pairs, even if not considering the previous issue of incomplete frame coverage. For example, the Experience bodily harm and Hostile encounter frames are not related through the Is Causative Of relation, even though it is reasonable to expect that a hostile encounter would result in bodily harm.4 Though researchers have used FrameNet relations for tasks such as recognizing textual entailment (RTE) (Aharon et al., 2010) and for text understanding (Fillmore and Baker, 2001), to the best of our knowledge there has been no work on automatically extending frame-frame relations. Frame Annotated Sentences FrameNet contains annotated sentences providing examples of: lexical units, frames those lexical units evoked, and frame elements present in the sentence (along with additional information). These annotated sentences can be divided into two types based on whether all the frame evoking words were marked as targets or not. The first type, which we call lexicographic, contains sentences with a single target per sent"
W14-2901,S07-1018,0,0.0353727,"A lexical unit is a tuple of three elements: the lemma of a word, its POS tag and the associated frame. FrameNet is large lexico-semantic dataset that contains manually annotated information including frame descriptions, frame-frame relations and frame annotated sentences. It has been used build to frame semantic parsers, which are systems that can analyze a sentence and annotate its words with the frames that they evoke and the corresponding frame elements. The task of frame semantic parsing was introduced by Gildea and Jurafsky (2002) and later it matured into a community-wide shared task (Baker et al., 2007), with CMU’s SEMAFOR system being the current state-of-the-art parser (Das et al., 2013). Common to rich, manually constructed semantic resources, the coverage of FrameNet across its 1 2 FrameNet Coverage FrameNet is a rich semantic resource, yet currently lacks complete coverage of the language. In the following we give examples of this incompleteness, in particular the OOV issue that we will focus on in latter sections. Frames A frame represents an event, a situation or a real life concept; FrameNet version 1.5 contains 1,019 such frames. These thousand frames do not cover all possible situa"
W14-2901,J05-1004,0,0.11281,". These annotated sentences can be divided into two types based on whether all the frame evoking words were marked as targets or not. The first type, which we call lexicographic, contains sentences with a single target per sentence. The second type, called fulltext, contains sentences that have been annotated more completely and they contain multiple targets per sentence. There are 4,026 fulltext sentences containing 23,921 targets. This data has proved to be useful for lexico-semantic tasks like RTE and paraphrasing e.g. (Aharon et al., 2010; Coyne and Rambow, 2009). As compared to PropBank (Palmer et al., 2005), which annotated all predicates occurring within a collection of preexisting documents, FrameNet provides examples, but not a corpus that allows for directly estimating relative frequencies. Frame-Frame Relations FrameNet encodes certain types of correlations between situations and events by adding defeasible typed-relations between frames encoding pairwise dependencies. There are eight types of frame-frame relations: Inherits from, Perspective on, Precedes, Subframe of, See also, Uses, Is Inchoative of, and 3 Is Causative of. For example the frame Being Born is related to Death through the r"
W14-2901,ferrandez-etal-2010-aligning,0,0.234633,"Missing"
W14-2901,P13-2130,0,0.0348196,"n: Is Used by, Is Inherited by, Is Perspectivized in, Has Subframe(s), Is Preceded by, however an antonym relation does not add any extra information over its corresponding relation. 4 Reasonable highlights the issue that we would optimally like to know things that are even just possible/not-toounlikely, even if not strictly entailed. 2 average human judgement score of a random sample from a larger collection. such a lemma); and (2) lemmas from the language may not be in FrameNet in any form. Most research on mitigating this limitation involves mapping FrameNet’s frames to WordNet’s synsets.5 Fossati et al. (2013) explored the feasibility of crowdsourcing FrameNet coverage, using the distributed manual labor of Mechanical Turk to complete the lemma coverage. 3 Approach We used the lexical rules sans features along with a 5-gram Kneser-Ney smoothed language model trained using KenLM (Heafield et al., 2013) on the raw English sequence of Annotated Gigaword (Napoles et al., 2012) to paraphrase the fulltext frame annotated sentences of FrameNet. We used a combination of the WordNet morphological analyzer and Morpha8 for lemmatization and Morphg9 for generation. Augmenting FrameNet with PPDB In order to exp"
W14-2901,N13-1092,1,0.875936,"Missing"
W14-2901,J02-3001,0,0.0980636,"ttributes:1 Agent, Theme, Place, Time, Manner, Duration, Explanation and Depictive. A lexical unit is a tuple of three elements: the lemma of a word, its POS tag and the associated frame. FrameNet is large lexico-semantic dataset that contains manually annotated information including frame descriptions, frame-frame relations and frame annotated sentences. It has been used build to frame semantic parsers, which are systems that can analyze a sentence and annotate its words with the frames that they evoke and the corresponding frame elements. The task of frame semantic parsing was introduced by Gildea and Jurafsky (2002) and later it matured into a community-wide shared task (Baker et al., 2007), with CMU’s SEMAFOR system being the current state-of-the-art parser (Das et al., 2013). Common to rich, manually constructed semantic resources, the coverage of FrameNet across its 1 2 FrameNet Coverage FrameNet is a rich semantic resource, yet currently lacks complete coverage of the language. In the following we give examples of this incompleteness, in particular the OOV issue that we will focus on in latter sections. Frames A frame represents an event, a situation or a real life concept; FrameNet version 1.5 conta"
W14-2901,P13-2121,0,0.0424809,"Missing"
W14-2901,W12-3018,1,0.880546,"Missing"
W14-2901,J14-1002,0,\N,Missing
W14-2907,W04-0803,0,0.0335914,"result Frames tend to be much finer grained than ACE/ERE events, and are more numerous by an order of magnitude. The Berkeley FrameNet Project (Baker et al., 1998) was developed as a machine readable database of distinct frames and lexical units (words and multi-word constructions) that were known to trigger specific frames.1 FrameNet 1.5 includes 1020 identified frames and 11830 lexical units. One of the most widespread uses of FrameNet has been as a resource for Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). FrameNet related SRL was promoted as a task by the SENSEVAL-3 workshop (Litkowski, 2004), and the SemEval-2007 workshop (Baker et al., 2007). (Das et al., 2010) is a current system for automatic FrameNet annotation. The relation and attribute types of TAC-KBP and the relation and event types in the ACE/ERE standards can be mapped to FrameNet frames. The mapping is complicated by two factors. The first is that FrameNet frames are generally more fine-grained than the ACE/ERE categories. As a result the mapping is sometimes one-to-many. For example, the ERE relation AfAdditional tasks Starting in 2012 TAC-KBP introduced the “Cold Start” task, which is to literally produce a KB based"
W14-2907,P98-1013,0,0.0831239,"82). Frames are descriptions of event (or state) types and contain information about event participants (frame elements), information as to how event types relate to each other (frame relations), and information about which words or multi-word expressions can trigger a given frame (lexical units). FrameNet is designed with text annotation in mind, but unlike ACE/ERE it prioritizes lexicographic and linguistic completeness over ease of annotation. As a result Frames tend to be much finer grained than ACE/ERE events, and are more numerous by an order of magnitude. The Berkeley FrameNet Project (Baker et al., 1998) was developed as a machine readable database of distinct frames and lexical units (words and multi-word constructions) that were known to trigger specific frames.1 FrameNet 1.5 includes 1020 identified frames and 11830 lexical units. One of the most widespread uses of FrameNet has been as a resource for Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). FrameNet related SRL was promoted as a task by the SENSEVAL-3 workshop (Litkowski, 2004), and the SemEval-2007 workshop (Baker et al., 2007). (Das et al., 2010) is a current system for automatic FrameNet annotation. The relation and att"
W14-2907,S07-1018,0,0.00934785,"ACE/ERE events, and are more numerous by an order of magnitude. The Berkeley FrameNet Project (Baker et al., 1998) was developed as a machine readable database of distinct frames and lexical units (words and multi-word constructions) that were known to trigger specific frames.1 FrameNet 1.5 includes 1020 identified frames and 11830 lexical units. One of the most widespread uses of FrameNet has been as a resource for Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). FrameNet related SRL was promoted as a task by the SENSEVAL-3 workshop (Litkowski, 2004), and the SemEval-2007 workshop (Baker et al., 2007). (Das et al., 2010) is a current system for automatic FrameNet annotation. The relation and attribute types of TAC-KBP and the relation and event types in the ACE/ERE standards can be mapped to FrameNet frames. The mapping is complicated by two factors. The first is that FrameNet frames are generally more fine-grained than the ACE/ERE categories. As a result the mapping is sometimes one-to-many. For example, the ERE relation AfAdditional tasks Starting in 2012 TAC-KBP introduced the “Cold Start” task, which is to literally produce a KB based on the Slot Filling schema. To date, Cold Start KBs"
W14-2907,mcnamee-etal-2010-evaluation,1,0.552655,"Event). Additionally, both annotation guidelines agree on the following: • ERE is limited to pre-specified arguments for each event and relation subtype. The possible arguments for ACE are: Event participants (limited to pre-specified roles for each event type); Event-specific attributes that are associated with a particular event type (e.g., the victim of an attack); and General event attributes that can apply to most or all event types (e.g., time, place). • Tagging of Resultative Events (states that result from taggable Events) 49 information to entity profiles that is missing from the KB (McNamee et al., 2010). Due to its generous license and large scale, a snapshot of English Wikipedia from late 2008 has been used as the reference KB in the TAC-KBP evaluations. • ACE tags arguments regardless of modal certainty of their involvement in the event. ERE only tags asserted participants in the event. • The full noun phrase is marked in both ERE and ACE arguments, but the head is only specified in ACE. This is because ACE handles entity annotation slightly differently than ERE does; ACE marks the full noun phrase with a head word for entity mention, and ERE treats mentions differently based on their synt"
W14-2907,N10-1138,0,0.0125127,"re more numerous by an order of magnitude. The Berkeley FrameNet Project (Baker et al., 1998) was developed as a machine readable database of distinct frames and lexical units (words and multi-word constructions) that were known to trigger specific frames.1 FrameNet 1.5 includes 1020 identified frames and 11830 lexical units. One of the most widespread uses of FrameNet has been as a resource for Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). FrameNet related SRL was promoted as a task by the SENSEVAL-3 workshop (Litkowski, 2004), and the SemEval-2007 workshop (Baker et al., 2007). (Das et al., 2010) is a current system for automatic FrameNet annotation. The relation and attribute types of TAC-KBP and the relation and event types in the ACE/ERE standards can be mapped to FrameNet frames. The mapping is complicated by two factors. The first is that FrameNet frames are generally more fine-grained than the ACE/ERE categories. As a result the mapping is sometimes one-to-many. For example, the ERE relation AfAdditional tasks Starting in 2012 TAC-KBP introduced the “Cold Start” task, which is to literally produce a KB based on the Slot Filling schema. To date, Cold Start KBs have been built fro"
W14-2907,doddington-etal-2004-automatic,1,0.823282,"Missing"
W14-2907,J02-3001,0,0.0107376,"nlike ACE/ERE it prioritizes lexicographic and linguistic completeness over ease of annotation. As a result Frames tend to be much finer grained than ACE/ERE events, and are more numerous by an order of magnitude. The Berkeley FrameNet Project (Baker et al., 1998) was developed as a machine readable database of distinct frames and lexical units (words and multi-word constructions) that were known to trigger specific frames.1 FrameNet 1.5 includes 1020 identified frames and 11830 lexical units. One of the most widespread uses of FrameNet has been as a resource for Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). FrameNet related SRL was promoted as a task by the SENSEVAL-3 workshop (Litkowski, 2004), and the SemEval-2007 workshop (Baker et al., 2007). (Das et al., 2010) is a current system for automatic FrameNet annotation. The relation and attribute types of TAC-KBP and the relation and event types in the ACE/ERE standards can be mapped to FrameNet frames. The mapping is complicated by two factors. The first is that FrameNet frames are generally more fine-grained than the ACE/ERE categories. As a result the mapping is sometimes one-to-many. For example, the ERE relation AfAdditional tasks Starting"
W14-2907,C98-1013,0,\N,Missing
W14-2908,W07-1427,0,0.0589673,"Missing"
W14-2908,de-marneffe-etal-2006-generating,0,0.0509743,"Missing"
W14-2908,H05-1049,0,0.0301898,"mantic phenomena to make this exercise meaningful. Introduction The Stanford dependency parser (De Marneffe et al., 2006) provides “deep” syntactic analysis of natural language by layering a set of hand-written post-processing rules on top of Stanford’s statistical constituency parser (Klein and Manning, 2003). Stanford dependency parses are commonly used as a semantic representation in natural language understanding and inference systems.1 For example, they have been used as a basic meaning representation for the Recognizing Textual Entailment task proposed by Dagan et al. (2005), such as by Haghighi et al. (2005) or MacCartney (2009) and in other inference systems (Chambers et al., 2007; MacCartney, 2009). Because of their popular use as a semantic representation, it is important to ask whether the Stanford Dependencies do, in fact, encode the kind of Our results indicate that in a number of cases, it is, in fact, possible to directly derive HLFs from Stanford dependency parses. At the same time, however, we also find difficult-to-map phenomena that reveal inherent limitations of the dependencies as a meaning representation. 2 Background This section provides a brief overview of the HLF and Stanford d"
W14-2908,P85-1008,0,0.842186,"n Logical Form The key insight of event-theoretic semantic representations is the reification of events (Davidson, 1967), or, treating events as entities in the world. As a logical, first-order representation, Hobbsian 1 Statement presented by Chris Manning at the *SEM 2013 Panel on Language Understanding http://nlpers.blogspot.com/2013/07/the-sem-2013-panelon-language.html. 54 Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 54–58, c Baltimore, Maryland, USA, June 22-27, 2014. 2014 Association for Computational Linguistics Logical Form (Hobbs, 1985) employs this approach by allowing for the reification of any predicate into an event variable. Specifically, for any predicate p(x1 , · · · , xn ), there is a corresponding predicate, p0 (E, x1 , · · · , xn ), where E refers to the predicate (or event) p(x1 , · · · , xn ). The reified predicates are related to their non-reified forms with the following axiom schema: extraction, argument identification, predicateargument assignment, and formula construction. We demonstrate these steps on the above example sentence “A boy wants to build a boat quickly.”3 The rule-based algorithm operates on the"
W14-2908,P03-1054,0,0.00929409,"t semantic standard against which to compare the Stanford Dependencies. We make no such claim. Rather, our intent is to provide a qualitative discussion of the Stanford Dependencies as a semantic resource through the lens of this HLF mapping task. It is only necessary that HLF capture some subset of important semantic phenomena to make this exercise meaningful. Introduction The Stanford dependency parser (De Marneffe et al., 2006) provides “deep” syntactic analysis of natural language by layering a set of hand-written post-processing rules on top of Stanford’s statistical constituency parser (Klein and Manning, 2003). Stanford dependency parses are commonly used as a semantic representation in natural language understanding and inference systems.1 For example, they have been used as a basic meaning representation for the Recognizing Textual Entailment task proposed by Dagan et al. (2005), such as by Haghighi et al. (2005) or MacCartney (2009) and in other inference systems (Chambers et al., 2007; MacCartney, 2009). Because of their popular use as a semantic representation, it is important to ask whether the Stanford Dependencies do, in fact, encode the kind of Our results indicate that in a number of case"
W14-2908,J05-1004,0,0.129137,"ch limitation arises in cases of event nominalizations. Because dependency parses are syntax-based, their structures do not distinguish between eventive noun phrases like “the bombing of the city” and non-eventive ones like “the mother of the child”; such a distinction, however, would be found in the corresponding HLFs. Certain syntactic alternations also prove problematic. For example, the dependency structure does not recognize that “window” takes the same semantic role in the sentences “John broke the mirror.” and “The mirror broke.” The use of additional semantic resources, like PropBank (Palmer et al., 2005), would be necessary to determine this. Our argument does not require that HLF be an optimal representation, only that it capture worthwhile aspects of semantics and that it not be readily derived from the Stanford representation. This is enough to conclude that the Stanford Dependencies are not complete as a meaning representation. While not surprising (as they are intended as a syntactic representation), we hope this short study will help further discussion on what the community wants or needs in a meaning representation: what gaps are acceptable, if any, and whether a more “complete” repres"
W14-3301,W11-2101,0,0.0261404,"Missing"
W14-3301,W11-2113,0,0.0533508,"Missing"
W14-3301,W12-3102,1,0.601062,"Missing"
W14-3301,P13-1139,0,0.553095,"§2). We evaluate on perplexity and also on accuracy, showing that the two are not always correlated, and arguing for the primacy of the latter (§3). We find that TrueSkill outperforms other models (§4). Moreover, TrueSkill also allows us to drastically reduce the amount of data that needs to be collected by sampling non-uniformly from the space of all competitions (§5), which also allows for greater separation of the systems into ranked clusters (§6). raised about how the evaluation is conducted (Bojar et al., 2011; Lopez, 2012). While these tweaks have been sensible (and later corroborated), Hopkins and May (2013) point out that this is essentially a model selection task, and should properly be driven by empirical performance on heldout data according to some metric. Instead of intuition, they suggest perplexity, and show that a novel graphical model outperforms existing approaches on that metric, with less amount of data. A second problem is the deficiency of the models used to produce the ranking, which work by computing simple ratios of wins (and, optionally, ties) to losses. Such approaches do not consider the relative difficulty of system matchups, and thus leave open the possibility that a system"
W14-3301,2012.iwslt-papers.5,0,0.340294,"ts metrics task, where evaluation metrics are evaluated. In machine translation, the longstanding disagreements about evaluation measures do not go away when moving from automatic metrics to human judges. This is due in no small part to the inherent ambiguity and subjectivity of the task, but also arises from the particular way that the WMT organizers produce the rankings. The systemlevel rankings are produced by collecting pairwise sentence-level comparisons between system outputs. These are then aggregated to produce a complete ordering of all systems, or, more recently, a partial ordering (Koehn, 2012), with systems clustered where they cannot be distinguished in a statistically significant way (Table 1, taken from Bojar et al. (2013)). Introduction The Workshop on Statistical Machine Translation (WMT) has long been a central event in the machine translation (MT) community for the evaluation of MT output. It hosts an annual set of shared translation tasks focused mostly on the translation of western European languages. One of its main functions is to publish a ranking of the systems for each task, which are produced by aggregating a large number of human judgments of sentencelevel pairwise"
W14-3301,W12-3101,0,0.0513987,"r provides an empirical comparison of a number of models of human evaluation (§2). We evaluate on perplexity and also on accuracy, showing that the two are not always correlated, and arguing for the primacy of the latter (§3). We find that TrueSkill outperforms other models (§4). Moreover, TrueSkill also allows us to drastically reduce the amount of data that needs to be collected by sampling non-uniformly from the space of all competitions (§5), which also allows for greater separation of the systems into ranked clusters (§6). raised about how the evaluation is conducted (Bojar et al., 2011; Lopez, 2012). While these tweaks have been sensible (and later corroborated), Hopkins and May (2013) point out that this is essentially a model selection task, and should properly be driven by empirical performance on heldout data according to some metric. Instead of intuition, they suggest perplexity, and show that a novel graphical model outperforms existing approaches on that metric, with less amount of data. A second problem is the deficiency of the models used to produce the ranking, which work by computing simple ratios of wins (and, optionally, ties) to losses. Such approaches do not consider the r"
W14-3301,P02-1040,0,0.103554,"is line of work by adapting the TrueSkillTM algorithm — an online approach for modeling the relative skills of players in ongoing competitions, such as Microsoft’s Xbox Live — to the human evaluation of machine translation output. Our experimental results show that TrueSkill outperforms other recently proposed models on accuracy, and also can significantly reduce the number of pairwise annotations that need to be collected by sampling non-uniformly from the space of system competitions. 1 score 0.638 0.604 0.591 0.571 0.562 0.541 0.512 0.486 0.439 0.429 0.420 0.389 0.322 reported (e.g., BLEU (Papineni et al., 2002)), the human evaluation is considered primary, and is in fact used as the gold standard for its metrics task, where evaluation metrics are evaluated. In machine translation, the longstanding disagreements about evaluation measures do not go away when moving from automatic metrics to human judges. This is due in no small part to the inherent ambiguity and subjectivity of the task, but also arises from the particular way that the WMT organizers produce the rankings. The systemlevel rankings are produced by collecting pairwise sentence-level comparisons between system outputs. These are then aggr"
W14-3301,W13-2201,1,\N,Missing
W16-5905,P98-1013,0,0.378848,"of NLP tasks including information extraction, question answering, and coreference resolution. Let x refer to a sentence and its POS tags and dependency parse. For this work, we are given x and a vector of predicate locations t = [t1 , t2 , ...tn ], where each ti is a span, most often representing a single verb like “love” in the sentence “John loves Mary”. SRL and FSP are defined with respect to a schema which provides a set of frames and roles which will serve as labels for predicates and arguments. We consider two schemas, Propbank (Kingsbury and Palmer, 2002) and FrameNet (Fillmore, 1982; Baker et al., 1998). Propbank frames concern different senses of a lexical unit (a lemma and POS tag), so the correct frame for “love” in the case above is the frame love-v-1, as opposed to love-v-2, which is only used in modal cases like “I would love to go on vacation”. In the FrameNet schema, frames are coarser grain situations which may have many lexical units which map to them. In this case frame would be Experiencer focus which could also be evoked by the adore.v or despise.v lexical units. These frames will constitute another vector f = [f1 , f2 , ...fn ] of frames for each predicate in t. Once t and f ar"
W16-5905,Q15-1039,0,0.0260718,"uction. We found that defining costs based on the Hamming loss of an action performed very poorly. We found much better results with the multiclass hinge encoding described in Lee et al. (2004). In figure 6 we show performance with various choices of roll-in and cost definitions. The best LOLS global 5 If you label a span as the Cognizer role for the frame Opinion and that span was the Cognizer role for the Judgment frame, then the label is wrong. 6 with the exception of ARG0 and ARG1 which typically correspond to proto-Agent and proto-Patient roles. 51 8.3 9 Absolute Performance Related Work Berant and Liang (2015) used imitation learning for learning a semantic parser. Choi and Palmer (2011) explored transition based SRL and proposed some global features (e.g. copy ARG0 from controlling predicates) but did not consider action (re-)ordering or imitation learning. Wiseman and Rush (2016) derive a learning to search framework which is related to LaSO (Daum´e III and Marcu, 2005). Similar to our hybrid roll-in, they “reset” the beam as soon as the oracle prefix falls off. 10 Conclusion In this work we study the use of imitation learning for greedy global models for SRL. We analyze the Violation Fixing Perc"
W16-5905,W11-0906,0,0.0217947,"ormed very poorly. We found much better results with the multiclass hinge encoding described in Lee et al. (2004). In figure 6 we show performance with various choices of roll-in and cost definitions. The best LOLS global 5 If you label a span as the Cognizer role for the frame Opinion and that span was the Cognizer role for the Judgment frame, then the label is wrong. 6 with the exception of ARG0 and ARG1 which typically correspond to proto-Agent and proto-Patient roles. 51 8.3 9 Absolute Performance Related Work Berant and Liang (2015) used imitation learning for learning a semantic parser. Choi and Palmer (2011) explored transition based SRL and proposed some global features (e.g. copy ARG0 from controlling predicates) but did not consider action (re-)ordering or imitation learning. Wiseman and Rush (2016) derive a learning to search framework which is related to LaSO (Daum´e III and Marcu, 2005). Similar to our hybrid roll-in, they “reset” the beam as soon as the oracle prefix falls off. 10 Conclusion In this work we study the use of imitation learning for greedy global models for SRL. We analyze the Violation Fixing Perceptron (VFP) and Locally Optimal Learning to Search (LOLS) frameworks, explaini"
W16-5905,P04-1015,0,0.561767,"ch is put on the next beam. In VFP, the core concept is a violation. A tuple (x, y, z), where x is a sentence as defined earlier and y is a string of correct actions (having zero cost/loss), and z is a string of predicted actions, is a violation if θ · f (x, z) &gt; θ · f (x, y) and z is “incorrect”. There are multiple ways of defining incorrect which yield different algorithms in the VFP family. In all variants y and z must be the same length and if there is more than one incorrect (x, y, z), the one with the largest difference in score is chosen. In the early update variant, first described by Collins and Roark (2004), z is incorrect if it differs from y only in the last position. In max violation z is incorrect if 47 Global Feature numArgs roleCooc argLoc roleCoocArgLoc full Gold f PB ∆` FN ∆` -0.4 -0.1 -0.4 -0.3 -1.2 -0.4 -2.0 -0.2 -1.5 -0.7 Auto f PB ∆` FN ∆` -1.3 +0.3 -0.1 +0.6 -1.9 +0.2 0.0 +0.2 -2.0 +0.2 Figure 1: Global model advantage using max violation VFP and freq. it differs any position. In latest update z is incorrect if it differs in the last position (but can include other differences, unlike early update). Results In figure 1 we plot the difference in performance between a model which incl"
W16-5905,N10-1138,0,0.0191829,"f the transition system on the usefulness of global features. We find that the order that actions are performed in can be as important as the training method, leading to better models with the same features and computational complexity. 2 Problem Formulation Semantic role labeling (Gildea and Jurafsky, 2002) (SRL) is the task of locating and labeling (with roles) the semantic arguments to predicates. Adding 44 Proceedings of the Workshop on Structured Prediction for NLP, pages 44–53, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics a step, frame semantic parsing (Das et al., 2010) (FSP), seeks to first disambiguate predicates by labeling them with a frame before performing SRL. Semantic roles abstract over grammatical function and provide information about particular arguments relation to an event, state, or fact. SRL has been shown to be helpful in a variety of NLP tasks including information extraction, question answering, and coreference resolution. Let x refer to a sentence and its POS tags and dependency parse. For this work, we are given x and a vector of predicate locations t = [t1 , t2 , ...tn ], where each ti is a span, most often representing a single verb li"
W16-5905,S12-1029,0,0.0184142,"ants as a linear interpolation between a global and local objective. models consistently improve over local models. 8.2 LOLS Throughout the paper we have listed relative performance. Our absolute performance is 73.0 for Propbank (dev) and 55.3 for FrameNet (dev). This falls significantly short of the work of Zhou and Xu (2015) at 81.1 (PB dev), FitzGerald et al. (2015) at 79.2 (PB dev), and 72.0 (FN). Those works used non-linear neural models with multi-task distributed representations, which are not comparable to our results. However, the models of Pradhan et al. (2013) at 77.5 (PB test) and Das et al. (2012) at 64.6 (FN test) are roughly comparable, and the performance gap is still significant. While our efforts do not advance the state of the art in SRL, we hope that they are enlightening with respect to the application of various imitation learning methods. LOLS performs a roll-in with the current policy. This causes many updates which are derived from mistakes during frame identification. Once the wrong frame is predicted, in argument identification the model’s cost incentives flip towards trying to predict ∅ for all roles so as not to incur false positives. The roles in FrameNet are defined b"
W16-5905,D15-1112,0,0.0230135,"Missing"
W16-5905,J02-3001,0,0.613884,"eness of the Violation Fixing Perceptron (VFP) (Huang et al., 2012) and Locally Optimal Learning to Search (LOLS) (Chang et al., 2015) frameworks with respect to SRL global features. We describe problems in applying each framework to SRL and evaluate the effectiveness of some solutions. We also show that action ordering, including easy first inference, has a large impact on the quality of greedy global models. 1 Introduction In structured prediction problems, global features express dependencies between related pieces of a label and make inference non-trivial. In Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002), global features and constraints have been studied extensively (Punyakanok et al., 2004; Toutanova et al., 2008; T¨ackstr¨om et al., 2015) inter alia. SRL has many phenomenon that relate labels such as syntactic control, role mutual exclusion, and structural constraints like span overlap. Previous work on inference for models with global features has studied a variety of method including dynamic programming, reranking, and ILP solvers. Greedy search and beam search are relatively understudied areas due to the difficulty in training models which perform well with the weak guarantees provided b"
W16-5905,N10-1115,0,0.031505,"th the structured perceptron. We use the local features described in Hermann et al. (2014) for argument and frame identification, but we did not use their feature embedding method since it performed about as well as the sparse feature method and was slower. We use the best refinements using the process described in §3. We are studying the fully greedy case of inference in this work (i.e. a beam size of 1). As far as we know, efficient greedy and easy first inference are mutually exclusive goals, and we focus on the latter. Our implementation uses a heap to store actions in a manner similar to Goldberg and Elhadad (2010). This way actions can be generated once, instead of once per transition, and global features perform sparse updates to the actions on the heap. For beam search, states cannot share a heap (since their histories, and thus global features, would be different), so actions generation, global features, and action sorting would have to occur at every transition. All performance values shown here are measured for the task of frame semantic parsing (FSP), meaning that we measure precision, recall, and F-measure where every index in f and k are considered predictions. Predictions in k are not correct"
W16-5905,P14-1136,0,0.0126481,"lobal feature template, we try each refinement and use the one with the best dev set F-measure when trained with LOLS. 4 Experimental Design We measure performance on two data sets, the Propbank annotations (Kingsbury and Palmer, 2002) available in the Ontonotes 5.0 corpus (Pradhan et al., 2012) and FrameNet 1.5 (Baker et al., 1998). For all learning methods we average the weights across all iterations of training (Freund and Schapire, 1999). This is explicitly called for as a part of LOLS and is also a standard trick used with the structured perceptron. We use the local features described in Hermann et al. (2014) for argument and frame identification, but we did not use their feature embedding method since it performed about as well as the sparse feature method and was slower. We use the best refinements using the process described in §3. We are studying the fully greedy case of inference in this work (i.e. a beam size of 1). As far as we know, efficient greedy and easy first inference are mutually exclusive goals, and we focus on the latter. Our implementation uses a heap to store actions in a manner similar to Goldberg and Elhadad (2010). This way actions can be generated once, instead of once per t"
W16-5905,N12-1015,0,0.0529926,"Missing"
W16-5905,kingsbury-palmer-2002-treebank,0,0.0872977,"e, or fact. SRL has been shown to be helpful in a variety of NLP tasks including information extraction, question answering, and coreference resolution. Let x refer to a sentence and its POS tags and dependency parse. For this work, we are given x and a vector of predicate locations t = [t1 , t2 , ...tn ], where each ti is a span, most often representing a single verb like “love” in the sentence “John loves Mary”. SRL and FSP are defined with respect to a schema which provides a set of frames and roles which will serve as labels for predicates and arguments. We consider two schemas, Propbank (Kingsbury and Palmer, 2002) and FrameNet (Fillmore, 1982; Baker et al., 1998). Propbank frames concern different senses of a lexical unit (a lemma and POS tag), so the correct frame for “love” in the case above is the frame love-v-1, as opposed to love-v-2, which is only used in modal cases like “I would love to go on vacation”. In the FrameNet schema, frames are coarser grain situations which may have many lexical units which map to them. In this case frame would be Experiencer focus which could also be evoked by the adore.v or despise.v lexical units. These frames will constitute another vector f = [f1 , f2 , ...fn ]"
W16-5905,W12-4501,0,0.0117087,"granularity for the global feature templates, we consider multiple refinements. A refinement of a template is the result of taking the pointwise product of the template with one or two label features templates. The label feature templates we consider are constant (a backoff feature), frame, role, and frame-role. For each global feature template, we try each refinement and use the one with the best dev set F-measure when trained with LOLS. 4 Experimental Design We measure performance on two data sets, the Propbank annotations (Kingsbury and Palmer, 2002) available in the Ontonotes 5.0 corpus (Pradhan et al., 2012) and FrameNet 1.5 (Baker et al., 1998). For all learning methods we average the weights across all iterations of training (Freund and Schapire, 1999). This is explicitly called for as a part of LOLS and is also a standard trick used with the structured perceptron. We use the local features described in Hermann et al. (2014) for argument and frame identification, but we did not use their feature embedding method since it performed about as well as the sparse feature method and was slower. We use the best refinements using the process described in §3. We are studying the fully greedy case of inf"
W16-5905,W13-3516,0,0.0336754,"Missing"
W16-5905,W04-2421,0,0.126698,"rning to Search (LOLS) (Chang et al., 2015) frameworks with respect to SRL global features. We describe problems in applying each framework to SRL and evaluate the effectiveness of some solutions. We also show that action ordering, including easy first inference, has a large impact on the quality of greedy global models. 1 Introduction In structured prediction problems, global features express dependencies between related pieces of a label and make inference non-trivial. In Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002), global features and constraints have been studied extensively (Punyakanok et al., 2004; Toutanova et al., 2008; T¨ackstr¨om et al., 2015) inter alia. SRL has many phenomenon that relate labels such as syntactic control, role mutual exclusion, and structural constraints like span overlap. Previous work on inference for models with global features has studied a variety of method including dynamic programming, reranking, and ILP solvers. Greedy search and beam search are relatively understudied areas due to the difficulty in training models which perform well with the weak guarantees provided by greedy search. The Violation Fixing Perceptron (VFP) framework (Huang et al., 2012) is"
W16-5905,D10-1048,0,0.0134057,"Results In figure 2 we plot global model advantage using the freq action orderings and LOLS training. There are mixed results; some global features are actually improving over the local model (something which was not achieved by VFP training). We will return to why this is in §8.2, but first we will analyze an orthogonal aspect of the model. 7 Action Ordering So far our transition system considers actions sorted by frequency of a role, which may not be optimal. Here we measure the effect of other orderings. Easy First The first motivation is related to easy first inference (Shen et al., 2007; Raghunathan et al., 2010) inter alia. The idea is that the “easiest” decisions should be made first because there is less risk that they are wrong and may be more safely conditioned on in making future decisions than any other action. To implement this heuristic, we define two variants of the easyfirst meta action ordering. easyfirst-dynamic chooses the variable index corresponding to the highest scoring action. easyfirst-static chooses variable indices sorted by 2 Every action fills in a label and we can say whether it is right or wrong, thus the reference policy is the one which always fills in a correct label. the"
W16-5905,P07-1096,0,0.0230825,"return to in §8.2. Results In figure 2 we plot global model advantage using the freq action orderings and LOLS training. There are mixed results; some global features are actually improving over the local model (something which was not achieved by VFP training). We will return to why this is in §8.2, but first we will analyze an orthogonal aspect of the model. 7 Action Ordering So far our transition system considers actions sorted by frequency of a role, which may not be optimal. Here we measure the effect of other orderings. Easy First The first motivation is related to easy first inference (Shen et al., 2007; Raghunathan et al., 2010) inter alia. The idea is that the “easiest” decisions should be made first because there is less risk that they are wrong and may be more safely conditioned on in making future decisions than any other action. To implement this heuristic, we define two variants of the easyfirst meta action ordering. easyfirst-dynamic chooses the variable index corresponding to the highest scoring action. easyfirst-static chooses variable indices sorted by 2 Every action fills in a label and we can say whether it is right or wrong, thus the reference policy is the one which always fil"
W16-5905,Q15-1003,0,0.0242568,"Missing"
W16-5905,J08-2002,0,0.0335123,"Missing"
W16-5905,D16-1137,0,0.0130458,"ns. The best LOLS global 5 If you label a span as the Cognizer role for the frame Opinion and that span was the Cognizer role for the Judgment frame, then the label is wrong. 6 with the exception of ARG0 and ARG1 which typically correspond to proto-Agent and proto-Patient roles. 51 8.3 9 Absolute Performance Related Work Berant and Liang (2015) used imitation learning for learning a semantic parser. Choi and Palmer (2011) explored transition based SRL and proposed some global features (e.g. copy ARG0 from controlling predicates) but did not consider action (re-)ordering or imitation learning. Wiseman and Rush (2016) derive a learning to search framework which is related to LaSO (Daum´e III and Marcu, 2005). Similar to our hybrid roll-in, they “reset” the beam as soon as the oracle prefix falls off. 10 Conclusion In this work we study the use of imitation learning for greedy global models for SRL. We analyze the Violation Fixing Perceptron (VFP) and Locally Optimal Learning to Search (LOLS) frameworks, explaining how they fall short and offer some methods for improving them. We also study the effect of inference order on learning and the utility of global features, finding that it is a very important fact"
W16-5905,W04-3212,0,0.0622974,"efines a function mapping a frame to a set of roles K(fi ) which each frame must have filled explicitly (by some mention span in the sentence) or implicitly (by some other discourse entity not directly mentioned in the sentence). For the latter case we say that an unfilled role is filled by a special dummy span called ∅. For the former case, we could in principle predict any span within the sentence, but to make systems faster and more accurate, a pruning step is often used which picks out only the spans which are plausible arguments to a particular predicate conditioned on a syntactic parse (Xue and Palmer, 2004). We call this set S(ti )1 and it always 1 Extensions like the one described in T¨ackstr¨om et al. (2015) consider the role during the pruning step, but we gloss 45 includes ∅. SRL is the task of predicting a matrix k = {kij : i ∈ [1..n], j ∈ K(fi ), kij ∈ S(ti )} where kij is the location of the j th role for frame fi evoked by the predicate at ti . For the rest of this paper, we will concern ourselves with the FSP task of predicting both f and k. Transition System A transition system provides a way to break down an assignment to (f, k) into a sequence of actions. The transition systems we us"
W16-5905,P15-1109,0,0.0194193,"gure 5: Global model advantage using roleCooc and easyfirst-dynamic across VFP variations and + CLASS. Figure 6: Global model advantage using roleCooc and easyfirst-dynamic across LOLS variations: roll-in and cost function. which is a pure CLASS update, so you can think of the + CLASS variants as a linear interpolation between a global and local objective. models consistently improve over local models. 8.2 LOLS Throughout the paper we have listed relative performance. Our absolute performance is 73.0 for Propbank (dev) and 55.3 for FrameNet (dev). This falls significantly short of the work of Zhou and Xu (2015) at 81.1 (PB dev), FitzGerald et al. (2015) at 79.2 (PB dev), and 72.0 (FN). Those works used non-linear neural models with multi-task distributed representations, which are not comparable to our results. However, the models of Pradhan et al. (2013) at 77.5 (PB test) and Das et al. (2012) at 64.6 (FN test) are roughly comparable, and the performance gap is still significant. While our efforts do not advance the state of the art in SRL, we hope that they are enlightening with respect to the application of various imitation learning methods. LOLS performs a roll-in with the current policy. This"
W16-5905,C98-1013,0,\N,Missing
W17-1609,P16-2096,0,0.0266098,"n in addition to a technological one, big data carries subjective aspects (Crawford et al., 2014). The data mining process involves defining a target variable and evaluation criteria, collecting a dataset, selecting a manner in which to represent the data, and sometimes eliciting annotations: bias, whether or implicit or explicit, may be introduced in the performance of each of these tasks (Barocas and Selbst, 2016). We focus on the problem of overgeneralization, in which a data mining model extrapolates excessively from observed patterns, leading to bias confirmation among the model’s users (Hovy and Spruit, 2016). High-profile cases of overgeneralization in the public sphere abound (Crawford, 2013; Crawford, 2016; Barocas and Selbst, 2016). Research on the measurement and correction of overgeneralization in NLP in particular is nascent. 2 The SNLI Dataset Bowman et al. (2015) introduce the Stanford Natural Language Inference corpus. The corpus was generated by presenting crowdworkers with * denotes equal contribution. 74 Proceedings of the First Workshop on Ethics in Natural Language Processing, pages 74–79, c 2017 Association for Computational Linguistics Valencia, Spain, April 4th, 2017. a photo cap"
W17-1609,D15-1075,0,0.0190482,"eliciting annotations: bias, whether or implicit or explicit, may be introduced in the performance of each of these tasks (Barocas and Selbst, 2016). We focus on the problem of overgeneralization, in which a data mining model extrapolates excessively from observed patterns, leading to bias confirmation among the model’s users (Hovy and Spruit, 2016). High-profile cases of overgeneralization in the public sphere abound (Crawford, 2013; Crawford, 2016; Barocas and Selbst, 2016). Research on the measurement and correction of overgeneralization in NLP in particular is nascent. 2 The SNLI Dataset Bowman et al. (2015) introduce the Stanford Natural Language Inference corpus. The corpus was generated by presenting crowdworkers with * denotes equal contribution. 74 Proceedings of the First Workshop on Ethics in Natural Language Processing, pages 74–79, c 2017 Association for Computational Linguistics Valencia, Spain, April 4th, 2017. a photo caption (but not the corresponding photo) from the Flickr30k corpus (Young et al., 2014) and instructing them to write a new alternate caption for the unseen photo under one of the following specifications: The new caption must either be [1] “definitely a true descriptio"
W17-1609,P09-1068,0,0.0115749,"other people stand around tables with checkered tablecloths and a ladder. HYPOTHESIS ( NEUTR .): The man is a transvestite. [ PL ] 6 The authors recognize the partially subjective nature of applying these labels. 77 idence of stereotypes, including stereotypes of intersectional identities, by merging the counts of semantically related terms (or, conversely, by decoupling the counts of homonyms). It could also be fruitful to infer dependency parses and compute co-occurrences between dependency paths rather than individual words to facilitate interpretation of the results (Lin and Pantel, 2001; Chambers and Jurafsky, 2009), if sparsity can be controlled. We have focused on the identities and accompanying biases present in the SNLI dataset, in particular those created in the hypothesis elicitation process; one complement to our study would measure the demographic bias in the corpus. Correlations introduced at any level in the data collection process—including real-world correlations present in the population—are subject to scrutiny, as they may be both creations and creators of structural inequality. As artificial intelligence absorbs the world’s collective knowledge with increasing efficiency and comprehension,"
W17-1609,J90-1003,0,0.167493,"en the indicator variables Xw1 = I{W1 =w1 } and Yw2 = I{W2 =w2 } with a likelihood ratio test. (Hereafter we omit subscripts w1 and w2 for ease of notation.) Denote the observed counts of X and Y over the corpus by C 0 (x, y) for x, y ∈ {0, 1}.2 The test statistic is Methodology  P Λ(C 0 ) = We are ultimately concerned with the impact of a dataset’s biases on the models and applications that are trained on it. To avoid dependence on a particular model or model family, we evaluate the SNLI dataset in a model-agnostic fashion using the pointwise mutual information (PMI) measure of association (Church and Hanks, 1990) and likelihood ratio tests of independence (Dunning, 1993) between lexical units. Given categorical random variables W1 and W2 representing word occurrences in a corpus, for each word type (or bigram) w1 in the range of W1 and for each word type (or bigram) w2 in the range x,y P Pˆ (X = x)Pˆ (Y = y) x,y C 0 (x,y) Pˆ (X = x, Y = y)C 0 (x,y) . where Pˆ is the maximum likelihood estimator (using C 0 ), the summations range over x, y ∈ {0, 1}, and we have dropped the subscripts w1 and w2 for ease of notation. The quantity −2 log Λ(C 0 ) is χ2 distributed with one degree of freedom, so we can use"
W17-1609,J93-1003,0,0.590759,"h a likelihood ratio test. (Hereafter we omit subscripts w1 and w2 for ease of notation.) Denote the observed counts of X and Y over the corpus by C 0 (x, y) for x, y ∈ {0, 1}.2 The test statistic is Methodology  P Λ(C 0 ) = We are ultimately concerned with the impact of a dataset’s biases on the models and applications that are trained on it. To avoid dependence on a particular model or model family, we evaluate the SNLI dataset in a model-agnostic fashion using the pointwise mutual information (PMI) measure of association (Church and Hanks, 1990) and likelihood ratio tests of independence (Dunning, 1993) between lexical units. Given categorical random variables W1 and W2 representing word occurrences in a corpus, for each word type (or bigram) w1 in the range of W1 and for each word type (or bigram) w2 in the range x,y P Pˆ (X = x)Pˆ (Y = y) x,y C 0 (x,y) Pˆ (X = x, Y = y)C 0 (x,y) . where Pˆ is the maximum likelihood estimator (using C 0 ), the summations range over x, y ∈ {0, 1}, and we have dropped the subscripts w1 and w2 for ease of notation. The quantity −2 log Λ(C 0 ) is χ2 distributed with one degree of freedom, so we can use it to test rejection of the null hypothesis (independence"
W17-1609,Q14-1006,0,\N,Missing
W17-6936,W14-4012,0,0.0412872,"Missing"
W17-6936,N16-1024,0,0.0233907,"scussed in §1, it has been observed that NLP approaches that embed an entire sentence into a single, fixed-size vector may degrade in performance on longer sentences. One answer to this problem is to use finer-grained, multi-vector sentence representations that can grow with sentence complexity. Indeed, most neural (or otherwise continuous-space) models of sentences provide some finer-grained vector representations, most notably at the token level (i.e., standard RNN implementations), sub-token level (Sennrich et al., 2016), character-level (Kim et al., 2016), and syntactic constituent level (Dyer et al., 2016), and are often used in task-specific attention mechanisms. For many tasks involving search or attention, however, such as open question-answering or document-level analysis, preserving each such intermediate representation may be prohibitively expensive. In comparison to other fine-grained, multi-vector representations, skip-prop offers two advantages: (1) the number of propositions (and hence vectors) per sentence is relatively few, and (2) the proposition is its own interpretable unit of meaning. Figs. 1 and 2 illustrate the granularity-expense tradeoff between one-vector-per sentence, prop"
W17-6936,P02-1026,0,0.0255171,"ically useful level of granularity. We demonstrate the feasibility of training skip-prop vectors, introducing a method adapted from skip-thought vectors, and compare skip-prop with “one vector per sentence” and “one vector per token” approaches. 1 Introduction The length and complexity of written natural language sentences is highly variable. Sentences from New York Times (NYT) stories (August 1997), for example, contain on average 23 tokens, with a standard deviation of 12. By information-theoretic measures, too, natural language sentences convey differing amounts of information (Hale, 2003; Genzel and Charniak, 2002). It is natural to suppose, then, that methods in computational linguistics that aim to learn fixed-size semantic representations of sentences, i.e., with vectors of fixed dimension, may be limited in their expressiveness or efficiency. Indeed, on many NLP tasks for which neural sentence embedding methods have been adapted, degraded performance on longer input sentences is commonly observed: in machine translation (Cho et al., 2014), question-answering (Kumar et al., 2016), and semantic role labeling (Zhou and Xu, 2015), for example. Motivated by these observations, we introduce skip-prop vect"
W17-6936,marelli-etal-2014-sick,0,0.224409,"les from Sept. 1997; and test is 5K random sentence triples from Oct. 1997. The vocabulary is approximately 39K tokens from Sept. 1997 NYT with minimum frequency of 15. Each model is trained for one epoch on the entire train set using mini-batches of size 1. As described in §4, a sentence triple (sl , sc , sr ) consists of a contiguous set of three sentences from a news story: a “left,” “center,” and “right” sentence. For the qualitative nearest-neighbor experiments, two datasets are used: (1) a 100K superset of the NYT development set (Sept. 1997), and (2) all sentences from the SICK corpus (Marelli et al., 2014). Results As a preliminary evaluation of skip-prop vectors, we present both quantitative and qualitative results. These results show that (1) it is feasible to train skip-prop vectors with our proposed method, and (2) some notion of semantic similarity over propositions is preserved in this representation. Table 1 shows the perplexity attained by each model. Here, perplexity is computed either from the two decoders’ predictions of the left and right context sentences (skip-thought objective), or one decoder’s prediction of the original sentence (autoencoder objective). In all cases, the skip-p"
W17-6936,N06-1020,0,0.0360103,"hows typical NYT documents contain more propositions than sentences, and many more tokens than propositions. (Log scale x-axis.) 3 Figure 2: Scatter plot shows longer sentences contain more propositions. Most sentences contain fewer than 50 tokens, and fewer than 8 propositions. Background Sequence-to-Sequence Models Sequence-to-sequence (seq-to-seq) models are a class of neural networks that compute the conditional probability of an output sequence given an input sequence, i.e., P (y1 ...yn |x1 ...xm ). They have been applied to many tasks in NLP (Bahdanau et al., 2014; Vinyals et al., 2015; McClosky et al., 2006), though here we train them to encode multi-vector sentence representations. Typical seq-to-seq models consist of two recurrent neural networks (RNNs): an encoder and decoder, which iterate over the input and output sequences, respectively. The final hidden state of the encoder RNN, hm , is passed as the initial state to the decoder RNN. Thus, the vector hm is a representation of the entireQ input, and the decoder computes the conditional distribution: P (y1 ...yn |x1 ...xm ) = P (y1 ...yn |hm ) = ni=1 P (yi |y&lt;i , hm ). We train skip-prop with a multi-encoder, multi-decoder variant of seq-to-"
W17-6936,P16-1162,0,0.0081757,"king a “one vector per proposition” approach to representing the meaning of a sentence. As discussed in §1, it has been observed that NLP approaches that embed an entire sentence into a single, fixed-size vector may degrade in performance on longer sentences. One answer to this problem is to use finer-grained, multi-vector sentence representations that can grow with sentence complexity. Indeed, most neural (or otherwise continuous-space) models of sentences provide some finer-grained vector representations, most notably at the token level (i.e., standard RNN implementations), sub-token level (Sennrich et al., 2016), character-level (Kim et al., 2016), and syntactic constituent level (Dyer et al., 2016), and are often used in task-specific attention mechanisms. For many tasks involving search or attention, however, such as open question-answering or document-level analysis, preserving each such intermediate representation may be prohibitively expensive. In comparison to other fine-grained, multi-vector representations, skip-prop offers two advantages: (1) the number of propositions (and hence vectors) per sentence is relatively few, and (2) the proposition is its own interpretable unit of meaning. Figs."
W17-6936,P15-1109,0,0.027445,"age sentences convey differing amounts of information (Hale, 2003; Genzel and Charniak, 2002). It is natural to suppose, then, that methods in computational linguistics that aim to learn fixed-size semantic representations of sentences, i.e., with vectors of fixed dimension, may be limited in their expressiveness or efficiency. Indeed, on many NLP tasks for which neural sentence embedding methods have been adapted, degraded performance on longer input sentences is commonly observed: in machine translation (Cho et al., 2014), question-answering (Kumar et al., 2016), and semantic role labeling (Zhou and Xu, 2015), for example. Motivated by these observations, we introduce skip-prop vectors, a method for learning multi-vector sentence representations following a “one vector per proposition” strategy. Our approach is based on the skip-thought method of Kiros et al. (2015), which combines neural sequence-to-sequence models (Sutskever et al., 2014) with a skip-gram-like training objective (Mikolov et al., 2013) to obtain generalpurpose sentence representations as a fixed-size vector. Skip-prop capitalizes on the idea that a complex sentence may be represented in terms of the simpler sentences, or proposit"
W17-6936,W12-3018,1,\N,Missing
W17-6944,P16-1231,0,0.0398294,"erns in PredPatt. In this work, we create gold annotations for predicate-argument extraction by converting PropBank annotations on English Web Treebank (EWT) (LDC2012T13) and the Penn Treebank II Wall Street Journal Corpus (WSJ) (Marcus et al., 1994).3 These two corpora have all verbal predicates annotated, and are used to evaluate PredPatt in different perspectives: EWT is the corpus where the gold standard English UD Treebank is built over, which enables an evaluation and analysis of PredPatt patterns; WSJ is used to evaluate PredPatt in a real-world scenario where we run SyntaxNet Parser4 (Andor et al., 2016) on the corpus to generate automated UD parses as input of PredPatt. Table 1 shows the statistics of the auto-converted gold annotations for predicate-argument extraction on EWT and WSJ. We convert the PropBank annotations for all verbal predicates in these two corpora, and ignore roles of directional (DIR), manner (MNR), modals (MOD), negation (NEG) and adverbials (ADV), as they aren’t extracted as distinct argument but instead are folded into the complex predicate by PredPatt and other systems for predicate-argument extraction (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015). Fo"
W17-6944,P15-1034,0,0.499723,"w brand] (3) [Chris] be [the designer] Figure 1: Predicates and arguments extracted by PredPatt.2 The underlying predicate-argument structure constructed by PredPatt is a directed graph, where a special dependency ARG is built between a predicate head token and its arguments’ head tokens, and the original UD relations are retained within predicate phrases and argument phrases. For example, Figure 2 shows the directed graph for the predicate-argument extraction (1) and (2) in Figure 1. Compared to other existing systems for predicate-argument extraction (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015), the use of manual language-agnostic patterns on UD makes PredPatt a well-founded component across languages. Additionally, the underlying structure constructed by PredPatt has been shown to be a well-formed syntax-semantics interface for NLP tasks: Zhang et al. (2016) utilizes PredPatt to extract possibilistic propositions in automatic common-sense inference generation. White et al. (2016) uses PredPatt to help augmenting data with Universal Decompositional Semantics. Zhang et al. (2017) adapts PredPatt to data generation for cross-lingual open information extraction. However, the evaluation"
W17-6944,W05-0620,0,0.221054,"Missing"
W17-6944,de-marneffe-etal-2014-universal,0,0.0591094,"Missing"
W17-6944,D11-1142,0,0.227212,"ner] to launch [a new brand] (3) [Chris] be [the designer] Figure 1: Predicates and arguments extracted by PredPatt.2 The underlying predicate-argument structure constructed by PredPatt is a directed graph, where a special dependency ARG is built between a predicate head token and its arguments’ head tokens, and the original UD relations are retained within predicate phrases and argument phrases. For example, Figure 2 shows the directed graph for the predicate-argument extraction (1) and (2) in Figure 1. Compared to other existing systems for predicate-argument extraction (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015), the use of manual language-agnostic patterns on UD makes PredPatt a well-founded component across languages. Additionally, the underlying structure constructed by PredPatt has been shown to be a well-formed syntax-semantics interface for NLP tasks: Zhang et al. (2016) utilizes PredPatt to extract possibilistic propositions in automatic common-sense inference generation. White et al. (2016) uses PredPatt to help augmenting data with Universal Decompositional Semantics. Zhang et al. (2017) adapts PredPatt to data generation for cross-lingual open information extraction. H"
W17-6944,D15-1076,0,0.0289745,"automated UD. Extraction Head Agreement The rich underlying structure in PredPatt (see Figure 2) contains head information for predicates and arguments, which enables a precision-recall metric based on the agreement of head information. Similar 6 OpenIE 4 is available at: https://github.com/allenai/openie-standalone. The scripts are available at: https://github.com/gabrielStanovsky/oie-benchmark. 8 Studies of PredPatt confidence prediction have been done before, but the current system does not output them. In this evaluation, we assign 1.0 confidence score to all PredPatt extractions. 7 4 to He et al. (2015), we first match an automated predicate with a gold predicate if they both agree on their head.9 With two matched predicates, we then match an automated argument with a gold argument if the automated argument head is within the gold argument span. We evaluate the precision and recall by a loose macro measure: For the i-th extractions that have two matched predicates, let the argument set of the gold predicate be Ai , and the argument set of the automated predicate be Aˆi . The number P of matched arguments is represented by |Ai ∩ Aˆi |. Then the 1 ˆ ˆ precision is computed by Precision = N N i"
W17-6944,N10-1137,0,0.0260359,"er prominent systems. The evaluation results demonstrate that we make a promising improvement on PredPatt, and it significantly outperforms other comparing systems. The scripts for creating gold annotations and evaluation are available at: https: //github.com/hltcoe/PredPatt/tree/master/eval 2 Creating Gold Annotations Open Information Extraction (Open IE) and Semantic Role Labeling (SRL) (Carreras and M`arquez, 2005) are quite related: semantically labeled arguments correspond to the arguments in Open IE extractions, and verbs often match up with Open IE relations (Christensen et al., 2011). Lang and Lapata (2010) has acknowledged that the SRL task can be viewed as a two stage process of (1) recognizing predicates and arguments then (2) assigning semantics. Therefore, predicate-argument extraction (i.e., Open IE) should primarily be considered the same as the first of two stages of SRL, and expert annotated SRL data would be an ideal resource for evaluating Open IE systems. This makes PropBank (Palmer et al., 2005) a natural choice from which we can create gold annotations for Open IE, Here, we choose to use expert annotations from PropBank, as compared to the recent suggestion to employ non-expert ann"
W17-6944,H94-1020,0,0.530322,"e gold annotations for Open IE, Here, we choose to use expert annotations from PropBank, as compared to the recent suggestion to employ non-expert annotations as a means of benchmarking systems Stanovsky and Dagan (2016). Another advantage of choosing PropBank is that PropBank has gold annotations for UD which lays the important groundwork for evaluating UD-based patterns in PredPatt. In this work, we create gold annotations for predicate-argument extraction by converting PropBank annotations on English Web Treebank (EWT) (LDC2012T13) and the Penn Treebank II Wall Street Journal Corpus (WSJ) (Marcus et al., 1994).3 These two corpora have all verbal predicates annotated, and are used to evaluate PredPatt in different perspectives: EWT is the corpus where the gold standard English UD Treebank is built over, which enables an evaluation and analysis of PredPatt patterns; WSJ is used to evaluate PredPatt in a real-world scenario where we run SyntaxNet Parser4 (Andor et al., 2016) on the corpus to generate automated UD parses as input of PredPatt. Table 1 shows the statistics of the auto-converted gold annotations for predicate-argument extraction on EWT and WSJ. We convert the PropBank annotations for all"
W17-6944,D12-1048,0,0.0623355,"Missing"
W17-6944,J05-1004,0,0.019606,"les with gray background. The arguments are colored purple in solid cycles. The head tokens of predicates and arguments are underlined in bold. A special dependency ARG is built between a predicate head token and its arguments head tokens. The UD relations are kept within predicates and arguments. The relations between predicate head tokens are also kept. The upper relations are UD. The lower relations are ARG relations added by PredPatt. In this work, we aim to conduct a large-scale and reproducible evaluation of PredPatt by introducing a large set of gold annotations gathered from PropBank (Palmer et al., 2005). We leverage these gold annotations to improve PredPatt and compare it with other prominent systems. The evaluation results demonstrate that we make a promising improvement on PredPatt, and it significantly outperforms other comparing systems. The scripts for creating gold annotations and evaluation are available at: https: //github.com/hltcoe/PredPatt/tree/master/eval 2 Creating Gold Annotations Open Information Extraction (Open IE) and Semantic Role Labeling (SRL) (Carreras and M`arquez, 2005) are quite related: semantically labeled arguments correspond to the arguments in Open IE extractio"
W17-6944,D16-1252,0,0.110172,"iewed as a two stage process of (1) recognizing predicates and arguments then (2) assigning semantics. Therefore, predicate-argument extraction (i.e., Open IE) should primarily be considered the same as the first of two stages of SRL, and expert annotated SRL data would be an ideal resource for evaluating Open IE systems. This makes PropBank (Palmer et al., 2005) a natural choice from which we can create gold annotations for Open IE, Here, we choose to use expert annotations from PropBank, as compared to the recent suggestion to employ non-expert annotations as a means of benchmarking systems Stanovsky and Dagan (2016). Another advantage of choosing PropBank is that PropBank has gold annotations for UD which lays the important groundwork for evaluating UD-based patterns in PredPatt. In this work, we create gold annotations for predicate-argument extraction by converting PropBank annotations on English Web Treebank (EWT) (LDC2012T13) and the Penn Treebank II Wall Street Journal Corpus (WSJ) (Marcus et al., 1994).3 These two corpora have all verbal predicates annotated, and are used to evaluate PredPatt in different perspectives: EWT is the corpus where the gold standard English UD Treebank is built over, whi"
W17-6944,D16-1177,1,0.803467,"Missing"
W17-6944,E17-2011,1,0.87758,"Missing"
W18-5441,D15-1075,0,0.450763,"lation Ward was born in Perth Extraction I Stefan had visited his son in Bulgaria Stefan was born in Bulgaria Puns I Kim heard masks have no face value Kim heard a pun I Tod heard that thrift is better than annuity Tod heard a pun 3 7 3 7 3 7 Table 1: Example sentence pairs for different semantic phe1 nomena. I indicates the line is a context and the following line is its corresponding hypothesis. 3 and 7 respectively indicate that the context entails, or does not entail the hypothesis. Introduction A plethora of new natural language inference (NLI)1 datasets has been created in recent years (Bowman et al., 2015; Williams et al., 2017; Lai et al., 2017; Khot et al., 2018). However, these datasets do not provide clear insight into what type of reasoning or inference a model may be performing. For example, these datasets cannot be used to evaluate whether competitive NLI models can determine if an event occurred, correctly differentiate between figurative and literal language, or accurately identify and categorize named entities. Consequently, these datasets cannot answer how well sentence representation learning models capture distinct semantic phenomena necessary for general natural language understa"
W18-5441,W17-7203,0,0.0813304,"7). We recast annotations from a total of 13 datasets across 7 NLP tasks into labeled NLI examples. The tasks include event factuality, named entity recognition, gendered anaphora resolution, sentiment analysis, relationship extraction, pun detection, and lexicosyntactic inference (Table 2). Currently, DNC contains over half a million labeled examples that can be used to probe a model’s ability to capture different types of semantic reasoning necessary for general NLU. In short, this work answers a recent plea to the community to test “more kinds of inference” than in previous challenge sets (Chatzikyriakidis et al., 2017). 1 The task of determining if a hypothesis would likely be inferred from a context, or premise; also known as Recognizing Textual Entailment (RTE) (Dagan et al., 2006, 2013). 337 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 337–340 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics 2 Motivation & Background Compared to eliciting NLI datasets directly, i.e. asking humans to author contexts and/or hypothesis sentences, recasting can 1) help determine whether an NLU model performs distinct types o"
W18-5441,P17-1117,0,0.0482847,"Missing"
W18-5441,P16-1204,1,0.834533,"ch that is entailed, neutral, and contradicted by a caption extracted from the Flickr30k corpus (Young et al., 2014). Although these datasets are widely used to train and evaluate sentence representations, a high accuracy is not indicative of what types of reasoning NLI models perform. Workers were free to create any type of hypothesis for each context and label. Such datasets cannot be used to determine how well an NLI model captures many desired capabilities of language understanding systems, e.g. paraphrastic inference, complex anaphora resolution (White et al., 2017), or compositionality (Pavlick and Callison-Burch, 2016; Dasgupta et al., 2018). By converting prior annotation of a specific phenomenon into NLI examples, recasting allows us to create a diverse NLI benchmark that tests a model’s ability to perform distinct types of reasoning. Phenomena Dataset Event Factuality Decomp (Rudinger et al., 2018b) UW (Lee et al., 2015) MeanTime (Minard et al., 2016) Named Entity Recognition Groningen (Bos et al., 2017) CoNLL (Tjong Kim Sang and De Meulder, 2003) Gendered Anaphora Winogender (Rudinger et al., 2018a) Lexicosyntactic Inference VerbCorner (Hartshorne et al., 2013) MegaVeridicality (White and Rawlins, 2018"
W18-5441,S18-2023,1,0.895623,"Missing"
W18-5441,W17-1609,1,0.891312,"Missing"
W18-5441,D15-1284,0,0.0485985,"Missing"
W18-5441,N18-2002,1,0.887236,"Missing"
W18-5441,Q14-1006,0,0.0417421,"or hypothesis sentences, recasting can 1) help determine whether an NLU model performs distinct types of reasoning; 2) limit types of biases observed in previous NLI data; and 3) generate examples cheaply, potentially at large scales. NLU Insights Popular NLI datasets, e.g. Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and its successor MultiNLI (Williams et al., 2017), were created by eliciting hypotheses from humans. Crowd-source workers were tasked with writing one sentence each that is entailed, neutral, and contradicted by a caption extracted from the Flickr30k corpus (Young et al., 2014). Although these datasets are widely used to train and evaluate sentence representations, a high accuracy is not indicative of what types of reasoning NLI models perform. Workers were free to create any type of hypothesis for each context and label. Such datasets cannot be used to determine how well an NLI model captures many desired capabilities of language understanding systems, e.g. paraphrastic inference, complex anaphora resolution (White et al., 2017), or compositionality (Pavlick and Callison-Burch, 2016; Dasgupta et al., 2018). By converting prior annotation of a specific phenomenon in"
W18-5441,N18-1067,1,0.871771,"Missing"
W18-5441,Q17-1027,1,0.883049,"Missing"
W18-5441,W03-0419,0,0.563777,"Missing"
W18-5441,L18-1239,0,0.0690592,"CoNLL (Tjong Kim Sang and De Meulder, 2003) Gendered Anaphora Winogender (Rudinger et al., 2018a) Lexicosyntactic Inference VerbCorner (Hartshorne et al., 2013) MegaVeridicality (White and Rawlins, 2018) VerbNet (Schuler, 2005) Puns (Yang et al., 2015) SemEval 2017 Task 7 (Miller et al., 2017) Relationship Extraction FACC1 (Gabrilovich et al., 2013) Sentiment Analysis (Kotzias et al., 2015) Table 2: List of each type of semantic phenomena paired with its corresponding dataset(s) we recast. cast.2 Experimental results using hypothesis-only models (Poliak et al., 2018; Gururangan et al., 2018; Tsuchiya, 2018) can indicate to what degree the recast datasets retain some biases that may be present in the original semantic datasets. NLI Examples at Large-scale Generating NLI datasets from scratch is costly. Humans must be paid to generate or label natural language text. This linearly scales costs as the amount of generated NLI-pairs increases. Existing annotations for a wide array of semantic NLP tasks are freely available. By leveraging existing semantic annotations already invested in by the community we can generate and label NLI pairs at little cost and create large NLI datasets to train data hung"
W18-5441,I17-1100,1,0.675938,"Missing"
